!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shards86
conflicting_files: null
created_at: 2023-11-27 07:07:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
      fullname: Jarno Auvinen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shards86
      type: user
    createdAt: '2023-11-27T07:07:53.000Z'
    data:
      edited: false
      editors:
      - Shards86
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8261179327964783
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
          fullname: Jarno Auvinen
          isHf: false
          isPro: false
          name: Shards86
          type: user
        html: '<p>I''ve been trying to convert gguf to run it on Raspberry pi 5. </p>

          <p>I have used two different approaches in llama.cpp conversion:</p>

          <ul>

          <li>create vocab.json and merges.txt using hf Tokenizer &gt; create extended
          tokenizer.model &gt; convert gguf</li>

          <li>use modified conversion script from this PR: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/3633">https://github.com/ggerganov/llama.cpp/pull/3633</a>
          &gt; create gguf out of only tokenizer.json</li>

          </ul>

          <p>I have tried different quantizations (Q5_0, Q4_K_M, Q4_0). For some reason
          all approaches end to the same result: when I try to load the model, I get
          error ''Byte not found in vocab''. Do you have any idea, what this could
          be related? The original model is kind of working when using Transformers,
          but is is way too slow for RPi. </p>

          '
        raw: "I've been trying to convert gguf to run it on Raspberry pi 5. \r\n\r\
          \nI have used two different approaches in llama.cpp conversion:\r\n- create\
          \ vocab.json and merges.txt using hf Tokenizer > create extended tokenizer.model\
          \ > convert gguf\r\n- use modified conversion script from this PR: https://github.com/ggerganov/llama.cpp/pull/3633\
          \ > create gguf out of only tokenizer.json\r\n\r\nI have tried different\
          \ quantizations (Q5_0, Q4_K_M, Q4_0). For some reason all approaches end\
          \ to the same result: when I try to load the model, I get error 'Byte not\
          \ found in vocab'. Do you have any idea, what this could be related? The\
          \ original model is kind of working when using Transformers, but is is way\
          \ too slow for RPi. "
        updatedAt: '2023-11-27T07:07:53.166Z'
      numEdits: 0
      reactions: []
    id: 656440492d309fa7e2d9e401
    type: comment
  author: Shards86
  content: "I've been trying to convert gguf to run it on Raspberry pi 5. \r\n\r\n\
    I have used two different approaches in llama.cpp conversion:\r\n- create vocab.json\
    \ and merges.txt using hf Tokenizer > create extended tokenizer.model > convert\
    \ gguf\r\n- use modified conversion script from this PR: https://github.com/ggerganov/llama.cpp/pull/3633\
    \ > create gguf out of only tokenizer.json\r\n\r\nI have tried different quantizations\
    \ (Q5_0, Q4_K_M, Q4_0). For some reason all approaches end to the same result:\
    \ when I try to load the model, I get error 'Byte not found in vocab'. Do you\
    \ have any idea, what this could be related? The original model is kind of working\
    \ when using Transformers, but is is way too slow for RPi. "
  created_at: 2023-11-27 07:07:53+00:00
  edited: false
  hidden: false
  id: 656440492d309fa7e2d9e401
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
      fullname: Ai Creator
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AiCreatornator
      type: user
    createdAt: '2023-11-30T06:10:08.000Z'
    data:
      edited: true
      editors:
      - AiCreatornator
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9859857559204102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
          fullname: Ai Creator
          isHf: false
          isPro: false
          name: AiCreatornator
          type: user
        html: '<p>Hi, have you done that tokenizer.model ? Could you share it? I''m
          trying to make gguf-file too, but as far I understand it needs that tokenizer.model
          that has been removed from this repo. Or can you tell where is a quide to
          create it?</p>

          <p>EDIT: I got the quantazion to work without tokenizer.model. I used that
          repo 3633 you linked. The problem was that I was trying to use it with llama.cpp
          in oogabooga, but it worked llamacpp_HF instead.</p>

          <p>EDIT 2: Correction, it needs tokenizer.model to run it with llamacpp_HF.
          But it seems to work somehow with some faulty tokenizer.model in the same
          folder. Also important step was:</p>

          <h1 id="update-the-gguf-filetype-to-current-if-older-version-is-unsupported-by-another-application">update
          the gguf filetype to current if older version is unsupported by another
          application</h1>

          <p>./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf
          COPY</p>

          '
        raw: 'Hi, have you done that tokenizer.model ? Could you share it? I''m trying
          to make gguf-file too, but as far I understand it needs that tokenizer.model
          that has been removed from this repo. Or can you tell where is a quide to
          create it?


          EDIT: I got the quantazion to work without tokenizer.model. I used that
          repo 3633 you linked. The problem was that I was trying to use it with llama.cpp
          in oogabooga, but it worked llamacpp_HF instead.


          EDIT 2: Correction, it needs tokenizer.model to run it with llamacpp_HF.
          But it seems to work somehow with some faulty tokenizer.model in the same
          folder. Also important step was:

          # update the gguf filetype to current if older version is unsupported by
          another application

          ./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf
          COPY'
        updatedAt: '2023-11-30T08:53:28.323Z'
      numEdits: 5
      reactions: []
    id: 65682740253c8b0b67b9be85
    type: comment
  author: AiCreatornator
  content: 'Hi, have you done that tokenizer.model ? Could you share it? I''m trying
    to make gguf-file too, but as far I understand it needs that tokenizer.model that
    has been removed from this repo. Or can you tell where is a quide to create it?


    EDIT: I got the quantazion to work without tokenizer.model. I used that repo 3633
    you linked. The problem was that I was trying to use it with llama.cpp in oogabooga,
    but it worked llamacpp_HF instead.


    EDIT 2: Correction, it needs tokenizer.model to run it with llamacpp_HF. But it
    seems to work somehow with some faulty tokenizer.model in the same folder. Also
    important step was:

    # update the gguf filetype to current if older version is unsupported by another
    application

    ./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf
    COPY'
  created_at: 2023-11-30 06:10:08+00:00
  edited: true
  hidden: false
  id: 65682740253c8b0b67b9be85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
      fullname: Jarno Auvinen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shards86
      type: user
    createdAt: '2023-11-30T18:45:09.000Z'
    data:
      edited: false
      editors:
      - Shards86
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9435675144195557
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
          fullname: Jarno Auvinen
          isHf: false
          isPro: false
          name: Shards86
          type: user
        html: '<p>Cool, you actually got it working! I will have to try again, I am
          not sure if I did the version conversion you mentioned in the end.</p>

          <p>FYI tokenizer.model conversion was done using instructions from here:
          <a rel="nofollow" href="https://github.com/huggingface/tokenizers/issues/521">https://github.com/huggingface/tokenizers/issues/521</a></p>

          '
        raw: 'Cool, you actually got it working! I will have to try again, I am not
          sure if I did the version conversion you mentioned in the end.


          FYI tokenizer.model conversion was done using instructions from here: https://github.com/huggingface/tokenizers/issues/521'
        updatedAt: '2023-11-30T18:45:09.283Z'
      numEdits: 0
      reactions: []
    id: 6568d8354b8d35447acf88a6
    type: comment
  author: Shards86
  content: 'Cool, you actually got it working! I will have to try again, I am not
    sure if I did the version conversion you mentioned in the end.


    FYI tokenizer.model conversion was done using instructions from here: https://github.com/huggingface/tokenizers/issues/521'
  created_at: 2023-11-30 18:45:09+00:00
  edited: false
  hidden: false
  id: 6568d8354b8d35447acf88a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
      fullname: Jarno Auvinen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shards86
      type: user
    createdAt: '2023-12-01T08:27:22.000Z'
    data:
      edited: true
      editors:
      - Shards86
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447575807571411
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632ea45d2636f057d5811ac2/_9faZTWbh1geyEwLvyUE5.png?w=200&h=200&f=face
          fullname: Jarno Auvinen
          isHf: false
          isPro: false
          name: Shards86
          type: user
        html: '<p>Yesterday I tested the conversions again with my laptop. I also
          installed oogabooga''s webui to PC - it indeed works on Windows using llamacpp_HF
          both with GPU and CPU only configs. </p>

          <p>I don''t get it why the exactly same model won''t work on Linux/aarch64.
          Shouldn''t be memory issue, because other 3B models are working great. </p>

          <p>Oh well, I will have to keep experimenting</p>

          '
        raw: "Yesterday I tested the conversions again with my laptop. I also installed\
          \ oogabooga's webui to PC - it indeed works on Windows using llamacpp_HF\
          \ both with GPU and CPU only configs. \n\nI don't get it why the exactly\
          \ same model won't work on Linux/aarch64. Shouldn't be memory issue, because\
          \ other 3B models are working great. \n\nOh well, I will have to keep experimenting"
        updatedAt: '2023-12-01T08:27:58.417Z'
      numEdits: 1
      reactions: []
    id: 656998ea4b8d35447af5d9fe
    type: comment
  author: Shards86
  content: "Yesterday I tested the conversions again with my laptop. I also installed\
    \ oogabooga's webui to PC - it indeed works on Windows using llamacpp_HF both\
    \ with GPU and CPU only configs. \n\nI don't get it why the exactly same model\
    \ won't work on Linux/aarch64. Shouldn't be memory issue, because other 3B models\
    \ are working great. \n\nOh well, I will have to keep experimenting"
  created_at: 2023-12-01 08:27:22+00:00
  edited: true
  hidden: false
  id: 656998ea4b8d35447af5d9fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Finnish-NLP/llama-3b-finnish
repo_type: model
status: open
target_branch: null
title: Byte not found in vocab
