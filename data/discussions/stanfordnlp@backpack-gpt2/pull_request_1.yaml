!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ivanzhouyq
conflicting_files: []
created_at: 2023-05-29 22:05:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313f5f4c093ff968e0ec6c8/LVTpwU-pXVDhnJcEbDAEx.jpeg?w=200&h=200&f=face
      fullname: Ivan Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanzhouyq
      type: user
    createdAt: '2023-05-29T23:05:45.000Z'
    data:
      edited: true
      editors:
      - ivanzhouyq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313f5f4c093ff968e0ec6c8/LVTpwU-pXVDhnJcEbDAEx.jpeg?w=200&h=200&f=face
          fullname: Ivan Zhou
          isHf: false
          isPro: false
          name: ivanzhouyq
          type: user
        html: '<p>This PR sets more precise shape to the attention''s weights, biases,
          and the outputs. </p>

          <p>The original implementation assumes that the embedding size is a multiple
          of senses. However, when the hyperparamters are picked so that the embedding
          size is no longer the multiple of the number of senses, it would cause different
          number of parameters in <code>encoded</code> before and after reshaping.
          I found this mismatching when testing with a larger model with  embedding
          size = 1280 and number of senses at 48. I received the following error:</p>

          <pre><code>- sense_weight_net.c_attn.bias: found shape torch.Size([2496])
          in the checkpoint and torch.Size([2560]) in the model instantiated

          - sense_weight_net.c_attn.weight: found shape torch.Size([2496, 1280]) in
          the checkpoint and torch.Size([2560, 1280]) in the model instantiated

          </code></pre>

          <p>This PR addresses this issue. Of course, a better solution should be
          recommending/enforcing the embedding size to be a full multiple of senses.</p>

          '
        raw: "This PR sets more precise shape to the attention's weights, biases,\
          \ and the outputs. \n\nThe original implementation assumes that the embedding\
          \ size is a multiple of senses. However, when the hyperparamters are picked\
          \ so that the embedding size is no longer the multiple of the number of\
          \ senses, it would cause different number of parameters in `encoded` before\
          \ and after reshaping. I found this mismatching when testing with a larger\
          \ model with  embedding size = 1280 and number of senses at 48. I received\
          \ the following error:\n\n```\n- sense_weight_net.c_attn.bias: found shape\
          \ torch.Size([2496]) in the checkpoint and torch.Size([2560]) in the model\
          \ instantiated\n- sense_weight_net.c_attn.weight: found shape torch.Size([2496,\
          \ 1280]) in the checkpoint and torch.Size([2560, 1280]) in the model instantiated\n\
          ```\n\nThis PR addresses this issue. Of course, a better solution should\
          \ be recommending/enforcing the embedding size to be a full multiple of\
          \ senses."
        updatedAt: '2023-05-29T23:06:27.664Z'
      numEdits: 1
      reactions: []
    id: 64752fc9d56974d0c06625b6
    type: comment
  author: ivanzhouyq
  content: "This PR sets more precise shape to the attention's weights, biases, and\
    \ the outputs. \n\nThe original implementation assumes that the embedding size\
    \ is a multiple of senses. However, when the hyperparamters are picked so that\
    \ the embedding size is no longer the multiple of the number of senses, it would\
    \ cause different number of parameters in `encoded` before and after reshaping.\
    \ I found this mismatching when testing with a larger model with  embedding size\
    \ = 1280 and number of senses at 48. I received the following error:\n\n```\n\
    - sense_weight_net.c_attn.bias: found shape torch.Size([2496]) in the checkpoint\
    \ and torch.Size([2560]) in the model instantiated\n- sense_weight_net.c_attn.weight:\
    \ found shape torch.Size([2496, 1280]) in the checkpoint and torch.Size([2560,\
    \ 1280]) in the model instantiated\n```\n\nThis PR addresses this issue. Of course,\
    \ a better solution should be recommending/enforcing the embedding size to be\
    \ a full multiple of senses."
  created_at: 2023-05-29 22:05:45+00:00
  edited: true
  hidden: false
  id: 64752fc9d56974d0c06625b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313f5f4c093ff968e0ec6c8/LVTpwU-pXVDhnJcEbDAEx.jpeg?w=200&h=200&f=face
      fullname: Ivan Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanzhouyq
      type: user
    createdAt: '2023-05-29T23:05:46.000Z'
    data:
      oid: b2c5167f662e0f000f52ee4dd00e67af376067ca
      parents:
      - 988edafd26cf347c37e2903136bee56b1b8d56da
      subject: Set more precise shape to the attention weights and outputs
    id: 64752fca0000000000000000
    type: commit
  author: ivanzhouyq
  created_at: 2023-05-29 22:05:46+00:00
  id: 64752fca0000000000000000
  oid: b2c5167f662e0f000f52ee4dd00e67af376067ca
  summary: Set more precise shape to the attention weights and outputs
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/QPCRraySl9zTnf7eG5ZJk.png?w=200&h=200&f=face
      fullname: John Hewitt
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: johnhew
      type: user
    createdAt: '2023-08-14T20:03:29.000Z'
    data:
      status: merged
    id: 64da889170446182be5b9246
    type: status-change
  author: johnhew
  created_at: 2023-08-14 19:03:29+00:00
  id: 64da889170446182be5b9246
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: a9697cc980518f960a8da42082510eebc6d966b9
num: 1
repo_id: stanfordnlp/backpack-gpt2
repo_type: model
status: merged
target_branch: refs/heads/main
title: Set more precise shape to the attention weights and outputs
