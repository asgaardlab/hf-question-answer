!!python/object:huggingface_hub.community.DiscussionWithDetails
author: samge
conflicting_files: null
created_at: 2023-05-22 07:08:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672984342967-639932daafe0d224cf2bdb3a.jpeg?w=200&h=200&f=face
      fullname: shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samge
      type: user
    createdAt: '2023-05-22T08:08:19.000Z'
    data:
      edited: false
      editors:
      - samge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672984342967-639932daafe0d224cf2bdb3a.jpeg?w=200&h=200&f=face
          fullname: shao
          isHf: false
          isPro: false
          name: samge
          type: user
        html: "<p>\u6211\u4F7F\u7528<code>text-generation-webui</code>\u7684<code>One-click\
          \ installers\uFF08Windows\uFF09</code>\uFF0C\u8FD0\u884C\u63D0\u793A\uFF1A\
          <br>\u9519\u8BEF\u4FE1\u606F\uFF1A</p>\n<pre><code class=\"language-text\"\
          >INFO:Cache capacity is 0 bytes\nllama.cpp: loading model from models\\\
          alpaca-13b-plus\\ggml-model-q4_0.bin\nllama_model_load_internal: format\
          \     = ggjt v1 (pre #1405)\nllama_model_load_internal: n_vocab    = 49954\n\
          llama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\n\
          llama_model_load_internal: n_ff       = 13824\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 13B\nerror loading\
          \ model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)\n\
          llama_init_from_file: failed to load model\nTraceback (most recent call\
          \ last):\n  File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 1045, in &lt;module&gt;\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\n  File \"D:\\APP\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 95, in load_model\n  \
          \  output = load_func(model_name)\n  File \"D:\\APP\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 258, in llamacpp_loader\n\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n  File\
          \ \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 50, in from_pretrained\n    self.model = Llama(**params)\n  File\
          \ \"D:\\APP\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp\\llama.py\", line 161, in __init__\n    assert self.ctx is not\
          \ None\nAssertionError\nException ignored in: &lt;function LlamaCppModel.__del__\
          \ at 0x00000232CE78C4C0&gt;\nTraceback (most recent call last):\n  File\
          \ \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 23, in __del__\n    self.model.__del__()\nAttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'\n\nDone!\n</code></pre>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/639932daafe0d224cf2bdb3a/1j0B056dlNFodAOAY0M_9.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/639932daafe0d224cf2bdb3a/1j0B056dlNFodAOAY0M_9.png\"\
          ></a></p>\n"
        raw: "\u6211\u4F7F\u7528`text-generation-webui`\u7684`One-click installers\uFF08\
          Windows\uFF09`\uFF0C\u8FD0\u884C\u63D0\u793A\uFF1A\r\n\u9519\u8BEF\u4FE1\
          \u606F\uFF1A\r\n```text\r\nINFO:Cache capacity is 0 bytes\r\nllama.cpp:\
          \ loading model from models\\alpaca-13b-plus\\ggml-model-q4_0.bin\r\nllama_model_load_internal:\
          \ format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab\
          \    = 49954\r\nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal:\
          \ n_embd     = 5120\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal:\
          \ n_head     = 40\r\nllama_model_load_internal: n_layer    = 40\r\nllama_model_load_internal:\
          \ n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly\
          \ Q4_0)\r\nllama_model_load_internal: n_ff       = 13824\r\nllama_model_load_internal:\
          \ n_parts    = 1\r\nllama_model_load_internal: model size = 13B\r\nerror\
          \ loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)\r\
          \nllama_init_from_file: failed to load model\r\nTraceback (most recent call\
          \ last):\r\n  File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 1045, in <module>\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\n  File \"D:\\APP\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 95, in load_model\r\n\
          \    output = load_func(model_name)\r\n  File \"D:\\APP\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 258, in llamacpp_loader\r\
          \n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n \
          \ File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          llamacpp_model.py\", line 50, in from_pretrained\r\n    self.model = Llama(**params)\r\
          \n  File \"D:\\APP\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp\\llama.py\", line 161, in __init__\r\n    assert self.ctx is not\
          \ None\r\nAssertionError\r\nException ignored in: <function LlamaCppModel.__del__\
          \ at 0x00000232CE78C4C0>\r\nTraceback (most recent call last):\r\n  File\
          \ \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 23, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'\r\n\r\nDone!\r\n```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/639932daafe0d224cf2bdb3a/1j0B056dlNFodAOAY0M_9.png)\r\
          \n"
        updatedAt: '2023-05-22T08:08:19.185Z'
      numEdits: 0
      reactions: []
    id: 646b22f3628e5b50b2dc75e0
    type: comment
  author: samge
  content: "\u6211\u4F7F\u7528`text-generation-webui`\u7684`One-click installers\uFF08\
    Windows\uFF09`\uFF0C\u8FD0\u884C\u63D0\u793A\uFF1A\r\n\u9519\u8BEF\u4FE1\u606F\
    \uFF1A\r\n```text\r\nINFO:Cache capacity is 0 bytes\r\nllama.cpp: loading model\
    \ from models\\alpaca-13b-plus\\ggml-model-q4_0.bin\r\nllama_model_load_internal:\
    \ format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    =\
    \ 49954\r\nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal:\
    \ n_embd     = 5120\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal:\
    \ n_head     = 40\r\nllama_model_load_internal: n_layer    = 40\r\nllama_model_load_internal:\
    \ n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\
    \nllama_model_load_internal: n_ff       = 13824\r\nllama_model_load_internal:\
    \ n_parts    = 1\r\nllama_model_load_internal: model size = 13B\r\nerror loading\
    \ model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)\r\
    \nllama_init_from_file: failed to load model\r\nTraceback (most recent call last):\r\
    \n  File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\server.py\"\
    , line 1045, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
    , line 95, in load_model\r\n    output = load_func(model_name)\r\n  File \"D:\\\
    APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line\
    \ 258, in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n  File \"D:\\APP\\AI\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 50, in from_pretrained\r\n    self.model = Llama(**params)\r\n  File \"\
    D:\\APP\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\llama_cpp\\\
    llama.py\", line 161, in __init__\r\n    assert self.ctx is not None\r\nAssertionError\r\
    \nException ignored in: <function LlamaCppModel.__del__ at 0x00000232CE78C4C0>\r\
    \nTraceback (most recent call last):\r\n  File \"D:\\APP\\AI\\oobabooga_windows\\\
    text-generation-webui\\modules\\llamacpp_model.py\", line 23, in __del__\r\n \
    \   self.model.__del__()\r\nAttributeError: 'LlamaCppModel' object has no attribute\
    \ 'model'\r\n\r\nDone!\r\n```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/639932daafe0d224cf2bdb3a/1j0B056dlNFodAOAY0M_9.png)\r\
    \n"
  created_at: 2023-05-22 07:08:19+00:00
  edited: false
  hidden: false
  id: 646b22f3628e5b50b2dc75e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2b5245dd8c778876df18413cfaec3d.svg
      fullname: guochang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: noooooob
      type: user
    createdAt: '2023-05-25T03:57:20.000Z'
    data:
      edited: false
      editors:
      - noooooob
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2b5245dd8c778876df18413cfaec3d.svg
          fullname: guochang
          isHf: false
          isPro: false
          name: noooooob
          type: user
        html: "<p>\u6211\u4E5F\u662F\u8FD9\u4E2A\u9519\u8BEF\u3002\u3002</p>\n"
        raw: "\u6211\u4E5F\u662F\u8FD9\u4E2A\u9519\u8BEF\u3002\u3002"
        updatedAt: '2023-05-25T03:57:20.024Z'
      numEdits: 0
      reactions: []
    id: 646edca034fde71fdaa7e30d
    type: comment
  author: noooooob
  content: "\u6211\u4E5F\u662F\u8FD9\u4E2A\u9519\u8BEF\u3002\u3002"
  created_at: 2023-05-25 02:57:20+00:00
  edited: false
  hidden: false
  id: 646edca034fde71fdaa7e30d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70663f932b69304ce2283057d4a25c25.svg
      fullname: John Lui
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: johnlui
      type: user
    createdAt: '2023-05-25T05:30:18.000Z'
    data:
      edited: false
      editors:
      - johnlui
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70663f932b69304ce2283057d4a25c25.svg
          fullname: John Lui
          isHf: false
          isPro: false
          name: johnlui
          type: user
        html: "<p>\u56E0\u4E3A\u6211\u662F\u7528\u8001\u7248\u672C\u7684 llama.cpp\
          \ \u8F6C\u6362\u7684\u683C\u5F0F\uFF0C\u662F V1 \u7684\uFF0C\u73B0\u5728\
          \u7684\u5DF2\u7ECF\u662F V2 \u7684\u4E86\uFF0C\u7B49\u6211\u6709\u7A7A\u4E86\
          \u66F4\u65B0\u4E00\u4E0B\u3002\u3002\u3002</p>\n"
        raw: "\u56E0\u4E3A\u6211\u662F\u7528\u8001\u7248\u672C\u7684 llama.cpp \u8F6C\
          \u6362\u7684\u683C\u5F0F\uFF0C\u662F V1 \u7684\uFF0C\u73B0\u5728\u7684\u5DF2\
          \u7ECF\u662F V2 \u7684\u4E86\uFF0C\u7B49\u6211\u6709\u7A7A\u4E86\u66F4\u65B0\
          \u4E00\u4E0B\u3002\u3002\u3002"
        updatedAt: '2023-05-25T05:30:18.818Z'
      numEdits: 0
      reactions: []
    id: 646ef26a7942c36e9dba2a20
    type: comment
  author: johnlui
  content: "\u56E0\u4E3A\u6211\u662F\u7528\u8001\u7248\u672C\u7684 llama.cpp \u8F6C\
    \u6362\u7684\u683C\u5F0F\uFF0C\u662F V1 \u7684\uFF0C\u73B0\u5728\u7684\u5DF2\u7ECF\
    \u662F V2 \u7684\u4E86\uFF0C\u7B49\u6211\u6709\u7A7A\u4E86\u66F4\u65B0\u4E00\u4E0B\
    \u3002\u3002\u3002"
  created_at: 2023-05-25 04:30:18+00:00
  edited: false
  hidden: false
  id: 646ef26a7942c36e9dba2a20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b3baeb6c5ccbabe2934d99c244f5c82.svg
      fullname: Melinda G. Serpa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MelindaGSerpa
      type: user
    createdAt: '2023-06-05T01:22:16.000Z'
    data:
      edited: false
      editors:
      - MelindaGSerpa
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9989523887634277
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b3baeb6c5ccbabe2934d99c244f5c82.svg
          fullname: Melinda G. Serpa
          isHf: false
          isPro: false
          name: MelindaGSerpa
          type: user
        html: "<p>\u4F60\u597D\u5927\u54E5 \u8ACB\u554F\u4F60\u6709\u65B0\u7684\u683C\
          \u5F0F\u4E86\u55CE?</p>\n"
        raw: "\u4F60\u597D\u5927\u54E5 \u8ACB\u554F\u4F60\u6709\u65B0\u7684\u683C\u5F0F\
          \u4E86\u55CE?"
        updatedAt: '2023-06-05T01:22:16.894Z'
      numEdits: 0
      reactions: []
    id: 647d38c81c0644de8d3b0855
    type: comment
  author: MelindaGSerpa
  content: "\u4F60\u597D\u5927\u54E5 \u8ACB\u554F\u4F60\u6709\u65B0\u7684\u683C\u5F0F\
    \u4E86\u55CE?"
  created_at: 2023-06-05 00:22:16+00:00
  edited: false
  hidden: false
  id: 647d38c81c0644de8d3b0855
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70663f932b69304ce2283057d4a25c25.svg
      fullname: John Lui
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: johnlui
      type: user
    createdAt: '2023-06-09T18:42:48.000Z'
    data:
      edited: false
      editors:
      - johnlui
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9897310137748718
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70663f932b69304ce2283057d4a25c25.svg
          fullname: John Lui
          isHf: false
          isPro: false
          name: johnlui
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;samge&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/samge\">@<span class=\"\
          underline\">samge</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;noooooob&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/noooooob\">@<span class=\"underline\">noooooob</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;MelindaGSerpa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MelindaGSerpa\"\
          >@<span class=\"underline\">MelindaGSerpa</span></a></span>\n\n\t</span></span>\
          \ \u6587\u4EF6\u5DF2\u7ECF\u66F4\u65B0\u5230\u6700\u65B0\u7684 v3 \u683C\
          \u5F0F\uFF0C\u53EF\u4EE5\u6B63\u5E38\u4F7F\u7528\u4E86</p>\n"
        raw: "@samge @noooooob @MelindaGSerpa \u6587\u4EF6\u5DF2\u7ECF\u66F4\u65B0\
          \u5230\u6700\u65B0\u7684 v3 \u683C\u5F0F\uFF0C\u53EF\u4EE5\u6B63\u5E38\u4F7F\
          \u7528\u4E86"
        updatedAt: '2023-06-09T18:42:48.656Z'
      numEdits: 0
      reactions: []
    id: 648372a80087ddd12631bf93
    type: comment
  author: johnlui
  content: "@samge @noooooob @MelindaGSerpa \u6587\u4EF6\u5DF2\u7ECF\u66F4\u65B0\u5230\
    \u6700\u65B0\u7684 v3 \u683C\u5F0F\uFF0C\u53EF\u4EE5\u6B63\u5E38\u4F7F\u7528\u4E86"
  created_at: 2023-06-09 17:42:48+00:00
  edited: false
  hidden: false
  id: 648372a80087ddd12631bf93
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: johnlui/chinese-alpaca-13b-plus-quantized
repo_type: model
status: open
target_branch: null
title: "\u8FD0\u884C\u63D0\u793A\uFF1AAttributeError: 'LlamaCppModel' object has no\
  \ attribute 'model'"
