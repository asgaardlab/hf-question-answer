!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BangJago
conflicting_files: null
created_at: 2023-10-09 22:14:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-09T23:14:31.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
          fullname: Bang Jago
          isHf: false
          isPro: false
          name: BangJago
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-19T15:20:27.545Z'
      numEdits: 0
      reactions: []
    id: 65248957b02238037e635e4a
    type: comment
  author: BangJago
  content: This comment has been hidden
  created_at: 2023-10-09 22:14:31+00:00
  edited: true
  hidden: true
  id: 65248957b02238037e635e4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-10T09:10:15.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.977303147315979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>I don''t think TPU support GPTQ.</p>

          <p>Colab should have large vram nvidia gpus instead. like A40?</p>

          '
        raw: 'I don''t think TPU support GPTQ.


          Colab should have large vram nvidia gpus instead. like A40?'
        updatedAt: '2023-10-10T09:10:15.889Z'
      numEdits: 0
      reactions: []
    id: 652514f73aa1a6ea921fde2a
    type: comment
  author: Yhyu13
  content: 'I don''t think TPU support GPTQ.


    Colab should have large vram nvidia gpus instead. like A40?'
  created_at: 2023-10-10 08:10:15+00:00
  edited: false
  hidden: false
  id: 652514f73aa1a6ea921fde2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-11T08:31:32.000Z'
    data:
      edited: true
      editors:
      - BangJago
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47359758615493774
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
          fullname: Bang Jago
          isHf: false
          isPro: false
          name: BangJago
          type: user
        html: '<blockquote>

          <p>I don''t think TPU support GPTQ.</p>

          <p>Colab should have large vram nvidia gpus instead. like A40?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6524867b0e7792fa3aad7fec/FpZRzzEHRWpvXdzMizpr8.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6524867b0e7792fa3aad7fec/FpZRzzEHRWpvXdzMizpr8.png"></a></p>

          <p>I saw this output so I wanted to make sure by asking here well if this
          model doesn''t support TPU alright then, I''ve given up using TPU. And Colab
          only has T4 GPU and TPU for the free ones and the paid ones have A100 GPU
          and V100 GPU.</p>

          <p>Because I don''t have a credit card so I can''t buy it and it''s quite
          expensive for me because im from a country with a smaller currency.</p>

          '
        raw: "> I don't think TPU support GPTQ.\n> \n> Colab should have large vram\
          \ nvidia gpus instead. like A40?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6524867b0e7792fa3aad7fec/FpZRzzEHRWpvXdzMizpr8.png)\n\
          \nI saw this output so I wanted to make sure by asking here well if this\
          \ model doesn't support TPU alright then, I've given up using TPU. And Colab\
          \ only has T4 GPU and TPU for the free ones and the paid ones have A100\
          \ GPU and V100 GPU.\n\nBecause I don't have a credit card so I can't buy\
          \ it and it's quite expensive for me because im from a country with a smaller\
          \ currency."
        updatedAt: '2023-10-19T10:47:34.453Z'
      numEdits: 1
      reactions: []
    id: 65265d642e3bb4cbb6012fa8
    type: comment
  author: BangJago
  content: "> I don't think TPU support GPTQ.\n> \n> Colab should have large vram\
    \ nvidia gpus instead. like A40?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6524867b0e7792fa3aad7fec/FpZRzzEHRWpvXdzMizpr8.png)\n\
    \nI saw this output so I wanted to make sure by asking here well if this model\
    \ doesn't support TPU alright then, I've given up using TPU. And Colab only has\
    \ T4 GPU and TPU for the free ones and the paid ones have A100 GPU and V100 GPU.\n\
    \nBecause I don't have a credit card so I can't buy it and it's quite expensive\
    \ for me because im from a country with a smaller currency."
  created_at: 2023-10-11 07:31:32+00:00
  edited: true
  hidden: false
  id: 65265d642e3bb4cbb6012fa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-14T15:20:36.000Z'
    data:
      status: closed
    id: 652ab1c466313ebb618a0320
    type: status-change
  author: BangJago
  created_at: 2023-10-14 14:20:36+00:00
  id: 652ab1c466313ebb618a0320
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-14T16:13:29.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9336273074150085
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>The reason it\u2019s not working is because you have to use a GPTQ\
          \ model not gguf? Also, use exllama or exllama v2 with flash attention and\
          \ you should be able to run it large contexts and less vram. </p>\n<p>Exllama\
          \ and exllama v2 is much faster than autogptq and transformers so that\u2019\
          s also good.</p>\n"
        raw: "The reason it\u2019s not working is because you have to use a GPTQ model\
          \ not gguf? Also, use exllama or exllama v2 with flash attention and you\
          \ should be able to run it large contexts and less vram. \n\nExllama and\
          \ exllama v2 is much faster than autogptq and transformers so that\u2019\
          s also good."
        updatedAt: '2023-10-14T16:13:29.826Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BangJago
    id: 652abe29c543a08aa9ed2286
    type: comment
  author: YaTharThShaRma999
  content: "The reason it\u2019s not working is because you have to use a GPTQ model\
    \ not gguf? Also, use exllama or exllama v2 with flash attention and you should\
    \ be able to run it large contexts and less vram. \n\nExllama and exllama v2 is\
    \ much faster than autogptq and transformers so that\u2019s also good."
  created_at: 2023-10-14 15:13:29+00:00
  edited: false
  hidden: false
  id: 652abe29c543a08aa9ed2286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-19T10:53:03.000Z'
    data:
      from: Uhh why can I run this model using GPU but can't use TPU on google Colab
      to: Uhh why can I run this model using GPU but I can't use it with TPU on google
        Colab
    id: 65310a8fa004ecb26ab4713f
    type: title-change
  author: BangJago
  created_at: 2023-10-19 09:53:03+00:00
  id: 65310a8fa004ecb26ab4713f
  new_title: Uhh why can I run this model using GPU but I can't use it with TPU on
    google Colab
  old_title: Uhh why can I run this model using GPU but can't use TPU on google Colab
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-19T10:53:30.000Z'
    data:
      from: Uhh why can I run this model using GPU but I can't use it with TPU on
        google Colab
      to: Uhh, I run this model using GPU but why can't I use it with TPU on google
        Colab
    id: 65310aaaf8231285122548b7
    type: title-change
  author: BangJago
  created_at: 2023-10-19 09:53:30+00:00
  id: 65310aaaf8231285122548b7
  new_title: Uhh, I run this model using GPU but why can't I use it with TPU on google
    Colab
  old_title: Uhh why can I run this model using GPU but I can't use it with TPU on
    google Colab
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
      fullname: Bang Jago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BangJago
      type: user
    createdAt: '2023-10-19T15:26:52.000Z'
    data:
      edited: false
      editors:
      - BangJago
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9374886751174927
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/a_mMK7InfM1Ws_F_AyrHP.png?w=200&h=200&f=face
          fullname: Bang Jago
          isHf: false
          isPro: false
          name: BangJago
          type: user
        html: "<blockquote>\n<p>The reason it\u2019s not working is because you have\
          \ to use a GPTQ model not gguf? Also, use exllama or exllama v2 with flash\
          \ attention and you should be able to run it large contexts and less vram.\
          \ </p>\n<p>Exllama and exllama v2 is much faster than autogptq and transformers\
          \ so that\u2019s also good.</p>\n</blockquote>\n<p>Okay, first of all thanks\
          \ for the advice and please just forget about my question, I'm really asking\
          \ a stupid question for not reading the model description properly. Sorry\
          \ for wasting your time reading this discussion... \U0001F613</p>\n"
        raw: "> The reason it\u2019s not working is because you have to use a GPTQ\
          \ model not gguf? Also, use exllama or exllama v2 with flash attention and\
          \ you should be able to run it large contexts and less vram. \n> \n> Exllama\
          \ and exllama v2 is much faster than autogptq and transformers so that\u2019\
          s also good.\n\n\nOkay, first of all thanks for the advice and please just\
          \ forget about my question, I'm really asking a stupid question for not\
          \ reading the model description properly. Sorry for wasting your time reading\
          \ this discussion... \U0001F613"
        updatedAt: '2023-10-19T15:26:52.841Z'
      numEdits: 0
      reactions: []
    id: 65314abc175adfb85c813931
    type: comment
  author: BangJago
  content: "> The reason it\u2019s not working is because you have to use a GPTQ model\
    \ not gguf? Also, use exllama or exllama v2 with flash attention and you should\
    \ be able to run it large contexts and less vram. \n> \n> Exllama and exllama\
    \ v2 is much faster than autogptq and transformers so that\u2019s also good.\n\
    \n\nOkay, first of all thanks for the advice and please just forget about my question,\
    \ I'm really asking a stupid question for not reading the model description properly.\
    \ Sorry for wasting your time reading this discussion... \U0001F613"
  created_at: 2023-10-19 14:26:52+00:00
  edited: false
  hidden: false
  id: 65314abc175adfb85c813931
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-19T15:32:42.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215076565742493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BangJago&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BangJago\">@<span class=\"\
          underline\">BangJago</span></a></span>\n\n\t</span></span> everyone has\
          \ to learn so it\u2019s not wasting Time. Anyway have a good time with these\
          \ models(: </p>\n"
        raw: "@BangJago everyone has to learn so it\u2019s not wasting Time. Anyway\
          \ have a good time with these models(: "
        updatedAt: '2023-10-19T15:32:42.353Z'
      numEdits: 0
      reactions: []
    id: 65314c1acfbf40552d90b485
    type: comment
  author: YaTharThShaRma999
  content: "@BangJago everyone has to learn so it\u2019s not wasting Time. Anyway\
    \ have a good time with these models(: "
  created_at: 2023-10-19 14:32:42+00:00
  edited: false
  hidden: false
  id: 65314c1acfbf40552d90b485
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Mythalion-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Uhh, I run this model using GPU but why can't I use it with TPU on google Colab
