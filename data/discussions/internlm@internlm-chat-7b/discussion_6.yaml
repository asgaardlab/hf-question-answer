!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-09-30 19:42:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-09-30T20:42:22.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.26644188165664673
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: "<p>(textgen) [root@pve-m7330 text-generation-webui]# python server.py\
          \ --api --listen --trust-remote-code --disk-cache-dir /data/tmp --use_double_quant\
          \ --quant_type nf4 --numa --load-in-4bit --settings settings-template.yaml\
          \ --model models/internlm-chat-7b/<br>2023-09-30 13:38:28 WARNING:trust_remote_code\
          \ is enabled. This is dangerous.<br>2023-09-30 13:38:28 WARNING:<br>You\
          \ are potentially exposing the web UI to the entire internet without any\
          \ access password.<br>You can create one with the \"--gradio-auth\" flag\
          \ like this:</p>\n<p>--gradio-auth username:password</p>\n<p>Make sure to\
          \ replace username:password with your own.<br>2023-09-30 13:38:31 INFO:Loading\
          \ settings from settings-template.yaml...<br>2023-09-30 13:38:31 INFO:Loading\
          \ internlm-chat-7b...<br>2023-09-30 13:38:31 INFO:Using the following 4-bit\
          \ params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16,\
          \ 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True}<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 8/8 [00:07&lt;00:00,  1.03it/s]<br>Traceback\
          \ (most recent call last):<br>  File \"/home/user/text-generation-webui/server.py\"\
          , line 222, in <br>    shared.model, shared.tokenizer = load_model(model_name)<br>\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 86,\
          \ in load_model<br>    tokenizer = load_tokenizer(model_name, model)<br>\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 105,\
          \ in load_tokenizer<br>    tokenizer = AutoTokenizer.from_pretrained(<br>\
          \  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 738, in from_pretrained<br>    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)<br>  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2042, in from_pretrained<br>    return cls._from_pretrained(<br>\
          \  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2253, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 68, in <strong>init</strong><br>    super().<strong>init</strong>(<br>\
          \  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 366, in <strong>init</strong><br>    self._add_tokens(self.all_special_tokens_extended,\
          \ special_tokens=True)<br>  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 454, in _add_tokens<br>    current_vocab = self.get_vocab().copy()<br>\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 108, in get_vocab<br>    vocab = {self.convert_ids_to_tokens(i):\
          \ i for i in range(self.vocab_size)}<br>  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 96, in vocab_size<br>    return self.sp_model.get_piece_size()<br>AttributeError:\
          \ 'InternLMTokenizer' object has no attribute 'sp_model'</p>\n"
        raw: "(textgen) [root@pve-m7330 text-generation-webui]# python server.py --api\
          \ --listen --trust-remote-code --disk-cache-dir /data/tmp --use_double_quant\
          \ --quant_type nf4 --numa --load-in-4bit --settings settings-template.yaml\
          \ --model models/internlm-chat-7b/\r\n2023-09-30 13:38:28 WARNING:trust_remote_code\
          \ is enabled. This is dangerous.\r\n2023-09-30 13:38:28 WARNING:\r\nYou\
          \ are potentially exposing the web UI to the entire internet without any\
          \ access password.\r\nYou can create one with the \"--gradio-auth\" flag\
          \ like this:\r\n\r\n--gradio-auth username:password\r\n\r\nMake sure to\
          \ replace username:password with your own.\r\n2023-09-30 13:38:31 INFO:Loading\
          \ settings from settings-template.yaml...\r\n2023-09-30 13:38:31 INFO:Loading\
          \ internlm-chat-7b...\r\n2023-09-30 13:38:31 INFO:Using the following 4-bit\
          \ params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16,\
          \ 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True}\r\nLoading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 8/8 [00:07<00:00,  1.03it/s]\r\nTraceback\
          \ (most recent call last):\r\n  File \"/home/user/text-generation-webui/server.py\"\
          , line 222, in <module>\r\n    shared.model, shared.tokenizer = load_model(model_name)\r\
          \n  File \"/home/user/text-generation-webui/modules/models.py\", line 86,\
          \ in load_model\r\n    tokenizer = load_tokenizer(model_name, model)\r\n\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 105,\
          \ in load_tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n\
          \  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 738, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2042, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2253, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 68, in __init__\r\n    super().__init__(\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 366, in __init__\r\n    self._add_tokens(self.all_special_tokens_extended,\
          \ special_tokens=True)\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 454, in _add_tokens\r\n    current_vocab = self.get_vocab().copy()\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 108, in get_vocab\r\n    vocab = {self.convert_ids_to_tokens(i):\
          \ i for i in range(self.vocab_size)}\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
          , line 96, in vocab_size\r\n    return self.sp_model.get_piece_size()\r\n\
          AttributeError: 'InternLMTokenizer' object has no attribute 'sp_model'\r\
          \n"
        updatedAt: '2023-09-30T20:42:23.001Z'
      numEdits: 0
      reactions: []
    id: 6518882ea1a5e5d617a2a2b9
    type: comment
  author: LaferriereJC
  content: "(textgen) [root@pve-m7330 text-generation-webui]# python server.py --api\
    \ --listen --trust-remote-code --disk-cache-dir /data/tmp --use_double_quant --quant_type\
    \ nf4 --numa --load-in-4bit --settings settings-template.yaml --model models/internlm-chat-7b/\r\
    \n2023-09-30 13:38:28 WARNING:trust_remote_code is enabled. This is dangerous.\r\
    \n2023-09-30 13:38:28 WARNING:\r\nYou are potentially exposing the web UI to the\
    \ entire internet without any access password.\r\nYou can create one with the\
    \ \"--gradio-auth\" flag like this:\r\n\r\n--gradio-auth username:password\r\n\
    \r\nMake sure to replace username:password with your own.\r\n2023-09-30 13:38:31\
    \ INFO:Loading settings from settings-template.yaml...\r\n2023-09-30 13:38:31\
    \ INFO:Loading internlm-chat-7b...\r\n2023-09-30 13:38:31 INFO:Using the following\
    \ 4-bit params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16,\
    \ 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True}\r\nLoading\
    \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 8/8 [00:07<00:00,  1.03it/s]\r\nTraceback (most\
    \ recent call last):\r\n  File \"/home/user/text-generation-webui/server.py\"\
    , line 222, in <module>\r\n    shared.model, shared.tokenizer = load_model(model_name)\r\
    \n  File \"/home/user/text-generation-webui/modules/models.py\", line 86, in load_model\r\
    \n    tokenizer = load_tokenizer(model_name, model)\r\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 105, in load_tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\
    \n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 738, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2042, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2253, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
    , line 68, in __init__\r\n    super().__init__(\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
    , line 366, in __init__\r\n    self._add_tokens(self.all_special_tokens_extended,\
    \ special_tokens=True)\r\n  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
    , line 454, in _add_tokens\r\n    current_vocab = self.get_vocab().copy()\r\n\
    \  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
    , line 108, in get_vocab\r\n    vocab = {self.convert_ids_to_tokens(i): i for\
    \ i in range(self.vocab_size)}\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/internlm-chat-7b/tokenization_internlm.py\"\
    , line 96, in vocab_size\r\n    return self.sp_model.get_piece_size()\r\nAttributeError:\
    \ 'InternLMTokenizer' object has no attribute 'sp_model'\r\n"
  created_at: 2023-09-30 19:42:22+00:00
  edited: false
  hidden: false
  id: 6518882ea1a5e5d617a2a2b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-01T01:58:28.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822627902030945
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>fixed by upgrading transformers =D</p>

          '
        raw: fixed by upgrading transformers =D
        updatedAt: '2023-10-01T01:58:28.089Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6518d2444b985268867716af
    id: 6518d2444b985268867716aa
    type: comment
  author: LaferriereJC
  content: fixed by upgrading transformers =D
  created_at: 2023-10-01 00:58:28+00:00
  edited: false
  hidden: false
  id: 6518d2444b985268867716aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-01T01:58:28.000Z'
    data:
      status: closed
    id: 6518d2444b985268867716af
    type: status-change
  author: LaferriereJC
  created_at: 2023-10-01 00:58:28+00:00
  id: 6518d2444b985268867716af
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: internlm/internlm-chat-7b
repo_type: model
status: closed
target_branch: null
title: 'AttributeError: ''InternLMTokenizer'' object has no attribute ''sp_model'''
