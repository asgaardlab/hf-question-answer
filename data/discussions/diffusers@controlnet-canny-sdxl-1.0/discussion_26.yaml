!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mashreve
conflicting_files: null
created_at: 2023-08-28 16:23:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f283fa28fcd4cb561e8c9f0b4117194f.svg
      fullname: Matthew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mashreve
      type: user
    createdAt: '2023-08-28T17:23:46.000Z'
    data:
      edited: true
      editors:
      - mashreve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8917912244796753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f283fa28fcd4cb561e8c9f0b4117194f.svg
          fullname: Matthew
          isHf: false
          isPro: false
          name: mashreve
          type: user
        html: '<p>Thank you for sharing this model. I am attempting to train it on
          my own control using the training script, but I am having OOM issues when
          using the same machine configuration (8xA100). Could you please provide
          information re: the accelerate configuration used? Did you have to use a
          fully sharded approach? Thank you.</p>

          '
        raw: 'Thank you for sharing this model. I am attempting to train it on my
          own control using the training script, but I am having OOM issues when using
          the same machine configuration (8xA100). Could you please provide information
          re: the accelerate configuration used? Did you have to use a fully sharded
          approach? Thank you.'
        updatedAt: '2023-08-28T17:24:36.841Z'
      numEdits: 2
      reactions: []
    id: 64ecd82225252dc61a9162b8
    type: comment
  author: mashreve
  content: 'Thank you for sharing this model. I am attempting to train it on my own
    control using the training script, but I am having OOM issues when using the same
    machine configuration (8xA100). Could you please provide information re: the accelerate
    configuration used? Did you have to use a fully sharded approach? Thank you.'
  created_at: 2023-08-28 16:23:46+00:00
  edited: true
  hidden: false
  id: 64ecd82225252dc61a9162b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668468846504-63407fadb78ed99eab00203d.jpeg?w=200&h=200&f=face
      fullname: Will Berman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: williamberman
      type: user
    createdAt: '2023-08-28T19:39:44.000Z'
    data:
      edited: false
      editors:
      - williamberman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9834439158439636
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668468846504-63407fadb78ed99eab00203d.jpeg?w=200&h=200&f=face
          fullname: Will Berman
          isHf: false
          isPro: false
          name: williamberman
          type: user
        html: '<p>I''m not sure if we officially released the training script, which
          one are you looking at? You can also just keep decreasing the batch size
          until you don''t get ooms, iirc batch size wasn''t terribly important</p>

          '
        raw: I'm not sure if we officially released the training script, which one
          are you looking at? You can also just keep decreasing the batch size until
          you don't get ooms, iirc batch size wasn't terribly important
        updatedAt: '2023-08-28T19:39:44.324Z'
      numEdits: 0
      reactions: []
    id: 64ecf80076a070468646549e
    type: comment
  author: williamberman
  content: I'm not sure if we officially released the training script, which one are
    you looking at? You can also just keep decreasing the batch size until you don't
    get ooms, iirc batch size wasn't terribly important
  created_at: 2023-08-28 18:39:44+00:00
  edited: false
  hidden: false
  id: 64ecf80076a070468646549e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f283fa28fcd4cb561e8c9f0b4117194f.svg
      fullname: Matthew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mashreve
      type: user
    createdAt: '2023-08-28T21:08:44.000Z'
    data:
      edited: true
      editors:
      - mashreve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4737204611301422
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f283fa28fcd4cb561e8c9f0b4117194f.svg
          fullname: Matthew
          isHf: false
          isPro: false
          name: mashreve
          type: user
        html: '<p>Thanks for the response. I am trying the script mentioned in the
          readme, which is based on the test circle dataset: <a rel="nofollow" href="https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md">https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md</a>.<br>I
          am using a batch size of 1 and getting an OOM: </p>

          <p>[Update]: I see now that in the reference training script, a single A100
          was used... If at all possible, it would be great to get the modifications
          you made to get it working on 8 GPUs.</p>

          <p>#!/bin/bash<br>export MODEL_DIR="sdxl-vae-fp16-fix"<br>export OUTPUT_DIR="test_circle_training"<br>accelerate
          launch train_controlnet_sdxl.py <br> --pretrained_model_name_or_path=$MODEL_DIR
          <br> --output_dir=$OUTPUT_DIR <br> --dataset_name=fusing/fill50k <br> --mixed_precision="fp16"
          <br> --resolution=512 <br> --learning_rate=1e-5 <br> --max_train_steps=15000
          <br> --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png"
          <br> --validation_prompt "red circle with blue background" "cyan circle
          with brown floral background" <br> --validation_steps=100 <br> --train_batch_size=1
          <br> --gradient_accumulation_steps=4 <br> --report_to="wandb" <br> --seed=42</p>

          '
        raw: "Thanks for the response. I am trying the script mentioned in the readme,\
          \ which is based on the test circle dataset: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md.\n\
          I am using a batch size of 1 and getting an OOM: \n\n[Update]: I see now\
          \ that in the reference training script, a single A100 was used... If at\
          \ all possible, it would be great to get the modifications you made to get\
          \ it working on 8 GPUs.\n\n#!/bin/bash\nexport MODEL_DIR=\"sdxl-vae-fp16-fix\"\
          \nexport OUTPUT_DIR=\"test_circle_training\"\naccelerate launch train_controlnet_sdxl.py\
          \ \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR\
          \ \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n\
          \ --resolution=512 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000\
          \ \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\"\
          \ \\\n --validation_prompt \"red circle with blue background\" \"cyan circle\
          \ with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1\
          \ \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42"
        updatedAt: '2023-08-28T21:14:29.337Z'
      numEdits: 2
      reactions: []
    id: 64ed0cdc2203a126eb1a7f10
    type: comment
  author: mashreve
  content: "Thanks for the response. I am trying the script mentioned in the readme,\
    \ which is based on the test circle dataset: https://github.com/huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md.\n\
    I am using a batch size of 1 and getting an OOM: \n\n[Update]: I see now that\
    \ in the reference training script, a single A100 was used... If at all possible,\
    \ it would be great to get the modifications you made to get it working on 8 GPUs.\n\
    \n#!/bin/bash\nexport MODEL_DIR=\"sdxl-vae-fp16-fix\"\nexport OUTPUT_DIR=\"test_circle_training\"\
    \naccelerate launch train_controlnet_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR\
    \ \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"\
    fp16\" \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000\
    \ \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\"\
    \ \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with\
    \ brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1\
    \ \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42"
  created_at: 2023-08-28 20:08:44+00:00
  edited: true
  hidden: false
  id: 64ed0cdc2203a126eb1a7f10
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: diffusers/controlnet-canny-sdxl-1.0
repo_type: model
status: open
target_branch: null
title: Training on 8 A100 machine
