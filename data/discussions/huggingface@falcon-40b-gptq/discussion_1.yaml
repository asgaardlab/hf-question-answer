!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arnocandel
conflicting_files: null
created_at: 2023-06-28 22:53:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&h=200&f=face
      fullname: Arno Candel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arnocandel
      type: user
    createdAt: '2023-06-28T23:53:47.000Z'
    data:
      edited: false
      editors:
      - arnocandel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.508910059928894
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&h=200&f=face
          fullname: Arno Candel
          isHf: false
          isPro: false
          name: arnocandel
          type: user
        html: "<pre><code>model=huggingface/falcon-40b-gptq\nnum_shard=2\nvolume=$PWD/data\
          \ # share a volume with the Docker container to avoid downloading weights\
          \ every run\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data\
          \ ghcr.io/huggingface/text-generation-inference:0.8 --model-id $model --num-shard\
          \ $num_shard --quantize gptq\n\n2023-06-28T23:51:56.447415Z  INFO text_generation_launcher:\
          \ Args { model_id: \"huggingface/falcon-40b-gptq\", revision: None, sharded:\
          \ None, num_shard: Some(2), quantize: Some(Gptq), trust_remote_code: false,\
          \ max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length:\
          \ 1000, max_total_tokens: 1512, max_batch_size: None, waiting_served_ratio:\
          \ 1.2, max_batch_total_tokens: 32000, max_waiting_tokens: 20, port: 80,\
          \ shard_uds_path: \"/tmp/text-generation-server\", master_addr: \"localhost\"\
          , master_port: 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None,\
          \ env: false }\n2023-06-28T23:51:56.447443Z  INFO text_generation_launcher:\
          \ Sharding model on 2 processes\n2023-06-28T23:51:56.447520Z  INFO text_generation_launcher:\
          \ Starting download process.\n2023-06-28T23:51:58.197443Z  INFO download:\
          \ text_generation_launcher: Files are already present on the host. Skipping\
          \ download.\n\n2023-06-28T23:51:58.450323Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-06-28T23:51:58.450498Z  INFO text_generation_launcher:\
          \ Starting shard 0\n2023-06-28T23:51:58.450721Z  INFO text_generation_launcher:\
          \ Starting shard 1\n2023-06-28T23:52:04.793519Z ERROR shard-manager: text_generation_launcher:\
          \ Error when initializing model\nTraceback (most recent call last):\n  File\
          \ \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 209, in get_model\n    return FlashRWSharded(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 150, in __init__\n    self.load_weights(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 186, in load_weights\n    module_name, param_name = name.rsplit(\"\
          .\", 1)\nValueError: not enough values to unpack (expected 2, got 1)\n</code></pre>\n"
        raw: "```\r\nmodel=huggingface/falcon-40b-gptq\r\nnum_shard=2\r\nvolume=$PWD/data\
          \ # share a volume with the Docker container to avoid downloading weights\
          \ every run\r\n\r\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data\
          \ ghcr.io/huggingface/text-generation-inference:0.8 --model-id $model --num-shard\
          \ $num_shard --quantize gptq\r\n\r\n2023-06-28T23:51:56.447415Z  INFO text_generation_launcher:\
          \ Args { model_id: \"huggingface/falcon-40b-gptq\", revision: None, sharded:\
          \ None, num_shard: Some(2), quantize: Some(Gptq), trust_remote_code: false,\
          \ max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_input_length:\
          \ 1000, max_total_tokens: 1512, max_batch_size: None, waiting_served_ratio:\
          \ 1.2, max_batch_total_tokens: 32000, max_waiting_tokens: 20, port: 80,\
          \ shard_uds_path: \"/tmp/text-generation-server\", master_addr: \"localhost\"\
          , master_port: 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None,\
          \ env: false }\r\n2023-06-28T23:51:56.447443Z  INFO text_generation_launcher:\
          \ Sharding model on 2 processes\r\n2023-06-28T23:51:56.447520Z  INFO text_generation_launcher:\
          \ Starting download process.\r\n2023-06-28T23:51:58.197443Z  INFO download:\
          \ text_generation_launcher: Files are already present on the host. Skipping\
          \ download.\r\n\r\n2023-06-28T23:51:58.450323Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\r\n2023-06-28T23:51:58.450498Z  INFO\
          \ text_generation_launcher: Starting shard 0\r\n2023-06-28T23:51:58.450721Z\
          \  INFO text_generation_launcher: Starting shard 1\r\n2023-06-28T23:52:04.793519Z\
          \ ERROR shard-manager: text_generation_launcher: Error when initializing\
          \ model\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision,\
          \ sharded, quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\r\n    model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 209, in get_model\r\n    return FlashRWSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 150, in __init__\r\n    self.load_weights(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
          , line 186, in load_weights\r\n    module_name, param_name = name.rsplit(\"\
          .\", 1)\r\nValueError: not enough values to unpack (expected 2, got 1)\r\
          \n```"
        updatedAt: '2023-06-28T23:53:47.411Z'
      numEdits: 0
      reactions: []
    id: 649cc80b6209253a76870eba
    type: comment
  author: arnocandel
  content: "```\r\nmodel=huggingface/falcon-40b-gptq\r\nnum_shard=2\r\nvolume=$PWD/data\
    \ # share a volume with the Docker container to avoid downloading weights every\
    \ run\r\n\r\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.8\
    \ --model-id $model --num-shard $num_shard --quantize gptq\r\n\r\n2023-06-28T23:51:56.447415Z\
    \  INFO text_generation_launcher: Args { model_id: \"huggingface/falcon-40b-gptq\"\
    , revision: None, sharded: None, num_shard: Some(2), quantize: Some(Gptq), trust_remote_code:\
    \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4,\
    \ max_input_length: 1000, max_total_tokens: 1512, max_batch_size: None, waiting_served_ratio:\
    \ 1.2, max_batch_total_tokens: 32000, max_waiting_tokens: 20, port: 80, shard_uds_path:\
    \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port: 29500,\
    \ huggingface_hub_cache: Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
    \ false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:\
    \ None, watermark_delta: None, env: false }\r\n2023-06-28T23:51:56.447443Z  INFO\
    \ text_generation_launcher: Sharding model on 2 processes\r\n2023-06-28T23:51:56.447520Z\
    \  INFO text_generation_launcher: Starting download process.\r\n2023-06-28T23:51:58.197443Z\
    \  INFO download: text_generation_launcher: Files are already present on the host.\
    \ Skipping download.\r\n\r\n2023-06-28T23:51:58.450323Z  INFO text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2023-06-28T23:51:58.450498Z  INFO text_generation_launcher:\
    \ Starting shard 0\r\n2023-06-28T23:51:58.450721Z  INFO text_generation_launcher:\
    \ Starting shard 1\r\n2023-06-28T23:52:04.793519Z ERROR shard-manager: text_generation_launcher:\
    \ Error when initializing model\r\nTraceback (most recent call last):\r\n  File\
    \ \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n    sys.exit(app())\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in\
    \ __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\n\
    \    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
    \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\r\n    model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 209, in get_model\r\n    return FlashRWSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
    , line 150, in __init__\r\n    self.load_weights(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_rw.py\"\
    , line 186, in load_weights\r\n    module_name, param_name = name.rsplit(\".\"\
    , 1)\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n```"
  created_at: 2023-06-28 22:53:47+00:00
  edited: false
  hidden: false
  id: 649cc80b6209253a76870eba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&h=200&f=face
      fullname: Arno Candel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arnocandel
      type: user
    createdAt: '2023-06-29T02:24:03.000Z'
    data:
      edited: false
      editors:
      - arnocandel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4544002115726471
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&h=200&f=face
          fullname: Arno Candel
          isHf: false
          isPro: false
          name: arnocandel
          type: user
        html: '<p><code>docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data
          ghcr.io/huggingface/text-generation-inference:latest --model-id $model --num-shard
          $num_shard --quantize gptq</code> works now that <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/pull/438">https://github.com/huggingface/text-generation-inference/pull/438</a>
          has been merged.</p>

          '
        raw: '`docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id $model --num-shard $num_shard --quantize gptq` works now that
          https://github.com/huggingface/text-generation-inference/pull/438 has been
          merged.

          '
        updatedAt: '2023-06-29T02:24:03.302Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649ceb43b4753e14497c45f5
    id: 649ceb43b4753e14497c45f4
    type: comment
  author: arnocandel
  content: '`docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
    --model-id $model --num-shard $num_shard --quantize gptq` works now that https://github.com/huggingface/text-generation-inference/pull/438
    has been merged.

    '
  created_at: 2023-06-29 01:24:03+00:00
  edited: false
  hidden: false
  id: 649ceb43b4753e14497c45f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633f4bd5c11d723b1809dbf8/iFvJ7jYSo0heZMgrnInqo.jpeg?w=200&h=200&f=face
      fullname: Arno Candel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arnocandel
      type: user
    createdAt: '2023-06-29T02:24:03.000Z'
    data:
      status: closed
    id: 649ceb43b4753e14497c45f5
    type: status-change
  author: arnocandel
  created_at: 2023-06-29 01:24:03+00:00
  id: 649ceb43b4753e14497c45f5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: huggingface/falcon-40b-gptq
repo_type: model
status: closed
target_branch: null
title: 'Example fails to run: ValueError: not enough values to unpack (expected 2,
  got 1)'
