!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hollinwilkins
conflicting_files: null
created_at: 2023-07-17 19:34:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38b6f70875eb92d0d172e367a7d14f30.svg
      fullname: Hollin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hollinwilkins
      type: user
    createdAt: '2023-07-17T20:34:28.000Z'
    data:
      edited: false
      editors:
      - hollinwilkins
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5075939297676086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38b6f70875eb92d0d172e367a7d14f30.svg
          fullname: Hollin
          isHf: false
          isPro: false
          name: hollinwilkins
          type: user
        html: '<p>I am using this docker container: ghcr.io/huggingface/text-generation-inference:0.9.2
          with these arguments:</p>

          <p>--model-id=huggingface/falcon-40b-gptq<br>--quantize=gptq<br>--num-shard=1<br>--max-input-length=10<br>--max-total-tokens=20<br>--max-batch-total-tokens=20<br>--max-batch-prefill-tokens=10</p>

          <p>Resource requests for the Docker container are:<br>        resources:<br>          limits:<br>            cpu:
          "11"<br>            memory: 120Gi](resources:<br>          limits:<br>            cpu:
          "11"<br>            memory: 120Gi)</p>

          <p>This is on an A100 with 80GB memory, but for some reason I get the following
          error:<br>Error: Warmup(Generation("Not enough memory to handle 20 total
          tokens with 10 prefill tokens. You need to decrease <code>--max-batch-total-tokens</code>
          or <code>--max-batch-prefill-tokens</code>"))</p>

          <p>The tiiuae/falcon-40b model works fine with this hardware setup and the
          default max_ arguments as long as I use --quantize bitsandbytes, so I don''t
          think it actually has no memory.</p>

          <p>Full Stack Trace:<br>Traceback (most recent call last):<br>  File "/opt/conda/bin/text-generation-server",
          line 8, in <br>    sys.exit(app())<br>  File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in <strong>call</strong><br>    return get_command(self)(*args,
          **kwargs)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1130, in <strong>call</strong><br>    return self.main(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/core.py", line 778, in main<br>    return
          _main(<br>  File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main<br>    rv = self.invoke(ctx)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1657, in invoke<br>    return _process_result(sub_ctx.command.invoke(sub_ctx))<br>  File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1404, in invoke<br>    return
          ctx.invoke(self.callback, **ctx.params)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 760, in invoke<br>    return __callback(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/main.py", line 683, in wrapper<br>    return
          callback(**use_params)  # type: ignore<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 78, in serve<br>    server.serve(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 175, in serve<br>    asyncio.run(<br>  File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run<br>    return loop.run_until_complete(main)<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete<br>    self.run_forever()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever<br>    self._run_once()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once<br>    handle._run()<br>  File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run<br>    self._context.run(self._callback, *self._args)<br>  File
          "/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py", line
          159, in invoke_intercept_method<br>    return await self.intercept(</p>

          <blockquote>

          <p>File "/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py",
          line 21, in intercept<br>    return await response<br>  File "/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py",
          line 82, in _unary_interceptor<br>    raise error<br>  File "/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py",
          line 73, in _unary_interceptor<br>    return await behavior(request_or_iterator,
          context)<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 63, in Warmup<br>    self.model.warmup(batch, request.max_total_tokens)<br>  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py",
          line 729, in warmup<br>    raise RuntimeError(<br>RuntimeError: Not enough
          memory to handle 20 total tokens with 10 prefill tokens. You need to decrease
          <code>--max-batch-total-tokens</code> or <code>--max-batch-prefill-tokens</code><br>
          rank=0<br>2023-07-17T20:09:34.579624Z ERROR warmup{max_input_length=10 max_prefill_tokens=10
          max_total_tokens=20}:warmup: text_generation_client: router/client/src/lib.rs:33:
          Server error: Not enough memory to handle 20 total tokens with 10 prefill
          tokens. You need to decrease <code>--max-batch-total-tokens</code> or <code>--max-batch-prefill-tokens</code><br>Error:
          Warmup(Generation("Not enough memory to handle 20 total tokens with 10 prefill
          tokens. You need to decrease <code>--max-batch-total-tokens</code> or <code>--max-batch-prefill-tokens</code>"))</p>

          </blockquote>

          '
        raw: "I am using this docker container: ghcr.io/huggingface/text-generation-inference:0.9.2\
          \ with these arguments:\r\n\r\n--model-id=huggingface/falcon-40b-gptq\r\n\
          --quantize=gptq\r\n--num-shard=1\r\n--max-input-length=10\r\n--max-total-tokens=20\r\
          \n--max-batch-total-tokens=20\r\n--max-batch-prefill-tokens=10\r\n\r\nResource\
          \ requests for the Docker container are:\r\n        resources:\r\n     \
          \     limits:\r\n            cpu: \"11\"\r\n            memory: 120Gi](resources:\r\
          \n          limits:\r\n            cpu: \"11\"\r\n            memory: 120Gi)\r\
          \n\r\nThis is on an A100 with 80GB memory, but for some reason I get the\
          \ following error:\r\nError: Warmup(Generation(\"Not enough memory to handle\
          \ 20 total tokens with 10 prefill tokens. You need to decrease `--max-batch-total-tokens`\
          \ or `--max-batch-prefill-tokens`\"))\r\n\r\nThe tiiuae/falcon-40b model\
          \ works fine with this hardware setup and the default max_ arguments as\
          \ long as I use --quantize bitsandbytes, so I don't think it actually has\
          \ no memory.\r\n\r\nFull Stack Trace:\r\nTraceback (most recent call last):\r\
          \n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 175, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\"\
          , line 159, in invoke_intercept_method\r\n    return await self.intercept(\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
          \ context)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 63, in Warmup\r\n    self.model.warmup(batch, request.max_total_tokens)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 729, in warmup\r\n    raise RuntimeError(\r\nRuntimeError: Not enough\
          \ memory to handle 20 total tokens with 10 prefill tokens. You need to decrease\
          \ `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\r\n rank=0\r\
          \n2023-07-17T20:09:34.579624Z ERROR warmup{max_input_length=10 max_prefill_tokens=10\
          \ max_total_tokens=20}:warmup: text_generation_client: router/client/src/lib.rs:33:\
          \ Server error: Not enough memory to handle 20 total tokens with 10 prefill\
          \ tokens. You need to decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\r\
          \nError: Warmup(Generation(\"Not enough memory to handle 20 total tokens\
          \ with 10 prefill tokens. You need to decrease `--max-batch-total-tokens`\
          \ or `--max-batch-prefill-tokens`\"))"
        updatedAt: '2023-07-17T20:34:28.788Z'
      numEdits: 0
      reactions: []
    id: 64b5a5d4f7534f1b49dd3d00
    type: comment
  author: hollinwilkins
  content: "I am using this docker container: ghcr.io/huggingface/text-generation-inference:0.9.2\
    \ with these arguments:\r\n\r\n--model-id=huggingface/falcon-40b-gptq\r\n--quantize=gptq\r\
    \n--num-shard=1\r\n--max-input-length=10\r\n--max-total-tokens=20\r\n--max-batch-total-tokens=20\r\
    \n--max-batch-prefill-tokens=10\r\n\r\nResource requests for the Docker container\
    \ are:\r\n        resources:\r\n          limits:\r\n            cpu: \"11\"\r\
    \n            memory: 120Gi](resources:\r\n          limits:\r\n            cpu:\
    \ \"11\"\r\n            memory: 120Gi)\r\n\r\nThis is on an A100 with 80GB memory,\
    \ but for some reason I get the following error:\r\nError: Warmup(Generation(\"\
    Not enough memory to handle 20 total tokens with 10 prefill tokens. You need to\
    \ decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\"))\r\n\r\
    \nThe tiiuae/falcon-40b model works fine with this hardware setup and the default\
    \ max_ arguments as long as I use --quantize bitsandbytes, so I don't think it\
    \ actually has no memory.\r\n\r\nFull Stack Trace:\r\nTraceback (most recent call\
    \ last):\r\n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
    \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 175, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\",\
    \ line 159, in invoke_intercept_method\r\n    return await self.intercept(\r\n\
    > File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\"\
    , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
    \ context)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 63, in Warmup\r\n    self.model.warmup(batch, request.max_total_tokens)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 729, in warmup\r\n    raise RuntimeError(\r\nRuntimeError: Not enough memory\
    \ to handle 20 total tokens with 10 prefill tokens. You need to decrease `--max-batch-total-tokens`\
    \ or `--max-batch-prefill-tokens`\r\n rank=0\r\n2023-07-17T20:09:34.579624Z ERROR\
    \ warmup{max_input_length=10 max_prefill_tokens=10 max_total_tokens=20}:warmup:\
    \ text_generation_client: router/client/src/lib.rs:33: Server error: Not enough\
    \ memory to handle 20 total tokens with 10 prefill tokens. You need to decrease\
    \ `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\r\nError: Warmup(Generation(\"\
    Not enough memory to handle 20 total tokens with 10 prefill tokens. You need to\
    \ decrease `--max-batch-total-tokens` or `--max-batch-prefill-tokens`\"))"
  created_at: 2023-07-17 19:34:28+00:00
  edited: false
  hidden: false
  id: 64b5a5d4f7534f1b49dd3d00
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: huggingface/falcon-40b-gptq
repo_type: model
status: open
target_branch: null
title: 'Error loading with text-generation-inference server version '
