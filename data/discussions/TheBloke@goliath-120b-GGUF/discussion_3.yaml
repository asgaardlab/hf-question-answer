!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bezale
conflicting_files: null
created_at: 2023-11-17 21:14:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-17T21:14:05.000Z'
    data:
      edited: false
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43290624022483826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> maybe you know\
          \ the quick fix to </p>\n<pre><code>ggml_new_object: not enough space in\
          \ the context's memory pool (needed 1638880, available 1638544)\n</code></pre>\n\
          <p>error while trying to run the goliath-120b.Q2_K.gguf model with llama-cpp-python?</p>\n\
          <p>Below are the model loading log:</p>\n<pre><code>llm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta:\
          \ n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\n\
          llm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd\
          \           = 8192\nllm_load_print_meta: n_head           = 64\nllm_load_print_meta:\
          \ n_head_kv        = 8\nllm_load_print_meta: n_layer          = 137\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 8\nllm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0.0e+00\nllm_load_print_meta: n_ff             = 28672\nllm_load_print_meta:\
          \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\n\
          llm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx\
          \  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
          \ model type       = ?B\nllm_load_print_meta: model ftype      = mostly\
          \ Q2_K\nllm_load_print_meta: model params     = 117.75 B\nllm_load_print_meta:\
          \ model size       = 46.22 GiB (3.37 BPW) \nllm_load_print_meta: general.name\
          \   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta:\
          \ EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\n\
          llm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml\
          \ ctx size =    0.45 MB\nllm_load_tensors: using CUDA for GPU acceleration\n\
          llm_load_tensors: mem required  = 2691.22 MB\nllm_load_tensors: offloading\
          \ 130 repeating layers to GPU\nllm_load_tensors: offloaded 130/140 layers\
          \ to GPU\nllm_load_tensors: VRAM used: 44638.75 MB\n....................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
          \ kv self size  = 1096.00 MB\nggml_new_object: not enough space in the context's\
          \ memory pool (needed 1638880, available 1638544)\n</code></pre>\n<p>I've\
          \ tried to change gpu_layers number, context length - nothing helps, and\
          \ always the same error with same numbers</p>\n<p>Thanks!</p>\n"
        raw: "@TheBloke maybe you know the quick fix to \r\n```\r\nggml_new_object:\
          \ not enough space in the context's memory pool (needed 1638880, available\
          \ 1638544)\r\n```\r\nerror while trying to run the goliath-120b.Q2_K.gguf\
          \ model with llama-cpp-python?\r\n\r\nBelow are the model loading log:\r\
          \n```\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta:\
          \ arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\
          \nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta:\
          \ n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\
          \nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head\
          \           = 64\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta:\
          \ n_layer          = 137\r\nllm_load_print_meta: n_rot            = 128\r\
          \nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: f_norm_eps\
          \       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             =\
          \ 28672\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta:\
          \ freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train =\
          \ 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta:\
          \ rope_finetuned   = unknown\r\nllm_load_print_meta: model type       =\
          \ ?B\r\nllm_load_print_meta: model ftype      = mostly Q2_K\r\nllm_load_print_meta:\
          \ model params     = 117.75 B\r\nllm_load_print_meta: model size       =\
          \ 46.22 GiB (3.37 BPW) \r\nllm_load_print_meta: general.name   = LLaMA v2\r\
          \nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token\
          \ = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta:\
          \ LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.45 MB\r\
          \nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors:\
          \ mem required  = 2691.22 MB\r\nllm_load_tensors: offloading 130 repeating\
          \ layers to GPU\r\nllm_load_tensors: offloaded 130/140 layers to GPU\r\n\
          llm_load_tensors: VRAM used: 44638.75 MB\r\n....................................................................................................\r\
          \nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\
          \nllama_new_context_with_model: kv self size  = 1096.00 MB\r\nggml_new_object:\
          \ not enough space in the context's memory pool (needed 1638880, available\
          \ 1638544)\r\n```\r\n\r\nI've tried to change gpu_layers number, context\
          \ length - nothing helps, and always the same error with same numbers\r\n\
          \r\nThanks!\r\n\r\n\r\n"
        updatedAt: '2023-11-17T21:14:05.882Z'
      numEdits: 0
      reactions: []
    id: 6557d79d9d249b4ab309a599
    type: comment
  author: bezale
  content: "@TheBloke maybe you know the quick fix to \r\n```\r\nggml_new_object:\
    \ not enough space in the context's memory pool (needed 1638880, available 1638544)\r\
    \n```\r\nerror while trying to run the goliath-120b.Q2_K.gguf model with llama-cpp-python?\r\
    \n\r\nBelow are the model loading log:\r\n```\r\nllm_load_print_meta: format \
    \          = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\
    \nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab\
    \          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta:\
    \ n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 8192\r\n\
    llm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv \
    \       = 8\r\nllm_load_print_meta: n_layer          = 137\r\nllm_load_print_meta:\
    \ n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\
    \nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\r\nllm_load_print_meta: n_ff             = 28672\r\nllm_load_print_meta:\
    \ rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\
    \nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx\
    \  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta:\
    \ model type       = ?B\r\nllm_load_print_meta: model ftype      = mostly Q2_K\r\
    \nllm_load_print_meta: model params     = 117.75 B\r\nllm_load_print_meta: model\
    \ size       = 46.22 GiB (3.37 BPW) \r\nllm_load_print_meta: general.name   =\
    \ LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta:\
    \ EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta:\
    \ LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.45 MB\r\n\
    llm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: mem required\
    \  = 2691.22 MB\r\nllm_load_tensors: offloading 130 repeating layers to GPU\r\n\
    llm_load_tensors: offloaded 130/140 layers to GPU\r\nllm_load_tensors: VRAM used:\
    \ 44638.75 MB\r\n....................................................................................................\r\
    \nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model:\
    \ kv self size  = 1096.00 MB\r\nggml_new_object: not enough space in the context's\
    \ memory pool (needed 1638880, available 1638544)\r\n```\r\n\r\nI've tried to\
    \ change gpu_layers number, context length - nothing helps, and always the same\
    \ error with same numbers\r\n\r\nThanks!\r\n\r\n\r\n"
  created_at: 2023-11-17 21:14:05+00:00
  edited: false
  hidden: false
  id: 6557d79d9d249b4ab309a599
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-11-19T05:25:50.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8827557563781738
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Same exact error attempting this model on runpod. I genuinely have
          no clue whats causing it. It works on my main machine...</p>

          '
        raw: Same exact error attempting this model on runpod. I genuinely have no
          clue whats causing it. It works on my main machine...
        updatedAt: '2023-11-19T05:25:50.513Z'
      numEdits: 0
      reactions: []
    id: 65599c5e8762c448a112d1e7
    type: comment
  author: dillfrescott
  content: Same exact error attempting this model on runpod. I genuinely have no clue
    whats causing it. It works on my main machine...
  created_at: 2023-11-19 05:25:50+00:00
  edited: false
  hidden: false
  id: 65599c5e8762c448a112d1e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-11-19T05:26:11.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.21567361056804657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> any ideas?</p>\n"
        raw: '@TheBloke any ideas?'
        updatedAt: '2023-11-19T05:26:11.964Z'
      numEdits: 0
      reactions: []
    id: 65599c739e9dfc1089f2d73e
    type: comment
  author: dillfrescott
  content: '@TheBloke any ideas?'
  created_at: 2023-11-19 05:26:11+00:00
  edited: false
  hidden: false
  id: 65599c739e9dfc1089f2d73e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-11-19T11:57:11.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7879921793937683
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Looks like it might be fixed with this commit <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/commit/bbecf3f415797f812893947998bda4f866fa900e">https://github.com/ggerganov/llama.cpp/commit/bbecf3f415797f812893947998bda4f866fa900e</a></p>

          '
        raw: Looks like it might be fixed with this commit https://github.com/ggerganov/llama.cpp/commit/bbecf3f415797f812893947998bda4f866fa900e
        updatedAt: '2023-11-19T11:57:11.948Z'
      numEdits: 0
      reactions: []
    id: 6559f8179e9dfc1089ffa51c
    type: comment
  author: dillfrescott
  content: Looks like it might be fixed with this commit https://github.com/ggerganov/llama.cpp/commit/bbecf3f415797f812893947998bda4f866fa900e
  created_at: 2023-11-19 11:57:11+00:00
  edited: false
  hidden: false
  id: 6559f8179e9dfc1089ffa51c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/968343ce01cd7cd7c9913b7edfff1a26.svg
      fullname: manec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: caga
      type: user
    createdAt: '2023-11-21T20:57:27.000Z'
    data:
      edited: false
      editors:
      - caga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5697193741798401
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/968343ce01cd7cd7c9913b7edfff1a26.svg
          fullname: manec
          isHf: false
          isPro: false
          name: caga
          type: user
        html: '<p>Same problem here: running goliath-120b.Q6_K.gguf with ctransformers
          in a 2xXeon, 128RAM, 8Gb NVIDIA.</p>

          '
        raw: 'Same problem here: running goliath-120b.Q6_K.gguf with ctransformers
          in a 2xXeon, 128RAM, 8Gb NVIDIA.'
        updatedAt: '2023-11-21T20:57:27.197Z'
      numEdits: 0
      reactions: []
    id: 655d19b7c166f4c0a78faae7
    type: comment
  author: caga
  content: 'Same problem here: running goliath-120b.Q6_K.gguf with ctransformers in
    a 2xXeon, 128RAM, 8Gb NVIDIA.'
  created_at: 2023-11-21 20:57:27+00:00
  edited: false
  hidden: false
  id: 655d19b7c166f4c0a78faae7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-11-21T21:43:35.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9930087327957153
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Seems to me that the same value that was increased in ccp needs
          to be increased somewhere in the ctransformers library as well.</p>

          '
        raw: Seems to me that the same value that was increased in ccp needs to be
          increased somewhere in the ctransformers library as well.
        updatedAt: '2023-11-21T21:43:35.526Z'
      numEdits: 0
      reactions: []
    id: 655d24877dbbdc709eb0f83b
    type: comment
  author: dillfrescott
  content: Seems to me that the same value that was increased in ccp needs to be increased
    somewhere in the ctransformers library as well.
  created_at: 2023-11-21 21:43:35+00:00
  edited: false
  hidden: false
  id: 655d24877dbbdc709eb0f83b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/968343ce01cd7cd7c9913b7edfff1a26.svg
      fullname: manec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: caga
      type: user
    createdAt: '2023-11-22T18:11:54.000Z'
    data:
      edited: false
      editors:
      - caga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8026413321495056
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/968343ce01cd7cd7c9913b7edfff1a26.svg
          fullname: manec
          isHf: false
          isPro: false
          name: caga
          type: user
        html: '<p>Problem solved using llama-cpp-python, without any changes in llama
          source code. Now I have to figure out how send to some layers to the GPU...
          noob issues :) Thanks!</p>

          '
        raw: Problem solved using llama-cpp-python, without any changes in llama source
          code. Now I have to figure out how send to some layers to the GPU... noob
          issues :) Thanks!
        updatedAt: '2023-11-22T18:11:54.421Z'
      numEdits: 0
      reactions: []
    id: 655e446a2ff001a46a3589f0
    type: comment
  author: caga
  content: Problem solved using llama-cpp-python, without any changes in llama source
    code. Now I have to figure out how send to some layers to the GPU... noob issues
    :) Thanks!
  created_at: 2023-11-22 18:11:54+00:00
  edited: false
  hidden: false
  id: 655e446a2ff001a46a3589f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b25584a29fa1889e9007772eeacbd02.svg
      fullname: Newman Rednero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rednero
      type: user
    createdAt: '2023-12-15T06:37:28.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/8b25584a29fa1889e9007772eeacbd02.svg
          fullname: Newman Rednero
          isHf: false
          isPro: false
          name: Rednero
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-16T23:59:44.391Z'
      numEdits: 0
      reactions: []
    id: 657bf4282cbb7f638255a709
    type: comment
  author: Rednero
  content: This comment has been hidden
  created_at: 2023-12-15 06:37:28+00:00
  edited: true
  hidden: true
  id: 657bf4282cbb7f638255a709
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/goliath-120b-GGUF
repo_type: model
status: open
target_branch: null
title: Strange error while running model
