!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shamito
conflicting_files: null
created_at: 2023-07-20 18:45:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66c09d0b70798172898c5cfba3c8036d.svg
      fullname: Abdullah Alshami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shamito
      type: user
    createdAt: '2023-07-20T19:45:18.000Z'
    data:
      edited: false
      editors:
      - Shamito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7023172378540039
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66c09d0b70798172898c5cfba3c8036d.svg
          fullname: Abdullah Alshami
          isHf: false
          isPro: false
          name: Shamito
          type: user
        html: '<p>import langchain<br>from langchain import PromptTemplate, LLMChain<br>from
          langchain.llms import TextGen</p>

          <p>langchain.debug = True</p>

          <p>template = """Question: {question}</p>

          <p>Answer: Let''s think step by step."""</p>

          <p>prompt = PromptTemplate(template=template, input_variables=["question"])<br>llm
          = TextGen(model_url=''<a href="https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML/blob/main/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin''">https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML/blob/main/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin''</a>)<br>llm_chain
          = LLMChain(prompt=prompt, llm=llm)<br>question = "What NFL team won the
          Super Bowl in the year Justin Bieber was born?"</p>

          <p>llm_chain.run(question)</p>

          <p>output:<br>(myenv) PS C:\Users\abdul\text-generation&gt; &amp; c:/Users/abdul/text-generation/myenv/Scripts/python.exe
          c:/Users/abdul/text-generation/lan.py<br>[chain/start] [1:RunTypeEnum.chain:LLMChain]
          Entering Chain run with input:<br>{<br>  "question": "What NFL team won
          the Super Bowl in the year Justin Bieber was born?"<br>}<br>[llm/start]
          [1:RunTypeEnum.chain:LLMChain &gt; 2:RunTypeEnum.llm:TextGen] Entering LLM
          run with input:<br>{<br>  "prompts": [<br>    "Question: What NFL team won
          the Super Bowl in the year Justin Bieber was born?\n\nAnswer: Let''s think
          step by step."<br>  ]<br>}<br>ERROR: response: &lt;Response [404]&gt;<br>[llm/end]
          [1:RunTypeEnum.chain:LLMChain &gt; 2:RunTypeEnum.llm:TextGen] [345.12100000000004ms]
          Exiting LLM run with output:<br>{<br>  "generations": [<br>    [<br>        "generation_info":
          null<br>      }<br>    ]<br>  ],<br>  "llm_output": null,<br>  "run": null<br>}<br>[chain/end]
          [1:RunTypeEnum.chain:LLMChain] [352.551ms] Exiting Chain run with output:<br>{<br>  "text":
          ""<br>}<br>(myenv) PS C:\Users\abdul\text-generation&gt; &amp; c:/Users/abdul/text-generation/myenv/Scripts/python.exe
          c:/Users/abdul/text-generation/lan.py<br>[chain/start] [1:RunTypeEnum.chain:LLMChain]
          Entering Chain run with input:<br>{<br>  "question": "What NFL team won
          the Super Bowl in the year Justin Bieber was born?"<br>}<br>[llm/start]
          [1:RunTypeEnum.chain:LLMChain &gt; 2:RunTypeEnum.llm:TextGen] Entering LLM
          run with input:<br>{<br>  "prompts": [<br>    "Question: What NFL team won
          the Super Bowl in the year Justin Bieber was born?\n\nAnswer: Let''s think
          step by step."<br>  ]<br>}<br>ERROR: response: &lt;Response [404]&gt;<br>[llm/end]
          [1:RunTypeEnum.chain:LLMChain &gt; 2:RunTypeEnum.llm:TextGen] [411.44ms]
          Exiting LLM run with output:<br>{<br>  "generations": [<br>    [<br>      {<br>        "text":
          "",<br>        "generation_info": null<br>      }<br>    ]<br>  ],<br>  "llm_output":
          null,<br>  "run": null<br>}<br>[chain/end] [1:RunTypeEnum.chain:LLMChain]
          [417.442ms] Exiting Chain run with output:<br>{<br>  "text": ""<br>}</p>

          '
        raw: "import langchain\r\nfrom langchain import PromptTemplate, LLMChain\r\
          \nfrom langchain.llms import TextGen\r\n\r\nlangchain.debug = True\r\n\r\
          \ntemplate = \"\"\"Question: {question}\r\n\r\nAnswer: Let's think step\
          \ by step.\"\"\"\r\n\r\n\r\nprompt = PromptTemplate(template=template, input_variables=[\"\
          question\"])\r\nllm = TextGen(model_url='https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML/blob/main/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin')\r\
          \nllm_chain = LLMChain(prompt=prompt, llm=llm)\r\nquestion = \"What NFL\
          \ team won the Super Bowl in the year Justin Bieber was born?\"\r\n\r\n\
          llm_chain.run(question)\r\n\r\noutput: \r\n(myenv) PS C:\\Users\\abdul\\\
          text-generation> & c:/Users/abdul/text-generation/myenv/Scripts/python.exe\
          \ c:/Users/abdul/text-generation/lan.py\r\n[chain/start] [1:RunTypeEnum.chain:LLMChain]\
          \ Entering Chain run with input:\r\n{\r\n  \"question\": \"What NFL team\
          \ won the Super Bowl in the year Justin Bieber was born?\"\r\n}\r\n[llm/start]\
          \ [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen] Entering LLM\
          \ run with input:\r\n{\r\n  \"prompts\": [\r\n    \"Question: What NFL team\
          \ won the Super Bowl in the year Justin Bieber was born?\\n\\nAnswer: Let's\
          \ think step by step.\"\r\n  ]\r\n}\r\nERROR: response: <Response [404]>\r\
          \n[llm/end] [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen] [345.12100000000004ms]\
          \ Exiting LLM run with output:\r\n{\r\n  \"generations\": [\r\n    [\r\n\
          \        \"generation_info\": null\r\n      }\r\n    ]\r\n  ],\r\n  \"llm_output\"\
          : null,\r\n  \"run\": null\r\n}\r\n[chain/end] [1:RunTypeEnum.chain:LLMChain]\
          \ [352.551ms] Exiting Chain run with output:\r\n{\r\n  \"text\": \"\"\r\n\
          }\r\n(myenv) PS C:\\Users\\abdul\\text-generation> & c:/Users/abdul/text-generation/myenv/Scripts/python.exe\
          \ c:/Users/abdul/text-generation/lan.py\r\n[chain/start] [1:RunTypeEnum.chain:LLMChain]\
          \ Entering Chain run with input:\r\n{\r\n  \"question\": \"What NFL team\
          \ won the Super Bowl in the year Justin Bieber was born?\"\r\n}\r\n[llm/start]\
          \ [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen] Entering LLM\
          \ run with input:\r\n{\r\n  \"prompts\": [\r\n    \"Question: What NFL team\
          \ won the Super Bowl in the year Justin Bieber was born?\\n\\nAnswer: Let's\
          \ think step by step.\"\r\n  ]\r\n}\r\nERROR: response: <Response [404]>\r\
          \n[llm/end] [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen] [411.44ms]\
          \ Exiting LLM run with output:\r\n{\r\n  \"generations\": [\r\n    [\r\n\
          \      {\r\n        \"text\": \"\",\r\n        \"generation_info\": null\r\
          \n      }\r\n    ]\r\n  ],\r\n  \"llm_output\": null,\r\n  \"run\": null\r\
          \n}\r\n[chain/end] [1:RunTypeEnum.chain:LLMChain] [417.442ms] Exiting Chain\
          \ run with output:\r\n{\r\n  \"text\": \"\"\r\n}"
        updatedAt: '2023-07-20T19:45:18.749Z'
      numEdits: 0
      reactions: []
    id: 64b98ece2cfce8f7b3cd757e
    type: comment
  author: Shamito
  content: "import langchain\r\nfrom langchain import PromptTemplate, LLMChain\r\n\
    from langchain.llms import TextGen\r\n\r\nlangchain.debug = True\r\n\r\ntemplate\
    \ = \"\"\"Question: {question}\r\n\r\nAnswer: Let's think step by step.\"\"\"\r\
    \n\r\n\r\nprompt = PromptTemplate(template=template, input_variables=[\"question\"\
    ])\r\nllm = TextGen(model_url='https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML/blob/main/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin')\r\
    \nllm_chain = LLMChain(prompt=prompt, llm=llm)\r\nquestion = \"What NFL team won\
    \ the Super Bowl in the year Justin Bieber was born?\"\r\n\r\nllm_chain.run(question)\r\
    \n\r\noutput: \r\n(myenv) PS C:\\Users\\abdul\\text-generation> & c:/Users/abdul/text-generation/myenv/Scripts/python.exe\
    \ c:/Users/abdul/text-generation/lan.py\r\n[chain/start] [1:RunTypeEnum.chain:LLMChain]\
    \ Entering Chain run with input:\r\n{\r\n  \"question\": \"What NFL team won the\
    \ Super Bowl in the year Justin Bieber was born?\"\r\n}\r\n[llm/start] [1:RunTypeEnum.chain:LLMChain\
    \ > 2:RunTypeEnum.llm:TextGen] Entering LLM run with input:\r\n{\r\n  \"prompts\"\
    : [\r\n    \"Question: What NFL team won the Super Bowl in the year Justin Bieber\
    \ was born?\\n\\nAnswer: Let's think step by step.\"\r\n  ]\r\n}\r\nERROR: response:\
    \ <Response [404]>\r\n[llm/end] [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen]\
    \ [345.12100000000004ms] Exiting LLM run with output:\r\n{\r\n  \"generations\"\
    : [\r\n    [\r\n        \"generation_info\": null\r\n      }\r\n    ]\r\n  ],\r\
    \n  \"llm_output\": null,\r\n  \"run\": null\r\n}\r\n[chain/end] [1:RunTypeEnum.chain:LLMChain]\
    \ [352.551ms] Exiting Chain run with output:\r\n{\r\n  \"text\": \"\"\r\n}\r\n\
    (myenv) PS C:\\Users\\abdul\\text-generation> & c:/Users/abdul/text-generation/myenv/Scripts/python.exe\
    \ c:/Users/abdul/text-generation/lan.py\r\n[chain/start] [1:RunTypeEnum.chain:LLMChain]\
    \ Entering Chain run with input:\r\n{\r\n  \"question\": \"What NFL team won the\
    \ Super Bowl in the year Justin Bieber was born?\"\r\n}\r\n[llm/start] [1:RunTypeEnum.chain:LLMChain\
    \ > 2:RunTypeEnum.llm:TextGen] Entering LLM run with input:\r\n{\r\n  \"prompts\"\
    : [\r\n    \"Question: What NFL team won the Super Bowl in the year Justin Bieber\
    \ was born?\\n\\nAnswer: Let's think step by step.\"\r\n  ]\r\n}\r\nERROR: response:\
    \ <Response [404]>\r\n[llm/end] [1:RunTypeEnum.chain:LLMChain > 2:RunTypeEnum.llm:TextGen]\
    \ [411.44ms] Exiting LLM run with output:\r\n{\r\n  \"generations\": [\r\n   \
    \ [\r\n      {\r\n        \"text\": \"\",\r\n        \"generation_info\": null\r\
    \n      }\r\n    ]\r\n  ],\r\n  \"llm_output\": null,\r\n  \"run\": null\r\n}\r\
    \n[chain/end] [1:RunTypeEnum.chain:LLMChain] [417.442ms] Exiting Chain run with\
    \ output:\r\n{\r\n  \"text\": \"\"\r\n}"
  created_at: 2023-07-20 18:45:18+00:00
  edited: false
  hidden: false
  id: 64b98ece2cfce8f7b3cd757e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T09:33:47.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422723650932312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Don''t know, please raise it as an issue with the developers of
          the Llama.cpp module for LangChain. I''ve never used it myself.</p>

          '
        raw: Don't know, please raise it as an issue with the developers of the Llama.cpp
          module for LangChain. I've never used it myself.
        updatedAt: '2023-07-21T09:33:56.429Z'
      numEdits: 1
      reactions: []
    id: 64ba50fb3d329540ee3aee07
    type: comment
  author: TheBloke
  content: Don't know, please raise it as an issue with the developers of the Llama.cpp
    module for LangChain. I've never used it myself.
  created_at: 2023-07-21 08:33:47+00:00
  edited: true
  hidden: false
  id: 64ba50fb3d329540ee3aee07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML
repo_type: model
status: open
target_branch: null
title: 'ERROR: response: <Response [404]> [llm/end] [1:RunTypeEnum.chain:LLMChain
  >'
