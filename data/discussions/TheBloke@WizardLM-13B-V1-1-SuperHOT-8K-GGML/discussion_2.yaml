!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Enferlain
conflicting_files: null
created_at: 2023-07-07 21:55:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
      fullname: Hoshino Imi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Enferlain
      type: user
    createdAt: '2023-07-07T22:55:17.000Z'
    data:
      edited: true
      editors:
      - Enferlain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729899764060974
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
          fullname: Hoshino Imi
          isHf: false
          isPro: false
          name: Enferlain
          type: user
        html: '<p>I think in koboldcpp you can use 4096 or maybe even more on models
          that aren''t superhot. Have you heard about whether or not there was a performance/quality
          impact for using either versions over the other for higher than 2k?</p>

          '
        raw: I think in koboldcpp you can use 4096 or maybe even more on models that
          aren't superhot. Have you heard about whether or not there was a performance/quality
          impact for using either versions over the other for higher than 2k?
        updatedAt: '2023-07-07T22:55:32.750Z'
      numEdits: 1
      reactions: []
    id: 64a897d56e727d56ae87ffc7
    type: comment
  author: Enferlain
  content: I think in koboldcpp you can use 4096 or maybe even more on models that
    aren't superhot. Have you heard about whether or not there was a performance/quality
    impact for using either versions over the other for higher than 2k?
  created_at: 2023-07-07 21:55:17+00:00
  edited: true
  hidden: false
  id: 64a897d56e727d56ae87ffc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T23:08:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9858266711235046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I haven''t heard of specific testing but yes I would expect a quality
          drop. The benefit of the SuperHOT models is that they gain training on responses
          that are over 2048 in length. Without that, quality is expected to degrade
          on longer responses.</p>

          <p>It may well still be usable though, so it''s worth playing with.</p>

          '
        raw: 'I haven''t heard of specific testing but yes I would expect a quality
          drop. The benefit of the SuperHOT models is that they gain training on responses
          that are over 2048 in length. Without that, quality is expected to degrade
          on longer responses.


          It may well still be usable though, so it''s worth playing with.'
        updatedAt: '2023-07-07T23:08:52.096Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Enferlain
    id: 64a89b046cadc7aca538e566
    type: comment
  author: TheBloke
  content: 'I haven''t heard of specific testing but yes I would expect a quality
    drop. The benefit of the SuperHOT models is that they gain training on responses
    that are over 2048 in length. Without that, quality is expected to degrade on
    longer responses.


    It may well still be usable though, so it''s worth playing with.'
  created_at: 2023-07-07 22:08:52+00:00
  edited: false
  hidden: false
  id: 64a89b046cadc7aca538e566
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
      fullname: Magaret Thoter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maggiet
      type: user
    createdAt: '2023-07-09T13:17:21.000Z'
    data:
      edited: true
      editors:
      - Maggiet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9835546612739563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
          fullname: Magaret Thoter
          isHf: false
          isPro: false
          name: Maggiet
          type: user
        html: '<blockquote>

          <p>I think in koboldcpp you can use 4096 or maybe even more on models that
          aren''t superhot. Have you heard about whether or not there was a performance/quality
          impact for using either versions over the other for higher than 2k?</p>

          </blockquote>

          <p>I have tried this just before the superhots were released. The answer
          is no. It "works" in that it''s not throwing error exceptions, but it will
          give you gibberish, mostly punctuation or repeat the same a few words, as
          if it were autistic. 2k is the max for those.<br>The superhots work. 8k
          even. But some models seem to behave better than other, and obviously the
          higher "b" count, the better the results.</p>

          '
        raw: '> I think in koboldcpp you can use 4096 or maybe even more on models
          that aren''t superhot. Have you heard about whether or not there was a performance/quality
          impact for using either versions over the other for higher than 2k?


          I have tried this just before the superhots were released. The answer is
          no. It "works" in that it''s not throwing error exceptions, but it will
          give you gibberish, mostly punctuation or repeat the same a few words, as
          if it were autistic. 2k is the max for those.

          The superhots work. 8k even. But some models seem to behave better than
          other, and obviously the higher "b" count, the better the results.'
        updatedAt: '2023-07-09T13:19:18.270Z'
      numEdits: 1
      reactions: []
    id: 64aab36181dda481726d8041
    type: comment
  author: Maggiet
  content: '> I think in koboldcpp you can use 4096 or maybe even more on models that
    aren''t superhot. Have you heard about whether or not there was a performance/quality
    impact for using either versions over the other for higher than 2k?


    I have tried this just before the superhots were released. The answer is no. It
    "works" in that it''s not throwing error exceptions, but it will give you gibberish,
    mostly punctuation or repeat the same a few words, as if it were autistic. 2k
    is the max for those.

    The superhots work. 8k even. But some models seem to behave better than other,
    and obviously the higher "b" count, the better the results.'
  created_at: 2023-07-09 12:17:21+00:00
  edited: true
  hidden: false
  id: 64aab36181dda481726d8041
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-09T13:35:29.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9847132563591003
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I have tried this just before the superhots were released. The answer
          is no. It "works" in that it''s not throwing error exceptions, but it will
          give you gibberish, mostly punctuation or repeat the same a few words, as
          if it were autistic. 2k is the max for those.<br>The superhots work. 8k
          even. But some models seem to behave better than other, and obviously the
          higher "b" count, the better the results.</p>

          </blockquote>

          <p>Ah, but if you tested before the SuperHOTs came out then I think you
          only tested by adjusting the UI setting, right?  You didn''t also have <code>--contextsize
          4096</code> on the command line, which is what applies the magic algorithm
          that should in theory work with any model.</p>

          <p>That algorithm was only added after SuperHOT, because it was Kaio Ken''s
          work on SuperHOT that showed everyone how to increase context with a very
          simple change to the code.</p>

          <p>The SuperHOT models then also feature training on &gt;2K prompts to boost
          its ability to answer those, but in theory it should also work without that,
          albeit probably not as well.</p>

          '
        raw: '> I have tried this just before the superhots were released. The answer
          is no. It "works" in that it''s not throwing error exceptions, but it will
          give you gibberish, mostly punctuation or repeat the same a few words, as
          if it were autistic. 2k is the max for those.

          > The superhots work. 8k even. But some models seem to behave better than
          other, and obviously the higher "b" count, the better the results.


          Ah, but if you tested before the SuperHOTs came out then I think you only
          tested by adjusting the UI setting, right?  You didn''t also have `--contextsize
          4096` on the command line, which is what applies the magic algorithm that
          should in theory work with any model.


          That algorithm was only added after SuperHOT, because it was Kaio Ken''s
          work on SuperHOT that showed everyone how to increase context with a very
          simple change to the code.


          The SuperHOT models then also feature training on >2K prompts to boost its
          ability to answer those, but in theory it should also work without that,
          albeit probably not as well.'
        updatedAt: '2023-07-09T13:37:15.383Z'
      numEdits: 1
      reactions: []
    id: 64aab7a1d4a402e8dce03337
    type: comment
  author: TheBloke
  content: '> I have tried this just before the superhots were released. The answer
    is no. It "works" in that it''s not throwing error exceptions, but it will give
    you gibberish, mostly punctuation or repeat the same a few words, as if it were
    autistic. 2k is the max for those.

    > The superhots work. 8k even. But some models seem to behave better than other,
    and obviously the higher "b" count, the better the results.


    Ah, but if you tested before the SuperHOTs came out then I think you only tested
    by adjusting the UI setting, right?  You didn''t also have `--contextsize 4096`
    on the command line, which is what applies the magic algorithm that should in
    theory work with any model.


    That algorithm was only added after SuperHOT, because it was Kaio Ken''s work
    on SuperHOT that showed everyone how to increase context with a very simple change
    to the code.


    The SuperHOT models then also feature training on >2K prompts to boost its ability
    to answer those, but in theory it should also work without that, albeit probably
    not as well.'
  created_at: 2023-07-09 12:35:29+00:00
  edited: true
  hidden: false
  id: 64aab7a1d4a402e8dce03337
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
      fullname: Magaret Thoter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maggiet
      type: user
    createdAt: '2023-07-09T22:07:43.000Z'
    data:
      edited: true
      editors:
      - Maggiet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9831267595291138
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
          fullname: Magaret Thoter
          isHf: false
          isPro: false
          name: Maggiet
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I have tried this just before the superhots were released. The answer
          is no. It "works" in that it''s not throwing error exceptions, but it will
          give you gibberish, mostly punctuation or repeat the same a few words, as
          if it were autistic. 2k is the max for those.<br>The superhots work. 8k
          even. But some models seem to behave better than other, and obviously the
          higher "b" count, the better the results.</p>

          </blockquote>

          <p>Ah, but if you tested before the SuperHOTs came out then I think you
          only tested by adjusting the UI setting, right?  You didn''t also have <code>--contextsize
          4096</code> on the command line, which is what applies the magic algorithm
          that should in theory work with any model.</p>

          <p>That algorithm was only added after SuperHOT, because it was Kaio Ken''s
          work on SuperHOT that showed everyone how to increase context with a very
          simple change to the code.</p>

          <p>The SuperHOT models then also feature training on &gt;2K prompts to boost
          its ability to answer those, but in theory it should also work without that,
          albeit probably not as well.</p>

          </blockquote>

          <p>I''m not a 100% certain, but I have batch file launchers with contextsize
          params. If you mean the models released within a window of a few days saying
          they were 4K capable, then those did work. But the ordinary 2K ones prior
          don''t.</p>

          <p>contextsize params were added for KoboldAI Lite when your readme''s called
          for it. Oogabooga parameters were used before the readme''s.</p>

          '
        raw: "> > I have tried this just before the superhots were released. The answer\
          \ is no. It \"works\" in that it's not throwing error exceptions, but it\
          \ will give you gibberish, mostly punctuation or repeat the same a few words,\
          \ as if it were autistic. 2k is the max for those.\n> > The superhots work.\
          \ 8k even. But some models seem to behave better than other, and obviously\
          \ the higher \"b\" count, the better the results.\n> \n> Ah, but if you\
          \ tested before the SuperHOTs came out then I think you only tested by adjusting\
          \ the UI setting, right?  You didn't also have `--contextsize 4096` on the\
          \ command line, which is what applies the magic algorithm that should in\
          \ theory work with any model.\n> \n> That algorithm was only added after\
          \ SuperHOT, because it was Kaio Ken's work on SuperHOT that showed everyone\
          \ how to increase context with a very simple change to the code.\n> \n>\
          \ The SuperHOT models then also feature training on >2K prompts to boost\
          \ its ability to answer those, but in theory it should also work without\
          \ that, albeit probably not as well.\n\nI'm not a 100% certain, but I have\
          \ batch file launchers with contextsize params. If you mean the models released\
          \ within a window of a few days saying they were 4K capable, then those\
          \ did work. But the ordinary 2K ones prior don't.\n\ncontextsize params\
          \ were added for KoboldAI Lite when your readme's called for it. Oogabooga\
          \ parameters were used before the readme's."
        updatedAt: '2023-07-10T07:17:56.086Z'
      numEdits: 1
      reactions: []
    id: 64ab2fafe368492ab8050a64
    type: comment
  author: Maggiet
  content: "> > I have tried this just before the superhots were released. The answer\
    \ is no. It \"works\" in that it's not throwing error exceptions, but it will\
    \ give you gibberish, mostly punctuation or repeat the same a few words, as if\
    \ it were autistic. 2k is the max for those.\n> > The superhots work. 8k even.\
    \ But some models seem to behave better than other, and obviously the higher \"\
    b\" count, the better the results.\n> \n> Ah, but if you tested before the SuperHOTs\
    \ came out then I think you only tested by adjusting the UI setting, right?  You\
    \ didn't also have `--contextsize 4096` on the command line, which is what applies\
    \ the magic algorithm that should in theory work with any model.\n> \n> That algorithm\
    \ was only added after SuperHOT, because it was Kaio Ken's work on SuperHOT that\
    \ showed everyone how to increase context with a very simple change to the code.\n\
    > \n> The SuperHOT models then also feature training on >2K prompts to boost its\
    \ ability to answer those, but in theory it should also work without that, albeit\
    \ probably not as well.\n\nI'm not a 100% certain, but I have batch file launchers\
    \ with contextsize params. If you mean the models released within a window of\
    \ a few days saying they were 4K capable, then those did work. But the ordinary\
    \ 2K ones prior don't.\n\ncontextsize params were added for KoboldAI Lite when\
    \ your readme's called for it. Oogabooga parameters were used before the readme's."
  created_at: 2023-07-09 21:07:43+00:00
  edited: true
  hidden: false
  id: 64ab2fafe368492ab8050a64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
      fullname: Magaret Thoter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maggiet
      type: user
    createdAt: '2023-07-10T15:29:08.000Z'
    data:
      edited: true
      editors:
      - Maggiet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9464328289031982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
          fullname: Magaret Thoter
          isHf: false
          isPro: false
          name: Maggiet
          type: user
        html: '<p>So after some tests, and as TheBloke said, it would seem that the
          changes within the software itself allows for larger contexts. And so the
          older models do work on higher context. minotaur-15b.ggmlv3.q4_0.bin crashes
          due to memory access violation, even if I have the space. Most have worked.
          There are problems with knowing which format the model follows (Alpaca,
          Vicuna, WizardLM, etc). I say this is a problem because there are no template
          functionality for KoboldAI as there is with Oobagooba, and so one must go
          to the settings, remember which format is to be used, and then enter the
          format manually. At least with Oobagooba one can quickly change templates
          when the incorrect format was used. I mostly cpu render, so results are
          EXTREMELY slow.</p>

          '
        raw: So after some tests, and as TheBloke said, it would seem that the changes
          within the software itself allows for larger contexts. And so the older
          models do work on higher context. minotaur-15b.ggmlv3.q4_0.bin crashes due
          to memory access violation, even if I have the space. Most have worked.
          There are problems with knowing which format the model follows (Alpaca,
          Vicuna, WizardLM, etc). I say this is a problem because there are no template
          functionality for KoboldAI as there is with Oobagooba, and so one must go
          to the settings, remember which format is to be used, and then enter the
          format manually. At least with Oobagooba one can quickly change templates
          when the incorrect format was used. I mostly cpu render, so results are
          EXTREMELY slow.
        updatedAt: '2023-07-10T15:30:50.564Z'
      numEdits: 1
      reactions: []
    id: 64ac23c414b4e5d0b2559c12
    type: comment
  author: Maggiet
  content: So after some tests, and as TheBloke said, it would seem that the changes
    within the software itself allows for larger contexts. And so the older models
    do work on higher context. minotaur-15b.ggmlv3.q4_0.bin crashes due to memory
    access violation, even if I have the space. Most have worked. There are problems
    with knowing which format the model follows (Alpaca, Vicuna, WizardLM, etc). I
    say this is a problem because there are no template functionality for KoboldAI
    as there is with Oobagooba, and so one must go to the settings, remember which
    format is to be used, and then enter the format manually. At least with Oobagooba
    one can quickly change templates when the incorrect format was used. I mostly
    cpu render, so results are EXTREMELY slow.
  created_at: 2023-07-10 14:29:08+00:00
  edited: true
  hidden: false
  id: 64ac23c414b4e5d0b2559c12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-10T15:47:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9740362763404846
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, thanks for letting us know.  Good to hear it''s working for
          most of them.</p>

          <p>And yes I agree that the KoboldCpp UI trails behind text-generation-webui
          in areas like this. I''ve been told it was primarily designed as a story
          telling UI, so things like instruct template haven''t received much attention.  I
          really dislike how small those settings text boxes are, for one thing :)  Not
          even big enough to display <code>### Instruction:</code> properly.</p>

          <p>Hopefully it''ll improve over time.</p>

          '
        raw: 'Great, thanks for letting us know.  Good to hear it''s working for most
          of them.


          And yes I agree that the KoboldCpp UI trails behind text-generation-webui
          in areas like this. I''ve been told it was primarily designed as a story
          telling UI, so things like instruct template haven''t received much attention.  I
          really dislike how small those settings text boxes are, for one thing :)  Not
          even big enough to display `### Instruction:` properly.


          Hopefully it''ll improve over time.'
        updatedAt: '2023-07-10T15:47:27.389Z'
      numEdits: 0
      reactions: []
    id: 64ac280f466a245246c43606
    type: comment
  author: TheBloke
  content: 'Great, thanks for letting us know.  Good to hear it''s working for most
    of them.


    And yes I agree that the KoboldCpp UI trails behind text-generation-webui in areas
    like this. I''ve been told it was primarily designed as a story telling UI, so
    things like instruct template haven''t received much attention.  I really dislike
    how small those settings text boxes are, for one thing :)  Not even big enough
    to display `### Instruction:` properly.


    Hopefully it''ll improve over time.'
  created_at: 2023-07-10 14:47:27+00:00
  edited: false
  hidden: false
  id: 64ac280f466a245246c43606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
      fullname: Magaret Thoter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maggiet
      type: user
    createdAt: '2023-07-10T17:12:36.000Z'
    data:
      edited: false
      editors:
      - Maggiet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9884621500968933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c569973b5722c35cc6e68feafc8e8a6f.svg
          fullname: Magaret Thoter
          isHf: false
          isPro: false
          name: Maggiet
          type: user
        html: '<p>I was running 1.33 KoboldAI. I''ll seeing that there is an update
          with improvements. </p>

          '
        raw: 'I was running 1.33 KoboldAI. I''ll seeing that there is an update with
          improvements. '
        updatedAt: '2023-07-10T17:12:36.829Z'
      numEdits: 0
      reactions: []
    id: 64ac3c04afb6aa5534ff756b
    type: comment
  author: Maggiet
  content: 'I was running 1.33 KoboldAI. I''ll seeing that there is an update with
    improvements. '
  created_at: 2023-07-10 16:12:36+00:00
  edited: false
  hidden: false
  id: 64ac3c04afb6aa5534ff756b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML
repo_type: model
status: open
target_branch: null
title: 'superhot vs normal '
