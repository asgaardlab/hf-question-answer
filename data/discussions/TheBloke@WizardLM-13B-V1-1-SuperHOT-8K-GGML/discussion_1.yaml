!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kameo209
conflicting_files: null
created_at: 2023-07-07 17:58:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fa6f28259c87b87c37ba065d36c64fd.svg
      fullname: Cristian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kameo209
      type: user
    createdAt: '2023-07-07T18:58:36.000Z'
    data:
      edited: true
      editors:
      - Kameo209
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.967377245426178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fa6f28259c87b87c37ba065d36c64fd.svg
          fullname: Cristian
          isHf: false
          isPro: false
          name: Kameo209
          type: user
        html: '<p>Hello, I''m new to all this about models and pygmalion, although
          I''m already joined to discords, the fact is that I recently bought a Radeon
          AMD rx 6950 xt graphics card and I''ve encountered the problem that people
          say that Exllama only It works on nvidia or on AMD with Linux, I don''t
          use Linux, I have windows 10 home official bought, I would like to ask if
          I can run some of your models with 6k tokens or 8k tokens even without using
          Exllama with my 16 vram and my 32 gigabytes of ram in windows 10 with my
          AMD graphics card?, in the event that it is required in a mandatory way
          use  Exllama, can you tell me if there is any way to make it work?, are
          you working on a version that works in windows for AMD??</p>

          '
        raw: 'Hello, I''m new to all this about models and pygmalion, although I''m
          already joined to discords, the fact is that I recently bought a Radeon
          AMD rx 6950 xt graphics card and I''ve encountered the problem that people
          say that Exllama only It works on nvidia or on AMD with Linux, I don''t
          use Linux, I have windows 10 home official bought, I would like to ask if
          I can run some of your models with 6k tokens or 8k tokens even without using
          Exllama with my 16 vram and my 32 gigabytes of ram in windows 10 with my
          AMD graphics card?, in the event that it is required in a mandatory way
          use  Exllama, can you tell me if there is any way to make it work?, are
          you working on a version that works in windows for AMD??

          '
        updatedAt: '2023-07-07T19:00:18.176Z'
      numEdits: 2
      reactions: []
    id: 64a8605c73790912c764a8b0
    type: comment
  author: Kameo209
  content: 'Hello, I''m new to all this about models and pygmalion, although I''m
    already joined to discords, the fact is that I recently bought a Radeon AMD rx
    6950 xt graphics card and I''ve encountered the problem that people say that Exllama
    only It works on nvidia or on AMD with Linux, I don''t use Linux, I have windows
    10 home official bought, I would like to ask if I can run some of your models
    with 6k tokens or 8k tokens even without using Exllama with my 16 vram and my
    32 gigabytes of ram in windows 10 with my AMD graphics card?, in the event that
    it is required in a mandatory way use  Exllama, can you tell me if there is any
    way to make it work?, are you working on a version that works in windows for AMD??

    '
  created_at: 2023-07-07 17:58:36+00:00
  edited: true
  hidden: false
  id: 64a8605c73790912c764a8b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T19:10:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533208608627319
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GPTQ models don''t work on AMD on Windows, no. Nothing I can do
          about that, only AMD can change that by releasing their ROCm code on Windows,
          which so far they haven''t done.</p>

          <p>But GGML models also provide GPU acceleration and they work with AMD
          GPUs on all platforms. You''re commenting on a GGML model here.   Check
          this README for details on using KoboldCpp, and read its instructions, and
          then use it with OpenCL acceleration.  That''ll give you AMD GPU acceleration
          with up to 8k context size.</p>

          '
        raw: 'GPTQ models don''t work on AMD on Windows, no. Nothing I can do about
          that, only AMD can change that by releasing their ROCm code on Windows,
          which so far they haven''t done.


          But GGML models also provide GPU acceleration and they work with AMD GPUs
          on all platforms. You''re commenting on a GGML model here.   Check this
          README for details on using KoboldCpp, and read its instructions, and then
          use it with OpenCL acceleration.  That''ll give you AMD GPU acceleration
          with up to 8k context size.'
        updatedAt: '2023-07-07T19:10:18.763Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - shafiqalibhai
    id: 64a8631af972a36c53a407d0
    type: comment
  author: TheBloke
  content: 'GPTQ models don''t work on AMD on Windows, no. Nothing I can do about
    that, only AMD can change that by releasing their ROCm code on Windows, which
    so far they haven''t done.


    But GGML models also provide GPU acceleration and they work with AMD GPUs on all
    platforms. You''re commenting on a GGML model here.   Check this README for details
    on using KoboldCpp, and read its instructions, and then use it with OpenCL acceleration.  That''ll
    give you AMD GPU acceleration with up to 8k context size.'
  created_at: 2023-07-07 18:10:18+00:00
  edited: false
  hidden: false
  id: 64a8631af972a36c53a407d0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML
repo_type: model
status: open
target_branch: null
title: doubt about 8K models with Amd graphic rx amd 6950 xt
