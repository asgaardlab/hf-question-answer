!!python/object:huggingface_hub.community.DiscussionWithDetails
author: taareshg
conflicting_files: null
created_at: 2023-08-07 05:24:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9d642545629ce24a8e3887685d23dc5.svg
      fullname: Taaresh Gautam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taareshg
      type: user
    createdAt: '2023-08-07T06:24:12.000Z'
    data:
      edited: false
      editors:
      - taareshg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444127678871155
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9d642545629ce24a8e3887685d23dc5.svg
          fullname: Taaresh Gautam
          isHf: false
          isPro: false
          name: taareshg
          type: user
        html: '<p>Hi, thanks for the colab notebook.</p>

          <p>I was going through the colab, I understand you did some post-processing
          in the final text generated.<br>                       print(generated_text.split(":
          ")[1].split(": ")[-1])</p>

          <p>I was doing a similar task, but after the correct response the model
          generates some extra text, Any way to solve this, so no post processing
          is needed.</p>

          '
        raw: "Hi, thanks for the colab notebook.\r\n\r\nI was going through the colab,\
          \ I understand you did some post-processing in the final text generated.\
          \ \r\n                       print(generated_text.split(\"<human>: \")[1].split(\"\
          <bot>: \")[-1])\r\n\r\nI was doing a similar task, but after the correct\
          \ response the model generates some extra text, Any way to solve this, so\
          \ no post processing is needed."
        updatedAt: '2023-08-07T06:24:12.670Z'
      numEdits: 0
      reactions: []
    id: 64d08e0c072225e7f0e395d8
    type: comment
  author: taareshg
  content: "Hi, thanks for the colab notebook.\r\n\r\nI was going through the colab,\
    \ I understand you did some post-processing in the final text generated. \r\n\
    \                       print(generated_text.split(\"<human>: \")[1].split(\"\
    <bot>: \")[-1])\r\n\r\nI was doing a similar task, but after the correct response\
    \ the model generates some extra text, Any way to solve this, so no post processing\
    \ is needed."
  created_at: 2023-08-07 05:24:12+00:00
  edited: false
  hidden: false
  id: 64d08e0c072225e7f0e395d8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: dfurman/Falcon-7B-Chat-v0.1
repo_type: model
status: open
target_branch: null
title: 'Post processing the output. '
