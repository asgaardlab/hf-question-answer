!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lazyDataScientist
conflicting_files: null
created_at: 2023-10-05 18:45:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
      fullname: Cedrick Hesketh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyDataScientist
      type: user
    createdAt: '2023-10-05T19:45:31.000Z'
    data:
      edited: false
      editors:
      - lazyDataScientist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39911219477653503
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
          fullname: Cedrick Hesketh
          isHf: false
          isPro: false
          name: lazyDataScientist
          type: user
        html: "<p>I haven't tested this but this should help handle the <code>int8_float16</code></p>\n\
          <pre><code>import ctranslate2\nfrom transformers import AutoTokenizer\n\n\
          model_name = \"blackmount8/falcon-7b-instruct-ct2-int8_float16\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name, use_fast=False, padding_side=\"\
          left\", truncation_side=\"left\")\nmodel = ctranslate2.Generator(model_name,\
          \ device=\"auto\", compute_type=\"float16\")\n\ninput_text = [\"What is\
          \ the meaning of stonehenge?\", \"Hello mate!\"]\n\ninput_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\", padding=True, truncation=True).input_ids\ninput_tokens\
          \ = [tokenizer.convert_ids_to_tokens(ele) for ele in input_ids]\n\noutputs\
          \ = model.generate_batch(input_tokens, max_length=128)\n\noutput_tokens\
          \ = [\n    ele.sequences_ids[0] for ele in outputs\n]\n\noutput = tokenizer.batch_decode(output_tokens)\n\
          \nprint(output)\n</code></pre>\n"
        raw: "I haven't tested this but this should help handle the ```int8_float16```\r\
          \n```\r\nimport ctranslate2\r\nfrom transformers import AutoTokenizer\r\n\
          \r\nmodel_name = \"blackmount8/falcon-7b-instruct-ct2-int8_float16\"\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False,\
          \ padding_side=\"left\", truncation_side=\"left\")\r\nmodel = ctranslate2.Generator(model_name,\
          \ device=\"auto\", compute_type=\"float16\")\r\n\r\ninput_text = [\"What\
          \ is the meaning of stonehenge?\", \"Hello mate!\"]\r\n\r\ninput_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\", padding=True, truncation=True).input_ids\r\ninput_tokens\
          \ = [tokenizer.convert_ids_to_tokens(ele) for ele in input_ids]\r\n\r\n\
          outputs = model.generate_batch(input_tokens, max_length=128)\r\n\r\noutput_tokens\
          \ = [\r\n    ele.sequences_ids[0] for ele in outputs\r\n]\r\n\r\noutput\
          \ = tokenizer.batch_decode(output_tokens)\r\n\r\nprint(output)\r\n```"
        updatedAt: '2023-10-05T19:45:31.016Z'
      numEdits: 0
      reactions: []
    id: 651f125bc2f92699e2954dd0
    type: comment
  author: lazyDataScientist
  content: "I haven't tested this but this should help handle the ```int8_float16```\r\
    \n```\r\nimport ctranslate2\r\nfrom transformers import AutoTokenizer\r\n\r\n\
    model_name = \"blackmount8/falcon-7b-instruct-ct2-int8_float16\"\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name, use_fast=False, padding_side=\"\
    left\", truncation_side=\"left\")\r\nmodel = ctranslate2.Generator(model_name,\
    \ device=\"auto\", compute_type=\"float16\")\r\n\r\ninput_text = [\"What is the\
    \ meaning of stonehenge?\", \"Hello mate!\"]\r\n\r\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\", padding=True, truncation=True).input_ids\r\ninput_tokens\
    \ = [tokenizer.convert_ids_to_tokens(ele) for ele in input_ids]\r\n\r\noutputs\
    \ = model.generate_batch(input_tokens, max_length=128)\r\n\r\noutput_tokens =\
    \ [\r\n    ele.sequences_ids[0] for ele in outputs\r\n]\r\n\r\noutput = tokenizer.batch_decode(output_tokens)\r\
    \n\r\nprint(output)\r\n```"
  created_at: 2023-10-05 18:45:31+00:00
  edited: false
  hidden: false
  id: 651f125bc2f92699e2954dd0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: blackmount8/falcon-7b-instruct-ct2-int8_float16
repo_type: model
status: open
target_branch: null
title: Sample Code for int8_float16
