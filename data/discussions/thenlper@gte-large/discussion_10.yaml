!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jinkin
conflicting_files: null
created_at: 2023-08-24 01:29:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/v6rEJybo42A251aXU2CaO.png?w=200&h=200&f=face
      fullname: HuangJunqin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jinkin
      type: user
    createdAt: '2023-08-24T02:29:24.000Z'
    data:
      edited: false
      editors:
      - Jinkin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9042665362358093
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/v6rEJybo42A251aXU2CaO.png?w=200&h=200&f=face
          fullname: HuangJunqin
          isHf: false
          isPro: false
          name: Jinkin
          type: user
        html: '<p>hello great work !</p>

          <p>One of my questions is how to get such a large global batch size during
          training. It is mentioned in Table 3 of your paper that you used a global
          batch size of 16384 for base model.</p>

          <p>For me, I used 8 32G V100, and used fp16, gradient checkpointing and
          ZERO1 to reduce memory usage. When the max length is 128, my batch size
          per gpu can reach 1024. This is without synchronizing the negative.</p>

          <p>However, once I synchronize the negative samples of 8 cards, my memory
          will be OOM, and my batch size per gpu will drop from 1024 to about 192
          when negative sample synchronization is turned on. And if I use more GPUs,
          this number drops even further.</p>

          <p>Is this normal? Do you use negative synchronizing across device during
          training ?</p>

          '
        raw: "hello great work !\r\n\r\nOne of my questions is how to get such a large\
          \ global batch size during training. It is mentioned in Table 3 of your\
          \ paper that you used a global batch size of 16384 for base model.\r\n\r\
          \nFor me, I used 8 32G V100, and used fp16, gradient checkpointing and ZERO1\
          \ to reduce memory usage. When the max length is 128, my batch size per\
          \ gpu can reach 1024. This is without synchronizing the negative.\r\n\r\n\
          However, once I synchronize the negative samples of 8 cards, my memory will\
          \ be OOM, and my batch size per gpu will drop from 1024 to about 192 when\
          \ negative sample synchronization is turned on. And if I use more GPUs,\
          \ this number drops even further.\r\n\r\nIs this normal? Do you use negative\
          \ synchronizing across device during training ?"
        updatedAt: '2023-08-24T02:29:24.453Z'
      numEdits: 0
      reactions: []
    id: 64e6c084ccfe005d2bfe0984
    type: comment
  author: Jinkin
  content: "hello great work !\r\n\r\nOne of my questions is how to get such a large\
    \ global batch size during training. It is mentioned in Table 3 of your paper\
    \ that you used a global batch size of 16384 for base model.\r\n\r\nFor me, I\
    \ used 8 32G V100, and used fp16, gradient checkpointing and ZERO1 to reduce memory\
    \ usage. When the max length is 128, my batch size per gpu can reach 1024. This\
    \ is without synchronizing the negative.\r\n\r\nHowever, once I synchronize the\
    \ negative samples of 8 cards, my memory will be OOM, and my batch size per gpu\
    \ will drop from 1024 to about 192 when negative sample synchronization is turned\
    \ on. And if I use more GPUs, this number drops even further.\r\n\r\nIs this normal?\
    \ Do you use negative synchronizing across device during training ?"
  created_at: 2023-08-24 01:29:24+00:00
  edited: false
  hidden: false
  id: 64e6c084ccfe005d2bfe0984
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf5c04a6032709f35e3fb48e1be6976f.svg
      fullname: Dingkun Long
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: thenlper
      type: user
    createdAt: '2023-08-26T05:52:25.000Z'
    data:
      edited: false
      editors:
      - thenlper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7522967457771301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf5c04a6032709f35e3fb48e1be6976f.svg
          fullname: Dingkun Long
          isHf: false
          isPro: false
          name: thenlper
          type: user
        html: '<p>we train our model on 8 A100-80G. on 32G V100, you can use gradient
          accumulation to increase the overall batch size </p>

          '
        raw: 'we train our model on 8 A100-80G. on 32G V100, you can use gradient
          accumulation to increase the overall batch size '
        updatedAt: '2023-08-26T05:52:25.897Z'
      numEdits: 0
      reactions: []
    id: 64e993195b8d8156f2a6a96f
    type: comment
  author: thenlper
  content: 'we train our model on 8 A100-80G. on 32G V100, you can use gradient accumulation
    to increase the overall batch size '
  created_at: 2023-08-26 04:52:25+00:00
  edited: false
  hidden: false
  id: 64e993195b8d8156f2a6a96f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/bf5c04a6032709f35e3fb48e1be6976f.svg
      fullname: Dingkun Long
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: thenlper
      type: user
    createdAt: '2023-12-21T13:59:41.000Z'
    data:
      status: closed
    id: 658444cdb02f38ef34935b15
    type: status-change
  author: thenlper
  created_at: 2023-12-21 13:59:41+00:00
  id: 658444cdb02f38ef34935b15
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: thenlper/gte-large
repo_type: model
status: closed
target_branch: null
title: How to get such a large global batch size
