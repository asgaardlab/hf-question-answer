!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ChudWestFallen
conflicting_files: null
created_at: 2023-07-07 08:04:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dcaed88f8312d352103290d873152353.svg
      fullname: asfdsd asdf sadf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChudWestFallen
      type: user
    createdAt: '2023-07-07T09:04:59.000Z'
    data:
      edited: false
      editors:
      - ChudWestFallen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42626750469207764
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dcaed88f8312d352103290d873152353.svg
          fullname: asfdsd asdf sadf
          isHf: false
          isPro: false
          name: ChudWestFallen
          type: user
        html: '<p>Hey Guys,</p>

          <p>im trying to load this into Oobabooga with the new ExLlama_HF loeader
          and im getting the following error:</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users*\OneDrive\Desktop\Bot2*\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\Users*\OneDrive\Desktop*\Richtig\text-generation-webui\modules\models.py",
          line 74, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users*\OneDrive\Desktop\Bot2*\text-generation-webui\modules\models.py",
          line 301, in ExLlama_HF_loader<br>    return ExllamaHF.from_pretrained(model_name)<br>  File
          "C:\Users*\OneDrive\Desktop\Bot2*\text-generation-webui\modules\exllama_hf.py",
          line 116, in from_pretrained<br>    return ExllamaHF(config)<br>  File "C:\Users*\OneDrive\Desktop\Bot2*\text-generation-webui\modules\exllama_hf.py",
          line 31, in <strong>init</strong><br>    self.ex_model = ExLlama(self.ex_config)<br>  File
          "C:\Users*\OneDrive\Desktop\Bot2*\installer_files\env\lib\site-packages\exllama\model.py",
          line 646, in <strong>init</strong><br>    with safe_open(self.config.model_path,
          framework="pt", device="cpu") as f:<br>safetensors_rust.SafetensorError:
          Error while deserializing header: HeaderTooLarge</p>

          <p>Can somebody help me with this? ty<br>I have a 2080 TI and a 1660. </p>

          '
        raw: "Hey Guys,\r\n\r\nim trying to load this into Oobabooga with the new\
          \ ExLlama_HF loeader and im getting the following error:\r\n\r\nTraceback\
          \ (most recent call last):\r\n  File \"C:\\Users\\*\\OneDrive\\Desktop\\\
          Bot2\\*\\text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\*\\Richtig\\text-generation-webui\\\
          modules\\models.py\", line 74, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\\
          modules\\models.py\", line 301, in ExLlama_HF_loader\r\n    return ExllamaHF.from_pretrained(model_name)\r\
          \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\\
          modules\\exllama_hf.py\", line 116, in from_pretrained\r\n    return ExllamaHF(config)\r\
          \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\\
          modules\\exllama_hf.py\", line 31, in __init__\r\n    self.ex_model = ExLlama(self.ex_config)\r\
          \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\installer_files\\env\\\
          lib\\site-packages\\exllama\\model.py\", line 646, in __init__\r\n    with\
          \ safe_open(self.config.model_path, framework=\"pt\", device=\"cpu\") as\
          \ f:\r\nsafetensors_rust.SafetensorError: Error while deserializing header:\
          \ HeaderTooLarge\r\n\r\nCan somebody help me with this? ty\r\nI have a 2080\
          \ TI and a 1660. "
        updatedAt: '2023-07-07T09:04:59.534Z'
      numEdits: 0
      reactions: []
    id: 64a7d53b470e2016179131bc
    type: comment
  author: ChudWestFallen
  content: "Hey Guys,\r\n\r\nim trying to load this into Oobabooga with the new ExLlama_HF\
    \ loeader and im getting the following error:\r\n\r\nTraceback (most recent call\
    \ last):\r\n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\\
    server.py\", line 68, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
    \ = load_model(shared.model_name, loader)\r\n  File \"C:\\Users\\*\\OneDrive\\\
    Desktop\\*\\Richtig\\text-generation-webui\\modules\\models.py\", line 74, in\
    \ load_model\r\n    output = load_func_map[loader](model_name)\r\n  File \"C:\\\
    Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\modules\\models.py\"\
    , line 301, in ExLlama_HF_loader\r\n    return ExllamaHF.from_pretrained(model_name)\r\
    \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\modules\\\
    exllama_hf.py\", line 116, in from_pretrained\r\n    return ExllamaHF(config)\r\
    \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\text-generation-webui\\modules\\\
    exllama_hf.py\", line 31, in __init__\r\n    self.ex_model = ExLlama(self.ex_config)\r\
    \n  File \"C:\\Users\\*\\OneDrive\\Desktop\\Bot2\\*\\installer_files\\env\\lib\\\
    site-packages\\exllama\\model.py\", line 646, in __init__\r\n    with safe_open(self.config.model_path,\
    \ framework=\"pt\", device=\"cpu\") as f:\r\nsafetensors_rust.SafetensorError:\
    \ Error while deserializing header: HeaderTooLarge\r\n\r\nCan somebody help me\
    \ with this? ty\r\nI have a 2080 TI and a 1660. "
  created_at: 2023-07-07 08:04:59+00:00
  edited: false
  hidden: false
  id: 64a7d53b470e2016179131bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:06:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7542722821235657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re loading the wrong model, this is the fp16, you want the
          GPTQ: <a href="https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ">https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ</a></p>

          '
        raw: 'You''re loading the wrong model, this is the fp16, you want the GPTQ:
          https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ'
        updatedAt: '2023-07-07T09:06:16.006Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ChudWestFallen
      - count: 1
        reaction: "\U0001F917"
        users:
        - ChudWestFallen
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ChudWestFallen
    id: 64a7d588e164a93d464cd050
    type: comment
  author: TheBloke
  content: 'You''re loading the wrong model, this is the fp16, you want the GPTQ:
    https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ'
  created_at: 2023-07-07 08:06:16+00:00
  edited: false
  hidden: false
  id: 64a7d588e164a93d464cd050
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-fp16
repo_type: model
status: open
target_branch: null
title: HeaderTooLarge Error
