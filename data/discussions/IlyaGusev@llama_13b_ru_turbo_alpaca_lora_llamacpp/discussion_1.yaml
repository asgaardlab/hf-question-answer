!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andre456
conflicting_files: null
created_at: 2023-04-15 08:01:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65dac2358fb5fc438423505db0b67fd6.svg
      fullname: al
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andre456
      type: user
    createdAt: '2023-04-15T09:01:53.000Z'
    data:
      edited: false
      editors:
      - andre456
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65dac2358fb5fc438423505db0b67fd6.svg
          fullname: al
          isHf: false
          isPro: false
          name: andre456
          type: user
        html: "<p>\u043F\u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435 \u043F\
          \u043E\u0436\u0430\u043B\u0443\u0439\u0441\u0442\u0430.<br>\u043D\u0430\
          \ \u0432\u0438\u043D\u0434\u0435 \u0440\u0430\u0437\u0432\u0435\u0440\u043D\
          \u0443\u0442 oobabooga/text-generation-webui<br>\u0432\u0438\u043A\u0443\
          \u043D\u044F \u0440\u0430\u0431\u043E\u0442\u0430\u0435\u0442 \u043D\u043E\
          \u0440\u043C\u0430\u043B\u044C\u043D\u043E.<br>\u043F\u0440\u0438 \u0437\
          \u0430\u043F\u0443\u0441\u043A\u0435 IlyaGusev/llama_13b_ru_turbo_alpaca_lora_llamacpp\
          \  \u0434\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\u0443<br>Traceback\
          \ (most recent call last):<br>File \u201CG:\\ChatGPT\\text-generation-webui\\\
          server.py\u201D, line 84, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>File \u201CG:\\ChatGPT\\text-generation-webui\\\
          modules\\models.py\u201D, line 171, in load_model<br>model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ **params)<br>File \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 441, in from_pretrained<br>config,\
          \ kwargs = AutoConfig.from_pretrained(<br>File \u201CG:\\ChatGPT\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u201D\
          , line 916, in from_pretrained<br>config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)<br>File \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
          transformers\\configuration_utils.py\u201D, line 573, in get_config_dict<br>config_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)<br>File\
          \ \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\transformers\\\
          configuration_utils.py\u201D, line 628, in _get_config_dict<br>resolved_config_file\
          \ = cached_file(<br>File \u201CG:\\ChatGPT\\installer_files\\env\\lib\\\
          site-packages\\transformers\\utils\\hub.py\u201D, line 380, in cached_file<br>raise\
          \ EnvironmentError(<br>OSError: models\\llama_13b_ru_turbo_alpaca_lora_llamacpp\
          \ does not appear to have a file named config.json. Checkout \u2018<a href=\"\
          https://huggingface.co/models%5Cllama_13b_ru_turbo_alpaca_lora_llamacpp/None%E2%80%99\"\
          >https://huggingface.co/models\\llama_13b_ru_turbo_alpaca_lora_llamacpp/None\u2019\
          </a> for available files.</p>\n<p>\u0441\u043A\u0440\u0438\u043D \u043D\u0430\
          \u0441\u0442\u0440\u043E\u0435\u043A \u043F\u0440\u0438\u043B\u0430\u0433\
          \u0430\u044E<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6439a7fd3ab54fdbab81a294/pEzFmgFAYjou-RyGfUVIu.png\"\
          ><img alt=\"Screenshot_105.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6439a7fd3ab54fdbab81a294/pEzFmgFAYjou-RyGfUVIu.png\"\
          ></a></p>\n<p>\u0447\u0442\u043E \u043C\u043E\u0436\u043D\u043E \u0441\u0434\
          \u0435\u043B\u0430\u0442\u044C?</p>\n"
        raw: "\u043F\u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435 \u043F\u043E\
          \u0436\u0430\u043B\u0443\u0439\u0441\u0442\u0430.\r\n\u043D\u0430 \u0432\
          \u0438\u043D\u0434\u0435 \u0440\u0430\u0437\u0432\u0435\u0440\u043D\u0443\
          \u0442 oobabooga/text-generation-webui\r\n\u0432\u0438\u043A\u0443\u043D\
          \u044F \u0440\u0430\u0431\u043E\u0442\u0430\u0435\u0442 \u043D\u043E\u0440\
          \u043C\u0430\u043B\u044C\u043D\u043E.\r\n\u043F\u0440\u0438 \u0437\u0430\
          \u043F\u0443\u0441\u043A\u0435 IlyaGusev/llama_13b_ru_turbo_alpaca_lora_llamacpp\
          \  \u0434\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\u0443\r\nTraceback\
          \ (most recent call last):\r\nFile \u201CG:\\ChatGPT\\text-generation-webui\\\
          server.py\u201D, line 84, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\nFile \u201CG:\\ChatGPT\\text-generation-webui\\\
          modules\\models.py\u201D, line 171, in load_model\r\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ **params)\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 441, in from_pretrained\r\
          \nconfig, kwargs = AutoConfig.from_pretrained(\r\nFile \u201CG:\\ChatGPT\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u201D\
          , line 916, in from_pretrained\r\nconfig_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
          transformers\\configuration_utils.py\u201D, line 573, in get_config_dict\r\
          \nconfig_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
          transformers\\configuration_utils.py\u201D, line 628, in _get_config_dict\r\
          \nresolved_config_file = cached_file(\r\nFile \u201CG:\\ChatGPT\\installer_files\\\
          env\\lib\\site-packages\\transformers\\utils\\hub.py\u201D, line 380, in\
          \ cached_file\r\nraise EnvironmentError(\r\nOSError: models\\llama_13b_ru_turbo_alpaca_lora_llamacpp\
          \ does not appear to have a file named config.json. Checkout \u2018https://huggingface.co/models\\\
          llama_13b_ru_turbo_alpaca_lora_llamacpp/None\u2019 for available files.\r\
          \n\r\n\u0441\u043A\u0440\u0438\u043D \u043D\u0430\u0441\u0442\u0440\u043E\
          \u0435\u043A \u043F\u0440\u0438\u043B\u0430\u0433\u0430\u044E\r\n![Screenshot_105.png](https://cdn-uploads.huggingface.co/production/uploads/6439a7fd3ab54fdbab81a294/pEzFmgFAYjou-RyGfUVIu.png)\r\
          \n\r\n\u0447\u0442\u043E \u043C\u043E\u0436\u043D\u043E \u0441\u0434\u0435\
          \u043B\u0430\u0442\u044C?"
        updatedAt: '2023-04-15T09:01:53.243Z'
      numEdits: 0
      reactions: []
    id: 643a6801b54b3465564e4598
    type: comment
  author: andre456
  content: "\u043F\u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435 \u043F\u043E\
    \u0436\u0430\u043B\u0443\u0439\u0441\u0442\u0430.\r\n\u043D\u0430 \u0432\u0438\
    \u043D\u0434\u0435 \u0440\u0430\u0437\u0432\u0435\u0440\u043D\u0443\u0442 oobabooga/text-generation-webui\r\
    \n\u0432\u0438\u043A\u0443\u043D\u044F \u0440\u0430\u0431\u043E\u0442\u0430\u0435\
    \u0442 \u043D\u043E\u0440\u043C\u0430\u043B\u044C\u043D\u043E.\r\n\u043F\u0440\
    \u0438 \u0437\u0430\u043F\u0443\u0441\u043A\u0435 IlyaGusev/llama_13b_ru_turbo_alpaca_lora_llamacpp\
    \  \u0434\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\u0443\r\nTraceback\
    \ (most recent call last):\r\nFile \u201CG:\\ChatGPT\\text-generation-webui\\\
    server.py\u201D, line 84, in load_model_wrapper\r\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\r\nFile \u201CG:\\ChatGPT\\text-generation-webui\\\
    modules\\models.py\u201D, line 171, in load_model\r\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\
    \ **params)\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 441, in from_pretrained\r\
    \nconfig, kwargs = AutoConfig.from_pretrained(\r\nFile \u201CG:\\ChatGPT\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u201D\
    , line 916, in from_pretrained\r\nconfig_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
    transformers\\configuration_utils.py\u201D, line 573, in get_config_dict\r\nconfig_dict,\
    \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\nFile\
    \ \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\transformers\\\
    configuration_utils.py\u201D, line 628, in _get_config_dict\r\nresolved_config_file\
    \ = cached_file(\r\nFile \u201CG:\\ChatGPT\\installer_files\\env\\lib\\site-packages\\\
    transformers\\utils\\hub.py\u201D, line 380, in cached_file\r\nraise EnvironmentError(\r\
    \nOSError: models\\llama_13b_ru_turbo_alpaca_lora_llamacpp does not appear to\
    \ have a file named config.json. Checkout \u2018https://huggingface.co/models\\\
    llama_13b_ru_turbo_alpaca_lora_llamacpp/None\u2019 for available files.\r\n\r\n\
    \u0441\u043A\u0440\u0438\u043D \u043D\u0430\u0441\u0442\u0440\u043E\u0435\u043A\
    \ \u043F\u0440\u0438\u043B\u0430\u0433\u0430\u044E\r\n![Screenshot_105.png](https://cdn-uploads.huggingface.co/production/uploads/6439a7fd3ab54fdbab81a294/pEzFmgFAYjou-RyGfUVIu.png)\r\
    \n\r\n\u0447\u0442\u043E \u043C\u043E\u0436\u043D\u043E \u0441\u0434\u0435\u043B\
    \u0430\u0442\u044C?"
  created_at: 2023-04-15 08:01:53+00:00
  edited: false
  hidden: false
  id: 643a6801b54b3465564e4598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-04-15T21:48:54.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: "<p>\u041D\u0435 \u0438\u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\
          \u0430\u0442\u044C llamacpp \u0432\u0435\u0440\u0441\u0438\u044E, \u0430\
          \ \u0438\u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\u0430\u0442\u044C\
          \ \u043E\u0440\u0438\u0433\u0438\u043D\u0430\u043B\u044C\u043D\u0443\u044E\
          : <a href=\"https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora\"\
          >https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora</a></p>\n"
        raw: "\u041D\u0435 \u0438\u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\u0430\
          \u0442\u044C llamacpp \u0432\u0435\u0440\u0441\u0438\u044E, \u0430 \u0438\
          \u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\u0430\u0442\u044C \u043E\
          \u0440\u0438\u0433\u0438\u043D\u0430\u043B\u044C\u043D\u0443\u044E: https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora"
        updatedAt: '2023-04-15T21:48:54.430Z'
      numEdits: 0
      reactions: []
    id: 643b1bc6e756b67eee1fa0fb
    type: comment
  author: IlyaGusev
  content: "\u041D\u0435 \u0438\u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\u0430\
    \u0442\u044C llamacpp \u0432\u0435\u0440\u0441\u0438\u044E, \u0430 \u0438\u0441\
    \u043F\u043E\u043B\u044C\u0437\u043E\u0432\u0430\u0442\u044C \u043E\u0440\u0438\
    \u0433\u0438\u043D\u0430\u043B\u044C\u043D\u0443\u044E: https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora"
  created_at: 2023-04-15 20:48:54+00:00
  edited: false
  hidden: false
  id: 643b1bc6e756b67eee1fa0fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-05-15T20:40:29.000Z'
    data:
      status: closed
    id: 646298bd48e13890ea551a59
    type: status-change
  author: IlyaGusev
  created_at: 2023-05-15 19:40:29+00:00
  id: 646298bd48e13890ea551a59
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: IlyaGusev/llama_13b_ru_turbo_alpaca_lora_llamacpp
repo_type: model
status: closed
target_branch: null
title: "\u0447\u0442\u043E \u0434\u0435\u043B\u0430\u0442\u044C"
