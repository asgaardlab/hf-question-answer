!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 0xK1ller
conflicting_files: null
created_at: 2023-04-18 14:10:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3dbc4d956b2f190897a4b102f0fcf04.svg
      fullname: Hristo Kalinov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0xK1ller
      type: user
    createdAt: '2023-04-18T15:10:29.000Z'
    data:
      edited: true
      editors:
      - 0xK1ller
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3dbc4d956b2f190897a4b102f0fcf04.svg
          fullname: Hristo Kalinov
          isHf: false
          isPro: false
          name: 0xK1ller
          type: user
        html: '<p>Trying to convert the model to ggml results in the following exception:</p>

          <p><code>Traceback (most recent call last):   File "D:\Games\huggingface\llama.cpp\convert.py",
          line 1149, in &lt;module&gt;     main()   File "D:\Games\huggingface\llama.cpp\convert.py",
          line 1144, in main     OutputFile.write_all(outfile, params, model, vocab)   File
          "D:\Games\huggingface\llama.cpp\convert.py", line 942, in write_all     check_vocab_size(params,
          vocab)   File "D:\Games\huggingface\llama.cpp\convert.py", line 896, in
          check_vocab_size     raise Exception(msg) Exception: Vocab size mismatch
          (model has 32001, but D:\Games\huggingface\huggingface\models\vicuna-7b\tokenizer.model
          has 32000).  Most likely you are missing added_tokens.json (should be in
          D:\Games\huggingface\huggingface\models\vicuna-7b).</code></p>

          <p>I verified all checksums and also I''m using the latest commit of llama.cpp.
          Does someone know how to resolve the issue?</p>

          <p>Edit: Tried it on google colab in case there was something wrong with
          the environment but I get the same error.</p>

          '
        raw: "Trying to convert the model to ggml results in the following exception:\n\
          ```Traceback (most recent call last):\n  File \"D:\\Games\\huggingface\\\
          llama.cpp\\convert.py\", line 1149, in <module>\n    main()\n  File \"D:\\\
          Games\\huggingface\\llama.cpp\\convert.py\", line 1144, in main\n    OutputFile.write_all(outfile,\
          \ params, model, vocab)\n  File \"D:\\Games\\huggingface\\llama.cpp\\convert.py\"\
          , line 942, in write_all\n    check_vocab_size(params, vocab)\n  File \"\
          D:\\Games\\huggingface\\llama.cpp\\convert.py\", line 896, in check_vocab_size\n\
          \    raise Exception(msg)\nException: Vocab size mismatch (model has 32001,\
          \ but D:\\Games\\huggingface\\huggingface\\models\\vicuna-7b\\tokenizer.model\
          \ has 32000).  Most likely you are missing added_tokens.json (should be\
          \ in D:\\Games\\huggingface\\huggingface\\models\\vicuna-7b).```\n\nI verified\
          \ all checksums and also I'm using the latest commit of llama.cpp. Does\
          \ someone know how to resolve the issue?\n\nEdit: Tried it on google colab\
          \ in case there was something wrong with the environment but I get the same\
          \ error."
        updatedAt: '2023-04-18T18:10:07.748Z'
      numEdits: 3
      reactions: []
    id: 643eb2e539bf5a7e13d30073
    type: comment
  author: 0xK1ller
  content: "Trying to convert the model to ggml results in the following exception:\n\
    ```Traceback (most recent call last):\n  File \"D:\\Games\\huggingface\\llama.cpp\\\
    convert.py\", line 1149, in <module>\n    main()\n  File \"D:\\Games\\huggingface\\\
    llama.cpp\\convert.py\", line 1144, in main\n    OutputFile.write_all(outfile,\
    \ params, model, vocab)\n  File \"D:\\Games\\huggingface\\llama.cpp\\convert.py\"\
    , line 942, in write_all\n    check_vocab_size(params, vocab)\n  File \"D:\\Games\\\
    huggingface\\llama.cpp\\convert.py\", line 896, in check_vocab_size\n    raise\
    \ Exception(msg)\nException: Vocab size mismatch (model has 32001, but D:\\Games\\\
    huggingface\\huggingface\\models\\vicuna-7b\\tokenizer.model has 32000).  Most\
    \ likely you are missing added_tokens.json (should be in D:\\Games\\huggingface\\\
    huggingface\\models\\vicuna-7b).```\n\nI verified all checksums and also I'm using\
    \ the latest commit of llama.cpp. Does someone know how to resolve the issue?\n\
    \nEdit: Tried it on google colab in case there was something wrong with the environment\
    \ but I get the same error."
  created_at: 2023-04-18 14:10:29+00:00
  edited: true
  hidden: false
  id: 643eb2e539bf5a7e13d30073
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: AlekseyKorshuk/vicuna-7b
repo_type: model
status: open
target_branch: null
title: Vocab size mismatch with ggml
