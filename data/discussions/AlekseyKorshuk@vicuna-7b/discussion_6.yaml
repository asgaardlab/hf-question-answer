!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wojhoiw
conflicting_files: null
created_at: 2023-04-11 06:18:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38f34eed31990117a392539dc365db33.svg
      fullname: agre rrwdeds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wojhoiw
      type: user
    createdAt: '2023-04-11T07:18:24.000Z'
    data:
      edited: false
      editors:
      - wojhoiw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38f34eed31990117a392539dc365db33.svg
          fullname: agre rrwdeds
          isHf: false
          isPro: false
          name: wojhoiw
          type: user
        html: '<p>Did anyone managed to convert it to ggml for llama.cpp? I''m having
          a hard time converting it.<br>Tried to convert this model and gptq model
          and both attempt ended up with error messages, and workarounds I searched
          on llama.cpp repository didn''t work.<br>Probably because I''m doing something
          stupid, since I''ve never done model converting before...<br>It would be
          great if someone could share the ggml model.</p>

          '
        raw: "Did anyone managed to convert it to ggml for llama.cpp? I'm having a\
          \ hard time converting it.\r\nTried to convert this model and gptq model\
          \ and both attempt ended up with error messages, and workarounds I searched\
          \ on llama.cpp repository didn't work. \r\nProbably because I'm doing something\
          \ stupid, since I've never done model converting before...\r\nIt would be\
          \ great if someone could share the ggml model."
        updatedAt: '2023-04-11T07:18:24.600Z'
      numEdits: 0
      reactions: []
    id: 643509c04ff6a19a90c9e655
    type: comment
  author: wojhoiw
  content: "Did anyone managed to convert it to ggml for llama.cpp? I'm having a hard\
    \ time converting it.\r\nTried to convert this model and gptq model and both attempt\
    \ ended up with error messages, and workarounds I searched on llama.cpp repository\
    \ didn't work. \r\nProbably because I'm doing something stupid, since I've never\
    \ done model converting before...\r\nIt would be great if someone could share\
    \ the ggml model."
  created_at: 2023-04-11 06:18:24+00:00
  edited: false
  hidden: false
  id: 643509c04ff6a19a90c9e655
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/38f34eed31990117a392539dc365db33.svg
      fullname: agre rrwdeds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wojhoiw
      type: user
    createdAt: '2023-04-11T07:19:22.000Z'
    data:
      from: Did anyone managed to convert it to ggml for llama.cpp?
      to: Did anyone managed to convert it to ggml 4bit for llama.cpp?
    id: 643509fae60b21004ec94e48
    type: title-change
  author: wojhoiw
  created_at: 2023-04-11 06:19:22+00:00
  id: 643509fae60b21004ec94e48
  new_title: Did anyone managed to convert it to ggml 4bit for llama.cpp?
  old_title: Did anyone managed to convert it to ggml for llama.cpp?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
      fullname: Aoum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ai2p
      type: user
    createdAt: '2023-04-11T18:38:26.000Z'
    data:
      edited: true
      editors:
      - ai2p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
          fullname: Aoum
          isHf: false
          isPro: false
          name: ai2p
          type: user
        html: '<blockquote>

          <p>Did anyone managed to convert it to ggml for llama.cpp? </p>

          </blockquote>

          <p>Yes. Unfiltered. 7B-GPTQ-4bit-128g-GGML<br><a href="https://huggingface.co/TheBloke/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g-GGML/blob/main/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g.GGML.bin">https://huggingface.co/TheBloke/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g-GGML/blob/main/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g.GGML.bin</a></p>

          '
        raw: "> Did anyone managed to convert it to ggml for llama.cpp? \n\nYes. Unfiltered.\
          \ 7B-GPTQ-4bit-128g-GGML\nhttps://huggingface.co/TheBloke/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g-GGML/blob/main/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g.GGML.bin"
        updatedAt: '2023-04-11T18:38:43.421Z'
      numEdits: 1
      reactions: []
    id: 6435a92287604914ba3697ba
    type: comment
  author: ai2p
  content: "> Did anyone managed to convert it to ggml for llama.cpp? \n\nYes. Unfiltered.\
    \ 7B-GPTQ-4bit-128g-GGML\nhttps://huggingface.co/TheBloke/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g-GGML/blob/main/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g.GGML.bin"
  created_at: 2023-04-11 17:38:26+00:00
  edited: true
  hidden: false
  id: 6435a92287604914ba3697ba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: AlekseyKorshuk/vicuna-7b
repo_type: model
status: open
target_branch: null
title: Did anyone managed to convert it to ggml 4bit for llama.cpp?
