!!python/object:huggingface_hub.community.DiscussionWithDetails
author: husjerry
conflicting_files: null
created_at: 2023-07-25 03:44:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6abc95a4754d88ee78a6e522d556b34.svg
      fullname: Jerry Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: husjerry
      type: user
    createdAt: '2023-07-25T04:44:11.000Z'
    data:
      edited: false
      editors:
      - husjerry
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7092238068580627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6abc95a4754d88ee78a6e522d556b34.svg
          fullname: Jerry Hu
          isHf: false
          isPro: false
          name: husjerry
          type: user
        html: '<p>Is finetuning supported for this model? If so can people give me
          some pointers?<br>I found this post: <a rel="nofollow" href="https://discuss.huggingface.co/t/finetune-blip-on-customer-dataset-20893/28446">https://discuss.huggingface.co/t/finetune-blip-on-customer-dataset-20893/28446</a><br>but
          this is about <code>Salesforce/blip-vqa-base</code> instead of this model
          <code>Salesforce/blip-image-captioning-large</code></p>

          '
        raw: "Is finetuning supported for this model? If so can people give me some\
          \ pointers?\r\nI found this post: https://discuss.huggingface.co/t/finetune-blip-on-customer-dataset-20893/28446\r\
          \nbut this is about `Salesforce/blip-vqa-base` instead of this model `Salesforce/blip-image-captioning-large`"
        updatedAt: '2023-07-25T04:44:11.330Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - husjerry
    id: 64bf531b5b8d826146128343
    type: comment
  author: husjerry
  content: "Is finetuning supported for this model? If so can people give me some\
    \ pointers?\r\nI found this post: https://discuss.huggingface.co/t/finetune-blip-on-customer-dataset-20893/28446\r\
    \nbut this is about `Salesforce/blip-vqa-base` instead of this model `Salesforce/blip-image-captioning-large`"
  created_at: 2023-07-25 03:44:11+00:00
  edited: false
  hidden: false
  id: 64bf531b5b8d826146128343
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-07T09:06:28.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7998560667037964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;husjerry&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/husjerry\"\
          >@<span class=\"underline\">husjerry</span></a></span>\n\n\t</span></span><br>Fine-tuning\
          \ the VQA should be done in the same way than the image captioning fine-tuning,\
          \ I think the only difference is on the way you prompt the model (but I\
          \ am not sure).<br>You can have a look at the instructions shared on that\
          \ thread or refer to the original vqa fine-tuning script:; <a rel=\"nofollow\"\
          \ href=\"https://github.com/salesforce/BLIP/blob/main/train_vqa.py\">https://github.com/salesforce/BLIP/blob/main/train_vqa.py</a>\
          \ and try to use the HF model or use their model, then convert it to HF\
          \ version using the conversion script here: <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py</a></p>\n"
        raw: "Hi @husjerry \nFine-tuning the VQA should be done in the same way than\
          \ the image captioning fine-tuning, I think the only difference is on the\
          \ way you prompt the model (but I am not sure). \nYou can have a look at\
          \ the instructions shared on that thread or refer to the original vqa fine-tuning\
          \ script:; https://github.com/salesforce/BLIP/blob/main/train_vqa.py and\
          \ try to use the HF model or use their model, then convert it to HF version\
          \ using the conversion script here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py"
        updatedAt: '2023-12-07T09:06:28.694Z'
      numEdits: 0
      reactions: []
    id: 65718b1476ef4bd9c59f3ec5
    type: comment
  author: ybelkada
  content: "Hi @husjerry \nFine-tuning the VQA should be done in the same way than\
    \ the image captioning fine-tuning, I think the only difference is on the way\
    \ you prompt the model (but I am not sure). \nYou can have a look at the instructions\
    \ shared on that thread or refer to the original vqa fine-tuning script:; https://github.com/salesforce/BLIP/blob/main/train_vqa.py\
    \ and try to use the HF model or use their model, then convert it to HF version\
    \ using the conversion script here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py"
  created_at: 2023-12-07 09:06:28+00:00
  edited: false
  hidden: false
  id: 65718b1476ef4bd9c59f3ec5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: Salesforce/blip-image-captioning-large
repo_type: model
status: open
target_branch: null
title: Is finetuning supported for this model? If so can people give me some pointers?
