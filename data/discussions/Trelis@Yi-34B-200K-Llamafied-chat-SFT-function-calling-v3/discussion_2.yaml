!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gunpal5
conflicting_files: null
created_at: 2023-12-12 11:54:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b57774642490c7a9174377e03242fbf0.svg
      fullname: Gunpal Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gunpal5
      type: user
    createdAt: '2023-12-12T11:54:56.000Z'
    data:
      edited: false
      editors:
      - gunpal5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9539163708686829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b57774642490c7a9174377e03242fbf0.svg
          fullname: Gunpal Jain
          isHf: false
          isPro: false
          name: gunpal5
          type: user
        html: '<p>Hello,<br>I have few questions before I purchase this.</p>

          <ol>

          <li>How can I use this in production environment? ScallLLM doesn''t support
          function calls.</li>

          <li>How much GPU memory will I need to run it properly?</li>

          <li>what will be the average time of responses on minimum hardware?</li>

          </ol>

          '
        raw: "Hello,\r\nI have few questions before I purchase this.\r\n\r\n1) How\
          \ can I use this in production environment? ScallLLM doesn't support function\
          \ calls.\r\n2) How much GPU memory will I need to run it properly?\r\n3)\
          \ what will be the average time of responses on minimum hardware?"
        updatedAt: '2023-12-12T11:54:56.521Z'
      numEdits: 0
      reactions: []
    id: 65784a1017166d821e16e031
    type: comment
  author: gunpal5
  content: "Hello,\r\nI have few questions before I purchase this.\r\n\r\n1) How can\
    \ I use this in production environment? ScallLLM doesn't support function calls.\r\
    \n2) How much GPU memory will I need to run it properly?\r\n3) what will be the\
    \ average time of responses on minimum hardware?"
  created_at: 2023-12-12 11:54:56+00:00
  edited: false
  hidden: false
  id: 65784a1017166d821e16e031
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-12T13:36:13.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8837671279907227
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy.</p>

          <ol>

          <li>Can you be specific about what kind of environment you want to do production
          with?</li>

          </ol>

          <ul>

          <li>You can, for example, deploy the model using TGI. There is a one-click
          runpod template in the model card.</li>

          <li>In terms of making calls to the api, you can use tokenizer.apply_chat_template
          (see model card) and manually build the code to handle functions OR you
          can purchase out-of-the-box scripts from <a rel="nofollow" href="https://trelis.com/function-calling/">here</a>.</li>

          </ul>

          <ol start="2">

          <li>GPU memory.</li>

          </ol>

          <ul>

          <li>This depends on what context length you are using.</li>

          <li>The smallest you can run on is using bitsandbytes-nf4 quantization (like
          the one click runpod template) and then you''ll need a minimum of about
          20 GB of VRAM.</li>

          </ul>

          <ol start="3">

          <li>Response times</li>

          </ol>

          <ul>

          <li>For short inputs and responses on an A6000 (which is probably overkill
          as it has 48 GB VRAM) I''m getting about 25 toks for short inputs.</li>

          </ul>

          '
        raw: 'Howdy.


          1. Can you be specific about what kind of environment you want to do production
          with?

          - You can, for example, deploy the model using TGI. There is a one-click
          runpod template in the model card.

          - In terms of making calls to the api, you can use tokenizer.apply_chat_template
          (see model card) and manually build the code to handle functions OR you
          can purchase out-of-the-box scripts from [here](https://trelis.com/function-calling/).


          2. GPU memory.

          - This depends on what context length you are using.

          - The smallest you can run on is using bitsandbytes-nf4 quantization (like
          the one click runpod template) and then you''ll need a minimum of about
          20 GB of VRAM.


          3. Response times

          - For short inputs and responses on an A6000 (which is probably overkill
          as it has 48 GB VRAM) I''m getting about 25 toks for short inputs.


          '
        updatedAt: '2023-12-12T13:36:13.838Z'
      numEdits: 0
      reactions: []
    id: 657861cddddc2360b017d512
    type: comment
  author: RonanMcGovern
  content: 'Howdy.


    1. Can you be specific about what kind of environment you want to do production
    with?

    - You can, for example, deploy the model using TGI. There is a one-click runpod
    template in the model card.

    - In terms of making calls to the api, you can use tokenizer.apply_chat_template
    (see model card) and manually build the code to handle functions OR you can purchase
    out-of-the-box scripts from [here](https://trelis.com/function-calling/).


    2. GPU memory.

    - This depends on what context length you are using.

    - The smallest you can run on is using bitsandbytes-nf4 quantization (like the
    one click runpod template) and then you''ll need a minimum of about 20 GB of VRAM.


    3. Response times

    - For short inputs and responses on an A6000 (which is probably overkill as it
    has 48 GB VRAM) I''m getting about 25 toks for short inputs.


    '
  created_at: 2023-12-12 13:36:13+00:00
  edited: false
  hidden: false
  id: 657861cddddc2360b017d512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b57774642490c7a9174377e03242fbf0.svg
      fullname: Gunpal Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gunpal5
      type: user
    createdAt: '2023-12-12T18:43:16.000Z'
    data:
      edited: true
      editors:
      - gunpal5
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b57774642490c7a9174377e03242fbf0.svg
          fullname: Gunpal Jain
          isHf: false
          isPro: false
          name: gunpal5
          type: user
        html: '<p>I am good, and you?</p>

          <ol>

          <li>I want to deploy it on a dedicated server. Hugging Face is too expensive
          to host.</li>

          <li>How can Yi-38b which is a ~80GB model run on 20GB vRAM? we don''t want
          to compromise on quality. We''ll probably have up to 100 people using any
          any given time.</li>

          <li>Input will be large, for example it will need to summarize or reinterpret
          a blog article.</li>

          </ol>

          <p>I''ve been experimenting on Yi-6b on my laptop with 12GB RTX 4080. its
          struggling to give output in reasonable amount of time.</p>

          '
        raw: 'I am good, and you?


          1) I want to deploy it on a dedicated server. Hugging Face is too expensive
          to host.

          2) How can Yi-38b which is a ~80GB model run on 20GB vRAM? we don''t want
          to compromise on quality. We''ll probably have up to 100 people using any
          any given time.

          3) Input will be large, for example it will need to summarize or reinterpret
          a blog article.


          I''ve been experimenting on Yi-6b on my laptop with 12GB RTX 4080. its struggling
          to give output in reasonable amount of time.'
        updatedAt: '2023-12-12T18:47:09.288Z'
      numEdits: 1
      reactions: []
    id: 6578a9c481923298f927c2cf
    type: comment
  author: gunpal5
  content: 'I am good, and you?


    1) I want to deploy it on a dedicated server. Hugging Face is too expensive to
    host.

    2) How can Yi-38b which is a ~80GB model run on 20GB vRAM? we don''t want to compromise
    on quality. We''ll probably have up to 100 people using any any given time.

    3) Input will be large, for example it will need to summarize or reinterpret a
    blog article.


    I''ve been experimenting on Yi-6b on my laptop with 12GB RTX 4080. its struggling
    to give output in reasonable amount of time.'
  created_at: 2023-12-12 18:43:16+00:00
  edited: true
  hidden: false
  id: 6578a9c481923298f927c2cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-12T23:17:49.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9228405356407166
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>How long are your inputs in number of tokens?</p>\n<p>I\u2019ll\
          \ have a video out soon on hosting.</p>\n<p>Bitsandbytes-NF4 or AWQ quality\
          \ is good and will get Yi from about 70 down to about 20 GB. </p>\n"
        raw: "How long are your inputs in number of tokens?\n\nI\u2019ll have a video\
          \ out soon on hosting.\n\nBitsandbytes-NF4 or AWQ quality is good and will\
          \ get Yi from about 70 down to about 20 GB. "
        updatedAt: '2023-12-12T23:17:49.945Z'
      numEdits: 0
      reactions: []
    id: 6578ea1df7d98f6f613201f2
    type: comment
  author: RonanMcGovern
  content: "How long are your inputs in number of tokens?\n\nI\u2019ll have a video\
    \ out soon on hosting.\n\nBitsandbytes-NF4 or AWQ quality is good and will get\
    \ Yi from about 70 down to about 20 GB. "
  created_at: 2023-12-12 23:17:49+00:00
  edited: false
  hidden: false
  id: 6578ea1df7d98f6f613201f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-05T15:18:55.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7635434865951538
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Here''s the video to help: <a rel="nofollow" href="https://www.youtube.com/watch?v=1TU9ZrZhqw0">https://www.youtube.com/watch?v=1TU9ZrZhqw0</a></p>

          '
        raw: 'Here''s the video to help: https://www.youtube.com/watch?v=1TU9ZrZhqw0'
        updatedAt: '2024-01-05T15:18:55.184Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65981ddfb1a78672696c150f
    id: 65981ddfb1a78672696c150a
    type: comment
  author: RonanMcGovern
  content: 'Here''s the video to help: https://www.youtube.com/watch?v=1TU9ZrZhqw0'
  created_at: 2024-01-05 15:18:55+00:00
  edited: false
  hidden: false
  id: 65981ddfb1a78672696c150a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-05T15:18:55.000Z'
    data:
      status: closed
    id: 65981ddfb1a78672696c150f
    type: status-change
  author: RonanMcGovern
  created_at: 2024-01-05 15:18:55+00:00
  id: 65981ddfb1a78672696c150f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v3
repo_type: model
status: closed
target_branch: null
title: Few Questions
