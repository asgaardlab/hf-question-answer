!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yiouyou
conflicting_files: null
created_at: 2023-12-09 06:27:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-09T06:27:17.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9656912684440613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>I bought this model, but how could I get the GPTQ (gptq-4bit-32g-actorder_True)
          version of this model, could you help me?</p>

          <p>Thanks,<br>Zack</p>

          '
        raw: "I bought this model, but how could I get the GPTQ (gptq-4bit-32g-actorder_True)\
          \ version of this model, could you help me?\r\n\r\nThanks,\r\nZack"
        updatedAt: '2023-12-09T06:27:17.142Z'
      numEdits: 0
      reactions: []
    id: 657408c5ca03b6c514ab58e2
    type: comment
  author: yiouyou
  content: "I bought this model, but how could I get the GPTQ (gptq-4bit-32g-actorder_True)\
    \ version of this model, could you help me?\r\n\r\nThanks,\r\nZack"
  created_at: 2023-12-09 06:27:17+00:00
  edited: false
  hidden: false
  id: 657408c5ca03b6c514ab58e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-09T06:27:42.000Z'
    data:
      from: May
      to: need help to generate the GPTQ version
    id: 657408deeb4b4e0bcffd1cfa
    type: title-change
  author: yiouyou
  created_at: 2023-12-09 06:27:42+00:00
  id: 657408deeb4b4e0bcffd1cfa
  new_title: need help to generate the GPTQ version
  old_title: May
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-09T15:18:25.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761595726013184
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy, making the GPTQ now, should be up shortly.</p>

          '
        raw: Howdy, making the GPTQ now, should be up shortly.
        updatedAt: '2023-12-09T15:18:25.183Z'
      numEdits: 0
      reactions: []
    id: 65748541244aefdfc48e475a
    type: comment
  author: RonanMcGovern
  content: Howdy, making the GPTQ now, should be up shortly.
  created_at: 2023-12-09 15:18:25+00:00
  edited: false
  hidden: false
  id: 65748541244aefdfc48e475a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-09T15:47:39.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7632736563682556
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>Thanks, please consider use the setting as below:</p>

          <p>quantize_config = BaseQuantizeConfig(<br>    bits=4,<br>    group_size=32,<br>    desc_act=True,<br>)</p>

          '
        raw: "Thanks, please consider use the setting as below:\n\nquantize_config\
          \ = BaseQuantizeConfig(\n    bits=4,\n    group_size=32,\n    desc_act=True,\n\
          )"
        updatedAt: '2023-12-09T15:47:39.631Z'
      numEdits: 0
      reactions: []
    id: 65748c1b244aefdfc48fbdb5
    type: comment
  author: yiouyou
  content: "Thanks, please consider use the setting as below:\n\nquantize_config =\
    \ BaseQuantizeConfig(\n    bits=4,\n    group_size=32,\n    desc_act=True,\n)"
  created_at: 2023-12-09 15:47:39+00:00
  edited: false
  hidden: false
  id: 65748c1b244aefdfc48fbdb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-09T19:15:00.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9409357309341431
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;yiouyou&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/yiouyou\">@<span class=\"\
          underline\">yiouyou</span></a></span>\n\n\t</span></span> , bad news, I\
          \ tried twice over 5 hours and failed to get the quant made.</p>\n<p>I'll\
          \ see if I have time on Monday to get to it.</p>\n<p>BTW, you have access\
          \ to the GPTQ quantization script in the ADVANCED-fine-tuning repo (see\
          \ the quant branch). In principle, it should take about an hour to run on\
          \ an A100.</p>\n<p>Also, what are you using for inference? AWQ is faster\
          \ and more accurate than GPTQ - and also easier for me to make if you need\
          \ it. It will work with vLLM or TGI provided you're using an A100 or A6000\
          \ or H100.</p>\n"
        raw: '@yiouyou , bad news, I tried twice over 5 hours and failed to get the
          quant made.


          I''ll see if I have time on Monday to get to it.


          BTW, you have access to the GPTQ quantization script in the ADVANCED-fine-tuning
          repo (see the quant branch). In principle, it should take about an hour
          to run on an A100.


          Also, what are you using for inference? AWQ is faster and more accurate
          than GPTQ - and also easier for me to make if you need it. It will work
          with vLLM or TGI provided you''re using an A100 or A6000 or H100.'
        updatedAt: '2023-12-09T22:42:45.604Z'
      numEdits: 1
      reactions: []
    id: 6574bcb44fffc3f08b4b2cb8
    type: comment
  author: RonanMcGovern
  content: '@yiouyou , bad news, I tried twice over 5 hours and failed to get the
    quant made.


    I''ll see if I have time on Monday to get to it.


    BTW, you have access to the GPTQ quantization script in the ADVANCED-fine-tuning
    repo (see the quant branch). In principle, it should take about an hour to run
    on an A100.


    Also, what are you using for inference? AWQ is faster and more accurate than GPTQ
    - and also easier for me to make if you need it. It will work with vLLM or TGI
    provided you''re using an A100 or A6000 or H100.'
  created_at: 2023-12-09 19:15:00+00:00
  edited: true
  hidden: false
  id: 6574bcb44fffc3f08b4b2cb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-10T09:05:03.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723636507987976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>I''m using A100 40G to run the model. I''ve tried to quantize the
          model with both AWQ and GPTQ scripts on a runpod A100 80G, but none of them
          finished in 3 hours. The jupyter code block seems running, no errors, but
          nothing happend for a very long time. I have to stop the runpod. </p>

          '
        raw: 'I''m using A100 40G to run the model. I''ve tried to quantize the model
          with both AWQ and GPTQ scripts on a runpod A100 80G, but none of them finished
          in 3 hours. The jupyter code block seems running, no errors, but nothing
          happend for a very long time. I have to stop the runpod. '
        updatedAt: '2023-12-10T09:05:03.105Z'
      numEdits: 0
      reactions: []
    id: 65757f3f769f3ee9bd430cff
    type: comment
  author: yiouyou
  content: 'I''m using A100 40G to run the model. I''ve tried to quantize the model
    with both AWQ and GPTQ scripts on a runpod A100 80G, but none of them finished
    in 3 hours. The jupyter code block seems running, no errors, but nothing happend
    for a very long time. I have to stop the runpod. '
  created_at: 2023-12-10 09:05:03+00:00
  edited: false
  hidden: false
  id: 65757f3f769f3ee9bd430cff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-10T11:42:26.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9097521901130676
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ How long will it take to generate a AWQ or GPTQ for a 34B model? Hope\
          \ you can generate them successfully when you have time.</p>\n<p>Thanks,</p>\n"
        raw: '@RonanMcGovern How long will it take to generate a AWQ or GPTQ for a
          34B model? Hope you can generate them successfully when you have time.


          Thanks,'
        updatedAt: '2023-12-10T11:42:26.392Z'
      numEdits: 0
      reactions: []
    id: 6575a422f9898ed3ab8f84de
    type: comment
  author: yiouyou
  content: '@RonanMcGovern How long will it take to generate a AWQ or GPTQ for a 34B
    model? Hope you can generate them successfully when you have time.


    Thanks,'
  created_at: 2023-12-10 11:42:26+00:00
  edited: false
  hidden: false
  id: 6575a422f9898ed3ab8f84de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-10T14:52:43.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9505314826965332
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;yiouyou&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yiouyou\"\
          >@<span class=\"underline\">yiouyou</span></a></span>\n\n\t</span></span>\
          \ , the AWQ version is live now on the AWQ branch. Unfortunately, TGI currently\
          \ is having an issue with AWQ, hopefully will get resolved soon: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/text-generation-inference/issues/1322\"\
          >https://github.com/huggingface/text-generation-inference/issues/1322</a></p>\n\
          <p>I believe the issue that we faced in making the AWQ was that in tokenizer_config,\
          \ the AutoTokenizer was set to the original model, not the llamafied version.</p>\n\
          <p>I'm running the GPTQ now, let's see if that fix helps there too.</p>\n"
        raw: 'Howdy @yiouyou , the AWQ version is live now on the AWQ branch. Unfortunately,
          TGI currently is having an issue with AWQ, hopefully will get resolved soon:
          https://github.com/huggingface/text-generation-inference/issues/1322


          I believe the issue that we faced in making the AWQ was that in tokenizer_config,
          the AutoTokenizer was set to the original model, not the llamafied version.


          I''m running the GPTQ now, let''s see if that fix helps there too.'
        updatedAt: '2023-12-10T14:52:43.980Z'
      numEdits: 0
      reactions: []
    id: 6575d0bbb951d40e7a551192
    type: comment
  author: RonanMcGovern
  content: 'Howdy @yiouyou , the AWQ version is live now on the AWQ branch. Unfortunately,
    TGI currently is having an issue with AWQ, hopefully will get resolved soon: https://github.com/huggingface/text-generation-inference/issues/1322


    I believe the issue that we faced in making the AWQ was that in tokenizer_config,
    the AutoTokenizer was set to the original model, not the llamafied version.


    I''m running the GPTQ now, let''s see if that fix helps there too.'
  created_at: 2023-12-10 14:52:43+00:00
  edited: false
  hidden: false
  id: 6575d0bbb951d40e7a551192
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-10T15:32:29.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8856794238090515
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>BTW, the issue with GPTQ quanting is slow packing. The fix is <a
          rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues/439#issuecomment-1820510399">here</a>.
          Hopefully I''ll have the GPTQ ready in an hour or two.</p>

          '
        raw: BTW, the issue with GPTQ quanting is slow packing. The fix is [here](https://github.com/PanQiWei/AutoGPTQ/issues/439#issuecomment-1820510399).
          Hopefully I'll have the GPTQ ready in an hour or two.
        updatedAt: '2023-12-10T15:32:29.369Z'
      numEdits: 0
      reactions: []
    id: 6575da0d9091da7dc66c1931
    type: comment
  author: RonanMcGovern
  content: BTW, the issue with GPTQ quanting is slow packing. The fix is [here](https://github.com/PanQiWei/AutoGPTQ/issues/439#issuecomment-1820510399).
    Hopefully I'll have the GPTQ ready in an hour or two.
  created_at: 2023-12-10 15:32:29+00:00
  edited: false
  hidden: false
  id: 6575da0d9091da7dc66c1931
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-10T17:17:04.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8827123641967773
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Ok, I've just pushed a GPTQ model. Unfortunately, it's the default\
          \ of:</p>\n<pre><code>quantize_config = BaseQuantizeConfig(\n    bits=4,\
          \  # quantize model to 4-bit\n    group_size=128,  # it is recommended to\
          \ set the value to 128\n    desc_act=False,  # desc_act and group size only\
          \ works on triton\n)\n</code></pre>\n<p>FWIW I've updated the GPTQ quantization\
          \ script in ADVANCED-fine-tuning and it runs in about 1 hour and 10 mins\
          \ on an A100.</p>\n<p>LMK if you absolutely need the 32g, True configuration.</p>\n"
        raw: "Ok, I've just pushed a GPTQ model. Unfortunately, it's the default of:\n\
          ```\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize model\
          \ to 4-bit\n    group_size=128,  # it is recommended to set the value to\
          \ 128\n    desc_act=False,  # desc_act and group size only works on triton\n\
          )\n```\n\nFWIW I've updated the GPTQ quantization script in ADVANCED-fine-tuning\
          \ and it runs in about 1 hour and 10 mins on an A100.\n\nLMK if you absolutely\
          \ need the 32g, True configuration."
        updatedAt: '2023-12-10T17:17:04.443Z'
      numEdits: 0
      reactions: []
    id: 6575f29012ae60c542bcefaf
    type: comment
  author: RonanMcGovern
  content: "Ok, I've just pushed a GPTQ model. Unfortunately, it's the default of:\n\
    ```\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize model to 4-bit\n\
    \    group_size=128,  # it is recommended to set the value to 128\n    desc_act=False,\
    \  # desc_act and group size only works on triton\n)\n```\n\nFWIW I've updated\
    \ the GPTQ quantization script in ADVANCED-fine-tuning and it runs in about 1\
    \ hour and 10 mins on an A100.\n\nLMK if you absolutely need the 32g, True configuration."
  created_at: 2023-12-10 17:17:04+00:00
  edited: false
  hidden: false
  id: 6575f29012ae60c542bcefaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-11T07:04:57.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6902608275413513
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ \"traindataset, testenc = get_wikitext2(128, 0, 2048, pretrained_model_dir)\"\
          \ this line taks too much time to run, do you have the same situation?</p>\n"
        raw: '@RonanMcGovern "traindataset, testenc = get_wikitext2(128, 0, 2048,
          pretrained_model_dir)" this line taks too much time to run, do you have
          the same situation?'
        updatedAt: '2023-12-11T07:04:57.468Z'
      numEdits: 0
      reactions: []
    id: 6576b499839aa088995ac5bf
    type: comment
  author: yiouyou
  content: '@RonanMcGovern "traindataset, testenc = get_wikitext2(128, 0, 2048, pretrained_model_dir)"
    this line taks too much time to run, do you have the same situation?'
  created_at: 2023-12-11 07:04:57+00:00
  edited: false
  hidden: false
  id: 6576b499839aa088995ac5bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-11T09:53:10.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9506233334541321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ I've tried AWQ script, it does generated all most required files successfully,\
          \ but without 'tokenizer.model' file. Do you know why is that? I started\
          \ a issue on repo of ADVANCED.</p>\n"
        raw: '@RonanMcGovern I''ve tried AWQ script, it does generated all most required
          files successfully, but without ''tokenizer.model'' file. Do you know why
          is that? I started a issue on repo of ADVANCED.'
        updatedAt: '2023-12-11T09:53:10.411Z'
      numEdits: 0
      reactions: []
    id: 6576dc06b4379e65a8dcf3db
    type: comment
  author: yiouyou
  content: '@RonanMcGovern I''ve tried AWQ script, it does generated all most required
    files successfully, but without ''tokenizer.model'' file. Do you know why is that?
    I started a issue on repo of ADVANCED.'
  created_at: 2023-12-11 09:53:10+00:00
  edited: false
  hidden: false
  id: 6576dc06b4379e65a8dcf3db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-11T10:19:27.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9476125836372375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ \"traindataset, testenc = get_wikitext2(128, 0, 2048, pretrained_model_dir)\"\
          \ this line taks too much time to run, do you have the same situation?</p>\n\
          </blockquote>\n<p>I don't have this issue. This line seems to execute quite\
          \ fast for me on an A100 runpod instance.</p>\n<blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span class=\"underline\"\
          >RonanMcGovern</span></a></span>\n\n\t</span></span> I've tried AWQ script,\
          \ it does generated all most required files successfully, but without 'tokenizer.model'\
          \ file. Do you know why is that? I started a issue on repo of ADVANCED.</p>\n\
          </blockquote>\n<p>AWQ (or GPTQ) don't generate the tokenizer.model file.\
          \ It is added by the repo owner on the base repo and then sometimes needs\
          \ to be manually copied across. The need for tokenizer.model is causing\
          \ issues in general with quantization because GGUF and GPTQ require it but\
          \ some newer models don't have it. See here: <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/llama.cpp/pull/3633#issuecomment-1849031538\"\
          >https://github.com/ggerganov/llama.cpp/pull/3633#issuecomment-1849031538</a>\
          \ . Hopefully this will get fixed soon, but the issue has been there for\
          \ a few weeks. In the meantime, manually copying often works.</p>\n"
        raw: '> @RonanMcGovern "traindataset, testenc = get_wikitext2(128, 0, 2048,
          pretrained_model_dir)" this line taks too much time to run, do you have
          the same situation?


          I don''t have this issue. This line seems to execute quite fast for me on
          an A100 runpod instance.


          > @RonanMcGovern I''ve tried AWQ script, it does generated all most required
          files successfully, but without ''tokenizer.model'' file. Do you know why
          is that? I started a issue on repo of ADVANCED.


          AWQ (or GPTQ) don''t generate the tokenizer.model file. It is added by the
          repo owner on the base repo and then sometimes needs to be manually copied
          across. The need for tokenizer.model is causing issues in general with quantization
          because GGUF and GPTQ require it but some newer models don''t have it. See
          here: https://github.com/ggerganov/llama.cpp/pull/3633#issuecomment-1849031538
          . Hopefully this will get fixed soon, but the issue has been there for a
          few weeks. In the meantime, manually copying often works.'
        updatedAt: '2023-12-11T10:19:27.533Z'
      numEdits: 0
      reactions: []
    id: 6576e22f39831c6862f2216c
    type: comment
  author: RonanMcGovern
  content: '> @RonanMcGovern "traindataset, testenc = get_wikitext2(128, 0, 2048,
    pretrained_model_dir)" this line taks too much time to run, do you have the same
    situation?


    I don''t have this issue. This line seems to execute quite fast for me on an A100
    runpod instance.


    > @RonanMcGovern I''ve tried AWQ script, it does generated all most required files
    successfully, but without ''tokenizer.model'' file. Do you know why is that? I
    started a issue on repo of ADVANCED.


    AWQ (or GPTQ) don''t generate the tokenizer.model file. It is added by the repo
    owner on the base repo and then sometimes needs to be manually copied across.
    The need for tokenizer.model is causing issues in general with quantization because
    GGUF and GPTQ require it but some newer models don''t have it. See here: https://github.com/ggerganov/llama.cpp/pull/3633#issuecomment-1849031538
    . Hopefully this will get fixed soon, but the issue has been there for a few weeks.
    In the meantime, manually copying often works.'
  created_at: 2023-12-11 10:19:27+00:00
  edited: false
  hidden: false
  id: 6576e22f39831c6862f2216c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-12-11T11:47:07.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.832460343837738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ When AWQ, is this msg \"Token indices sequence length is longer than the\
          \ specified maximum sequence length for this model (8948 &gt; 4096). Running\
          \ this sequence through the model will result in indexing errors\" a concern\
          \ for you? If it is, how to fix it? Thanks~</p>\n"
        raw: '@RonanMcGovern When AWQ, is this msg "Token indices sequence length
          is longer than the specified maximum sequence length for this model (8948
          > 4096). Running this sequence through the model will result in indexing
          errors" a concern for you? If it is, how to fix it? Thanks~'
        updatedAt: '2023-12-11T11:47:07.768Z'
      numEdits: 0
      reactions: []
    id: 6576f6bbf9cd531693cec591
    type: comment
  author: yiouyou
  content: '@RonanMcGovern When AWQ, is this msg "Token indices sequence length is
    longer than the specified maximum sequence length for this model (8948 > 4096).
    Running this sequence through the model will result in indexing errors" a concern
    for you? If it is, how to fix it? Thanks~'
  created_at: 2023-12-11 11:47:07+00:00
  edited: false
  hidden: false
  id: 6576f6bbf9cd531693cec591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-12T11:08:56.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363986849784851
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ When AWQ, is this msg \"Token indices sequence length is longer than the\
          \ specified maximum sequence length for this model (8948 &gt; 4096). Running\
          \ this sequence through the model will result in indexing errors\" a concern\
          \ for you? If it is, how to fix it? Thanks~</p>\n</blockquote>\n<p>I don't\
          \ believe this is an issue. It didn't stop me quantizing.</p>\n<p>BTW, I\
          \ spent a few hours today trying to get AWQ and GPTQ to work. I'm puzzled\
          \ because the current model doesn't work, but <a href=\"https://huggingface.co/Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ\"\
          >this v2 model does work running on tgi</a>! I've given you access, let\
          \ me know if you see any inconsistencies.</p>\n"
        raw: '> @RonanMcGovern When AWQ, is this msg "Token indices sequence length
          is longer than the specified maximum sequence length for this model (8948
          > 4096). Running this sequence through the model will result in indexing
          errors" a concern for you? If it is, how to fix it? Thanks~


          I don''t believe this is an issue. It didn''t stop me quantizing.


          BTW, I spent a few hours today trying to get AWQ and GPTQ to work. I''m
          puzzled because the current model doesn''t work, but [this v2 model does
          work running on tgi](https://huggingface.co/Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ)!
          I''ve given you access, let me know if you see any inconsistencies.

          '
        updatedAt: '2023-12-12T11:08:56.005Z'
      numEdits: 0
      reactions: []
    id: 65783f487f5da1deb67a09f8
    type: comment
  author: RonanMcGovern
  content: '> @RonanMcGovern When AWQ, is this msg "Token indices sequence length
    is longer than the specified maximum sequence length for this model (8948 > 4096).
    Running this sequence through the model will result in indexing errors" a concern
    for you? If it is, how to fix it? Thanks~


    I don''t believe this is an issue. It didn''t stop me quantizing.


    BTW, I spent a few hours today trying to get AWQ and GPTQ to work. I''m puzzled
    because the current model doesn''t work, but [this v2 model does work running
    on tgi](https://huggingface.co/Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ)!
    I''ve given you access, let me know if you see any inconsistencies.

    '
  created_at: 2023-12-12 11:08:56+00:00
  edited: false
  hidden: false
  id: 65783f487f5da1deb67a09f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-05T15:20:51.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9154106378555298
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>FYI, the SUSChat function calling model now has an AWQ branch and
          runpod vLLM template. SUSChat is a fine-tuned version of Yi.</p>

          <p>Reach out by commenting further here if anyone needs an AWQ of this Yi
          model (for now, running with TGI and eetq is the recommended way to inference
          this Yi model.</p>

          '
        raw: 'FYI, the SUSChat function calling model now has an AWQ branch and runpod
          vLLM template. SUSChat is a fine-tuned version of Yi.


          Reach out by commenting further here if anyone needs an AWQ of this Yi model
          (for now, running with TGI and eetq is the recommended way to inference
          this Yi model.'
        updatedAt: '2024-01-05T15:20:51.866Z'
      numEdits: 0
      reactions: []
    id: 65981e53ada2ade50bc3c29f
    type: comment
  author: RonanMcGovern
  content: 'FYI, the SUSChat function calling model now has an AWQ branch and runpod
    vLLM template. SUSChat is a fine-tuned version of Yi.


    Reach out by commenting further here if anyone needs an AWQ of this Yi model (for
    now, running with TGI and eetq is the recommended way to inference this Yi model.'
  created_at: 2024-01-05 15:20:51+00:00
  edited: false
  hidden: false
  id: 65981e53ada2ade50bc3c29f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v3
repo_type: model
status: open
target_branch: null
title: need help to generate the GPTQ version
