!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abhishek3jangid
conflicting_files: null
created_at: 2024-01-15 06:19:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abbd0aef8c29519ecb7756bf261a8733.svg
      fullname: Abhishek Jangid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhishek3jangid
      type: user
    createdAt: '2024-01-15T06:19:10.000Z'
    data:
      edited: true
      editors:
      - abhishek3jangid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5469666719436646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abbd0aef8c29519ecb7756bf261a8733.svg
          fullname: Abhishek Jangid
          isHf: false
          isPro: false
          name: abhishek3jangid
          type: user
        html: '<p>Command: docker run -d --gpus all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
          -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.0 --model-id
          TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --port 3000 --quantize awq --max-input-length
          3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096</p>

          <p>Error:<br>2024-01-15T06:12:44.273487Z  INFO text_generation_launcher:
          Args { model_id: "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ", revision: None,
          validation_workers: 2, sharded: None, num_shard: None, quantize: Some(Awq),
          dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:
          2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 3696, max_total_tokens:
          4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:
          None, max_waiting_tokens: 20, hostname: "e2a73fd51cfe", port: 3000, shard_uds_path:
          "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500,
          huggingface_hub_cache: Some("/data"), weights_cache_override: None, disable_custom_kernels:
          false, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None,
          json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:
          None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge:
          None, env: false }<br>2024-01-15T06:12:44.273622Z  INFO download: text_generation_launcher:
          Starting download process.<br>2024-01-15T06:12:48.305137Z  INFO text_generation_launcher:
          Files are already present on the host. Skipping download.</p>

          <p>2024-01-15T06:12:48.878449Z  INFO download: text_generation_launcher:
          Successfully downloaded weights.<br>2024-01-15T06:12:48.878703Z  INFO shard-manager:
          text_generation_launcher: Starting shard rank=0<br>2024-01-15T06:12:52.595232Z
          ERROR text_generation_launcher: Error when initializing model<br>Traceback
          (most recent call last):<br>  File "/opt/conda/bin/text-generation-server",
          line 8, in <br>    sys.exit(app())<br>  File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in <strong>call</strong><br>    return get_command(self)(*args,
          **kwargs)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1157, in <strong>call</strong><br>    return self.main(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/core.py", line 778, in main<br>    return
          _main(<br>  File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main<br>    rv = self.invoke(ctx)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1688, in invoke<br>    return _process_result(sub_ctx.command.invoke(sub_ctx))<br>  File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1434, in invoke<br>    return
          ctx.invoke(self.callback, **ctx.params)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 783, in invoke<br>    return __callback(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/main.py", line 683, in wrapper<br>    return
          callback(**use_params)  # type: ignore<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve<br>    server.serve(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve<br>    asyncio.run(<br>  File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run<br>    return loop.run_until_complete(main)<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete<br>    self.run_forever()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever<br>    self._run_once()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once<br>    handle._run()<br>  File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run<br>    self._context.run(self._callback, *self._args)</p>

          <blockquote>

          <p>File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 159, in serve_inner<br>    model = get_model(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 297, in get_model<br>    raise ValueError("awq quantization is not
          supported for AutoModel")<br>ValueError: awq quantization is not supported
          for AutoModel</p>

          </blockquote>

          <p>2024-01-15T06:12:53.184670Z ERROR shard-manager: text_generation_launcher:
          Shard complete standard error output:</p>

          <p>Traceback (most recent call last):</p>

          <p>  File "/opt/conda/bin/text-generation-server", line 8, in <br>    sys.exit(app())</p>

          <p>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve<br>    server.serve(</p>

          <p>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve<br>    asyncio.run(</p>

          <p>  File "/opt/conda/lib/python3.9/asyncio/runners.py", line 44, in run<br>    return
          loop.run_until_complete(main)</p>

          <p>  File "/opt/conda/lib/python3.9/asyncio/base_events.py", line 647, in
          run_until_complete<br>    return future.result()</p>

          <p>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 159, in serve_inner<br>    model = get_model(</p>

          <p>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 297, in get_model<br>    raise ValueError("awq quantization is not
          supported for AutoModel")</p>

          <p>ValueError: awq quantization is not supported for AutoModel<br> rank=0<br>Error:
          ShardCannotStart<br>2024-01-15T06:12:53.282399Z ERROR text_generation_launcher:
          Shard 0 failed to start<br>2024-01-15T06:12:53.282426Z  INFO text_generation_launcher:
          Shutting down shards</p>

          '
        raw: "Command: docker run -d --gpus all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN\
          \ -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.0 --model-id\
          \ TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --port 3000 --quantize awq --max-input-length\
          \ 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096\n\nError:\
          \ \n2024-01-15T06:12:44.273487Z  INFO text_generation_launcher: Args { model_id:\
          \ \"TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\", revision: None, validation_workers:\
          \ 2, sharded: None, num_shard: None, quantize: Some(Awq), dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_top_n_tokens: 5, max_input_length: 3696, max_total_tokens: 4096,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ None, max_waiting_tokens: 20, hostname: \"e2a73fd51cfe\", port: 3000,\
          \ shard_uds_path: \"/tmp/text-generation-server\", master_addr: \"localhost\"\
          , master_port: 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
          \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
          \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_edge: None, env: false }\n2024-01-15T06:12:44.273622Z  INFO\
          \ download: text_generation_launcher: Starting download process.\n2024-01-15T06:12:48.305137Z\
          \  INFO text_generation_launcher: Files are already present on the host.\
          \ Skipping download.\n\n2024-01-15T06:12:48.878449Z  INFO download: text_generation_launcher:\
          \ Successfully downloaded weights.\n2024-01-15T06:12:48.878703Z  INFO shard-manager:\
          \ text_generation_launcher: Starting shard rank=0\n2024-01-15T06:12:52.595232Z\
          \ ERROR text_generation_launcher: Error when initializing model\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          > File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 297, in get_model\n    raise ValueError(\"awq quantization is not\
          \ supported for AutoModel\")\nValueError: awq quantization is not supported\
          \ for AutoModel\n\n2024-01-15T06:12:53.184670Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\n\nTraceback (most recent call last):\n\
          \n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n\
          \    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 297, in get_model\n    raise ValueError(\"awq quantization is not\
          \ supported for AutoModel\")\n\nValueError: awq quantization is not supported\
          \ for AutoModel\n rank=0\nError: ShardCannotStart\n2024-01-15T06:12:53.282399Z\
          \ ERROR text_generation_launcher: Shard 0 failed to start\n2024-01-15T06:12:53.282426Z\
          \  INFO text_generation_launcher: Shutting down shards"
        updatedAt: '2024-01-15T08:20:34.217Z'
      numEdits: 1
      reactions: []
    id: 65a4ce5e680cb2eb94fe1d2b
    type: comment
  author: abhishek3jangid
  content: "Command: docker run -d --gpus all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN\
    \ -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.1.0 --model-id\
    \ TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --port 3000 --quantize awq --max-input-length\
    \ 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096\n\nError: \n2024-01-15T06:12:44.273487Z\
    \  INFO text_generation_launcher: Args { model_id: \"TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\"\
    , revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize:\
    \ Some(Awq), dtype: None, trust_remote_code: false, max_concurrent_requests: 128,\
    \ max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length:\
    \ 3696, max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens:\
    \ 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname: \"e2a73fd51cfe\"\
    , port: 3000, shard_uds_path: \"/tmp/text-generation-server\", master_addr: \"\
    localhost\", master_port: 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
    \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
    \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
    \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
    \ None, ngrok_edge: None, env: false }\n2024-01-15T06:12:44.273622Z  INFO download:\
    \ text_generation_launcher: Starting download process.\n2024-01-15T06:12:48.305137Z\
    \  INFO text_generation_launcher: Files are already present on the host. Skipping\
    \ download.\n\n2024-01-15T06:12:48.878449Z  INFO download: text_generation_launcher:\
    \ Successfully downloaded weights.\n2024-01-15T06:12:48.878703Z  INFO shard-manager:\
    \ text_generation_launcher: Starting shard rank=0\n2024-01-15T06:12:52.595232Z\
    \ ERROR text_generation_launcher: Error when initializing model\nTraceback (most\
    \ recent call last):\n  File \"/opt/conda/bin/text-generation-server\", line 8,\
    \ in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n\
    \    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in\
    \ invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line\
    \ 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n> File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 297, in get_model\n    raise ValueError(\"awq quantization is not supported\
    \ for AutoModel\")\nValueError: awq quantization is not supported for AutoModel\n\
    \n2024-01-15T06:12:53.184670Z ERROR shard-manager: text_generation_launcher: Shard\
    \ complete standard error output:\n\nTraceback (most recent call last):\n\n  File\
    \ \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n    sys.exit(app())\n\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 297, in get_model\n    raise ValueError(\"awq quantization is not supported\
    \ for AutoModel\")\n\nValueError: awq quantization is not supported for AutoModel\n\
    \ rank=0\nError: ShardCannotStart\n2024-01-15T06:12:53.282399Z ERROR text_generation_launcher:\
    \ Shard 0 failed to start\n2024-01-15T06:12:53.282426Z  INFO text_generation_launcher:\
    \ Shutting down shards"
  created_at: 2024-01-15 06:19:10+00:00
  edited: true
  hidden: false
  id: 65a4ce5e680cb2eb94fe1d2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ
repo_type: model
status: open
target_branch: null
title: Not supporting with TGI
