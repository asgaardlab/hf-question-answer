!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kreabs
conflicting_files: null
created_at: 2023-08-15 08:21:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbd9df25308b21195f398273ba1ab507.svg
      fullname: Tim Dees
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kreabs
      type: user
    createdAt: '2023-08-15T09:21:59.000Z'
    data:
      edited: false
      editors:
      - kreabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7433202266693115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbd9df25308b21195f398273ba1ab507.svg
          fullname: Tim Dees
          isHf: false
          isPro: false
          name: kreabs
          type: user
        html: '<p>I am following this tutorial by Phillip Schmid<br><a rel="nofollow"
          href="https://www.philschmid.de/fine-tune-flan-t5-peft">https://www.philschmid.de/fine-tune-flan-t5-peft</a></p>

          <p>But when i exchange the flan model with either :<br>flashvenom/mpt-7b-base-lora-fix
          or<br>mosaicml/mpt-7b</p>

          <p>it doesnt work.</p>

          <p>I adjusted these two lines:<br>model = AutoModelForCausalLM.from_pretrained(model_id,
          load_in_8bit=True, trust_remote_code=True)<br>and this before the preprocess_function:<br>tokenizer.pad_token
          = tokenizer.eos_token </p>

          <p>what else do i have to adjust or has somebody managed to let a script
          run?</p>

          <p>i''d be happy for any code example or explanation:</p>

          <p>Greetings<br>kreabs</p>

          '
        raw: "I am following this tutorial by Phillip Schmid\r\nhttps://www.philschmid.de/fine-tune-flan-t5-peft\r\
          \n\r\nBut when i exchange the flan model with either :\r\nflashvenom/mpt-7b-base-lora-fix\
          \ or \r\nmosaicml/mpt-7b\r\n\r\nit doesnt work.\r\n\r\nI adjusted these\
          \ two lines:\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True,\
          \ trust_remote_code=True)\r\nand this before the preprocess_function:\r\n\
          tokenizer.pad_token = tokenizer.eos_token \r\n\r\nwhat else do i have to\
          \ adjust or has somebody managed to let a script run?\r\n\r\ni'd be happy\
          \ for any code example or explanation:\r\n\r\nGreetings\r\nkreabs"
        updatedAt: '2023-08-15T09:21:59.774Z'
      numEdits: 0
      reactions: []
    id: 64db43b7a1182ee16c492087
    type: comment
  author: kreabs
  content: "I am following this tutorial by Phillip Schmid\r\nhttps://www.philschmid.de/fine-tune-flan-t5-peft\r\
    \n\r\nBut when i exchange the flan model with either :\r\nflashvenom/mpt-7b-base-lora-fix\
    \ or \r\nmosaicml/mpt-7b\r\n\r\nit doesnt work.\r\n\r\nI adjusted these two lines:\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, trust_remote_code=True)\r\
    \nand this before the preprocess_function:\r\ntokenizer.pad_token = tokenizer.eos_token\
    \ \r\n\r\nwhat else do i have to adjust or has somebody managed to let a script\
    \ run?\r\n\r\ni'd be happy for any code example or explanation:\r\n\r\nGreetings\r\
    \nkreabs"
  created_at: 2023-08-15 08:21:59+00:00
  edited: false
  hidden: false
  id: 64db43b7a1182ee16c492087
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flashvenom/mpt-7b-base-lora-fix
repo_type: model
status: open
target_branch: null
title: Running MPT-7B with LORA
