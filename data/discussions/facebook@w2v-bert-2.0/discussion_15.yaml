!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Imran1
conflicting_files: null
created_at: 2024-01-25 06:29:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2024-01-25T06:29:43.000Z'
    data:
      edited: true
      editors:
      - Imran1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38317984342575073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: "<p>loading the vocab file showing this </p>\n<pre><code>from transformers\
          \ import Wav2Vec2CTCTokenizer\n\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"\
          ./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"\
          |\")\n</code></pre>\n<p>error</p>\n<pre><code>TypeError                \
          \                 Traceback (most recent call last)\n&lt;ipython-input-26-1f25f0d516f8&gt;\
          \ in &lt;cell line: 3&gt;()\n      1 from transformers import Wav2Vec2CTCTokenizer\n\
          \      2 \n----&gt; 3 tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"\
          ./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"\
          |\")\n\n7 frames\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in added_tokens_encoder(self)\n    389         optimisation in `self._added_tokens_encoder`\
          \ for the slow tokenizers.\n    390         \"\"\"\n--&gt; 391         return\
          \ {k.content: v for v, k in sorted(self._added_tokens_decoder.items(), key=lambda\
          \ item: item[0])}\n    392 \n    393     @property\n\nTypeError: '&lt;'\
          \ not supported between instances of 'str' and 'int'\n</code></pre>\n"
        raw: "loading the vocab file showing this \n```\nfrom transformers import\
          \ Wav2Vec2CTCTokenizer\n\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"\
          ./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"\
          |\")\n```\n\n\nerror\n\n```\nTypeError                                 Traceback\
          \ (most recent call last)\n<ipython-input-26-1f25f0d516f8> in <cell line:\
          \ 3>()\n      1 from transformers import Wav2Vec2CTCTokenizer\n      2 \n\
          ----> 3 tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"\
          [UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n\n7 frames\n\
          /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in added_tokens_encoder(self)\n    389         optimisation in `self._added_tokens_encoder`\
          \ for the slow tokenizers.\n    390         \"\"\"\n--> 391         return\
          \ {k.content: v for v, k in sorted(self._added_tokens_decoder.items(), key=lambda\
          \ item: item[0])}\n    392 \n    393     @property\n\nTypeError: '<' not\
          \ supported between instances of 'str' and 'int'\n```"
        updatedAt: '2024-01-25T06:30:10.312Z'
      numEdits: 1
      reactions: []
    id: 65b1ffd704eec72abf098e06
    type: comment
  author: Imran1
  content: "loading the vocab file showing this \n```\nfrom transformers import Wav2Vec2CTCTokenizer\n\
    \ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\"\
    , pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n```\n\n\nerror\n\n```\nTypeError\
    \                                 Traceback (most recent call last)\n<ipython-input-26-1f25f0d516f8>\
    \ in <cell line: 3>()\n      1 from transformers import Wav2Vec2CTCTokenizer\n\
    \      2 \n----> 3 tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"\
    [UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n\n7 frames\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
    \ in added_tokens_encoder(self)\n    389         optimisation in `self._added_tokens_encoder`\
    \ for the slow tokenizers.\n    390         \"\"\"\n--> 391         return {k.content:\
    \ v for v, k in sorted(self._added_tokens_decoder.items(), key=lambda item: item[0])}\n\
    \    392 \n    393     @property\n\nTypeError: '<' not supported between instances\
    \ of 'str' and 'int'\n```"
  created_at: 2024-01-25 06:29:43+00:00
  edited: true
  hidden: false
  id: 65b1ffd704eec72abf098e06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2024-01-25T06:31:01.000Z'
    data:
      edited: false
      editors:
      - Imran1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.12900255620479584
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: "<p>here is my vocab dict</p>\n<pre><code>vocab_dict= {\n    0: ' ',\n\
          \    1: '\u0627',\n    2: '\u0622',\n    3: '\u0628',\n    4: '\u067E',\n\
          \    5: '\u062A',\n    6: '\u067C', \n    7: '\u062B',\n    8: '\u062C',\n\
          \    9: '\u0686',\n    10: '\u062D',\n    11: '\u062E',\n    12: '\u062F\
          ',\n    13: '\u0689',\n    14: '\u0681',\n    15: '\u0631',\n    16: '\u0693\
          ',\n    17: '\u0698',\n    18: '\u0633',\n    19: '\u0634',\n    20: '\u0635\
          ',\n    21: '\u0636',\n    22: '\u0637',\n    23: '\u0638',\n    24: '\u0639\
          ',\n    25: '\u063A',\n    26: '\u0641',\n    27: '\u0642',\n    28: '\u06A9\
          ',\n    29: '\u06AB',\n    30: '\u0644',\n    31: '\u0645',\n    32: '\u0646\
          ',\n    33: '\u06BC',\n    34: '\u0648',\n    35: '\u06C1',\n    36: '\u06BE\
          ',\n    37: '\u0621',\n    38: '\u0626',\n    39: '\u06CC',\n    40: '\u06D2\
          '\n}\n</code></pre>\n"
        raw: "here is my vocab dict\n```\nvocab_dict= {\n    0: ' ',\n    1: '\u0627\
          ',\n    2: '\u0622',\n    3: '\u0628',\n    4: '\u067E',\n    5: '\u062A\
          ',\n    6: '\u067C', \n    7: '\u062B',\n    8: '\u062C',\n    9: '\u0686\
          ',\n    10: '\u062D',\n    11: '\u062E',\n    12: '\u062F',\n    13: '\u0689\
          ',\n    14: '\u0681',\n    15: '\u0631',\n    16: '\u0693',\n    17: '\u0698\
          ',\n    18: '\u0633',\n    19: '\u0634',\n    20: '\u0635',\n    21: '\u0636\
          ',\n    22: '\u0637',\n    23: '\u0638',\n    24: '\u0639',\n    25: '\u063A\
          ',\n    26: '\u0641',\n    27: '\u0642',\n    28: '\u06A9',\n    29: '\u06AB\
          ',\n    30: '\u0644',\n    31: '\u0645',\n    32: '\u0646',\n    33: '\u06BC\
          ',\n    34: '\u0648',\n    35: '\u06C1',\n    36: '\u06BE',\n    37: '\u0621\
          ',\n    38: '\u0626',\n    39: '\u06CC',\n    40: '\u06D2'\n}\n```"
        updatedAt: '2024-01-25T06:31:01.306Z'
      numEdits: 0
      reactions: []
    id: 65b20025b0a5a381b6a3e76b
    type: comment
  author: Imran1
  content: "here is my vocab dict\n```\nvocab_dict= {\n    0: ' ',\n    1: '\u0627\
    ',\n    2: '\u0622',\n    3: '\u0628',\n    4: '\u067E',\n    5: '\u062A',\n \
    \   6: '\u067C', \n    7: '\u062B',\n    8: '\u062C',\n    9: '\u0686',\n    10:\
    \ '\u062D',\n    11: '\u062E',\n    12: '\u062F',\n    13: '\u0689',\n    14:\
    \ '\u0681',\n    15: '\u0631',\n    16: '\u0693',\n    17: '\u0698',\n    18:\
    \ '\u0633',\n    19: '\u0634',\n    20: '\u0635',\n    21: '\u0636',\n    22:\
    \ '\u0637',\n    23: '\u0638',\n    24: '\u0639',\n    25: '\u063A',\n    26:\
    \ '\u0641',\n    27: '\u0642',\n    28: '\u06A9',\n    29: '\u06AB',\n    30:\
    \ '\u0644',\n    31: '\u0645',\n    32: '\u0646',\n    33: '\u06BC',\n    34:\
    \ '\u0648',\n    35: '\u06C1',\n    36: '\u06BE',\n    37: '\u0621',\n    38:\
    \ '\u0626',\n    39: '\u06CC',\n    40: '\u06D2'\n}\n```"
  created_at: 2024-01-25 06:31:01+00:00
  edited: false
  hidden: false
  id: 65b20025b0a5a381b6a3e76b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: facebook/w2v-bert-2.0
repo_type: model
status: open
target_branch: null
title: tokenizer issues
