!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Renqing
conflicting_files: null
created_at: 2023-06-20 05:48:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
      fullname: Renqing Nie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renqing
      type: user
    createdAt: '2023-06-20T06:48:13.000Z'
    data:
      edited: true
      editors:
      - Renqing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8448465466499329
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
          fullname: Renqing Nie
          isHf: false
          isPro: false
          name: Renqing
          type: user
        html: "<p><em>|</em> First of all, congratulations to you.<br><em>|</em> \
          \ I saw in <code>InSilicoPerturber</code> function, parameter model_type\
          \  has three options<code> {\"Pretrained\",\"GeneClassifier\",\"CellClassifier\"\
          }</code>,<br><code>'Pretrained' </code> indicates that the pre-trained model\
          \ has not been fine-tuned downstream for specific task\uFF0C'GeneClassifier'\
          \ is the model in your manuscript for distinguishing <code>dose-sensitive\
          \ or insensitive</code>, and 'CellClassifier' is the model in your manuscript\
          \ for distinguishing<code> cardiomyocytes</code> from non-failing hearts\
          \  or hearts affected by hypertrophic or dilated cardiomyopathy?<br><em>|</em>\
          \  The parameters of this function <code>isp.perturb_data(\"path/to/model\"\
          , \"path/to/input_data\", \"path/to/output_directory\",\"output_prefix\"\
          )</code>, model and data must all be consistent with the function InSilicoPerturber,\
          \ right?<br>If I want to use these two functions <code> (\"GeneClassifier\"\
          , \"CellClassifier\")</code>, do I need to fine-tune the two models myself\
          \ with the sample data you provided? The model I cloned from huggingface\
          \ only contains the pre-trained model, right?<br><em>|</em>  If I use <code>silico\
          \ perturbation</code>, what is the result <code>difference between the three\
          \ models (\"Pretrained\",\"GeneClassifier\",\"CellClassifier\")</code>?<br>\
          \         Would you provide a notebook that reproduces the article?<br>\
          \        Looking forward to your reply</p>\n"
        raw: "_|_ First of all, congratulations to you.\n_|_  I saw in `InSilicoPerturber`\
          \ function, parameter model_type  has three options` {\"Pretrained\",\"\
          GeneClassifier\",\"CellClassifier\"}`,\n`'Pretrained' ` indicates that the\
          \ pre-trained model has not been fine-tuned downstream for specific task\uFF0C\
          'GeneClassifier' is the model in your manuscript for distinguishing `dose-sensitive\
          \ or insensitive`, and 'CellClassifier' is the model in your manuscript\
          \ for distinguishing` cardiomyocytes` from non-failing hearts  or hearts\
          \ affected by hypertrophic or dilated cardiomyopathy?\n_|_  The parameters\
          \ of this function `isp.perturb_data(\"path/to/model\", \"path/to/input_data\"\
          ,\n\"path/to/output_directory\",\"output_prefix\")`, model and data must\
          \ all be consistent with the function InSilicoPerturber, right?\nIf I want\
          \ to use these two functions ` (\"GeneClassifier\", \"CellClassifier\")`,\
          \ do I need to fine-tune the two models myself with the sample data you\
          \ provided? The model I cloned from huggingface only contains the pre-trained\
          \ model, right?\n_|_  If I use `silico perturbation`, what is the result\
          \ `difference between the three models (\"Pretrained\",\"GeneClassifier\"\
          ,\"CellClassifier\")`?\n         Would you provide a notebook that reproduces\
          \ the article?\n        Looking forward to your reply"
        updatedAt: '2023-06-20T06:50:31.091Z'
      numEdits: 2
      reactions: []
    id: 64914bad0fff8f78f7981dfa
    type: comment
  author: Renqing
  content: "_|_ First of all, congratulations to you.\n_|_  I saw in `InSilicoPerturber`\
    \ function, parameter model_type  has three options` {\"Pretrained\",\"GeneClassifier\"\
    ,\"CellClassifier\"}`,\n`'Pretrained' ` indicates that the pre-trained model has\
    \ not been fine-tuned downstream for specific task\uFF0C'GeneClassifier' is the\
    \ model in your manuscript for distinguishing `dose-sensitive or insensitive`,\
    \ and 'CellClassifier' is the model in your manuscript for distinguishing` cardiomyocytes`\
    \ from non-failing hearts  or hearts affected by hypertrophic or dilated cardiomyopathy?\n\
    _|_  The parameters of this function `isp.perturb_data(\"path/to/model\", \"path/to/input_data\"\
    ,\n\"path/to/output_directory\",\"output_prefix\")`, model and data must all be\
    \ consistent with the function InSilicoPerturber, right?\nIf I want to use these\
    \ two functions ` (\"GeneClassifier\", \"CellClassifier\")`, do I need to fine-tune\
    \ the two models myself with the sample data you provided? The model I cloned\
    \ from huggingface only contains the pre-trained model, right?\n_|_  If I use\
    \ `silico perturbation`, what is the result `difference between the three models\
    \ (\"Pretrained\",\"GeneClassifier\",\"CellClassifier\")`?\n         Would you\
    \ provide a notebook that reproduces the article?\n        Looking forward to\
    \ your reply"
  created_at: 2023-06-20 05:48:13+00:00
  edited: true
  hidden: false
  id: 64914bad0fff8f78f7981dfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-20T07:20:35.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8686081171035767
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. Geneformer is a foundation
          model pretrained on 30 million single cell transcriptomes to gain a fundamental
          understanding of network dynamics that can then be democratized to a multitude
          of downstream applications. The pretrained model can be used for inference
          directly with zero-shot learning, or the model can be fine-tuned for downstream
          applications with a task-specific learning objective and task-specific data.
          In this repository we provide the pretrained model (the 6 layer version
          reported in the manuscript as well as a 12 layer version). Fine-tuning towards
          the particular question at hand with relevant data will generally improve
          the predictive power in that task. The central idea of transfer learning
          is that this pretrained model can transfer its knowledge to a vast range
          of downstream tasks, not just 1 task, or the 2 tasks you mentioned. In the
          manuscript, we demonstrate the model''s predictive power in a diverse panel
          of downstream applications (see below and in the manuscript). </p>

          <p>The notebooks provided in this repository are meant to serve as general
          examples, but are by no means the only applications of the model. Users
          can fine-tune the model to any downstream application that they are interested
          in with limited task-specific data. For example, users can fine-tune the
          model to distinguish genes of different classes/characteristics (GeneClassifier,
          analogous to token classification in NLP) or cells of different states (CellClassifier,
          analogous to sequence classification in NLP). Specifying the type of model
          to the in silico perturber allows it to load the model of the appropriate
          type. For example, fine-tuning for gene or cell classification adds a head
          layer to the model so the number of total layers will be more than the pretrained
          model. It does not change the procedure of in silico deletion or activation.
          Please also refer to the code of the in silico perturber to understand the
          procedure.</p>

          <p>Geneformer applications demonstrated in the manuscript:<br>Fine-tuning:</p>

          <ul>

          <li>transcription factor dosage sensitivity </li>

          <li>chromatin dynamics (bivalently marked promoters)</li>

          <li>transcription factor regulatory range</li>

          <li>gene network centrality</li>

          <li>transcription factor targets</li>

          <li>cell type annotation</li>

          <li>cell state classification across differentiation</li>

          <li>disease classification</li>

          <li>in silico perturbation to determine disease-driving genes</li>

          <li>in silico treatment to determine candidate therapeutic targets</li>

          </ul>

          <p>Zero-shot learning:</p>

          <ul>

          <li>gene context specificity</li>

          <li>in silico reprogramming</li>

          <li>in silico differentiation</li>

          <li>in silico perturbation to determine impact on cell state</li>

          <li>in silico perturbation to determine transcription factor targets</li>

          <li>in silico perturbation to determine transcription factor cooperativity</li>

          </ul>

          '
        raw: "Thank you for your interest in Geneformer. Geneformer is a foundation\
          \ model pretrained on 30 million single cell transcriptomes to gain a fundamental\
          \ understanding of network dynamics that can then be democratized to a multitude\
          \ of downstream applications. The pretrained model can be used for inference\
          \ directly with zero-shot learning, or the model can be fine-tuned for downstream\
          \ applications with a task-specific learning objective and task-specific\
          \ data. In this repository we provide the pretrained model (the 6 layer\
          \ version reported in the manuscript as well as a 12 layer version). Fine-tuning\
          \ towards the particular question at hand with relevant data will generally\
          \ improve the predictive power in that task. The central idea of transfer\
          \ learning is that this pretrained model can transfer its knowledge to a\
          \ vast range of downstream tasks, not just 1 task, or the 2 tasks you mentioned.\
          \ In the manuscript, we demonstrate the model's predictive power in a diverse\
          \ panel of downstream applications (see below and in the manuscript). \n\
          \nThe notebooks provided in this repository are meant to serve as general\
          \ examples, but are by no means the only applications of the model. Users\
          \ can fine-tune the model to any downstream application that they are interested\
          \ in with limited task-specific data. For example, users can fine-tune the\
          \ model to distinguish genes of different classes/characteristics (GeneClassifier,\
          \ analogous to token classification in NLP) or cells of different states\
          \ (CellClassifier, analogous to sequence classification in NLP). Specifying\
          \ the type of model to the in silico perturber allows it to load the model\
          \ of the appropriate type. For example, fine-tuning for gene or cell classification\
          \ adds a head layer to the model so the number of total layers will be more\
          \ than the pretrained model. It does not change the procedure of in silico\
          \ deletion or activation. Please also refer to the code of the in silico\
          \ perturber to understand the procedure.\n\nGeneformer applications demonstrated\
          \ in the manuscript:\nFine-tuning:\n- transcription factor dosage sensitivity\
          \ \n- chromatin dynamics (bivalently marked promoters)\n- transcription\
          \ factor regulatory range\n- gene network centrality\n- transcription factor\
          \ targets\n- cell type annotation\n- cell state classification across differentiation\n\
          - disease classification\n- in silico perturbation to determine disease-driving\
          \ genes\n- in silico treatment to determine candidate therapeutic targets\n\
          \nZero-shot learning:\n- gene context specificity\n- in silico reprogramming\n\
          - in silico differentiation\n- in silico perturbation to determine impact\
          \ on cell state\n- in silico perturbation to determine transcription factor\
          \ targets\n- in silico perturbation to determine transcription factor cooperativity"
        updatedAt: '2023-06-20T07:20:35.464Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64915343ddee981c98d5f8e8
    id: 64915343ddee981c98d5f8e6
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer. Geneformer is a foundation\
    \ model pretrained on 30 million single cell transcriptomes to gain a fundamental\
    \ understanding of network dynamics that can then be democratized to a multitude\
    \ of downstream applications. The pretrained model can be used for inference directly\
    \ with zero-shot learning, or the model can be fine-tuned for downstream applications\
    \ with a task-specific learning objective and task-specific data. In this repository\
    \ we provide the pretrained model (the 6 layer version reported in the manuscript\
    \ as well as a 12 layer version). Fine-tuning towards the particular question\
    \ at hand with relevant data will generally improve the predictive power in that\
    \ task. The central idea of transfer learning is that this pretrained model can\
    \ transfer its knowledge to a vast range of downstream tasks, not just 1 task,\
    \ or the 2 tasks you mentioned. In the manuscript, we demonstrate the model's\
    \ predictive power in a diverse panel of downstream applications (see below and\
    \ in the manuscript). \n\nThe notebooks provided in this repository are meant\
    \ to serve as general examples, but are by no means the only applications of the\
    \ model. Users can fine-tune the model to any downstream application that they\
    \ are interested in with limited task-specific data. For example, users can fine-tune\
    \ the model to distinguish genes of different classes/characteristics (GeneClassifier,\
    \ analogous to token classification in NLP) or cells of different states (CellClassifier,\
    \ analogous to sequence classification in NLP). Specifying the type of model to\
    \ the in silico perturber allows it to load the model of the appropriate type.\
    \ For example, fine-tuning for gene or cell classification adds a head layer to\
    \ the model so the number of total layers will be more than the pretrained model.\
    \ It does not change the procedure of in silico deletion or activation. Please\
    \ also refer to the code of the in silico perturber to understand the procedure.\n\
    \nGeneformer applications demonstrated in the manuscript:\nFine-tuning:\n- transcription\
    \ factor dosage sensitivity \n- chromatin dynamics (bivalently marked promoters)\n\
    - transcription factor regulatory range\n- gene network centrality\n- transcription\
    \ factor targets\n- cell type annotation\n- cell state classification across differentiation\n\
    - disease classification\n- in silico perturbation to determine disease-driving\
    \ genes\n- in silico treatment to determine candidate therapeutic targets\n\n\
    Zero-shot learning:\n- gene context specificity\n- in silico reprogramming\n-\
    \ in silico differentiation\n- in silico perturbation to determine impact on cell\
    \ state\n- in silico perturbation to determine transcription factor targets\n\
    - in silico perturbation to determine transcription factor cooperativity"
  created_at: 2023-06-20 06:20:35+00:00
  edited: false
  hidden: false
  id: 64915343ddee981c98d5f8e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-20T07:20:35.000Z'
    data:
      status: closed
    id: 64915343ddee981c98d5f8e8
    type: status-change
  author: ctheodoris
  created_at: 2023-06-20 06:20:35+00:00
  id: 64915343ddee981c98d5f8e8
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
      fullname: Renqing Nie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renqing
      type: user
    createdAt: '2023-06-21T09:49:11.000Z'
    data:
      edited: false
      editors:
      - Renqing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9486148953437805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
          fullname: Renqing Nie
          isHf: false
          isPro: false
          name: Renqing
          type: user
        html: '<p>Could you provide  notebook files of the results in the article
          for a better understanding of the model and how to use it?<br>Now the code
          and notebook in the repository seem to be very unfriendly to reproduce the
          results of your article.</p>

          '
        raw: 'Could you provide  notebook files of the results in the article for
          a better understanding of the model and how to use it?

          Now the code and notebook in the repository seem to be very unfriendly to
          reproduce the results of your article.'
        updatedAt: '2023-06-21T09:49:11.753Z'
      numEdits: 0
      reactions: []
    id: 6492c79791c22c02004f5cdf
    type: comment
  author: Renqing
  content: 'Could you provide  notebook files of the results in the article for a
    better understanding of the model and how to use it?

    Now the code and notebook in the repository seem to be very unfriendly to reproduce
    the results of your article.'
  created_at: 2023-06-21 08:49:11+00:00
  edited: false
  hidden: false
  id: 6492c79791c22c02004f5cdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-21T17:11:20.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9232328534126282
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The code in this repository is actually
          very friendly to using Geneformer. :) We have composed all the components
          into modules for easy use and reproduction, provided generalizable examples,
          and integrated with Huggingface so users can take advantage of their user-friendly
          infrastructure. Please email me if you are still confused and are having
          trouble reproducing any particular analysis and I would be happy to help
          you troubleshoot.</p>

          '
        raw: Thank you for your question. The code in this repository is actually
          very friendly to using Geneformer. :) We have composed all the components
          into modules for easy use and reproduction, provided generalizable examples,
          and integrated with Huggingface so users can take advantage of their user-friendly
          infrastructure. Please email me if you are still confused and are having
          trouble reproducing any particular analysis and I would be happy to help
          you troubleshoot.
        updatedAt: '2023-06-21T17:11:20.339Z'
      numEdits: 0
      reactions: []
    id: 64932f386b12736afd782b8b
    type: comment
  author: ctheodoris
  content: Thank you for your question. The code in this repository is actually very
    friendly to using Geneformer. :) We have composed all the components into modules
    for easy use and reproduction, provided generalizable examples, and integrated
    with Huggingface so users can take advantage of their user-friendly infrastructure.
    Please email me if you are still confused and are having trouble reproducing any
    particular analysis and I would be happy to help you troubleshoot.
  created_at: 2023-06-21 16:11:20+00:00
  edited: false
  hidden: false
  id: 64932f386b12736afd782b8b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: About silico perturbation
