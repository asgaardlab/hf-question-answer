!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FarzanehN
conflicting_files: null
created_at: 2023-07-26 19:01:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
      fullname: Farzaneh Nasirian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FarzanehN
      type: user
    createdAt: '2023-07-26T20:01:43.000Z'
    data:
      edited: false
      editors:
      - FarzanehN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6941207051277161
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
          fullname: Farzaneh Nasirian
          isHf: false
          isPro: false
          name: FarzanehN
          type: user
        html: '<p>I used the in_silico_perturber module to evaluate the impact of
          gene deletions on shifting the embedding of DCM towards a normal state.
          To prevent out-of-memory (OOM) errors, I carefully selected the following
          parameters:</p>

          <p>perturb_type="delete",<br>perturb_rank_shift=None,<br>genes_to_perturb=''all'',<br>combos=0,<br>anchor_gene=None,<br>model_type="CellClassifier",<br>num_classes=2,<br>emb_mode="cell",<br>cell_emb_style="mean_pool",<br>filter_data=None,<br>cell_states_to_model={"cell_type":(["dcm"],["normal"],[])},<br>max_ncells=200,<br>emb_layer=0,<br>forward_batch_size=10,<br>nproc=16</p>

          <p>Running the code on a NVIDIA Tesla V100 took a considerable amount of
          time, around 4 hours. Given the current setup, it won''t be feasible to
          run it on 780k cells even with 4 GPUs. Is this long runtime expected? What
          do you recommend to do to reduce the runtime? Thanks</p>

          '
        raw: "I used the in_silico_perturber module to evaluate the impact of gene\
          \ deletions on shifting the embedding of DCM towards a normal state. To\
          \ prevent out-of-memory (OOM) errors, I carefully selected the following\
          \ parameters:\r\n\r\nperturb_type=\"delete\",\r\nperturb_rank_shift=None,\r\
          \ngenes_to_perturb='all',\r\ncombos=0,\r\nanchor_gene=None,\r\nmodel_type=\"\
          CellClassifier\",\r\nnum_classes=2,\r\nemb_mode=\"cell\",\r\ncell_emb_style=\"\
          mean_pool\",\r\nfilter_data=None,\r\ncell_states_to_model={\"cell_type\"\
          :([\"dcm\"],[\"normal\"],[])},\r\nmax_ncells=200,\r\nemb_layer=0,\r\nforward_batch_size=10,\r\
          \nnproc=16\r\n\r\nRunning the code on a NVIDIA Tesla V100 took a considerable\
          \ amount of time, around 4 hours. Given the current setup, it won't be feasible\
          \ to run it on 780k cells even with 4 GPUs. Is this long runtime expected?\
          \ What do you recommend to do to reduce the runtime? Thanks"
        updatedAt: '2023-07-26T20:01:43.548Z'
      numEdits: 0
      reactions: []
    id: 64c17ba7b005aab93d537d45
    type: comment
  author: FarzanehN
  content: "I used the in_silico_perturber module to evaluate the impact of gene deletions\
    \ on shifting the embedding of DCM towards a normal state. To prevent out-of-memory\
    \ (OOM) errors, I carefully selected the following parameters:\r\n\r\nperturb_type=\"\
    delete\",\r\nperturb_rank_shift=None,\r\ngenes_to_perturb='all',\r\ncombos=0,\r\
    \nanchor_gene=None,\r\nmodel_type=\"CellClassifier\",\r\nnum_classes=2,\r\nemb_mode=\"\
    cell\",\r\ncell_emb_style=\"mean_pool\",\r\nfilter_data=None,\r\ncell_states_to_model={\"\
    cell_type\":([\"dcm\"],[\"normal\"],[])},\r\nmax_ncells=200,\r\nemb_layer=0,\r\
    \nforward_batch_size=10,\r\nnproc=16\r\n\r\nRunning the code on a NVIDIA Tesla\
    \ V100 took a considerable amount of time, around 4 hours. Given the current setup,\
    \ it won't be feasible to run it on 780k cells even with 4 GPUs. Is this long\
    \ runtime expected? What do you recommend to do to reduce the runtime? Thanks"
  created_at: 2023-07-26 19:01:43+00:00
  edited: false
  hidden: false
  id: 64c17ba7b005aab93d537d45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-26T21:37:19.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9605525732040405
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question! The batch size you are using is extremely
          small. I would expect batch sizes of 200+ on a 40G V100. Are you running
          out of memory with larger than 10?</p>

          '
        raw: Thank you for your question! The batch size you are using is extremely
          small. I would expect batch sizes of 200+ on a 40G V100. Are you running
          out of memory with larger than 10?
        updatedAt: '2023-07-26T21:37:19.340Z'
      numEdits: 0
      reactions: []
    id: 64c1920f77655fcf3fdd1b0e
    type: comment
  author: ctheodoris
  content: Thank you for your question! The batch size you are using is extremely
    small. I would expect batch sizes of 200+ on a 40G V100. Are you running out of
    memory with larger than 10?
  created_at: 2023-07-26 20:37:19+00:00
  edited: false
  hidden: false
  id: 64c1920f77655fcf3fdd1b0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
      fullname: Farzaneh Nasirian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FarzanehN
      type: user
    createdAt: '2023-07-26T22:29:00.000Z'
    data:
      edited: false
      editors:
      - FarzanehN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9068763852119446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
          fullname: Farzaneh Nasirian
          isHf: false
          isPro: false
          name: FarzanehN
          type: user
        html: '<p>That''s correct. I am running it on V100 with 16GB RAM.</p>

          '
        raw: That's correct. I am running it on V100 with 16GB RAM.
        updatedAt: '2023-07-26T22:29:00.075Z'
      numEdits: 0
      reactions: []
    id: 64c19e2cd1ca220e3031ea9f
    type: comment
  author: FarzanehN
  content: That's correct. I am running it on V100 with 16GB RAM.
  created_at: 2023-07-26 21:29:00+00:00
  edited: false
  hidden: false
  id: 64c19e2cd1ca220e3031ea9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-26T23:53:36.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9365363717079163
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for clarifying your GPU size. Can you monitor with nvtop
          and confirm the GPU cache is emptied between batches? If not, please let
          me know. I also assume you are using the 6 layer model rather than the 12
          layer model, but if you are not, you may consider using the 6 layer model
          given the resource limitation. You can also try distributing the job with
          Deepspeed if you have additional 16G GPUs. This will distribute the model
          so that you don''t need to replicate the model on each GPU and you can have
          more room for batches. </p>

          '
        raw: 'Thank you for clarifying your GPU size. Can you monitor with nvtop and
          confirm the GPU cache is emptied between batches? If not, please let me
          know. I also assume you are using the 6 layer model rather than the 12 layer
          model, but if you are not, you may consider using the 6 layer model given
          the resource limitation. You can also try distributing the job with Deepspeed
          if you have additional 16G GPUs. This will distribute the model so that
          you don''t need to replicate the model on each GPU and you can have more
          room for batches. '
        updatedAt: '2023-07-26T23:53:36.029Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c1b2000bfb901b0441c381
    id: 64c1b2000bfb901b0441c37c
    type: comment
  author: ctheodoris
  content: 'Thank you for clarifying your GPU size. Can you monitor with nvtop and
    confirm the GPU cache is emptied between batches? If not, please let me know.
    I also assume you are using the 6 layer model rather than the 12 layer model,
    but if you are not, you may consider using the 6 layer model given the resource
    limitation. You can also try distributing the job with Deepspeed if you have additional
    16G GPUs. This will distribute the model so that you don''t need to replicate
    the model on each GPU and you can have more room for batches. '
  created_at: 2023-07-26 22:53:36+00:00
  edited: false
  hidden: false
  id: 64c1b2000bfb901b0441c37c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-26T23:53:36.000Z'
    data:
      status: closed
    id: 64c1b2000bfb901b0441c381
    type: status-change
  author: ctheodoris
  created_at: 2023-07-26 22:53:36+00:00
  id: 64c1b2000bfb901b0441c381
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
      fullname: Farzaneh Nasirian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FarzanehN
      type: user
    createdAt: '2023-08-15T01:35:18.000Z'
    data:
      edited: false
      editors:
      - FarzanehN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6883854866027832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
          fullname: Farzaneh Nasirian
          isHf: false
          isPro: false
          name: FarzanehN
          type: user
        html: '<p>Thank you for your input. To execute the in-silico-pertuber.py using
          DeepSpeed-Inference, I integrated the subsequent command within the load_model
          function:</p>

          <p>model = deepspeed.init_inference(model,<br>exchange_with_kernel_inject=True,<br>tensor_partitioned={"activated":True,
          "partition_size":4, "multi_processing_unit":None, "partition_group":None},<br>enable_cuda_graph=True,<br>data_type=torch.float,<br>)</p>

          <p>But, I am getting the "RuntimeError: Tensor a''s size (512) must align
          with tensor b''s size (0) along the shared dimension 1". Would you be able
          to provide any suggestions?<br>FYI, I am running on the recently updated
          codes. </p>

          '
        raw: 'Thank you for your input. To execute the in-silico-pertuber.py using
          DeepSpeed-Inference, I integrated the subsequent command within the load_model
          function:


          model = deepspeed.init_inference(model,

          exchange_with_kernel_inject=True,

          tensor_partitioned={"activated":True, "partition_size":4, "multi_processing_unit":None,
          "partition_group":None},

          enable_cuda_graph=True,

          data_type=torch.float,

          )


          But, I am getting the "RuntimeError: Tensor a''s size (512) must align with
          tensor b''s size (0) along the shared dimension 1". Would you be able to
          provide any suggestions?

          FYI, I am running on the recently updated codes. '
        updatedAt: '2023-08-15T01:35:18.745Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64dad656923e4576500564a6
    id: 64dad656923e4576500564a3
    type: comment
  author: FarzanehN
  content: 'Thank you for your input. To execute the in-silico-pertuber.py using DeepSpeed-Inference,
    I integrated the subsequent command within the load_model function:


    model = deepspeed.init_inference(model,

    exchange_with_kernel_inject=True,

    tensor_partitioned={"activated":True, "partition_size":4, "multi_processing_unit":None,
    "partition_group":None},

    enable_cuda_graph=True,

    data_type=torch.float,

    )


    But, I am getting the "RuntimeError: Tensor a''s size (512) must align with tensor
    b''s size (0) along the shared dimension 1". Would you be able to provide any
    suggestions?

    FYI, I am running on the recently updated codes. '
  created_at: 2023-08-15 00:35:18+00:00
  edited: false
  hidden: false
  id: 64dad656923e4576500564a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/00cbb3754a556cea971f87b713377baf.svg
      fullname: Farzaneh Nasirian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FarzanehN
      type: user
    createdAt: '2023-08-15T01:35:18.000Z'
    data:
      status: open
    id: 64dad656923e4576500564a6
    type: status-change
  author: FarzanehN
  created_at: 2023-08-15 00:35:18+00:00
  id: 64dad656923e4576500564a6
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e19b0efd2e371d62a492c416f5d9ca1.svg
      fullname: Matthew Pace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pingiotto
      type: user
    createdAt: '2023-08-18T11:03:38.000Z'
    data:
      edited: false
      editors:
      - Pingiotto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973213791847229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e19b0efd2e371d62a492c416f5d9ca1.svg
          fullname: Matthew Pace
          isHf: false
          isPro: false
          name: Pingiotto
          type: user
        html: '<p>I estimated that with max_ncells=None, running on eight T4 GPUs,
          it would take 24 days to process all cells. </p>

          '
        raw: 'I estimated that with max_ncells=None, running on eight T4 GPUs, it
          would take 24 days to process all cells. '
        updatedAt: '2023-08-18T11:03:38.361Z'
      numEdits: 0
      reactions: []
    id: 64df500a9bc64767a646a12e
    type: comment
  author: Pingiotto
  content: 'I estimated that with max_ncells=None, running on eight T4 GPUs, it would
    take 24 days to process all cells. '
  created_at: 2023-08-18 10:03:38+00:00
  edited: false
  hidden: false
  id: 64df500a9bc64767a646a12e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57cf7d5fb8343b9858403fcf59da722a.svg
      fullname: Hongfu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lihf
      type: user
    createdAt: '2023-08-20T18:06:58.000Z'
    data:
      edited: true
      editors:
      - lihf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8913106322288513
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57cf7d5fb8343b9858403fcf59da722a.svg
          fullname: Hongfu
          isHf: false
          isPro: false
          name: lihf
          type: user
        html: '<p>Hello, I have tried the in_silico_perturbation.ipynb code located
          in the example folder. I used the dataset located at ./Geneformers-30M/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset
          as the input file. The maximum VRAM usage is approximately 18GB when I set
          the forward_batch_size to 100 in your code. So, if you are using a 16G V100,
          I suggest trying a batch size slightly smaller than 100 for better speed,
          instead of using 10. I hope this helps.</p>

          '
        raw: Hello, I have tried the in_silico_perturbation.ipynb code located in
          the example folder. I used the dataset located at ./Geneformers-30M/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset
          as the input file. The maximum VRAM usage is approximately 18GB when I set
          the forward_batch_size to 100 in your code. So, if you are using a 16G V100,
          I suggest trying a batch size slightly smaller than 100 for better speed,
          instead of using 10. I hope this helps.
        updatedAt: '2023-08-20T18:13:32.681Z'
      numEdits: 3
      reactions: []
    id: 64e25642825f4133e723f30b
    type: comment
  author: lihf
  content: Hello, I have tried the in_silico_perturbation.ipynb code located in the
    example folder. I used the dataset located at ./Geneformers-30M/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset
    as the input file. The maximum VRAM usage is approximately 18GB when I set the
    forward_batch_size to 100 in your code. So, if you are using a 16G V100, I suggest
    trying a batch size slightly smaller than 100 for better speed, instead of using
    10. I hope this helps.
  created_at: 2023-08-20 17:06:58+00:00
  edited: true
  hidden: false
  id: 64e25642825f4133e723f30b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-18T07:28:45.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9584130644798279
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you all for your input on this discussion. Regarding Deepspeed,
          we have used Deepspeed for training but not for inference. I would suggest
          finding out which step in the code is causing the tensor mismatch. Sometimes
          if there is a remainder batch that is just 1 tensor, the dimensions will
          be different than expected from the other batches that were 3 dimension
          tensors, so this will cause an issue when stacking them together. If you
          are distributing the model with Deepspeed, it should allow larger batch
          sizes that would possibly be less likely to cause this issue. If you are
          not able to run larger batch sizes with Deepspeed than if you don''t use
          Deepspeed, I would suggest looking into how it''s setting up the analysis
          to ensure the model is distributed properly. Also, as mentioned before,
          if you are using the 12 layer model and encountering memory limitations,
          I would suggest you trial the 6 layer model (outer directory of this repository)
          which will be more memory-efficient.</p>

          '
        raw: Thank you all for your input on this discussion. Regarding Deepspeed,
          we have used Deepspeed for training but not for inference. I would suggest
          finding out which step in the code is causing the tensor mismatch. Sometimes
          if there is a remainder batch that is just 1 tensor, the dimensions will
          be different than expected from the other batches that were 3 dimension
          tensors, so this will cause an issue when stacking them together. If you
          are distributing the model with Deepspeed, it should allow larger batch
          sizes that would possibly be less likely to cause this issue. If you are
          not able to run larger batch sizes with Deepspeed than if you don't use
          Deepspeed, I would suggest looking into how it's setting up the analysis
          to ensure the model is distributed properly. Also, as mentioned before,
          if you are using the 12 layer model and encountering memory limitations,
          I would suggest you trial the 6 layer model (outer directory of this repository)
          which will be more memory-efficient.
        updatedAt: '2023-09-18T07:28:45.455Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6507fc2d14302b1d76daf660
    id: 6507fc2d14302b1d76daf65a
    type: comment
  author: ctheodoris
  content: Thank you all for your input on this discussion. Regarding Deepspeed, we
    have used Deepspeed for training but not for inference. I would suggest finding
    out which step in the code is causing the tensor mismatch. Sometimes if there
    is a remainder batch that is just 1 tensor, the dimensions will be different than
    expected from the other batches that were 3 dimension tensors, so this will cause
    an issue when stacking them together. If you are distributing the model with Deepspeed,
    it should allow larger batch sizes that would possibly be less likely to cause
    this issue. If you are not able to run larger batch sizes with Deepspeed than
    if you don't use Deepspeed, I would suggest looking into how it's setting up the
    analysis to ensure the model is distributed properly. Also, as mentioned before,
    if you are using the 12 layer model and encountering memory limitations, I would
    suggest you trial the 6 layer model (outer directory of this repository) which
    will be more memory-efficient.
  created_at: 2023-09-18 06:28:45+00:00
  edited: false
  hidden: false
  id: 6507fc2d14302b1d76daf65a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-18T07:28:45.000Z'
    data:
      status: closed
    id: 6507fc2d14302b1d76daf660
    type: status-change
  author: ctheodoris
  created_at: 2023-09-18 06:28:45+00:00
  id: 6507fc2d14302b1d76daf660
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 137
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: In_silico_perturber runtime
