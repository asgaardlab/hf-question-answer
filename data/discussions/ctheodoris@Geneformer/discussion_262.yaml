!!python/object:huggingface_hub.community.DiscussionWithDetails
author: patrick-yu
conflicting_files: null
created_at: 2023-10-16 19:46:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d78a7c977b77aee128032ca58167ea7.svg
      fullname: Patrick Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrick-yu
      type: user
    createdAt: '2023-10-16T20:46:25.000Z'
    data:
      edited: false
      editors:
      - patrick-yu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8862515091896057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d78a7c977b77aee128032ca58167ea7.svg
          fullname: Patrick Yu
          isHf: false
          isPro: false
          name: patrick-yu
          type: user
        html: '<p>Just wondering if there''s any way to expand the maximum 2048 token
          length for Geneformer (e.g. for bigger inputs/datasets)? </p>

          <p>Or perhaps is there some easy way to use/pretrain a different (e.g. BERT-like)
          model that accommodates &gt;2048 tokens in the input but still utilizes
          some of the same learned weights from the pretrained (6L/12L) Geneformer?</p>

          <p>Thanks in advance!</p>

          '
        raw: "Just wondering if there's any way to expand the maximum 2048 token length\
          \ for Geneformer (e.g. for bigger inputs/datasets)? \r\n\r\nOr perhaps is\
          \ there some easy way to use/pretrain a different (e.g. BERT-like) model\
          \ that accommodates >2048 tokens in the input but still utilizes some of\
          \ the same learned weights from the pretrained (6L/12L) Geneformer?\r\n\r\
          \nThanks in advance!"
        updatedAt: '2023-10-16T20:46:25.372Z'
      numEdits: 0
      reactions: []
    id: 652da121c3424254ba69b5c2
    type: comment
  author: patrick-yu
  content: "Just wondering if there's any way to expand the maximum 2048 token length\
    \ for Geneformer (e.g. for bigger inputs/datasets)? \r\n\r\nOr perhaps is there\
    \ some easy way to use/pretrain a different (e.g. BERT-like) model that accommodates\
    \ >2048 tokens in the input but still utilizes some of the same learned weights\
    \ from the pretrained (6L/12L) Geneformer?\r\n\r\nThanks in advance!"
  created_at: 2023-10-16 19:46:25+00:00
  edited: false
  hidden: false
  id: 652da121c3424254ba69b5c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-25T19:24:02.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.815963864326477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! Yes, you can use the
          pretraining code in the example on this repository and increase the maximum
          input size to pretrain a model with a larger input size with Genecorpus-30M.</p>

          '
        raw: Thank you for your interest in Geneformer! Yes, you can use the pretraining
          code in the example on this repository and increase the maximum input size
          to pretrain a model with a larger input size with Genecorpus-30M.
        updatedAt: '2023-10-25T19:24:02.780Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65396b5229f8a911551a9b61
    id: 65396b5229f8a911551a9b5a
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! Yes, you can use the pretraining
    code in the example on this repository and increase the maximum input size to
    pretrain a model with a larger input size with Genecorpus-30M.
  created_at: 2023-10-25 18:24:02+00:00
  edited: false
  hidden: false
  id: 65396b5229f8a911551a9b5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-25T19:24:02.000Z'
    data:
      status: closed
    id: 65396b5229f8a911551a9b61
    type: status-change
  author: ctheodoris
  created_at: 2023-10-25 18:24:02+00:00
  id: 65396b5229f8a911551a9b61
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 262
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Expanding the maximum input size (2048 tokens) of a pre-trained Geneformer?
