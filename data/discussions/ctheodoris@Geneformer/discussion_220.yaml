!!python/object:huggingface_hub.community.DiscussionWithDetails
author: allenxiao
conflicting_files: null
created_at: 2023-08-16 01:32:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-08-16T02:32:32.000Z'
    data:
      edited: true
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8493470549583435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>Hi Christina! Thank you for the amazing work. I have two questions.<br>First,
          regarding the downstream task like cell-type classification, the dataset
          need to be split into training set and testing set for finetuning and predicting
          respectively. When tokenizing the data using tk.tokenize_data, there are
          two methods:<br>1, tokenize the whole data and then split them;<br>2, split
          them and then tokenize them.<br>Are they different in the generated results?</p>

          <p>Second, the data distribution in the downstream task may be different
          from the pretraining corpus, so should I replace the gene_median_dictionary
          of the tokenizer where the dictionary would be generated according to the
          downstream dataset following the instructions from the tutorial "Obtain
          non-zero median expression value" or just use the same tokenizer as the
          one used in the pretraining procedure?</p>

          '
        raw: 'Hi Christina! Thank you for the amazing work. I have two questions.

          First, regarding the downstream task like cell-type classification, the
          dataset need to be split into training set and testing set for finetuning
          and predicting respectively. When tokenizing the data using tk.tokenize_data,
          there are two methods:

          1, tokenize the whole data and then split them;

          2, split them and then tokenize them.

          Are they different in the generated results?


          Second, the data distribution in the downstream task may be different from
          the pretraining corpus, so should I replace the gene_median_dictionary of
          the tokenizer where the dictionary would be generated according to the downstream
          dataset following the instructions from the tutorial "Obtain non-zero median
          expression value" or just use the same tokenizer as the one used in the
          pretraining procedure?'
        updatedAt: '2023-08-16T02:54:44.460Z'
      numEdits: 1
      reactions: []
    id: 64dc354050d53d4f535b2e22
    type: comment
  author: allenxiao
  content: 'Hi Christina! Thank you for the amazing work. I have two questions.

    First, regarding the downstream task like cell-type classification, the dataset
    need to be split into training set and testing set for finetuning and predicting
    respectively. When tokenizing the data using tk.tokenize_data, there are two methods:

    1, tokenize the whole data and then split them;

    2, split them and then tokenize them.

    Are they different in the generated results?


    Second, the data distribution in the downstream task may be different from the
    pretraining corpus, so should I replace the gene_median_dictionary of the tokenizer
    where the dictionary would be generated according to the downstream dataset following
    the instructions from the tutorial "Obtain non-zero median expression value" or
    just use the same tokenizer as the one used in the pretraining procedure?'
  created_at: 2023-08-16 01:32:32+00:00
  edited: true
  hidden: false
  id: 64dc354050d53d4f535b2e22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-23T17:49:16.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8786712884902954
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your questions! </p>

          <ol>

          <li><p>There should be no difference in the two methods above. You should
          just be sure to keep your training, validation, and test sets separate to
          avoid contamination of the training data.</p>

          </li>

          <li><p>You should definitely use the provided tokenizer and NOT regenerate
          the gene_median_dictionary using the task-specific data. Please read the
          instructions in the tutorial you referenced for more information about this.
          This paragraph is of relevance to your question:</p>

          </li>

          </ol>

          <p>"If using Geneformer, to ensure consistency of the normalization factor
          used for each gene for all future datasets, users should use the Geneformer
          transcriptome tokenizer to tokenize their datasets and should not re-calculate
          this normalization factor for their individual dataset . This code for re-calculating
          the normalization factor should only be used by users who are pretraining
          a new model from scratch with a new pretraining corpus other than Genecorpus-30M."</p>

          '
        raw: "Thank you for your questions! \n\n1) There should be no difference in\
          \ the two methods above. You should just be sure to keep your training,\
          \ validation, and test sets separate to avoid contamination of the training\
          \ data.\n\n2) You should definitely use the provided tokenizer and NOT regenerate\
          \ the gene_median_dictionary using the task-specific data. Please read the\
          \ instructions in the tutorial you referenced for more information about\
          \ this. This paragraph is of relevance to your question:\n\n\"If using Geneformer,\
          \ to ensure consistency of the normalization factor used for each gene for\
          \ all future datasets, users should use the Geneformer transcriptome tokenizer\
          \ to tokenize their datasets and should not re-calculate this normalization\
          \ factor for their individual dataset . This code for re-calculating the\
          \ normalization factor should only be used by users who are pretraining\
          \ a new model from scratch with a new pretraining corpus other than Genecorpus-30M.\""
        updatedAt: '2023-08-23T17:49:16.143Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - allenxiao
      relatedEventId: 64e6469c9cb7c83a9d6b122b
    id: 64e6469c9cb7c83a9d6b122a
    type: comment
  author: ctheodoris
  content: "Thank you for your questions! \n\n1) There should be no difference in\
    \ the two methods above. You should just be sure to keep your training, validation,\
    \ and test sets separate to avoid contamination of the training data.\n\n2) You\
    \ should definitely use the provided tokenizer and NOT regenerate the gene_median_dictionary\
    \ using the task-specific data. Please read the instructions in the tutorial you\
    \ referenced for more information about this. This paragraph is of relevance to\
    \ your question:\n\n\"If using Geneformer, to ensure consistency of the normalization\
    \ factor used for each gene for all future datasets, users should use the Geneformer\
    \ transcriptome tokenizer to tokenize their datasets and should not re-calculate\
    \ this normalization factor for their individual dataset . This code for re-calculating\
    \ the normalization factor should only be used by users who are pretraining a\
    \ new model from scratch with a new pretraining corpus other than Genecorpus-30M.\""
  created_at: 2023-08-23 16:49:16+00:00
  edited: false
  hidden: false
  id: 64e6469c9cb7c83a9d6b122a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-23T17:49:16.000Z'
    data:
      status: closed
    id: 64e6469c9cb7c83a9d6b122b
    type: status-change
  author: ctheodoris
  created_at: 2023-08-23 16:49:16+00:00
  id: 64e6469c9cb7c83a9d6b122b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 220
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: About tk.tokenize_data
