!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wzw14541
conflicting_files: null
created_at: 2023-08-09 13:37:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97ed9053c3a845fea5ca5ae9d93d25d3.svg
      fullname: Ziwei Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wzw14541
      type: user
    createdAt: '2023-08-09T14:37:22.000Z'
    data:
      edited: false
      editors:
      - wzw14541
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5962910056114197
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97ed9053c3a845fea5ca5ae9d93d25d3.svg
          fullname: Ziwei Wang
          isHf: false
          isPro: false
          name: wzw14541
          type: user
        html: '<p>Hello everyone,</p>

          <p>During the process of tokenizing a dataset of approximately one million
          cells, I encountered the following error. I suspect that it might be due
          to an excessively large batch size or insufficient memory. The specific
          error message is as follows:</p>

          <p>Traceback (most recent call last):<br>  File "1.token.py", line 8, in
          <br>    tk.tokenize_data(loom_data_directory, output_directory, output_prefix)<br>  File
          "/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py",
          line 102, in tokenize_data<br>    tokenized_dataset = self.create_dataset(tokenized_cells,
          cell_metadata)<br>  File "/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py",
          line 204, in create_dataset<br>    output_dataset = Dataset.from_dict(dataset_dict)<br>  File
          "/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py",
          line 911, in from_dict<br>    pa_table = InMemoryTable.from_pydict(mapping=mapping)<br>  File
          "/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/table.py",
          line 799, in from_pydict<br>    return cls(pa.Table.from_pydict(*args, **kwargs))<br>  File
          "pyarrow/table.pxi", line 3849, in pyarrow.lib.Table.from_pydict<br>  File
          "pyarrow/table.pxi", line 5401, in pyarrow.lib._from_pydict<br>  File "pyarrow/array.pxi",
          line 357, in pyarrow.lib.asarray<br>  File "pyarrow/array.pxi", line 243,
          in pyarrow.lib.array<br>  File "pyarrow/array.pxi", line 110, in pyarrow.lib._handle_arrow_array_protocol<br>  File
          "/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_writer.py",
          line 245, in <strong>arrow_array</strong><br>    raise OverflowError(<br>OverflowError:
          There was an overflow with type &lt;class ''list''&gt;. Try to reduce writer_batch_size
          to have batches smaller than 2GB.<br>(offset overflow while concatenating
          arrays)</p>

          <p>Does anyone know what could be causing the error? I would greatly appreciate
          any assistance.</p>

          '
        raw: "Hello everyone,\r\n      \r\nDuring the process of tokenizing a dataset\
          \ of approximately one million cells, I encountered the following error.\
          \ I suspect that it might be due to an excessively large batch size or insufficient\
          \ memory. The specific error message is as follows:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"1.token.py\", line 8, in <module>\r\n \
          \   tk.tokenize_data(loom_data_directory, output_directory, output_prefix)\r\
          \n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 102, in tokenize_data\r\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
          \ cell_metadata)\r\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 204, in create_dataset\r\n    output_dataset = Dataset.from_dict(dataset_dict)\r\
          \n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py\"\
          , line 911, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\
          \n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/table.py\"\
          , line 799, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args,\
          \ **kwargs))\r\n  File \"pyarrow/table.pxi\", line 3849, in pyarrow.lib.Table.from_pydict\r\
          \n  File \"pyarrow/table.pxi\", line 5401, in pyarrow.lib._from_pydict\r\
          \n  File \"pyarrow/array.pxi\", line 357, in pyarrow.lib.asarray\r\n  File\
          \ \"pyarrow/array.pxi\", line 243, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\"\
          , line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_writer.py\"\
          , line 245, in __arrow_array__\r\n    raise OverflowError(\r\nOverflowError:\
          \ There was an overflow with type <class 'list'>. Try to reduce writer_batch_size\
          \ to have batches smaller than 2GB.\r\n(offset overflow while concatenating\
          \ arrays)\r\n\r\nDoes anyone know what could be causing the error? I would\
          \ greatly appreciate any assistance."
        updatedAt: '2023-08-09T14:37:22.308Z'
      numEdits: 0
      reactions: []
    id: 64d3a4a263f2477c3c0312ea
    type: comment
  author: wzw14541
  content: "Hello everyone,\r\n      \r\nDuring the process of tokenizing a dataset\
    \ of approximately one million cells, I encountered the following error. I suspect\
    \ that it might be due to an excessively large batch size or insufficient memory.\
    \ The specific error message is as follows:\r\n\r\nTraceback (most recent call\
    \ last):\r\n  File \"1.token.py\", line 8, in <module>\r\n    tk.tokenize_data(loom_data_directory,\
    \ output_directory, output_prefix)\r\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
    , line 102, in tokenize_data\r\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
    \ cell_metadata)\r\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
    , line 204, in create_dataset\r\n    output_dataset = Dataset.from_dict(dataset_dict)\r\
    \n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py\"\
    , line 911, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\
    \n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/table.py\"\
    , line 799, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\
    \n  File \"pyarrow/table.pxi\", line 3849, in pyarrow.lib.Table.from_pydict\r\n\
    \  File \"pyarrow/table.pxi\", line 5401, in pyarrow.lib._from_pydict\r\n  File\
    \ \"pyarrow/array.pxi\", line 357, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\"\
    , line 243, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in\
    \ pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_writer.py\"\
    , line 245, in __arrow_array__\r\n    raise OverflowError(\r\nOverflowError: There\
    \ was an overflow with type <class 'list'>. Try to reduce writer_batch_size to\
    \ have batches smaller than 2GB.\r\n(offset overflow while concatenating arrays)\r\
    \n\r\nDoes anyone know what could be causing the error? I would greatly appreciate\
    \ any assistance."
  created_at: 2023-08-09 13:37:22+00:00
  edited: false
  hidden: false
  id: 64d3a4a263f2477c3c0312ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-09T21:23:53.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441622495651245
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! This is likely due to
          a known issue with Huggingface Datasets that they are working on resolving.
          Please see <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/170">PR
          170</a> for more information as well as a possible solution using Dataset.from_generator.
          We are working on resolving a couple of errors and determining a faster
          solution with this PR before merging it but you could substitute the Dataset.from_generator
          instead of Dataset.from_dict for an intermediate solution (it is slower
          but seems to effectively resolve the overflow error encountered by other
          users with large datasets). Because we do not encounter this error, even
          when tokenizing datasets &gt;1M, we cannot very effectively troubleshoot
          it. If you are able to email me a link to your dataset that caused the error,
          that would be very helpful so we can ensure any faster solution we put in
          place would truly resolve the error.</p>

          '
        raw: Thank you for your interest in Geneformer! This is likely due to a known
          issue with Huggingface Datasets that they are working on resolving. Please
          see [PR 170](https://huggingface.co/ctheodoris/Geneformer/discussions/170)
          for more information as well as a possible solution using Dataset.from_generator.
          We are working on resolving a couple of errors and determining a faster
          solution with this PR before merging it but you could substitute the Dataset.from_generator
          instead of Dataset.from_dict for an intermediate solution (it is slower
          but seems to effectively resolve the overflow error encountered by other
          users with large datasets). Because we do not encounter this error, even
          when tokenizing datasets >1M, we cannot very effectively troubleshoot it.
          If you are able to email me a link to your dataset that caused the error,
          that would be very helpful so we can ensure any faster solution we put in
          place would truly resolve the error.
        updatedAt: '2023-08-09T21:23:53.207Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d403e93756703d46308b8b
    id: 64d403e93756703d46308b89
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! This is likely due to a known
    issue with Huggingface Datasets that they are working on resolving. Please see
    [PR 170](https://huggingface.co/ctheodoris/Geneformer/discussions/170) for more
    information as well as a possible solution using Dataset.from_generator. We are
    working on resolving a couple of errors and determining a faster solution with
    this PR before merging it but you could substitute the Dataset.from_generator
    instead of Dataset.from_dict for an intermediate solution (it is slower but seems
    to effectively resolve the overflow error encountered by other users with large
    datasets). Because we do not encounter this error, even when tokenizing datasets
    >1M, we cannot very effectively troubleshoot it. If you are able to email me a
    link to your dataset that caused the error, that would be very helpful so we can
    ensure any faster solution we put in place would truly resolve the error.
  created_at: 2023-08-09 20:23:53+00:00
  edited: false
  hidden: false
  id: 64d403e93756703d46308b89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-09T21:23:53.000Z'
    data:
      status: closed
    id: 64d403e93756703d46308b8b
    type: status-change
  author: ctheodoris
  created_at: 2023-08-09 20:23:53+00:00
  id: 64d403e93756703d46308b8b
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97ed9053c3a845fea5ca5ae9d93d25d3.svg
      fullname: Ziwei Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wzw14541
      type: user
    createdAt: '2023-08-11T17:13:43.000Z'
    data:
      edited: false
      editors:
      - wzw14541
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4741970896720886
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97ed9053c3a845fea5ca5ae9d93d25d3.svg
          fullname: Ziwei Wang
          isHf: false
          isPro: false
          name: wzw14541
          type: user
        html: "<p>Hi,ctheodoris\uFF0C</p>\n<p>Even after modifying the  Dataset.from_generator\
          \  function, the following error still occurs\uFF1A</p>\n<p>HF google storage\
          \ unreachable. Downloading and preparing it from source<br>Generating train\
          \ split: 0 examples [00:00, ? examples/s]<br>Traceback (most recent call\
          \ last):<br>  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1676, in _prepare_split_single<br>    for key, record in generator:<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/packaged_modules/generator/generator.py\"\
          , line 30, in _generate_examples<br>    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):<br>TypeError:\
          \ 'dict' object is not callable</p>\n<p>The above exception was the direct\
          \ cause of the following exception:</p>\n<p>Traceback (most recent call\
          \ last):<br>  File \"1.token.py\", line 8, in <br>    tk.tokenize_data(loom_data_directory,\
          \ output_directory, output_prefix)<br>  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 102, in tokenize_data<br>    tokenized_dataset = self.create_dataset(tokenized_cells,\
          \ cell_metadata)<br>  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 206, in create_dataset<br>    output_dataset=Dataset.from_generator(dataset_dict)<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py\"\
          , line 1064, in from_generator<br>    return GeneratorDatasetInputStream(<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/io/generator.py\"\
          , line 47, in read<br>    self.builder.download_and_prepare(<br>  File \"\
          /jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 954, in download_and_prepare<br>    self._download_and_prepare(<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1717, in _download_and_prepare<br>    super()._download_and_prepare(<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1049, in _download_and_prepare<br>    self._prepare_split(split_generator,\
          \ **prepare_split_kwargs)<br>  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1555, in _prepare_split<br>    for job_id, done, content in self._prepare_split_single(<br>\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1712, in _prepare_split_single<br>    raise DatasetGenerationError(\"\
          An error occurred while generating the dataset\") from e<br>datasets.builder.DatasetGenerationError:\
          \ An error occurred while generating the dataset</p>\n<p>We have decided\
          \ to send you an email to resolve this issue.  thank you for your patience\uFF01\
          </p>\n"
        raw: "Hi,ctheodoris\uFF0C\n\nEven after modifying the  Dataset.from_generator\
          \  function, the following error still occurs\uFF1A\n\nHF google storage\
          \ unreachable. Downloading and preparing it from source\nGenerating train\
          \ split: 0 examples [00:00, ? examples/s]\nTraceback (most recent call last):\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1676, in _prepare_split_single\n    for key, record in generator:\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/packaged_modules/generator/generator.py\"\
          , line 30, in _generate_examples\n    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n\
          TypeError: 'dict' object is not callable\n\nThe above exception was the\
          \ direct cause of the following exception:\n\nTraceback (most recent call\
          \ last):\n  File \"1.token.py\", line 8, in <module>\n    tk.tokenize_data(loom_data_directory,\
          \ output_directory, output_prefix)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 102, in tokenize_data\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
          \ cell_metadata)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
          , line 206, in create_dataset\n    output_dataset=Dataset.from_generator(dataset_dict)\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py\"\
          , line 1064, in from_generator\n    return GeneratorDatasetInputStream(\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/io/generator.py\"\
          , line 47, in read\n    self.builder.download_and_prepare(\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 954, in download_and_prepare\n    self._download_and_prepare(\n \
          \ File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1717, in _download_and_prepare\n    super()._download_and_prepare(\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1049, in _download_and_prepare\n    self._prepare_split(split_generator,\
          \ **prepare_split_kwargs)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1555, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n\
          \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
          , line 1712, in _prepare_split_single\n    raise DatasetGenerationError(\"\
          An error occurred while generating the dataset\") from e\ndatasets.builder.DatasetGenerationError:\
          \ An error occurred while generating the dataset\n\nWe have decided to send\
          \ you an email to resolve this issue.  thank you for your patience\uFF01"
        updatedAt: '2023-08-11T17:13:43.315Z'
      numEdits: 0
      reactions: []
    id: 64d66c4746bda7d8f5e765bd
    type: comment
  author: wzw14541
  content: "Hi,ctheodoris\uFF0C\n\nEven after modifying the  Dataset.from_generator\
    \  function, the following error still occurs\uFF1A\n\nHF google storage unreachable.\
    \ Downloading and preparing it from source\nGenerating train split: 0 examples\
    \ [00:00, ? examples/s]\nTraceback (most recent call last):\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 1676, in _prepare_split_single\n    for key, record in generator:\n  File\
    \ \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/packaged_modules/generator/generator.py\"\
    , line 30, in _generate_examples\n    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n\
    TypeError: 'dict' object is not callable\n\nThe above exception was the direct\
    \ cause of the following exception:\n\nTraceback (most recent call last):\n  File\
    \ \"1.token.py\", line 8, in <module>\n    tk.tokenize_data(loom_data_directory,\
    \ output_directory, output_prefix)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
    , line 102, in tokenize_data\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
    \ cell_metadata)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/geneformer/tokenizer.py\"\
    , line 206, in create_dataset\n    output_dataset=Dataset.from_generator(dataset_dict)\n\
    \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/arrow_dataset.py\"\
    , line 1064, in from_generator\n    return GeneratorDatasetInputStream(\n  File\
    \ \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/io/generator.py\"\
    , line 47, in read\n    self.builder.download_and_prepare(\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 954, in download_and_prepare\n    self._download_and_prepare(\n  File \"\
    /jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 1717, in _download_and_prepare\n    super()._download_and_prepare(\n  File\
    \ \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 1049, in _download_and_prepare\n    self._prepare_split(split_generator,\
    \ **prepare_split_kwargs)\n  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 1555, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n\
    \  File \"/jdfssz1/ST_HEALTH/P20Z10200N0015/chenxuan/wzw/cancer/tools/anaconda3/envs/geneformer/lib/python3.8/site-packages/datasets/builder.py\"\
    , line 1712, in _prepare_split_single\n    raise DatasetGenerationError(\"An error\
    \ occurred while generating the dataset\") from e\ndatasets.builder.DatasetGenerationError:\
    \ An error occurred while generating the dataset\n\nWe have decided to send you\
    \ an email to resolve this issue.  thank you for your patience\uFF01"
  created_at: 2023-08-11 16:13:43+00:00
  edited: false
  hidden: false
  id: 64d66c4746bda7d8f5e765bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 207
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: ' Encountered a Program Bug during Tokenization, Error Message: OverflowError:
  There was an overflow with type <class ''list''>. Try to reduce writer_batch_size
  to have batches smaller than 2GB.'
