!!python/object:huggingface_hub.community.DiscussionWithDetails
author: katarinayuan
conflicting_files: null
created_at: 2023-06-24 23:00:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e52676a080f9887cff45bfa36dbe70.svg
      fullname: Xinyu Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katarinayuan
      type: user
    createdAt: '2023-06-25T00:00:10.000Z'
    data:
      edited: false
      editors:
      - katarinayuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8472482562065125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e52676a080f9887cff45bfa36dbe70.svg
          fullname: Xinyu Yuan
          isHf: false
          isPro: false
          name: katarinayuan
          type: user
        html: '<p>Hi,<br>    I noticed that the provided "example_lengths_file" is
          recommended to be passed as argument <code>example_lengths_file</code> in
          <code>GeneformerPretrainer</code>, however the lengths in it are sorted.
          While in <code>CustomDistributedLengthGroupedSampler</code>, the lengths
          should be the length the same order in <code>genecorpus_30M_2048.dataset</code>instead
          of being sorted. Could you help to clarify this? Thank you!</p>

          '
        raw: "Hi,\r\n    I noticed that the provided \"example_lengths_file\" is recommended\
          \ to be passed as argument `example_lengths_file` in `GeneformerPretrainer`,\
          \ however the lengths in it are sorted. While in `CustomDistributedLengthGroupedSampler`,\
          \ the lengths should be the length the same order in `genecorpus_30M_2048.dataset`instead\
          \ of being sorted. Could you help to clarify this? Thank you!"
        updatedAt: '2023-06-25T00:00:10.146Z'
      numEdits: 0
      reactions: []
    id: 6497838a4b1964ff910566d8
    type: comment
  author: katarinayuan
  content: "Hi,\r\n    I noticed that the provided \"example_lengths_file\" is recommended\
    \ to be passed as argument `example_lengths_file` in `GeneformerPretrainer`, however\
    \ the lengths in it are sorted. While in `CustomDistributedLengthGroupedSampler`,\
    \ the lengths should be the length the same order in `genecorpus_30M_2048.dataset`instead\
    \ of being sorted. Could you help to clarify this? Thank you!"
  created_at: 2023-06-24 23:00:10+00:00
  edited: false
  hidden: false
  id: 6497838a4b1964ff910566d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-25T06:13:24.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280614852905273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer and for catching this!
          I added the unsorted lengths file to the datasets repository and updated
          the links in the example for pretraining. For future reference, if you need
          to extract a lengths file from a different .dataset, you can do so as below.
          We previously provided the lengths file pre-extracted from the .dataset
          because it was slow to extract dynamically during training with large datasets,
          but Huggingface has had some updates since then that I believe resolved
          this issue so it''s less necessary now.</p>

          <p>from datasets import load_from_disk<br>genecorpus30m = load_from_disk("/path/to/genecorpus_30M_2048.dataset/")<br>genecorpus30m_lengths  =
          genecorpus30m["length"]</p>

          '
        raw: 'Thank you for your interest in Geneformer and for catching this! I added
          the unsorted lengths file to the datasets repository and updated the links
          in the example for pretraining. For future reference, if you need to extract
          a lengths file from a different .dataset, you can do so as below. We previously
          provided the lengths file pre-extracted from the .dataset because it was
          slow to extract dynamically during training with large datasets, but Huggingface
          has had some updates since then that I believe resolved this issue so it''s
          less necessary now.


          from datasets import load_from_disk

          genecorpus30m = load_from_disk("/path/to/genecorpus_30M_2048.dataset/")

          genecorpus30m_lengths  = genecorpus30m["length"]'
        updatedAt: '2023-06-25T06:13:24.641Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - katarinayuan
      relatedEventId: 6497db04878fffabd80623f7
    id: 6497db04878fffabd80623f5
    type: comment
  author: ctheodoris
  content: 'Thank you for your interest in Geneformer and for catching this! I added
    the unsorted lengths file to the datasets repository and updated the links in
    the example for pretraining. For future reference, if you need to extract a lengths
    file from a different .dataset, you can do so as below. We previously provided
    the lengths file pre-extracted from the .dataset because it was slow to extract
    dynamically during training with large datasets, but Huggingface has had some
    updates since then that I believe resolved this issue so it''s less necessary
    now.


    from datasets import load_from_disk

    genecorpus30m = load_from_disk("/path/to/genecorpus_30M_2048.dataset/")

    genecorpus30m_lengths  = genecorpus30m["length"]'
  created_at: 2023-06-25 05:13:24+00:00
  edited: false
  hidden: false
  id: 6497db04878fffabd80623f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-25T06:13:24.000Z'
    data:
      status: closed
    id: 6497db04878fffabd80623f7
    type: status-change
  author: ctheodoris
  created_at: 2023-06-25 05:13:24+00:00
  id: 6497db04878fffabd80623f7
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-28T18:13:47.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8205918669700623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>If I have a new dataset, how to put the ["length"] attribute into
          the dataset? </p>

          '
        raw: 'If I have a new dataset, how to put the ["length"] attribute into the
          dataset? '
        updatedAt: '2023-06-28T18:13:47.060Z'
      numEdits: 0
      reactions: []
    id: 649c785b4235cd45b25247da
    type: comment
  author: allenxiao
  content: 'If I have a new dataset, how to put the ["length"] attribute into the
    dataset? '
  created_at: 2023-06-28 17:13:47+00:00
  edited: false
  hidden: false
  id: 649c785b4235cd45b25247da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-28T18:30:04.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9196410179138184
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The provided transcriptome tokenizer
          does this for you.</p>

          '
        raw: Thank you for your question. The provided transcriptome tokenizer does
          this for you.
        updatedAt: '2023-06-28T18:30:04.838Z'
      numEdits: 0
      reactions: []
    id: 649c7c2c7f9700d6fa0d9157
    type: comment
  author: ctheodoris
  content: Thank you for your question. The provided transcriptome tokenizer does
    this for you.
  created_at: 2023-06-28 17:30:04+00:00
  edited: false
  hidden: false
  id: 649c7c2c7f9700d6fa0d9157
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-28T18:37:18.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6001244783401489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>I notice "Cells should be labeled with the total read count in the
          cell (column attribute "n_counts")", so we can do like this: TranscriptomeTokenizer({"n_counts":
          "length"}, nproc=4)?</p>

          '
        raw: 'I notice "Cells should be labeled with the total read count in the cell
          (column attribute "n_counts")", so we can do like this: TranscriptomeTokenizer({"n_counts":
          "length"}, nproc=4)?

          '
        updatedAt: '2023-06-28T18:37:18.909Z'
      numEdits: 0
      reactions: []
    id: 649c7ddeb7d38a881cd6ccda
    type: comment
  author: allenxiao
  content: 'I notice "Cells should be labeled with the total read count in the cell
    (column attribute "n_counts")", so we can do like this: TranscriptomeTokenizer({"n_counts":
    "length"}, nproc=4)?

    '
  created_at: 2023-06-28 17:37:18+00:00
  edited: false
  hidden: false
  id: 649c7ddeb7d38a881cd6ccda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-28T18:39:29.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9035443663597107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>No, these are different values. "n_counts" is the total read count
          in the cell. "length" is the length of the rank value encoding (number of
          genes in the final rank value encoding). The "length" will be added automatically
          by the transcriptome tokenizer - you don''t need to do anything to add it.</p>

          '
        raw: No, these are different values. "n_counts" is the total read count in
          the cell. "length" is the length of the rank value encoding (number of genes
          in the final rank value encoding). The "length" will be added automatically
          by the transcriptome tokenizer - you don't need to do anything to add it.
        updatedAt: '2023-06-28T18:39:29.231Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - allenxiao
    id: 649c7e6191241fbd22b5ae34
    type: comment
  author: ctheodoris
  content: No, these are different values. "n_counts" is the total read count in the
    cell. "length" is the length of the rank value encoding (number of genes in the
    final rank value encoding). The "length" will be added automatically by the transcriptome
    tokenizer - you don't need to do anything to add it.
  created_at: 2023-06-28 17:39:29+00:00
  edited: false
  hidden: false
  id: 649c7e6191241fbd22b5ae34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-28T18:56:14.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.909864068031311
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: "<p>Thanks a lot for your amazing fast reply \U0001F604</p>\n"
        raw: "Thanks a lot for your amazing fast reply \U0001F604"
        updatedAt: '2023-06-28T18:56:14.324Z'
      numEdits: 0
      reactions: []
    id: 649c824e4af39cdaf658dbab
    type: comment
  author: allenxiao
  content: "Thanks a lot for your amazing fast reply \U0001F604"
  created_at: 2023-06-28 17:56:14+00:00
  edited: false
  hidden: false
  id: 649c824e4af39cdaf658dbab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-29T10:51:47.000Z'
    data:
      edited: true
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8068549633026123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: "<p>About the tokenizer, \"Genes should be labeled with Ensembl IDs\
          \ (row attribute \"ensembl_id\"), which provide a unique identifer for conversion\
          \ to tokens. Cells should be labeled with the total read count in the cell\
          \ (column attribute \"n_counts\") to be used for normalization.\"<br>Could\
          \ genes be labeled with gene names instead of Ensembl IDs? And the token_dictionary.pkl\
          \ file may be as follows:<br>{'&lt;pad&gt;': 0, '&lt;mask&gt;': 1, gene_name1:\
          \ 2,  gene_name2: 3, \u2026.gene_name100: 101}<br>Would this change influence\
          \ the tokenizer's computation?<br>Thank you very much for your patient and\
          \ fast reply.</p>\n"
        raw: "About the tokenizer, \"Genes should be labeled with Ensembl IDs (row\
          \ attribute \"ensembl_id\"), which provide a unique identifer for conversion\
          \ to tokens. Cells should be labeled with the total read count in the cell\
          \ (column attribute \"n_counts\") to be used for normalization.\"\nCould\
          \ genes be labeled with gene names instead of Ensembl IDs? And the token_dictionary.pkl\
          \ file may be as follows:\n{'\\<pad\\>': 0, '\\<mask\\>': 1, gene_name1:\
          \ 2,  gene_name2: 3, \u2026.gene_name100: 101}\nWould this change influence\
          \ the tokenizer's computation?\nThank you very much for your patient and\
          \ fast reply."
        updatedAt: '2023-06-29T10:53:16.173Z'
      numEdits: 2
      reactions: []
    id: 649d624358f002d65fe6dd76
    type: comment
  author: allenxiao
  content: "About the tokenizer, \"Genes should be labeled with Ensembl IDs (row attribute\
    \ \"ensembl_id\"), which provide a unique identifer for conversion to tokens.\
    \ Cells should be labeled with the total read count in the cell (column attribute\
    \ \"n_counts\") to be used for normalization.\"\nCould genes be labeled with gene\
    \ names instead of Ensembl IDs? And the token_dictionary.pkl file may be as follows:\n\
    {'\\<pad\\>': 0, '\\<mask\\>': 1, gene_name1: 2,  gene_name2: 3, \u2026.gene_name100:\
    \ 101}\nWould this change influence the tokenizer's computation?\nThank you very\
    \ much for your patient and fast reply."
  created_at: 2023-06-29 09:51:47+00:00
  edited: true
  hidden: false
  id: 649d624358f002d65fe6dd76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-29T13:59:20.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9156180024147034
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. Please convert the gene names in your
          dataset to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the
          provided instructions. We use Ensembl IDs so that they are unique and consistent,
          while gene names arise from various naming schemes. If you convert the token
          dictionary instead of your dataset, you risk changing a token to be a different
          gene then what it was trained as, and also the code expects this dictionary
          to be in the current format in various places so you would have to change
          many parts of the code to accept the new format. </p>

          '
        raw: 'Thank you for your question. Please convert the gene names in your dataset
          to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the provided
          instructions. We use Ensembl IDs so that they are unique and consistent,
          while gene names arise from various naming schemes. If you convert the token
          dictionary instead of your dataset, you risk changing a token to be a different
          gene then what it was trained as, and also the code expects this dictionary
          to be in the current format in various places so you would have to change
          many parts of the code to accept the new format. '
        updatedAt: '2023-06-29T13:59:20.888Z'
      numEdits: 0
      reactions: []
    id: 649d8e38c10f40d26c0b5d57
    type: comment
  author: ctheodoris
  content: 'Thank you for your question. Please convert the gene names in your dataset
    to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the provided instructions.
    We use Ensembl IDs so that they are unique and consistent, while gene names arise
    from various naming schemes. If you convert the token dictionary instead of your
    dataset, you risk changing a token to be a different gene then what it was trained
    as, and also the code expects this dictionary to be in the current format in various
    places so you would have to change many parts of the code to accept the new format. '
  created_at: 2023-06-29 12:59:20+00:00
  edited: false
  hidden: false
  id: 649d8e38c10f40d26c0b5d57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-29T18:45:43.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752151370048523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: "<p>When I changed the sorted version into the unsorted version, I found\
          \ an interesting fact that after 3 epochs the loss of the unsorted one was\
          \ bigger than the sorted one. Why this happened? I think the unsorted version\
          \ may be closer to the actual situation. If I misunderstand, forgive me\
          \ please...<br>However, another fact that the unsorted version only took\
          \ half time was amazing! The two versions' format is the same, and the only\
          \ difference is the order, even their file sizes are the same as well, so\
          \ why did they take so much different time?<br>I really appreciate you responding\
          \ so quickly every time O(\u2229_\u2229)O~</p>\n"
        raw: "When I changed the sorted version into the unsorted version, I found\
          \ an interesting fact that after 3 epochs the loss of the unsorted one was\
          \ bigger than the sorted one. Why this happened? I think the unsorted version\
          \ may be closer to the actual situation. If I misunderstand, forgive me\
          \ please... \nHowever, another fact that the unsorted version only took\
          \ half time was amazing! The two versions' format is the same, and the only\
          \ difference is the order, even their file sizes are the same as well, so\
          \ why did they take so much different time?\nI really appreciate you responding\
          \ so quickly every time O(\u2229_\u2229)O~"
        updatedAt: '2023-06-29T18:45:43.334Z'
      numEdits: 0
      reactions: []
    id: 649dd157d643074934a4fd89
    type: comment
  author: allenxiao
  content: "When I changed the sorted version into the unsorted version, I found an\
    \ interesting fact that after 3 epochs the loss of the unsorted one was bigger\
    \ than the sorted one. Why this happened? I think the unsorted version may be\
    \ closer to the actual situation. If I misunderstand, forgive me please... \n\
    However, another fact that the unsorted version only took half time was amazing!\
    \ The two versions' format is the same, and the only difference is the order,\
    \ even their file sizes are the same as well, so why did they take so much different\
    \ time?\nI really appreciate you responding so quickly every time O(\u2229_\u2229\
    )O~"
  created_at: 2023-06-29 17:45:43+00:00
  edited: false
  hidden: false
  id: 649dd157d643074934a4fd89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-29T19:53:12.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.962415874004364
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The length-grouped training improves
          the speed of training, as discussed in the manuscript, so using the matched
          lengths likely sped up the training by leading to better length-grouping
          than random, which would be the result of non-matched lengths. In our experience
          the model performance is essentially identical when using different seeds,
          etc., as shown in the manuscript. I would suggest you zoom out to see the
          entire training loss plot to confirm that the pattern is the same in terms
          of smooth improvement in loss and to confirm it''s not just a scaling issue
          of the y axis. We provide the pretrained model in this repository so that
          users do not need to retrain it from scratch.</p>

          '
        raw: Thank you for your question. The length-grouped training improves the
          speed of training, as discussed in the manuscript, so using the matched
          lengths likely sped up the training by leading to better length-grouping
          than random, which would be the result of non-matched lengths. In our experience
          the model performance is essentially identical when using different seeds,
          etc., as shown in the manuscript. I would suggest you zoom out to see the
          entire training loss plot to confirm that the pattern is the same in terms
          of smooth improvement in loss and to confirm it's not just a scaling issue
          of the y axis. We provide the pretrained model in this repository so that
          users do not need to retrain it from scratch.
        updatedAt: '2023-06-29T19:53:12.273Z'
      numEdits: 0
      reactions: []
    id: 649de128c36e24ac5d6673a3
    type: comment
  author: ctheodoris
  content: Thank you for your question. The length-grouped training improves the speed
    of training, as discussed in the manuscript, so using the matched lengths likely
    sped up the training by leading to better length-grouping than random, which would
    be the result of non-matched lengths. In our experience the model performance
    is essentially identical when using different seeds, etc., as shown in the manuscript.
    I would suggest you zoom out to see the entire training loss plot to confirm that
    the pattern is the same in terms of smooth improvement in loss and to confirm
    it's not just a scaling issue of the y axis. We provide the pretrained model in
    this repository so that users do not need to retrain it from scratch.
  created_at: 2023-06-29 18:53:12+00:00
  edited: false
  hidden: false
  id: 649de128c36e24ac5d6673a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e52676a080f9887cff45bfa36dbe70.svg
      fullname: Xinyu Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katarinayuan
      type: user
    createdAt: '2023-06-29T19:59:24.000Z'
    data:
      edited: true
      editors:
      - katarinayuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710937738418579
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e52676a080f9887cff45bfa36dbe70.svg
          fullname: Xinyu Yuan
          isHf: false
          isPro: false
          name: katarinayuan
          type: user
        html: "<p>Hi, nice to see the discussion here. Just wanna mention something.\
          \ I have similar observation as <span data-props=\"{&quot;user&quot;:&quot;allenxiao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/allenxiao\"\
          >@<span class=\"underline\">allenxiao</span></a></span>\n\n\t</span></span>\
          \ did, that converged loss when pretrained with matched lengths is larger\
          \ than that when pretrained with unmatched lengths. I confirm the steps\
          \ of loss curves are aligned so it's unlikely to be the problem of scaling\
          \ issue of curves. Any guess on why it happens?</p>\n"
        raw: Hi, nice to see the discussion here. Just wanna mention something. I
          have similar observation as @allenxiao did, that converged loss when pretrained
          with matched lengths is larger than that when pretrained with unmatched
          lengths. I confirm the steps of loss curves are aligned so it's unlikely
          to be the problem of scaling issue of curves. Any guess on why it happens?
        updatedAt: '2023-06-29T20:01:12.227Z'
      numEdits: 5
      reactions: []
    id: 649de29c1b00eaf1eea3de70
    type: comment
  author: katarinayuan
  content: Hi, nice to see the discussion here. Just wanna mention something. I have
    similar observation as @allenxiao did, that converged loss when pretrained with
    matched lengths is larger than that when pretrained with unmatched lengths. I
    confirm the steps of loss curves are aligned so it's unlikely to be the problem
    of scaling issue of curves. Any guess on why it happens?
  created_at: 2023-06-29 18:59:24+00:00
  edited: true
  hidden: false
  id: 649de29c1b00eaf1eea3de70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-29T22:14:14.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9545059204101562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thanks for your comment. If you look in the pretrainer module, you\
          \ can see where the variable \"lengths\" is used. More randomization may\
          \ generally lead to improved pretraining, though in our experience in subsets\
          \ of the data it did not make a significant difference to warrant the slowdown\
          \ of training. It also could be differentially affected by the batch size\
          \ used (i.e. better randomization has bigger effect dependent on batch size)\
          \ - I\u2019m not sure if you changed that based on your resources. I also\
          \ don\u2019t know how significant the difference in loss was for you and\
          \ whether this translates to improved performance in downstream tasks or\
          \ not.</p>\n"
        raw: "Thanks for your comment. If you look in the pretrainer module, you can\
          \ see where the variable \"lengths\" is used. More randomization may generally\
          \ lead to improved pretraining, though in our experience in subsets of the\
          \ data it did not make a significant difference to warrant the slowdown\
          \ of training. It also could be differentially affected by the batch size\
          \ used (i.e. better randomization has bigger effect dependent on batch size)\
          \ - I\u2019m not sure if you changed that based on your resources. I also\
          \ don\u2019t know how significant the difference in loss was for you and\
          \ whether this translates to improved performance in downstream tasks or\
          \ not."
        updatedAt: '2023-06-29T22:14:14.744Z'
      numEdits: 0
      reactions: []
    id: 649e02368f9890dcc25f5dc6
    type: comment
  author: ctheodoris
  content: "Thanks for your comment. If you look in the pretrainer module, you can\
    \ see where the variable \"lengths\" is used. More randomization may generally\
    \ lead to improved pretraining, though in our experience in subsets of the data\
    \ it did not make a significant difference to warrant the slowdown of training.\
    \ It also could be differentially affected by the batch size used (i.e. better\
    \ randomization has bigger effect dependent on batch size) - I\u2019m not sure\
    \ if you changed that based on your resources. I also don\u2019t know how significant\
    \ the difference in loss was for you and whether this translates to improved performance\
    \ in downstream tasks or not."
  created_at: 2023-06-29 21:14:14+00:00
  edited: false
  hidden: false
  id: 649e02368f9890dcc25f5dc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-07-02T19:40:25.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9303063750267029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<blockquote>

          <p>Thank you for your question. Please convert the gene names in your dataset
          to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the provided
          instructions. We use Ensembl IDs so that they are unique and consistent,
          while gene names arise from various naming schemes. If you convert the token
          dictionary instead of your dataset, you risk changing a token to be a different
          gene then what it was trained as, and also the code expects this dictionary
          to be in the current format in various places so you would have to change
          many parts of the code to accept the new format.</p>

          </blockquote>

          <p>When converting the gene names to Ensembl IDs for some datasets, we found
          some IDs have been retired. In this situation, should we abandon the retired
          gene or just use the retired ID? If we use retired IDs in the model, would
          it have bad impact for the model training?<br>Thank you for your reply.</p>

          '
        raw: "> Thank you for your question. Please convert the gene names in your\
          \ dataset to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the\
          \ provided instructions. We use Ensembl IDs so that they are unique and\
          \ consistent, while gene names arise from various naming schemes. If you\
          \ convert the token dictionary instead of your dataset, you risk changing\
          \ a token to be a different gene then what it was trained as, and also the\
          \ code expects this dictionary to be in the current format in various places\
          \ so you would have to change many parts of the code to accept the new format.\n\
          \nWhen converting the gene names to Ensembl IDs for some datasets, we found\
          \ some IDs have been retired. In this situation, should we abandon the retired\
          \ gene or just use the retired ID? If we use retired IDs in the model, would\
          \ it have bad impact for the model training? \nThank you for your reply."
        updatedAt: '2023-07-02T19:40:25.737Z'
      numEdits: 0
      reactions: []
    id: 64a1d2a9f7fb82324686b139
    type: comment
  author: allenxiao
  content: "> Thank you for your question. Please convert the gene names in your dataset\
    \ to Ensembl IDs (e.g. using Ensembl Biomart) as indicated in the provided instructions.\
    \ We use Ensembl IDs so that they are unique and consistent, while gene names\
    \ arise from various naming schemes. If you convert the token dictionary instead\
    \ of your dataset, you risk changing a token to be a different gene then what\
    \ it was trained as, and also the code expects this dictionary to be in the current\
    \ format in various places so you would have to change many parts of the code\
    \ to accept the new format.\n\nWhen converting the gene names to Ensembl IDs for\
    \ some datasets, we found some IDs have been retired. In this situation, should\
    \ we abandon the retired gene or just use the retired ID? If we use retired IDs\
    \ in the model, would it have bad impact for the model training? \nThank you for\
    \ your reply."
  created_at: 2023-07-02 18:40:25+00:00
  edited: false
  hidden: false
  id: 64a1d2a9f7fb82324686b139
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-03T07:09:00.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9673028588294983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. I cannot tell if you mean that your
          dataset has genes that are no longer within the annotations of Ensembl or
          if you mean that the token dictionary has retired IDs. The purpose of converting
          the gene names to Ensembl IDs is just to arrive at a common consistent and
          unique annotation for the tokenizer to use to convert the genes to tokens
          for use with the model. If the Ensembl IDs are not within the provided token
          dictionary, then those genes were not within the vocabulary of the pretrained
          model. This can happen with genes that were not detected in any of the 30
          million cells used for pretraining and also with genes that were not protein-coding
          or miRNA genes, since that was our inclusion criteria. These genes can be
          added to the vocabulary but were just not pretrained. If you present the
          model with additional data during fine-tuning that contains these genes,
          then it will learn from that data.</p>

          '
        raw: Thank you for your question. I cannot tell if you mean that your dataset
          has genes that are no longer within the annotations of Ensembl or if you
          mean that the token dictionary has retired IDs. The purpose of converting
          the gene names to Ensembl IDs is just to arrive at a common consistent and
          unique annotation for the tokenizer to use to convert the genes to tokens
          for use with the model. If the Ensembl IDs are not within the provided token
          dictionary, then those genes were not within the vocabulary of the pretrained
          model. This can happen with genes that were not detected in any of the 30
          million cells used for pretraining and also with genes that were not protein-coding
          or miRNA genes, since that was our inclusion criteria. These genes can be
          added to the vocabulary but were just not pretrained. If you present the
          model with additional data during fine-tuning that contains these genes,
          then it will learn from that data.
        updatedAt: '2023-07-03T07:09:00.579Z'
      numEdits: 0
      reactions: []
    id: 64a2740c2925c3f551b269da
    type: comment
  author: ctheodoris
  content: Thank you for your question. I cannot tell if you mean that your dataset
    has genes that are no longer within the annotations of Ensembl or if you mean
    that the token dictionary has retired IDs. The purpose of converting the gene
    names to Ensembl IDs is just to arrive at a common consistent and unique annotation
    for the tokenizer to use to convert the genes to tokens for use with the model.
    If the Ensembl IDs are not within the provided token dictionary, then those genes
    were not within the vocabulary of the pretrained model. This can happen with genes
    that were not detected in any of the 30 million cells used for pretraining and
    also with genes that were not protein-coding or miRNA genes, since that was our
    inclusion criteria. These genes can be added to the vocabulary but were just not
    pretrained. If you present the model with additional data during fine-tuning that
    contains these genes, then it will learn from that data.
  created_at: 2023-07-03 06:09:00+00:00
  edited: false
  hidden: false
  id: 64a2740c2925c3f551b269da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-07-03T08:42:15.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955190896987915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>Sorry, I didn''t make myself clear. I mean,  for the purpose of
          pretraining with my own dataset (not the Geneformer''s 30 million cells)
          , when converting gene names to Ensembl IDs, there are some genes no longer
          within the annotations of Ensembl''s latest version. For example, one gene
          ID was included in GRCh37, but when GRCh38 released, it was no longer included
          in GRCh38, so it''s retired. However, cells'' expression data related to
          the retired gene still exist, because the collected dataset is organized
          in unit of gene names. So could I use the old version Ensembl IDs (converted
          from gene names) for model pretraining? If yes, the dataset may have genes
          from both GRCh37 and GRCh38, and would it be allowed in the Geneformer''s
          pretraining schema regarding the consistency( "The purpose of converting
          the gene names to Ensembl IDs is just to arrive at a common consistent and
          unique annotation for the tokenizer " )? Despite that, I think the uniqueness
          for gene ID annotation is satisfied.<br>Thank you for the great tool and
          your reply.</p>

          '
        raw: 'Sorry, I didn''t make myself clear. I mean,  for the purpose of pretraining
          with my own dataset (not the Geneformer''s 30 million cells) , when converting
          gene names to Ensembl IDs, there are some genes no longer within the annotations
          of Ensembl''s latest version. For example, one gene ID was included in GRCh37,
          but when GRCh38 released, it was no longer included in GRCh38, so it''s
          retired. However, cells'' expression data related to the retired gene still
          exist, because the collected dataset is organized in unit of gene names.
          So could I use the old version Ensembl IDs (converted from gene names) for
          model pretraining? If yes, the dataset may have genes from both GRCh37 and
          GRCh38, and would it be allowed in the Geneformer''s pretraining schema
          regarding the consistency( "The purpose of converting the gene names to
          Ensembl IDs is just to arrive at a common consistent and unique annotation
          for the tokenizer " )? Despite that, I think the uniqueness for gene ID
          annotation is satisfied.

          Thank you for the great tool and your reply.'
        updatedAt: '2023-07-03T08:42:15.769Z'
      numEdits: 0
      reactions: []
    id: 64a289e7b3762e5002f2f88d
    type: comment
  author: allenxiao
  content: 'Sorry, I didn''t make myself clear. I mean,  for the purpose of pretraining
    with my own dataset (not the Geneformer''s 30 million cells) , when converting
    gene names to Ensembl IDs, there are some genes no longer within the annotations
    of Ensembl''s latest version. For example, one gene ID was included in GRCh37,
    but when GRCh38 released, it was no longer included in GRCh38, so it''s retired.
    However, cells'' expression data related to the retired gene still exist, because
    the collected dataset is organized in unit of gene names. So could I use the old
    version Ensembl IDs (converted from gene names) for model pretraining? If yes,
    the dataset may have genes from both GRCh37 and GRCh38, and would it be allowed
    in the Geneformer''s pretraining schema regarding the consistency( "The purpose
    of converting the gene names to Ensembl IDs is just to arrive at a common consistent
    and unique annotation for the tokenizer " )? Despite that, I think the uniqueness
    for gene ID annotation is satisfied.

    Thank you for the great tool and your reply.'
  created_at: 2023-07-03 07:42:15+00:00
  edited: false
  hidden: false
  id: 64a289e7b3762e5002f2f88d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-03T22:25:48.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9545238614082336
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for the additional information. If I understand correctly,
          from a pure computational standpoint this will not pose an issue for the
          pretraining as long the same transcript is not labeled as two different
          Ensembl IDs depending on what genome assembly was used for the original
          data preprocessing. However, from a biological standpoint, it would be prudent
          to determine the reasons why the genes were retired as this may provide
          information on whether it would be best to exclude them or not.</p>

          '
        raw: Thank you for the additional information. If I understand correctly,
          from a pure computational standpoint this will not pose an issue for the
          pretraining as long the same transcript is not labeled as two different
          Ensembl IDs depending on what genome assembly was used for the original
          data preprocessing. However, from a biological standpoint, it would be prudent
          to determine the reasons why the genes were retired as this may provide
          information on whether it would be best to exclude them or not.
        updatedAt: '2023-07-03T22:25:48.493Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - allenxiao
    id: 64a34aec275825d2c9a42807
    type: comment
  author: ctheodoris
  content: Thank you for the additional information. If I understand correctly, from
    a pure computational standpoint this will not pose an issue for the pretraining
    as long the same transcript is not labeled as two different Ensembl IDs depending
    on what genome assembly was used for the original data preprocessing. However,
    from a biological standpoint, it would be prudent to determine the reasons why
    the genes were retired as this may provide information on whether it would be
    best to exclude them or not.
  created_at: 2023-07-03 21:25:48+00:00
  edited: false
  hidden: false
  id: 64a34aec275825d2c9a42807
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 61
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Question for mis-alignment between example_lengths_file and genecorpus_30M_2048.dataset
