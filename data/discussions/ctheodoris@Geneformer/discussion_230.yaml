!!python/object:huggingface_hub.community.DiscussionWithDetails
author: swang12
conflicting_files: null
created_at: 2023-08-22 19:31:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dfad9b282f034de3ccdaf7ab2b999559.svg
      fullname: Su Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swang12
      type: user
    createdAt: '2023-08-22T20:31:14.000Z'
    data:
      edited: false
      editors:
      - swang12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.628765344619751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dfad9b282f034de3ccdaf7ab2b999559.svg
          fullname: Su Wang
          isHf: false
          isPro: false
          name: swang12
          type: user
        html: "<p>Hi, I was running InSilicoPerturber for each gene one by one in\
          \ a loop and I got torch.cuda.OutOfMemoryError for the first gene. I reduced\
          \ the number of cells to 200 but this didn't resolve the issue. In comparison,\
          \ previously when I ran InSilicoPerturber for all genes with 2,000 cells,\
          \ I didn't have this issue. I'm pasting the code and error message below:</p>\n\
          <p>for gene in tqdm(genes_to_perturb):<br>    output_directory = f'{root_directory}{gene}/'<br>\
          \    os.mkdir(output_directory)</p>\n<pre><code>shutil.copytree('tokenized_.dataset',\
          \ 'tokenized_copy.dataset')\n\nisp = InSilicoPerturber(perturb_type='delete',\
          \ \n                        perturb_rank_shift=None, \n                \
          \        genes_to_perturb=[gene], \n                        combos=0, \n\
          \                        anchor_gene=None, \n                        model_type='Pretrained',\
          \ \n                        num_classes=0, \n                        emb_mode='cell_and_gene',\
          \ \n                        cell_emb_style='mean_pool', \n             \
          \           cell_states_to_model=None, \n                        max_ncells=200,\
          \ \n                        emb_layer=-1, \n                        forward_batch_size=32,\
          \ \n                        nproc=16\n                        )\n\nisp.perturb_data(model_directory='Geneformer/',\
          \ \n                input_data_file='tokenized_copy.dataset', \n       \
          \         output_directory=output_directory, \n                output_prefix='perturbed'\n\
          \                )\n\nispstats = InSilicoPerturberStats(mode='mixture_model',\
          \ \n                                combos=0, \n                       \
          \         anchor_gene=None, \n                                cell_states_to_model=None\n\
          \                                )\n\nispstats.get_stats(input_data_directory=output_directory,\n\
          \                null_dist_data_directory=None,\n                output_directory=output_directory,\n\
          \                output_prefix='stats_emb_mode_gene'\n                )\n\
          \nshutil.rmtree('tokenized_copy.dataset')\n\ngc.collect()\ntorch.cuda.empty_cache()\n\
          </code></pre>\n<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
          \ to allocate 2.00 GiB (GPU 0; 31.74 GiB total capacity; 7.07 GiB already\
          \ allocated; 1.25 GiB free; 7.08 GiB reserved in total by PyTorch) If reserved\
          \ memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid\
          \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n\
          <p>I was wondering if you could help me resolve this issue. Thank you very\
          \ much!</p>\n<p>Sincerely,<br>Su Wang</p>\n"
        raw: "Hi, I was running InSilicoPerturber for each gene one by one in a loop\
          \ and I got torch.cuda.OutOfMemoryError for the first gene. I reduced the\
          \ number of cells to 200 but this didn't resolve the issue. In comparison,\
          \ previously when I ran InSilicoPerturber for all genes with 2,000 cells,\
          \ I didn't have this issue. I'm pasting the code and error message below:\r\
          \n\r\nfor gene in tqdm(genes_to_perturb):\r\n    output_directory = f'{root_directory}{gene}/'\r\
          \n    os.mkdir(output_directory)\r\n\r\n    shutil.copytree('tokenized_.dataset',\
          \ 'tokenized_copy.dataset')\r\n\r\n    isp = InSilicoPerturber(perturb_type='delete',\
          \ \r\n                            perturb_rank_shift=None, \r\n        \
          \                    genes_to_perturb=[gene], \r\n                     \
          \       combos=0, \r\n                            anchor_gene=None, \r\n\
          \                            model_type='Pretrained', \r\n             \
          \               num_classes=0, \r\n                            emb_mode='cell_and_gene',\
          \ \r\n                            cell_emb_style='mean_pool', \r\n     \
          \                       cell_states_to_model=None, \r\n                \
          \            max_ncells=200, \r\n                            emb_layer=-1,\
          \ \r\n                            forward_batch_size=32, \r\n          \
          \                  nproc=16\r\n                            )\r\n\r\n   \
          \ isp.perturb_data(model_directory='Geneformer/', \r\n                 \
          \   input_data_file='tokenized_copy.dataset', \r\n                    output_directory=output_directory,\
          \ \r\n                    output_prefix='perturbed'\r\n                \
          \    )\r\n\r\n    ispstats = InSilicoPerturberStats(mode='mixture_model',\
          \ \r\n                                    combos=0, \r\n               \
          \                     anchor_gene=None, \r\n                           \
          \         cell_states_to_model=None\r\n                                \
          \    )\r\n\r\n    ispstats.get_stats(input_data_directory=output_directory,\r\
          \n                    null_dist_data_directory=None,\r\n               \
          \     output_directory=output_directory,\r\n                    output_prefix='stats_emb_mode_gene'\r\
          \n                    )\r\n    \r\n    shutil.rmtree('tokenized_copy.dataset')\r\
          \n    \r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 31.74 GiB total\
          \ capacity; 7.07 GiB already allocated; 1.25 GiB free; 7.08 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nI was wondering if you could\
          \ help me resolve this issue. Thank you very much!\r\n\r\nSincerely,\r\n\
          Su Wang"
        updatedAt: '2023-08-22T20:31:14.393Z'
      numEdits: 0
      reactions: []
    id: 64e51b120520ffd9184a1c6a
    type: comment
  author: swang12
  content: "Hi, I was running InSilicoPerturber for each gene one by one in a loop\
    \ and I got torch.cuda.OutOfMemoryError for the first gene. I reduced the number\
    \ of cells to 200 but this didn't resolve the issue. In comparison, previously\
    \ when I ran InSilicoPerturber for all genes with 2,000 cells, I didn't have this\
    \ issue. I'm pasting the code and error message below:\r\n\r\nfor gene in tqdm(genes_to_perturb):\r\
    \n    output_directory = f'{root_directory}{gene}/'\r\n    os.mkdir(output_directory)\r\
    \n\r\n    shutil.copytree('tokenized_.dataset', 'tokenized_copy.dataset')\r\n\r\
    \n    isp = InSilicoPerturber(perturb_type='delete', \r\n                    \
    \        perturb_rank_shift=None, \r\n                            genes_to_perturb=[gene],\
    \ \r\n                            combos=0, \r\n                            anchor_gene=None,\
    \ \r\n                            model_type='Pretrained', \r\n              \
    \              num_classes=0, \r\n                            emb_mode='cell_and_gene',\
    \ \r\n                            cell_emb_style='mean_pool', \r\n           \
    \                 cell_states_to_model=None, \r\n                            max_ncells=200,\
    \ \r\n                            emb_layer=-1, \r\n                         \
    \   forward_batch_size=32, \r\n                            nproc=16\r\n      \
    \                      )\r\n\r\n    isp.perturb_data(model_directory='Geneformer/',\
    \ \r\n                    input_data_file='tokenized_copy.dataset', \r\n     \
    \               output_directory=output_directory, \r\n                    output_prefix='perturbed'\r\
    \n                    )\r\n\r\n    ispstats = InSilicoPerturberStats(mode='mixture_model',\
    \ \r\n                                    combos=0, \r\n                     \
    \               anchor_gene=None, \r\n                                    cell_states_to_model=None\r\
    \n                                    )\r\n\r\n    ispstats.get_stats(input_data_directory=output_directory,\r\
    \n                    null_dist_data_directory=None,\r\n                    output_directory=output_directory,\r\
    \n                    output_prefix='stats_emb_mode_gene'\r\n                \
    \    )\r\n    \r\n    shutil.rmtree('tokenized_copy.dataset')\r\n    \r\n    gc.collect()\r\
    \n    torch.cuda.empty_cache()\r\n\r\ntorch.cuda.OutOfMemoryError: CUDA out of\
    \ memory. Tried to allocate 2.00 GiB (GPU 0; 31.74 GiB total capacity; 7.07 GiB\
    \ already allocated; 1.25 GiB free; 7.08 GiB reserved in total by PyTorch) If\
    \ reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n\r\nI was wondering if you could help me resolve this issue. Thank you very\
    \ much!\r\n\r\nSincerely,\r\nSu Wang"
  created_at: 2023-08-22 19:31:14+00:00
  edited: false
  hidden: false
  id: 64e51b120520ffd9184a1c6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ecd4d674b85b6b6038b47181966e7b53.svg
      fullname: Julie Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: junguyen
      type: user
    createdAt: '2023-08-30T14:31:36.000Z'
    data:
      edited: false
      editors:
      - junguyen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.870768666267395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ecd4d674b85b6b6038b47181966e7b53.svg
          fullname: Julie Nguyen
          isHf: false
          isPro: false
          name: junguyen
          type: user
        html: '<p>I''ve also been running into this issue lately where I receive the
          same error message when running with large <code>forward_batch_size</code>.
          When I reduce <code>forward_batch_size=10</code>, I receive a new error
          message:</p>

          <pre><code>RuntimeError: Sizes of tensors must match except in dimension
          0. Expected size 2047 but got size 2004 for tensor number 7 in the list.

          </code></pre>

          <p>However, I''m thinking this may be related to Discussion <a href="/ctheodoris/Geneformer/discussions/85">#85</a>.
          Have you tried changing <code>emb_mode=''gene''</code>?</p>

          <p>A couple of things I''ve tried but haven''t worked:</p>

          <ol>

          <li>Setting <code>max_ncells=50</code> and <code>forward_batch_size=50</code></li>

          <li><code>torch.cuda.empty_cache()</code> and <code>gc.collect()</code>,
          as shown above</li>

          <li>Modifying <code>in_silico_perturber.py</code> to clear the memory every
          500 cells instead of every 1000 cells</li>

          <li>Setting <code>os.environ["PYTORCH_CUDA_ALLOC_CONF"]="max_split_size_mb:
          512"</code></li>

          <li>Restarting my Python session and only loading in necessary objects</li>

          </ol>

          <p>I make sure to have the latest version of Geneformer pulled every time
          as well. Open to any suggestions anyone has!</p>

          '
        raw: 'I''ve also been running into this issue lately where I receive the same
          error message when running with large `forward_batch_size`. When I reduce
          `forward_batch_size=10`, I receive a new error message:

          ```

          RuntimeError: Sizes of tensors must match except in dimension 0. Expected
          size 2047 but got size 2004 for tensor number 7 in the list.

          ```

          However, I''m thinking this may be related to Discussion #85. Have you tried
          changing `emb_mode=''gene''`?


          A couple of things I''ve tried but haven''t worked:

          1. Setting `max_ncells=50` and `forward_batch_size=50`

          2. `torch.cuda.empty_cache()` and `gc.collect()`, as shown above

          3. Modifying `in_silico_perturber.py` to clear the memory every 500 cells
          instead of every 1000 cells

          4. Setting `os.environ["PYTORCH_CUDA_ALLOC_CONF"]="max_split_size_mb: 512"`

          5. Restarting my Python session and only loading in necessary objects


          I make sure to have the latest version of Geneformer pulled every time as
          well. Open to any suggestions anyone has!'
        updatedAt: '2023-08-30T14:31:36.036Z'
      numEdits: 0
      reactions: []
    id: 64ef52c855eb616c1bee04f8
    type: comment
  author: junguyen
  content: 'I''ve also been running into this issue lately where I receive the same
    error message when running with large `forward_batch_size`. When I reduce `forward_batch_size=10`,
    I receive a new error message:

    ```

    RuntimeError: Sizes of tensors must match except in dimension 0. Expected size
    2047 but got size 2004 for tensor number 7 in the list.

    ```

    However, I''m thinking this may be related to Discussion #85. Have you tried changing
    `emb_mode=''gene''`?


    A couple of things I''ve tried but haven''t worked:

    1. Setting `max_ncells=50` and `forward_batch_size=50`

    2. `torch.cuda.empty_cache()` and `gc.collect()`, as shown above

    3. Modifying `in_silico_perturber.py` to clear the memory every 500 cells instead
    of every 1000 cells

    4. Setting `os.environ["PYTORCH_CUDA_ALLOC_CONF"]="max_split_size_mb: 512"`

    5. Restarting my Python session and only loading in necessary objects


    I make sure to have the latest version of Geneformer pulled every time as well.
    Open to any suggestions anyone has!'
  created_at: 2023-08-30 13:31:36+00:00
  edited: false
  hidden: false
  id: 64ef52c855eb616c1bee04f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-18T07:20:10.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338710308074951
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! A few notes:</p>

          <ul>

          <li><p>if you are running the analysis for many genes, it will likely be
          more efficient to run it with the genes_to_perturb="all" option because
          the batches will be run as all genes for an individual cell, which will
          omit the need to pad for variable size cells so that there is no computation
          spent on padding. </p>

          </li>

          <li><p>when you run the mode as "cell_and_gene", gene embeddings are also
          outputted in addition to cell embeddings, which increases the memory requirements</p>

          </li>

          <li><p>you are just barely out of memory so by decreasing the batch size
          a bit more from 32 you may be able to fit your analysis on the GPU. I would
          suggest successively reducing the batch sizes to choose the largest possible
          without running out of memory. The cells are sorted to encounter memory
          errors sooner so the initial batch size should work for the remainder.</p>

          </li>

          <li><p>emptying the cache of GPUs is often non-trivial. Rather than running
          each gene as a loop, I would suggest you run a script with xargs, for example,
          so that the GPU is completely reset between runs.</p>

          </li>

          <li><p>if you are using the 12 layer model, you may consider using the 6
          layer model (outer directory of this repository), which will be more memory-efficient.</p>

          </li>

          </ul>

          '
        raw: "Thank you for your interest in Geneformer! A few notes:\n\n- if you\
          \ are running the analysis for many genes, it will likely be more efficient\
          \ to run it with the genes_to_perturb=\"all\" option because the batches\
          \ will be run as all genes for an individual cell, which will omit the need\
          \ to pad for variable size cells so that there is no computation spent on\
          \ padding. \n\n- when you run the mode as \"cell_and_gene\", gene embeddings\
          \ are also outputted in addition to cell embeddings, which increases the\
          \ memory requirements\n\n- you are just barely out of memory so by decreasing\
          \ the batch size a bit more from 32 you may be able to fit your analysis\
          \ on the GPU. I would suggest successively reducing the batch sizes to choose\
          \ the largest possible without running out of memory. The cells are sorted\
          \ to encounter memory errors sooner so the initial batch size should work\
          \ for the remainder.\n\n- emptying the cache of GPUs is often non-trivial.\
          \ Rather than running each gene as a loop, I would suggest you run a script\
          \ with xargs, for example, so that the GPU is completely reset between runs.\n\
          \n- if you are using the 12 layer model, you may consider using the 6 layer\
          \ model (outer directory of this repository), which will be more memory-efficient."
        updatedAt: '2023-09-18T07:20:10.915Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - junguyen
      relatedEventId: 6507fa2a7de4710baf0ae5ea
    id: 6507fa2a7de4710baf0ae5e8
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer! A few notes:\n\n- if you are\
    \ running the analysis for many genes, it will likely be more efficient to run\
    \ it with the genes_to_perturb=\"all\" option because the batches will be run\
    \ as all genes for an individual cell, which will omit the need to pad for variable\
    \ size cells so that there is no computation spent on padding. \n\n- when you\
    \ run the mode as \"cell_and_gene\", gene embeddings are also outputted in addition\
    \ to cell embeddings, which increases the memory requirements\n\n- you are just\
    \ barely out of memory so by decreasing the batch size a bit more from 32 you\
    \ may be able to fit your analysis on the GPU. I would suggest successively reducing\
    \ the batch sizes to choose the largest possible without running out of memory.\
    \ The cells are sorted to encounter memory errors sooner so the initial batch\
    \ size should work for the remainder.\n\n- emptying the cache of GPUs is often\
    \ non-trivial. Rather than running each gene as a loop, I would suggest you run\
    \ a script with xargs, for example, so that the GPU is completely reset between\
    \ runs.\n\n- if you are using the 12 layer model, you may consider using the 6\
    \ layer model (outer directory of this repository), which will be more memory-efficient."
  created_at: 2023-09-18 06:20:10+00:00
  edited: false
  hidden: false
  id: 6507fa2a7de4710baf0ae5e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-18T07:20:10.000Z'
    data:
      status: closed
    id: 6507fa2a7de4710baf0ae5ea
    type: status-change
  author: ctheodoris
  created_at: 2023-09-18 06:20:10+00:00
  id: 6507fa2a7de4710baf0ae5ea
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 230
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: torch.cuda.OutOfMemoryError
