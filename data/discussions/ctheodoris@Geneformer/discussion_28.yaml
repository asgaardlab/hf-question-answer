!!python/object:huggingface_hub.community.DiscussionWithDetails
author: weilangchan
conflicting_files: null
created_at: 2023-06-12 10:55:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae859a330991691f7125208d8ab1906b.svg
      fullname: weilang chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: weilangchan
      type: user
    createdAt: '2023-06-12T11:55:46.000Z'
    data:
      edited: false
      editors:
      - weilangchan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9554716944694519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae859a330991691f7125208d8ab1906b.svg
          fullname: weilang chan
          isHf: false
          isPro: false
          name: weilangchan
          type: user
        html: '<p>Thanks for your fantastic work!  And I want to train a new model
          based on data downloaded from the Internet, such as cellxgene. But the system
          memory problem araised when I tried to merge these .h5ad format data into
          datasets. Could you please give me some advice? </p>

          '
        raw: 'Thanks for your fantastic work!  And I want to train a new model based
          on data downloaded from the Internet, such as cellxgene. But the system
          memory problem araised when I tried to merge these .h5ad format data into
          datasets. Could you please give me some advice? '
        updatedAt: '2023-06-12T11:55:46.619Z'
      numEdits: 0
      reactions: []
    id: 648707c280cc5d9b389b81bf
    type: comment
  author: weilangchan
  content: 'Thanks for your fantastic work!  And I want to train a new model based
    on data downloaded from the Internet, such as cellxgene. But the system memory
    problem araised when I tried to merge these .h5ad format data into datasets. Could
    you please give me some advice? '
  created_at: 2023-06-12 10:55:46+00:00
  edited: false
  hidden: false
  id: 648707c280cc5d9b389b81bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-12T19:50:19.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.928085446357727
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. Could you clarify where
          you are encountering memory issues? Is this during some preprocessing while
          you are assembling the data prior to running the transcriptome tokenizer?
          Or is it while running the transcriptome tokenizer and if so, during which
          step? The transcriptome tokenizer scans through the .loom input files without
          loading the whole file into memory, which we do to avoid memory issues.
          We did not encounter memory issues while tokenizing ~30 million cells for
          Genecorpus-30M, but we pretrained the model over 2 years ago so there is
          much more data available now and I''m not sure how many cells you are working
          with.</p>

          '
        raw: Thank you for your interest in Geneformer. Could you clarify where you
          are encountering memory issues? Is this during some preprocessing while
          you are assembling the data prior to running the transcriptome tokenizer?
          Or is it while running the transcriptome tokenizer and if so, during which
          step? The transcriptome tokenizer scans through the .loom input files without
          loading the whole file into memory, which we do to avoid memory issues.
          We did not encounter memory issues while tokenizing ~30 million cells for
          Genecorpus-30M, but we pretrained the model over 2 years ago so there is
          much more data available now and I'm not sure how many cells you are working
          with.
        updatedAt: '2023-06-12T19:50:19.252Z'
      numEdits: 0
      reactions: []
    id: 648776fb340a0f4d7221b19f
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer. Could you clarify where you
    are encountering memory issues? Is this during some preprocessing while you are
    assembling the data prior to running the transcriptome tokenizer? Or is it while
    running the transcriptome tokenizer and if so, during which step? The transcriptome
    tokenizer scans through the .loom input files without loading the whole file into
    memory, which we do to avoid memory issues. We did not encounter memory issues
    while tokenizing ~30 million cells for Genecorpus-30M, but we pretrained the model
    over 2 years ago so there is much more data available now and I'm not sure how
    many cells you are working with.
  created_at: 2023-06-12 18:50:19+00:00
  edited: false
  hidden: false
  id: 648776fb340a0f4d7221b19f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-20T00:30:29.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723108410835266
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>I am closing this issue for now as there have been no updates but
          please feel free to open a new issue if needed.</p>

          '
        raw: I am closing this issue for now as there have been no updates but please
          feel free to open a new issue if needed.
        updatedAt: '2023-06-20T00:30:29.452Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6490f325f1f8e4ae865fad71
    id: 6490f325f1f8e4ae865fad6e
    type: comment
  author: ctheodoris
  content: I am closing this issue for now as there have been no updates but please
    feel free to open a new issue if needed.
  created_at: 2023-06-19 23:30:29+00:00
  edited: false
  hidden: false
  id: 6490f325f1f8e4ae865fad6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-20T00:30:29.000Z'
    data:
      status: closed
    id: 6490f325f1f8e4ae865fad71
    type: status-change
  author: ctheodoris
  created_at: 2023-06-19 23:30:29+00:00
  id: 6490f325f1f8e4ae865fad71
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: how do you transfer the original data to datasets?
