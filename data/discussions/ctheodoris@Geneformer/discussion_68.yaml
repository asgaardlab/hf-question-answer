!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aribenjamin
conflicting_files: null
created_at: 2023-06-26 21:34:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d64ae5c32717d8d73a4628fdcfdeaef.svg
      fullname: Ari Benjamin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aribenjamin
      type: user
    createdAt: '2023-06-26T22:34:23.000Z'
    data:
      edited: false
      editors:
      - aribenjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297245144844055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d64ae5c32717d8d73a4628fdcfdeaef.svg
          fullname: Ari Benjamin
          isHf: false
          isPro: false
          name: aribenjamin
          type: user
        html: '<p>Hi, first of all, thank you for releasing this code and working
          so hard to make it accessible to the community. This repo is great.</p>

          <p>I see the tokenizing scripts, which access the token dictionary, but
          I''m wondering how to create new tokens for new genes. What was your process
          of creating tokens in the first place?</p>

          <p>The reason I''m asking is that I''m considering retraining Geneformer
          for cells from another species. This will require creating new tokens for
          the new EnsemblIDs.  If possible I''d like to keep the token embeddings
          close in latent space for orthologous genes.</p>

          <p>Thanks!</p>

          '
        raw: "Hi, first of all, thank you for releasing this code and working so hard\
          \ to make it accessible to the community. This repo is great.\r\n\r\nI see\
          \ the tokenizing scripts, which access the token dictionary, but I'm wondering\
          \ how to create new tokens for new genes. What was your process of creating\
          \ tokens in the first place?\r\n\r\nThe reason I'm asking is that I'm considering\
          \ retraining Geneformer for cells from another species. This will require\
          \ creating new tokens for the new EnsemblIDs.  If possible I'd like to keep\
          \ the token embeddings close in latent space for orthologous genes.\r\n\r\
          \nThanks!"
        updatedAt: '2023-06-26T22:34:23.148Z'
      numEdits: 0
      reactions: []
    id: 649a126f721b5646d04d1a21
    type: comment
  author: aribenjamin
  content: "Hi, first of all, thank you for releasing this code and working so hard\
    \ to make it accessible to the community. This repo is great.\r\n\r\nI see the\
    \ tokenizing scripts, which access the token dictionary, but I'm wondering how\
    \ to create new tokens for new genes. What was your process of creating tokens\
    \ in the first place?\r\n\r\nThe reason I'm asking is that I'm considering retraining\
    \ Geneformer for cells from another species. This will require creating new tokens\
    \ for the new EnsemblIDs.  If possible I'd like to keep the token embeddings close\
    \ in latent space for orthologous genes.\r\n\r\nThanks!"
  created_at: 2023-06-26 21:34:23+00:00
  edited: false
  hidden: false
  id: 649a126f721b5646d04d1a21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-26T23:35:19.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9399546384811401
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The untrained tokens do not have a
          specific position. The pretraining process embeds them within a latent space
          that is updated to optimize the training objective. If you''d like to take
          advantage of the pretraining with human genes, one way would be to assign
          orthologous genes the same token (at least for the closest ortholog in non-1:1
          cases), and genes without an ortholog new tokens. Then, instead of starting
          with randomly initialized weights as you usually would when pretraining
          a new model, you could start with the pretrained Geneformer weights. If
          you have a large amount of pretraining data, this should re-adjust the weights
          to your organism of interest without overweighting to the human setting,
          while also potentially achieving better results because the genes are already
          starting close to where they should be relative to one another in the case
          of orthologs, as opposed to randomly positioned.</p>

          '
        raw: Thank you for your question. The untrained tokens do not have a specific
          position. The pretraining process embeds them within a latent space that
          is updated to optimize the training objective. If you'd like to take advantage
          of the pretraining with human genes, one way would be to assign orthologous
          genes the same token (at least for the closest ortholog in non-1:1 cases),
          and genes without an ortholog new tokens. Then, instead of starting with
          randomly initialized weights as you usually would when pretraining a new
          model, you could start with the pretrained Geneformer weights. If you have
          a large amount of pretraining data, this should re-adjust the weights to
          your organism of interest without overweighting to the human setting, while
          also potentially achieving better results because the genes are already
          starting close to where they should be relative to one another in the case
          of orthologs, as opposed to randomly positioned.
        updatedAt: '2023-06-26T23:35:19.118Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649a20b7020cb9f6dd0f0bc2
    id: 649a20b7020cb9f6dd0f0bc1
    type: comment
  author: ctheodoris
  content: Thank you for your question. The untrained tokens do not have a specific
    position. The pretraining process embeds them within a latent space that is updated
    to optimize the training objective. If you'd like to take advantage of the pretraining
    with human genes, one way would be to assign orthologous genes the same token
    (at least for the closest ortholog in non-1:1 cases), and genes without an ortholog
    new tokens. Then, instead of starting with randomly initialized weights as you
    usually would when pretraining a new model, you could start with the pretrained
    Geneformer weights. If you have a large amount of pretraining data, this should
    re-adjust the weights to your organism of interest without overweighting to the
    human setting, while also potentially achieving better results because the genes
    are already starting close to where they should be relative to one another in
    the case of orthologs, as opposed to randomly positioned.
  created_at: 2023-06-26 22:35:19+00:00
  edited: false
  hidden: false
  id: 649a20b7020cb9f6dd0f0bc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-26T23:35:19.000Z'
    data:
      status: closed
    id: 649a20b7020cb9f6dd0f0bc2
    type: status-change
  author: ctheodoris
  created_at: 2023-06-26 22:35:19+00:00
  id: 649a20b7020cb9f6dd0f0bc2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/548db241f47679a4d1c78c78b45bcb55.svg
      fullname: Minzhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minnnzhou
      type: user
    createdAt: '2023-08-07T16:35:41.000Z'
    data:
      edited: true
      editors:
      - Minnnzhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9377248883247375
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/548db241f47679a4d1c78c78b45bcb55.svg
          fullname: Minzhou
          isHf: false
          isPro: false
          name: Minnnzhou
          type: user
        html: "<blockquote>\n<p>Hi, first of all, thank you for releasing this code\
          \ and working so hard to make it accessible to the community. This repo\
          \ is great.</p>\n<p>I see the tokenizing scripts, which access the token\
          \ dictionary, but I'm wondering how to create new tokens for new genes.\
          \ What was your process of creating tokens in the first place?</p>\n<p>The\
          \ reason I'm asking is that I'm considering retraining Geneformer for cells\
          \ from another species. This will require creating new tokens for the new\
          \ EnsemblIDs.  If possible I'd like to keep the token embeddings close in\
          \ latent space for orthologous genes.</p>\n<p>Thanks!</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;aribenjamin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aribenjamin\">@<span\
          \ class=\"underline\">aribenjamin</span></a></span>\n\n\t</span></span>\
          \ Hi, sorry to bother you, I have also encountered the same problem. Have\
          \ you resolved it? And may I ask for your advice? </p>\n<p>Thank you!</p>\n"
        raw: "> Hi, first of all, thank you for releasing this code and working so\
          \ hard to make it accessible to the community. This repo is great.\n> \n\
          > I see the tokenizing scripts, which access the token dictionary, but I'm\
          \ wondering how to create new tokens for new genes. What was your process\
          \ of creating tokens in the first place?\n> \n> The reason I'm asking is\
          \ that I'm considering retraining Geneformer for cells from another species.\
          \ This will require creating new tokens for the new EnsemblIDs.  If possible\
          \ I'd like to keep the token embeddings close in latent space for orthologous\
          \ genes.\n> \n> Thanks!\n\n@aribenjamin Hi, sorry to bother you, I have\
          \ also encountered the same problem. Have you resolved it? And may I ask\
          \ for your advice? \n \nThank you!\n"
        updatedAt: '2023-08-07T16:36:31.170Z'
      numEdits: 1
      reactions: []
    id: 64d11d5d37a8b7adafa27a04
    type: comment
  author: Minnnzhou
  content: "> Hi, first of all, thank you for releasing this code and working so hard\
    \ to make it accessible to the community. This repo is great.\n> \n> I see the\
    \ tokenizing scripts, which access the token dictionary, but I'm wondering how\
    \ to create new tokens for new genes. What was your process of creating tokens\
    \ in the first place?\n> \n> The reason I'm asking is that I'm considering retraining\
    \ Geneformer for cells from another species. This will require creating new tokens\
    \ for the new EnsemblIDs.  If possible I'd like to keep the token embeddings close\
    \ in latent space for orthologous genes.\n> \n> Thanks!\n\n@aribenjamin Hi, sorry\
    \ to bother you, I have also encountered the same problem. Have you resolved it?\
    \ And may I ask for your advice? \n \nThank you!\n"
  created_at: 2023-08-07 15:35:41+00:00
  edited: true
  hidden: false
  id: 64d11d5d37a8b7adafa27a04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d64ae5c32717d8d73a4628fdcfdeaef.svg
      fullname: Ari Benjamin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aribenjamin
      type: user
    createdAt: '2023-08-07T20:02:28.000Z'
    data:
      edited: false
      editors:
      - aribenjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9506796598434448
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d64ae5c32717d8d73a4628fdcfdeaef.svg
          fullname: Ari Benjamin
          isHf: false
          isPro: false
          name: aribenjamin
          type: user
        html: '<p>Hi! Yes, I''ve tried a few things, but I''m still figuring out the
          best way to do this (with regard to success at downstream tasks). My current
          metric is the MLM loss on an evaluation set in the new organism. By that
          metric, it appears Christina''s advice above is the best. Take care in creating
          your new dataset, though. I found it necessary to normalize genes in the
          new organism by the median of those (ortholog) human genes in the Geneformer
          dataset, which were created and distributed by Christina. If you use the
          medians of those genes in the new dataset, you''re essentially introducing
          a distribution shift in the ordering that can be avoided. (Genes without
          orthologs of course must receive a new median.)</p>

          '
        raw: Hi! Yes, I've tried a few things, but I'm still figuring out the best
          way to do this (with regard to success at downstream tasks). My current
          metric is the MLM loss on an evaluation set in the new organism. By that
          metric, it appears Christina's advice above is the best. Take care in creating
          your new dataset, though. I found it necessary to normalize genes in the
          new organism by the median of those (ortholog) human genes in the Geneformer
          dataset, which were created and distributed by Christina. If you use the
          medians of those genes in the new dataset, you're essentially introducing
          a distribution shift in the ordering that can be avoided. (Genes without
          orthologs of course must receive a new median.)
        updatedAt: '2023-08-07T20:02:28.247Z'
      numEdits: 0
      reactions: []
    id: 64d14dd4abc3308f05528675
    type: comment
  author: aribenjamin
  content: Hi! Yes, I've tried a few things, but I'm still figuring out the best way
    to do this (with regard to success at downstream tasks). My current metric is
    the MLM loss on an evaluation set in the new organism. By that metric, it appears
    Christina's advice above is the best. Take care in creating your new dataset,
    though. I found it necessary to normalize genes in the new organism by the median
    of those (ortholog) human genes in the Geneformer dataset, which were created
    and distributed by Christina. If you use the medians of those genes in the new
    dataset, you're essentially introducing a distribution shift in the ordering that
    can be avoided. (Genes without orthologs of course must receive a new median.)
  created_at: 2023-08-07 19:02:28+00:00
  edited: false
  hidden: false
  id: 64d14dd4abc3308f05528675
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: How were gene tokens created?
