!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sarudak
conflicting_files: null
created_at: 2024-01-09 18:24:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-09T18:24:18.000Z'
    data:
      edited: false
      editors:
      - sarudak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.542669415473938
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: sarudak
          type: user
        html: '<p>Perturbation works fine when set to "all" but when attempting to
          perturb a single gene it fails with an unhelpful error: "RuntimeError: One
          of the subprocesses has abruptly died during map operation.To debug the
          error, disable multiprocessing."</p>

          <p>Code:<br>isp = InSilicoPerturber(perturb_type="overexpress",<br>                        perturb_rank_shift=None,<br>                        genes_to_perturb=["ENSG00000254535"],<br>                        combos=0,<br>                        anchor_gene=None,<br>                        model_type="CellClassifier",<br>                        num_classes=3,<br>                        emb_mode="cell",<br>                        cell_emb_style="mean_pool",<br>                        filter_data=filter_data_dict,<br>                        cell_states_to_model=cell_states_to_model,<br>                        state_embs_dict=state_embs_dict,<br>                        max_ncells=2000,<br>                        emb_layer=0,<br>                        forward_batch_size=40,<br>                        nproc=16)</p>

          <p>isp.perturb_data(model_path,<br>                 dataset_path,<br>                 output_path,<br>                 prefix)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64bae13b796f20daad315778/i1dBOdrvVXLwGgN4Dt9Rr.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64bae13b796f20daad315778/i1dBOdrvVXLwGgN4Dt9Rr.png"></a></p>

          '
        raw: "Perturbation works fine when set to \"all\" but when attempting to perturb\
          \ a single gene it fails with an unhelpful error: \"RuntimeError: One of\
          \ the subprocesses has abruptly died during map operation.To debug the error,\
          \ disable multiprocessing.\"\r\n\r\nCode:\r\nisp = InSilicoPerturber(perturb_type=\"\
          overexpress\",\r\n                        perturb_rank_shift=None,\r\n \
          \                       genes_to_perturb=[\"ENSG00000254535\"],\r\n    \
          \                    combos=0,\r\n                        anchor_gene=None,\r\
          \n                        model_type=\"CellClassifier\",\r\n           \
          \             num_classes=3,\r\n                        emb_mode=\"cell\"\
          ,\r\n                        cell_emb_style=\"mean_pool\",\r\n         \
          \               filter_data=filter_data_dict,\r\n                      \
          \  cell_states_to_model=cell_states_to_model,\r\n                      \
          \  state_embs_dict=state_embs_dict,\r\n                        max_ncells=2000,\r\
          \n                        emb_layer=0,\r\n                        forward_batch_size=40,\r\
          \n                        nproc=16)\r\n\r\nisp.perturb_data(model_path,\r\
          \n                 dataset_path,\r\n                 output_path,\r\n  \
          \               prefix)\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bae13b796f20daad315778/i1dBOdrvVXLwGgN4Dt9Rr.png)\r\
          \n"
        updatedAt: '2024-01-09T18:24:18.713Z'
      numEdits: 0
      reactions: []
    id: 659d8f5273410185e52e7b01
    type: comment
  author: sarudak
  content: "Perturbation works fine when set to \"all\" but when attempting to perturb\
    \ a single gene it fails with an unhelpful error: \"RuntimeError: One of the subprocesses\
    \ has abruptly died during map operation.To debug the error, disable multiprocessing.\"\
    \r\n\r\nCode:\r\nisp = InSilicoPerturber(perturb_type=\"overexpress\",\r\n   \
    \                     perturb_rank_shift=None,\r\n                        genes_to_perturb=[\"\
    ENSG00000254535\"],\r\n                        combos=0,\r\n                 \
    \       anchor_gene=None,\r\n                        model_type=\"CellClassifier\"\
    ,\r\n                        num_classes=3,\r\n                        emb_mode=\"\
    cell\",\r\n                        cell_emb_style=\"mean_pool\",\r\n         \
    \               filter_data=filter_data_dict,\r\n                        cell_states_to_model=cell_states_to_model,\r\
    \n                        state_embs_dict=state_embs_dict,\r\n               \
    \         max_ncells=2000,\r\n                        emb_layer=0,\r\n       \
    \                 forward_batch_size=40,\r\n                        nproc=16)\r\
    \n\r\nisp.perturb_data(model_path,\r\n                 dataset_path,\r\n     \
    \            output_path,\r\n                 prefix)\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bae13b796f20daad315778/i1dBOdrvVXLwGgN4Dt9Rr.png)\r\
    \n"
  created_at: 2024-01-09 18:24:18+00:00
  edited: false
  hidden: false
  id: 659d8f5273410185e52e7b01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-11T19:34:43.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9074496626853943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! Please disable multiprocessing
          as the error message suggests by setting nproc=1 to help debug the error.
          If you are still unable to determine the solution, please respond with the
          error message in that case so we can help troubleshoot. Also, it can be
          helpful to ensure your data is in a scratch space that is not limited by
          network connection while running the perturbation as a loss of connection
          can sometimes disrupt processes. Also, please ensure you are using the most
          up-to-date Geneformer by pulling the current version.</p>

          '
        raw: Thank you for your interest in Geneformer! Please disable multiprocessing
          as the error message suggests by setting nproc=1 to help debug the error.
          If you are still unable to determine the solution, please respond with the
          error message in that case so we can help troubleshoot. Also, it can be
          helpful to ensure your data is in a scratch space that is not limited by
          network connection while running the perturbation as a loss of connection
          can sometimes disrupt processes. Also, please ensure you are using the most
          up-to-date Geneformer by pulling the current version.
        updatedAt: '2024-01-11T19:34:43.603Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a042d3230f8846b16f9f69
    id: 65a042d3230f8846b16f9f64
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! Please disable multiprocessing
    as the error message suggests by setting nproc=1 to help debug the error. If you
    are still unable to determine the solution, please respond with the error message
    in that case so we can help troubleshoot. Also, it can be helpful to ensure your
    data is in a scratch space that is not limited by network connection while running
    the perturbation as a loss of connection can sometimes disrupt processes. Also,
    please ensure you are using the most up-to-date Geneformer by pulling the current
    version.
  created_at: 2024-01-11 19:34:43+00:00
  edited: false
  hidden: false
  id: 65a042d3230f8846b16f9f64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-11T19:34:43.000Z'
    data:
      status: closed
    id: 65a042d3230f8846b16f9f69
    type: status-change
  author: ctheodoris
  created_at: 2024-01-11 19:34:43+00:00
  id: 65a042d3230f8846b16f9f69
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-16T20:50:16.000Z'
    data:
      edited: false
      editors:
      - sarudak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8722237944602966
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: sarudak
          type: user
        html: '<p>Thanks for the reply. Unsure why I didn''t immediately think of
          setting nproc=1. However doing so does not seem to help the issue as the
          process completes without issue when using nproc=1. To recap</p>

          <p>genes_to_perturb="all" + nproc=16 =&gt; Runs to completion<br>genes_to_perturb=["ENSG00000198523"]
          + nproc=1 =&gt; Runs to completion<br>genes_to_perturb=["ENSG00000198523"]
          + nproc=2 =&gt; Errors with above error</p>

          <p>Things I have tried:</p>

          <ul>

          <li>Pulling the latest version</li>

          <li>Setting all dependencies to those now defined in the requirement.txt
          (thanks for that!)</li>

          </ul>

          <p>Assuming you or others can run genes_to_perturb=["ENSG00000198523"] +
          nproc=2 to completion with no issues on the latest code in hugging face
          the only possibility I can think of is breaking differences in uncontrolled
          dependencies. CUDA seems the most likely given the runtime error: "RuntimeError:
          Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing,
          you must use the ''spawn'' start method"</p>

          '
        raw: 'Thanks for the reply. Unsure why I didn''t immediately think of setting
          nproc=1. However doing so does not seem to help the issue as the process
          completes without issue when using nproc=1. To recap


          genes_to_perturb="all" + nproc=16 => Runs to completion

          genes_to_perturb=["ENSG00000198523"] + nproc=1 => Runs to completion

          genes_to_perturb=["ENSG00000198523"] + nproc=2 => Errors with above error


          Things I have tried:

          - Pulling the latest version

          - Setting all dependencies to those now defined in the requirement.txt (thanks
          for that!)


          Assuming you or others can run genes_to_perturb=["ENSG00000198523"] + nproc=2
          to completion with no issues on the latest code in hugging face the only
          possibility I can think of is breaking differences in uncontrolled dependencies.
          CUDA seems the most likely given the runtime error: "RuntimeError: Cannot
          re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing,
          you must use the ''spawn'' start method"'
        updatedAt: '2024-01-16T20:50:16.847Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a6ec083efe2c547c3ecd48
    id: 65a6ec083efe2c547c3ecd44
    type: comment
  author: sarudak
  content: 'Thanks for the reply. Unsure why I didn''t immediately think of setting
    nproc=1. However doing so does not seem to help the issue as the process completes
    without issue when using nproc=1. To recap


    genes_to_perturb="all" + nproc=16 => Runs to completion

    genes_to_perturb=["ENSG00000198523"] + nproc=1 => Runs to completion

    genes_to_perturb=["ENSG00000198523"] + nproc=2 => Errors with above error


    Things I have tried:

    - Pulling the latest version

    - Setting all dependencies to those now defined in the requirement.txt (thanks
    for that!)


    Assuming you or others can run genes_to_perturb=["ENSG00000198523"] + nproc=2
    to completion with no issues on the latest code in hugging face the only possibility
    I can think of is breaking differences in uncontrolled dependencies. CUDA seems
    the most likely given the runtime error: "RuntimeError: Cannot re-initialize CUDA
    in forked subprocess. To use CUDA with multiprocessing, you must use the ''spawn''
    start method"'
  created_at: 2024-01-16 20:50:16+00:00
  edited: false
  hidden: false
  id: 65a6ec083efe2c547c3ecd44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-16T20:50:16.000Z'
    data:
      status: open
    id: 65a6ec083efe2c547c3ecd48
    type: status-change
  author: sarudak
  created_at: 2024-01-16 20:50:16+00:00
  id: 65a6ec083efe2c547c3ecd48
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-17T01:00:30.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9515199661254883
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. Yes, we are able to run a single gene
          perturbation with multiple processes. Could you confirm the data is located
          in a scratch space (rather than a mounted network drive) such that this
          issue is not potentially caused by network connection issues? If you have
          ruled out this issue, could you try setting your CUDA_VISIBLE_DEVICES to
          a single GPU to see if that resolves this issue while keeping nproc (CPU
          processes) at &gt;1?</p>

          <pre><code># for example, add the following to the beginning of your script:

          import os

          GPU_NUMBER = [0]

          os.environ["CUDA_VISIBLE_DEVICES"] = ",".join([str(s) for s in GPU_NUMBER])


          # this can also be helpful for troubleshooting:

          os.environ["NCCL_DEBUG"] = "INFO"

          </code></pre>

          '
        raw: 'Thank you for following up. Yes, we are able to run a single gene perturbation
          with multiple processes. Could you confirm the data is located in a scratch
          space (rather than a mounted network drive) such that this issue is not
          potentially caused by network connection issues? If you have ruled out this
          issue, could you try setting your CUDA_VISIBLE_DEVICES to a single GPU to
          see if that resolves this issue while keeping nproc (CPU processes) at >1?


          ```

          # for example, add the following to the beginning of your script:

          import os

          GPU_NUMBER = [0]

          os.environ["CUDA_VISIBLE_DEVICES"] = ",".join([str(s) for s in GPU_NUMBER])


          # this can also be helpful for troubleshooting:

          os.environ["NCCL_DEBUG"] = "INFO"

          ```'
        updatedAt: '2024-01-17T01:02:00.901Z'
      numEdits: 2
      reactions: []
    id: 65a726aed6e5c1ed6c56365f
    type: comment
  author: ctheodoris
  content: 'Thank you for following up. Yes, we are able to run a single gene perturbation
    with multiple processes. Could you confirm the data is located in a scratch space
    (rather than a mounted network drive) such that this issue is not potentially
    caused by network connection issues? If you have ruled out this issue, could you
    try setting your CUDA_VISIBLE_DEVICES to a single GPU to see if that resolves
    this issue while keeping nproc (CPU processes) at >1?


    ```

    # for example, add the following to the beginning of your script:

    import os

    GPU_NUMBER = [0]

    os.environ["CUDA_VISIBLE_DEVICES"] = ",".join([str(s) for s in GPU_NUMBER])


    # this can also be helpful for troubleshooting:

    os.environ["NCCL_DEBUG"] = "INFO"

    ```'
  created_at: 2024-01-17 01:00:30+00:00
  edited: true
  hidden: false
  id: 65a726aed6e5c1ed6c56365f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-17T01:19:05.000Z'
    data:
      edited: false
      editors:
      - sarudak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.988323450088501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: sarudak
          type: user
        html: '<p>All of this is being run locally. Data is on my local drive and
          the model is running on an RTX4090. I was able to run this previously with
          no issue. I ran with the script you provided but it produced no change in
          behavior. Since I only have one GPU available on my machine this seems expected.</p>

          '
        raw: All of this is being run locally. Data is on my local drive and the model
          is running on an RTX4090. I was able to run this previously with no issue.
          I ran with the script you provided but it produced no change in behavior.
          Since I only have one GPU available on my machine this seems expected.
        updatedAt: '2024-01-17T01:19:05.382Z'
      numEdits: 0
      reactions: []
    id: 65a72b09b3c1a539e0545714
    type: comment
  author: sarudak
  content: All of this is being run locally. Data is on my local drive and the model
    is running on an RTX4090. I was able to run this previously with no issue. I ran
    with the script you provided but it produced no change in behavior. Since I only
    have one GPU available on my machine this seems expected.
  created_at: 2024-01-17 01:19:05+00:00
  edited: false
  hidden: false
  id: 65a72b09b3c1a539e0545714
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-17T01:29:29.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8982691168785095
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for the additional information. The good news is that
          generally the functions where CPU multiprocessing occurs are not likely
          the rate limiting step of the in silico perturbation so setting nproc=1
          will likely not have a major impact on the total runtime. However, could
          you try adding this at the beginning of your script?:</p>

          <pre><code>from multiprocess import set_start_method

          set_start_method("spawn")

          </code></pre>

          '
        raw: 'Thank you for the additional information. The good news is that generally
          the functions where CPU multiprocessing occurs are not likely the rate limiting
          step of the in silico perturbation so setting nproc=1 will likely not have
          a major impact on the total runtime. However, could you try adding this
          at the beginning of your script?:


          ```

          from multiprocess import set_start_method

          set_start_method("spawn")

          ```'
        updatedAt: '2024-01-17T01:29:29.582Z'
      numEdits: 0
      reactions: []
    id: 65a72d79b3c1a539e054bb2d
    type: comment
  author: ctheodoris
  content: 'Thank you for the additional information. The good news is that generally
    the functions where CPU multiprocessing occurs are not likely the rate limiting
    step of the in silico perturbation so setting nproc=1 will likely not have a major
    impact on the total runtime. However, could you try adding this at the beginning
    of your script?:


    ```

    from multiprocess import set_start_method

    set_start_method("spawn")

    ```'
  created_at: 2024-01-17 01:29:29+00:00
  edited: false
  hidden: false
  id: 65a72d79b3c1a539e054bb2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-17T17:44:48.000Z'
    data:
      edited: false
      editors:
      - sarudak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7992233037948608
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: sarudak
          type: user
        html: '<p>Using this setting I was able to run through the insilico perturb
          without error. I do get spammed with a lot of numba warnings though.</p>

          <p>NumbaDeprecationWarning: The ''nopython'' keyword argument was not supplied
          to the ''numba.jit'' decorator. The implicit default value for this argument
          is currently False, but it will be changed to True in Numba 0.59.0. See
          <a rel="nofollow" href="https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit">https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit</a>
          for details.</p>

          <p>and</p>

          <p>NumbaWarning: The TBB threading layer requires TBB version 2021 update
          6 or later i.e., TBB_INTERFACE_VERSION &gt;= 12060. Found TBB_INTERFACE_VERSION
          = 12050. The TBB threading layer is disabled.</p>

          '
        raw: 'Using this setting I was able to run through the insilico perturb without
          error. I do get spammed with a lot of numba warnings though.


          NumbaDeprecationWarning: The ''nopython'' keyword argument was not supplied
          to the ''numba.jit'' decorator. The implicit default value for this argument
          is currently False, but it will be changed to True in Numba 0.59.0. See
          https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit
          for details.


          and


          NumbaWarning: The TBB threading layer requires TBB version 2021 update 6
          or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION
          = 12050. The TBB threading layer is disabled.'
        updatedAt: '2024-01-17T17:44:48.940Z'
      numEdits: 0
      reactions: []
    id: 65a81210ef14f9e603477efc
    type: comment
  author: sarudak
  content: 'Using this setting I was able to run through the insilico perturb without
    error. I do get spammed with a lot of numba warnings though.


    NumbaDeprecationWarning: The ''nopython'' keyword argument was not supplied to
    the ''numba.jit'' decorator. The implicit default value for this argument is currently
    False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit
    for details.


    and


    NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later
    i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The
    TBB threading layer is disabled.'
  created_at: 2024-01-17 17:44:48+00:00
  edited: false
  hidden: false
  id: 65a81210ef14f9e603477efc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-17T20:01:33.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8880689740180969
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Great - glad to hear that resolved the issue with multiprocessing
          with Huggingface Datasets .map.</p>

          <p>For the NumbaDeprecationWarning, this is coming from the loompy package.
          The jit decorator needs to include the nopython keyword. They fixed this
          in commit <a rel="nofollow" href="https://github.com/linnarsson-lab/loompy/commit/07d5ad7bda1b140f05b0d294dcf11de64351acc8">07d5ad7</a>
          so you could update to the most recent version, or you could ignore this
          warning. We use loompy only in the tokenizer.py and have included a line
          to filter out this warning.</p>

          <p>For the TBB threading, you could install tbb to resolve this, but it''s
          not necessary. See <a rel="nofollow" href="https://numba.pydata.org/numba-doc/latest/user/threading-layer.html">here</a>.</p>

          '
        raw: 'Great - glad to hear that resolved the issue with multiprocessing with
          Huggingface Datasets .map.


          For the NumbaDeprecationWarning, this is coming from the loompy package.
          The jit decorator needs to include the nopython keyword. They fixed this
          in commit [07d5ad7](https://github.com/linnarsson-lab/loompy/commit/07d5ad7bda1b140f05b0d294dcf11de64351acc8)
          so you could update to the most recent version, or you could ignore this
          warning. We use loompy only in the tokenizer.py and have included a line
          to filter out this warning.


          For the TBB threading, you could install tbb to resolve this, but it''s
          not necessary. See [here](https://numba.pydata.org/numba-doc/latest/user/threading-layer.html).'
        updatedAt: '2024-01-17T20:01:33.469Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a8321d669921943c027a44
    id: 65a8321d669921943c027a3c
    type: comment
  author: ctheodoris
  content: 'Great - glad to hear that resolved the issue with multiprocessing with
    Huggingface Datasets .map.


    For the NumbaDeprecationWarning, this is coming from the loompy package. The jit
    decorator needs to include the nopython keyword. They fixed this in commit [07d5ad7](https://github.com/linnarsson-lab/loompy/commit/07d5ad7bda1b140f05b0d294dcf11de64351acc8)
    so you could update to the most recent version, or you could ignore this warning.
    We use loompy only in the tokenizer.py and have included a line to filter out
    this warning.


    For the TBB threading, you could install tbb to resolve this, but it''s not necessary.
    See [here](https://numba.pydata.org/numba-doc/latest/user/threading-layer.html).'
  created_at: 2024-01-17 20:01:33+00:00
  edited: false
  hidden: false
  id: 65a8321d669921943c027a3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2024-01-17T20:01:33.000Z'
    data:
      status: closed
    id: 65a8321d669921943c027a44
    type: status-change
  author: ctheodoris
  created_at: 2024-01-17 20:01:33+00:00
  id: 65a8321d669921943c027a44
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarudak
      type: user
    createdAt: '2024-01-17T22:28:58.000Z'
    data:
      edited: false
      editors:
      - sarudak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9957293272018433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85d55f730fd0412d08ae8d82f77f3d4c.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: sarudak
          type: user
        html: '<p>Thanks for working through all that with me.</p>

          '
        raw: Thanks for working through all that with me.
        updatedAt: '2024-01-17T22:28:58.574Z'
      numEdits: 0
      reactions: []
    id: 65a854aa6a7418d9af511823
    type: comment
  author: sarudak
  content: Thanks for working through all that with me.
  created_at: 2024-01-17 22:28:58+00:00
  edited: false
  hidden: false
  id: 65a854aa6a7418d9af511823
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 288
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: In silico perturbation errors when perturbing a single gene
