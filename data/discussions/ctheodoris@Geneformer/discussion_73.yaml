!!python/object:huggingface_hub.community.DiscussionWithDetails
author: allenxiao
conflicting_files: null
created_at: 2023-06-29 17:13:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-06-29T18:13:44.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8483685851097107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>Thanks for providing this amazing model again.<br>When I modified
          the pretraining example''s default parameters to the 12-layer version, i.e.
          num_embed_dim = 512, num_attn_heads = 8, and num_layers = 12, the RuntimeError:
          CUDA out of memory appeared, and I believe you may also face this problem
          before, so could you please provide some suggestions to avoid the error?  </p>

          '
        raw: "Thanks for providing this amazing model again.\r\nWhen I modified the\
          \ pretraining example's default parameters to the 12-layer version, i.e.\
          \ num_embed_dim = 512, num_attn_heads = 8, and num_layers = 12, the RuntimeError:\
          \ CUDA out of memory appeared, and I believe you may also face this problem\
          \ before, so could you please provide some suggestions to avoid the error?\
          \  "
        updatedAt: '2023-06-29T18:13:44.549Z'
      numEdits: 0
      reactions: []
    id: 649dc9d8a39f07caa62b17ab
    type: comment
  author: allenxiao
  content: "Thanks for providing this amazing model again.\r\nWhen I modified the\
    \ pretraining example's default parameters to the 12-layer version, i.e. num_embed_dim\
    \ = 512, num_attn_heads = 8, and num_layers = 12, the RuntimeError: CUDA out of\
    \ memory appeared, and I believe you may also face this problem before, so could\
    \ you please provide some suggestions to avoid the error?  "
  created_at: 2023-06-29 17:13:44+00:00
  edited: false
  hidden: false
  id: 649dc9d8a39f07caa62b17ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-29T18:23:21.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9266071319580078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. We trained the 12 layer
          model with the same resources and distributed training algorithms as discussed
          in our manuscript for the 6 layer model. We trained the 12 layer model at
          the same time as the 6 layer model, over 2 years ago, so there are more
          advances in efficient training algorithms since then that you could consider
          implementing (e.g. FlashAttention).</p>

          <p>Because we expected that users may face memory limitations, we focused
          on the 6 layer model for our manuscript to allow it to be accessible to
          more researchers. However, we release here the pretrained 12 layer model
          as well for users with resources that are capable of working with this size
          model. Of note, fine-tuning the 12 layer model we have already pretrained
          will of course be much less resource-intensive than repeating the pretraining
          from scratch.</p>

          '
        raw: 'Thank you for your interest in Geneformer. We trained the 12 layer model
          with the same resources and distributed training algorithms as discussed
          in our manuscript for the 6 layer model. We trained the 12 layer model at
          the same time as the 6 layer model, over 2 years ago, so there are more
          advances in efficient training algorithms since then that you could consider
          implementing (e.g. FlashAttention).


          Because we expected that users may face memory limitations, we focused on
          the 6 layer model for our manuscript to allow it to be accessible to more
          researchers. However, we release here the pretrained 12 layer model as well
          for users with resources that are capable of working with this size model.
          Of note, fine-tuning the 12 layer model we have already pretrained will
          of course be much less resource-intensive than repeating the pretraining
          from scratch.'
        updatedAt: '2023-06-29T18:23:21.488Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - allenxiao
      relatedEventId: 649dcc19d8f9b2e2cb4b3cc9
    id: 649dcc19d8f9b2e2cb4b3cc6
    type: comment
  author: ctheodoris
  content: 'Thank you for your interest in Geneformer. We trained the 12 layer model
    with the same resources and distributed training algorithms as discussed in our
    manuscript for the 6 layer model. We trained the 12 layer model at the same time
    as the 6 layer model, over 2 years ago, so there are more advances in efficient
    training algorithms since then that you could consider implementing (e.g. FlashAttention).


    Because we expected that users may face memory limitations, we focused on the
    6 layer model for our manuscript to allow it to be accessible to more researchers.
    However, we release here the pretrained 12 layer model as well for users with
    resources that are capable of working with this size model. Of note, fine-tuning
    the 12 layer model we have already pretrained will of course be much less resource-intensive
    than repeating the pretraining from scratch.'
  created_at: 2023-06-29 17:23:21+00:00
  edited: false
  hidden: false
  id: 649dcc19d8f9b2e2cb4b3cc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-29T18:23:21.000Z'
    data:
      status: closed
    id: 649dcc19d8f9b2e2cb4b3cc9
    type: status-change
  author: ctheodoris
  created_at: 2023-06-29 17:23:21+00:00
  id: 649dcc19d8f9b2e2cb4b3cc9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-07-02T19:35:17.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8795822262763977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>If I reduce the batch_size parameter to avoid OOM, would smaller
          batch_size weaken the model''s performance? In other words, for the Geneformer
          model, does batch_size play an important role in the model''s performance?<br>Thank
          you for your patience.</p>

          '
        raw: 'If I reduce the batch_size parameter to avoid OOM, would smaller batch_size
          weaken the model''s performance? In other words, for the Geneformer model,
          does batch_size play an important role in the model''s performance?

          Thank you for your patience.'
        updatedAt: '2023-07-02T19:35:17.773Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Renqing
    id: 64a1d175c143d323bf0ea390
    type: comment
  author: allenxiao
  content: 'If I reduce the batch_size parameter to avoid OOM, would smaller batch_size
    weaken the model''s performance? In other words, for the Geneformer model, does
    batch_size play an important role in the model''s performance?

    Thank you for your patience.'
  created_at: 2023-07-02 18:35:17+00:00
  edited: false
  hidden: false
  id: 64a1d175c143d323bf0ea390
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
      fullname: Renqing Nie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renqing
      type: user
    createdAt: '2023-07-03T01:19:11.000Z'
    data:
      edited: false
      editors:
      - Renqing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9188628792762756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96dfc7cd6acde7bc305a6e3d7cfd8f81.svg
          fullname: Renqing Nie
          isHf: false
          isPro: false
          name: Renqing
          type: user
        html: '<blockquote>

          <p>If I reduce the batch_size parameter to avoid OOM, would smaller batch_size
          weaken the model''s performance? In other words, for the Geneformer model,
          does batch_size play an important role in the model''s performance?<br>Thank
          you for your patience.<br>I want to know that, too.</p>

          </blockquote>

          '
        raw: '> If I reduce the batch_size parameter to avoid OOM, would smaller batch_size
          weaken the model''s performance? In other words, for the Geneformer model,
          does batch_size play an important role in the model''s performance?

          > Thank you for your patience.

          I want to know that, too.

          '
        updatedAt: '2023-07-03T01:19:11.399Z'
      numEdits: 0
      reactions: []
    id: 64a2220feacb4b50ba2c33fd
    type: comment
  author: Renqing
  content: '> If I reduce the batch_size parameter to avoid OOM, would smaller batch_size
    weaken the model''s performance? In other words, for the Geneformer model, does
    batch_size play an important role in the model''s performance?

    > Thank you for your patience.

    I want to know that, too.

    '
  created_at: 2023-07-03 00:19:11+00:00
  edited: false
  hidden: false
  id: 64a2220feacb4b50ba2c33fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-03T07:58:32.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.919356107711792
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. Deep learning models in general can
          be critically affected by learning hyperparameters. That is why we recommend
          always optimizing learning hyperparameters for fine-tuning the model, for
          example. Pretraining is more computationally intensive so it may not be
          feasible to optimize hyperparameters to the same degree, but we certainly
          recommend optimizing them as is possible with the available resources. Changing
          the batch size can definitely affect the model''s training, but reducing
          it is not necessarily going to affect it in a negative way. One strategy
          is to set the maximum batch size allowed by your resources to facilitate
          more efficient training and then to optimize the remainder of the hyperparameters
          with this fixed batch size. Of note, the hyperparameters we used to train
          the 12 layer model were different than the 6 layer model. </p>

          '
        raw: 'Thank you for your question. Deep learning models in general can be
          critically affected by learning hyperparameters. That is why we recommend
          always optimizing learning hyperparameters for fine-tuning the model, for
          example. Pretraining is more computationally intensive so it may not be
          feasible to optimize hyperparameters to the same degree, but we certainly
          recommend optimizing them as is possible with the available resources. Changing
          the batch size can definitely affect the model''s training, but reducing
          it is not necessarily going to affect it in a negative way. One strategy
          is to set the maximum batch size allowed by your resources to facilitate
          more efficient training and then to optimize the remainder of the hyperparameters
          with this fixed batch size. Of note, the hyperparameters we used to train
          the 12 layer model were different than the 6 layer model. '
        updatedAt: '2023-07-03T07:58:32.538Z'
      numEdits: 0
      reactions: []
    id: 64a27fa8660cce8b86ddeab2
    type: comment
  author: ctheodoris
  content: 'Thank you for your question. Deep learning models in general can be critically
    affected by learning hyperparameters. That is why we recommend always optimizing
    learning hyperparameters for fine-tuning the model, for example. Pretraining is
    more computationally intensive so it may not be feasible to optimize hyperparameters
    to the same degree, but we certainly recommend optimizing them as is possible
    with the available resources. Changing the batch size can definitely affect the
    model''s training, but reducing it is not necessarily going to affect it in a
    negative way. One strategy is to set the maximum batch size allowed by your resources
    to facilitate more efficient training and then to optimize the remainder of the
    hyperparameters with this fixed batch size. Of note, the hyperparameters we used
    to train the 12 layer model were different than the 6 layer model. '
  created_at: 2023-07-03 06:58:32+00:00
  edited: false
  hidden: false
  id: 64a27fa8660cce8b86ddeab2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 73
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: CUDA out of memory. '
