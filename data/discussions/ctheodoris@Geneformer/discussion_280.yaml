!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abreschi
conflicting_files: null
created_at: 2023-11-30 18:17:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
      fullname: Alessandra Breschi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abreschi
      type: user
    createdAt: '2023-11-30T18:17:47.000Z'
    data:
      edited: false
      editors:
      - abreschi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9628958106040955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
          fullname: Alessandra Breschi
          isHf: false
          isPro: false
          name: abreschi
          type: user
        html: '<p>Hi, excellent work!</p>

          <p>I have a question about implementing in silico gene knock out.</p>

          <p>If we consider a token vector of 2048 genes for one cell, from the code
          it seems that when doing knockout of one gene, the corresponding token is
          removed from the vector. So the new vector after KO has now 2047 tokens
          and then it''s padded to 2048. Am I reading the code correctly?</p>

          <p>If so, I was wondering if it makes sense instead to store the entire
          gene vector even if it''s longer than 2048, remove the target gene, then
          slice the resulting vector up to 2048. This way the last token would be
          the first gene that was left out by the first vector slicing during tokenization.</p>

          <p>I was curious to know your opinion about the two approaches, and if you
          think one is to be preferred over the other.</p>

          <p>Thanks!</p>

          '
        raw: "Hi, excellent work!\r\n\r\nI have a question about implementing in silico\
          \ gene knock out.\r\n\r\nIf we consider a token vector of 2048 genes for\
          \ one cell, from the code it seems that when doing knockout of one gene,\
          \ the corresponding token is removed from the vector. So the new vector\
          \ after KO has now 2047 tokens and then it's padded to 2048. Am I reading\
          \ the code correctly?\r\n\r\nIf so, I was wondering if it makes sense instead\
          \ to store the entire gene vector even if it's longer than 2048, remove\
          \ the target gene, then slice the resulting vector up to 2048. This way\
          \ the last token would be the first gene that was left out by the first\
          \ vector slicing during tokenization.\r\n\r\nI was curious to know your\
          \ opinion about the two approaches, and if you think one is to be preferred\
          \ over the other.\r\n\r\nThanks!"
        updatedAt: '2023-11-30T18:17:47.280Z'
      numEdits: 0
      reactions: []
    id: 6568d1cb28639db76eb3e55c
    type: comment
  author: abreschi
  content: "Hi, excellent work!\r\n\r\nI have a question about implementing in silico\
    \ gene knock out.\r\n\r\nIf we consider a token vector of 2048 genes for one cell,\
    \ from the code it seems that when doing knockout of one gene, the corresponding\
    \ token is removed from the vector. So the new vector after KO has now 2047 tokens\
    \ and then it's padded to 2048. Am I reading the code correctly?\r\n\r\nIf so,\
    \ I was wondering if it makes sense instead to store the entire gene vector even\
    \ if it's longer than 2048, remove the target gene, then slice the resulting vector\
    \ up to 2048. This way the last token would be the first gene that was left out\
    \ by the first vector slicing during tokenization.\r\n\r\nI was curious to know\
    \ your opinion about the two approaches, and if you think one is to be preferred\
    \ over the other.\r\n\r\nThanks!"
  created_at: 2023-11-30 18:17:47+00:00
  edited: false
  hidden: false
  id: 6568d1cb28639db76eb3e55c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-30T18:43:17.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.936919629573822
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your question! If you are comparing the gene deletion\
          \ to the original cell, then the original cell will require the full 2048\
          \ since it includes the deleted gene. By then comparing to the cell where\
          \ a gene is deleted but a new gene is also added, it may then be modeling\
          \ the effect of two perturbations and won\u2019t isolate as clearly the\
          \ effect of the particular gene deletion. </p>\n<p>Regarding padding, the\
          \ current implementation of the attention mask should preclude attention\
          \ on padding. However, when performing the deletions with the \u201Call\u201D\
          \ setting for genes to perturb, the perturbed encoding will not be padded\
          \ because the encodings are batched by genes rather than cells so they all\
          \ have the same length (the batch will be all 2047 in your example with\
          \ each having one gene deleted from the initial cell of length 2048) and\
          \ do not require padding.</p>\n"
        raw: "Thank you for your question! If you are comparing the gene deletion\
          \ to the original cell, then the original cell will require the full 2048\
          \ since it includes the deleted gene. By then comparing to the cell where\
          \ a gene is deleted but a new gene is also added, it may then be modeling\
          \ the effect of two perturbations and won\u2019t isolate as clearly the\
          \ effect of the particular gene deletion. \n\nRegarding padding, the current\
          \ implementation of the attention mask should preclude attention on padding.\
          \ However, when performing the deletions with the \u201Call\u201D setting\
          \ for genes to perturb, the perturbed encoding will not be padded because\
          \ the encodings are batched by genes rather than cells so they all have\
          \ the same length (the batch will be all 2047 in your example with each\
          \ having one gene deleted from the initial cell of length 2048) and do not\
          \ require padding."
        updatedAt: '2023-11-30T18:43:17.290Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6568d7c595ab5c3a5953ee53
    id: 6568d7c595ab5c3a5953ee50
    type: comment
  author: ctheodoris
  content: "Thank you for your question! If you are comparing the gene deletion to\
    \ the original cell, then the original cell will require the full 2048 since it\
    \ includes the deleted gene. By then comparing to the cell where a gene is deleted\
    \ but a new gene is also added, it may then be modeling the effect of two perturbations\
    \ and won\u2019t isolate as clearly the effect of the particular gene deletion.\
    \ \n\nRegarding padding, the current implementation of the attention mask should\
    \ preclude attention on padding. However, when performing the deletions with the\
    \ \u201Call\u201D setting for genes to perturb, the perturbed encoding will not\
    \ be padded because the encodings are batched by genes rather than cells so they\
    \ all have the same length (the batch will be all 2047 in your example with each\
    \ having one gene deleted from the initial cell of length 2048) and do not require\
    \ padding."
  created_at: 2023-11-30 18:43:17+00:00
  edited: false
  hidden: false
  id: 6568d7c595ab5c3a5953ee50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-30T18:43:17.000Z'
    data:
      status: closed
    id: 6568d7c595ab5c3a5953ee53
    type: status-change
  author: ctheodoris
  created_at: 2023-11-30 18:43:17+00:00
  id: 6568d7c595ab5c3a5953ee53
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
      fullname: Alessandra Breschi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abreschi
      type: user
    createdAt: '2023-11-30T19:15:33.000Z'
    data:
      edited: false
      editors:
      - abreschi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9698302745819092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
          fullname: Alessandra Breschi
          isHf: false
          isPro: false
          name: abreschi
          type: user
        html: '<p>Thanks for your prompt response! I guess my question was more around
          what happens after the 2048th token when one gene is removed. Right now
          it seems to be ignored because the tokenization truncates the gene list
          to 2048. But I was wondering if the 2049th gene should be included in the
          vector after one gene is removed rather than just padding the vector.</p>

          '
        raw: Thanks for your prompt response! I guess my question was more around
          what happens after the 2048th token when one gene is removed. Right now
          it seems to be ignored because the tokenization truncates the gene list
          to 2048. But I was wondering if the 2049th gene should be included in the
          vector after one gene is removed rather than just padding the vector.
        updatedAt: '2023-11-30T19:15:33.437Z'
      numEdits: 0
      reactions: []
    id: 6568df558e96268a527f53a2
    type: comment
  author: abreschi
  content: Thanks for your prompt response! I guess my question was more around what
    happens after the 2048th token when one gene is removed. Right now it seems to
    be ignored because the tokenization truncates the gene list to 2048. But I was
    wondering if the 2049th gene should be included in the vector after one gene is
    removed rather than just padding the vector.
  created_at: 2023-11-30 19:15:33+00:00
  edited: false
  hidden: false
  id: 6568df558e96268a527f53a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-30T19:42:23.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579503536224365
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. The shifts are calculated based on the
          cosine similarity between the original and perturbed embedding, so it would
          be best to isolate the perturbation you are testing as much as possible.
          Taking the example where the first gene in the rank value encoding is deleted,
          if the original encoding has genes 0--&gt;N and is compared to a perturbed
          encoding with genes 1--&gt;N, the only impact will be the deletion of gene
          0. However, if you calculate the shift between the original encoding 0--&gt;N
          and the perturbed encoding 1--&gt;N+1, you have introduced two changes:
          deleting 0 and adding N+1. For this reason, to isolate the change of interest,
          it would be best to avoid adding an additional change. That being said,
          if the gene N+1 is near the border of 2048 it may often be included or excluded
          from the same type of cell state depending on slight noise in its expression,
          so the model will likely not interpret that addition to have a large impact
          and therefore I don''t expect it would make a big difference either way;
          it would just be a more controlled setting to avoid multiple perturbation
          if you are trying to test the effect of a specific gene''s deletion.</p>

          '
        raw: 'Thank you for following up. The shifts are calculated based on the cosine
          similarity between the original and perturbed embedding, so it would be
          best to isolate the perturbation you are testing as much as possible. Taking
          the example where the first gene in the rank value encoding is deleted,
          if the original encoding has genes 0-->N and is compared to a perturbed
          encoding with genes 1-->N, the only impact will be the deletion of gene
          0. However, if you calculate the shift between the original encoding 0-->N
          and the perturbed encoding 1-->N+1, you have introduced two changes: deleting
          0 and adding N+1. For this reason, to isolate the change of interest, it
          would be best to avoid adding an additional change. That being said, if
          the gene N+1 is near the border of 2048 it may often be included or excluded
          from the same type of cell state depending on slight noise in its expression,
          so the model will likely not interpret that addition to have a large impact
          and therefore I don''t expect it would make a big difference either way;
          it would just be a more controlled setting to avoid multiple perturbation
          if you are trying to test the effect of a specific gene''s deletion.'
        updatedAt: '2023-11-30T19:42:23.117Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - JoeySama
        - Minnnzhou
    id: 6568e59f64a420d59a31888a
    type: comment
  author: ctheodoris
  content: 'Thank you for following up. The shifts are calculated based on the cosine
    similarity between the original and perturbed embedding, so it would be best to
    isolate the perturbation you are testing as much as possible. Taking the example
    where the first gene in the rank value encoding is deleted, if the original encoding
    has genes 0-->N and is compared to a perturbed encoding with genes 1-->N, the
    only impact will be the deletion of gene 0. However, if you calculate the shift
    between the original encoding 0-->N and the perturbed encoding 1-->N+1, you have
    introduced two changes: deleting 0 and adding N+1. For this reason, to isolate
    the change of interest, it would be best to avoid adding an additional change.
    That being said, if the gene N+1 is near the border of 2048 it may often be included
    or excluded from the same type of cell state depending on slight noise in its
    expression, so the model will likely not interpret that addition to have a large
    impact and therefore I don''t expect it would make a big difference either way;
    it would just be a more controlled setting to avoid multiple perturbation if you
    are trying to test the effect of a specific gene''s deletion.'
  created_at: 2023-11-30 19:42:23+00:00
  edited: false
  hidden: false
  id: 6568e59f64a420d59a31888a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
      fullname: Alessandra Breschi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abreschi
      type: user
    createdAt: '2023-11-30T20:47:04.000Z'
    data:
      edited: false
      editors:
      - abreschi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8021785020828247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c7ff1c4ce4d6d27950c2d9c74b997b1.svg
          fullname: Alessandra Breschi
          isHf: false
          isPro: false
          name: abreschi
          type: user
        html: '<p>That makes sense. Thank you so much!</p>

          '
        raw: That makes sense. Thank you so much!
        updatedAt: '2023-11-30T20:47:04.826Z'
      numEdits: 0
      reactions: []
    id: 6568f4c89c96f1a47beb8dd9
    type: comment
  author: abreschi
  content: That makes sense. Thank you so much!
  created_at: 2023-11-30 20:47:04+00:00
  edited: false
  hidden: false
  id: 6568f4c89c96f1a47beb8dd9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 280
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Shift vs delete in gene KO
