!!python/object:huggingface_hub.community.DiscussionWithDetails
author: maestriev
conflicting_files: null
created_at: 2023-08-12 02:04:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70dade0d37250f00b7690357c8ed0f6d.svg
      fullname: Evan Maestri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maestriev
      type: user
    createdAt: '2023-08-12T03:04:22.000Z'
    data:
      edited: false
      editors:
      - maestriev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351031184196472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70dade0d37250f00b7690357c8ed0f6d.svg
          fullname: Evan Maestri
          isHf: false
          isPro: false
          name: maestriev
          type: user
        html: '<p>Hi, thank you so much for your wonderful work! I am running into
          a similar issue as <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/80">https://huggingface.co/ctheodoris/Geneformer/discussions/80</a>.
          I am working with a large directory of .loom datasets encompassing ~4M cells
          spanning ~10+ datasets. The tokenizer is working well to tokenize each dataset
          individually. As an example, it worked on a dataset with 1.2million cells
          and average nCount_RNA per cell of 2395. However, I also received the "pyarrow.lib.ArrowInvalid:
          Value 2147484068 too large to fit in C integer type" error as I up the #
          of datasets in the directory. The proposed changes in <a href="/ctheodoris/Geneformer/discussions/80">#80</a>
          fixed the issue partially and that initial error resolved. However, I still
          seem to run out of memory when trying to tokenize all at once. Thus, for
          memory reasons, could I tokenize in batches of say 3-4 .loom files, then
          the next 3-4 .loom files and put all tokenized .arrow files into the same
          directory at the end and modify the state.json to include all dataset names.</p>

          <p>In essence, my question is can I tokenize each dataset individually or
          in smaller batches? Does tk.tokenize_data() tokenize each .loom separately
          or is there some factor which gets calculated taking into account all .loom
          files in the directory? Could you please clarify the implementation. Thanks
          for your time</p>

          '
        raw: "Hi, thank you so much for your wonderful work! I am running into a similar\
          \ issue as https://huggingface.co/ctheodoris/Geneformer/discussions/80.\
          \ I am working with a large directory of .loom datasets encompassing ~4M\
          \ cells spanning ~10+ datasets. The tokenizer is working well to tokenize\
          \ each dataset individually. As an example, it worked on a dataset with\
          \ 1.2million cells and average nCount_RNA per cell of 2395. However, I also\
          \ received the \"pyarrow.lib.ArrowInvalid: Value 2147484068 too large to\
          \ fit in C integer type\" error as I up the # of datasets in the directory.\
          \ The proposed changes in #80 fixed the issue partially and that initial\
          \ error resolved. However, I still seem to run out of memory when trying\
          \ to tokenize all at once. Thus, for memory reasons, could I tokenize in\
          \ batches of say 3-4 .loom files, then the next 3-4 .loom files and put\
          \ all tokenized .arrow files into the same directory at the end and modify\
          \ the state.json to include all dataset names.\r\n\r\nIn essence, my question\
          \ is can I tokenize each dataset individually or in smaller batches? Does\
          \ tk.tokenize_data() tokenize each .loom separately or is there some factor\
          \ which gets calculated taking into account all .loom files in the directory?\
          \ Could you please clarify the implementation. Thanks for your time"
        updatedAt: '2023-08-12T03:04:22.440Z'
      numEdits: 0
      reactions: []
    id: 64d6f6b6a787c9bc7b9c5ec3
    type: comment
  author: maestriev
  content: "Hi, thank you so much for your wonderful work! I am running into a similar\
    \ issue as https://huggingface.co/ctheodoris/Geneformer/discussions/80. I am working\
    \ with a large directory of .loom datasets encompassing ~4M cells spanning ~10+\
    \ datasets. The tokenizer is working well to tokenize each dataset individually.\
    \ As an example, it worked on a dataset with 1.2million cells and average nCount_RNA\
    \ per cell of 2395. However, I also received the \"pyarrow.lib.ArrowInvalid: Value\
    \ 2147484068 too large to fit in C integer type\" error as I up the # of datasets\
    \ in the directory. The proposed changes in #80 fixed the issue partially and\
    \ that initial error resolved. However, I still seem to run out of memory when\
    \ trying to tokenize all at once. Thus, for memory reasons, could I tokenize in\
    \ batches of say 3-4 .loom files, then the next 3-4 .loom files and put all tokenized\
    \ .arrow files into the same directory at the end and modify the state.json to\
    \ include all dataset names.\r\n\r\nIn essence, my question is can I tokenize\
    \ each dataset individually or in smaller batches? Does tk.tokenize_data() tokenize\
    \ each .loom separately or is there some factor which gets calculated taking into\
    \ account all .loom files in the directory? Could you please clarify the implementation.\
    \ Thanks for your time"
  created_at: 2023-08-12 02:04:22+00:00
  edited: false
  hidden: false
  id: 64d6f6b6a787c9bc7b9c5ec3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-12T06:00:23.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9567691683769226
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! Yes, it would be fine
          to tokenize them separately. You can then concatenate them once they are
          in the .dataset format. Thank you for linking to the prior discussions.
          It sounds like Huggingface Datasets is working on solving the overflow error
          based on the discussions in their repository, but we are looking further
          into workarounds in the meantime.</p>

          '
        raw: Thank you for your interest in Geneformer! Yes, it would be fine to tokenize
          them separately. You can then concatenate them once they are in the .dataset
          format. Thank you for linking to the prior discussions. It sounds like Huggingface
          Datasets is working on solving the overflow error based on the discussions
          in their repository, but we are looking further into workarounds in the
          meantime.
        updatedAt: '2023-08-12T06:00:23.633Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d71ff7d9d00dcb117e025e
    id: 64d71ff7d9d00dcb117e025a
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! Yes, it would be fine to tokenize
    them separately. You can then concatenate them once they are in the .dataset format.
    Thank you for linking to the prior discussions. It sounds like Huggingface Datasets
    is working on solving the overflow error based on the discussions in their repository,
    but we are looking further into workarounds in the meantime.
  created_at: 2023-08-12 05:00:23+00:00
  edited: false
  hidden: false
  id: 64d71ff7d9d00dcb117e025a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-12T06:00:23.000Z'
    data:
      status: closed
    id: 64d71ff7d9d00dcb117e025e
    type: status-change
  author: ctheodoris
  created_at: 2023-08-12 05:00:23+00:00
  id: 64d71ff7d9d00dcb117e025e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 214
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: tokenizer implementation question [for large directory of datasets]
