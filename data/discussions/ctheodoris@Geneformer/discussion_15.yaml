!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hua6886
conflicting_files: null
created_at: 2023-06-07 07:58:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
      fullname: Yuwei Hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hua6886
      type: user
    createdAt: '2023-06-07T08:58:18.000Z'
    data:
      edited: false
      editors:
      - hua6886
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5157288908958435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
          fullname: Yuwei Hua
          isHf: false
          isPro: false
          name: hua6886
          type: user
        html: '<p>Hi, congratulations on your wonderful work! I''m trying to repeat
          the pre-training process to get familiar with the workflow of the Geneformer.
          I refer to the code in pretrain_geneformer_w_deepspeed.py. In line 102 it
          mentions that token_dictionary.pkl should be a gene_ensembl_id:token dictionary,
          but when I load it, I found that it seems to be a token_id:gene_ensembl_id
          dictionary.<br>I am modifying it with token_dictionary={v:k for k,v in token_dictionary.items()},
          but when defining the trainer, the program throws an exception. Can you
          tell me what''s wrong with the code? Thank you very much!<br>The following
          is the exception information:</p>

          <hr>

          <p>AttributeError                            Traceback (most recent call
          last)<br>Cell In[26], line 2<br>      1 # define the trainer<br>----&gt;
          2 trainer = GeneformerPretrainer(<br>      3     model=model,<br>      4     args=training_args,<br>      5     #
          pretraining corpus (e.g. <a href="https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048.dataset">https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048.dataset</a>)<br>      6     train_dataset=train_dataset,<br>      7     #
          file of lengths of each example cell (e.g. <a href="https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048_sorted_lengths.pkl">https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048_sorted_lengths.pkl</a>)<br>      8     example_lengths_file="/home/container_workspace/Geneformer/0.data/Genecorpus-30M/genecorpus_30M_2048_sorted_lengths.pkl",<br>      9     token_dictionary=token_dictionary,<br>     10
          )</p>

          <p>File /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:599,
          in GeneformerPretrainer.<strong>init</strong>(self, *args, **kwargs)<br>    596
          token_dictionary = kwargs.get("token_dictionary")<br>    598 if data_collator
          is None:<br>--&gt; 599     precollator = GeneformerPreCollator(token_dictionary=token_dictionary)<br>    601     #
          # Data Collator Functions<br>    602     data_collator = DataCollatorForLanguageModeling(<br>    603         tokenizer=precollator,
          mlm=True, mlm_probability=0.15<br>    604     )</p>

          <p>File /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:111,
          in GeneformerPreCollator.<strong>init</strong>(self, *args, **kwargs)<br>    109
          self.token_dictionary = kwargs.get("token_dictionary")<br>    110 self.mask_token
          = "</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1226,
          in SpecialTokensMixin.mask_token_id(self, value)<br>   1224 @mask_token_id.setter<br>   1225
          def mask_token_id(self, value):<br>-&gt; 1226     self._mask_token = self.convert_ids_to_tokens(value)
          if value is not None else None</p>

          <p>AttributeError: ''GeneformerPreCollator'' object has no attribute ''convert_ids_to_tokens''</p>

          '
        raw: "Hi, congratulations on your wonderful work! I'm trying to repeat the\
          \ pre-training process to get familiar with the workflow of the Geneformer.\
          \ I refer to the code in pretrain_geneformer_w_deepspeed.py. In line 102\
          \ it mentions that token_dictionary.pkl should be a gene_ensembl_id:token\
          \ dictionary, but when I load it, I found that it seems to be a token_id:gene_ensembl_id\
          \ dictionary.\r\nI am modifying it with token_dictionary={v:k for k,v in\
          \ token_dictionary.items()}, but when defining the trainer, the program\
          \ throws an exception. Can you tell me what's wrong with the code? Thank\
          \ you very much!\r\nThe following is the exception information:\r\n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\nCell In[26], line 2\r\n      1 # define the trainer\r\n---->\
          \ 2 trainer = GeneformerPretrainer(\r\n      3     model=model,\r\n    \
          \  4     args=training_args,\r\n      5     # pretraining corpus (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048.dataset)\r\
          \n      6     train_dataset=train_dataset,\r\n      7     # file of lengths\
          \ of each example cell (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048_sorted_lengths.pkl)\r\
          \n      8     example_lengths_file=\"/home/container_workspace/Geneformer/0.data/Genecorpus-30M/genecorpus_30M_2048_sorted_lengths.pkl\"\
          ,\r\n      9     token_dictionary=token_dictionary,\r\n     10 )\r\n\r\n\
          File /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:599,\
          \ in GeneformerPretrainer.__init__(self, *args, **kwargs)\r\n    596 token_dictionary\
          \ = kwargs.get(\"token_dictionary\")\r\n    598 if data_collator is None:\r\
          \n--> 599     precollator = GeneformerPreCollator(token_dictionary=token_dictionary)\r\
          \n    601     # # Data Collator Functions\r\n    602     data_collator =\
          \ DataCollatorForLanguageModeling(\r\n    603         tokenizer=precollator,\
          \ mlm=True, mlm_probability=0.15\r\n    604     )\r\n\r\nFile /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:111,\
          \ in GeneformerPreCollator.__init__(self, *args, **kwargs)\r\n    109 self.token_dictionary\
          \ = kwargs.get(\"token_dictionary\")\r\n    110 self.mask_token = \"<mask>\"\
          \r\n--> 111 self.mask_token_id = self.token_dictionary.get(\"<mask>\")\r\
          \n    112 self.pad_token = \"<pad>\"\r\n    113 self.pad_token_id = self.token_dictionary.get(\"\
          <pad>\")\r\n\r\nFile /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1226,\
          \ in SpecialTokensMixin.mask_token_id(self, value)\r\n   1224 @mask_token_id.setter\r\
          \n   1225 def mask_token_id(self, value):\r\n-> 1226     self._mask_token\
          \ = self.convert_ids_to_tokens(value) if value is not None else None\r\n\
          \r\nAttributeError: 'GeneformerPreCollator' object has no attribute 'convert_ids_to_tokens'"
        updatedAt: '2023-06-07T08:58:18.217Z'
      numEdits: 0
      reactions: []
    id: 648046aa2a5ff6c1d1fe4976
    type: comment
  author: hua6886
  content: "Hi, congratulations on your wonderful work! I'm trying to repeat the pre-training\
    \ process to get familiar with the workflow of the Geneformer. I refer to the\
    \ code in pretrain_geneformer_w_deepspeed.py. In line 102 it mentions that token_dictionary.pkl\
    \ should be a gene_ensembl_id:token dictionary, but when I load it, I found that\
    \ it seems to be a token_id:gene_ensembl_id dictionary.\r\nI am modifying it with\
    \ token_dictionary={v:k for k,v in token_dictionary.items()}, but when defining\
    \ the trainer, the program throws an exception. Can you tell me what's wrong with\
    \ the code? Thank you very much!\r\nThe following is the exception information:\r\
    \n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \nCell In[26], line 2\r\n      1 # define the trainer\r\n----> 2 trainer = GeneformerPretrainer(\r\
    \n      3     model=model,\r\n      4     args=training_args,\r\n      5     #\
    \ pretraining corpus (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048.dataset)\r\
    \n      6     train_dataset=train_dataset,\r\n      7     # file of lengths of\
    \ each example cell (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048_sorted_lengths.pkl)\r\
    \n      8     example_lengths_file=\"/home/container_workspace/Geneformer/0.data/Genecorpus-30M/genecorpus_30M_2048_sorted_lengths.pkl\"\
    ,\r\n      9     token_dictionary=token_dictionary,\r\n     10 )\r\n\r\nFile /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:599,\
    \ in GeneformerPretrainer.__init__(self, *args, **kwargs)\r\n    596 token_dictionary\
    \ = kwargs.get(\"token_dictionary\")\r\n    598 if data_collator is None:\r\n\
    --> 599     precollator = GeneformerPreCollator(token_dictionary=token_dictionary)\r\
    \n    601     # # Data Collator Functions\r\n    602     data_collator = DataCollatorForLanguageModeling(\r\
    \n    603         tokenizer=precollator, mlm=True, mlm_probability=0.15\r\n  \
    \  604     )\r\n\r\nFile /usr/local/lib/python3.8/dist-packages/geneformer/pretrainer.py:111,\
    \ in GeneformerPreCollator.__init__(self, *args, **kwargs)\r\n    109 self.token_dictionary\
    \ = kwargs.get(\"token_dictionary\")\r\n    110 self.mask_token = \"<mask>\"\r\
    \n--> 111 self.mask_token_id = self.token_dictionary.get(\"<mask>\")\r\n    112\
    \ self.pad_token = \"<pad>\"\r\n    113 self.pad_token_id = self.token_dictionary.get(\"\
    <pad>\")\r\n\r\nFile /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1226,\
    \ in SpecialTokensMixin.mask_token_id(self, value)\r\n   1224 @mask_token_id.setter\r\
    \n   1225 def mask_token_id(self, value):\r\n-> 1226     self._mask_token = self.convert_ids_to_tokens(value)\
    \ if value is not None else None\r\n\r\nAttributeError: 'GeneformerPreCollator'\
    \ object has no attribute 'convert_ids_to_tokens'"
  created_at: 2023-06-07 07:58:18+00:00
  edited: false
  hidden: false
  id: 648046aa2a5ff6c1d1fe4976
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-07T13:52:00.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8481194376945496
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. This is a duplicate issue,
          please see <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/5">https://huggingface.co/ctheodoris/Geneformer/discussions/5</a>.</p>

          '
        raw: Thank you for your interest in Geneformer. This is a duplicate issue,
          please see https://huggingface.co/ctheodoris/Geneformer/discussions/5.
        updatedAt: '2023-06-07T13:52:00.981Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64808b8140facadc556dbb9d
    id: 64808b8040facadc556dbb9c
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer. This is a duplicate issue, please
    see https://huggingface.co/ctheodoris/Geneformer/discussions/5.
  created_at: 2023-06-07 12:52:00+00:00
  edited: false
  hidden: false
  id: 64808b8040facadc556dbb9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-07T13:52:01.000Z'
    data:
      status: closed
    id: 64808b8140facadc556dbb9d
    type: status-change
  author: ctheodoris
  created_at: 2023-06-07 12:52:01+00:00
  id: 64808b8140facadc556dbb9d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
      fullname: Yuwei Hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hua6886
      type: user
    createdAt: '2023-06-08T03:30:04.000Z'
    data:
      edited: false
      editors:
      - hua6886
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5845763087272644
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
          fullname: Yuwei Hua
          isHf: false
          isPro: false
          name: hua6886
          type: user
        html: '<blockquote>

          <p>Thank you for your interest in Geneformer. This is a duplicate issue,
          please see <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/5">https://huggingface.co/ctheodoris/Geneformer/discussions/5</a>.</p>

          </blockquote>

          <p>Thanks for the reply, but my question remains. Is the token_dictionary.pkl
          used in pretrain_geneformer_w_deepspeed.py supposed to be a gene_ensembl_id:token
          dictionary or a token_id:gene_ensembl_id dictionary?<br>Also, I have used
          the latest geneformer and transformers 4.28.0, but no matter which dictionary
          I use, the program throws an exception during training. How can I solve
          this? Please let me know if you need more information.</p>

          <h2 id="here-is-the-exception-information">Here is the exception information:</h2>

          <p>KeyError                                  Traceback (most recent call
          last)<br>Cell In[24], line 2<br>      1 # train<br>----&gt; 2 trainer.train()</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1662,
          in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,
          **kwargs)<br>   1657     self.model_wrapped = self.model<br>   1659 inner_training_loop
          = find_executable_batch_size(<br>   1660     self._inner_training_loop,
          self._train_batch_size, args.auto_find_batch_size<br>   1661 )<br>-&gt;
          1662 return inner_training_loop(<br>   1663     args=args,<br>   1664     resume_from_checkpoint=resume_from_checkpoint,<br>   1665     trial=trial,<br>   1666     ignore_keys_for_eval=ignore_keys_for_eval,<br>   1667
          )</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1899,
          in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint,
          trial, ignore_keys_for_eval)<br>   1896     rng_to_sync = True<br>   1898
          step = -1<br>-&gt; 1899 for step, inputs in enumerate(epoch_iterator):<br>   1900     total_batched_samples
          += 1<br>   1901     if rng_to_sync:</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634,
          in _BaseDataLoaderIter.<strong>next</strong>(self)<br>    631 if self._sampler_iter
          is None:<br>    632     # TODO(<a rel="nofollow" href="https://github.com/pytorch/pytorch/issues/76750">https://github.com/pytorch/pytorch/issues/76750</a>)<br>    633     self._reset()  #
          type: ignore[call-arg]<br>--&gt; 634 data = self._next_data()<br>    635
          self._num_yielded += 1<br>    636 if self._dataset_kind == _DatasetKind.Iterable
          and <br>    637         self._IterableDataset_len_called is not None and
          <br>    638         self._num_yielded &gt; self._IterableDataset_len_called:</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:677,
          in _SingleProcessDataLoaderIter._next_data(self)<br>    676 def _next_data(self):<br>--&gt;
          677     index = self._next_index()  # may raise StopIteration<br>    678     data
          = self._dataset_fetcher.fetch(index)  # may raise StopIteration<br>    679     if
          self._pin_memory:</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:624,
          in _BaseDataLoaderIter._next_index(self)<br>    623 def _next_index(self):<br>--&gt;
          624     return next(self._sampler_iter)</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/utils/data/sampler.py:254,
          in BatchSampler.<strong>iter</strong>(self)<br>    252 batch = [0] * self.batch_size<br>    253
          idx_in_batch = 0<br>--&gt; 254 for idx in self.sampler:<br>    255     batch[idx_in_batch]
          = idx<br>    256     idx_in_batch += 1</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:592,
          in LengthGroupedSampler.<strong>iter</strong>(self)<br>    591 def <strong>iter</strong>(self):<br>--&gt;
          592     indices = get_length_grouped_indices(self.lengths, self.batch_size,
          generator=self.generator)<br>    593     return iter(indices)</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,
          in get_length_grouped_indices(lengths, batch_size, mega_batch_mult, generator)<br>    536
          megabatch_size = mega_batch_mult * batch_size<br>    537 megabatches = [indices[i
          : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]<br>--&gt;
          538 megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True)
          for megabatch in megabatches]<br>    540 # The rest is to get the biggest
          batch first.<br>    541 # Since each megabatch is sorted by descending length,
          the longest element is the first<br>    542 megabatch_maximums = [lengths[megabatch[0]]
          for megabatch in megabatches]</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,
          in (.0)<br>    536 megabatch_size = mega_batch_mult * batch_size<br>    537
          megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0,
          len(lengths), megabatch_size)]<br>--&gt; 538 megabatches = [sorted(megabatch,
          key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]<br>    540
          # The rest is to get the biggest batch first.<br>    541 # Since each megabatch
          is sorted by descending length, the longest element is the first<br>    542
          megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,
          in get_length_grouped_indices..(i)<br>    536 megabatch_size = mega_batch_mult
          * batch_size<br>    537 megabatches = [indices[i : i + megabatch_size].tolist()
          for i in range(0, len(lengths), megabatch_size)]<br>--&gt; 538 megabatches
          = [sorted(megabatch, key=lambda i: lengths[i], reverse=True) for megabatch
          in megabatches]<br>    540 # The rest is to get the biggest batch first.<br>    541
          # Since each megabatch is sorted by descending length, the longest element
          is the first<br>    542 megabatch_maximums = [lengths[megabatch[0]] for
          megabatch in megabatches]</p>

          <p>KeyError: 1208</p>

          '
        raw: "> Thank you for your interest in Geneformer. This is a duplicate issue,\
          \ please see https://huggingface.co/ctheodoris/Geneformer/discussions/5.\n\
          \nThanks for the reply, but my question remains. Is the token_dictionary.pkl\
          \ used in pretrain_geneformer_w_deepspeed.py supposed to be a gene_ensembl_id:token\
          \ dictionary or a token_id:gene_ensembl_id dictionary?\nAlso, I have used\
          \ the latest geneformer and transformers 4.28.0, but no matter which dictionary\
          \ I use, the program throws an exception during training. How can I solve\
          \ this? Please let me know if you need more information.\n\nHere is the\
          \ exception information:\n---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[24], line 2\n      1 # train\n----> 2 trainer.train()\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1662,\
          \ in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,\
          \ **kwargs)\n   1657     self.model_wrapped = self.model\n   1659 inner_training_loop\
          \ = find_executable_batch_size(\n   1660     self._inner_training_loop,\
          \ self._train_batch_size, args.auto_find_batch_size\n   1661 )\n-> 1662\
          \ return inner_training_loop(\n   1663     args=args,\n   1664     resume_from_checkpoint=resume_from_checkpoint,\n\
          \   1665     trial=trial,\n   1666     ignore_keys_for_eval=ignore_keys_for_eval,\n\
          \   1667 )\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1899,\
          \ in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
          \ trial, ignore_keys_for_eval)\n   1896     rng_to_sync = True\n   1898\
          \ step = -1\n-> 1899 for step, inputs in enumerate(epoch_iterator):\n  \
          \ 1900     total_batched_samples += 1\n   1901     if rng_to_sync:\n\nFile\
          \ /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634,\
          \ in _BaseDataLoaderIter.__next__(self)\n    631 if self._sampler_iter is\
          \ None:\n    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n\
          \    633     self._reset()  # type: ignore[call-arg]\n--> 634 data = self._next_data()\n\
          \    635 self._num_yielded += 1\n    636 if self._dataset_kind == _DatasetKind.Iterable\
          \ and \\\n    637         self._IterableDataset_len_called is not None and\
          \ \\\n    638         self._num_yielded > self._IterableDataset_len_called:\n\
          \nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:677,\
          \ in _SingleProcessDataLoaderIter._next_data(self)\n    676 def _next_data(self):\n\
          --> 677     index = self._next_index()  # may raise StopIteration\n    678\
          \     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n\
          \    679     if self._pin_memory:\n\nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:624,\
          \ in _BaseDataLoaderIter._next_index(self)\n    623 def _next_index(self):\n\
          --> 624     return next(self._sampler_iter)\n\nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/sampler.py:254,\
          \ in BatchSampler.__iter__(self)\n    252 batch = [0] * self.batch_size\n\
          \    253 idx_in_batch = 0\n--> 254 for idx in self.sampler:\n    255   \
          \  batch[idx_in_batch] = idx\n    256     idx_in_batch += 1\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:592,\
          \ in LengthGroupedSampler.__iter__(self)\n    591 def __iter__(self):\n\
          --> 592     indices = get_length_grouped_indices(self.lengths, self.batch_size,\
          \ generator=self.generator)\n    593     return iter(indices)\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
          \ in get_length_grouped_indices(lengths, batch_size, mega_batch_mult, generator)\n\
          \    536 megabatch_size = mega_batch_mult * batch_size\n    537 megabatches\
          \ = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths),\
          \ megabatch_size)]\n--> 538 megabatches = [sorted(megabatch, key=lambda\
          \ i: lengths[i], reverse=True) for megabatch in megabatches]\n    540 #\
          \ The rest is to get the biggest batch first.\n    541 # Since each megabatch\
          \ is sorted by descending length, the longest element is the first\n   \
          \ 542 megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]\n\
          \nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
          \ in <listcomp>(.0)\n    536 megabatch_size = mega_batch_mult * batch_size\n\
          \    537 megabatches = [indices[i : i + megabatch_size].tolist() for i in\
          \ range(0, len(lengths), megabatch_size)]\n--> 538 megabatches = [sorted(megabatch,\
          \ key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]\n\
          \    540 # The rest is to get the biggest batch first.\n    541 # Since\
          \ each megabatch is sorted by descending length, the longest element is\
          \ the first\n    542 megabatch_maximums = [lengths[megabatch[0]] for megabatch\
          \ in megabatches]\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
          \ in get_length_grouped_indices.<locals>.<lambda>(i)\n    536 megabatch_size\
          \ = mega_batch_mult * batch_size\n    537 megabatches = [indices[i : i +\
          \ megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n\
          --> 538 megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True)\
          \ for megabatch in megabatches]\n    540 # The rest is to get the biggest\
          \ batch first.\n    541 # Since each megabatch is sorted by descending length,\
          \ the longest element is the first\n    542 megabatch_maximums = [lengths[megabatch[0]]\
          \ for megabatch in megabatches]\n\nKeyError: 1208\n"
        updatedAt: '2023-06-08T03:30:04.105Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64814b3c40facadc5579f477
    id: 64814b3c40facadc5579f475
    type: comment
  author: hua6886
  content: "> Thank you for your interest in Geneformer. This is a duplicate issue,\
    \ please see https://huggingface.co/ctheodoris/Geneformer/discussions/5.\n\nThanks\
    \ for the reply, but my question remains. Is the token_dictionary.pkl used in\
    \ pretrain_geneformer_w_deepspeed.py supposed to be a gene_ensembl_id:token dictionary\
    \ or a token_id:gene_ensembl_id dictionary?\nAlso, I have used the latest geneformer\
    \ and transformers 4.28.0, but no matter which dictionary I use, the program throws\
    \ an exception during training. How can I solve this? Please let me know if you\
    \ need more information.\n\nHere is the exception information:\n---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    Cell In[24], line 2\n      1 # train\n----> 2 trainer.train()\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1662,\
    \ in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,\
    \ **kwargs)\n   1657     self.model_wrapped = self.model\n   1659 inner_training_loop\
    \ = find_executable_batch_size(\n   1660     self._inner_training_loop, self._train_batch_size,\
    \ args.auto_find_batch_size\n   1661 )\n-> 1662 return inner_training_loop(\n\
    \   1663     args=args,\n   1664     resume_from_checkpoint=resume_from_checkpoint,\n\
    \   1665     trial=trial,\n   1666     ignore_keys_for_eval=ignore_keys_for_eval,\n\
    \   1667 )\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1899,\
    \ in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
    \ trial, ignore_keys_for_eval)\n   1896     rng_to_sync = True\n   1898 step =\
    \ -1\n-> 1899 for step, inputs in enumerate(epoch_iterator):\n   1900     total_batched_samples\
    \ += 1\n   1901     if rng_to_sync:\n\nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634,\
    \ in _BaseDataLoaderIter.__next__(self)\n    631 if self._sampler_iter is None:\n\
    \    632     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    633\
    \     self._reset()  # type: ignore[call-arg]\n--> 634 data = self._next_data()\n\
    \    635 self._num_yielded += 1\n    636 if self._dataset_kind == _DatasetKind.Iterable\
    \ and \\\n    637         self._IterableDataset_len_called is not None and \\\n\
    \    638         self._num_yielded > self._IterableDataset_len_called:\n\nFile\
    \ /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:677, in\
    \ _SingleProcessDataLoaderIter._next_data(self)\n    676 def _next_data(self):\n\
    --> 677     index = self._next_index()  # may raise StopIteration\n    678   \
    \  data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    679\
    \     if self._pin_memory:\n\nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:624,\
    \ in _BaseDataLoaderIter._next_index(self)\n    623 def _next_index(self):\n-->\
    \ 624     return next(self._sampler_iter)\n\nFile /usr/local/lib/python3.8/dist-packages/torch/utils/data/sampler.py:254,\
    \ in BatchSampler.__iter__(self)\n    252 batch = [0] * self.batch_size\n    253\
    \ idx_in_batch = 0\n--> 254 for idx in self.sampler:\n    255     batch[idx_in_batch]\
    \ = idx\n    256     idx_in_batch += 1\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:592,\
    \ in LengthGroupedSampler.__iter__(self)\n    591 def __iter__(self):\n--> 592\
    \     indices = get_length_grouped_indices(self.lengths, self.batch_size, generator=self.generator)\n\
    \    593     return iter(indices)\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
    \ in get_length_grouped_indices(lengths, batch_size, mega_batch_mult, generator)\n\
    \    536 megabatch_size = mega_batch_mult * batch_size\n    537 megabatches =\
    \ [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n\
    --> 538 megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True)\
    \ for megabatch in megabatches]\n    540 # The rest is to get the biggest batch\
    \ first.\n    541 # Since each megabatch is sorted by descending length, the longest\
    \ element is the first\n    542 megabatch_maximums = [lengths[megabatch[0]] for\
    \ megabatch in megabatches]\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
    \ in <listcomp>(.0)\n    536 megabatch_size = mega_batch_mult * batch_size\n \
    \   537 megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0,\
    \ len(lengths), megabatch_size)]\n--> 538 megabatches = [sorted(megabatch, key=lambda\
    \ i: lengths[i], reverse=True) for megabatch in megabatches]\n    540 # The rest\
    \ is to get the biggest batch first.\n    541 # Since each megabatch is sorted\
    \ by descending length, the longest element is the first\n    542 megabatch_maximums\
    \ = [lengths[megabatch[0]] for megabatch in megabatches]\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:538,\
    \ in get_length_grouped_indices.<locals>.<lambda>(i)\n    536 megabatch_size =\
    \ mega_batch_mult * batch_size\n    537 megabatches = [indices[i : i + megabatch_size].tolist()\
    \ for i in range(0, len(lengths), megabatch_size)]\n--> 538 megabatches = [sorted(megabatch,\
    \ key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]\n    540\
    \ # The rest is to get the biggest batch first.\n    541 # Since each megabatch\
    \ is sorted by descending length, the longest element is the first\n    542 megabatch_maximums\
    \ = [lengths[megabatch[0]] for megabatch in megabatches]\n\nKeyError: 1208\n"
  created_at: 2023-06-08 02:30:04+00:00
  edited: false
  hidden: false
  id: 64814b3c40facadc5579f475
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
      fullname: Yuwei Hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hua6886
      type: user
    createdAt: '2023-06-08T03:30:04.000Z'
    data:
      status: open
    id: 64814b3c40facadc5579f477
    type: status-change
  author: hua6886
  created_at: 2023-06-08 02:30:04+00:00
  id: 64814b3c40facadc5579f477
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-08T05:32:19.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.934816837310791
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your follow up question. The token dictionary should
          be a gene_ensembl_id:token dictionary as indicated in the example notebook.
          The token dictionary in this model hub repository was in this format, but
          I updated the one in the Genecorpus-30M dataset repository to be consistent
          - I assume that is where you were pulling the dictionary from.</p>

          <p>Based on your follow up, it appears the initial error you were encountering
          is now resolved based on changes made in response to the duplicate issue
          (<a href="https://huggingface.co/ctheodoris/Geneformer/discussions/5">https://huggingface.co/ctheodoris/Geneformer/discussions/5</a>).
          </p>

          <p>The new error that you are encountering appears to be coming from "lengths[i]"
          in the transformers trainer_pt_utils.py based on what you provided. Could
          you provide more information regarding what you are using as the lengths
          and dataset file for pretraining so that I can reproduce the error?</p>

          '
        raw: "Thank you for your follow up question. The token dictionary should be\
          \ a gene_ensembl_id:token dictionary as indicated in the example notebook.\
          \ The token dictionary in this model hub repository was in this format,\
          \ but I updated the one in the Genecorpus-30M dataset repository to be consistent\
          \ - I assume that is where you were pulling the dictionary from.\n\nBased\
          \ on your follow up, it appears the initial error you were encountering\
          \ is now resolved based on changes made in response to the duplicate issue\
          \ (https://huggingface.co/ctheodoris/Geneformer/discussions/5). \n\nThe\
          \ new error that you are encountering appears to be coming from \"lengths[i]\"\
          \ in the transformers trainer_pt_utils.py based on what you provided. Could\
          \ you provide more information regarding what you are using as the lengths\
          \ and dataset file for pretraining so that I can reproduce the error?"
        updatedAt: '2023-06-08T05:32:19.387Z'
      numEdits: 0
      reactions: []
    id: 648167e3e1421e205fe677a5
    type: comment
  author: ctheodoris
  content: "Thank you for your follow up question. The token dictionary should be\
    \ a gene_ensembl_id:token dictionary as indicated in the example notebook. The\
    \ token dictionary in this model hub repository was in this format, but I updated\
    \ the one in the Genecorpus-30M dataset repository to be consistent - I assume\
    \ that is where you were pulling the dictionary from.\n\nBased on your follow\
    \ up, it appears the initial error you were encountering is now resolved based\
    \ on changes made in response to the duplicate issue (https://huggingface.co/ctheodoris/Geneformer/discussions/5).\
    \ \n\nThe new error that you are encountering appears to be coming from \"lengths[i]\"\
    \ in the transformers trainer_pt_utils.py based on what you provided. Could you\
    \ provide more information regarding what you are using as the lengths and dataset\
    \ file for pretraining so that I can reproduce the error?"
  created_at: 2023-06-08 04:32:19+00:00
  edited: false
  hidden: false
  id: 648167e3e1421e205fe677a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-10T22:25:47.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9708748459815979
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>I believe you emailed me separately regarding this issue, and the
          issue was that you were providing a gene name dictionary rather than the
          lengths dictionary to the trainer. I am assuming providing the lengths dictionary
          resolved the issue and am closing this issue. However, feel free to reopen
          if needed.</p>

          '
        raw: I believe you emailed me separately regarding this issue, and the issue
          was that you were providing a gene name dictionary rather than the lengths
          dictionary to the trainer. I am assuming providing the lengths dictionary
          resolved the issue and am closing this issue. However, feel free to reopen
          if needed.
        updatedAt: '2023-06-10T22:25:47.237Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6484f86bd86bf0201ebe38a4
    id: 6484f86bd86bf0201ebe3894
    type: comment
  author: ctheodoris
  content: I believe you emailed me separately regarding this issue, and the issue
    was that you were providing a gene name dictionary rather than the lengths dictionary
    to the trainer. I am assuming providing the lengths dictionary resolved the issue
    and am closing this issue. However, feel free to reopen if needed.
  created_at: 2023-06-10 21:25:47+00:00
  edited: false
  hidden: false
  id: 6484f86bd86bf0201ebe3894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-10T22:25:47.000Z'
    data:
      status: closed
    id: 6484f86bd86bf0201ebe38a4
    type: status-change
  author: ctheodoris
  created_at: 2023-06-10 21:25:47+00:00
  id: 6484f86bd86bf0201ebe38a4
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
      fullname: Yuwei Hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hua6886
      type: user
    createdAt: '2023-06-12T08:25:14.000Z'
    data:
      edited: false
      editors:
      - hua6886
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.98613440990448
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e329306394c1fa6b23a666a9d8c9dfb.svg
          fullname: Yuwei Hua
          isHf: false
          isPro: false
          name: hua6886
          type: user
        html: '<p>I am pleased to report that all the issues I previously faced have
          now been resolved. I greatly appreciate your diligent and patient guidance
          throughout this process. Thank you very much for your assistance!</p>

          '
        raw: I am pleased to report that all the issues I previously faced have now
          been resolved. I greatly appreciate your diligent and patient guidance throughout
          this process. Thank you very much for your assistance!
        updatedAt: '2023-06-12T08:25:14.350Z'
      numEdits: 0
      reactions: []
    id: 6486d66aabb704802413be60
    type: comment
  author: hua6886
  content: I am pleased to report that all the issues I previously faced have now
    been resolved. I greatly appreciate your diligent and patient guidance throughout
    this process. Thank you very much for your assistance!
  created_at: 2023-06-12 07:25:14+00:00
  edited: false
  hidden: false
  id: 6486d66aabb704802413be60
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: 'Confusing "token_dictionary.pkl" file '
