!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yanwu2014
conflicting_files: null
created_at: 2023-07-01 15:31:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1e635345913ce28d70351e1df73ecf.svg
      fullname: Yan Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanwu2014
      type: user
    createdAt: '2023-07-01T16:31:38.000Z'
    data:
      edited: false
      editors:
      - yanwu2014
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6016225814819336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1e635345913ce28d70351e1df73ecf.svg
          fullname: Yan Wu
          isHf: false
          isPro: false
          name: yanwu2014
          type: user
        html: "<p>Thanks for the great package! So when I try to tokenize a large\
          \ dataset (1 million + cells) I'm getting this error</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/ywu/git-repos/prime-analysis/scripts/tokenize_data.py\"\
          , line 12, in &lt;module&gt;\n\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
          , line 102, in tokenize_data\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
          \ cell_metadata)\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
          , line 194, in create_dataset\n    output_dataset = Dataset.from_dict(dataset_dict)\n\
          \  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_dataset.py\"\
          , line 897, in from_dict\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\n\
          \  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/table.py\"\
          , line 785, in from_pydict\n    return cls(pa.Table.from_pydict(*args, **kwargs))\n\
          \  File \"pyarrow/table.pxi\", line 3725, in pyarrow.lib.Table.from_pydict\n\
          \  File \"pyarrow/table.pxi\", line 5254, in pyarrow.lib._from_pydict\n\
          \  File \"pyarrow/array.pxi\", line 350, in pyarrow.lib.asarray\n  File\
          \ \"pyarrow/array.pxi\", line 236, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\"\
          , line 110, in pyarrow.lib._handle_arrow_array_protocol\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_writer.py\"\
          , line 186, in __arrow_array__\n    out = list_of_np_array_to_pyarrow_listarray(data)\n\
          \  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
          , line 1406, in list_of_np_array_to_pyarrow_listarray\n    return list_of_pa_arrays_to_pyarrow_listarray(\n\
          \  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
          , line 1398, in list_of_pa_arrays_to_pyarrow_listarray\n    offsets = pa.array(offsets,\
          \ type=pa.int32())\n  File \"pyarrow/array.pxi\", line 316, in pyarrow.lib.array\n\
          \  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n\
          \  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid:\
          \ Value 2147486544 too large to fit in C integer type\n</code></pre>\n<p>It's\
          \ a bit unclear to me where the very large number that doesn't fit in a\
          \ 32 bit int even comes from, any thoughts on what might be happening?</p>\n"
        raw: "Thanks for the great package! So when I try to tokenize a large dataset\
          \ (1 million + cells) I'm getting this error\r\n```\r\nTraceback (most recent\
          \ call last):\r\n  File \"/home/ywu/git-repos/prime-analysis/scripts/tokenize_data.py\"\
          , line 12, in <module>\r\n\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
          , line 102, in tokenize_data\r\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
          \ cell_metadata)\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
          , line 194, in create_dataset\r\n    output_dataset = Dataset.from_dict(dataset_dict)\r\
          \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_dataset.py\"\
          , line 897, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\
          \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/table.py\"\
          , line 785, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args,\
          \ **kwargs))\r\n  File \"pyarrow/table.pxi\", line 3725, in pyarrow.lib.Table.from_pydict\r\
          \n  File \"pyarrow/table.pxi\", line 5254, in pyarrow.lib._from_pydict\r\
          \n  File \"pyarrow/array.pxi\", line 350, in pyarrow.lib.asarray\r\n  File\
          \ \"pyarrow/array.pxi\", line 236, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\"\
          , line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_writer.py\"\
          , line 186, in __arrow_array__\r\n    out = list_of_np_array_to_pyarrow_listarray(data)\r\
          \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
          , line 1406, in list_of_np_array_to_pyarrow_listarray\r\n    return list_of_pa_arrays_to_pyarrow_listarray(\r\
          \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
          , line 1398, in list_of_pa_arrays_to_pyarrow_listarray\r\n    offsets =\
          \ pa.array(offsets, type=pa.int32())\r\n  File \"pyarrow/array.pxi\", line\
          \ 316, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 83, in\
          \ pyarrow.lib._ndarray_to_array\r\n  File \"pyarrow/error.pxi\", line 100,\
          \ in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Value 2147486544\
          \ too large to fit in C integer type\r\n```\r\n\r\nIt's a bit unclear to\
          \ me where the very large number that doesn't fit in a 32 bit int even comes\
          \ from, any thoughts on what might be happening?"
        updatedAt: '2023-07-01T16:31:38.261Z'
      numEdits: 0
      reactions: []
    id: 64a054ea19a15b1fc994f48b
    type: comment
  author: yanwu2014
  content: "Thanks for the great package! So when I try to tokenize a large dataset\
    \ (1 million + cells) I'm getting this error\r\n```\r\nTraceback (most recent\
    \ call last):\r\n  File \"/home/ywu/git-repos/prime-analysis/scripts/tokenize_data.py\"\
    , line 12, in <module>\r\n\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
    , line 102, in tokenize_data\r\n    tokenized_dataset = self.create_dataset(tokenized_cells,\
    \ cell_metadata)\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/geneformer/tokenizer.py\"\
    , line 194, in create_dataset\r\n    output_dataset = Dataset.from_dict(dataset_dict)\r\
    \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_dataset.py\"\
    , line 897, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\
    \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/table.py\"\
    , line 785, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\
    \n  File \"pyarrow/table.pxi\", line 3725, in pyarrow.lib.Table.from_pydict\r\n\
    \  File \"pyarrow/table.pxi\", line 5254, in pyarrow.lib._from_pydict\r\n  File\
    \ \"pyarrow/array.pxi\", line 350, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\"\
    , line 236, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in\
    \ pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/arrow_writer.py\"\
    , line 186, in __arrow_array__\r\n    out = list_of_np_array_to_pyarrow_listarray(data)\r\
    \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
    , line 1406, in list_of_np_array_to_pyarrow_listarray\r\n    return list_of_pa_arrays_to_pyarrow_listarray(\r\
    \n  File \"/share/ywu/anaconda3/envs/geneformer-env/lib/python3.10/site-packages/datasets/features/features.py\"\
    , line 1398, in list_of_pa_arrays_to_pyarrow_listarray\r\n    offsets = pa.array(offsets,\
    \ type=pa.int32())\r\n  File \"pyarrow/array.pxi\", line 316, in pyarrow.lib.array\r\
    \n  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\r\n\
    \  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid:\
    \ Value 2147486544 too large to fit in C integer type\r\n```\r\n\r\nIt's a bit\
    \ unclear to me where the very large number that doesn't fit in a 32 bit int even\
    \ comes from, any thoughts on what might be happening?"
  created_at: 2023-07-01 15:31:38+00:00
  edited: false
  hidden: false
  id: 64a054ea19a15b1fc994f48b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-01T18:10:58.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9196269512176514
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! We did not encounter
          this error when tokenizing Genecorpus-30M, which is ~30M cells. We would
          suggest you check the issues reported for Huggingface Datasets and/or open
          a new issue relating to this. It will be helpful to include information
          relating to the number of examples (cells) and the number of tokens per
          example (genes per cell) you in your dataset.</p>

          '
        raw: Thank you for your interest in Geneformer! We did not encounter this
          error when tokenizing Genecorpus-30M, which is ~30M cells. We would suggest
          you check the issues reported for Huggingface Datasets and/or open a new
          issue relating to this. It will be helpful to include information relating
          to the number of examples (cells) and the number of tokens per example (genes
          per cell) you in your dataset.
        updatedAt: '2023-07-01T18:10:58.253Z'
      numEdits: 0
      reactions: []
    id: 64a06c329744fc9bc7f9ce92
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! We did not encounter this error
    when tokenizing Genecorpus-30M, which is ~30M cells. We would suggest you check
    the issues reported for Huggingface Datasets and/or open a new issue relating
    to this. It will be helpful to include information relating to the number of examples
    (cells) and the number of tokens per example (genes per cell) you in your dataset.
  created_at: 2023-07-01 17:10:58+00:00
  edited: false
  hidden: false
  id: 64a06c329744fc9bc7f9ce92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-13T01:15:21.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580767154693604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Closing this discussion for now, but please feel free to update
          with information that may be helpful to others who have the same question
          if you resolved this.</p>

          '
        raw: Closing this discussion for now, but please feel free to update with
          information that may be helpful to others who have the same question if
          you resolved this.
        updatedAt: '2023-07-13T01:15:21.949Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64af5029a609b29cc7b11262
    id: 64af5029a609b29cc7b11260
    type: comment
  author: ctheodoris
  content: Closing this discussion for now, but please feel free to update with information
    that may be helpful to others who have the same question if you resolved this.
  created_at: 2023-07-13 00:15:21+00:00
  edited: false
  hidden: false
  id: 64af5029a609b29cc7b11260
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-13T01:15:21.000Z'
    data:
      status: closed
    id: 64af5029a609b29cc7b11262
    type: status-change
  author: ctheodoris
  created_at: 2023-07-13 00:15:21+00:00
  id: 64af5029a609b29cc7b11262
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-03T22:56:53.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2528248727321625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ctheodoris&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ctheodoris\">@<span class=\"\
          underline\">ctheodoris</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;yanwu2014&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/yanwu2014\">@<span class=\"underline\">yanwu2014</span></a></span>\n\
          \n\t</span></span> I ran into the same error and fixed it by changing the\
          \ <code>TranscriptomeTokenizer.create_dataset()</code> function to the following:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-comment\"># [...]</span>\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >create_dataset</span>(<span class=\"hljs-params\">self, tokenized_cells,\
          \ cell_metadata</span>):\n        <span class=\"hljs-comment\"># create\
          \ dict for dataset creation</span>\n        dataset_dict = {<span class=\"\
          hljs-string\">\"input_ids\"</span>: tokenized_cells}\n        <span class=\"\
          hljs-keyword\">if</span> self.custom_attr_name_dict <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n            dataset_dict.update(cell_metadata)\n       \n\
          \        <span class=\"hljs-comment\"># changed this line:</span>\n    \
          \    <span class=\"hljs-comment\"># output_dataset = Dataset.from_dict(dataset_dict)</span>\n\
          \n        <span class=\"hljs-comment\"># to this:</span>\n        <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >gen</span>():\n            <span class=\"hljs-keyword\">for</span> i <span\
          \ class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span\
          \ class=\"hljs-built_in\">len</span>(tokenized_cells)):\n              \
          \  <span class=\"hljs-keyword\">yield</span> {<span class=\"hljs-string\"\
          >'input_ids'</span>: dataset_dict[<span class=\"hljs-string\">'input_ids'</span>][i],\
          \ <span class=\"hljs-string\">'cell_type'</span>: dataset_dict[<span class=\"\
          hljs-string\">'cell_type'</span>][i]}\n        output_dataset = Dataset.from_generator(gen,\
          \ num_proc=self.nproc)\n\n        <span class=\"hljs-comment\"># truncate\
          \ dataset</span>\n        <span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">truncate</span>(<span class=\"hljs-params\"\
          >example</span>):\n            example[<span class=\"hljs-string\">\"input_ids\"\
          </span>] = example[<span class=\"hljs-string\">\"input_ids\"</span>][<span\
          \ class=\"hljs-number\">0</span>:<span class=\"hljs-number\">2048</span>]\n\
          \            <span class=\"hljs-keyword\">return</span> example\n\n    \
          \    output_dataset_truncated = output_dataset.<span class=\"hljs-built_in\"\
          >map</span>(truncate, num_proc=self.nproc)\n\n        <span class=\"hljs-comment\"\
          ># measure lengths of dataset</span>\n        <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">measure_length</span>(<span\
          \ class=\"hljs-params\">example</span>):\n            example[<span class=\"\
          hljs-string\">\"length\"</span>] = <span class=\"hljs-built_in\">len</span>(example[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>])\n            <span class=\"\
          hljs-keyword\">return</span> example\n\n        output_dataset_truncated_w_length\
          \ = output_dataset_truncated.<span class=\"hljs-built_in\">map</span>(\n\
          \            measure_length, num_proc=self.nproc\n        )\n\n        <span\
          \ class=\"hljs-keyword\">return</span> output_dataset_truncated_w_length\n\
          </code></pre>\n"
        raw: "@ctheodoris @yanwu2014 I ran into the same error and fixed it by changing\
          \ the `TranscriptomeTokenizer.create_dataset()` function to the following:\n\
          \n```python\n# [...]\n\ndef create_dataset(self, tokenized_cells, cell_metadata):\n\
          \        # create dict for dataset creation\n        dataset_dict = {\"\
          input_ids\": tokenized_cells}\n        if self.custom_attr_name_dict is\
          \ not None:\n            dataset_dict.update(cell_metadata)\n       \n \
          \       # changed this line:\n        # output_dataset = Dataset.from_dict(dataset_dict)\n\
          \n        # to this:\n        def gen():\n            for i in range(len(tokenized_cells)):\n\
          \                yield {'input_ids': dataset_dict['input_ids'][i], 'cell_type':\
          \ dataset_dict['cell_type'][i]}\n        output_dataset = Dataset.from_generator(gen,\
          \ num_proc=self.nproc)\n\n        # truncate dataset\n        def truncate(example):\n\
          \            example[\"input_ids\"] = example[\"input_ids\"][0:2048]\n \
          \           return example\n\n        output_dataset_truncated = output_dataset.map(truncate,\
          \ num_proc=self.nproc)\n\n        # measure lengths of dataset\n       \
          \ def measure_length(example):\n            example[\"length\"] = len(example[\"\
          input_ids\"])\n            return example\n\n        output_dataset_truncated_w_length\
          \ = output_dataset_truncated.map(\n            measure_length, num_proc=self.nproc\n\
          \        )\n\n        return output_dataset_truncated_w_length\n```"
        updatedAt: '2023-08-03T22:56:53.148Z'
      numEdits: 0
      reactions: []
    id: 64cc30b5275c7630460b3d7f
    type: comment
  author: ricomnl
  content: "@ctheodoris @yanwu2014 I ran into the same error and fixed it by changing\
    \ the `TranscriptomeTokenizer.create_dataset()` function to the following:\n\n\
    ```python\n# [...]\n\ndef create_dataset(self, tokenized_cells, cell_metadata):\n\
    \        # create dict for dataset creation\n        dataset_dict = {\"input_ids\"\
    : tokenized_cells}\n        if self.custom_attr_name_dict is not None:\n     \
    \       dataset_dict.update(cell_metadata)\n       \n        # changed this line:\n\
    \        # output_dataset = Dataset.from_dict(dataset_dict)\n\n        # to this:\n\
    \        def gen():\n            for i in range(len(tokenized_cells)):\n     \
    \           yield {'input_ids': dataset_dict['input_ids'][i], 'cell_type': dataset_dict['cell_type'][i]}\n\
    \        output_dataset = Dataset.from_generator(gen, num_proc=self.nproc)\n\n\
    \        # truncate dataset\n        def truncate(example):\n            example[\"\
    input_ids\"] = example[\"input_ids\"][0:2048]\n            return example\n\n\
    \        output_dataset_truncated = output_dataset.map(truncate, num_proc=self.nproc)\n\
    \n        # measure lengths of dataset\n        def measure_length(example):\n\
    \            example[\"length\"] = len(example[\"input_ids\"])\n            return\
    \ example\n\n        output_dataset_truncated_w_length = output_dataset_truncated.map(\n\
    \            measure_length, num_proc=self.nproc\n        )\n\n        return\
    \ output_dataset_truncated_w_length\n```"
  created_at: 2023-08-03 21:56:53+00:00
  edited: false
  hidden: false
  id: 64cc30b5275c7630460b3d7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-03T23:24:59.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629766345024109
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for this update! From your experience did you notice that\
          \ this error was occurring with datasets above a given size? Also did you\
          \ happen to note whether the method you changed it to is just as fast as\
          \ the original method for datasets where they don\u2019t trigger this error?\
          \ Thank you!</p>\n"
        raw: "Thank you for this update! From your experience did you notice that\
          \ this error was occurring with datasets above a given size? Also did you\
          \ happen to note whether the method you changed it to is just as fast as\
          \ the original method for datasets where they don\u2019t trigger this error?\
          \ Thank you!"
        updatedAt: '2023-08-03T23:24:59.760Z'
      numEdits: 0
      reactions: []
    id: 64cc374b710645aa7b1922f2
    type: comment
  author: ctheodoris
  content: "Thank you for this update! From your experience did you notice that this\
    \ error was occurring with datasets above a given size? Also did you happen to\
    \ note whether the method you changed it to is just as fast as the original method\
    \ for datasets where they don\u2019t trigger this error? Thank you!"
  created_at: 2023-08-03 22:24:59+00:00
  edited: false
  hidden: false
  id: 64cc374b710645aa7b1922f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-04T01:33:30.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9503157734870911
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: "<blockquote>\n<p>From your experience did you notice that this error\
          \ was occurring with datasets above a given size?</p>\n</blockquote>\n<p>Roughly,\
          \ it worked for 500k cells and broke for 1M. This updated version immediately\
          \ worked for 1M. </p>\n<blockquote>\n<p>Also did you happen to note whether\
          \ the method you changed it to is just as fast as the original method for\
          \ datasets where they don\u2019t trigger this error? </p>\n</blockquote>\n\
          <p>I should mention that I used the anndata tokenizer from here: <a href=\"\
          https://huggingface.co/ctheodoris/Geneformer/discussions/102\">https://huggingface.co/ctheodoris/Geneformer/discussions/102</a>\
          \ because when converting from adata to loom apparently loom needs to allocate\
          \ a whole dense array and that broke my RAM. With that, tokenizing took\
          \ 251.39s for 500k cells with my updated version above and 311.28s with\
          \ the old version (that used <code>.from_dict()</code>).</p>\n"
        raw: "> From your experience did you notice that this error was occurring\
          \ with datasets above a given size?\n\nRoughly, it worked for 500k cells\
          \ and broke for 1M. This updated version immediately worked for 1M. \n\n\
          > Also did you happen to note whether the method you changed it to is just\
          \ as fast as the original method for datasets where they don\u2019t trigger\
          \ this error? \n\nI should mention that I used the anndata tokenizer from\
          \ here: https://huggingface.co/ctheodoris/Geneformer/discussions/102 because\
          \ when converting from adata to loom apparently loom needs to allocate a\
          \ whole dense array and that broke my RAM. With that, tokenizing took 251.39s\
          \ for 500k cells with my updated version above and 311.28s with the old\
          \ version (that used `.from_dict()`)."
        updatedAt: '2023-08-04T01:33:30.476Z'
      numEdits: 0
      reactions: []
    id: 64cc556a86d8dc0caa687684
    type: comment
  author: ricomnl
  content: "> From your experience did you notice that this error was occurring with\
    \ datasets above a given size?\n\nRoughly, it worked for 500k cells and broke\
    \ for 1M. This updated version immediately worked for 1M. \n\n> Also did you happen\
    \ to note whether the method you changed it to is just as fast as the original\
    \ method for datasets where they don\u2019t trigger this error? \n\nI should mention\
    \ that I used the anndata tokenizer from here: https://huggingface.co/ctheodoris/Geneformer/discussions/102\
    \ because when converting from adata to loom apparently loom needs to allocate\
    \ a whole dense array and that broke my RAM. With that, tokenizing took 251.39s\
    \ for 500k cells with my updated version above and 311.28s with the old version\
    \ (that used `.from_dict()`)."
  created_at: 2023-08-04 00:33:30+00:00
  edited: false
  hidden: false
  id: 64cc556a86d8dc0caa687684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-04T01:55:36.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9122191667556763
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for the information and for also confirming that the anndata\
          \ tokenizer version works with large datasets. Do you want to submit a pull\
          \ request with the version you implemented? We did not encounter the .from_dict\
          \ error when tokenizing the 30M cells in Genecorpus-30M so I am not certain\
          \ the specific case that is revealing this error, but it\u2019s very helpful\
          \ that you updated here with your solution.</p>\n"
        raw: "Thank you for the information and for also confirming that the anndata\
          \ tokenizer version works with large datasets. Do you want to submit a pull\
          \ request with the version you implemented? We did not encounter the .from_dict\
          \ error when tokenizing the 30M cells in Genecorpus-30M so I am not certain\
          \ the specific case that is revealing this error, but it\u2019s very helpful\
          \ that you updated here with your solution."
        updatedAt: '2023-08-04T01:55:36.756Z'
      numEdits: 0
      reactions: []
    id: 64cc5a987221ef3c7e67c917
    type: comment
  author: ctheodoris
  content: "Thank you for the information and for also confirming that the anndata\
    \ tokenizer version works with large datasets. Do you want to submit a pull request\
    \ with the version you implemented? We did not encounter the .from_dict error\
    \ when tokenizing the 30M cells in Genecorpus-30M so I am not certain the specific\
    \ case that is revealing this error, but it\u2019s very helpful that you updated\
    \ here with your solution."
  created_at: 2023-08-04 00:55:36+00:00
  edited: false
  hidden: false
  id: 64cc5a987221ef3c7e67c917
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-04T03:08:55.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6850226521492004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: '<p>Sure, here goes: <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/170">https://huggingface.co/ctheodoris/Geneformer/discussions/170</a></p>

          '
        raw: 'Sure, here goes: https://huggingface.co/ctheodoris/Geneformer/discussions/170'
        updatedAt: '2023-08-04T03:08:55.431Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - yanwu2014
    id: 64cc6bc7dc4e838857016543
    type: comment
  author: ricomnl
  content: 'Sure, here goes: https://huggingface.co/ctheodoris/Geneformer/discussions/170'
  created_at: 2023-08-04 02:08:55+00:00
  edited: false
  hidden: false
  id: 64cc6bc7dc4e838857016543
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a1e635345913ce28d70351e1df73ecf.svg
      fullname: Yan Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanwu2014
      type: user
    createdAt: '2023-08-07T22:59:18.000Z'
    data:
      edited: false
      editors:
      - yanwu2014
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712759256362915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a1e635345913ce28d70351e1df73ecf.svg
          fullname: Yan Wu
          isHf: false
          isPro: false
          name: yanwu2014
          type: user
        html: '<p>Just tested the PR and it seemed to fix my issue, thanks for writing
          this!</p>

          '
        raw: Just tested the PR and it seemed to fix my issue, thanks for writing
          this!
        updatedAt: '2023-08-07T22:59:18.614Z'
      numEdits: 0
      reactions: []
    id: 64d17746132efbe2dc0608eb
    type: comment
  author: yanwu2014
  content: Just tested the PR and it seemed to fix my issue, thanks for writing this!
  created_at: 2023-08-07 21:59:18+00:00
  edited: false
  hidden: false
  id: 64d17746132efbe2dc0608eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 80
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Error when tokenizing large datasets
