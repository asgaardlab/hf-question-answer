!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fczqx
conflicting_files: null
created_at: 2023-11-21 01:07:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-21T01:07:29.000Z'
    data:
      edited: true
      editors:
      - fczqx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9566130042076111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
          fullname: Chao Fang
          isHf: false
          isPro: false
          name: fczqx
          type: user
        html: '<p>I''ve several single cell expression files (about 1Tb size and 100+
          expression files ) and I want to fine-tuning them based on geneformer basic
          foundation model, however when I followed the recipe with following link
          as :<a href="https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/cell_classification.ipynb">https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/cell_classification.ipynb</a>,
          it was too slow to training the process just one file by one file(load_from_disk
          function seems only support to read one dataset ). how can I accelerate
          the training process or how to parallel run them. Have you any suggestion
          about it? By the way, we have totally 24 4090 GC spread across 12 servers.
          Thank you.</p>

          '
        raw: 'I''ve several single cell expression files (about 1Tb size and 100+
          expression files ) and I want to fine-tuning them based on geneformer basic
          foundation model, however when I followed the recipe with following link
          as :https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/cell_classification.ipynb,
          it was too slow to training the process just one file by one file(load_from_disk
          function seems only support to read one dataset ). how can I accelerate
          the training process or how to parallel run them. Have you any suggestion
          about it? By the way, we have totally 24 4090 GC spread across 12 servers.
          Thank you.

          '
        updatedAt: '2023-11-21T01:15:40.961Z'
      numEdits: 1
      reactions: []
    id: 655c02d113309a611b9b62f3
    type: comment
  author: fczqx
  content: 'I''ve several single cell expression files (about 1Tb size and 100+ expression
    files ) and I want to fine-tuning them based on geneformer basic foundation model,
    however when I followed the recipe with following link as :https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/cell_classification.ipynb,
    it was too slow to training the process just one file by one file(load_from_disk
    function seems only support to read one dataset ). how can I accelerate the training
    process or how to parallel run them. Have you any suggestion about it? By the
    way, we have totally 24 4090 GC spread across 12 servers. Thank you.

    '
  created_at: 2023-11-21 01:07:29+00:00
  edited: true
  hidden: false
  id: 655c02d113309a611b9b62f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-21T06:31:32.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9375383853912354
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! My understanding based
          on your comment is that you have multiple datasets for fine-tuning but you
          are running the fine-tuning with one dataset at a time. For fine-tuning
          the model, this is best done by shuffling all of the training data so that
          the model sees a good representation of the data space with each batch.
          You can tokenize the data into a single .dataset by providing the directory
          with all of the loom or anndata files contained. Then, you would only need
          to load the data once and the batches can draw from the full representation
          of the training data. The first time loading a large dataset can take a
          while, but after that operations should be fast as Huggingface Datasets
          operates with caches and zero copy reads. Regarding distributed training,
          I would suggest using Deepspeed. Hugging Face has Deepspeed integration
          that you can use with the trainer in the cell_classification.ipynb example.
          </p>

          <p><a href="https://huggingface.co/docs/transformers/main_classes/deepspeed">https://huggingface.co/docs/transformers/main_classes/deepspeed</a></p>

          '
        raw: "Thank you for your interest in Geneformer! My understanding based on\
          \ your comment is that you have multiple datasets for fine-tuning but you\
          \ are running the fine-tuning with one dataset at a time. For fine-tuning\
          \ the model, this is best done by shuffling all of the training data so\
          \ that the model sees a good representation of the data space with each\
          \ batch. You can tokenize the data into a single .dataset by providing the\
          \ directory with all of the loom or anndata files contained. Then, you would\
          \ only need to load the data once and the batches can draw from the full\
          \ representation of the training data. The first time loading a large dataset\
          \ can take a while, but after that operations should be fast as Huggingface\
          \ Datasets operates with caches and zero copy reads. Regarding distributed\
          \ training, I would suggest using Deepspeed. Hugging Face has Deepspeed\
          \ integration that you can use with the trainer in the cell_classification.ipynb\
          \ example. \n\nhttps://huggingface.co/docs/transformers/main_classes/deepspeed"
        updatedAt: '2023-11-21T06:31:32.509Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655c4ec4935d0f9a75ee233b
    id: 655c4ec4935d0f9a75ee233a
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer! My understanding based on your\
    \ comment is that you have multiple datasets for fine-tuning but you are running\
    \ the fine-tuning with one dataset at a time. For fine-tuning the model, this\
    \ is best done by shuffling all of the training data so that the model sees a\
    \ good representation of the data space with each batch. You can tokenize the\
    \ data into a single .dataset by providing the directory with all of the loom\
    \ or anndata files contained. Then, you would only need to load the data once\
    \ and the batches can draw from the full representation of the training data.\
    \ The first time loading a large dataset can take a while, but after that operations\
    \ should be fast as Huggingface Datasets operates with caches and zero copy reads.\
    \ Regarding distributed training, I would suggest using Deepspeed. Hugging Face\
    \ has Deepspeed integration that you can use with the trainer in the cell_classification.ipynb\
    \ example. \n\nhttps://huggingface.co/docs/transformers/main_classes/deepspeed"
  created_at: 2023-11-21 06:31:32+00:00
  edited: false
  hidden: false
  id: 655c4ec4935d0f9a75ee233a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-21T06:31:32.000Z'
    data:
      status: closed
    id: 655c4ec4935d0f9a75ee233b
    type: status-change
  author: ctheodoris
  created_at: 2023-11-21 06:31:32+00:00
  id: 655c4ec4935d0f9a75ee233b
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-21T09:34:41.000Z'
    data:
      edited: false
      editors:
      - fczqx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8655585646629333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
          fullname: Chao Fang
          isHf: false
          isPro: false
          name: fczqx
          type: user
        html: '<p>thanks for your reply. As you said, I''ve converted these anndata
          files into arrow format already. And they are restored by each tissue as
          following image, so the only way can I  do is rename the arrow file with
          their tissue name and mv them into the same directory?<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/Q9bT_Bmm_Sm-rQO-Ssnnc.png"><img
          alt="ss1.png" src="https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/Q9bT_Bmm_Sm-rQO-Ssnnc.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/5WDYTVNh_sil4ZyKtGL_k.png"><img
          alt="ss2.png" src="https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/5WDYTVNh_sil4ZyKtGL_k.png"></a></p>

          '
        raw: "thanks for your reply. As you said, I've converted these anndata files\
          \ into arrow format already. And they are restored by each tissue as following\
          \ image, so the only way can I  do is rename the arrow file with their tissue\
          \ name and mv them into the same directory? \n![ss1.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/Q9bT_Bmm_Sm-rQO-Ssnnc.png)\n\
          ![ss2.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/5WDYTVNh_sil4ZyKtGL_k.png)\n"
        updatedAt: '2023-11-21T09:34:41.895Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655c79b1a9b72292b16fd631
    id: 655c79b1a9b72292b16fd62f
    type: comment
  author: fczqx
  content: "thanks for your reply. As you said, I've converted these anndata files\
    \ into arrow format already. And they are restored by each tissue as following\
    \ image, so the only way can I  do is rename the arrow file with their tissue\
    \ name and mv them into the same directory? \n![ss1.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/Q9bT_Bmm_Sm-rQO-Ssnnc.png)\n\
    ![ss2.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/5WDYTVNh_sil4ZyKtGL_k.png)\n"
  created_at: 2023-11-21 09:34:41+00:00
  edited: false
  hidden: false
  id: 655c79b1a9b72292b16fd62f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-21T09:34:41.000Z'
    data:
      status: open
    id: 655c79b1a9b72292b16fd631
    type: status-change
  author: fczqx
  created_at: 2023-11-21 09:34:41+00:00
  id: 655c79b1a9b72292b16fd631
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-21T09:44:50.000Z'
    data:
      edited: true
      editors:
      - fczqx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9268843531608582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
          fullname: Chao Fang
          isHf: false
          isPro: false
          name: fczqx
          type: user
        html: '<p>by the way, if I convert all h5ad files into one arrow file, it
          will reports like as "arrow storage error with out of range of maximum int32".
          so what''s your suggestion of the convertion strategy? should I concatenate
          each small arrow files into large one ?</p>

          '
        raw: by the way, if I convert all h5ad files into one arrow file, it will
          reports like as "arrow storage error with out of range of maximum int32".
          so what's your suggestion of the convertion strategy? should I concatenate
          each small arrow files into large one ?
        updatedAt: '2023-11-21T09:47:25.247Z'
      numEdits: 2
      reactions: []
    id: 655c7c12a30d9b361b71becb
    type: comment
  author: fczqx
  content: by the way, if I convert all h5ad files into one arrow file, it will reports
    like as "arrow storage error with out of range of maximum int32". so what's your
    suggestion of the convertion strategy? should I concatenate each small arrow files
    into large one ?
  created_at: 2023-11-21 09:44:50+00:00
  edited: true
  hidden: false
  id: 655c7c12a30d9b361b71becb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-21T09:48:06.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9482736587524414
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. My suggestion is that you put all the
          initial h5ad files into a single directory, and then re-run the transcriptome
          tokenizer to tokenize them into a single .dataset file. If you are getting
          the error you mentioned when doing this, try setting use_generator to True.
          It will be a bit slower but will likely avoid this error. Please see prior
          discussions on this for further information - it is a known issue with Hugging
          Face Datasets that they are working on resolving.</p>

          '
        raw: Thank you for following up. My suggestion is that you put all the initial
          h5ad files into a single directory, and then re-run the transcriptome tokenizer
          to tokenize them into a single .dataset file. If you are getting the error
          you mentioned when doing this, try setting use_generator to True. It will
          be a bit slower but will likely avoid this error. Please see prior discussions
          on this for further information - it is a known issue with Hugging Face
          Datasets that they are working on resolving.
        updatedAt: '2023-11-21T09:48:06.928Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655c7cd60ef822dd5d70a296
    id: 655c7cd60ef822dd5d70a294
    type: comment
  author: ctheodoris
  content: Thank you for following up. My suggestion is that you put all the initial
    h5ad files into a single directory, and then re-run the transcriptome tokenizer
    to tokenize them into a single .dataset file. If you are getting the error you
    mentioned when doing this, try setting use_generator to True. It will be a bit
    slower but will likely avoid this error. Please see prior discussions on this
    for further information - it is a known issue with Hugging Face Datasets that
    they are working on resolving.
  created_at: 2023-11-21 09:48:06+00:00
  edited: false
  hidden: false
  id: 655c7cd60ef822dd5d70a294
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-11-21T09:48:06.000Z'
    data:
      status: closed
    id: 655c7cd60ef822dd5d70a296
    type: status-change
  author: ctheodoris
  created_at: 2023-11-21 09:48:06+00:00
  id: 655c7cd60ef822dd5d70a296
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-22T05:10:28.000Z'
    data:
      edited: true
      editors:
      - fczqx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5391775369644165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
          fullname: Chao Fang
          isHf: false
          isPro: false
          name: fczqx
          type: user
        html: "<p>when I ran the Trainer function, it reports \"model.to\" error with\
          \ Transfomers Trainer.  Can you help me to figure it out? I pasted the function\
          \ scripts as following:<br> def model_init():<br>        model = BertForSequenceClassification.from_pretrained(path_model,<br>\
          \                                                              num_labels=len(organ_label_dict.keys()),<br>\
          \                                                              output_attentions=False,<br>\
          \                                                              output_hidden_states=False)<br>\
          \        if freeze_layers is not None:<br>            modules_to_freeze\
          \ = model.bert.encoder.layer[:freeze_layers]<br>            for module in\
          \ modules_to_freeze:<br>                for param in module.parameters():<br>\
          \                    param.requires_grad = False</p>\n<pre><code>    model\
          \ = model.to(\"cuda:0\")\n    return model\n</code></pre>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/69-3kgGkpGXqStl3edlQ0.png\"\
          ><img alt=\"ss1.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/69-3kgGkpGXqStl3edlQ0.png\"\
          ></a></p>\n<p>Thank you.<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/ttn9dwJEA5-G806lln7Sm.png\"\
          ><img alt=\"ss2.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/ttn9dwJEA5-G806lln7Sm.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/aTr-lJWq0AUQvlTTUVA-4.png\"\
          ><img alt=\"ss3.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/aTr-lJWq0AUQvlTTUVA-4.png\"\
          ></a></p>\n<p>recipe:<br><a href=\"https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/hyperparam_optimiz_for_disease_classifier.py\"\
          >https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/hyperparam_optimiz_for_disease_classifier.py</a></p>\n"
        raw: "when I ran the Trainer function, it reports \"model.to\" error with\
          \ Transfomers Trainer.  Can you help me to figure it out? I pasted the function\
          \ scripts as following:\n def model_init():\n        model = BertForSequenceClassification.from_pretrained(path_model,\n\
          \                                                              num_labels=len(organ_label_dict.keys()),\n\
          \                                                              output_attentions=False,\n\
          \                                                              output_hidden_states=False)\n\
          \        if freeze_layers is not None:\n            modules_to_freeze =\
          \ model.bert.encoder.layer[:freeze_layers]\n            for module in modules_to_freeze:\n\
          \                for param in module.parameters():\n                   \
          \ param.requires_grad = False\n\n        model = model.to(\"cuda:0\")\n\
          \        return model\n\n\n![ss1.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/69-3kgGkpGXqStl3edlQ0.png)\n\
          \nThank you.\n![ss2.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/ttn9dwJEA5-G806lln7Sm.png)\n\
          ![ss3.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/aTr-lJWq0AUQvlTTUVA-4.png)\n\
          \nrecipe:\nhttps://huggingface.co/ctheodoris/Geneformer/blob/main/examples/hyperparam_optimiz_for_disease_classifier.py"
        updatedAt: '2023-11-22T05:24:13.106Z'
      numEdits: 3
      reactions: []
      relatedEventId: 655d8d441b960c5c614d5a64
    id: 655d8d441b960c5c614d5a61
    type: comment
  author: fczqx
  content: "when I ran the Trainer function, it reports \"model.to\" error with Transfomers\
    \ Trainer.  Can you help me to figure it out? I pasted the function scripts as\
    \ following:\n def model_init():\n        model = BertForSequenceClassification.from_pretrained(path_model,\n\
    \                                                              num_labels=len(organ_label_dict.keys()),\n\
    \                                                              output_attentions=False,\n\
    \                                                              output_hidden_states=False)\n\
    \        if freeze_layers is not None:\n            modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n\
    \            for module in modules_to_freeze:\n                for param in module.parameters():\n\
    \                    param.requires_grad = False\n\n        model = model.to(\"\
    cuda:0\")\n        return model\n\n\n![ss1.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/69-3kgGkpGXqStl3edlQ0.png)\n\
    \nThank you.\n![ss2.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/ttn9dwJEA5-G806lln7Sm.png)\n\
    ![ss3.png](https://cdn-uploads.huggingface.co/production/uploads/654aeb4b3b78e73b43be75d9/aTr-lJWq0AUQvlTTUVA-4.png)\n\
    \nrecipe:\nhttps://huggingface.co/ctheodoris/Geneformer/blob/main/examples/hyperparam_optimiz_for_disease_classifier.py"
  created_at: 2023-11-22 05:10:28+00:00
  edited: true
  hidden: false
  id: 655d8d441b960c5c614d5a61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-22T05:10:28.000Z'
    data:
      status: open
    id: 655d8d441b960c5c614d5a64
    type: status-change
  author: fczqx
  created_at: 2023-11-22 05:10:28+00:00
  id: 655d8d441b960c5c614d5a64
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-23T02:33:30.000Z'
    data:
      edited: true
      editors:
      - fczqx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9968563914299011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
          fullname: Chao Fang
          isHf: false
          isPro: false
          name: fczqx
          type: user
        html: '<p>I''ve solved it by myself, thx.</p>

          '
        raw: I've solved it by myself, thx.
        updatedAt: '2023-11-23T02:33:48.935Z'
      numEdits: 1
      reactions: []
    id: 655eb9fab11e49dd1fdab9ff
    type: comment
  author: fczqx
  content: I've solved it by myself, thx.
  created_at: 2023-11-23 02:33:30+00:00
  edited: true
  hidden: false
  id: 655eb9fab11e49dd1fdab9ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8a7e9ae3e7e2264b45435f5738bd9101.svg
      fullname: Chao Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fczqx
      type: user
    createdAt: '2023-11-23T02:33:34.000Z'
    data:
      status: closed
    id: 655eb9fe6e89f16eccf92e4d
    type: status-change
  author: fczqx
  created_at: 2023-11-23 02:33:34+00:00
  id: 655eb9fe6e89f16eccf92e4d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 279
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: how to fine-tune cell classification with DP/TP/PP
