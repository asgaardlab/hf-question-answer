!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Gigo6
conflicting_files: null
created_at: 2023-10-27 17:35:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abfdc8e4508d20b3a2ddc097081f2411.svg
      fullname: momo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gigo6
      type: user
    createdAt: '2023-10-27T18:35:15.000Z'
    data:
      edited: false
      editors:
      - Gigo6
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7259902954101562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abfdc8e4508d20b3a2ddc097081f2411.svg
          fullname: momo
          isHf: false
          isPro: false
          name: Gigo6
          type: user
        html: "<p>hi, nice work!</p>\n<p>I am trying to follow the example code \"\
          cell_classification.ipynb\", but  have some trouble.</p>\n<ol>\n<li><p>when\
          \  set \"output_attentions=False\" \uFF0CThe entire pipeline can be successfully\
          \ executed.</p>\n</li>\n<li><p>when set \"output_attentions=True\" ,got\
          \ some error message.</p>\n</li>\n</ol>\n<pre><code>model = BertForSequenceClassification.from_pretrained(\"\
          ../models/geneformer-6L-30M/\", \n                                     \
          \                 num_labels=len(organ_label_dict.keys()),\n           \
          \                                           output_attentions = True,\n\
          \                                                      output_hidden_states\
          \ = False)\n</code></pre>\n<pre><code>RuntimeError: Sizes of tensors must\
          \ match except in dimension 0. Expected size 1487 but got size 1301 for\
          \ tensor number 1 in the list.\n</code></pre>\n"
        raw: "hi, nice work!\r\n\r\nI am trying to follow the example code \"cell_classification.ipynb\"\
          , but  have some trouble.\r\n\r\n1. when  set \"output_attentions=False\"\
          \ \uFF0CThe entire pipeline can be successfully executed.\r\n\r\n2. when\
          \ set \"output_attentions=True\" ,got some error message.\r\n```\r\nmodel\
          \ = BertForSequenceClassification.from_pretrained(\"../models/geneformer-6L-30M/\"\
          , \r\n                                                      num_labels=len(organ_label_dict.keys()),\r\
          \n                                                      output_attentions\
          \ = True,\r\n                                                      output_hidden_states\
          \ = False)\r\n```\r\n```\r\nRuntimeError: Sizes of tensors must match except\
          \ in dimension 0. Expected size 1487 but got size 1301 for tensor number\
          \ 1 in the list.\r\n```"
        updatedAt: '2023-10-27T18:35:15.745Z'
      numEdits: 0
      reactions: []
    id: 653c02e37604aeec05fae40a
    type: comment
  author: Gigo6
  content: "hi, nice work!\r\n\r\nI am trying to follow the example code \"cell_classification.ipynb\"\
    , but  have some trouble.\r\n\r\n1. when  set \"output_attentions=False\" \uFF0C\
    The entire pipeline can be successfully executed.\r\n\r\n2. when set \"output_attentions=True\"\
    \ ,got some error message.\r\n```\r\nmodel = BertForSequenceClassification.from_pretrained(\"\
    ../models/geneformer-6L-30M/\", \r\n                                         \
    \             num_labels=len(organ_label_dict.keys()),\r\n                   \
    \                                   output_attentions = True,\r\n            \
    \                                          output_hidden_states = False)\r\n```\r\
    \n```\r\nRuntimeError: Sizes of tensors must match except in dimension 0. Expected\
    \ size 1487 but got size 1301 for tensor number 1 in the list.\r\n```"
  created_at: 2023-10-27 17:35:15+00:00
  edited: false
  hidden: false
  id: 653c02e37604aeec05fae40a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-27T18:47:40.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9457158446311951
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! You should set this to
          False while fine-tuning, as the example code does. If you want to examine
          the attention weights of the model, you can do so after fine-tuning by loading
          the model in eval mode and outputting attention weights.</p>

          '
        raw: Thank you for your interest in Geneformer! You should set this to False
          while fine-tuning, as the example code does. If you want to examine the
          attention weights of the model, you can do so after fine-tuning by loading
          the model in eval mode and outputting attention weights.
        updatedAt: '2023-10-27T18:47:40.776Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653c05cca3ad95ac0d03948c
    id: 653c05cca3ad95ac0d03948a
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! You should set this to False
    while fine-tuning, as the example code does. If you want to examine the attention
    weights of the model, you can do so after fine-tuning by loading the model in
    eval mode and outputting attention weights.
  created_at: 2023-10-27 17:47:40+00:00
  edited: false
  hidden: false
  id: 653c05cca3ad95ac0d03948a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-27T18:47:40.000Z'
    data:
      status: closed
    id: 653c05cca3ad95ac0d03948c
    type: status-change
  author: ctheodoris
  created_at: 2023-10-27 17:47:40+00:00
  id: 653c05cca3ad95ac0d03948c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abfdc8e4508d20b3a2ddc097081f2411.svg
      fullname: momo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gigo6
      type: user
    createdAt: '2023-10-28T04:22:43.000Z'
    data:
      edited: false
      editors:
      - Gigo6
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4064244329929352
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abfdc8e4508d20b3a2ddc097081f2411.svg
          fullname: momo
          isHf: false
          isPro: false
          name: Gigo6
          type: user
        html: "<p>thank for your reply. However I think the error came from the eval\
          \ mode.</p>\n<pre><code>RuntimeError                              Traceback\
          \ (most recent call last)\n&lt;ipython-input-8-f39447d005ad&gt; in &lt;cell\
          \ line: 36&gt;()\n     96     # train the cell type classifier\n     97\
          \     # print(trainer.model.training)\n---&gt; 98     trainer.train()\n\
          \     99     # print(trainer.model.training)\n    100     # predictions\
          \ = trainer.predict(organ_trainset)\n\n10 frames\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n\
          \   1589                 hf_hub_utils.enable_progress_bars()\n   1590  \
          \       else:\n-&gt; 1591             return inner_training_loop(\n   1592\
          \                 args=args,\n   1593                 resume_from_checkpoint=resume_from_checkpoint,\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in _inner_training_loop(self,\
          \ batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n\
          \   1997 \n   1998             self.control = self.callback_handler.on_epoch_end(args,\
          \ self.state, self.control)\n-&gt; 1999             self._maybe_log_save_evaluate(tr_loss,\
          \ model, trial, epoch, ignore_keys_for_eval)\n   2000 \n   2001        \
          \     if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\
          \   2326                     metrics.update(dataset_metrics)\n   2327  \
          \           else:\n-&gt; 2328                 metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n\
          \   2329             self._report_to_hp_search(trial, self.state.global_step,\
          \ metrics)\n   2330 \n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in evaluate(self, eval_dataset, ignore_keys, metric_key_prefix)\n   3064\
          \ \n   3065         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop\
          \ else self.evaluation_loop\n-&gt; 3066         output = eval_loop(\n  \
          \ 3067             eval_dataloader,\n   3068             description=\"\
          Evaluation\",\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in evaluation_loop(self, dataloader, description, prediction_loss_only,\
          \ ignore_keys, metric_key_prefix)\n   3279                     logits =\
          \ self.preprocess_logits_for_metrics(logits, labels)\n   3280          \
          \       logits = self.accelerator.gather_for_metrics((logits))\n-&gt; 3281\
          \                 preds_host = logits if preds_host is None else nested_concat(preds_host,\
          \ logits, padding_index=-100)\n   3282 \n   3283             if labels is\
          \ not None:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ),\
          \ f\"Expected `tensors` and `new_tensors` to have the same type but found\
          \ {type(tensors)} and {type(new_tensors)}.\"\n    120     if isinstance(tensors,\
          \ (list, tuple)):\n--&gt; 121         return type(tensors)(nested_concat(t,\
          \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n\
          \    122     elif isinstance(tensors, torch.Tensor):\n    123         return\
          \ torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in &lt;genexpr&gt;(.0)\n    119     ), f\"Expected `tensors` and `new_tensors`\
          \ to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\
          \n    120     if isinstance(tensors, (list, tuple)):\n--&gt; 121       \
          \  return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
          \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ),\
          \ f\"Expected `tensors` and `new_tensors` to have the same type but found\
          \ {type(tensors)} and {type(new_tensors)}.\"\n    120     if isinstance(tensors,\
          \ (list, tuple)):\n--&gt; 121         return type(tensors)(nested_concat(t,\
          \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n\
          \    122     elif isinstance(tensors, torch.Tensor):\n    123         return\
          \ torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in &lt;genexpr&gt;(.0)\n    119     ), f\"Expected `tensors` and `new_tensors`\
          \ to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\
          \n    120     if isinstance(tensors, (list, tuple)):\n--&gt; 121       \
          \  return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
          \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    121       \
          \  return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
          \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n--&gt; 123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n    124     elif isinstance(tensors,\
          \ Mapping):\n    125         return type(tensors)(\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in torch_pad_and_concatenate(tensor1, tensor2, padding_index)\n     80\
          \ \n     81     if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:\n\
          ---&gt; 82         return torch.cat((tensor1, tensor2), dim=0)\n     83\
          \ \n     84     # Let's figure out the new shape\n\nRuntimeError: Sizes\
          \ of tensors must match except in dimension 0. Expected size 1119 but got\
          \ size 1063 for tensor number 1 in the list.\n</code></pre>\n<p>Of course,\
          \ I also tried explicitly specifying 'eval,' but still the same error\uFF0C\
          Is this correct\uFF1F</p>\n<pre><code>    # trainer.train()\n    print(trainer.model.training)\
          \  //value is False\n    trainer.model.eval() // ensure\n    predictions\
          \ = trainer.predict(organ_trainset)\n</code></pre>\n"
        raw: "thank for your reply. However I think the error came from the eval mode.\n\
          \n```\nRuntimeError                              Traceback (most recent\
          \ call last)\n<ipython-input-8-f39447d005ad> in <cell line: 36>()\n    \
          \ 96     # train the cell type classifier\n     97     # print(trainer.model.training)\n\
          ---> 98     trainer.train()\n     99     # print(trainer.model.training)\n\
          \    100     # predictions = trainer.predict(organ_trainset)\n\n10 frames\n\
          /usr/local/lib/python3.10/dist-packages/transformers/trainer.py in train(self,\
          \ resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   1589\
          \                 hf_hub_utils.enable_progress_bars()\n   1590         else:\n\
          -> 1591             return inner_training_loop(\n   1592               \
          \  args=args,\n   1593                 resume_from_checkpoint=resume_from_checkpoint,\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in _inner_training_loop(self,\
          \ batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n\
          \   1997 \n   1998             self.control = self.callback_handler.on_epoch_end(args,\
          \ self.state, self.control)\n-> 1999             self._maybe_log_save_evaluate(tr_loss,\
          \ model, trial, epoch, ignore_keys_for_eval)\n   2000 \n   2001        \
          \     if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\
          \   2326                     metrics.update(dataset_metrics)\n   2327  \
          \           else:\n-> 2328                 metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n\
          \   2329             self._report_to_hp_search(trial, self.state.global_step,\
          \ metrics)\n   2330 \n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
          \ in evaluate(self, eval_dataset, ignore_keys, metric_key_prefix)\n   3064\
          \ \n   3065         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop\
          \ else self.evaluation_loop\n-> 3066         output = eval_loop(\n   3067\
          \             eval_dataloader,\n   3068             description=\"Evaluation\"\
          ,\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in\
          \ evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys,\
          \ metric_key_prefix)\n   3279                     logits = self.preprocess_logits_for_metrics(logits,\
          \ labels)\n   3280                 logits = self.accelerator.gather_for_metrics((logits))\n\
          -> 3281                 preds_host = logits if preds_host is None else nested_concat(preds_host,\
          \ logits, padding_index=-100)\n   3282 \n   3283             if labels is\
          \ not None:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ),\
          \ f\"Expected `tensors` and `new_tensors` to have the same type but found\
          \ {type(tensors)} and {type(new_tensors)}.\"\n    120     if isinstance(tensors,\
          \ (list, tuple)):\n--> 121         return type(tensors)(nested_concat(t,\
          \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n\
          \    122     elif isinstance(tensors, torch.Tensor):\n    123         return\
          \ torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in <genexpr>(.0)\n    119     ), f\"Expected `tensors` and `new_tensors`\
          \ to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\
          \n    120     if isinstance(tensors, (list, tuple)):\n--> 121         return\
          \ type(tensors)(nested_concat(t, n, padding_index=padding_index) for t,\
          \ n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ),\
          \ f\"Expected `tensors` and `new_tensors` to have the same type but found\
          \ {type(tensors)} and {type(new_tensors)}.\"\n    120     if isinstance(tensors,\
          \ (list, tuple)):\n--> 121         return type(tensors)(nested_concat(t,\
          \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n\
          \    122     elif isinstance(tensors, torch.Tensor):\n    123         return\
          \ torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in <genexpr>(.0)\n    119     ), f\"Expected `tensors` and `new_tensors`\
          \ to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\
          \n    120     if isinstance(tensors, (list, tuple)):\n--> 121         return\
          \ type(tensors)(nested_concat(t, n, padding_index=padding_index) for t,\
          \ n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in nested_concat(tensors, new_tensors, padding_index)\n    121       \
          \  return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
          \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
          \ torch.Tensor):\n--> 123         return torch_pad_and_concatenate(tensors,\
          \ new_tensors, padding_index=padding_index)\n    124     elif isinstance(tensors,\
          \ Mapping):\n    125         return type(tensors)(\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
          \ in torch_pad_and_concatenate(tensor1, tensor2, padding_index)\n     80\
          \ \n     81     if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:\n\
          ---> 82         return torch.cat((tensor1, tensor2), dim=0)\n     83 \n\
          \     84     # Let's figure out the new shape\n\nRuntimeError: Sizes of\
          \ tensors must match except in dimension 0. Expected size 1119 but got size\
          \ 1063 for tensor number 1 in the list.\n```\nOf course, I also tried explicitly\
          \ specifying 'eval,' but still the same error\uFF0CIs this correct\uFF1F\
          \n```\n    # trainer.train()\n    print(trainer.model.training)  //value\
          \ is False\n    trainer.model.eval() // ensure\n    predictions = trainer.predict(organ_trainset)\n\
          ```"
        updatedAt: '2023-10-28T04:22:43.257Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653c8c939107029eb0fba980
    id: 653c8c939107029eb0fba97c
    type: comment
  author: Gigo6
  content: "thank for your reply. However I think the error came from the eval mode.\n\
    \n```\nRuntimeError                              Traceback (most recent call last)\n\
    <ipython-input-8-f39447d005ad> in <cell line: 36>()\n     96     # train the cell\
    \ type classifier\n     97     # print(trainer.model.training)\n---> 98     trainer.train()\n\
    \     99     # print(trainer.model.training)\n    100     # predictions = trainer.predict(organ_trainset)\n\
    \n10 frames\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in\
    \ train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n\
    \   1589                 hf_hub_utils.enable_progress_bars()\n   1590        \
    \ else:\n-> 1591             return inner_training_loop(\n   1592            \
    \     args=args,\n   1593                 resume_from_checkpoint=resume_from_checkpoint,\n\
    \n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in _inner_training_loop(self,\
    \ batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n   1997\
    \ \n   1998             self.control = self.callback_handler.on_epoch_end(args,\
    \ self.state, self.control)\n-> 1999             self._maybe_log_save_evaluate(tr_loss,\
    \ model, trial, epoch, ignore_keys_for_eval)\n   2000 \n   2001             if\
    \ DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
    \ in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\
    \   2326                     metrics.update(dataset_metrics)\n   2327        \
    \     else:\n-> 2328                 metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n\
    \   2329             self._report_to_hp_search(trial, self.state.global_step,\
    \ metrics)\n   2330 \n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\
    \ in evaluate(self, eval_dataset, ignore_keys, metric_key_prefix)\n   3064 \n\
    \   3065         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop\
    \ else self.evaluation_loop\n-> 3066         output = eval_loop(\n   3067    \
    \         eval_dataloader,\n   3068             description=\"Evaluation\",\n\n\
    /usr/local/lib/python3.10/dist-packages/transformers/trainer.py in evaluation_loop(self,\
    \ dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\n\
    \   3279                     logits = self.preprocess_logits_for_metrics(logits,\
    \ labels)\n   3280                 logits = self.accelerator.gather_for_metrics((logits))\n\
    -> 3281                 preds_host = logits if preds_host is None else nested_concat(preds_host,\
    \ logits, padding_index=-100)\n   3282 \n   3283             if labels is not\
    \ None:\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
    \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ), f\"Expected\
    \ `tensors` and `new_tensors` to have the same type but found {type(tensors)}\
    \ and {type(new_tensors)}.\"\n    120     if isinstance(tensors, (list, tuple)):\n\
    --> 121         return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
    \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
    \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors, new_tensors,\
    \ padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
    \ in <genexpr>(.0)\n    119     ), f\"Expected `tensors` and `new_tensors` to\
    \ have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n  \
    \  120     if isinstance(tensors, (list, tuple)):\n--> 121         return type(tensors)(nested_concat(t,\
    \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n   \
    \ 122     elif isinstance(tensors, torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
    \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
    \ in nested_concat(tensors, new_tensors, padding_index)\n    119     ), f\"Expected\
    \ `tensors` and `new_tensors` to have the same type but found {type(tensors)}\
    \ and {type(new_tensors)}.\"\n    120     if isinstance(tensors, (list, tuple)):\n\
    --> 121         return type(tensors)(nested_concat(t, n, padding_index=padding_index)\
    \ for t, n in zip(tensors, new_tensors))\n    122     elif isinstance(tensors,\
    \ torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors, new_tensors,\
    \ padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
    \ in <genexpr>(.0)\n    119     ), f\"Expected `tensors` and `new_tensors` to\
    \ have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n  \
    \  120     if isinstance(tensors, (list, tuple)):\n--> 121         return type(tensors)(nested_concat(t,\
    \ n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n   \
    \ 122     elif isinstance(tensors, torch.Tensor):\n    123         return torch_pad_and_concatenate(tensors,\
    \ new_tensors, padding_index=padding_index)\n\n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\
    \ in nested_concat(tensors, new_tensors, padding_index)\n    121         return\
    \ type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors,\
    \ new_tensors))\n    122     elif isinstance(tensors, torch.Tensor):\n--> 123\
    \         return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n\
    \    124     elif isinstance(tensors, Mapping):\n    125         return type(tensors)(\n\
    \n/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py in\
    \ torch_pad_and_concatenate(tensor1, tensor2, padding_index)\n     80 \n     81\
    \     if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:\n--->\
    \ 82         return torch.cat((tensor1, tensor2), dim=0)\n     83 \n     84  \
    \   # Let's figure out the new shape\n\nRuntimeError: Sizes of tensors must match\
    \ except in dimension 0. Expected size 1119 but got size 1063 for tensor number\
    \ 1 in the list.\n```\nOf course, I also tried explicitly specifying 'eval,' but\
    \ still the same error\uFF0CIs this correct\uFF1F\n```\n    # trainer.train()\n\
    \    print(trainer.model.training)  //value is False\n    trainer.model.eval()\
    \ // ensure\n    predictions = trainer.predict(organ_trainset)\n```"
  created_at: 2023-10-28 03:22:43+00:00
  edited: false
  hidden: false
  id: 653c8c939107029eb0fba97c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/abfdc8e4508d20b3a2ddc097081f2411.svg
      fullname: momo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gigo6
      type: user
    createdAt: '2023-10-28T04:22:43.000Z'
    data:
      status: open
    id: 653c8c939107029eb0fba980
    type: status-change
  author: Gigo6
  created_at: 2023-10-28 03:22:43+00:00
  id: 653c8c939107029eb0fba980
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-28T05:30:05.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633780121803284
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. If you would like to output attention
          weights, we do not recommend you do that within the context of the code
          for cell classification, which is not written for the purpose of outputting
          attention weights. After you fine-tune the model, you can load it in eval
          mode with setting it to output attention weights. Then you can output the
          attention weights for your data and analyze them accordingly. Please note
          that, as with all tensors, if you run batches of cells through the model
          rather than one by one, you will need to pad them to maintain a consistent
          length per batch.</p>

          '
        raw: Thank you for following up. If you would like to output attention weights,
          we do not recommend you do that within the context of the code for cell
          classification, which is not written for the purpose of outputting attention
          weights. After you fine-tune the model, you can load it in eval mode with
          setting it to output attention weights. Then you can output the attention
          weights for your data and analyze them accordingly. Please note that, as
          with all tensors, if you run batches of cells through the model rather than
          one by one, you will need to pad them to maintain a consistent length per
          batch.
        updatedAt: '2023-10-28T05:30:05.264Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653c9c5d06a7bb4b1d592a74
    id: 653c9c5d06a7bb4b1d592a73
    type: comment
  author: ctheodoris
  content: Thank you for following up. If you would like to output attention weights,
    we do not recommend you do that within the context of the code for cell classification,
    which is not written for the purpose of outputting attention weights. After you
    fine-tune the model, you can load it in eval mode with setting it to output attention
    weights. Then you can output the attention weights for your data and analyze them
    accordingly. Please note that, as with all tensors, if you run batches of cells
    through the model rather than one by one, you will need to pad them to maintain
    a consistent length per batch.
  created_at: 2023-10-28 04:30:05+00:00
  edited: false
  hidden: false
  id: 653c9c5d06a7bb4b1d592a73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-28T05:30:05.000Z'
    data:
      status: closed
    id: 653c9c5d06a7bb4b1d592a74
    type: status-change
  author: ctheodoris
  created_at: 2023-10-28 04:30:05+00:00
  id: 653c9c5d06a7bb4b1d592a74
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 271
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: RuntimeError when "output_attentions=True"
