!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kchen360
conflicting_files: null
created_at: 2023-10-17 21:27:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/947ad2da1f7a9a2466ab90e75e86ab07.svg
      fullname: Kai Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kchen360
      type: user
    createdAt: '2023-10-17T22:27:50.000Z'
    data:
      edited: false
      editors:
      - kchen360
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8209187388420105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/947ad2da1f7a9a2466ab90e75e86ab07.svg
          fullname: Kai Chen
          isHf: false
          isPro: false
          name: kchen360
          type: user
        html: "<p>Hi,</p>\n<p>I am trying to reproduce the fine-tuned model from hyperparameter\
          \ optimization in disease classification case using disease classification\
          \ dataset (human_dcm_hcm_nf.dataset provided from Genecorpus-30M  repo with\
          \ script hyperparam_optimiz_for_disease_classifier.ipynb. I load pretrained\
          \ model like </p>\n<pre><code>model = BertForSequenceClassification.from_pretrained(\"\
          /home/Geneformer/geneformer-12L-30M/\",\n                              \
          \                            num_labels=len(target_names),\n           \
          \                                               output_attentions = False,\n\
          \                                                          output_hidden_states\
          \ = False)\n</code></pre>\n<p>Yet I got error:</p>\n<pre><code>RuntimeError:\
          \ Error(s) in loading state_dict for BertForSequenceClassification:\n  \
          \      size mismatch for bert.embeddings.word_embeddings.weight: copying\
          \ a param with shape torch.Size([25426, 256]) from checkpoint, the shape\
          \ in current model is torch.Size([25426, 512]).\n        size mismatch for\
          \ bert.embeddings.position_embeddings.weight: copying a param with shape\
          \ torch.Size([2048, 256]) from checkpoint, the shape in current model is\
          \ torch.Size([2048, 512]).\n        size mismatch for bert.embeddings.token_type_embeddings.weight:\
          \ copying a param with shape torch.Size([2, 256]) from checkpoint, the shape\
          \ in current model is torch.Size([2, 512]).\n        size mismatch for bert.embeddings.LayerNorm.weight:\
          \ copying a param with shape torch.Size([256]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\n        size mismatch for bert.embeddings.LayerNorm.bias:\
          \ copying a param with shape torch.Size([256]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\n</code></pre>\n<p>I was wondering\
          \ if I didn't use pretrained model and input dataset correctly since there\
          \ is 2-fold difference between input torch size and model torch size (256\
          \ and 512), any part that I may miss? Any tip will be helpful. Thank you\
          \ very much.</p>\n"
        raw: "Hi,\r\n\r\nI am trying to reproduce the fine-tuned model from hyperparameter\
          \ optimization in disease classification case using disease classification\
          \ dataset (human_dcm_hcm_nf.dataset provided from Genecorpus-30M  repo with\
          \ script hyperparam_optimiz_for_disease_classifier.ipynb. I load pretrained\
          \ model like \r\n```\r\nmodel = BertForSequenceClassification.from_pretrained(\"\
          /home/Geneformer/geneformer-12L-30M/\",\r\n                            \
          \                              num_labels=len(target_names),\r\n       \
          \                                                   output_attentions =\
          \ False,\r\n                                                          output_hidden_states\
          \ = False)\r\n```\r\n\r\nYet I got error:\r\n```\r\nRuntimeError: Error(s)\
          \ in loading state_dict for BertForSequenceClassification:\r\n        size\
          \ mismatch for bert.embeddings.word_embeddings.weight: copying a param with\
          \ shape torch.Size([25426, 256]) from checkpoint, the shape in current model\
          \ is torch.Size([25426, 512]).\r\n        size mismatch for bert.embeddings.position_embeddings.weight:\
          \ copying a param with shape torch.Size([2048, 256]) from checkpoint, the\
          \ shape in current model is torch.Size([2048, 512]).\r\n        size mismatch\
          \ for bert.embeddings.token_type_embeddings.weight: copying a param with\
          \ shape torch.Size([2, 256]) from checkpoint, the shape in current model\
          \ is torch.Size([2, 512]).\r\n        size mismatch for bert.embeddings.LayerNorm.weight:\
          \ copying a param with shape torch.Size([256]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\r\n        size mismatch for bert.embeddings.LayerNorm.bias:\
          \ copying a param with shape torch.Size([256]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\r\n```\r\n\r\nI was wondering if\
          \ I didn't use pretrained model and input dataset correctly since there\
          \ is 2-fold difference between input torch size and model torch size (256\
          \ and 512), any part that I may miss? Any tip will be helpful. Thank you\
          \ very much."
        updatedAt: '2023-10-17T22:27:50.011Z'
      numEdits: 0
      reactions: []
    id: 652f0a66d7808dbb9d0c9272
    type: comment
  author: kchen360
  content: "Hi,\r\n\r\nI am trying to reproduce the fine-tuned model from hyperparameter\
    \ optimization in disease classification case using disease classification dataset\
    \ (human_dcm_hcm_nf.dataset provided from Genecorpus-30M  repo with script hyperparam_optimiz_for_disease_classifier.ipynb.\
    \ I load pretrained model like \r\n```\r\nmodel = BertForSequenceClassification.from_pretrained(\"\
    /home/Geneformer/geneformer-12L-30M/\",\r\n                                  \
    \                        num_labels=len(target_names),\r\n                   \
    \                                       output_attentions = False,\r\n       \
    \                                                   output_hidden_states = False)\r\
    \n```\r\n\r\nYet I got error:\r\n```\r\nRuntimeError: Error(s) in loading state_dict\
    \ for BertForSequenceClassification:\r\n        size mismatch for bert.embeddings.word_embeddings.weight:\
    \ copying a param with shape torch.Size([25426, 256]) from checkpoint, the shape\
    \ in current model is torch.Size([25426, 512]).\r\n        size mismatch for bert.embeddings.position_embeddings.weight:\
    \ copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape\
    \ in current model is torch.Size([2048, 512]).\r\n        size mismatch for bert.embeddings.token_type_embeddings.weight:\
    \ copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in\
    \ current model is torch.Size([2, 512]).\r\n        size mismatch for bert.embeddings.LayerNorm.weight:\
    \ copying a param with shape torch.Size([256]) from checkpoint, the shape in current\
    \ model is torch.Size([512]).\r\n        size mismatch for bert.embeddings.LayerNorm.bias:\
    \ copying a param with shape torch.Size([256]) from checkpoint, the shape in current\
    \ model is torch.Size([512]).\r\n```\r\n\r\nI was wondering if I didn't use pretrained\
    \ model and input dataset correctly since there is 2-fold difference between input\
    \ torch size and model torch size (256 and 512), any part that I may miss? Any\
    \ tip will be helpful. Thank you very much."
  created_at: 2023-10-17 21:27:50+00:00
  edited: false
  hidden: false
  id: 652f0a66d7808dbb9d0c9272
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-17T23:23:52.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8239498734474182
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! The 6 layer model (outer
          directory of this repository) has an embedding size of 256, while the 12
          layer model has an embedding size of 512.</p>

          '
        raw: Thank you for your interest in Geneformer! The 6 layer model (outer directory
          of this repository) has an embedding size of 256, while the 12 layer model
          has an embedding size of 512.
        updatedAt: '2023-10-17T23:23:52.517Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652f17887a8c08f81e61f84d
    id: 652f17887a8c08f81e61f848
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! The 6 layer model (outer directory
    of this repository) has an embedding size of 256, while the 12 layer model has
    an embedding size of 512.
  created_at: 2023-10-17 22:23:52+00:00
  edited: false
  hidden: false
  id: 652f17887a8c08f81e61f848
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-17T23:23:52.000Z'
    data:
      status: closed
    id: 652f17887a8c08f81e61f84d
    type: status-change
  author: ctheodoris
  created_at: 2023-10-17 22:23:52+00:00
  id: 652f17887a8c08f81e61f84d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 263
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Hyperparameter optimization in disease classification
