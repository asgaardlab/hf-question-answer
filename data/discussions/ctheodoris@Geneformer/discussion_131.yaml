!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iftesha1
conflicting_files: null
created_at: 2023-07-22 11:16:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e1190850c86d6e55b7ca16be0ca1f58.svg
      fullname: Iftesha Najnin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iftesha1
      type: user
    createdAt: '2023-07-22T12:16:13.000Z'
    data:
      edited: false
      editors:
      - iftesha1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8017067909240723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e1190850c86d6e55b7ca16be0ca1f58.svg
          fullname: Iftesha Najnin
          isHf: false
          isPro: false
          name: iftesha1
          type: user
        html: '<p>Hi, I want to create the token embeddings for the vocabulary. So
          that I can add the position embedding and token embedding as the input of
          my customised model.</p>

          <p>Currently I am using the following<br>token_embed = nn.Embedding(target_vocab_size,
          d_model).to(device)<br>but this token embeddings won''t match to Geneformer''s
          embedding.<br>So can you please guide me what you have used to create the
          token embeddings for complete vocabulary.</p>

          <p>Thank you!</p>

          '
        raw: "Hi, I want to create the token embeddings for the vocabulary. So that\
          \ I can add the position embedding and token embedding as the input of my\
          \ customised model.\r\n\r\nCurrently I am using the following\r\ntoken_embed\
          \ = nn.Embedding(target_vocab_size, d_model).to(device)\r\nbut this token\
          \ embeddings won't match to Geneformer's embedding.\r\nSo can you please\
          \ guide me what you have used to create the token embeddings for complete\
          \ vocabulary.\r\n\r\nThank you!"
        updatedAt: '2023-07-22T12:16:13.611Z'
      numEdits: 0
      reactions: []
    id: 64bbc88d1363b5c7998328dd
    type: comment
  author: iftesha1
  content: "Hi, I want to create the token embeddings for the vocabulary. So that\
    \ I can add the position embedding and token embedding as the input of my customised\
    \ model.\r\n\r\nCurrently I am using the following\r\ntoken_embed = nn.Embedding(target_vocab_size,\
    \ d_model).to(device)\r\nbut this token embeddings won't match to Geneformer's\
    \ embedding.\r\nSo can you please guide me what you have used to create the token\
    \ embeddings for complete vocabulary.\r\n\r\nThank you!"
  created_at: 2023-07-22 11:16:13+00:00
  edited: false
  hidden: false
  id: 64bbc88d1363b5c7998328dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/6e1190850c86d6e55b7ca16be0ca1f58.svg
      fullname: Iftesha Najnin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iftesha1
      type: user
    createdAt: '2023-07-22T12:16:26.000Z'
    data:
      from: How to create the word-embbedings for the whole vocabulary.
      to: How to create the token-embbedings for the whole vocabulary.
    id: 64bbc89ae38420aabaa9d3ab
    type: title-change
  author: iftesha1
  created_at: 2023-07-22 11:16:26+00:00
  id: 64bbc89ae38420aabaa9d3ab
  new_title: How to create the token-embbedings for the whole vocabulary.
  old_title: How to create the word-embbedings for the whole vocabulary.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-22T15:26:11.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9147008657455444
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your interest in Geneformer! Geneformer\u2019s input\
          \ is rank value encodings. Please see the transcriptome tokenizer and manuscript\
          \ Methods to understand how these are generated. For the tokens themselves,\
          \ each token corresponds to a particular gene as identified by Ensembl ID.\
          \ The embeddings for the genes are generated by the model. The embeddings\
          \ are context-aware, so they will represent the gene within the particular\
          \ context of the individual cell presented to the model. If you are interested\
          \ in leveraging the context-aware embeddings for each gene that are generated\
          \ by Geneformer, the approach would be to build a model on top of Geneformer\
          \ that extracts embeddings for each gene in the particular context (e.g.\
          \ particular cell type or normal vs disease, etc) and then provides this\
          \ as an input to a model that uses this embedding to accomplish a particular\
          \ task, such as explicitly inferring gene network connections, etc. The\
          \ best layer to extract the embeddings from depends on how close the scientific\
          \ question is to the pretraining or fine-tuning objective. The last layer\
          \ weights are often very tuned to the particular objective, whereas the\
          \ second to last layer may provide a more general representation of the\
          \ genes. For example, if using the pretrained model without further fine-tuning,\
          \ one may choose the second to last layer embeddings as an input to their\
          \ model. Please note that because the embeddings are context-aware, it would\
          \ make the most sense to generate the embeddings dynamically in a unified\
          \ manner with the model on top of Geneformer rather than trying to enforce\
          \ a frozen non-context-aware embedding for each gene from a random cell\
          \ state or from a collection of random cell states. If that doesn\u2019\
          t answer your question, feel free to respond with more details of what you\
          \ are trying to do with your model either here or by email so I can further\
          \ guide the best use of Geneformer to accomplish your goals.</p>\n"
        raw: "Thank you for your interest in Geneformer! Geneformer\u2019s input is\
          \ rank value encodings. Please see the transcriptome tokenizer and manuscript\
          \ Methods to understand how these are generated. For the tokens themselves,\
          \ each token corresponds to a particular gene as identified by Ensembl ID.\
          \ The embeddings for the genes are generated by the model. The embeddings\
          \ are context-aware, so they will represent the gene within the particular\
          \ context of the individual cell presented to the model. If you are interested\
          \ in leveraging the context-aware embeddings for each gene that are generated\
          \ by Geneformer, the approach would be to build a model on top of Geneformer\
          \ that extracts embeddings for each gene in the particular context (e.g.\
          \ particular cell type or normal vs disease, etc) and then provides this\
          \ as an input to a model that uses this embedding to accomplish a particular\
          \ task, such as explicitly inferring gene network connections, etc. The\
          \ best layer to extract the embeddings from depends on how close the scientific\
          \ question is to the pretraining or fine-tuning objective. The last layer\
          \ weights are often very tuned to the particular objective, whereas the\
          \ second to last layer may provide a more general representation of the\
          \ genes. For example, if using the pretrained model without further fine-tuning,\
          \ one may choose the second to last layer embeddings as an input to their\
          \ model. Please note that because the embeddings are context-aware, it would\
          \ make the most sense to generate the embeddings dynamically in a unified\
          \ manner with the model on top of Geneformer rather than trying to enforce\
          \ a frozen non-context-aware embedding for each gene from a random cell\
          \ state or from a collection of random cell states. If that doesn\u2019\
          t answer your question, feel free to respond with more details of what you\
          \ are trying to do with your model either here or by email so I can further\
          \ guide the best use of Geneformer to accomplish your goals."
        updatedAt: '2023-07-22T15:26:11.753Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64bbf5132e66dc7b8b9f48b4
    id: 64bbf5132e66dc7b8b9f48af
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer! Geneformer\u2019s input is\
    \ rank value encodings. Please see the transcriptome tokenizer and manuscript\
    \ Methods to understand how these are generated. For the tokens themselves, each\
    \ token corresponds to a particular gene as identified by Ensembl ID. The embeddings\
    \ for the genes are generated by the model. The embeddings are context-aware,\
    \ so they will represent the gene within the particular context of the individual\
    \ cell presented to the model. If you are interested in leveraging the context-aware\
    \ embeddings for each gene that are generated by Geneformer, the approach would\
    \ be to build a model on top of Geneformer that extracts embeddings for each gene\
    \ in the particular context (e.g. particular cell type or normal vs disease, etc)\
    \ and then provides this as an input to a model that uses this embedding to accomplish\
    \ a particular task, such as explicitly inferring gene network connections, etc.\
    \ The best layer to extract the embeddings from depends on how close the scientific\
    \ question is to the pretraining or fine-tuning objective. The last layer weights\
    \ are often very tuned to the particular objective, whereas the second to last\
    \ layer may provide a more general representation of the genes. For example, if\
    \ using the pretrained model without further fine-tuning, one may choose the second\
    \ to last layer embeddings as an input to their model. Please note that because\
    \ the embeddings are context-aware, it would make the most sense to generate the\
    \ embeddings dynamically in a unified manner with the model on top of Geneformer\
    \ rather than trying to enforce a frozen non-context-aware embedding for each\
    \ gene from a random cell state or from a collection of random cell states. If\
    \ that doesn\u2019t answer your question, feel free to respond with more details\
    \ of what you are trying to do with your model either here or by email so I can\
    \ further guide the best use of Geneformer to accomplish your goals."
  created_at: 2023-07-22 14:26:11+00:00
  edited: false
  hidden: false
  id: 64bbf5132e66dc7b8b9f48af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-22T15:26:11.000Z'
    data:
      status: closed
    id: 64bbf5132e66dc7b8b9f48b4
    type: status-change
  author: ctheodoris
  created_at: 2023-07-22 14:26:11+00:00
  id: 64bbf5132e66dc7b8b9f48b4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 131
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: How to create the token-embbedings for the whole vocabulary.
