!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ricomnl
conflicting_files: null
created_at: 2023-07-06 16:41:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-07-06T17:41:58.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629283547401428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: '<p>Hi there,</p>

          <p>Thanks for your amazing work (love the almost daily new commits)! I was
          curious, how was the 12 layer geneformer model trained? Did you use the
          same training strategy / hyperparameters or what changed compared to how
          the 6 layer model was trained?</p>

          <p>Cheers!</p>

          '
        raw: "Hi there,\r\n\r\nThanks for your amazing work (love the almost daily\
          \ new commits)! I was curious, how was the 12 layer geneformer model trained?\
          \ Did you use the same training strategy / hyperparameters or what changed\
          \ compared to how the 6 layer model was trained?\r\n\r\nCheers!"
        updatedAt: '2023-07-06T17:41:58.659Z'
      numEdits: 0
      reactions: []
    id: 64a6fce69f738a9011649e6a
    type: comment
  author: ricomnl
  content: "Hi there,\r\n\r\nThanks for your amazing work (love the almost daily new\
    \ commits)! I was curious, how was the 12 layer geneformer model trained? Did\
    \ you use the same training strategy / hyperparameters or what changed compared\
    \ to how the 6 layer model was trained?\r\n\r\nCheers!"
  created_at: 2023-07-06 16:41:58+00:00
  edited: false
  hidden: false
  id: 64a6fce69f738a9011649e6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-06T19:03:24.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7412563562393188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! Yes, it is the same strategy
          but different hyperparameters. You can load the training_args.bin to see
          the hyperparameters used:</p>

          <pre><code>import torch

          training_args = torch.load(path_to_training_args)

          training_args

          </code></pre>

          '
        raw: "Thank you for your interest in Geneformer! Yes, it is the same strategy\
          \ but different hyperparameters. You can load the training_args.bin to see\
          \ the hyperparameters used:\n\n    import torch\n    training_args = torch.load(path_to_training_args)\n\
          \    training_args"
        updatedAt: '2023-07-06T19:03:52.938Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Minnnzhou
        - la12
      relatedEventId: 64a70ffc8ef353d4f8cc9658
    id: 64a70ffc8ef353d4f8cc9657
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer! Yes, it is the same strategy\
    \ but different hyperparameters. You can load the training_args.bin to see the\
    \ hyperparameters used:\n\n    import torch\n    training_args = torch.load(path_to_training_args)\n\
    \    training_args"
  created_at: 2023-07-06 18:03:24+00:00
  edited: true
  hidden: false
  id: 64a70ffc8ef353d4f8cc9657
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-06T19:03:24.000Z'
    data:
      status: closed
    id: 64a70ffc8ef353d4f8cc9658
    type: status-change
  author: ctheodoris
  created_at: 2023-07-06 18:03:24+00:00
  id: 64a70ffc8ef353d4f8cc9658
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 98
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: How was the 12 layer geneformer model trained?
