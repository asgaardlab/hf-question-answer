!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yanayrosen
conflicting_files: null
created_at: 2023-09-30 00:10:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/748c3e89a4e8b58c5a8817fcfee4a6f3.svg
      fullname: Yanay Rosen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanayrosen
      type: user
    createdAt: '2023-09-30T01:10:20.000Z'
    data:
      edited: true
      editors:
      - yanayrosen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9159561395645142
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/748c3e89a4e8b58c5a8817fcfee4a6f3.svg
          fullname: Yanay Rosen
          isHf: false
          isPro: false
          name: yanayrosen
          type: user
        html: '<p>Hello,</p>

          <p>There are a number of issues with the codebase that damage usability.
          It would be great if these could be fixed!</p>

          <p>emb_extractor.py</p>

          <ul>

          <li>This file shuffles the dataset. For situations in which you are doing
          evaluation, this is very unwanted. It is extremely annoying to have to reverse
          this.</li>

          <li>This file saves csvs in the following format:</li>

          </ul>

          <pre><code>8,tensor(-0.0535),tensor(-0.0137),tensor(0.0381),tensor(0.0003),tensor(0.0163),tensor(-0.0026),tensor(-0.0553)...

          </code></pre>

          <p>which is not usable. It is also incredibly slow to save this csv when
          dealing with large datasets.</p>

          <p>Tensors should be saved as torch files, or maybe as numpy array files.
          If the embeddings are saved as a csv, they should be saved as the full floats,
          not as the .__str__ version of the tensor. </p>

          <ul>

          <li>the embeddings variable, when created using the code in the notebook
          example, should be a numpy array of floats, or a torch tensor of floats.
          Currently it is created as an array of lists of tensors.</li>

          </ul>

          <p>In scripts with progress bars, <code>tqdm.notebook</code> is imported,
          which does not work when using the in a terminal. <code>tqdm.auto</code>
          can be imported instead.</p>

          <p>Thanks! </p>

          '
        raw: "Hello,\n\nThere are a number of issues with the codebase that damage\
          \ usability. It would be great if these could be fixed!\n\nemb_extractor.py\n\
          - This file shuffles the dataset. For situations in which you are doing\
          \ evaluation, this is very unwanted. It is extremely annoying to have to\
          \ reverse this.\n- This file saves csvs in the following format:\n```\n\
          8,tensor(-0.0535),tensor(-0.0137),tensor(0.0381),tensor(0.0003),tensor(0.0163),tensor(-0.0026),tensor(-0.0553)...\n\
          ```\nwhich is not usable. It is also incredibly slow to save this csv when\
          \ dealing with large datasets.\n\nTensors should be saved as torch files,\
          \ or maybe as numpy array files. If the embeddings are saved as a csv, they\
          \ should be saved as the full floats, not as the .\\_\\_str\\_\\_ version\
          \ of the tensor. \n\n- the embeddings variable, when created using the code\
          \ in the notebook example, should be a numpy array of floats, or a torch\
          \ tensor of floats. Currently it is created as an array of lists of tensors.\n\
          \nIn scripts with progress bars, `tqdm.notebook` is imported, which does\
          \ not work when using the in a terminal. `tqdm.auto` can be imported instead.\n\
          \nThanks! "
        updatedAt: '2023-09-30T01:26:21.627Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hansen7
    id: 6517757c70746a75c101544f
    type: comment
  author: yanayrosen
  content: "Hello,\n\nThere are a number of issues with the codebase that damage usability.\
    \ It would be great if these could be fixed!\n\nemb_extractor.py\n- This file\
    \ shuffles the dataset. For situations in which you are doing evaluation, this\
    \ is very unwanted. It is extremely annoying to have to reverse this.\n- This\
    \ file saves csvs in the following format:\n```\n8,tensor(-0.0535),tensor(-0.0137),tensor(0.0381),tensor(0.0003),tensor(0.0163),tensor(-0.0026),tensor(-0.0553)...\n\
    ```\nwhich is not usable. It is also incredibly slow to save this csv when dealing\
    \ with large datasets.\n\nTensors should be saved as torch files, or maybe as\
    \ numpy array files. If the embeddings are saved as a csv, they should be saved\
    \ as the full floats, not as the .\\_\\_str\\_\\_ version of the tensor. \n\n\
    - the embeddings variable, when created using the code in the notebook example,\
    \ should be a numpy array of floats, or a torch tensor of floats. Currently it\
    \ is created as an array of lists of tensors.\n\nIn scripts with progress bars,\
    \ `tqdm.notebook` is imported, which does not work when using the in a terminal.\
    \ `tqdm.auto` can be imported instead.\n\nThanks! "
  created_at: 2023-09-30 00:10:20+00:00
  edited: true
  hidden: false
  id: 6517757c70746a75c101544f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-30T11:16:50.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9016129374504089
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer and for your suggestions!
          </p>

          <ul>

          <li><p>Shuffling: the emb_extractor actually sorts the dataset by length
          so that the largest input is first. This is done to encounter memory limitations
          earlier so users can more easily optimize the maximum batch size they can
          use based on their resources. You can include labels for the cells so that
          the output embeddings are labeled accordingly. That way you can arrange
          the output in any order you prefer.</p>

          </li>

          <li><p>Output data type: Thank you for your suggestion to include an option
          to output the embeddings as torch files. Right now the output is a dataframe
          so that it can be used for plotting, but if users are not interested in
          plotting, we can add the option to instead output as torch files. If you
          have already implemented this, please submit a pull request so we can test
          it and merge it. I will note though, the output .csv should be a dataframe
          with floats, not the string format of tensors that you mentioned. Please
          make sure you are using the current version.</p>

          </li>

          <li><p>Embeddings variable format: Please specify which variable you are
          referring to (line of code would be helpful).</p>

          </li>

          <li><p>Progress bar: Thank you for the suggestion - we can update this for
          improved usability with batch jobs.</p>

          </li>

          <li><p>Current solution: Most of the issues you are having may be resolved
          for you by just importing and using the function "get_embs" within the embedding
          extractor. That way you can prepare the data how you''d like, without sorting,
          and output embedding tensors in the format you are interested in.</p>

          </li>

          </ul>

          '
        raw: "Thank you for your interest in Geneformer and for your suggestions!\
          \ \n\n- Shuffling: the emb_extractor actually sorts the dataset by length\
          \ so that the largest input is first. This is done to encounter memory limitations\
          \ earlier so users can more easily optimize the maximum batch size they\
          \ can use based on their resources. You can include labels for the cells\
          \ so that the output embeddings are labeled accordingly. That way you can\
          \ arrange the output in any order you prefer.\n\n- Output data type: Thank\
          \ you for your suggestion to include an option to output the embeddings\
          \ as torch files. Right now the output is a dataframe so that it can be\
          \ used for plotting, but if users are not interested in plotting, we can\
          \ add the option to instead output as torch files. If you have already implemented\
          \ this, please submit a pull request so we can test it and merge it. I will\
          \ note though, the output .csv should be a dataframe with floats, not the\
          \ string format of tensors that you mentioned. Please make sure you are\
          \ using the current version.\n\n- Embeddings variable format: Please specify\
          \ which variable you are referring to (line of code would be helpful).\n\
          \n- Progress bar: Thank you for the suggestion - we can update this for\
          \ improved usability with batch jobs.\n\n- Current solution: Most of the\
          \ issues you are having may be resolved for you by just importing and using\
          \ the function \"get_embs\" within the embedding extractor. That way you\
          \ can prepare the data how you'd like, without sorting, and output embedding\
          \ tensors in the format you are interested in.\n"
        updatedAt: '2023-09-30T11:16:50.958Z'
      numEdits: 0
      reactions: []
    id: 651803a21660b68bb2995516
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer and for your suggestions! \n\
    \n- Shuffling: the emb_extractor actually sorts the dataset by length so that\
    \ the largest input is first. This is done to encounter memory limitations earlier\
    \ so users can more easily optimize the maximum batch size they can use based\
    \ on their resources. You can include labels for the cells so that the output\
    \ embeddings are labeled accordingly. That way you can arrange the output in any\
    \ order you prefer.\n\n- Output data type: Thank you for your suggestion to include\
    \ an option to output the embeddings as torch files. Right now the output is a\
    \ dataframe so that it can be used for plotting, but if users are not interested\
    \ in plotting, we can add the option to instead output as torch files. If you\
    \ have already implemented this, please submit a pull request so we can test it\
    \ and merge it. I will note though, the output .csv should be a dataframe with\
    \ floats, not the string format of tensors that you mentioned. Please make sure\
    \ you are using the current version.\n\n- Embeddings variable format: Please specify\
    \ which variable you are referring to (line of code would be helpful).\n\n- Progress\
    \ bar: Thank you for the suggestion - we can update this for improved usability\
    \ with batch jobs.\n\n- Current solution: Most of the issues you are having may\
    \ be resolved for you by just importing and using the function \"get_embs\" within\
    \ the embedding extractor. That way you can prepare the data how you'd like, without\
    \ sorting, and output embedding tensors in the format you are interested in.\n"
  created_at: 2023-09-30 10:16:50+00:00
  edited: false
  hidden: false
  id: 651803a21660b68bb2995516
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/748c3e89a4e8b58c5a8817fcfee4a6f3.svg
      fullname: Yanay Rosen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanayrosen
      type: user
    createdAt: '2023-09-30T23:43:59.000Z'
    data:
      edited: false
      editors:
      - yanayrosen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8606606721878052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/748c3e89a4e8b58c5a8817fcfee4a6f3.svg
          fullname: Yanay Rosen
          isHf: false
          isPro: false
          name: yanayrosen
          type: user
        html: '<p>Thanks for the reply!</p>

          <ul>

          <li><p>I found this sorting later (it was the reason for the issue in <a
          href="https://huggingface.co/ctheodoris/Geneformer/discussions/253">https://huggingface.co/ctheodoris/Geneformer/discussions/253</a>).
          It would be beneficial to include this in the embedding extraction notebook.
          It would also be useful to add options to disable this behavior.</p>

          </li>

          <li><p>For saving torch or numpy output,  the function should just return
          embs.cpu() or embs.cpu().numpy().  This is relevant to the embeddings variable
          format bullet point also:</p>

          </li>

          <li><p>this was for this line in the extract_and_plot notebook. Pandas seems
          to convert the rows to arrays, but not the actual values of the tensors
          to floats. <a href="https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/emb_extractor.py#L401">https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/emb_extractor.py#L401</a>
          instead of just claling .cpu(), .numpy() can also be added.</p>

          </li>

          </ul>

          <p><code>embs = embex.extract_embs("../fine_tuned_models/geneformer-6L-30M_CellClassifier_cardiomyopathies_220224",                           "path/to/input_data/",                           "path/to/output_directory/",                           "output_prefix")</code></p>

          <ul>

          <li>Thanks! It would be great if there was a specific tutorial on how to
          get embeddings for a h5ad, since this is probably the most used file format,
          and since the tokenizer now supports it.</li>

          </ul>

          <p>Thanks!</p>

          '
        raw: "Thanks for the reply!\n\n- I found this sorting later (it was the reason\
          \ for the issue in https://huggingface.co/ctheodoris/Geneformer/discussions/253).\
          \ It would be beneficial to include this in the embedding extraction notebook.\
          \ It would also be useful to add options to disable this behavior.\n\n-\
          \ For saving torch or numpy output,  the function should just return embs.cpu()\
          \ or embs.cpu().numpy().  This is relevant to the embeddings variable format\
          \ bullet point also:\n\n- this was for this line in the extract_and_plot\
          \ notebook. Pandas seems to convert the rows to arrays, but not the actual\
          \ values of the tensors to floats. https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/emb_extractor.py#L401\
          \ instead of just claling .cpu(), .numpy() can also be added. \n\n```embs\
          \ = embex.extract_embs(\"../fine_tuned_models/geneformer-6L-30M_CellClassifier_cardiomyopathies_220224\"\
          ,\n                          \"path/to/input_data/\",\n                \
          \          \"path/to/output_directory/\",\n                          \"\
          output_prefix\")```\n\n- Thanks! It would be great if there was a specific\
          \ tutorial on how to get embeddings for a h5ad, since this is probably the\
          \ most used file format, and since the tokenizer now supports it.\n\nThanks!"
        updatedAt: '2023-09-30T23:43:59.064Z'
      numEdits: 0
      reactions: []
    id: 6518b2bf4fadbeb643df0a72
    type: comment
  author: yanayrosen
  content: "Thanks for the reply!\n\n- I found this sorting later (it was the reason\
    \ for the issue in https://huggingface.co/ctheodoris/Geneformer/discussions/253).\
    \ It would be beneficial to include this in the embedding extraction notebook.\
    \ It would also be useful to add options to disable this behavior.\n\n- For saving\
    \ torch or numpy output,  the function should just return embs.cpu() or embs.cpu().numpy().\
    \  This is relevant to the embeddings variable format bullet point also:\n\n-\
    \ this was for this line in the extract_and_plot notebook. Pandas seems to convert\
    \ the rows to arrays, but not the actual values of the tensors to floats. https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/emb_extractor.py#L401\
    \ instead of just claling .cpu(), .numpy() can also be added. \n\n```embs = embex.extract_embs(\"\
    ../fine_tuned_models/geneformer-6L-30M_CellClassifier_cardiomyopathies_220224\"\
    ,\n                          \"path/to/input_data/\",\n                      \
    \    \"path/to/output_directory/\",\n                          \"output_prefix\"\
    )```\n\n- Thanks! It would be great if there was a specific tutorial on how to\
    \ get embeddings for a h5ad, since this is probably the most used file format,\
    \ and since the tokenizer now supports it.\n\nThanks!"
  created_at: 2023-09-30 22:43:59+00:00
  edited: false
  hidden: false
  id: 6518b2bf4fadbeb643df0a72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95cff7686299947d02ede1bd5c1ed8eb.svg
      fullname: Hanchen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansen7
      type: user
    createdAt: '2023-10-01T22:32:05.000Z'
    data:
      edited: true
      editors:
      - hansen7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7068421840667725
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95cff7686299947d02ede1bd5c1ed8eb.svg
          fullname: Hanchen
          isHf: false
          isPro: false
          name: hansen7
          type: user
        html: '<p>Probably related to this discussion. During my trying: 1. tokenizing
          external .h5ad data; then 2. extract cell embeddings from pre-trained geneformers,
          I have to replace the following line with<br><a href="https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213">https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213</a></p>

          <p>which is:<br># X_view = adata[idx, coding_miRNA_loc].X<br>adata = adata.to_memory()<br>X_view
          = adata[:, coding_miRNA_loc][idx, :].X</p>

          <p>Probably due to the fact that I installed the <code>AnnData</code> of  <code>0.9.2</code>,
          a more recent version.</p>

          '
        raw: "Probably related to this discussion. During my trying: 1. tokenizing\
          \ external .h5ad data; then 2. extract cell embeddings from pre-trained\
          \ geneformers, I have to replace the following line with \nhttps://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213\n\
          \nwhich is:\n\\# X_view = adata[idx, coding_miRNA_loc].X\nadata = adata.to_memory()\n\
          X_view = adata[:, coding_miRNA_loc][idx, :].X\n\n\nProbably due to the fact\
          \ that I installed the `AnnData` of  `0.9.2`, a more recent version."
        updatedAt: '2023-10-02T00:12:13.891Z'
      numEdits: 1
      reactions: []
    id: 6519f365757417990d4accc2
    type: comment
  author: hansen7
  content: "Probably related to this discussion. During my trying: 1. tokenizing external\
    \ .h5ad data; then 2. extract cell embeddings from pre-trained geneformers, I\
    \ have to replace the following line with \nhttps://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213\n\
    \nwhich is:\n\\# X_view = adata[idx, coding_miRNA_loc].X\nadata = adata.to_memory()\n\
    X_view = adata[:, coding_miRNA_loc][idx, :].X\n\n\nProbably due to the fact that\
    \ I installed the `AnnData` of  `0.9.2`, a more recent version."
  created_at: 2023-10-01 21:32:05+00:00
  edited: true
  hidden: false
  id: 6519f365757417990d4accc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-04T00:54:26.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8709686994552612
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up! </p>

          <ul>

          <li><p>Sorting: by providing the argument "emb_label" with the necessary
          label from the .dataset column, this will associate the final embeddings
          with the necessary label. There is a reason for the sorting (to encounter
          memory constraints sooner, as I described earlier). If the label is used,
          the sorting should not provide an issue. </p>

          </li>

          <li><p>Tensor output: We added an option "output_torch_embs" to the extract_embs
          function. Please note this will output the embeddings as a tensor as well
          as a dataframe with the associated labels so expect 2 outputs.</p>

          </li>

          <li><p>Pandas output: When running the code, the pandas dataframe embedding
          values are floats. Please provide the pandas version you are using in case
          this is the reason we are not able to reproduce this issue. We added .numpy()
          in case this resolves the issue for you, but again, when we run this code
          the output is already in the float format.</p>

          </li>

          <li><p>Anndata --&gt; embeddings: The input to the model is rank value encodings,
          so anndata files (or any other scRNAseq files) first need to be converted
          to this format using the provided tokenizer before being used for any modeling
          (e.g. extracting embeddings, fine-tuning the model, etc). We have an example
          dataset already indicated in the example notebook for extracting embeddings,
          but for additional clarity, we will also link to the example notebook for
          tokenization.</p>

          </li>

          </ul>

          '
        raw: "Thank you for following up! \n\n- Sorting: by providing the argument\
          \ \"emb_label\" with the necessary label from the .dataset column, this\
          \ will associate the final embeddings with the necessary label. There is\
          \ a reason for the sorting (to encounter memory constraints sooner, as I\
          \ described earlier). If the label is used, the sorting should not provide\
          \ an issue. \n\n- Tensor output: We added an option \"output_torch_embs\"\
          \ to the extract_embs function. Please note this will output the embeddings\
          \ as a tensor as well as a dataframe with the associated labels so expect\
          \ 2 outputs.\n\n- Pandas output: When running the code, the pandas dataframe\
          \ embedding values are floats. Please provide the pandas version you are\
          \ using in case this is the reason we are not able to reproduce this issue.\
          \ We added .numpy() in case this resolves the issue for you, but again,\
          \ when we run this code the output is already in the float format.\n\n-\
          \ Anndata --> embeddings: The input to the model is rank value encodings,\
          \ so anndata files (or any other scRNAseq files) first need to be converted\
          \ to this format using the provided tokenizer before being used for any\
          \ modeling (e.g. extracting embeddings, fine-tuning the model, etc). We\
          \ have an example dataset already indicated in the example notebook for\
          \ extracting embeddings, but for additional clarity, we will also link to\
          \ the example notebook for tokenization."
        updatedAt: '2023-10-04T00:54:26.156Z'
      numEdits: 0
      reactions: []
      relatedEventId: 651cb7c2d651a1b446abed3d
    id: 651cb7c2d651a1b446abed38
    type: comment
  author: ctheodoris
  content: "Thank you for following up! \n\n- Sorting: by providing the argument \"\
    emb_label\" with the necessary label from the .dataset column, this will associate\
    \ the final embeddings with the necessary label. There is a reason for the sorting\
    \ (to encounter memory constraints sooner, as I described earlier). If the label\
    \ is used, the sorting should not provide an issue. \n\n- Tensor output: We added\
    \ an option \"output_torch_embs\" to the extract_embs function. Please note this\
    \ will output the embeddings as a tensor as well as a dataframe with the associated\
    \ labels so expect 2 outputs.\n\n- Pandas output: When running the code, the pandas\
    \ dataframe embedding values are floats. Please provide the pandas version you\
    \ are using in case this is the reason we are not able to reproduce this issue.\
    \ We added .numpy() in case this resolves the issue for you, but again, when we\
    \ run this code the output is already in the float format.\n\n- Anndata --> embeddings:\
    \ The input to the model is rank value encodings, so anndata files (or any other\
    \ scRNAseq files) first need to be converted to this format using the provided\
    \ tokenizer before being used for any modeling (e.g. extracting embeddings, fine-tuning\
    \ the model, etc). We have an example dataset already indicated in the example\
    \ notebook for extracting embeddings, but for additional clarity, we will also\
    \ link to the example notebook for tokenization."
  created_at: 2023-10-03 23:54:26+00:00
  edited: false
  hidden: false
  id: 651cb7c2d651a1b446abed38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-04T00:54:26.000Z'
    data:
      status: closed
    id: 651cb7c2d651a1b446abed3d
    type: status-change
  author: ctheodoris
  created_at: 2023-10-03 23:54:26+00:00
  id: 651cb7c2d651a1b446abed3d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-10-04T00:56:43.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8027936220169067
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<blockquote>

          <p>Probably related to this discussion. During my trying: 1. tokenizing
          external .h5ad data; then 2. extract cell embeddings from pre-trained geneformers,
          I have to replace the following line with<br><a href="https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213">https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213</a></p>

          <p>which is:<br># X_view = adata[idx, coding_miRNA_loc].X<br>adata = adata.to_memory()<br>X_view
          = adata[:, coding_miRNA_loc][idx, :].X</p>

          <p>Probably due to the fact that I installed the <code>AnnData</code> of  <code>0.9.2</code>,
          a more recent version.</p>

          </blockquote>

          <p>Thank you for noting this! Would you mind starting a new discussion with
          a title relevant to this as it is a separate issue? It will be helpful to
          future users who are looking for the answer to the same question. Thank
          you!</p>

          '
        raw: "> Probably related to this discussion. During my trying: 1. tokenizing\
          \ external .h5ad data; then 2. extract cell embeddings from pre-trained\
          \ geneformers, I have to replace the following line with \n> https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213\n\
          > \n> which is:\n> \\# X_view = adata[idx, coding_miRNA_loc].X\n> adata\
          \ = adata.to_memory()\n> X_view = adata[:, coding_miRNA_loc][idx, :].X\n\
          > \n> \n> Probably due to the fact that I installed the `AnnData` of  `0.9.2`,\
          \ a more recent version.\n\n\nThank you for noting this! Would you mind\
          \ starting a new discussion with a title relevant to this as it is a separate\
          \ issue? It will be helpful to future users who are looking for the answer\
          \ to the same question. Thank you!"
        updatedAt: '2023-10-04T00:56:43.931Z'
      numEdits: 0
      reactions: []
    id: 651cb84b2a77c48f29c9b8dc
    type: comment
  author: ctheodoris
  content: "> Probably related to this discussion. During my trying: 1. tokenizing\
    \ external .h5ad data; then 2. extract cell embeddings from pre-trained geneformers,\
    \ I have to replace the following line with \n> https://huggingface.co/ctheodoris/Geneformer/blob/4302f4835eda5320b13de85092c97f2c6679b36e/geneformer/tokenizer.py#L213\n\
    > \n> which is:\n> \\# X_view = adata[idx, coding_miRNA_loc].X\n> adata = adata.to_memory()\n\
    > X_view = adata[:, coding_miRNA_loc][idx, :].X\n> \n> \n> Probably due to the\
    \ fact that I installed the `AnnData` of  `0.9.2`, a more recent version.\n\n\n\
    Thank you for noting this! Would you mind starting a new discussion with a title\
    \ relevant to this as it is a separate issue? It will be helpful to future users\
    \ who are looking for the answer to the same question. Thank you!"
  created_at: 2023-10-03 23:56:43+00:00
  edited: false
  hidden: false
  id: 651cb84b2a77c48f29c9b8dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95cff7686299947d02ede1bd5c1ed8eb.svg
      fullname: Hanchen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansen7
      type: user
    createdAt: '2023-10-04T21:29:35.000Z'
    data:
      edited: false
      editors:
      - hansen7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7302242517471313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95cff7686299947d02ede1bd5c1ed8eb.svg
          fullname: Hanchen
          isHf: false
          isPro: false
          name: hansen7
          type: user
        html: '<p>Its now <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/255">here</a>!</p>

          '
        raw: Its now [here](https://huggingface.co/ctheodoris/Geneformer/discussions/255)!
        updatedAt: '2023-10-04T21:29:35.944Z'
      numEdits: 0
      reactions: []
    id: 651dd93fd627ffeef4dd1b75
    type: comment
  author: hansen7
  content: Its now [here](https://huggingface.co/ctheodoris/Geneformer/discussions/255)!
  created_at: 2023-10-04 20:29:35+00:00
  edited: false
  hidden: false
  id: 651dd93fd627ffeef4dd1b75
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 252
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Issues with the codebase
