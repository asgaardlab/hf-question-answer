!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pchiang5
conflicting_files: null
created_at: 2023-06-24 01:35:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-24T02:35:50.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5461460947990417
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>Hello</p>\n<p>Because my graphic device is of limited RAM, I tried\
          \ to test if the code could run with CPU at the expense of time with the\
          \ following argument:</p>\n<h1 id=\"gpu_number--0\">GPU_NUMBER = [0]</h1>\n\
          <p>os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"<br>os.environ[\"NCCL_DEBUG\"\
          ] = \"INFO\"</p>\n<pre><code>        predict_logits += [torch.squeeze(outputs.logits.to(\"\
          cpu\"))]\n        predict_labels += [torch.squeeze(label_batch.to(\"cpu\"\
          ))]\n\n    model = model.to(\"cpu\")\n</code></pre>\n<p>training_args =\
          \ {<br>    \"learning_rate\": max_lr,<br>    \"do_train\": True,<br>   \
          \ \"evaluation_strategy\": \"no\",<br>    \"save_strategy\": \"epoch\",<br>\
          \    \"logging_steps\": 100,<br>    \"group_by_length\": True,<br>    \"\
          length_column_name\": \"length\",<br>    \"disable_tqdm\": False,<br>  \
          \  \"lr_scheduler_type\": lr_schedule_fn,<br>    \"warmup_steps\": warmup_steps,<br>\
          \    \"fp16\": False,<br>    \"bf16\": True,<br>    \"fp16_full_eval\":\
          \ False,<br>    \"no_cuda\": True,<br>    \"half_precision_backend\": \"\
          cpu_amp\",<br>    \"weight_decay\": 0.001,<br>    \"per_device_train_batch_size\"\
          : geneformer_batch_size,<br>    \"per_device_eval_batch_size\": geneformer_batch_size,<br>\
          \    \"num_train_epochs\": epochs,<br>}</p>\n<p> However, I got the following\
          \ error:</p>\n<p>File /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1340,\
          \ in TrainingArguments.<strong>post_init</strong>(self)<br>   1334     if\
          \ version.parse(version.parse(torch.<strong>version</strong>).base_version)\
          \ == version.parse(\"2.0.0\") and self.fp16:<br>   1335         raise ValueError(\"\
          --optim adamw_torch_fused with --fp16 requires PyTorch&gt;2.0\")<br>   1337\
          \ if (<br>   1338     self.framework == \"pt\"<br>   1339     and is_torch_available()<br>-&gt;\
          \ 1340     and (self.device.type != \"cuda\")<br>   1341     and (get_xla_device_type(self.device)\
          \ != \"GPU\")<br>   1342     and (self.fp16 or self.fp16_full_eval)<br>\
          \   1343 ):<br>   1344     raise ValueError(<br>   1345         \"FP16 Mixed\
          \ precision training with AMP or APEX (<code>--fp16</code>) and FP16 half\
          \ precision evaluation\"<br>   1346         \" (<code>--fp16_full_eval</code>)\
          \ can only be used on CUDA devices.\"<br>   1347     )<br>   1349 if (<br>\
          \   1350     self.framework == \"pt\"<br>   1351     and is_torch_available()<br>\
          \   (...)<br>   1356     and (self.bf16 or self.bf16_full_eval)<br>   1357\
          \ ):</p>\n<p>File /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1764,\
          \ in TrainingArguments.device(self)<br>   1760 \"\"\"<br>   1761 The device\
          \ used by this process.<br>   1762 \"\"\"<br>   1763 requires_backends(self,\
          \ [\"torch\"])<br>-&gt; 1764 return self._setup_devices</p>\n<p>File /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/utils/generic.py:54,\
          \ in cached_property.<strong>get</strong>(self, obj, objtype)<br>     52\
          \ cached = getattr(obj, attr, None)<br>     53 if cached is None:<br>---&gt;\
          \ 54     cached = self.fget(obj)<br>     55     setattr(obj, attr, cached)<br>\
          \     56 return cached</p>\n<p>File /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1672,\
          \ in TrainingArguments._setup_devices(self)<br>   1670 if not is_sagemaker_mp_enabled():<br>\
          \   1671     if not is_accelerate_available(min_version=\"0.20.1\"):<br>-&gt;\
          \ 1672         raise ImportError(<br>   1673             \"Using the <code>Trainer</code>\
          \ with <code>PyTorch</code> requires <code>accelerate&gt;=0.20.1</code>:\
          \ Please run <code>pip install transformers[torch]</code> or <code>pip install\
          \ accelerate -U</code>\"<br>   1674         )<br>   1675     AcceleratorState._reset_state(reset_partial_state=True)<br>\
          \   1676 self.distributed_state = None</p>\n<p>ImportError: Using the <code>Trainer</code>\
          \ with <code>PyTorch</code> requires <code>accelerate&gt;=0.20.1</code>:\
          \ Please run <code>pip install transformers[torch]</code> or <code>pip install\
          \ accelerate -U</code></p>\n<p>I have tried installing accelerate==0.20.1\
          \ and it has not worked out. How shall I resolve the issue? Thank you.</p>\n"
        raw: "Hello\r\n\r\nBecause my graphic device is of limited RAM, I tried to\
          \ test if the code could run with CPU at the expense of time with the following\
          \ argument:\r\n\r\n# GPU_NUMBER = [0]\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ] = \"\"\r\nos.environ[\"NCCL_DEBUG\"] = \"INFO\"\r\n\r\n            predict_logits\
          \ += [torch.squeeze(outputs.logits.to(\"cpu\"))]\r\n            predict_labels\
          \ += [torch.squeeze(label_batch.to(\"cpu\"))]\r\n\r\n        model = model.to(\"\
          cpu\")\r\n\r\ntraining_args = {\r\n    \"learning_rate\": max_lr,\r\n  \
          \  \"do_train\": True,\r\n    \"evaluation_strategy\": \"no\",\r\n    \"\
          save_strategy\": \"epoch\",\r\n    \"logging_steps\": 100,\r\n    \"group_by_length\"\
          : True,\r\n    \"length_column_name\": \"length\",\r\n    \"disable_tqdm\"\
          : False,\r\n    \"lr_scheduler_type\": lr_schedule_fn,\r\n    \"warmup_steps\"\
          : warmup_steps,\r\n    \"fp16\": False,\r\n    \"bf16\": True,\r\n    \"\
          fp16_full_eval\": False,\r\n    \"no_cuda\": True,\r\n    \"half_precision_backend\"\
          : \"cpu_amp\",\r\n    \"weight_decay\": 0.001,\r\n    \"per_device_train_batch_size\"\
          : geneformer_batch_size,\r\n    \"per_device_eval_batch_size\": geneformer_batch_size,\r\
          \n    \"num_train_epochs\": epochs,\r\n}\r\n\r\n However, I got the following\
          \ error:\r\n\r\n\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1340,\
          \ in TrainingArguments.__post_init__(self)\r\n   1334     if version.parse(version.parse(torch.__version__).base_version)\
          \ == version.parse(\"2.0.0\") and self.fp16:\r\n   1335         raise ValueError(\"\
          --optim adamw_torch_fused with --fp16 requires PyTorch>2.0\")\r\n   1337\
          \ if (\r\n   1338     self.framework == \"pt\"\r\n   1339     and is_torch_available()\r\
          \n-> 1340     and (self.device.type != \"cuda\")\r\n   1341     and (get_xla_device_type(self.device)\
          \ != \"GPU\")\r\n   1342     and (self.fp16 or self.fp16_full_eval)\r\n\
          \   1343 ):\r\n   1344     raise ValueError(\r\n   1345         \"FP16 Mixed\
          \ precision training with AMP or APEX (`--fp16`) and FP16 half precision\
          \ evaluation\"\r\n   1346         \" (`--fp16_full_eval`) can only be used\
          \ on CUDA devices.\"\r\n   1347     )\r\n   1349 if (\r\n   1350     self.framework\
          \ == \"pt\"\r\n   1351     and is_torch_available()\r\n   (...)\r\n   1356\
          \     and (self.bf16 or self.bf16_full_eval)\r\n   1357 ):\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1764,\
          \ in TrainingArguments.device(self)\r\n   1760 \"\"\"\r\n   1761 The device\
          \ used by this process.\r\n   1762 \"\"\"\r\n   1763 requires_backends(self,\
          \ [\"torch\"])\r\n-> 1764 return self._setup_devices\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/utils/generic.py:54,\
          \ in cached_property.__get__(self, obj, objtype)\r\n     52 cached = getattr(obj,\
          \ attr, None)\r\n     53 if cached is None:\r\n---> 54     cached = self.fget(obj)\r\
          \n     55     setattr(obj, attr, cached)\r\n     56 return cached\r\n\r\n\
          File /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1672,\
          \ in TrainingArguments._setup_devices(self)\r\n   1670 if not is_sagemaker_mp_enabled():\r\
          \n   1671     if not is_accelerate_available(min_version=\"0.20.1\"):\r\n\
          -> 1672         raise ImportError(\r\n   1673             \"Using the `Trainer`\
          \ with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install\
          \ transformers[torch]` or `pip install accelerate -U`\"\r\n   1674     \
          \    )\r\n   1675     AcceleratorState._reset_state(reset_partial_state=True)\r\
          \n   1676 self.distributed_state = None\r\n\r\nImportError: Using the `Trainer`\
          \ with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install\
          \ transformers[torch]` or `pip install accelerate -U`\r\n\r\n\r\nI have\
          \ tried installing accelerate==0.20.1 and it has not worked out. How shall\
          \ I resolve the issue? Thank you.\r\n"
        updatedAt: '2023-06-24T02:35:50.973Z'
      numEdits: 0
      reactions: []
    id: 649656864331859e374a5f6c
    type: comment
  author: pchiang5
  content: "Hello\r\n\r\nBecause my graphic device is of limited RAM, I tried to test\
    \ if the code could run with CPU at the expense of time with the following argument:\r\
    \n\r\n# GPU_NUMBER = [0]\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\nos.environ[\"\
    NCCL_DEBUG\"] = \"INFO\"\r\n\r\n            predict_logits += [torch.squeeze(outputs.logits.to(\"\
    cpu\"))]\r\n            predict_labels += [torch.squeeze(label_batch.to(\"cpu\"\
    ))]\r\n\r\n        model = model.to(\"cpu\")\r\n\r\ntraining_args = {\r\n    \"\
    learning_rate\": max_lr,\r\n    \"do_train\": True,\r\n    \"evaluation_strategy\"\
    : \"no\",\r\n    \"save_strategy\": \"epoch\",\r\n    \"logging_steps\": 100,\r\
    \n    \"group_by_length\": True,\r\n    \"length_column_name\": \"length\",\r\n\
    \    \"disable_tqdm\": False,\r\n    \"lr_scheduler_type\": lr_schedule_fn,\r\n\
    \    \"warmup_steps\": warmup_steps,\r\n    \"fp16\": False,\r\n    \"bf16\":\
    \ True,\r\n    \"fp16_full_eval\": False,\r\n    \"no_cuda\": True,\r\n    \"\
    half_precision_backend\": \"cpu_amp\",\r\n    \"weight_decay\": 0.001,\r\n   \
    \ \"per_device_train_batch_size\": geneformer_batch_size,\r\n    \"per_device_eval_batch_size\"\
    : geneformer_batch_size,\r\n    \"num_train_epochs\": epochs,\r\n}\r\n\r\n However,\
    \ I got the following error:\r\n\r\n\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1340,\
    \ in TrainingArguments.__post_init__(self)\r\n   1334     if version.parse(version.parse(torch.__version__).base_version)\
    \ == version.parse(\"2.0.0\") and self.fp16:\r\n   1335         raise ValueError(\"\
    --optim adamw_torch_fused with --fp16 requires PyTorch>2.0\")\r\n   1337 if (\r\
    \n   1338     self.framework == \"pt\"\r\n   1339     and is_torch_available()\r\
    \n-> 1340     and (self.device.type != \"cuda\")\r\n   1341     and (get_xla_device_type(self.device)\
    \ != \"GPU\")\r\n   1342     and (self.fp16 or self.fp16_full_eval)\r\n   1343\
    \ ):\r\n   1344     raise ValueError(\r\n   1345         \"FP16 Mixed precision\
    \ training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\"\r\n\
    \   1346         \" (`--fp16_full_eval`) can only be used on CUDA devices.\"\r\
    \n   1347     )\r\n   1349 if (\r\n   1350     self.framework == \"pt\"\r\n  \
    \ 1351     and is_torch_available()\r\n   (...)\r\n   1356     and (self.bf16\
    \ or self.bf16_full_eval)\r\n   1357 ):\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1764,\
    \ in TrainingArguments.device(self)\r\n   1760 \"\"\"\r\n   1761 The device used\
    \ by this process.\r\n   1762 \"\"\"\r\n   1763 requires_backends(self, [\"torch\"\
    ])\r\n-> 1764 return self._setup_devices\r\n\r\nFile /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/utils/generic.py:54,\
    \ in cached_property.__get__(self, obj, objtype)\r\n     52 cached = getattr(obj,\
    \ attr, None)\r\n     53 if cached is None:\r\n---> 54     cached = self.fget(obj)\r\
    \n     55     setattr(obj, attr, cached)\r\n     56 return cached\r\n\r\nFile\
    \ /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/training_args.py:1672,\
    \ in TrainingArguments._setup_devices(self)\r\n   1670 if not is_sagemaker_mp_enabled():\r\
    \n   1671     if not is_accelerate_available(min_version=\"0.20.1\"):\r\n-> 1672\
    \         raise ImportError(\r\n   1673             \"Using the `Trainer` with\
    \ `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]`\
    \ or `pip install accelerate -U`\"\r\n   1674         )\r\n   1675     AcceleratorState._reset_state(reset_partial_state=True)\r\
    \n   1676 self.distributed_state = None\r\n\r\nImportError: Using the `Trainer`\
    \ with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]`\
    \ or `pip install accelerate -U`\r\n\r\n\r\nI have tried installing accelerate==0.20.1\
    \ and it has not worked out. How shall I resolve the issue? Thank you.\r\n"
  created_at: 2023-06-24 01:35:50+00:00
  edited: false
  hidden: false
  id: 649656864331859e374a5f6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-24T18:32:44.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9613685607910156
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your interest in Geneformer. We have not tried fine-tuning\
          \ the model on CPUs, but based on your error message, it could be that it\u2019\
          s because bf16 is not available for training on CPUs. However, this error\
          \ is coming from Huggingface transformers so we would suggest you check\
          \ their documentation to learn more. If you do find out the solution, please\
          \ feel free to post it here to help others who may have a similar question\
          \ in the future.</p>\n"
        raw: "Thank you for your interest in Geneformer. We have not tried fine-tuning\
          \ the model on CPUs, but based on your error message, it could be that it\u2019\
          s because bf16 is not available for training on CPUs. However, this error\
          \ is coming from Huggingface transformers so we would suggest you check\
          \ their documentation to learn more. If you do find out the solution, please\
          \ feel free to post it here to help others who may have a similar question\
          \ in the future."
        updatedAt: '2023-06-24T18:32:44.804Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649736cc27e41e26a3294f29
    id: 649736cc27e41e26a3294f28
    type: comment
  author: ctheodoris
  content: "Thank you for your interest in Geneformer. We have not tried fine-tuning\
    \ the model on CPUs, but based on your error message, it could be that it\u2019\
    s because bf16 is not available for training on CPUs. However, this error is coming\
    \ from Huggingface transformers so we would suggest you check their documentation\
    \ to learn more. If you do find out the solution, please feel free to post it\
    \ here to help others who may have a similar question in the future."
  created_at: 2023-06-24 17:32:44+00:00
  edited: false
  hidden: false
  id: 649736cc27e41e26a3294f28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-24T18:32:44.000Z'
    data:
      status: closed
    id: 649736cc27e41e26a3294f29
    type: status-change
  author: ctheodoris
  created_at: 2023-06-24 17:32:44+00:00
  id: 649736cc27e41e26a3294f29
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-25T01:20:17.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7434319853782654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>Thank you for your reply.</p>\n<p>I found it worked by removing\
          \ the following block in the training_args.py in the geneformer. Probably\
          \ \"is_torch_available()\" conflicted with  \"(self.device.type != \"cuda\"\
          )\" even I set \"\"no_cuda\": True\" in the training_args.</p>\n<pre><code>\
          \    # if (\n    #     self.framework == \"pt\"\n    #     and is_torch_available()\n\
          \    #     and (self.device.type != \"cuda\")\n    #     and (get_xla_device_type(self.device)\
          \ != \"GPU\")\n    #     and (self.fp16 or self.fp16_full_eval)\n    # ):\n\
          \    #     raise ValueError(\n    #         \"FP16 Mixed precision training\
          \ with AMP or APEX (`--fp16`) and FP16 half precision evaluation\"\n   \
          \ #         \" (`--fp16_full_eval`) can only be used on CUDA devices.\"\n\
          \    #     )\n</code></pre>\n"
        raw: "Thank you for your reply.\n\nI found it worked by removing the following\
          \ block in the training_args.py in the geneformer. Probably \"is_torch_available()\"\
          \ conflicted with  \"(self.device.type != \"cuda\")\" even I set \"\"no_cuda\"\
          : True\" in the training_args.\n\n        # if (\n        #     self.framework\
          \ == \"pt\"\n        #     and is_torch_available()\n        #     and (self.device.type\
          \ != \"cuda\")\n        #     and (get_xla_device_type(self.device) != \"\
          GPU\")\n        #     and (self.fp16 or self.fp16_full_eval)\n        #\
          \ ):\n        #     raise ValueError(\n        #         \"FP16 Mixed precision\
          \ training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\"\
          \n        #         \" (`--fp16_full_eval`) can only be used on CUDA devices.\"\
          \n        #     )"
        updatedAt: '2023-06-25T01:20:17.483Z'
      numEdits: 0
      reactions: []
    id: 64979651b36c3153085c2627
    type: comment
  author: pchiang5
  content: "Thank you for your reply.\n\nI found it worked by removing the following\
    \ block in the training_args.py in the geneformer. Probably \"is_torch_available()\"\
    \ conflicted with  \"(self.device.type != \"cuda\")\" even I set \"\"no_cuda\"\
    : True\" in the training_args.\n\n        # if (\n        #     self.framework\
    \ == \"pt\"\n        #     and is_torch_available()\n        #     and (self.device.type\
    \ != \"cuda\")\n        #     and (get_xla_device_type(self.device) != \"GPU\"\
    )\n        #     and (self.fp16 or self.fp16_full_eval)\n        # ):\n      \
    \  #     raise ValueError(\n        #         \"FP16 Mixed precision training\
    \ with AMP or APEX (`--fp16`) and FP16 half precision evaluation\"\n        #\
    \         \" (`--fp16_full_eval`) can only be used on CUDA devices.\"\n      \
    \  #     )"
  created_at: 2023-06-25 00:20:17+00:00
  edited: false
  hidden: false
  id: 64979651b36c3153085c2627
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-25T01:53:46.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5339471697807312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>After a while, another error 'tensor size mismatch' occurred as\
          \ below with panglao_SRA553822-SRS2119548.dataset. Was it associated with\
          \ the use of CPU as well?  Thank you. </p>\n<p>In [20]: # cross-validate\
          \ gene classifier<br>    ...: all_roc_auc, roc_auc, roc_auc_sd, mean_fpr,\
          \ mean_tpr, confusion, label_dicts <br>    ...:     = cross_validate(subsampled_train_dataset,\
          \ targets, labels, nsplits, subsample_size, tr<br>    ...: aining_args,\
          \ freeze_layers, training_output_dir, 1)<br>0it [00:00, ?it/s]<br>******\
          \ Crossval split: 0/4 ******</p>\n<p>Filtering training data<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2fbdb7063800f6bd.arrow<br>Filtered\
          \ 50%; 14897 remain</p>\n<p>Filtering evalation data<br>Loading cached processed\
          \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-53fef5c1bf2aa95d.arrow<br>Filtered\
          \ 74%; 7860 remain</p>\n<p>Labeling training data<br>Loading cached processed\
          \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-04592d8012bd56bc.arrow<br>Labeling\
          \ evaluation data<br>Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-553f2b895bb22c3b.arrow<br>Labeling\
          \ evaluation OOS data<br>Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-5342eb45c1ad0501.arrow<br>Some\
          \ weights of the model checkpoint at ctheodoris/Geneformer were not used\
          \ when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight',\
          \ 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias',\
          \ 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight',\
          \ 'cls.predictions.decoder.bias']</p>\n<ul>\n<li>This IS expected if you\
          \ are initializing BertForTokenClassification from the checkpoint of a model\
          \ trained on another task or with another architecture (e.g. initializing\
          \ a BertForSequenceClassification model from a BertForPreTraining model).</li>\n\
          <li>This IS NOT expected if you are initializing BertForTokenClassification\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).<br>Some weights of BertForTokenClassification were not initialized\
          \ from the model checkpoint at ctheodoris/Geneformer and are newly initialized:\
          \ ['classifier.weight', 'classifier.bias']<br>You should probably TRAIN\
          \ this model on a down-stream task to be able to use it for predictions\
          \ and inference.<br>/home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/optimization.py:411:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set <code>no_deprecation_warning=True</code> to disable this\
          \ warning<br>warnings.warn(<br>                                        \
          \                                                     /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/geneformer/collator_for_classification.py:581:\
          \ UserWarning: To copy construct from a tensor, it is recommended to use\
          \ sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True),\
          \ rather than torch.tensor(sourceTensor).<br>batch = {k: torch.tensor(v,\
          \ dtype=torch.int64) for k, v in batch.items()}<br>{'loss': 0.7035, 'learning_rate':\
          \ 1e-05, 'epoch': 0.12}<br>{'loss': 0.6153, 'learning_rate': 2e-05, 'epoch':\
          \ 0.24}<br>{'loss': 0.5162, 'learning_rate': 3e-05, 'epoch': 0.36}<br>{'loss':\
          \ 0.3814, 'learning_rate': 4e-05, 'epoch': 0.48}<br>{'loss': 0.287, 'learning_rate':\
          \ 5e-05, 'epoch': 0.6}<br>{'loss': 0.2037, 'learning_rate': 3.502994011976048e-05,\
          \ 'epoch': 0.72}<br>{'loss': 0.1862, 'learning_rate': 2.0059880239520957e-05,\
          \ 'epoch': 0.84}<br>{'loss': 0.1583, 'learning_rate': 5.0898203592814375e-06,\
          \ 'epoch': 0.96}<br>{'train_runtime': 240.3808, 'train_samples_per_second':\
          \ 41.601, 'train_steps_per_second': 3.469, 'train_loss': 0.37374645285755037,\
          \ 'epoch': 1.0}<br>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 834/834 [04:00&lt;00:00,  3.47it/s]<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow<br>\
          \                                                                      \
          \                       ^[[B                                           \
          \                                                     ---------------------------------------------------------------------------<br>RuntimeError\
          \                              Traceback (most recent call last)<br>Cell\
          \ In[20], line 3<br>1 # cross-validate gene classifier<br>2 all_roc_auc,\
          \ roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts <br>----&gt;\
          \ 3     = cross_validate(subsampled_train_dataset, targets, labels, nsplits,\
          \ subsample_size, training_args, freeze_layers, training_output_dir, 1)</li>\n\
          </ul>\n<p>Cell In[14], line 128, in cross_validate(data, targets, labels,\
          \ nsplits, subsample_size, training_args, freeze_layers, output_dir, num_proc)<br>\
          \    125 trainer.save_model(ksplit_model_dir)<br>    127 # evaluate model<br>--&gt;\
          \ 128 fpr, tpr, interp_tpr, conf_mat = classifier_predict(trainer.model,\
          \ evalset_oos_labeled, 200, mean_fpr)<br>    130 # append to tpr and roc\
          \ lists<br>    131 confusion = confusion + conf_mat</p>\n<p>Cell In[13],\
          \ line 38, in classifier_predict(model, evalset, forward_batch_size, mean_fpr)<br>\
          \     35         predict_logits += [torch.squeeze(outputs.logits.to(\"cpu\"\
          ))]<br>     36         predict_labels += [torch.squeeze(label_batch.to(\"\
          cpu\"))]<br>---&gt; 38 logits_by_cell = torch.cat(predict_logits)<br>  \
          \   39 all_logits = logits_by_cell.reshape(-1, logits_by_cell.shape[2])<br>\
          \     40 labels_by_cell = torch.cat(predict_labels)</p>\n<p>RuntimeError:\
          \ Sizes of tensors must match except in dimension 0. Expected size 1062\
          \ but got size 1268 for tensor number 1 in the list.</p>\n"
        raw: "After a while, another error 'tensor size mismatch' occurred as below\
          \ with panglao_SRA553822-SRS2119548.dataset. Was it associated with the\
          \ use of CPU as well?  Thank you. \n\nIn [20]: # cross-validate gene classifier\n\
          \    ...: all_roc_auc, roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion,\
          \ label_dicts \\\n    ...:     = cross_validate(subsampled_train_dataset,\
          \ targets, labels, nsplits, subsample_size, tr\n    ...: aining_args, freeze_layers,\
          \ training_output_dir, 1)\n0it [00:00, ?it/s]\n****** Crossval split: 0/4\
          \ ******\n\nFiltering training data\nLoading cached processed dataset at\
          \ /mnt/c/Users/pc/Downloads/GS_example/cache-2fbdb7063800f6bd.arrow\nFiltered\
          \ 50%; 14897 remain\n\nFiltering evalation data\nLoading cached processed\
          \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-53fef5c1bf2aa95d.arrow\n\
          Filtered 74%; 7860 remain\n\nLabeling training data\nLoading cached processed\
          \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-04592d8012bd56bc.arrow\n\
          Labeling evaluation data\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-553f2b895bb22c3b.arrow\n\
          Labeling evaluation OOS data\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-5342eb45c1ad0501.arrow\n\
          Some weights of the model checkpoint at ctheodoris/Geneformer were not used\
          \ when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight',\
          \ 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias',\
          \ 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight',\
          \ 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing\
          \ BertForTokenClassification from the checkpoint of a model trained on another\
          \ task or with another architecture (e.g. initializing a BertForSequenceClassification\
          \ model from a BertForPreTraining model).\n- This IS NOT expected if you\
          \ are initializing BertForTokenClassification from the checkpoint of a model\
          \ that you expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification\
          \ were not initialized from the model checkpoint at ctheodoris/Geneformer\
          \ and are newly initialized: ['classifier.weight', 'classifier.bias']\n\
          You should probably TRAIN this model on a down-stream task to be able to\
          \ use it for predictions and inference.\n/home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/optimization.py:411:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\n\
          \  warnings.warn(\n                                                    \
          \                                               /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/geneformer/collator_for_classification.py:581:\
          \ UserWarning: To copy construct from a tensor, it is recommended to use\
          \ sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True),\
          \ rather than torch.tensor(sourceTensor).\n  batch = {k: torch.tensor(v,\
          \ dtype=torch.int64) for k, v in batch.items()}\n{'loss': 0.7035, 'learning_rate':\
          \ 1e-05, 'epoch': 0.12}\n{'loss': 0.6153, 'learning_rate': 2e-05, 'epoch':\
          \ 0.24}\n{'loss': 0.5162, 'learning_rate': 3e-05, 'epoch': 0.36}\n{'loss':\
          \ 0.3814, 'learning_rate': 4e-05, 'epoch': 0.48}\n{'loss': 0.287, 'learning_rate':\
          \ 5e-05, 'epoch': 0.6}\n{'loss': 0.2037, 'learning_rate': 3.502994011976048e-05,\
          \ 'epoch': 0.72}\n{'loss': 0.1862, 'learning_rate': 2.0059880239520957e-05,\
          \ 'epoch': 0.84}\n{'loss': 0.1583, 'learning_rate': 5.0898203592814375e-06,\
          \ 'epoch': 0.96}\n{'train_runtime': 240.3808, 'train_samples_per_second':\
          \ 41.601, 'train_steps_per_second': 3.469, 'train_loss': 0.37374645285755037,\
          \ 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 834/834 [04:00<00:00,  3.47it/s]\nLoading cached\
          \ processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow\n\
          \                                                                      \
          \                             ^[[B                                     \
          \                                                           ---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[20], line 3\n      1 # cross-validate gene classifier\n      2 all_roc_auc,\
          \ roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts \\\n---->\
          \ 3     = cross_validate(subsampled_train_dataset, targets, labels, nsplits,\
          \ subsample_size, training_args, freeze_layers, training_output_dir, 1)\n\
          \nCell In[14], line 128, in cross_validate(data, targets, labels, nsplits,\
          \ subsample_size, training_args, freeze_layers, output_dir, num_proc)\n\
          \    125 trainer.save_model(ksplit_model_dir)\n    127 # evaluate model\n\
          --> 128 fpr, tpr, interp_tpr, conf_mat = classifier_predict(trainer.model,\
          \ evalset_oos_labeled, 200, mean_fpr)\n    130 # append to tpr and roc lists\n\
          \    131 confusion = confusion + conf_mat\n\nCell In[13], line 38, in classifier_predict(model,\
          \ evalset, forward_batch_size, mean_fpr)\n     35         predict_logits\
          \ += [torch.squeeze(outputs.logits.to(\"cpu\"))]\n     36         predict_labels\
          \ += [torch.squeeze(label_batch.to(\"cpu\"))]\n---> 38 logits_by_cell =\
          \ torch.cat(predict_logits)\n     39 all_logits = logits_by_cell.reshape(-1,\
          \ logits_by_cell.shape[2])\n     40 labels_by_cell = torch.cat(predict_labels)\n\
          \nRuntimeError: Sizes of tensors must match except in dimension 0. Expected\
          \ size 1062 but got size 1268 for tensor number 1 in the list.\n"
        updatedAt: '2023-06-25T01:53:46.624Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64979e2a5478e697e616f279
    id: 64979e2a5478e697e616f277
    type: comment
  author: pchiang5
  content: "After a while, another error 'tensor size mismatch' occurred as below\
    \ with panglao_SRA553822-SRS2119548.dataset. Was it associated with the use of\
    \ CPU as well?  Thank you. \n\nIn [20]: # cross-validate gene classifier\n   \
    \ ...: all_roc_auc, roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts\
    \ \\\n    ...:     = cross_validate(subsampled_train_dataset, targets, labels,\
    \ nsplits, subsample_size, tr\n    ...: aining_args, freeze_layers, training_output_dir,\
    \ 1)\n0it [00:00, ?it/s]\n****** Crossval split: 0/4 ******\n\nFiltering training\
    \ data\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2fbdb7063800f6bd.arrow\n\
    Filtered 50%; 14897 remain\n\nFiltering evalation data\nLoading cached processed\
    \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-53fef5c1bf2aa95d.arrow\n\
    Filtered 74%; 7860 remain\n\nLabeling training data\nLoading cached processed\
    \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-04592d8012bd56bc.arrow\n\
    Labeling evaluation data\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-553f2b895bb22c3b.arrow\n\
    Labeling evaluation OOS data\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-5342eb45c1ad0501.arrow\n\
    Some weights of the model checkpoint at ctheodoris/Geneformer were not used when\
    \ initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight',\
    \ 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias',\
    \ 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight',\
    \ 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing\
    \ BertForTokenClassification from the checkpoint of a model trained on another\
    \ task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ BertForTokenClassification from the checkpoint of a model that you expect to\
    \ be exactly identical (initializing a BertForSequenceClassification model from\
    \ a BertForSequenceClassification model).\nSome weights of BertForTokenClassification\
    \ were not initialized from the model checkpoint at ctheodoris/Geneformer and\
    \ are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should\
    \ probably TRAIN this model on a down-stream task to be able to use it for predictions\
    \ and inference.\n/home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/optimization.py:411:\
    \ FutureWarning: This implementation of AdamW is deprecated and will be removed\
    \ in a future version. Use the PyTorch implementation torch.optim.AdamW instead,\
    \ or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\
    \                                                                            \
    \                       /home/pc/miniconda3/envs/geneformer/lib/python3.10/site-packages/geneformer/collator_for_classification.py:581:\
    \ UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach()\
    \ or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\
    \  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n\
    {'loss': 0.7035, 'learning_rate': 1e-05, 'epoch': 0.12}\n{'loss': 0.6153, 'learning_rate':\
    \ 2e-05, 'epoch': 0.24}\n{'loss': 0.5162, 'learning_rate': 3e-05, 'epoch': 0.36}\n\
    {'loss': 0.3814, 'learning_rate': 4e-05, 'epoch': 0.48}\n{'loss': 0.287, 'learning_rate':\
    \ 5e-05, 'epoch': 0.6}\n{'loss': 0.2037, 'learning_rate': 3.502994011976048e-05,\
    \ 'epoch': 0.72}\n{'loss': 0.1862, 'learning_rate': 2.0059880239520957e-05, 'epoch':\
    \ 0.84}\n{'loss': 0.1583, 'learning_rate': 5.0898203592814375e-06, 'epoch': 0.96}\n\
    {'train_runtime': 240.3808, 'train_samples_per_second': 41.601, 'train_steps_per_second':\
    \ 3.469, 'train_loss': 0.37374645285755037, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 834/834 [04:00<00:00,  3.47it/s]\nLoading\
    \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow\n\
    \                                                                            \
    \                       ^[[B                                                 \
    \                                               ---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Cell In[20], line 3\n      1 # cross-validate gene classifier\n      2 all_roc_auc,\
    \ roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts \\\n----> 3\
    \     = cross_validate(subsampled_train_dataset, targets, labels, nsplits, subsample_size,\
    \ training_args, freeze_layers, training_output_dir, 1)\n\nCell In[14], line 128,\
    \ in cross_validate(data, targets, labels, nsplits, subsample_size, training_args,\
    \ freeze_layers, output_dir, num_proc)\n    125 trainer.save_model(ksplit_model_dir)\n\
    \    127 # evaluate model\n--> 128 fpr, tpr, interp_tpr, conf_mat = classifier_predict(trainer.model,\
    \ evalset_oos_labeled, 200, mean_fpr)\n    130 # append to tpr and roc lists\n\
    \    131 confusion = confusion + conf_mat\n\nCell In[13], line 38, in classifier_predict(model,\
    \ evalset, forward_batch_size, mean_fpr)\n     35         predict_logits += [torch.squeeze(outputs.logits.to(\"\
    cpu\"))]\n     36         predict_labels += [torch.squeeze(label_batch.to(\"cpu\"\
    ))]\n---> 38 logits_by_cell = torch.cat(predict_logits)\n     39 all_logits =\
    \ logits_by_cell.reshape(-1, logits_by_cell.shape[2])\n     40 labels_by_cell\
    \ = torch.cat(predict_labels)\n\nRuntimeError: Sizes of tensors must match except\
    \ in dimension 0. Expected size 1062 but got size 1268 for tensor number 1 in\
    \ the list.\n"
  created_at: 2023-06-25 00:53:46+00:00
  edited: false
  hidden: false
  id: 64979e2a5478e697e616f277
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-25T01:53:46.000Z'
    data:
      status: open
    id: 64979e2a5478e697e616f279
    type: status-change
  author: pchiang5
  created_at: 2023-06-25 00:53:46+00:00
  id: 64979e2a5478e697e616f279
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-25T04:58:41.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802653789520264
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. This appears similar to closed issue
          <a href="/ctheodoris/Geneformer/discussions/31">#31</a>. Have you pulled
          the updated version? If not, please try that.</p>

          '
        raw: 'Thank you for following up. This appears similar to closed issue #31.
          Have you pulled the updated version? If not, please try that.'
        updatedAt: '2023-06-25T04:58:41.628Z'
      numEdits: 0
      reactions: []
    id: 6497c98161ec06aa7a39e088
    type: comment
  author: ctheodoris
  content: 'Thank you for following up. This appears similar to closed issue #31.
    Have you pulled the updated version? If not, please try that.'
  created_at: 2023-06-25 03:58:41+00:00
  edited: false
  hidden: false
  id: 6497c98161ec06aa7a39e088
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-26T04:10:55.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4015900492668152
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>Thank you for the suggestion. </p>\n<p>I tried the updated version,\
          \ but a similar error occurred below:</p>\n<p>mnt/c/Users/pc/Downloads/geneformer/geneformer/collator_for_classification.py:581:\
          \ UserWarning: To copy construct from a tensor, it is recommended to use\
          \ sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True),\
          \ rather than torch.tensor(sourceTensor).<br>  batch = {k: torch.tensor(v,\
          \ dtype=torch.int64) for k, v in batch.items()}<br>{'loss': 0.6515, 'learning_rate':\
          \ 1e-05, 'epoch': 0.12}<br>{'loss': 0.5868, 'learning_rate': 2e-05, 'epoch':\
          \ 0.24}<br>{'loss': 0.4944, 'learning_rate': 3e-05, 'epoch': 0.36}<br>{'loss':\
          \ 0.3692, 'learning_rate': 4e-05, 'epoch': 0.48}<br>{'loss': 0.2849, 'learning_rate':\
          \ 5e-05, 'epoch': 0.6}<br>{'loss': 0.2078, 'learning_rate': 3.502994011976048e-05,\
          \ 'epoch': 0.72}<br>{'loss': 0.1842, 'learning_rate': 2.0059880239520957e-05,\
          \ 'epoch': 0.84}<br>{'loss': 0.1596, 'learning_rate': 5.0898203592814375e-06,\
          \ 'epoch': 0.96}<br>{'train_runtime': 257.4132, 'train_samples_per_second':\
          \ 38.848, 'train_steps_per_second': 3.24, 'train_loss': 0.3598546204235342,\
          \ 'epoch': 1.0}<br>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 834/834 [04:17&lt;00:00,  3.24it/s]<br>Loading cached processed\
          \ dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow7&lt;00:00,\
          \  9.90it/s]<br>Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c02fb696b3d5e061.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d74bd43d854bd585.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ef0a8d586fb36dfe.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2299ba01f3f3324a.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ec26febb779ef58e.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d9ba779b30352d3b.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-1cae902e9833d530.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-eef97f1e3ce741ff.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-e0aa69b5c969bbb0.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c79eff583fd31baa.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d98e737501b6e111.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-a07fe905fe45fec6.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-4830ad31848f15f1.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-f30f4f93e63a369f.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-45377aad03d16995.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2ecda395f441d64e.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-829ae39672d6280d.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-bfe0c60751641870.arrow<br>Loading\
          \ cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-13e2b37bb38c0512.arrow<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in :3 \
          \                                                                      \
          \             \u2502<br>\u2502 in cross_validate:128                   \
          \                                                         \u2502<br>\u2502\
          \ in classifier_predict:38                                             \
          \                            \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>RuntimeError:\
          \ Sizes of tensors must match except in dimension 0. Expected size 1062\
          \ but got size 1268 for tensor number<br>1 in the list.</p>\n"
        raw: "Thank you for the suggestion. \n\nI tried the updated version, but a\
          \ similar error occurred below:\n\nmnt/c/Users/pc/Downloads/geneformer/geneformer/collator_for_classification.py:581:\
          \ UserWarning: To copy construct from a tensor, it is recommended to use\
          \ sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True),\
          \ rather than torch.tensor(sourceTensor).\n  batch = {k: torch.tensor(v,\
          \ dtype=torch.int64) for k, v in batch.items()}\n{'loss': 0.6515, 'learning_rate':\
          \ 1e-05, 'epoch': 0.12}\n{'loss': 0.5868, 'learning_rate': 2e-05, 'epoch':\
          \ 0.24}\n{'loss': 0.4944, 'learning_rate': 3e-05, 'epoch': 0.36}\n{'loss':\
          \ 0.3692, 'learning_rate': 4e-05, 'epoch': 0.48}\n{'loss': 0.2849, 'learning_rate':\
          \ 5e-05, 'epoch': 0.6}\n{'loss': 0.2078, 'learning_rate': 3.502994011976048e-05,\
          \ 'epoch': 0.72}\n{'loss': 0.1842, 'learning_rate': 2.0059880239520957e-05,\
          \ 'epoch': 0.84}\n{'loss': 0.1596, 'learning_rate': 5.0898203592814375e-06,\
          \ 'epoch': 0.96}\n{'train_runtime': 257.4132, 'train_samples_per_second':\
          \ 38.848, 'train_steps_per_second': 3.24, 'train_loss': 0.3598546204235342,\
          \ 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 834/834 [04:17<00:00,  3.24it/s]\nLoading cached processed dataset at\
          \ /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow7<00:00,\
          \  9.90it/s]\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c02fb696b3d5e061.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d74bd43d854bd585.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ef0a8d586fb36dfe.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2299ba01f3f3324a.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ec26febb779ef58e.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d9ba779b30352d3b.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-1cae902e9833d530.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-eef97f1e3ce741ff.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-e0aa69b5c969bbb0.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c79eff583fd31baa.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d98e737501b6e111.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-a07fe905fe45fec6.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-4830ad31848f15f1.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-f30f4f93e63a369f.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-45377aad03d16995.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2ecda395f441d64e.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-829ae39672d6280d.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-bfe0c60751641870.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-13e2b37bb38c0512.arrow\n\
          \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in <module>:3                                                        \
          \                            \u2502\n\u2502 in cross_validate:128      \
          \                                                                      \u2502\
          \n\u2502 in classifier_predict:38                                      \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nRuntimeError:\
          \ Sizes of tensors must match except in dimension 0. Expected size 1062\
          \ but got size 1268 for tensor number\n1 in the list."
        updatedAt: '2023-06-26T04:10:55.768Z'
      numEdits: 0
      reactions: []
    id: 64990fcf0651355205f4a676
    type: comment
  author: pchiang5
  content: "Thank you for the suggestion. \n\nI tried the updated version, but a similar\
    \ error occurred below:\n\nmnt/c/Users/pc/Downloads/geneformer/geneformer/collator_for_classification.py:581:\
    \ UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach()\
    \ or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\
    \  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n\
    {'loss': 0.6515, 'learning_rate': 1e-05, 'epoch': 0.12}\n{'loss': 0.5868, 'learning_rate':\
    \ 2e-05, 'epoch': 0.24}\n{'loss': 0.4944, 'learning_rate': 3e-05, 'epoch': 0.36}\n\
    {'loss': 0.3692, 'learning_rate': 4e-05, 'epoch': 0.48}\n{'loss': 0.2849, 'learning_rate':\
    \ 5e-05, 'epoch': 0.6}\n{'loss': 0.2078, 'learning_rate': 3.502994011976048e-05,\
    \ 'epoch': 0.72}\n{'loss': 0.1842, 'learning_rate': 2.0059880239520957e-05, 'epoch':\
    \ 0.84}\n{'loss': 0.1596, 'learning_rate': 5.0898203592814375e-06, 'epoch': 0.96}\n\
    {'train_runtime': 257.4132, 'train_samples_per_second': 38.848, 'train_steps_per_second':\
    \ 3.24, 'train_loss': 0.3598546204235342, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 834/834 [04:17<00:00,  3.24it/s]\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-b509cef10cae8da3.arrow7<00:00,\
    \  9.90it/s]\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c02fb696b3d5e061.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d74bd43d854bd585.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ef0a8d586fb36dfe.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2299ba01f3f3324a.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-ec26febb779ef58e.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d9ba779b30352d3b.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-1cae902e9833d530.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-eef97f1e3ce741ff.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-e0aa69b5c969bbb0.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-c79eff583fd31baa.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-d98e737501b6e111.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-a07fe905fe45fec6.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-4830ad31848f15f1.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-f30f4f93e63a369f.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-45377aad03d16995.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-2ecda395f441d64e.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-829ae39672d6280d.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-bfe0c60751641870.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GS_example/cache-13e2b37bb38c0512.arrow\n\
    \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u256E\n\u2502 in <module>:3                         \
    \                                                           \u2502\n\u2502 in\
    \ cross_validate:128                                                         \
    \                   \u2502\n\u2502 in classifier_predict:38                  \
    \                                                       \u2502\n\u2570\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256F\nRuntimeError: Sizes of tensors must match\
    \ except in dimension 0. Expected size 1062 but got size 1268 for tensor number\n\
    1 in the list."
  created_at: 2023-06-26 03:10:55+00:00
  edited: false
  hidden: false
  id: 64990fcf0651355205f4a676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-26T06:25:06.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9151921272277832
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for following up. Could you please run a diff between
          your notebook and the current one in this repository to confirm it is up
          to date?</p>

          <p>For example, in your notebook, is your variable padded_batch defined
          as follows?:<br>padded_batch = preprocess_classifier_batch(batch_evalset,
          max_evalset_len)</p>

          <p>If it''s up to date, please provide the information of anything you changed
          in the notebook so I can try to reproduce the error because I am not encountering
          this error when I run the current notebook.</p>

          '
        raw: 'Thank you for following up. Could you please run a diff between your
          notebook and the current one in this repository to confirm it is up to date?


          For example, in your notebook, is your variable padded_batch defined as
          follows?:

          padded_batch = preprocess_classifier_batch(batch_evalset, max_evalset_len)


          If it''s up to date, please provide the information of anything you changed
          in the notebook so I can try to reproduce the error because I am not encountering
          this error when I run the current notebook.'
        updatedAt: '2023-06-26T06:25:06.564Z'
      numEdits: 0
      reactions: []
    id: 64992f42d8b0e7ce8ae71175
    type: comment
  author: ctheodoris
  content: 'Thank you for following up. Could you please run a diff between your notebook
    and the current one in this repository to confirm it is up to date?


    For example, in your notebook, is your variable padded_batch defined as follows?:

    padded_batch = preprocess_classifier_batch(batch_evalset, max_evalset_len)


    If it''s up to date, please provide the information of anything you changed in
    the notebook so I can try to reproduce the error because I am not encountering
    this error when I run the current notebook.'
  created_at: 2023-06-26 05:25:06+00:00
  edited: false
  hidden: false
  id: 64992f42d8b0e7ce8ae71175
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-26T08:46:52.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9756376147270203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: '<p>Thank you for your reminder. You are right. I just updated the package
          but forgot to do the notebook as well. Now it worked. </p>

          '
        raw: 'Thank you for your reminder. You are right. I just updated the package
          but forgot to do the notebook as well. Now it worked. '
        updatedAt: '2023-06-26T08:46:52.854Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6499507c7ac1bf03e2226239
    id: 6499507c7ac1bf03e2226237
    type: comment
  author: pchiang5
  content: 'Thank you for your reminder. You are right. I just updated the package
    but forgot to do the notebook as well. Now it worked. '
  created_at: 2023-06-26 07:46:52+00:00
  edited: false
  hidden: false
  id: 6499507c7ac1bf03e2226237
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-26T08:46:52.000Z'
    data:
      status: closed
    id: 6499507c7ac1bf03e2226239
    type: status-change
  author: pchiang5
  created_at: 2023-06-26 07:46:52+00:00
  id: 6499507c7ac1bf03e2226239
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 60
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: 'gene_classification.ipynb: error with no_cuda = True'
