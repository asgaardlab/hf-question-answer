!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abayegan
conflicting_files: null
created_at: 2023-07-11 14:01:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4447541e01b9f8594991e40972ba08ac.svg
      fullname: Amir Bayegan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abayegan
      type: user
    createdAt: '2023-07-11T15:01:35.000Z'
    data:
      edited: false
      editors:
      - abayegan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8642809391021729
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4447541e01b9f8594991e40972ba08ac.svg
          fullname: Amir Bayegan
          isHf: false
          isPro: false
          name: abayegan
          type: user
        html: '<p>Hi, great work! Do you have any suggestions for updating the pre-trained
          model without a down-stream task? Would it be legit to use <code>BertForMaskedLM.from_pretrained(&lt;path
          to your pretrained model&gt;)</code> and continue training it on a new dataset?
          Which datacollator should I use?</p>

          '
        raw: Hi, great work! Do you have any suggestions for updating the pre-trained
          model without a down-stream task? Would it be legit to use `BertForMaskedLM.from_pretrained(<path
          to your pretrained model>)` and continue training it on a new dataset? Which
          datacollator should I use?
        updatedAt: '2023-07-11T15:01:35.781Z'
      numEdits: 0
      reactions: []
    id: 64ad6ecf5c6fc404f0eb40b3
    type: comment
  author: abayegan
  content: Hi, great work! Do you have any suggestions for updating the pre-trained
    model without a down-stream task? Would it be legit to use `BertForMaskedLM.from_pretrained(<path
    to your pretrained model>)` and continue training it on a new dataset? Which datacollator
    should I use?
  created_at: 2023-07-11 14:01:35+00:00
  edited: false
  hidden: false
  id: 64ad6ecf5c6fc404f0eb40b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-11T18:49:29.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7888941168785095
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! Yes, I would suggest
          following the example script for pretraining, but substituting "model =
          BertForMaskedLM(config)" with "model = BertForMaskedLM.from_pretrained(/path/to/pretrained_model/)"</p>

          '
        raw: Thank you for your interest in Geneformer! Yes, I would suggest following
          the example script for pretraining, but substituting "model = BertForMaskedLM(config)"
          with "model = BertForMaskedLM.from_pretrained(/path/to/pretrained_model/)"
        updatedAt: '2023-07-11T18:49:42.426Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64ada4396e9a4384cf936c04
    id: 64ada4396e9a4384cf936c02
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! Yes, I would suggest following
    the example script for pretraining, but substituting "model = BertForMaskedLM(config)"
    with "model = BertForMaskedLM.from_pretrained(/path/to/pretrained_model/)"
  created_at: 2023-07-11 17:49:29+00:00
  edited: true
  hidden: false
  id: 64ada4396e9a4384cf936c02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-11T18:49:29.000Z'
    data:
      status: closed
    id: 64ada4396e9a4384cf936c04
    type: status-change
  author: ctheodoris
  created_at: 2023-07-11 17:49:29+00:00
  id: 64ada4396e9a4384cf936c04
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-07-16T02:18:44.000Z'
    data:
      edited: true
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4520174264907837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>Thank you for the suggestion about this. I encountered the <code>invalid\
          \ key' error</code> when I substituted </p>\n<pre><code>config = {\n   \
          \ \"hidden_size\": num_embed_dim,\n    \"num_hidden_layers\": num_layers,\n\
          \    \"initializer_range\": initializer_range,\n    \"layer_norm_eps\":\
          \ layer_norm_eps,\n    \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n\
          \    \"hidden_dropout_prob\": hidden_dropout_prob,\n    \"intermediate_size\"\
          : intermed_size,\n    \"hidden_act\": activ_fn,\n    \"max_position_embeddings\"\
          : max_input_size,\n    \"model_type\": model_type,\n    \"num_attention_heads\"\
          : num_attn_heads,\n    \"pad_token_id\": token_dictionary.get(\"&lt;pad&gt;\"\
          ),\n    \"vocab_size\": len(token_dictionary),  # genes+2 for &lt;mask&gt;\
          \ and &lt;pad&gt; tokens\n}\n\nconfig = BertConfig(**config)\nmodel = BertForMaskedLM(config)\n\
          </code></pre>\n<p>to </p>\n<pre><code>model = BertForMaskedLM.from_pretrained(\"\
          /mnt/c/Users/pc/Downloads/Geneformer/geneformer-12L-30M\", \n          \
          \                                                output_attentions = False,\n\
          \                                                          output_hidden_states\
          \ = False) \n</code></pre>\n<p>It seemed the 25196624 were the cells you\
          \ used for the pre-trained model, and the 53950 were the cells I used for\
          \ further fine-tuning. I found the problem might be the use of improper\
          \ lengths_file (e.g. <a href=\"https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/genecorpus_30M_2048_lengths.pkl\"\
          >https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/genecorpus_30M_2048_lengths.pkl</a>).\
          \  Could you kindly refer me to the way to generate my own lengths_file.pkl?\
          \ Thanks.</p>\n<p>error:</p>\n<pre><code>DESKTOP-6FHRRIO:5553:5700 [0] NCCL\
          \ INFO comm 0x55c7ab4ecae0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE\n\
          \  0%|                                                                 \
          \                      | 0/6851556 [00:00&lt;?, ?it/s]Traceback (most recent\
          \ call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
          , line 168, in &lt;module&gt;\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          /home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2796, in __getitems__\n    batch = self.__getitem__(keys)\n     \
          \       ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2792, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2776, in _getitem\n    pa_subtable = query_table(self._data, key,\
          \ indices=self._indices if self._indices is not None else None)\n      \
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 583, in query_table\n    _check_valid_index_key(key, size)\n  File\
          \ \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)),\
          \ size=size)\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key:\
          \ {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 25196624\
          \ is out of bounds for size 53950\n  0%|                               \
          \                                                        | 0/6851556 [00:18&lt;?,\
          \ ?it/s]\n</code></pre>\n"
        raw: "Thank you for the suggestion about this. I encountered the `invalid\
          \ key' error` when I substituted \n```\nconfig = {\n    \"hidden_size\"\
          : num_embed_dim,\n    \"num_hidden_layers\": num_layers,\n    \"initializer_range\"\
          : initializer_range,\n    \"layer_norm_eps\": layer_norm_eps,\n    \"attention_probs_dropout_prob\"\
          : attention_probs_dropout_prob,\n    \"hidden_dropout_prob\": hidden_dropout_prob,\n\
          \    \"intermediate_size\": intermed_size,\n    \"hidden_act\": activ_fn,\n\
          \    \"max_position_embeddings\": max_input_size,\n    \"model_type\": model_type,\n\
          \    \"num_attention_heads\": num_attn_heads,\n    \"pad_token_id\": token_dictionary.get(\"\
          <pad>\"),\n    \"vocab_size\": len(token_dictionary),  # genes+2 for <mask>\
          \ and <pad> tokens\n}\n\nconfig = BertConfig(**config)\nmodel = BertForMaskedLM(config)\n\
          ```\nto \n```\nmodel = BertForMaskedLM.from_pretrained(\"/mnt/c/Users/pc/Downloads/Geneformer/geneformer-12L-30M\"\
          , \n                                                          output_attentions\
          \ = False,\n                                                          output_hidden_states\
          \ = False) \n```\n\nIt seemed the 25196624 were the cells you used for the\
          \ pre-trained model, and the 53950 were the cells I used for further fine-tuning.\
          \ I found the problem might be the use of improper lengths_file (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/genecorpus_30M_2048_lengths.pkl).\
          \  Could you kindly refer me to the way to generate my own lengths_file.pkl?\
          \ Thanks.\n\n\n\nerror:\n```\nDESKTOP-6FHRRIO:5553:5700 [0] NCCL INFO comm\
          \ 0x55c7ab4ecae0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE\n\
          \  0%|                                                                 \
          \                      | 0/6851556 [00:00<?, ?it/s]Traceback (most recent\
          \ call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
          , line 168, in <module>\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          /home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2796, in __getitems__\n    batch = self.__getitem__(keys)\n     \
          \       ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2792, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
          , line 2776, in _getitem\n    pa_subtable = query_table(self._data, key,\
          \ indices=self._indices if self._indices is not None else None)\n      \
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 583, in query_table\n    _check_valid_index_key(key, size)\n  File\
          \ \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)),\
          \ size=size)\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
          , line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key:\
          \ {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 25196624\
          \ is out of bounds for size 53950\n  0%|                               \
          \                                                        | 0/6851556 [00:18<?,\
          \ ?it/s]\n```\n\n"
        updatedAt: '2023-07-16T02:57:24.979Z'
      numEdits: 1
      reactions: []
    id: 64b3538461fd81bcefad33ec
    type: comment
  author: pchiang5
  content: "Thank you for the suggestion about this. I encountered the `invalid key'\
    \ error` when I substituted \n```\nconfig = {\n    \"hidden_size\": num_embed_dim,\n\
    \    \"num_hidden_layers\": num_layers,\n    \"initializer_range\": initializer_range,\n\
    \    \"layer_norm_eps\": layer_norm_eps,\n    \"attention_probs_dropout_prob\"\
    : attention_probs_dropout_prob,\n    \"hidden_dropout_prob\": hidden_dropout_prob,\n\
    \    \"intermediate_size\": intermed_size,\n    \"hidden_act\": activ_fn,\n  \
    \  \"max_position_embeddings\": max_input_size,\n    \"model_type\": model_type,\n\
    \    \"num_attention_heads\": num_attn_heads,\n    \"pad_token_id\": token_dictionary.get(\"\
    <pad>\"),\n    \"vocab_size\": len(token_dictionary),  # genes+2 for <mask> and\
    \ <pad> tokens\n}\n\nconfig = BertConfig(**config)\nmodel = BertForMaskedLM(config)\n\
    ```\nto \n```\nmodel = BertForMaskedLM.from_pretrained(\"/mnt/c/Users/pc/Downloads/Geneformer/geneformer-12L-30M\"\
    , \n                                                          output_attentions\
    \ = False,\n                                                          output_hidden_states\
    \ = False) \n```\n\nIt seemed the 25196624 were the cells you used for the pre-trained\
    \ model, and the 53950 were the cells I used for further fine-tuning. I found\
    \ the problem might be the use of improper lengths_file (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/genecorpus_30M_2048_lengths.pkl).\
    \  Could you kindly refer me to the way to generate my own lengths_file.pkl? Thanks.\n\
    \n\n\nerror:\n```\nDESKTOP-6FHRRIO:5553:5700 [0] NCCL INFO comm 0x55c7ab4ecae0\
    \ rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE\n  0%|                \
    \                                                                       | 0/6851556\
    \ [00:00<?, ?it/s]Traceback (most recent call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
    , line 168, in <module>\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
    , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
    , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may\
    \ raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    /home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
    , line 2796, in __getitems__\n    batch = self.__getitem__(keys)\n           \
    \ ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
    , line 2792, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/arrow_dataset.py\"\
    , line 2776, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices\
    \ if self._indices is not None else None)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
    , line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
    , line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)),\
    \ size=size)\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/datasets/formatting/formatting.py\"\
    , line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key}\
    \ is out of bounds for size {size}\")\nIndexError: Invalid key: 25196624 is out\
    \ of bounds for size 53950\n  0%|                                            \
    \                                           | 0/6851556 [00:18<?, ?it/s]\n```\n\
    \n"
  created_at: 2023-07-16 01:18:44+00:00
  edited: true
  hidden: false
  id: 64b3538461fd81bcefad33ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-16T12:31:48.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7500646710395813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The lengths file is the list of lengths
          of each rank value encoding in the pretraining corpus. The transcriptomic
          tokenizer already maps the lengths so they should be in a column in the
          resulting .dataset. They can be extracted with:<br>dataset_lengths = dataset["length"]</p>

          <p>Please also see the relevant closed discussion here: <a href="https://huggingface.co/ctheodoris/Geneformer/discussions/61">https://huggingface.co/ctheodoris/Geneformer/discussions/61</a></p>

          <p>If you did not use the transcriptome tokenizer, you can map the lengths
          as shown in the tokenizer:</p>

          <p>def measure_length(example):<br>    example["length"] = len(example["input_ids"])<br>    return
          example</p>

          <p>dataset = dataset.map(measure_length, num_proc=nproc)</p>

          '
        raw: "Thank you for your question. The lengths file is the list of lengths\
          \ of each rank value encoding in the pretraining corpus. The transcriptomic\
          \ tokenizer already maps the lengths so they should be in a column in the\
          \ resulting .dataset. They can be extracted with:\ndataset_lengths = dataset[\"\
          length\"]\n\nPlease also see the relevant closed discussion here: https://huggingface.co/ctheodoris/Geneformer/discussions/61\n\
          \nIf you did not use the transcriptome tokenizer, you can map the lengths\
          \ as shown in the tokenizer:\n\ndef measure_length(example):\n    example[\"\
          length\"] = len(example[\"input_ids\"])\n    return example\n\ndataset =\
          \ dataset.map(measure_length, num_proc=nproc)\n"
        updatedAt: '2023-07-16T12:31:48.000Z'
      numEdits: 0
      reactions: []
    id: 64b3e3349a88b423da735254
    type: comment
  author: ctheodoris
  content: "Thank you for your question. The lengths file is the list of lengths of\
    \ each rank value encoding in the pretraining corpus. The transcriptomic tokenizer\
    \ already maps the lengths so they should be in a column in the resulting .dataset.\
    \ They can be extracted with:\ndataset_lengths = dataset[\"length\"]\n\nPlease\
    \ also see the relevant closed discussion here: https://huggingface.co/ctheodoris/Geneformer/discussions/61\n\
    \nIf you did not use the transcriptome tokenizer, you can map the lengths as shown\
    \ in the tokenizer:\n\ndef measure_length(example):\n    example[\"length\"] =\
    \ len(example[\"input_ids\"])\n    return example\n\ndataset = dataset.map(measure_length,\
    \ num_proc=nproc)\n"
  created_at: 2023-07-16 11:31:48+00:00
  edited: false
  hidden: false
  id: 64b3e3349a88b423da735254
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-07-17T01:23:56.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5034752488136292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: "<p>Thank you for your answer. I successfully get the length file into\
          \ pkl with the following code</p>\n<pre><code>data = train_dataset['length']\n\
          file_path = '/mnt/c/Users/pc/Downloads/train_dataset_length.pkl'\n\n# Save\
          \ data as a pickle file\nwith open(file_path, 'wb') as file:\n    pickle.dump(data,\
          \ file)\n\nprint(\"Data saved as pickle file:\", file_path)\n</code></pre>\n\
          <p>However, another error occurred below:</p>\n<pre><code>DESKTOP-6FHRRIO:4363:4517\
          \ [0] NCCL INFO comm 0x555c70f150e0 rank 0 nranks 1 cudaDev 0 busId 3000\
          \ - Init COMPLETE\n  0%|                                               \
          \                                          | 0/13488 [00:00&lt;?, ?it/s]Traceback\
          \ (most recent call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
          , line 192, in &lt;module&gt;\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
          , line 45, in __call__\n    return self.torch_call(features)\n         \
          \  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
          , line 732, in torch_call\n    batch = self.tokenizer.pad(examples, return_tensors=\"\
          pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
          , line 397, in pad\n    padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n\
          \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
          , line 250, in _get_padding_truncation_strategies\n    not self.pad_token\
          \ or self.pad_token_id &lt; 0\n                          ^^^^^^^^^^^^^^^^^^^^^\n\
          TypeError: '&lt;' not supported between instances of 'NoneType' and 'int'\n\
          \  0%|                                                                 \
          \                        | 0/13488 [00:00&lt;?, ?it/s]\nDESKTOP-6FHRRIO:4363:4518\
          \ [0] NCCL INFO [Service thread] Connection closed by localRank 0\nDESKTOP-6FHRRIO:4363:4363\
          \ [0] NCCL INFO comm 0x555c70f150e0 rank 0 nranks 1 cudaDev 0 busId 3000\
          \ - Abort COMPLETE\n[2023-07-17 09:13:53,853] [INFO] [launch.py:315:sigkill_handler]\
          \ Killing subprocess 4363\n[2023-07-17 09:13:53,853] [ERROR] [launch.py:321:sigkill_handler]\
          \ ['/home/pc/miniconda3/envs/Transformers/bin/python3.11', '-u', '/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py',\
          \ '--local_rank=0', '--deepspeed', '/home/pc/transformers/tests/deepspeed/ds_config_zero3.json']\
          \ exits with return code = 1\n</code></pre>\n<p>It was due to line 250 <code>not\
          \ self.pad_token or self.pad_token_id &lt; 0</code> in the pretrainer.py.\
          \  Was that due to something wrong with the token dictionary, the original\
          \ pretrained model, or my dataset(Below is the head of the token_dictionary)?\
          \ </p>\n<pre><code>In [5]: token_dictionary\nOut[5]:\n{0: '&lt;pad&gt;',\n\
          \ 1: '&lt;mask&gt;',\n 2: 'ENSG00000000003',\n 3: 'ENSG00000000005',\n 4:\
          \ 'ENSG00000000419',\n 5: 'ENSG00000000457',\n 6: 'ENSG00000000460',\n 7:\
          \ 'ENSG00000000938',\n 8: 'ENSG00000000971',\n 9: 'ENSG00000001036',\n 10:\
          \ 'ENSG00000001084',\n 11: 'ENSG00000001167',\n 12: 'ENSG00000001460',\n\
          \ 13: 'ENSG00000001461',\n 14: 'ENSG00000001497',\n 15: 'ENSG00000001561',\n\
          \ 16: 'ENSG00000001617',\n 17: 'ENSG00000001626',\n 18: 'ENSG00000001629',\n\
          \ 19: 'ENSG00000001630',\n 20: 'ENSG00000001631',\n 21: 'ENSG00000002016',\n\
          .\n.\n</code></pre>\n"
        raw: "Thank you for your answer. I successfully get the length file into pkl\
          \ with the following code\n```\ndata = train_dataset['length']\nfile_path\
          \ = '/mnt/c/Users/pc/Downloads/train_dataset_length.pkl'\n\n# Save data\
          \ as a pickle file\nwith open(file_path, 'wb') as file:\n    pickle.dump(data,\
          \ file)\n\nprint(\"Data saved as pickle file:\", file_path)\n```\n\nHowever,\
          \ another error occurred below:\n```\nDESKTOP-6FHRRIO:4363:4517 [0] NCCL\
          \ INFO comm 0x555c70f150e0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE\n\
          \  0%|                                                                 \
          \                        | 0/13488 [00:00<?, ?it/s]Traceback (most recent\
          \ call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
          , line 192, in <module>\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
          , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
          , line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
          , line 45, in __call__\n    return self.torch_call(features)\n         \
          \  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
          , line 732, in torch_call\n    batch = self.tokenizer.pad(examples, return_tensors=\"\
          pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
          , line 397, in pad\n    padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n\
          \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
          , line 250, in _get_padding_truncation_strategies\n    not self.pad_token\
          \ or self.pad_token_id < 0\n                          ^^^^^^^^^^^^^^^^^^^^^\n\
          TypeError: '<' not supported between instances of 'NoneType' and 'int'\n\
          \  0%|                                                                 \
          \                        | 0/13488 [00:00<?, ?it/s]\nDESKTOP-6FHRRIO:4363:4518\
          \ [0] NCCL INFO [Service thread] Connection closed by localRank 0\nDESKTOP-6FHRRIO:4363:4363\
          \ [0] NCCL INFO comm 0x555c70f150e0 rank 0 nranks 1 cudaDev 0 busId 3000\
          \ - Abort COMPLETE\n[2023-07-17 09:13:53,853] [INFO] [launch.py:315:sigkill_handler]\
          \ Killing subprocess 4363\n[2023-07-17 09:13:53,853] [ERROR] [launch.py:321:sigkill_handler]\
          \ ['/home/pc/miniconda3/envs/Transformers/bin/python3.11', '-u', '/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py',\
          \ '--local_rank=0', '--deepspeed', '/home/pc/transformers/tests/deepspeed/ds_config_zero3.json']\
          \ exits with return code = 1\n```\n\nIt was due to line 250 `not self.pad_token\
          \ or self.pad_token_id < 0` in the pretrainer.py.  Was that due to something\
          \ wrong with the token dictionary, the original pretrained model, or my\
          \ dataset(Below is the head of the token_dictionary)? \n```\nIn [5]: token_dictionary\n\
          Out[5]:\n{0: '<pad>',\n 1: '<mask>',\n 2: 'ENSG00000000003',\n 3: 'ENSG00000000005',\n\
          \ 4: 'ENSG00000000419',\n 5: 'ENSG00000000457',\n 6: 'ENSG00000000460',\n\
          \ 7: 'ENSG00000000938',\n 8: 'ENSG00000000971',\n 9: 'ENSG00000001036',\n\
          \ 10: 'ENSG00000001084',\n 11: 'ENSG00000001167',\n 12: 'ENSG00000001460',\n\
          \ 13: 'ENSG00000001461',\n 14: 'ENSG00000001497',\n 15: 'ENSG00000001561',\n\
          \ 16: 'ENSG00000001617',\n 17: 'ENSG00000001626',\n 18: 'ENSG00000001629',\n\
          \ 19: 'ENSG00000001630',\n 20: 'ENSG00000001631',\n 21: 'ENSG00000002016',\n\
          .\n.\n```"
        updatedAt: '2023-07-17T01:23:56.661Z'
      numEdits: 0
      reactions: []
    id: 64b4982c102ed6e7ae09e7d7
    type: comment
  author: pchiang5
  content: "Thank you for your answer. I successfully get the length file into pkl\
    \ with the following code\n```\ndata = train_dataset['length']\nfile_path = '/mnt/c/Users/pc/Downloads/train_dataset_length.pkl'\n\
    \n# Save data as a pickle file\nwith open(file_path, 'wb') as file:\n    pickle.dump(data,\
    \ file)\n\nprint(\"Data saved as pickle file:\", file_path)\n```\n\nHowever, another\
    \ error occurred below:\n```\nDESKTOP-6FHRRIO:4363:4517 [0] NCCL INFO comm 0x555c70f150e0\
    \ rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE\n  0%|                \
    \                                                                         | 0/13488\
    \ [00:00<?, ?it/s]Traceback (most recent call last):\n  File \"/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py\"\
    , line 192, in <module>\n    trainer.train()\n  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1645, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1916, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
    , line 633, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py\"\
    , line 677, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may\
    \ raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    /home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\"\
    , line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
    , line 45, in __call__\n    return self.torch_call(features)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformers/data/data_collator.py\"\
    , line 732, in torch_call\n    batch = self.tokenizer.pad(examples, return_tensors=\"\
    pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
    , line 397, in pad\n    padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n\
    \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/pc/miniconda3/envs/Transformers/lib/python3.11/site-packages/geneformer/pretrainer.py\"\
    , line 250, in _get_padding_truncation_strategies\n    not self.pad_token or self.pad_token_id\
    \ < 0\n                          ^^^^^^^^^^^^^^^^^^^^^\nTypeError: '<' not supported\
    \ between instances of 'NoneType' and 'int'\n  0%|                           \
    \                                                              | 0/13488 [00:00<?,\
    \ ?it/s]\nDESKTOP-6FHRRIO:4363:4518 [0] NCCL INFO [Service thread] Connection\
    \ closed by localRank 0\nDESKTOP-6FHRRIO:4363:4363 [0] NCCL INFO comm 0x555c70f150e0\
    \ rank 0 nranks 1 cudaDev 0 busId 3000 - Abort COMPLETE\n[2023-07-17 09:13:53,853]\
    \ [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4363\n[2023-07-17\
    \ 09:13:53,853] [ERROR] [launch.py:321:sigkill_handler] ['/home/pc/miniconda3/envs/Transformers/bin/python3.11',\
    \ '-u', '/mnt/c/Users/pc/Downloads/pretrain_geneformer_w_deepspeed.py', '--local_rank=0',\
    \ '--deepspeed', '/home/pc/transformers/tests/deepspeed/ds_config_zero3.json']\
    \ exits with return code = 1\n```\n\nIt was due to line 250 `not self.pad_token\
    \ or self.pad_token_id < 0` in the pretrainer.py.  Was that due to something wrong\
    \ with the token dictionary, the original pretrained model, or my dataset(Below\
    \ is the head of the token_dictionary)? \n```\nIn [5]: token_dictionary\nOut[5]:\n\
    {0: '<pad>',\n 1: '<mask>',\n 2: 'ENSG00000000003',\n 3: 'ENSG00000000005',\n\
    \ 4: 'ENSG00000000419',\n 5: 'ENSG00000000457',\n 6: 'ENSG00000000460',\n 7: 'ENSG00000000938',\n\
    \ 8: 'ENSG00000000971',\n 9: 'ENSG00000001036',\n 10: 'ENSG00000001084',\n 11:\
    \ 'ENSG00000001167',\n 12: 'ENSG00000001460',\n 13: 'ENSG00000001461',\n 14: 'ENSG00000001497',\n\
    \ 15: 'ENSG00000001561',\n 16: 'ENSG00000001617',\n 17: 'ENSG00000001626',\n 18:\
    \ 'ENSG00000001629',\n 19: 'ENSG00000001630',\n 20: 'ENSG00000001631',\n 21: 'ENSG00000002016',\n\
    .\n.\n```"
  created_at: 2023-07-17 00:23:56+00:00
  edited: false
  hidden: false
  id: 64b4982c102ed6e7ae09e7d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-17T19:08:16.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9087616205215454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The token dictionary in the repository
          is inverted from the one you pasted in your comment. Please pull the current
          repository to ensure you are not using an outdated version.</p>

          '
        raw: Thank you for your question. The token dictionary in the repository is
          inverted from the one you pasted in your comment. Please pull the current
          repository to ensure you are not using an outdated version.
        updatedAt: '2023-07-17T19:08:16.277Z'
      numEdits: 0
      reactions: []
    id: 64b591a056faab47dde493a7
    type: comment
  author: ctheodoris
  content: Thank you for your question. The token dictionary in the repository is
    inverted from the one you pasted in your comment. Please pull the current repository
    to ensure you are not using an outdated version.
  created_at: 2023-07-17 18:08:16+00:00
  edited: false
  hidden: false
  id: 64b591a056faab47dde493a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-07-18T01:37:46.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9333146214485168
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: '<p>The update worked. Thank you.</p>

          <p>Two more questions: </p>

          <ol>

          <li>If I have 3 new datasets (A, B, and C), would you recommend sequential
          fine-tuning (A first, B, and finally C) or doing all 3 together concurrently?
          In other words, will the classifying accuracy of A be worse after the fine-tuning
          of C if I do it sequentially?</li>

          <li>If I cannot do this at the batch size of 12, is a smaller batch size
          detrimental to this sort of "fine-tuning"?</li>

          </ol>

          '
        raw: "The update worked. Thank you.\n\nTwo more questions: \n1. If I have\
          \ 3 new datasets (A, B, and C), would you recommend sequential fine-tuning\
          \ (A first, B, and finally C) or doing all 3 together concurrently? In other\
          \ words, will the classifying accuracy of A be worse after the fine-tuning\
          \ of C if I do it sequentially?\n2. If I cannot do this at the batch size\
          \ of 12, is a smaller batch size detrimental to this sort of \"fine-tuning\"\
          ?    "
        updatedAt: '2023-07-18T01:37:46.998Z'
      numEdits: 0
      reactions: []
    id: 64b5ecea7d1c24f1bae06f96
    type: comment
  author: pchiang5
  content: "The update worked. Thank you.\n\nTwo more questions: \n1. If I have 3\
    \ new datasets (A, B, and C), would you recommend sequential fine-tuning (A first,\
    \ B, and finally C) or doing all 3 together concurrently? In other words, will\
    \ the classifying accuracy of A be worse after the fine-tuning of C if I do it\
    \ sequentially?\n2. If I cannot do this at the batch size of 12, is a smaller\
    \ batch size detrimental to this sort of \"fine-tuning\"?    "
  created_at: 2023-07-18 00:37:46+00:00
  edited: false
  hidden: false
  id: 64b5ecea7d1c24f1bae06f96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-18T22:40:57.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389841556549072
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The best route depends on the data
          and your scientific question. If the three datasets are equivalent, it would
          likely be better to fine tune with them together so the model gets a more
          generalizable understanding of the data. If there is a specific reason you
          would do them sequentially (e.g. the first dataset is more general to get
          a baseline view of the informational space, the second fine-tunes to a more
          specific question, etc.), then it may be helpful to fine-tune sequentially.
          In terms of whether the last dataset will be better remembered than the
          first if you fine-tune sequentially, this could happen, but it depends on
          other factors as well such as the size of each dataset and how many layers
          you freeze vs. allow to be tunable at each step. </p>

          '
        raw: 'Thank you for your question. The best route depends on the data and
          your scientific question. If the three datasets are equivalent, it would
          likely be better to fine tune with them together so the model gets a more
          generalizable understanding of the data. If there is a specific reason you
          would do them sequentially (e.g. the first dataset is more general to get
          a baseline view of the informational space, the second fine-tunes to a more
          specific question, etc.), then it may be helpful to fine-tune sequentially.
          In terms of whether the last dataset will be better remembered than the
          first if you fine-tune sequentially, this could happen, but it depends on
          other factors as well such as the size of each dataset and how many layers
          you freeze vs. allow to be tunable at each step. '
        updatedAt: '2023-07-18T22:40:57.887Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pchiang5
    id: 64b714f92fbbea73ad81e329
    type: comment
  author: ctheodoris
  content: 'Thank you for your question. The best route depends on the data and your
    scientific question. If the three datasets are equivalent, it would likely be
    better to fine tune with them together so the model gets a more generalizable
    understanding of the data. If there is a specific reason you would do them sequentially
    (e.g. the first dataset is more general to get a baseline view of the informational
    space, the second fine-tunes to a more specific question, etc.), then it may be
    helpful to fine-tune sequentially. In terms of whether the last dataset will be
    better remembered than the first if you fine-tune sequentially, this could happen,
    but it depends on other factors as well such as the size of each dataset and how
    many layers you freeze vs. allow to be tunable at each step. '
  created_at: 2023-07-18 21:40:57+00:00
  edited: false
  hidden: false
  id: 64b714f92fbbea73ad81e329
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 110
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Question on fine-tuning the model without a down-stream task
