!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pchiang5
conflicting_files: null
created_at: 2023-06-16 01:23:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-16T02:23:35.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6551181077957153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: '<p>Hello,</p>

          <p>Thank you for the work!</p>

          <p>When I tried running the example in silico perturbation with the code
          below, I found that I could only use forward_batch_size=80, instead of 400,
          with a 24Gb-ram graphic drive. Is the batch size too small for the inference?
          If it is the case, could you recommend a better ram size for the purpose
          in general? </p>

          <p>isp = InSilicoPerturber(perturb_type="delete",<br>                      perturb_rank_shift=None,<br>                      genes_to_perturb="all",<br>                      combos=0,<br>                      anchor_gene=None,<br>                      model_type="CellClassifier",<br>                      num_classes=3,<br>                      emb_mode="cell",<br>                      cell_emb_style="mean_pool",<br>                      filter_data={"cell_type":["Cardiomyocyte3"]},<br>                      cell_states_to_model={"disease":(["dcm"],["nf"],["hcm"])},<br>                      max_ncells=2000,<br>                      emb_layer=0,<br>                      forward_batch_size=80,<br>                      nproc=16,<br>                      save_raw_data=True)</p>

          '
        raw: "Hello,\r\n\r\nThank you for the work!\r\n\r\nWhen I tried running the\
          \ example in silico perturbation with the code below, I found that I could\
          \ only use forward_batch_size=80, instead of 400, with a 24Gb-ram graphic\
          \ drive. Is the batch size too small for the inference? If it is the case,\
          \ could you recommend a better ram size for the purpose in general? \r\n\
          \r\nisp = InSilicoPerturber(perturb_type=\"delete\",\r\n               \
          \       perturb_rank_shift=None,\r\n                      genes_to_perturb=\"\
          all\",\r\n                      combos=0,\r\n                      anchor_gene=None,\r\
          \n                      model_type=\"CellClassifier\",\r\n             \
          \         num_classes=3,\r\n                      emb_mode=\"cell\",\r\n\
          \                      cell_emb_style=\"mean_pool\",\r\n               \
          \       filter_data={\"cell_type\":[\"Cardiomyocyte3\"]},\r\n          \
          \            cell_states_to_model={\"disease\":([\"dcm\"],[\"nf\"],[\"hcm\"\
          ])},\r\n                      max_ncells=2000,\r\n                     \
          \ emb_layer=0,\r\n                      forward_batch_size=80,\r\n     \
          \                 nproc=16,\r\n                      save_raw_data=True)"
        updatedAt: '2023-06-16T02:23:35.295Z'
      numEdits: 0
      reactions: []
    id: 648bc7a7d0f0f5b02eafb8b7
    type: comment
  author: pchiang5
  content: "Hello,\r\n\r\nThank you for the work!\r\n\r\nWhen I tried running the\
    \ example in silico perturbation with the code below, I found that I could only\
    \ use forward_batch_size=80, instead of 400, with a 24Gb-ram graphic drive. Is\
    \ the batch size too small for the inference? If it is the case, could you recommend\
    \ a better ram size for the purpose in general? \r\n\r\nisp = InSilicoPerturber(perturb_type=\"\
    delete\",\r\n                      perturb_rank_shift=None,\r\n              \
    \        genes_to_perturb=\"all\",\r\n                      combos=0,\r\n    \
    \                  anchor_gene=None,\r\n                      model_type=\"CellClassifier\"\
    ,\r\n                      num_classes=3,\r\n                      emb_mode=\"\
    cell\",\r\n                      cell_emb_style=\"mean_pool\",\r\n           \
    \           filter_data={\"cell_type\":[\"Cardiomyocyte3\"]},\r\n            \
    \          cell_states_to_model={\"disease\":([\"dcm\"],[\"nf\"],[\"hcm\"])},\r\
    \n                      max_ncells=2000,\r\n                      emb_layer=0,\r\
    \n                      forward_batch_size=80,\r\n                      nproc=16,\r\
    \n                      save_raw_data=True)"
  created_at: 2023-06-16 01:23:35+00:00
  edited: false
  hidden: false
  id: 648bc7a7d0f0f5b02eafb8b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-16T03:25:06.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9300699830055237
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your question. The batch size depends on many factors,\
          \ including how many genes there are detected per cell in your dataset,\
          \ whether you are outputting gene-level embeddings or just cell-level embeddings,\
          \ and your GPU resources. Because you are just running inference and not\
          \ training the model, the results are deterministic and will be the same\
          \ regardless of what you use for your batch size. Therefore, you should\
          \ reduce your batch size until what is possible with your resources, or\
          \ increase your resources if you want to run it with a larger batch size\
          \ for faster processing. Of note, the code sorts the cells by input size\
          \ so the cells with the most genes detected are presented first to encounter\
          \ memory issues earlier rather than later when you are at an intermediate\
          \ point in the analysis. However, this also means that it\u2019s possible\
          \ that you can run larger batch sizes later in the dataset. You can add\
          \ a wrapper to optimize the batch size dynamically if you\u2019d like. </p>\n"
        raw: "Thank you for your question. The batch size depends on many factors,\
          \ including how many genes there are detected per cell in your dataset,\
          \ whether you are outputting gene-level embeddings or just cell-level embeddings,\
          \ and your GPU resources. Because you are just running inference and not\
          \ training the model, the results are deterministic and will be the same\
          \ regardless of what you use for your batch size. Therefore, you should\
          \ reduce your batch size until what is possible with your resources, or\
          \ increase your resources if you want to run it with a larger batch size\
          \ for faster processing. Of note, the code sorts the cells by input size\
          \ so the cells with the most genes detected are presented first to encounter\
          \ memory issues earlier rather than later when you are at an intermediate\
          \ point in the analysis. However, this also means that it\u2019s possible\
          \ that you can run larger batch sizes later in the dataset. You can add\
          \ a wrapper to optimize the batch size dynamically if you\u2019d like. "
        updatedAt: '2023-06-16T03:25:06.397Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pchiang5
      relatedEventId: 648bd612b465091c34dd8186
    id: 648bd612b465091c34dd8183
    type: comment
  author: ctheodoris
  content: "Thank you for your question. The batch size depends on many factors, including\
    \ how many genes there are detected per cell in your dataset, whether you are\
    \ outputting gene-level embeddings or just cell-level embeddings, and your GPU\
    \ resources. Because you are just running inference and not training the model,\
    \ the results are deterministic and will be the same regardless of what you use\
    \ for your batch size. Therefore, you should reduce your batch size until what\
    \ is possible with your resources, or increase your resources if you want to run\
    \ it with a larger batch size for faster processing. Of note, the code sorts the\
    \ cells by input size so the cells with the most genes detected are presented\
    \ first to encounter memory issues earlier rather than later when you are at an\
    \ intermediate point in the analysis. However, this also means that it\u2019s\
    \ possible that you can run larger batch sizes later in the dataset. You can add\
    \ a wrapper to optimize the batch size dynamically if you\u2019d like. "
  created_at: 2023-06-16 02:25:06+00:00
  edited: false
  hidden: false
  id: 648bd612b465091c34dd8183
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-16T03:25:06.000Z'
    data:
      status: closed
    id: 648bd612b465091c34dd8186
    type: status-change
  author: ctheodoris
  created_at: 2023-06-16 02:25:06+00:00
  id: 648bd612b465091c34dd8186
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-26T05:21:12.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.697024941444397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: '<p>Thank you for your response. However, the progress was stuck at
          0% for a long time (at least overnight) without any error message (GTX1650
          4Gb, compute capability 7.5). I tried the identical setup and dataset with
          google colab (T4 16Gb, compute capability 7.5) and it finished without any
          problem. It was not due to VRAM insufficiency (3.4 of the 4Gb used). Could
          you kindly give me some hints about how to troubleshoot or resolve this
          issue? </p>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>from geneformer import InSilicoPerturber<br>from geneformer import InSilicoPerturberStats</p>

          <p>isp = InSilicoPerturber(perturb_type="delete",<br>...                       perturb_rank_shift=None,<br>...                       genes_to_perturb="all",<br>...                       combos=0,<br>...                       anchor_gene=None,<br>...                       model_type="CellClassifier",<br>...                       num_classes=3,<br>...                       emb_mode="cell",<br>...                       cell_emb_style="mean_pool",<br>...                       filter_data={"cell_type":["Cardiomyocyte1","Cardiomyocyte2","Cardiomyocyte3"]},<br>...                       cell_states_to_model={"disease":(["dcm"],["nf"],["hcm"])},<br>...                       max_ncells=100,<br>...                       emb_layer=0,<br>...                       forward_batch_size=20,<br>...                       nproc=16,<br>...                       save_raw_data=True)</p>

          <p>#change all cuda or cuda:0 to cpu in the in_silico_pertuber.py</p>

          <p>isp.perturb_data("ctheodoris/Geneformer",<br>...                "/mnt/c/Users/pc/Downloads/GF_DCM",<br>...                "/mnt/c/Users/pc/Downloads/GF",<br>...                "output_prefix")<br>Loading
          cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1cc9cdd7e6000d3b_*_of_00016.arrow<br>Loading
          cached shuffled indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-2358227ccbc43242.arrow<br>Loading
          cached sorted indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1a7a0c949a32027e.arrow<br>Some
          weights of the model checkpoint at ctheodoris/Geneformer were not used when
          initializing BertForSequenceClassification: [''cls.predictions.decoder.bias'',
          ''cls.predictions.bias'', ''cls.predictions.transform.LayerNorm.bias'',
          ''cls.predictions.transform.LayerNorm.weight'', ''cls.predictions.decoder.weight'',
          ''cls.predictions.transform.dense.weight'', ''cls.predictions.transform.dense.bias'']</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <ul>

          <li>This IS expected if you are initializing BertForSequenceClassification
          from the checkpoint of a model trained on another task or with another architecture
          (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining
          model).</li>

          <li>This IS NOT expected if you are initializing BertForSequenceClassification
          from the checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).<br>Some weights of BertForSequenceClassification were not initialized
          from the model checkpoint at ctheodoris/Geneformer and are newly initialized:
          [''bert.pooler.dense.weight'', ''classifier.bias'', ''classifier.weight'',
          ''bert.pooler.dense.bias'']<br>You should probably TRAIN this model on a
          down-stream task to be able to use it for predictions and inference.<br>Loading
          cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-364fded4f33ad57b_*<em>of_00016.arrow<br>Loading
          cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-5e71ce5595a074e1</em><em><em>of_00016.arrow<br>Loading
          cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-694066f5f18596b0</em></em>_of_00016.arrow<br>0%|          |
          0/100 [00:00&lt;?, ?it/s]</li>

          </ul>

          '
        raw: "Thank you for your response. However, the progress was stuck at 0% for\
          \ a long time (at least overnight) without any error message (GTX1650 4Gb,\
          \ compute capability 7.5). I tried the identical setup and dataset with\
          \ google colab (T4 16Gb, compute capability 7.5) and it finished without\
          \ any problem. It was not due to VRAM insufficiency (3.4 of the 4Gb used).\
          \ Could you kindly give me some hints about how to troubleshoot or resolve\
          \ this issue? \n\n>>> from geneformer import InSilicoPerturber\n>>> from\
          \ geneformer import InSilicoPerturberStats\n>>>\n>>>\n>>> isp = InSilicoPerturber(perturb_type=\"\
          delete\",\n...                       perturb_rank_shift=None,\n...     \
          \                  genes_to_perturb=\"all\",\n...                      \
          \ combos=0,\n...                       anchor_gene=None,\n...          \
          \             model_type=\"CellClassifier\",\n...                      \
          \ num_classes=3,\n...                       emb_mode=\"cell\",\n...    \
          \                   cell_emb_style=\"mean_pool\",\n...                 \
          \      filter_data={\"cell_type\":[\"Cardiomyocyte1\",\"Cardiomyocyte2\"\
          ,\"Cardiomyocyte3\"]},\n...                       cell_states_to_model={\"\
          disease\":([\"dcm\"],[\"nf\"],[\"hcm\"])},\n...                       max_ncells=100,\n\
          ...                       emb_layer=0,\n...                       forward_batch_size=20,\n\
          ...                       nproc=16,\n...                       save_raw_data=True)\n\
          >>>\n>>>\n>>> #change all cuda or cuda:0 to cpu in the in_silico_pertuber.py\n\
          >>>\n>>> isp.perturb_data(\"ctheodoris/Geneformer\",\n...              \
          \  \"/mnt/c/Users/pc/Downloads/GF_DCM\",\n...                \"/mnt/c/Users/pc/Downloads/GF\"\
          ,\n...                \"output_prefix\")\nLoading cached processed dataset\
          \ at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1cc9cdd7e6000d3b_*_of_00016.arrow\n\
          Loading cached shuffled indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-2358227ccbc43242.arrow\n\
          Loading cached sorted indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1a7a0c949a32027e.arrow\n\
          Some weights of the model checkpoint at ctheodoris/Geneformer were not used\
          \ when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias',\
          \ 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight',\
          \ 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight',\
          \ 'cls.predictions.transform.dense.bias']\n- This IS expected if you are\
          \ initializing BertForSequenceClassification from the checkpoint of a model\
          \ trained on another task or with another architecture (e.g. initializing\
          \ a BertForSequenceClassification model from a BertForPreTraining model).\n\
          - This IS NOT expected if you are initializing BertForSequenceClassification\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\nSome weights of BertForSequenceClassification were not initialized\
          \ from the model checkpoint at ctheodoris/Geneformer and are newly initialized:\
          \ ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias']\n\
          You should probably TRAIN this model on a down-stream task to be able to\
          \ use it for predictions and inference.\nLoading cached processed dataset\
          \ at /mnt/c/Users/pc/Downloads/GF_DCM/cache-364fded4f33ad57b_*_of_00016.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-5e71ce5595a074e1_*_of_00016.arrow\n\
          Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-694066f5f18596b0_*_of_00016.arrow\n\
          \  0%|          | 0/100 [00:00<?, ?it/s]\n\n "
        updatedAt: '2023-06-26T05:21:12.853Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649920487ac1bf03e21dff3b
    id: 649920487ac1bf03e21dff38
    type: comment
  author: pchiang5
  content: "Thank you for your response. However, the progress was stuck at 0% for\
    \ a long time (at least overnight) without any error message (GTX1650 4Gb, compute\
    \ capability 7.5). I tried the identical setup and dataset with google colab (T4\
    \ 16Gb, compute capability 7.5) and it finished without any problem. It was not\
    \ due to VRAM insufficiency (3.4 of the 4Gb used). Could you kindly give me some\
    \ hints about how to troubleshoot or resolve this issue? \n\n>>> from geneformer\
    \ import InSilicoPerturber\n>>> from geneformer import InSilicoPerturberStats\n\
    >>>\n>>>\n>>> isp = InSilicoPerturber(perturb_type=\"delete\",\n...          \
    \             perturb_rank_shift=None,\n...                       genes_to_perturb=\"\
    all\",\n...                       combos=0,\n...                       anchor_gene=None,\n\
    ...                       model_type=\"CellClassifier\",\n...                \
    \       num_classes=3,\n...                       emb_mode=\"cell\",\n...    \
    \                   cell_emb_style=\"mean_pool\",\n...                       filter_data={\"\
    cell_type\":[\"Cardiomyocyte1\",\"Cardiomyocyte2\",\"Cardiomyocyte3\"]},\n...\
    \                       cell_states_to_model={\"disease\":([\"dcm\"],[\"nf\"],[\"\
    hcm\"])},\n...                       max_ncells=100,\n...                    \
    \   emb_layer=0,\n...                       forward_batch_size=20,\n...      \
    \                 nproc=16,\n...                       save_raw_data=True)\n>>>\n\
    >>>\n>>> #change all cuda or cuda:0 to cpu in the in_silico_pertuber.py\n>>>\n\
    >>> isp.perturb_data(\"ctheodoris/Geneformer\",\n...                \"/mnt/c/Users/pc/Downloads/GF_DCM\"\
    ,\n...                \"/mnt/c/Users/pc/Downloads/GF\",\n...                \"\
    output_prefix\")\nLoading cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1cc9cdd7e6000d3b_*_of_00016.arrow\n\
    Loading cached shuffled indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-2358227ccbc43242.arrow\n\
    Loading cached sorted indices for dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-1a7a0c949a32027e.arrow\n\
    Some weights of the model checkpoint at ctheodoris/Geneformer were not used when\
    \ initializing BertForSequenceClassification: ['cls.predictions.decoder.bias',\
    \ 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight',\
    \ 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight',\
    \ 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing\
    \ BertForSequenceClassification from the checkpoint of a model trained on another\
    \ task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ BertForSequenceClassification from the checkpoint of a model that you expect\
    \ to be exactly identical (initializing a BertForSequenceClassification model\
    \ from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification\
    \ were not initialized from the model checkpoint at ctheodoris/Geneformer and\
    \ are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight',\
    \ 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream\
    \ task to be able to use it for predictions and inference.\nLoading cached processed\
    \ dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-364fded4f33ad57b_*_of_00016.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-5e71ce5595a074e1_*_of_00016.arrow\n\
    Loading cached processed dataset at /mnt/c/Users/pc/Downloads/GF_DCM/cache-694066f5f18596b0_*_of_00016.arrow\n\
    \  0%|          | 0/100 [00:00<?, ?it/s]\n\n "
  created_at: 2023-06-26 04:21:12+00:00
  edited: false
  hidden: false
  id: 649920487ac1bf03e21dff38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-06-26T05:21:12.000Z'
    data:
      status: open
    id: 649920487ac1bf03e21dff3b
    type: status-change
  author: pchiang5
  created_at: 2023-06-26 04:21:12+00:00
  id: 649920487ac1bf03e21dff3b
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-26T06:06:45.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9362545013427734
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your question. If it is working for you on Colab with\
          \ the larger GPU, it sounds like it\u2019s probably just running very slowly\
          \ due to low resources. This computation involves deleting each individual\
          \ gene in the cell (so 2048 perturbations per cell if the cell has at least\
          \ 2048 genes detected), with gene embedding outputs of 2047x256 for each\
          \ of those perturbations, compared back to the original gene embeddings\
          \ of 2047x256. You may consider running it on Colab since it\u2019s completing\
          \ there. As I noted in my previous response, the first cell will take the\
          \ most time due to the sorting by size to encounter memory constraints earlier\
          \ rather than later, so if later cells have less genes detected, they will\
          \ run faster.</p>\n<p>Also, I wanted to mention that it looks like you are\
          \ loading the Geneformer pretrained model without fine-tuning, but you are\
          \ incorrectly indicating to the in silico perturber that it is a CellClassifier\
          \ fine-tuned model. As the warning states, this means it\u2019s loading\
          \ it with the head layer untrained, with randomly initialized weights. Since\
          \ you are asking it to quantify embeddings from the head layer (emb_layer=0),\
          \ your results will be random. You should either first fine-tune your model,\
          \ which is recommended in this case as discussed below, or indicate to the\
          \ in silico perturber that you are loading the pretrained model. </p>\n\
          <p>From closed issue <a href=\"/ctheodoris/Geneformer/discussions/63\">#63</a>:<br>Also,\
          \ I wanted to note that when using in silico perturbation to test for perturbations\
          \ that shift cells between two very similar states, it will likely be more\
          \ effective if you first fine-tune the model to distinguish between the\
          \ states so that they are better separated within the embedding space before\
          \ testing what perturbation shifts between them. Specifically, the end-stage\
          \ failing heart states are similar between the dilated and hypertrophic\
          \ cardiomyopathy samples in Chaffin et al. Nature 2022, so it will likely\
          \ be more effective to first fine-tune the model to distinguish them before\
          \ running the in silico perturbation/treatment analysis. Fine-tuning the\
          \ model is less necessary when testing the shift between two states that\
          \ are already very separable by the pretrained model (e.g. fibroblasts vs.\
          \ cardiomyocytes). However, fine-tuning with relevant data may still be\
          \ helpful to orient the model's weights towards the specific downstream\
          \ objective.</p>\n"
        raw: "Thank you for your question. If it is working for you on Colab with\
          \ the larger GPU, it sounds like it\u2019s probably just running very slowly\
          \ due to low resources. This computation involves deleting each individual\
          \ gene in the cell (so 2048 perturbations per cell if the cell has at least\
          \ 2048 genes detected), with gene embedding outputs of 2047x256 for each\
          \ of those perturbations, compared back to the original gene embeddings\
          \ of 2047x256. You may consider running it on Colab since it\u2019s completing\
          \ there. As I noted in my previous response, the first cell will take the\
          \ most time due to the sorting by size to encounter memory constraints earlier\
          \ rather than later, so if later cells have less genes detected, they will\
          \ run faster.\n\nAlso, I wanted to mention that it looks like you are loading\
          \ the Geneformer pretrained model without fine-tuning, but you are incorrectly\
          \ indicating to the in silico perturber that it is a CellClassifier fine-tuned\
          \ model. As the warning states, this means it\u2019s loading it with the\
          \ head layer untrained, with randomly initialized weights. Since you are\
          \ asking it to quantify embeddings from the head layer (emb_layer=0), your\
          \ results will be random. You should either first fine-tune your model,\
          \ which is recommended in this case as discussed below, or indicate to the\
          \ in silico perturber that you are loading the pretrained model. \n\nFrom\
          \ closed issue #63:\nAlso, I wanted to note that when using in silico perturbation\
          \ to test for perturbations that shift cells between two very similar states,\
          \ it will likely be more effective if you first fine-tune the model to distinguish\
          \ between the states so that they are better separated within the embedding\
          \ space before testing what perturbation shifts between them. Specifically,\
          \ the end-stage failing heart states are similar between the dilated and\
          \ hypertrophic cardiomyopathy samples in Chaffin et al. Nature 2022, so\
          \ it will likely be more effective to first fine-tune the model to distinguish\
          \ them before running the in silico perturbation/treatment analysis. Fine-tuning\
          \ the model is less necessary when testing the shift between two states\
          \ that are already very separable by the pretrained model (e.g. fibroblasts\
          \ vs. cardiomyocytes). However, fine-tuning with relevant data may still\
          \ be helpful to orient the model's weights towards the specific downstream\
          \ objective.\n"
        updatedAt: '2023-06-26T06:06:45.311Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64992af547446ba3f1d6ce68
    id: 64992af547446ba3f1d6ce64
    type: comment
  author: ctheodoris
  content: "Thank you for your question. If it is working for you on Colab with the\
    \ larger GPU, it sounds like it\u2019s probably just running very slowly due to\
    \ low resources. This computation involves deleting each individual gene in the\
    \ cell (so 2048 perturbations per cell if the cell has at least 2048 genes detected),\
    \ with gene embedding outputs of 2047x256 for each of those perturbations, compared\
    \ back to the original gene embeddings of 2047x256. You may consider running it\
    \ on Colab since it\u2019s completing there. As I noted in my previous response,\
    \ the first cell will take the most time due to the sorting by size to encounter\
    \ memory constraints earlier rather than later, so if later cells have less genes\
    \ detected, they will run faster.\n\nAlso, I wanted to mention that it looks like\
    \ you are loading the Geneformer pretrained model without fine-tuning, but you\
    \ are incorrectly indicating to the in silico perturber that it is a CellClassifier\
    \ fine-tuned model. As the warning states, this means it\u2019s loading it with\
    \ the head layer untrained, with randomly initialized weights. Since you are asking\
    \ it to quantify embeddings from the head layer (emb_layer=0), your results will\
    \ be random. You should either first fine-tune your model, which is recommended\
    \ in this case as discussed below, or indicate to the in silico perturber that\
    \ you are loading the pretrained model. \n\nFrom closed issue #63:\nAlso, I wanted\
    \ to note that when using in silico perturbation to test for perturbations that\
    \ shift cells between two very similar states, it will likely be more effective\
    \ if you first fine-tune the model to distinguish between the states so that they\
    \ are better separated within the embedding space before testing what perturbation\
    \ shifts between them. Specifically, the end-stage failing heart states are similar\
    \ between the dilated and hypertrophic cardiomyopathy samples in Chaffin et al.\
    \ Nature 2022, so it will likely be more effective to first fine-tune the model\
    \ to distinguish them before running the in silico perturbation/treatment analysis.\
    \ Fine-tuning the model is less necessary when testing the shift between two states\
    \ that are already very separable by the pretrained model (e.g. fibroblasts vs.\
    \ cardiomyocytes). However, fine-tuning with relevant data may still be helpful\
    \ to orient the model's weights towards the specific downstream objective.\n"
  created_at: 2023-06-26 05:06:45+00:00
  edited: false
  hidden: false
  id: 64992af547446ba3f1d6ce64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-26T06:06:45.000Z'
    data:
      status: closed
    id: 64992af547446ba3f1d6ce68
    type: status-change
  author: ctheodoris
  created_at: 2023-06-26 05:06:45+00:00
  id: 64992af547446ba3f1d6ce68
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: about  the values of forward_batch_size
