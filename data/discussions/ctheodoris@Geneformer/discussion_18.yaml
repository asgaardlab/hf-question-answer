!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viofa
conflicting_files: null
created_at: 2023-06-07 18:34:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/461aff597c6c3b35cde87a6f6354b30b.svg
      fullname: viola
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viofa
      type: user
    createdAt: '2023-06-07T19:34:54.000Z'
    data:
      edited: false
      editors:
      - viofa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9567450881004333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/461aff597c6c3b35cde87a6f6354b30b.svg
          fullname: viola
          isHf: false
          isPro: false
          name: viofa
          type: user
        html: '<p>Hi, terrific work! Congratulations!<br>I am trying to understand
          what the input data is, and I am getting confused.<br>What does the 2048
          input size represent? I read it is 93% of rank value encodings, but I cannot
          understand how you get there.<br>Also, would it be possible to have a sample
          tokenized dataset available? I think a minimal working example would be
          helpful.<br>Thanks!</p>

          '
        raw: "Hi, terrific work! Congratulations!\r\nI am trying to understand what\
          \ the input data is, and I am getting confused.\r\nWhat does the 2048 input\
          \ size represent? I read it is 93% of rank value encodings, but I cannot\
          \ understand how you get there. \r\nAlso, would it be possible to have a\
          \ sample tokenized dataset available? I think a minimal working example\
          \ would be helpful.\r\nThanks!"
        updatedAt: '2023-06-07T19:34:54.734Z'
      numEdits: 0
      reactions: []
    id: 6480dbdecacb1c4a06977aea
    type: comment
  author: viofa
  content: "Hi, terrific work! Congratulations!\r\nI am trying to understand what\
    \ the input data is, and I am getting confused.\r\nWhat does the 2048 input size\
    \ represent? I read it is 93% of rank value encodings, but I cannot understand\
    \ how you get there. \r\nAlso, would it be possible to have a sample tokenized\
    \ dataset available? I think a minimal working example would be helpful.\r\nThanks!"
  created_at: 2023-06-07 18:34:54+00:00
  edited: false
  hidden: false
  id: 6480dbdecacb1c4a06977aea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4850ee4f9cbabc7dce4a7e876f03f69.svg
      fullname: Yongjian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yjyang027
      type: user
    createdAt: '2023-06-07T19:55:54.000Z'
    data:
      edited: false
      editors:
      - yjyang027
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6989390850067139
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4850ee4f9cbabc7dce4a7e876f03f69.svg
          fullname: Yongjian
          isHf: false
          isPro: false
          name: yjyang027
          type: user
        html: '<p>same here</p>

          '
        raw: same here
        updatedAt: '2023-06-07T19:55:54.019Z'
      numEdits: 0
      reactions: []
    id: 6480e0ca9aafd41918aedd3f
    type: comment
  author: yjyang027
  content: same here
  created_at: 2023-06-07 18:55:54+00:00
  edited: false
  hidden: false
  id: 6480e0ca9aafd41918aedd3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-07T21:45:37.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8095236420631409
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. Please refer to the methods
          section of the manuscript (rdcu.be/ddrx0) for a discussion of the model
          parameters (excerpt below). Attention-based models have various parameters
          that comprise their architecture. One of these is the input size, which
          indicates the maximum number of tokens in each input encoding presented
          to the model. For example, the language model BERT-large has an input size
          of 1024 (<a rel="nofollow" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a>).
          The input size for Geneformer is 2048. We provide the pretraining corpus
          as a tokenized dataset in the Genecorpus-30M dataset repository (<a href="https://huggingface.co/datasets/ctheodoris/Genecorpus-30M">https://huggingface.co/datasets/ctheodoris/Genecorpus-30M</a>).
          Users can tokenize their own single cell RNAseq .loom datasets using the
          transcriptome tokenizer provided in this Geneformer repository (tokenizer.py).</p>

          <p>Excerpt from manuscript methods discussing Geneformer model parameters:<br>"Geneformer
          is composed of six transformer encoder units, each composed of a self-attention
          layer and feed forward neural network layer with the following parameters:
          input size of 2,048 (fully represents 93% of rank value encodings in Genecorpus-30M),
          256 embedding dimensions, four attention heads per layer and feed forward
          size of 512 (Fig. 1c). Geneformer uses full dense self-attention across
          the input size of 2,048."</p>

          '
        raw: 'Thank you for your interest in Geneformer. Please refer to the methods
          section of the manuscript (rdcu.be/ddrx0) for a discussion of the model
          parameters (excerpt below). Attention-based models have various parameters
          that comprise their architecture. One of these is the input size, which
          indicates the maximum number of tokens in each input encoding presented
          to the model. For example, the language model BERT-large has an input size
          of 1024 (https://arxiv.org/pdf/1810.04805.pdf). The input size for Geneformer
          is 2048. We provide the pretraining corpus as a tokenized dataset in the
          Genecorpus-30M dataset repository (https://huggingface.co/datasets/ctheodoris/Genecorpus-30M).
          Users can tokenize their own single cell RNAseq .loom datasets using the
          transcriptome tokenizer provided in this Geneformer repository (tokenizer.py).


          Excerpt from manuscript methods discussing Geneformer model parameters:

          "Geneformer is composed of six transformer encoder units, each composed
          of a self-attention layer and feed forward neural network layer with the
          following parameters: input size of 2,048 (fully represents 93% of rank
          value encodings in Genecorpus-30M), 256 embedding dimensions, four attention
          heads per layer and feed forward size of 512 (Fig. 1c). Geneformer uses
          full dense self-attention across the input size of 2,048."'
        updatedAt: '2023-06-07T21:45:37.104Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6480fa8140facadc557555b2
    id: 6480fa8140facadc557555b0
    type: comment
  author: ctheodoris
  content: 'Thank you for your interest in Geneformer. Please refer to the methods
    section of the manuscript (rdcu.be/ddrx0) for a discussion of the model parameters
    (excerpt below). Attention-based models have various parameters that comprise
    their architecture. One of these is the input size, which indicates the maximum
    number of tokens in each input encoding presented to the model. For example, the
    language model BERT-large has an input size of 1024 (https://arxiv.org/pdf/1810.04805.pdf).
    The input size for Geneformer is 2048. We provide the pretraining corpus as a
    tokenized dataset in the Genecorpus-30M dataset repository (https://huggingface.co/datasets/ctheodoris/Genecorpus-30M).
    Users can tokenize their own single cell RNAseq .loom datasets using the transcriptome
    tokenizer provided in this Geneformer repository (tokenizer.py).


    Excerpt from manuscript methods discussing Geneformer model parameters:

    "Geneformer is composed of six transformer encoder units, each composed of a self-attention
    layer and feed forward neural network layer with the following parameters: input
    size of 2,048 (fully represents 93% of rank value encodings in Genecorpus-30M),
    256 embedding dimensions, four attention heads per layer and feed forward size
    of 512 (Fig. 1c). Geneformer uses full dense self-attention across the input size
    of 2,048."'
  created_at: 2023-06-07 20:45:37+00:00
  edited: false
  hidden: false
  id: 6480fa8140facadc557555b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-07T21:45:37.000Z'
    data:
      status: closed
    id: 6480fa8140facadc557555b2
    type: status-change
  author: ctheodoris
  created_at: 2023-06-07 20:45:37+00:00
  id: 6480fa8140facadc557555b2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4850ee4f9cbabc7dce4a7e876f03f69.svg
      fullname: Yongjian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yjyang027
      type: user
    createdAt: '2023-06-07T21:49:55.000Z'
    data:
      edited: false
      editors:
      - yjyang027
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411184787750244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4850ee4f9cbabc7dce4a7e876f03f69.svg
          fullname: Yongjian
          isHf: false
          isPro: false
          name: yjyang027
          type: user
        html: '<p>Thanks for your reply. Just to be sure, does your nonparametric
          rank-based input look like a vector whose elements range from 1 to 2048?</p>

          '
        raw: Thanks for your reply. Just to be sure, does your nonparametric rank-based
          input look like a vector whose elements range from 1 to 2048?
        updatedAt: '2023-06-07T21:49:55.970Z'
      numEdits: 0
      reactions: []
    id: 6480fb83e1421e205fe031b9
    type: comment
  author: yjyang027
  content: Thanks for your reply. Just to be sure, does your nonparametric rank-based
    input look like a vector whose elements range from 1 to 2048?
  created_at: 2023-06-07 20:49:55+00:00
  edited: false
  hidden: false
  id: 6480fb83e1421e205fe031b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-06-07T22:48:31.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8916231393814087
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>The rank value encoding of each single cell''s transcriptome is
          a sequence of tokens (token=gene) ordered by their rank in that particular
          given cell (please see the manuscript methods for the ranking procedure).
          The maximum length of the sequence is 2048 for this iteration of the model.
          If less than 2048 genes are detected within that single cell''s transcriptome
          in the RNAseq data, then the length of the sequence will be less than 2048.
          93% of the single cell transcriptomes in the pretraining corpus had 2048
          genes or less detected in their RNAseq dataset.</p>

          '
        raw: The rank value encoding of each single cell's transcriptome is a sequence
          of tokens (token=gene) ordered by their rank in that particular given cell
          (please see the manuscript methods for the ranking procedure). The maximum
          length of the sequence is 2048 for this iteration of the model. If less
          than 2048 genes are detected within that single cell's transcriptome in
          the RNAseq data, then the length of the sequence will be less than 2048.
          93% of the single cell transcriptomes in the pretraining corpus had 2048
          genes or less detected in their RNAseq dataset.
        updatedAt: '2023-06-07T22:48:31.832Z'
      numEdits: 0
      reactions: []
    id: 6481093fe1421e205fe0f071
    type: comment
  author: ctheodoris
  content: The rank value encoding of each single cell's transcriptome is a sequence
    of tokens (token=gene) ordered by their rank in that particular given cell (please
    see the manuscript methods for the ranking procedure). The maximum length of the
    sequence is 2048 for this iteration of the model. If less than 2048 genes are
    detected within that single cell's transcriptome in the RNAseq data, then the
    length of the sequence will be less than 2048. 93% of the single cell transcriptomes
    in the pretraining corpus had 2048 genes or less detected in their RNAseq dataset.
  created_at: 2023-06-07 21:48:31+00:00
  edited: false
  hidden: false
  id: 6481093fe1421e205fe0f071
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Input data
