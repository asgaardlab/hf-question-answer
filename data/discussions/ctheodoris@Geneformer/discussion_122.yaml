!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LudensZhang
conflicting_files: null
created_at: 2023-07-17 15:49:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b054bd608aa651b673109b9ff5f16f14.svg
      fullname: Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LudensZhang
      type: user
    createdAt: '2023-07-17T16:49:13.000Z'
    data:
      edited: false
      editors:
      - LudensZhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9367072582244873
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b054bd608aa651b673109b9ff5f16f14.svg
          fullname: Zhang
          isHf: false
          isPro: false
          name: LudensZhang
          type: user
        html: '<p>Thank you for your exceptional work. I am interested in obtaining
          the gene embedding right from the start. As far as my understanding goes,
          the rank value represents the sequential order of genes within a single
          cell, analogous to considering a cell as a sentence and each gene as a word.
          However, I have been unable to locate any description regarding the process
          of embedding each gene into 2048 dimensions as the input for transformer
          encoders. Could you please provide information on whether a word2vec method
          or any other algorithms are employed to achieve this?</p>

          '
        raw: Thank you for your exceptional work. I am interested in obtaining the
          gene embedding right from the start. As far as my understanding goes, the
          rank value represents the sequential order of genes within a single cell,
          analogous to considering a cell as a sentence and each gene as a word. However,
          I have been unable to locate any description regarding the process of embedding
          each gene into 2048 dimensions as the input for transformer encoders. Could
          you please provide information on whether a word2vec method or any other
          algorithms are employed to achieve this?
        updatedAt: '2023-07-17T16:49:13.570Z'
      numEdits: 0
      reactions: []
    id: 64b5710904fa6584c03cc6a4
    type: comment
  author: LudensZhang
  content: Thank you for your exceptional work. I am interested in obtaining the gene
    embedding right from the start. As far as my understanding goes, the rank value
    represents the sequential order of genes within a single cell, analogous to considering
    a cell as a sentence and each gene as a word. However, I have been unable to locate
    any description regarding the process of embedding each gene into 2048 dimensions
    as the input for transformer encoders. Could you please provide information on
    whether a word2vec method or any other algorithms are employed to achieve this?
  created_at: 2023-07-17 15:49:13+00:00
  edited: false
  hidden: false
  id: 64b5710904fa6584c03cc6a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-17T19:20:28.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9089916348457336
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer! 2048 is the input size
          parameter, also referred to as sequence length in NLP models. So, the input
          is 2048 tokens, and in this case each token represents a gene as indicated
          in the token dictionary. We allow the Geneformer model to embed the genes
          into the meaningful context-aware embedding space, which is 256 dimensions
          for the 6 layer model and 512 for the 12 layer model. We do not use word2vec
          for initial embeddings, as approaches like word2vec result in fixed embeddings
          that do not vary by context, and a large proportion of gene functions are
          known to be highly context-dependent. Please also see the manuscript Methods
          and the code in the transcriptome tokenizer for more information on how
          the rank value encoding is generated. A key aspect is the normalization
          factor by each gene''s expression across the entire 30M cells in the pretraining
          corpus Genecorpus-30M, which prioritizes for the model genes that highly
          distinguish cell state.</p>

          '
        raw: Thank you for your interest in Geneformer! 2048 is the input size parameter,
          also referred to as sequence length in NLP models. So, the input is 2048
          tokens, and in this case each token represents a gene as indicated in the
          token dictionary. We allow the Geneformer model to embed the genes into
          the meaningful context-aware embedding space, which is 256 dimensions for
          the 6 layer model and 512 for the 12 layer model. We do not use word2vec
          for initial embeddings, as approaches like word2vec result in fixed embeddings
          that do not vary by context, and a large proportion of gene functions are
          known to be highly context-dependent. Please also see the manuscript Methods
          and the code in the transcriptome tokenizer for more information on how
          the rank value encoding is generated. A key aspect is the normalization
          factor by each gene's expression across the entire 30M cells in the pretraining
          corpus Genecorpus-30M, which prioritizes for the model genes that highly
          distinguish cell state.
        updatedAt: '2023-07-17T19:20:28.216Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64b5947c107dfba5fd0fa8ef
    id: 64b5947c107dfba5fd0fa8ec
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer! 2048 is the input size parameter,
    also referred to as sequence length in NLP models. So, the input is 2048 tokens,
    and in this case each token represents a gene as indicated in the token dictionary.
    We allow the Geneformer model to embed the genes into the meaningful context-aware
    embedding space, which is 256 dimensions for the 6 layer model and 512 for the
    12 layer model. We do not use word2vec for initial embeddings, as approaches like
    word2vec result in fixed embeddings that do not vary by context, and a large proportion
    of gene functions are known to be highly context-dependent. Please also see the
    manuscript Methods and the code in the transcriptome tokenizer for more information
    on how the rank value encoding is generated. A key aspect is the normalization
    factor by each gene's expression across the entire 30M cells in the pretraining
    corpus Genecorpus-30M, which prioritizes for the model genes that highly distinguish
    cell state.
  created_at: 2023-07-17 18:20:28+00:00
  edited: false
  hidden: false
  id: 64b5947c107dfba5fd0fa8ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-17T19:20:28.000Z'
    data:
      status: closed
    id: 64b5947c107dfba5fd0fa8ef
    type: status-change
  author: ctheodoris
  created_at: 2023-07-17 18:20:28+00:00
  id: 64b5947c107dfba5fd0fa8ef
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 122
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Exploring Methods for Gene Embedding
