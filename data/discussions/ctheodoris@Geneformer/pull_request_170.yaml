!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ricomnl
conflicting_files: []
created_at: 2023-08-04 01:59:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-04T02:59:30.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-08-04T02:59:30.263Z'
      numEdits: 0
      reactions: []
    id: 64cc69923576a06fa26226be
    type: comment
  author: ricomnl
  content: ''
  created_at: 2023-08-04 01:59:30+00:00
  edited: false
  hidden: false
  id: 64cc69923576a06fa26226be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-04T03:03:21.000Z'
    data:
      oid: b6ca56647a16de97f6f95785eb76cb1a965f9960
      parents:
      - c4b1f940a88342268ad9e315709de6cddb1c305e
      subject: Added anndata tokenizer and switched to Dataset.from_generator
    id: 64cc6a790000000000000000
    type: commit
  author: ricomnl
  created_at: 2023-08-04 02:03:21+00:00
  id: 64cc6a790000000000000000
  oid: b6ca56647a16de97f6f95785eb76cb1a965f9960
  summary: Added anndata tokenizer and switched to Dataset.from_generator
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-04T03:07:31.000Z'
    data:
      oid: 5cb733f4fed85e1568ba9671e5b1f9babd7b8491
      parents:
      - b6ca56647a16de97f6f95785eb76cb1a965f9960
      subject: Generalized
    id: 64cc6b730000000000000000
    type: commit
  author: ricomnl
  created_at: 2023-08-04 02:07:31+00:00
  id: 64cc6b730000000000000000
  oid: 5cb733f4fed85e1568ba9671e5b1f9babd7b8491
  summary: Generalized
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-05T06:46:38.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7009885311126709
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for sharing your contribution in this pull request! </p>\n\
          <ol>\n<li>Regarding the Dataset.from_generator, when testing it with multiple\
          \ runs to get a better estimate (since runs can vary in time at random),\
          \ it unfortunately does seem to be slower than Dataset.from_dict for larger\
          \ datasets. This is perhaps to be expected given the for loop and generation\
          \ portion. Of note, Datasets caches the generator so repeated runs may lead\
          \ to the misleading impression that it is faster, while repeated generation\
          \ is not practically how users would access this function.</li>\n</ol>\n\
          <p><strong>Dataset size: from_dict vs from_generator</strong><br>Small dataset\
          \ ~5K cells: 0.071 vs 0.072<br>Medium dataset ~200K cells:  3.000 vs 20.635<br>Large\
          \ dataset ~1M cells: 15.761 vs 100.186</p>\n<p>Upon searching the Huggingface\
          \ Datasets issues though, the error in <a href=\"https://huggingface.co/ctheodoris/Geneformer/discussions/80#64cc30b5275c7630460b3d7f\"\
          >discussion </a><a href=\"/ctheodoris/Geneformer/discussions/80\">#80</a>\
          \ appears to be a known problem that they are working on resolving, so hopefully\
          \ in future versions this will not be an issue. I would like to explore\
          \ some other options but I am not encountering the error when testing datasets\
          \ with ~1M cells. Would you be able to share one of the problematic datasets\
          \ with me so that I can reproduce the error and therefore work on resolving\
          \ it?</p>\n<ol start=\"2\">\n<li>For the anndata tokenizer, I tested it\
          \ with an .h5ad dataset with ~1M cells and encountered a few errors so far.\
          \ The first error was not accounting for the possibility of no metadata\
          \ dictionary, so I added if statements to handle that case similarly to\
          \ the loom version. However, the next error is below. I believe it may be\
          \ due to not handling the case of the filter_pass existing, leading to adata_filter.X\
          \ involving the data filtered by both cells and genes on line 203 below,\
          \ while the adata.X is not filtered by cells.</li>\n</ol>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n\
          \      2 tk = TranscriptomeTokenizer(nproc=16)\n----&gt; 3 tk.tokenize_data(\"\
          /path/to/h5ad_1Mcells\", \n      4                  \"/path/to/h5ad_1Mcells/output\"\
          , \n      5                  \"tokenized_h5ad_1Mcells\",\n      6      \
          \            file_format=\"h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:117,\
          \ in TranscriptomeTokenizer.tokenize_data(self, data_directory, output_directory,\
          \ output_prefix, file_format)\n     97 def tokenize_data(\n     98     self,\n\
          \     99     data_directory: Path | str,\n   (...)\n    102     file_format:\
          \ Literal[\"loom\", \"h5ad\"] = \"loom\",\n    103 ):\n    104     \"\"\"\
          \n    105     Tokenize .loom files in loom_data_directory and save as tokenized\
          \ .dataset in output_directory.\n    106     Parameters\n   (...)\n    115\
          \         Format of input files. Can be \"loom\" or \"h5ad\".\n    116 \
          \    \"\"\"\n--&gt; 117     tokenized_cells, cell_metadata = self.tokenize_files(\n\
          \    118         Path(data_directory), file_format\n    119     )\n    120\
          \     tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata)\n\
          \    122     output_path = (Path(output_directory) / output_prefix).with_suffix(\"\
          .dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:146, in TranscriptomeTokenizer.tokenize_files(self,\
          \ data_directory, file_format)\n    144 file_found = 1\n    145 print(f\"\
          Tokenizing {file_path}\")\n--&gt; 146 file_tokenized_cells, file_cell_metadata\
          \ = tokenize_file_fn(file_path)\n    147 tokenized_cells += file_tokenized_cells\n\
          \    148 if self.custom_attr_name_dict is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:203,\
          \ in TranscriptomeTokenizer.tokenize_anndata(self, adata_file_path)\n  \
          \  198 tokenized_cells = []\n    199 adata_filter = adata[\n    200    \
          \ filter_pass_loc, coding_miRNA_loc  # filter cells and genes\n    201 ]\n\
          --&gt; 203 X_norm = (adata_filter.X / adata.X.sum(1) * 10_000 / norm_factor_vector).tocsr()\n\
          \    205 tokenized_cells += [\n    206     tokenize_cell(X_norm[i, ...].A.flatten(),\
          \ coding_miRNA_tokens)\n    207     for i in range(X_norm.shape[0])\n  \
          \  208 ]\n    210 # add custom attributes for subview to dict\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:686,\
          \ in spmatrix.__truediv__(self, other)\n    685 def __truediv__(self, other):\n\
          --&gt; 686     return self._divide(other, true_divide=True)\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:665,\
          \ in spmatrix._divide(self, other, true_divide, rdivide)\n    663 if not\
          \ rdivide:\n    664     if true_divide:\n--&gt; 665         return np.true_divide(self.todense(),\
          \ other)\n    666     else:\n    667         return np.divide(self.todense(),\
          \ other)\n\nValueError: operands could not be broadcast together with shapes\
          \ (986122,24124) (1002756,1) \n</code></pre>\n<p>However, when I add filtering\
          \ for the cells to get adata_cell_filter (see below), the operation leads\
          \ to the error that the matrix object has no attribute \"tocsr\". </p>\n\
          <pre><code>File ~/Geneformer/geneformer/tokenizer.py:209, in TranscriptomeTokenizer.tokenize_anndata(self,\
          \ adata_file_path)\n    202 adata_filter = adata[\n    203     filter_pass_loc,\
          \ coding_miRNA_loc  # filter cells and genes\n    204 ]\n    205 adata_cell_filter\
          \ = adata[\n    206     filter_pass_loc, :  # filter cells only\n    207\
          \ ]\n--&gt; 209 X_norm = (adata_filter.X / adata_cell_filter.X.sum(1) *\
          \ 10_000 / norm_factor_vector).tocsr()\n    211 tokenized_cells += [\n \
          \   212     tokenize_cell(X_norm[i, ...].A.flatten(), coding_miRNA_tokens)\n\
          \    213     for i in range(X_norm.shape[0])\n    214 ]\n    216 # add custom\
          \ attributes for subview to dict\n\nAttributeError: 'matrix' object has\
          \ no attribute 'tocsr'\n</code></pre>\n<p>It would be great if you could\
          \ take a look into resolving these for the anndata version. I'm not sure\
          \ if you encountered something similar.</p>\n<p>Additionally, I noticed\
          \ that the anndata version does not perform this operation by scanning through\
          \ the file the way that the look version does. It requires &gt;500G RAM\
          \ to perform this operation for large datasets ~1M cells. If you know of\
          \ an anndata function that would be preferable to allow scanning through\
          \ the file in chunks similar to the loom version, that would be great to\
          \ avoid memory constraints.</p>\n<p>Thank you for your collaboration on\
          \ this!</p>\n"
        raw: "Thank you for sharing your contribution in this pull request! \n\n1)\
          \ Regarding the Dataset.from_generator, when testing it with multiple runs\
          \ to get a better estimate (since runs can vary in time at random), it unfortunately\
          \ does seem to be slower than Dataset.from_dict for larger datasets. This\
          \ is perhaps to be expected given the for loop and generation portion. Of\
          \ note, Datasets caches the generator so repeated runs may lead to the misleading\
          \ impression that it is faster, while repeated generation is not practically\
          \ how users would access this function.\n\n**Dataset size: from_dict vs\
          \ from_generator**\nSmall dataset ~5K cells: 0.071 vs 0.072\nMedium dataset\
          \ ~200K cells:  3.000 vs 20.635\nLarge dataset ~1M cells: 15.761 vs 100.186\n\
          \nUpon searching the Huggingface Datasets issues though, the error in [discussion\
          \ #80](https://huggingface.co/ctheodoris/Geneformer/discussions/80#64cc30b5275c7630460b3d7f)\
          \ appears to be a known problem that they are working on resolving, so hopefully\
          \ in future versions this will not be an issue. I would like to explore\
          \ some other options but I am not encountering the error when testing datasets\
          \ with ~1M cells. Would you be able to share one of the problematic datasets\
          \ with me so that I can reproduce the error and therefore work on resolving\
          \ it?\n\n2) For the anndata tokenizer, I tested it with an .h5ad dataset\
          \ with ~1M cells and encountered a few errors so far. The first error was\
          \ not accounting for the possibility of no metadata dictionary, so I added\
          \ if statements to handle that case similarly to the loom version. However,\
          \ the next error is below. I believe it may be due to not handling the case\
          \ of the filter_pass existing, leading to adata_filter.X involving the data\
          \ filtered by both cells and genes on line 203 below, while the adata.X\
          \ is not filtered by cells. \n\n```\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n\
          \      2 tk = TranscriptomeTokenizer(nproc=16)\n----> 3 tk.tokenize_data(\"\
          /path/to/h5ad_1Mcells\", \n      4                  \"/path/to/h5ad_1Mcells/output\"\
          , \n      5                  \"tokenized_h5ad_1Mcells\",\n      6      \
          \            file_format=\"h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:117,\
          \ in TranscriptomeTokenizer.tokenize_data(self, data_directory, output_directory,\
          \ output_prefix, file_format)\n     97 def tokenize_data(\n     98     self,\n\
          \     99     data_directory: Path | str,\n   (...)\n    102     file_format:\
          \ Literal[\"loom\", \"h5ad\"] = \"loom\",\n    103 ):\n    104     \"\"\"\
          \n    105     Tokenize .loom files in loom_data_directory and save as tokenized\
          \ .dataset in output_directory.\n    106     Parameters\n   (...)\n    115\
          \         Format of input files. Can be \"loom\" or \"h5ad\".\n    116 \
          \    \"\"\"\n--> 117     tokenized_cells, cell_metadata = self.tokenize_files(\n\
          \    118         Path(data_directory), file_format\n    119     )\n    120\
          \     tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata)\n\
          \    122     output_path = (Path(output_directory) / output_prefix).with_suffix(\"\
          .dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:146, in TranscriptomeTokenizer.tokenize_files(self,\
          \ data_directory, file_format)\n    144 file_found = 1\n    145 print(f\"\
          Tokenizing {file_path}\")\n--> 146 file_tokenized_cells, file_cell_metadata\
          \ = tokenize_file_fn(file_path)\n    147 tokenized_cells += file_tokenized_cells\n\
          \    148 if self.custom_attr_name_dict is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:203,\
          \ in TranscriptomeTokenizer.tokenize_anndata(self, adata_file_path)\n  \
          \  198 tokenized_cells = []\n    199 adata_filter = adata[\n    200    \
          \ filter_pass_loc, coding_miRNA_loc  # filter cells and genes\n    201 ]\n\
          --> 203 X_norm = (adata_filter.X / adata.X.sum(1) * 10_000 / norm_factor_vector).tocsr()\n\
          \    205 tokenized_cells += [\n    206     tokenize_cell(X_norm[i, ...].A.flatten(),\
          \ coding_miRNA_tokens)\n    207     for i in range(X_norm.shape[0])\n  \
          \  208 ]\n    210 # add custom attributes for subview to dict\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:686,\
          \ in spmatrix.__truediv__(self, other)\n    685 def __truediv__(self, other):\n\
          --> 686     return self._divide(other, true_divide=True)\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:665,\
          \ in spmatrix._divide(self, other, true_divide, rdivide)\n    663 if not\
          \ rdivide:\n    664     if true_divide:\n--> 665         return np.true_divide(self.todense(),\
          \ other)\n    666     else:\n    667         return np.divide(self.todense(),\
          \ other)\n\nValueError: operands could not be broadcast together with shapes\
          \ (986122,24124) (1002756,1) \n```\n\nHowever, when I add filtering for\
          \ the cells to get adata_cell_filter (see below), the operation leads to\
          \ the error that the matrix object has no attribute \"tocsr\". \n\n```\n\
          File ~/Geneformer/geneformer/tokenizer.py:209, in TranscriptomeTokenizer.tokenize_anndata(self,\
          \ adata_file_path)\n    202 adata_filter = adata[\n    203     filter_pass_loc,\
          \ coding_miRNA_loc  # filter cells and genes\n    204 ]\n    205 adata_cell_filter\
          \ = adata[\n    206     filter_pass_loc, :  # filter cells only\n    207\
          \ ]\n--> 209 X_norm = (adata_filter.X / adata_cell_filter.X.sum(1) * 10_000\
          \ / norm_factor_vector).tocsr()\n    211 tokenized_cells += [\n    212 \
          \    tokenize_cell(X_norm[i, ...].A.flatten(), coding_miRNA_tokens)\n  \
          \  213     for i in range(X_norm.shape[0])\n    214 ]\n    216 # add custom\
          \ attributes for subview to dict\n\nAttributeError: 'matrix' object has\
          \ no attribute 'tocsr'\n```\n\nIt would be great if you could take a look\
          \ into resolving these for the anndata version. I'm not sure if you encountered\
          \ something similar.\n\nAdditionally, I noticed that the anndata version\
          \ does not perform this operation by scanning through the file the way that\
          \ the look version does. It requires >500G RAM to perform this operation\
          \ for large datasets ~1M cells. If you know of an anndata function that\
          \ would be preferable to allow scanning through the file in chunks similar\
          \ to the loom version, that would be great to avoid memory constraints.\n\
          \nThank you for your collaboration on this!"
        updatedAt: '2023-08-05T06:48:04.748Z'
      numEdits: 2
      reactions: []
    id: 64cdf04ea785f2043b48ef92
    type: comment
  author: ctheodoris
  content: "Thank you for sharing your contribution in this pull request! \n\n1) Regarding\
    \ the Dataset.from_generator, when testing it with multiple runs to get a better\
    \ estimate (since runs can vary in time at random), it unfortunately does seem\
    \ to be slower than Dataset.from_dict for larger datasets. This is perhaps to\
    \ be expected given the for loop and generation portion. Of note, Datasets caches\
    \ the generator so repeated runs may lead to the misleading impression that it\
    \ is faster, while repeated generation is not practically how users would access\
    \ this function.\n\n**Dataset size: from_dict vs from_generator**\nSmall dataset\
    \ ~5K cells: 0.071 vs 0.072\nMedium dataset ~200K cells:  3.000 vs 20.635\nLarge\
    \ dataset ~1M cells: 15.761 vs 100.186\n\nUpon searching the Huggingface Datasets\
    \ issues though, the error in [discussion #80](https://huggingface.co/ctheodoris/Geneformer/discussions/80#64cc30b5275c7630460b3d7f)\
    \ appears to be a known problem that they are working on resolving, so hopefully\
    \ in future versions this will not be an issue. I would like to explore some other\
    \ options but I am not encountering the error when testing datasets with ~1M cells.\
    \ Would you be able to share one of the problematic datasets with me so that I\
    \ can reproduce the error and therefore work on resolving it?\n\n2) For the anndata\
    \ tokenizer, I tested it with an .h5ad dataset with ~1M cells and encountered\
    \ a few errors so far. The first error was not accounting for the possibility\
    \ of no metadata dictionary, so I added if statements to handle that case similarly\
    \ to the loom version. However, the next error is below. I believe it may be due\
    \ to not handling the case of the filter_pass existing, leading to adata_filter.X\
    \ involving the data filtered by both cells and genes on line 203 below, while\
    \ the adata.X is not filtered by cells. \n\n```\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n  \
    \    2 tk = TranscriptomeTokenizer(nproc=16)\n----> 3 tk.tokenize_data(\"/path/to/h5ad_1Mcells\"\
    , \n      4                  \"/path/to/h5ad_1Mcells/output\", \n      5     \
    \             \"tokenized_h5ad_1Mcells\",\n      6                  file_format=\"\
    h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:117, in TranscriptomeTokenizer.tokenize_data(self,\
    \ data_directory, output_directory, output_prefix, file_format)\n     97 def tokenize_data(\n\
    \     98     self,\n     99     data_directory: Path | str,\n   (...)\n    102\
    \     file_format: Literal[\"loom\", \"h5ad\"] = \"loom\",\n    103 ):\n    104\
    \     \"\"\"\n    105     Tokenize .loom files in loom_data_directory and save\
    \ as tokenized .dataset in output_directory.\n    106     Parameters\n   (...)\n\
    \    115         Format of input files. Can be \"loom\" or \"h5ad\".\n    116\
    \     \"\"\"\n--> 117     tokenized_cells, cell_metadata = self.tokenize_files(\n\
    \    118         Path(data_directory), file_format\n    119     )\n    120   \
    \  tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata)\n \
    \   122     output_path = (Path(output_directory) / output_prefix).with_suffix(\"\
    .dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:146, in TranscriptomeTokenizer.tokenize_files(self,\
    \ data_directory, file_format)\n    144 file_found = 1\n    145 print(f\"Tokenizing\
    \ {file_path}\")\n--> 146 file_tokenized_cells, file_cell_metadata = tokenize_file_fn(file_path)\n\
    \    147 tokenized_cells += file_tokenized_cells\n    148 if self.custom_attr_name_dict\
    \ is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:203, in TranscriptomeTokenizer.tokenize_anndata(self,\
    \ adata_file_path)\n    198 tokenized_cells = []\n    199 adata_filter = adata[\n\
    \    200     filter_pass_loc, coding_miRNA_loc  # filter cells and genes\n   \
    \ 201 ]\n--> 203 X_norm = (adata_filter.X / adata.X.sum(1) * 10_000 / norm_factor_vector).tocsr()\n\
    \    205 tokenized_cells += [\n    206     tokenize_cell(X_norm[i, ...].A.flatten(),\
    \ coding_miRNA_tokens)\n    207     for i in range(X_norm.shape[0])\n    208 ]\n\
    \    210 # add custom attributes for subview to dict\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:686,\
    \ in spmatrix.__truediv__(self, other)\n    685 def __truediv__(self, other):\n\
    --> 686     return self._divide(other, true_divide=True)\n\nFile ~/miniconda3/lib/python3.10/site-packages/scipy/sparse/_base.py:665,\
    \ in spmatrix._divide(self, other, true_divide, rdivide)\n    663 if not rdivide:\n\
    \    664     if true_divide:\n--> 665         return np.true_divide(self.todense(),\
    \ other)\n    666     else:\n    667         return np.divide(self.todense(),\
    \ other)\n\nValueError: operands could not be broadcast together with shapes (986122,24124)\
    \ (1002756,1) \n```\n\nHowever, when I add filtering for the cells to get adata_cell_filter\
    \ (see below), the operation leads to the error that the matrix object has no\
    \ attribute \"tocsr\". \n\n```\nFile ~/Geneformer/geneformer/tokenizer.py:209,\
    \ in TranscriptomeTokenizer.tokenize_anndata(self, adata_file_path)\n    202 adata_filter\
    \ = adata[\n    203     filter_pass_loc, coding_miRNA_loc  # filter cells and\
    \ genes\n    204 ]\n    205 adata_cell_filter = adata[\n    206     filter_pass_loc,\
    \ :  # filter cells only\n    207 ]\n--> 209 X_norm = (adata_filter.X / adata_cell_filter.X.sum(1)\
    \ * 10_000 / norm_factor_vector).tocsr()\n    211 tokenized_cells += [\n    212\
    \     tokenize_cell(X_norm[i, ...].A.flatten(), coding_miRNA_tokens)\n    213\
    \     for i in range(X_norm.shape[0])\n    214 ]\n    216 # add custom attributes\
    \ for subview to dict\n\nAttributeError: 'matrix' object has no attribute 'tocsr'\n\
    ```\n\nIt would be great if you could take a look into resolving these for the\
    \ anndata version. I'm not sure if you encountered something similar.\n\nAdditionally,\
    \ I noticed that the anndata version does not perform this operation by scanning\
    \ through the file the way that the look version does. It requires >500G RAM to\
    \ perform this operation for large datasets ~1M cells. If you know of an anndata\
    \ function that would be preferable to allow scanning through the file in chunks\
    \ similar to the loom version, that would be great to avoid memory constraints.\n\
    \nThank you for your collaboration on this!"
  created_at: 2023-08-05 05:46:38+00:00
  edited: true
  hidden: false
  id: 64cdf04ea785f2043b48ef92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-11T18:09:34.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8729538917541504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: "<blockquote>\n<p>The first error was not accounting for the possibility\
          \ of no metadata dictionary, so I added if statements to handle that case\
          \ similarly to the loom version</p>\n</blockquote>\n<p>What arguments did\
          \ you pass to the TranscriptomeTokenizer that made those if statements necessary?\
          \ I ran it with:</p>\n<pre><code class=\"language-python\">tk = TranscriptomeTokenizer({})\n\
          \n<span class=\"hljs-comment\"># and</span>\n\ntk = TranscriptomeTokenizer({<span\
          \ class=\"hljs-string\">\"cell_type\"</span>: <span class=\"hljs-string\"\
          >\"cell_type\"</span>}) \n</code></pre>\n<p>and both worked fine</p>\n"
        raw: "> The first error was not accounting for the possibility of no metadata\
          \ dictionary, so I added if statements to handle that case similarly to\
          \ the loom version\n\nWhat arguments did you pass to the TranscriptomeTokenizer\
          \ that made those if statements necessary? I ran it with:\n```python\ntk\
          \ = TranscriptomeTokenizer({})\n\n# and\n\ntk = TranscriptomeTokenizer({\"\
          cell_type\": \"cell_type\"}) \n```\nand both worked fine"
        updatedAt: '2023-08-11T18:09:34.718Z'
      numEdits: 0
      reactions: []
    id: 64d6795e67c967b015c6f2ec
    type: comment
  author: ricomnl
  content: "> The first error was not accounting for the possibility of no metadata\
    \ dictionary, so I added if statements to handle that case similarly to the loom\
    \ version\n\nWhat arguments did you pass to the TranscriptomeTokenizer that made\
    \ those if statements necessary? I ran it with:\n```python\ntk = TranscriptomeTokenizer({})\n\
    \n# and\n\ntk = TranscriptomeTokenizer({\"cell_type\": \"cell_type\"}) \n```\n\
    and both worked fine"
  created_at: 2023-08-11 17:09:34+00:00
  edited: false
  hidden: false
  id: 64d6795e67c967b015c6f2ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-11T18:22:23.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.818760335445404
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: '<p>I also did not run into the 2nd issue you mentioned above. For me,
          changing line 203 in <code>tokenizer.py</code> to:</p>

          <pre><code class="language-python">X_norm = (adata_filter.X / adata[filter_pass_loc].X.<span
          class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) * <span
          class="hljs-number">10_000</span> / norm_factor_vector).tocsr()

          </code></pre>

          <p>made it work.</p>

          <p>My relevant package versions are:</p>

          <pre><code class="language-bash">anndata==0.9.1

          arrow==1.2.3

          datasets==2.13.1

          numpy==1.24.3

          pyarrow==12.0.1

          scipy==1.11.0

          torch==2.0.1

          transformers==4.30.2

          </code></pre>

          <p>Which versions are you using?</p>

          '
        raw: 'I also did not run into the 2nd issue you mentioned above. For me, changing
          line 203 in `tokenizer.py` to:

          ```python

          X_norm = (adata_filter.X / adata[filter_pass_loc].X.sum(1) * 10_000 / norm_factor_vector).tocsr()

          ```

          made it work.


          My relevant package versions are:

          ```bash

          anndata==0.9.1

          arrow==1.2.3

          datasets==2.13.1

          numpy==1.24.3

          pyarrow==12.0.1

          scipy==1.11.0

          torch==2.0.1

          transformers==4.30.2

          ```

          Which versions are you using?'
        updatedAt: '2023-08-11T18:22:23.478Z'
      numEdits: 0
      reactions: []
    id: 64d67c5f7705391d3944566a
    type: comment
  author: ricomnl
  content: 'I also did not run into the 2nd issue you mentioned above. For me, changing
    line 203 in `tokenizer.py` to:

    ```python

    X_norm = (adata_filter.X / adata[filter_pass_loc].X.sum(1) * 10_000 / norm_factor_vector).tocsr()

    ```

    made it work.


    My relevant package versions are:

    ```bash

    anndata==0.9.1

    arrow==1.2.3

    datasets==2.13.1

    numpy==1.24.3

    pyarrow==12.0.1

    scipy==1.11.0

    torch==2.0.1

    transformers==4.30.2

    ```

    Which versions are you using?'
  created_at: 2023-08-11 17:22:23+00:00
  edited: false
  hidden: false
  id: 64d67c5f7705391d3944566a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-12T00:07:13.000Z'
    data:
      oid: b24676d0956565ff7b4f8b57d1e936ad17f8740e
      parents:
      - 5cb733f4fed85e1568ba9671e5b1f9babd7b8491
      subject: Addressed issues for tokenizer, anndata tokenizer now uses a fraction
        of memory
    id: 64d6cd310000000000000000
    type: commit
  author: ricomnl
  created_at: 2023-08-11 23:07:13+00:00
  id: 64d6cd310000000000000000
  oid: b24676d0956565ff7b4f8b57d1e936ad17f8740e
  summary: Addressed issues for tokenizer, anndata tokenizer now uses a fraction of
    memory
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-08-12T00:10:09.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7596344351768494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ctheodoris&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ctheodoris\">@<span class=\"\
          underline\">ctheodoris</span></a></span>\n\n\t</span></span> I addressed\
          \ the issues you mentioned above by casting the matrix to a csr matrix,\
          \ scanning through the anndata object (in \"backed\" mode) instead of loading\
          \ it into memory and using a parameter to decide whether to use <code>from_dict</code>\
          \ or <code>from_generator</code></p>\n"
        raw: '@ctheodoris I addressed the issues you mentioned above by casting the
          matrix to a csr matrix, scanning through the anndata object (in "backed"
          mode) instead of loading it into memory and using a parameter to decide
          whether to use `from_dict` or `from_generator`'
        updatedAt: '2023-08-12T00:10:09.956Z'
      numEdits: 0
      reactions: []
    id: 64d6cde1089bc502ceab2443
    type: comment
  author: ricomnl
  content: '@ctheodoris I addressed the issues you mentioned above by casting the
    matrix to a csr matrix, scanning through the anndata object (in "backed" mode)
    instead of loading it into memory and using a parameter to decide whether to use
    `from_dict` or `from_generator`'
  created_at: 2023-08-11 23:10:09+00:00
  edited: false
  hidden: false
  id: 64d6cde1089bc502ceab2443
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-14T00:01:39.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.810737133026123
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you so much! That\u2019s wonderful. I\u2019m currently away\
          \ with limited internet access so will test the updated version when I return\
          \ and merge it if all looks good. Thank you for your key contribution to\
          \ the code base!</p>\n"
        raw: "Thank you so much! That\u2019s wonderful. I\u2019m currently away with\
          \ limited internet access so will test the updated version when I return\
          \ and merge it if all looks good. Thank you for your key contribution to\
          \ the code base!"
        updatedAt: '2023-08-14T00:01:39.994Z'
      numEdits: 0
      reactions: []
    id: 64d96ee39f48c6ff12f7758f
    type: comment
  author: ctheodoris
  content: "Thank you so much! That\u2019s wonderful. I\u2019m currently away with\
    \ limited internet access so will test the updated version when I return and merge\
    \ it if all looks good. Thank you for your key contribution to the code base!"
  created_at: 2023-08-13 23:01:39+00:00
  edited: false
  hidden: false
  id: 64d96ee39f48c6ff12f7758f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-08-23T19:50:52.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5090017318725586
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you again for your collaboration on this. I have returned\
          \ and am testing out the new version. A couple of remaining issues:</p>\n\
          <ol>\n<li>The default for custom_attr_name_dict is None so if it isn't specified\
          \ (see how I ran the code in the error trace below) the anndata tokenizer\
          \ has an error. There are if statements in the loom version that account\
          \ for this. I added them to resolve this:</li>\n</ol>\n<p>Changes to add:<br>Lines\
          \ 168-171:</p>\n<pre><code>        if self.custom_attr_name_dict is not\
          \ None:\n            file_cell_metadata = {\n                attr_key: []\
          \ for attr_key in self.custom_attr_name_dict.keys()\n            }\n</code></pre>\n\
          <p>Lines 218-223:</p>\n<pre><code>            # add custom attributes for\
          \ subview to dict\n            if self.custom_attr_name_dict is not None:\n\
          \                for k in file_cell_metadata.keys():\n                 \
          \   file_cell_metadata[k] += adata[idx].obs[k].tolist()\n            else:\n\
          \                file_cell_metadata = None\n</code></pre>\n<ol start=\"\
          2\">\n<li>I encountered the error below when calculating the X_norm. Are\
          \ you able to resolve this? (see how I ran the code in the error trace below;\
          \ the anndata file has a filter_pass cell attribute)</li>\n</ol>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n\
          \      2 tk = TranscriptomeTokenizer(nproc=16)\n----&gt; 3 tk.tokenize_data(\"\
          /path/to/h5ad_1Mcells\", \n      4                  \"/path/to/h5ad_1Mcells/output\"\
          , \n      5                  \"tokenized_h5ad_1Mcells\",\n      6      \
          \            file_format=\"h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:128,\
          \ in TranscriptomeTokenizer.tokenize_data(self, data_directory, output_directory,\
          \ output_prefix, file_format, use_generator)\n    105 def tokenize_data(\n\
          \    106     self,\n    107     data_directory: Path | str,\n   (...)\n\
          \    111     use_generator: bool = False,\n    112 ):\n    113     \"\"\"\
          \n    114     Tokenize .loom files in loom_data_directory and save as tokenized\
          \ .dataset in output_directory.\n    115     Parameters\n   (...)\n    126\
          \         Whether to use generator or dict for tokenization.\n    127  \
          \   \"\"\"\n--&gt; 128     tokenized_cells, cell_metadata = self.tokenize_files(\n\
          \    129         Path(data_directory), file_format\n    130     )\n    131\
          \     tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata,\
          \ use_generator=use_generator)\n    133     output_path = (Path(output_directory)\
          \ / output_prefix).with_suffix(\".dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:152,\
          \ in TranscriptomeTokenizer.tokenize_files(self, data_directory, file_format)\n\
          \    150 file_found = 1\n    151 print(f\"Tokenizing {file_path}\")\n--&gt;\
          \ 152 file_tokenized_cells, file_cell_metadata = tokenize_file_fn(file_path)\n\
          \    153 tokenized_cells += file_tokenized_cells\n    154 if self.custom_attr_name_dict\
          \ is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:210, in TranscriptomeTokenizer.tokenize_anndata(self,\
          \ adata_file_path, target_sum, chunk_size)\n    207 idx = filter_pass_loc[i:i+chunk_size]\n\
          \    208 X = adata[idx].X\n--&gt; 210 X_norm = (X / X[:, coding_miRNA_loc].sum(axis=1)\
          \ * target_sum / norm_factor_vector)\n    211 X_norm = sp.csr_matrix(X_norm)\n\
          \    213 tokenized_cells += [\n    214     rank_genes(X_norm[i].data, coding_miRNA_tokens[X_norm[i].indices])\n\
          \    215     for i in range(X_norm.shape[0])\n    216 ]\n\nValueError: operands\
          \ could not be broadcast together with shapes (512,63561) (24124,) \n</code></pre>\n"
        raw: "Thank you again for your collaboration on this. I have returned and\
          \ am testing out the new version. A couple of remaining issues:\n\n1) The\
          \ default for custom_attr_name_dict is None so if it isn't specified (see\
          \ how I ran the code in the error trace below) the anndata tokenizer has\
          \ an error. There are if statements in the loom version that account for\
          \ this. I added them to resolve this:\n\nChanges to add:\nLines 168-171:\n\
          ```\n        if self.custom_attr_name_dict is not None:\n            file_cell_metadata\
          \ = {\n                attr_key: [] for attr_key in self.custom_attr_name_dict.keys()\n\
          \            }\n```\nLines 218-223:\n```\n            # add custom attributes\
          \ for subview to dict\n            if self.custom_attr_name_dict is not\
          \ None:\n                for k in file_cell_metadata.keys():\n         \
          \           file_cell_metadata[k] += adata[idx].obs[k].tolist()\n      \
          \      else:\n                file_cell_metadata = None\n```\n2) I encountered\
          \ the error below when calculating the X_norm. Are you able to resolve this?\
          \ (see how I ran the code in the error trace below; the anndata file has\
          \ a filter_pass cell attribute)\n\n```\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n\
          \      2 tk = TranscriptomeTokenizer(nproc=16)\n----> 3 tk.tokenize_data(\"\
          /path/to/h5ad_1Mcells\", \n      4                  \"/path/to/h5ad_1Mcells/output\"\
          , \n      5                  \"tokenized_h5ad_1Mcells\",\n      6      \
          \            file_format=\"h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:128,\
          \ in TranscriptomeTokenizer.tokenize_data(self, data_directory, output_directory,\
          \ output_prefix, file_format, use_generator)\n    105 def tokenize_data(\n\
          \    106     self,\n    107     data_directory: Path | str,\n   (...)\n\
          \    111     use_generator: bool = False,\n    112 ):\n    113     \"\"\"\
          \n    114     Tokenize .loom files in loom_data_directory and save as tokenized\
          \ .dataset in output_directory.\n    115     Parameters\n   (...)\n    126\
          \         Whether to use generator or dict for tokenization.\n    127  \
          \   \"\"\"\n--> 128     tokenized_cells, cell_metadata = self.tokenize_files(\n\
          \    129         Path(data_directory), file_format\n    130     )\n    131\
          \     tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata,\
          \ use_generator=use_generator)\n    133     output_path = (Path(output_directory)\
          \ / output_prefix).with_suffix(\".dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:152,\
          \ in TranscriptomeTokenizer.tokenize_files(self, data_directory, file_format)\n\
          \    150 file_found = 1\n    151 print(f\"Tokenizing {file_path}\")\n-->\
          \ 152 file_tokenized_cells, file_cell_metadata = tokenize_file_fn(file_path)\n\
          \    153 tokenized_cells += file_tokenized_cells\n    154 if self.custom_attr_name_dict\
          \ is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:210, in TranscriptomeTokenizer.tokenize_anndata(self,\
          \ adata_file_path, target_sum, chunk_size)\n    207 idx = filter_pass_loc[i:i+chunk_size]\n\
          \    208 X = adata[idx].X\n--> 210 X_norm = (X / X[:, coding_miRNA_loc].sum(axis=1)\
          \ * target_sum / norm_factor_vector)\n    211 X_norm = sp.csr_matrix(X_norm)\n\
          \    213 tokenized_cells += [\n    214     rank_genes(X_norm[i].data, coding_miRNA_tokens[X_norm[i].indices])\n\
          \    215     for i in range(X_norm.shape[0])\n    216 ]\n\nValueError: operands\
          \ could not be broadcast together with shapes (512,63561) (24124,) \n```\n"
        updatedAt: '2023-08-23T19:51:47.028Z'
      numEdits: 1
      reactions: []
    id: 64e6631c032d13c348bb4793
    type: comment
  author: ctheodoris
  content: "Thank you again for your collaboration on this. I have returned and am\
    \ testing out the new version. A couple of remaining issues:\n\n1) The default\
    \ for custom_attr_name_dict is None so if it isn't specified (see how I ran the\
    \ code in the error trace below) the anndata tokenizer has an error. There are\
    \ if statements in the loom version that account for this. I added them to resolve\
    \ this:\n\nChanges to add:\nLines 168-171:\n```\n        if self.custom_attr_name_dict\
    \ is not None:\n            file_cell_metadata = {\n                attr_key:\
    \ [] for attr_key in self.custom_attr_name_dict.keys()\n            }\n```\nLines\
    \ 218-223:\n```\n            # add custom attributes for subview to dict\n   \
    \         if self.custom_attr_name_dict is not None:\n                for k in\
    \ file_cell_metadata.keys():\n                    file_cell_metadata[k] += adata[idx].obs[k].tolist()\n\
    \            else:\n                file_cell_metadata = None\n```\n2) I encountered\
    \ the error below when calculating the X_norm. Are you able to resolve this? (see\
    \ how I ran the code in the error trace below; the anndata file has a filter_pass\
    \ cell attribute)\n\n```\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[1], line 3\n      1 from geneformer import TranscriptomeTokenizer\n  \
    \    2 tk = TranscriptomeTokenizer(nproc=16)\n----> 3 tk.tokenize_data(\"/path/to/h5ad_1Mcells\"\
    , \n      4                  \"/path/to/h5ad_1Mcells/output\", \n      5     \
    \             \"tokenized_h5ad_1Mcells\",\n      6                  file_format=\"\
    h5ad\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:128, in TranscriptomeTokenizer.tokenize_data(self,\
    \ data_directory, output_directory, output_prefix, file_format, use_generator)\n\
    \    105 def tokenize_data(\n    106     self,\n    107     data_directory: Path\
    \ | str,\n   (...)\n    111     use_generator: bool = False,\n    112 ):\n   \
    \ 113     \"\"\"\n    114     Tokenize .loom files in loom_data_directory and\
    \ save as tokenized .dataset in output_directory.\n    115     Parameters\n  \
    \ (...)\n    126         Whether to use generator or dict for tokenization.\n\
    \    127     \"\"\"\n--> 128     tokenized_cells, cell_metadata = self.tokenize_files(\n\
    \    129         Path(data_directory), file_format\n    130     )\n    131   \
    \  tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata, use_generator=use_generator)\n\
    \    133     output_path = (Path(output_directory) / output_prefix).with_suffix(\"\
    .dataset\")\n\nFile ~/Geneformer/geneformer/tokenizer.py:152, in TranscriptomeTokenizer.tokenize_files(self,\
    \ data_directory, file_format)\n    150 file_found = 1\n    151 print(f\"Tokenizing\
    \ {file_path}\")\n--> 152 file_tokenized_cells, file_cell_metadata = tokenize_file_fn(file_path)\n\
    \    153 tokenized_cells += file_tokenized_cells\n    154 if self.custom_attr_name_dict\
    \ is not None:\n\nFile ~/Geneformer/geneformer/tokenizer.py:210, in TranscriptomeTokenizer.tokenize_anndata(self,\
    \ adata_file_path, target_sum, chunk_size)\n    207 idx = filter_pass_loc[i:i+chunk_size]\n\
    \    208 X = adata[idx].X\n--> 210 X_norm = (X / X[:, coding_miRNA_loc].sum(axis=1)\
    \ * target_sum / norm_factor_vector)\n    211 X_norm = sp.csr_matrix(X_norm)\n\
    \    213 tokenized_cells += [\n    214     rank_genes(X_norm[i].data, coding_miRNA_tokens[X_norm[i].indices])\n\
    \    215     for i in range(X_norm.shape[0])\n    216 ]\n\nValueError: operands\
    \ could not be broadcast together with shapes (512,63561) (24124,) \n```\n"
  created_at: 2023-08-23 18:50:52+00:00
  edited: true
  hidden: false
  id: 64e6631c032d13c348bb4793
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-09-05T21:14:49.000Z'
    data:
      oid: 94e8d231c41a89232a55c064e5c8ea95edd51d2e
      parents:
      - b24676d0956565ff7b4f8b57d1e936ad17f8740e
      subject: Fixed issues
    id: 64f79a490000000000000000
    type: commit
  author: ricomnl
  created_at: 2023-09-05 20:14:49+00:00
  id: 64f79a490000000000000000
  oid: 94e8d231c41a89232a55c064e5c8ea95edd51d2e
  summary: Fixed issues
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-09-05T21:16:32.000Z'
    data:
      edited: false
      editors:
      - ricomnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9531711935997009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
          fullname: Rico Meinl
          isHf: false
          isPro: false
          name: ricomnl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ctheodoris&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ctheodoris\">@<span class=\"\
          underline\">ctheodoris</span></a></span>\n\n\t</span></span> I addressed\
          \ your issues, let me know if they're fixed</p>\n"
        raw: '@ctheodoris I addressed your issues, let me know if they''re fixed'
        updatedAt: '2023-09-05T21:16:32.522Z'
      numEdits: 0
      reactions: []
    id: 64f79ab0b7c50aef83a0a781
    type: comment
  author: ricomnl
  content: '@ctheodoris I addressed your issues, let me know if they''re fixed'
  created_at: 2023-09-05 20:16:32+00:00
  edited: false
  hidden: false
  id: 64f79ab0b7c50aef83a0a781
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-13T07:14:20.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8910178542137146
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you so much for addressing these! Indeed the code is able\
          \ to run now. However, when I check the results, it seems they are not the\
          \ same between the anndata and loom versions unfortunately. I used scanpy\
          \ to convert a .loom file to an .h5ad file so they would be the same, and\
          \ then ran them each through the transcriptome tokenizer, either specifying\
          \ to use the anndata version or not specifying a file type (thereby using\
          \ the default loom one). When I use the following to create a checksum column\
          \ in the datasets, and then create a set out of the checksum column, the\
          \ two sets are not the same. They have the same number of entries (cells)\
          \ but the checksum itself is not the same (each set has 986122 cells, but\
          \ then merging them has more than 986122 (986320), indicating that some\
          \ are not the same). I am happy to send you these input/output datasets\
          \ if you email me so we can troubleshoot the reasons behind this. Did you\
          \ check the outputs previously and find they were the same though?</p>\n\
          <pre><code>def create_checksum(example):\n    example[\"checksum\"] = hash(tuple(example[\"\
          input_ids\"]))\n    return example\ntest_dataset = test_dataset.map(create_checksum,\
          \ num_proc=16)\n</code></pre>\n<p>Thank you again for your collaboration\
          \ on this!</p>\n"
        raw: "Thank you so much for addressing these! Indeed the code is able to run\
          \ now. However, when I check the results, it seems they are not the same\
          \ between the anndata and loom versions unfortunately. I used scanpy to\
          \ convert a .loom file to an .h5ad file so they would be the same, and then\
          \ ran them each through the transcriptome tokenizer, either specifying to\
          \ use the anndata version or not specifying a file type (thereby using the\
          \ default loom one). When I use the following to create a checksum column\
          \ in the datasets, and then create a set out of the checksum column, the\
          \ two sets are not the same. They have the same number of entries (cells)\
          \ but the checksum itself is not the same (each set has 986122 cells, but\
          \ then merging them has more than 986122 (986320), indicating that some\
          \ are not the same). I am happy to send you these input/output datasets\
          \ if you email me so we can troubleshoot the reasons behind this. Did you\
          \ check the outputs previously and find they were the same though?\n\n```\n\
          def create_checksum(example):\n    example[\"checksum\"] = hash(tuple(example[\"\
          input_ids\"]))\n    return example\ntest_dataset = test_dataset.map(create_checksum,\
          \ num_proc=16)\n```\n\nThank you again for your collaboration on this!"
        updatedAt: '2023-09-13T07:14:20.818Z'
      numEdits: 0
      reactions: []
    id: 6501614c68c6cc778c062475
    type: comment
  author: ctheodoris
  content: "Thank you so much for addressing these! Indeed the code is able to run\
    \ now. However, when I check the results, it seems they are not the same between\
    \ the anndata and loom versions unfortunately. I used scanpy to convert a .loom\
    \ file to an .h5ad file so they would be the same, and then ran them each through\
    \ the transcriptome tokenizer, either specifying to use the anndata version or\
    \ not specifying a file type (thereby using the default loom one). When I use\
    \ the following to create a checksum column in the datasets, and then create a\
    \ set out of the checksum column, the two sets are not the same. They have the\
    \ same number of entries (cells) but the checksum itself is not the same (each\
    \ set has 986122 cells, but then merging them has more than 986122 (986320), indicating\
    \ that some are not the same). I am happy to send you these input/output datasets\
    \ if you email me so we can troubleshoot the reasons behind this. Did you check\
    \ the outputs previously and find they were the same though?\n\n```\ndef create_checksum(example):\n\
    \    example[\"checksum\"] = hash(tuple(example[\"input_ids\"]))\n    return example\n\
    test_dataset = test_dataset.map(create_checksum, num_proc=16)\n```\n\nThank you\
    \ again for your collaboration on this!"
  created_at: 2023-09-13 06:14:20+00:00
  edited: false
  hidden: false
  id: 6501614c68c6cc778c062475
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/e4be47b76e1d02bf20a381b92ed677f0.svg
      fullname: Rico Meinl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricomnl
      type: user
    createdAt: '2023-09-19T04:57:43.000Z'
    data:
      oid: 9c62f4c6e3ab0afd1943079306655bd20eba2246
      parents:
      - 94e8d231c41a89232a55c064e5c8ea95edd51d2e
      subject: Fixed issue, loom and h5ad now produce same checksums
    id: 65092a470000000000000000
    type: commit
  author: ricomnl
  created_at: 2023-09-19 03:57:43+00:00
  id: 65092a470000000000000000
  oid: 9c62f4c6e3ab0afd1943079306655bd20eba2246
  summary: Fixed issue, loom and h5ad now produce same checksums
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    createdAt: '2023-09-20T19:58:59.000Z'
    data:
      oid: 4fdb85061180ccf022ab761c2557be6ba67d848e
      parents:
      - 9c62f4c6e3ab0afd1943079306655bd20eba2246
      subject: Rename loom tokenizer function and modify example notebook for adata.
    id: 650b4f030000000000000000
    type: commit
  author: deleted
  created_at: 2023-09-20 18:58:59+00:00
  id: 650b4f030000000000000000
  oid: 4fdb85061180ccf022ab761c2557be6ba67d848e
  summary: Rename loom tokenizer function and modify example notebook for adata.
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-20T20:55:37.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9431131482124329
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Looks great, checksums match - thank you so much for all your collaboration
          on this. It is a valuable contribution that will be helpful to many researchers.</p>

          '
        raw: Looks great, checksums match - thank you so much for all your collaboration
          on this. It is a valuable contribution that will be helpful to many researchers.
        updatedAt: '2023-09-20T20:55:37.190Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650b5c4901d6078e7515223e
    id: 650b5c4901d6078e75152239
    type: comment
  author: ctheodoris
  content: Looks great, checksums match - thank you so much for all your collaboration
    on this. It is a valuable contribution that will be helpful to many researchers.
  created_at: 2023-09-20 19:55:37+00:00
  edited: false
  hidden: false
  id: 650b5c4901d6078e75152239
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-20T20:55:37.000Z'
    data:
      status: open
    id: 650b5c4901d6078e7515223e
    type: status-change
  author: ctheodoris
  created_at: 2023-09-20 19:55:37+00:00
  id: 650b5c4901d6078e7515223e
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-09-20T20:55:44.000Z'
    data:
      status: merged
    id: 650b5c507c99ca283e48d59c
    type: status-change
  author: ctheodoris
  created_at: 2023-09-20 19:55:44+00:00
  id: 650b5c507c99ca283e48d59c
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 4302f4835eda5320b13de85092c97f2c6679b36e
num: 170
repo_id: ctheodoris/Geneformer
repo_type: model
status: merged
target_branch: refs/heads/main
title: anndata_tokenizer
