!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pchiang5
conflicting_files: null
created_at: 2023-07-05 00:36:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
      fullname: PC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pchiang5
      type: user
    createdAt: '2023-07-05T01:36:13.000Z'
    data:
      edited: false
      editors:
      - pchiang5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9021555185317993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1092f31a9ffd1e9813369b91311f765a.svg
          fullname: PC
          isHf: false
          isPro: false
          name: pchiang5
          type: user
        html: '<p>Thank you for the great tool. I found it is possible to use DeepSpeed
          to perform pertaining in the example. Is it possible to extend this tool
          with fine-tuning and inference as well? Although my installation of zero
          3 offload worked with other larger models, directly including <code>straining_args_init
          = TrainingArguments(**training_args, deepspeed =''ds_config_zero3.json'')</code>
          in <code>cell classification</code> example did not work. If DeepSpeed could
          be used with the other steps, those with limited computing resources could
          analyze with the 12L model. </p>

          '
        raw: 'Thank you for the great tool. I found it is possible to use DeepSpeed
          to perform pertaining in the example. Is it possible to extend this tool
          with fine-tuning and inference as well? Although my installation of zero
          3 offload worked with other larger models, directly including `straining_args_init
          = TrainingArguments(**training_args, deepspeed =''ds_config_zero3.json'')`
          in `cell classification` example did not work. If DeepSpeed could be used
          with the other steps, those with limited computing resources could analyze
          with the 12L model. '
        updatedAt: '2023-07-05T01:36:13.492Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - giovp
    id: 64a4c90ddc0544712a6ac1d6
    type: comment
  author: pchiang5
  content: 'Thank you for the great tool. I found it is possible to use DeepSpeed
    to perform pertaining in the example. Is it possible to extend this tool with
    fine-tuning and inference as well? Although my installation of zero 3 offload
    worked with other larger models, directly including `straining_args_init = TrainingArguments(**training_args,
    deepspeed =''ds_config_zero3.json'')` in `cell classification` example did not
    work. If DeepSpeed could be used with the other steps, those with limited computing
    resources could analyze with the 12L model. '
  created_at: 2023-07-05 00:36:13+00:00
  edited: false
  hidden: false
  id: 64a4c90ddc0544712a6ac1d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-06T00:52:39.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532964825630188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your interest in Geneformer. We are glad to hear that
          DeepSpeed worked for the pretraining example. We definitely agree that using
          DeepSpeed with the fine-tuning and inference steps would be great for enabling
          use of the deeper model with less resources. We took care to integrate with
          Hugging Face to enable use of their extensive and user-friendly tools. We
          have modified their data loaders to function with our biological data inputs,
          but otherwise the trainer should function the same as for NLP applications.
          We pretrained both the 6L and 12L models over 2 years ago so there have
          been updates to Hugging Face''s integration of DeepSpeed since then. You
          may consider running the fine-tuning as a script with DeepSpeed from the
          command line as we have shown for the pretraining example (and since that
          was working for you). Otherwise, we would suggest searching for the error
          you encountered in the Hugging Face transformers repository open/closed
          issues and/or opening a new issue. If you do so, we encourage you to update
          the discussion here with a reference to the relevant issue on Hugging Face
          transformers as that would be helpful to others in the community who encounter
          the same question.</p>

          '
        raw: Thank you for your interest in Geneformer. We are glad to hear that DeepSpeed
          worked for the pretraining example. We definitely agree that using DeepSpeed
          with the fine-tuning and inference steps would be great for enabling use
          of the deeper model with less resources. We took care to integrate with
          Hugging Face to enable use of their extensive and user-friendly tools. We
          have modified their data loaders to function with our biological data inputs,
          but otherwise the trainer should function the same as for NLP applications.
          We pretrained both the 6L and 12L models over 2 years ago so there have
          been updates to Hugging Face's integration of DeepSpeed since then. You
          may consider running the fine-tuning as a script with DeepSpeed from the
          command line as we have shown for the pretraining example (and since that
          was working for you). Otherwise, we would suggest searching for the error
          you encountered in the Hugging Face transformers repository open/closed
          issues and/or opening a new issue. If you do so, we encourage you to update
          the discussion here with a reference to the relevant issue on Hugging Face
          transformers as that would be helpful to others in the community who encounter
          the same question.
        updatedAt: '2023-07-06T00:52:39.341Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pchiang5
      relatedEventId: 64a61057500beb50968b5151
    id: 64a61057500beb50968b514f
    type: comment
  author: ctheodoris
  content: Thank you for your interest in Geneformer. We are glad to hear that DeepSpeed
    worked for the pretraining example. We definitely agree that using DeepSpeed with
    the fine-tuning and inference steps would be great for enabling use of the deeper
    model with less resources. We took care to integrate with Hugging Face to enable
    use of their extensive and user-friendly tools. We have modified their data loaders
    to function with our biological data inputs, but otherwise the trainer should
    function the same as for NLP applications. We pretrained both the 6L and 12L models
    over 2 years ago so there have been updates to Hugging Face's integration of DeepSpeed
    since then. You may consider running the fine-tuning as a script with DeepSpeed
    from the command line as we have shown for the pretraining example (and since
    that was working for you). Otherwise, we would suggest searching for the error
    you encountered in the Hugging Face transformers repository open/closed issues
    and/or opening a new issue. If you do so, we encourage you to update the discussion
    here with a reference to the relevant issue on Hugging Face transformers as that
    would be helpful to others in the community who encounter the same question.
  created_at: 2023-07-05 23:52:39+00:00
  edited: false
  hidden: false
  id: 64a61057500beb50968b514f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-06T00:52:39.000Z'
    data:
      status: closed
    id: 64a61057500beb50968b5151
    type: status-change
  author: ctheodoris
  created_at: 2023-07-05 23:52:39+00:00
  id: 64a61057500beb50968b5151
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 88
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: DeepSpeed compatibility
