!!python/object:huggingface_hub.community.DiscussionWithDetails
author: allenxiao
conflicting_files: null
created_at: 2023-07-13 03:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
      fullname: Allen Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allenxiao
      type: user
    createdAt: '2023-07-13T04:28:14.000Z'
    data:
      edited: false
      editors:
      - allenxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8581514358520508
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a925c2d07d72b9ddad29d3638a4491c3.svg
          fullname: Allen Xiao
          isHf: false
          isPro: false
          name: allenxiao
          type: user
        html: '<p>Hi, thank you for the useful tool.<br>For pretraining a new model
          from scratch with a new pretraining corpus other than Genecorpus-30M, if
          the memory for the device is not enough for the large dataset, could I split
          the dataset into multiple parts and generate a .loom file for each part?
          And then, tokenize each .loom file to a .arrow file. Finally merge those
          .arrow files into one .arrow file with the help of Dataset.concatenate_datasets.
          Is that correct?</p>

          '
        raw: "Hi, thank you for the useful tool.\r\nFor pretraining a new model from\
          \ scratch with a new pretraining corpus other than Genecorpus-30M, if the\
          \ memory for the device is not enough for the large dataset, could I split\
          \ the dataset into multiple parts and generate a .loom file for each part?\
          \ And then, tokenize each .loom file to a .arrow file. Finally merge those\
          \ .arrow files into one .arrow file with the help of Dataset.concatenate_datasets.\
          \ Is that correct?"
        updatedAt: '2023-07-13T04:28:14.107Z'
      numEdits: 0
      reactions: []
    id: 64af7d5e0d8a0c9ccf2beb4a
    type: comment
  author: allenxiao
  content: "Hi, thank you for the useful tool.\r\nFor pretraining a new model from\
    \ scratch with a new pretraining corpus other than Genecorpus-30M, if the memory\
    \ for the device is not enough for the large dataset, could I split the dataset\
    \ into multiple parts and generate a .loom file for each part? And then, tokenize\
    \ each .loom file to a .arrow file. Finally merge those .arrow files into one\
    \ .arrow file with the help of Dataset.concatenate_datasets. Is that correct?"
  created_at: 2023-07-13 03:28:14+00:00
  edited: false
  hidden: false
  id: 64af7d5e0d8a0c9ccf2beb4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-14T20:17:17.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8107487559318542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>Thank you for your question. The transcriptome tokenizer scans through
          .loom files without loading the whole file into memory so if you have many
          .loom files, you can provide the transcriptome tokenizer the directory of
          the files and it can compose them into a single .dataset. If you are encountering
          memory limitations in the step of generating the .dataset though, yes, you
          could generate a separate .dataset for each batch of data and then concatenate
          them once in the .dataset format. </p>

          '
        raw: 'Thank you for your question. The transcriptome tokenizer scans through
          .loom files without loading the whole file into memory so if you have many
          .loom files, you can provide the transcriptome tokenizer the directory of
          the files and it can compose them into a single .dataset. If you are encountering
          memory limitations in the step of generating the .dataset though, yes, you
          could generate a separate .dataset for each batch of data and then concatenate
          them once in the .dataset format. '
        updatedAt: '2023-07-14T20:17:17.182Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64b1ad4d4e5902e2fda44706
    id: 64b1ad4d4e5902e2fda44704
    type: comment
  author: ctheodoris
  content: 'Thank you for your question. The transcriptome tokenizer scans through
    .loom files without loading the whole file into memory so if you have many .loom
    files, you can provide the transcriptome tokenizer the directory of the files
    and it can compose them into a single .dataset. If you are encountering memory
    limitations in the step of generating the .dataset though, yes, you could generate
    a separate .dataset for each batch of data and then concatenate them once in the
    .dataset format. '
  created_at: 2023-07-14 19:17:17+00:00
  edited: false
  hidden: false
  id: 64b1ad4d4e5902e2fda44704
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-14T20:17:17.000Z'
    data:
      status: closed
    id: 64b1ad4d4e5902e2fda44706
    type: status-change
  author: ctheodoris
  created_at: 2023-07-14 19:17:17+00:00
  id: 64b1ad4d4e5902e2fda44706
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 115
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Aboout multiple .loom files
