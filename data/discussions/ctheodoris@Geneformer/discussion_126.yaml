!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DYXDAVE
conflicting_files: null
created_at: 2023-07-19 05:05:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58015c33c4c76a3882ea428ae3c3d399.svg
      fullname: Yuxuan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DYXDAVE
      type: user
    createdAt: '2023-07-19T06:05:22.000Z'
    data:
      edited: false
      editors:
      - DYXDAVE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9107084274291992
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58015c33c4c76a3882ea428ae3c3d399.svg
          fullname: Yuxuan Du
          isHf: false
          isPro: false
          name: DYXDAVE
          type: user
        html: '<p>Thank you for your model!<br>I''m currently trying to analyze the
          cell embedding and I''m not sure I understand the extraction of cell embedding
          correctly.<br>The last layer I got from the model.predict should be batch_size
          x 2048 x 256 matrix, right? So I need to average across gene dimension to
          get a matrix of batch_size x 256, which would be my cell embedding. I''m
          wondering do the dimension of gene still represent the input length? Since
          in the input if a cell got less gene than 2048, there will be padding filled
          into the input.<br>In other words, do we add all numbers in the gene dimension
          and divide by 2048 or we should divide it by the number of gene that the
          current cell actually has(for example, maybe we only detect 2000 genes in
          that cell and we should divide the result by 2000)</p>

          '
        raw: "Thank you for your model!\r\nI'm currently trying to analyze the cell\
          \ embedding and I'm not sure I understand the extraction of cell embedding\
          \ correctly.\r\nThe last layer I got from the model.predict should be batch_size\
          \ x 2048 x 256 matrix, right? So I need to average across gene dimension\
          \ to get a matrix of batch_size x 256, which would be my cell embedding.\
          \ I'm wondering do the dimension of gene still represent the input length?\
          \ Since in the input if a cell got less gene than 2048, there will be padding\
          \ filled into the input.\r\nIn other words, do we add all numbers in the\
          \ gene dimension and divide by 2048 or we should divide it by the number\
          \ of gene that the current cell actually has(for example, maybe we only\
          \ detect 2000 genes in that cell and we should divide the result by 2000)"
        updatedAt: '2023-07-19T06:05:22.973Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ekernf01
    id: 64b77d22fa7eabaae5fdc4e4
    type: comment
  author: DYXDAVE
  content: "Thank you for your model!\r\nI'm currently trying to analyze the cell\
    \ embedding and I'm not sure I understand the extraction of cell embedding correctly.\r\
    \nThe last layer I got from the model.predict should be batch_size x 2048 x 256\
    \ matrix, right? So I need to average across gene dimension to get a matrix of\
    \ batch_size x 256, which would be my cell embedding. I'm wondering do the dimension\
    \ of gene still represent the input length? Since in the input if a cell got less\
    \ gene than 2048, there will be padding filled into the input.\r\nIn other words,\
    \ do we add all numbers in the gene dimension and divide by 2048 or we should\
    \ divide it by the number of gene that the current cell actually has(for example,\
    \ maybe we only detect 2000 genes in that cell and we should divide the result\
    \ by 2000)"
  created_at: 2023-07-19 05:05:22+00:00
  edited: false
  hidden: false
  id: 64b77d22fa7eabaae5fdc4e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-19T19:32:07.000Z'
    data:
      edited: true
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8038676977157593
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: "<p>Thank you for your question! Yes, 2048 is the input size of the\
          \ model and 256 is the embedding dimensions parameter. So yes, the genes\
          \ would be averaged for each cell to generate a 256-dimension embedding\
          \ vector for each cell. If you have padding, you could consider removing\
          \ the padding before averaging your embedding dimensions. Here is an example\
          \ of an approach for this: <a rel=\"nofollow\" href=\"https://stackoverflow.com/questions/76015844/how-to-efficiently-mean-pool-bert-embeddings-while-excluding-padding\"\
          >https://stackoverflow.com/questions/76015844/how-to-efficiently-mean-pool-bert-embeddings-while-excluding-padding</a></p>\n\
          <p>Update:<br><span data-props=\"{&quot;user&quot;:&quot;DYXDAVE&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DYXDAVE\"\
          >@<span class=\"underline\">DYXDAVE</span></a></span>\n\n\t</span></span><br>We\
          \ have now added a function to extract and plot cell embeddings. Please\
          \ see example here:<br><a href=\"https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/extract_and_plot_cell_embeddings.ipynb\"\
          >https://huggingface.co/ctheodoris/Geneformer/blob/main/examples/extract_and_plot_cell_embeddings.ipynb</a></p>\n"
        raw: "Thank you for your question! Yes, 2048 is the input size of the model\
          \ and 256 is the embedding dimensions parameter. So yes, the genes would\
          \ be averaged for each cell to generate a 256-dimension embedding vector\
          \ for each cell. If you have padding, you could consider removing the padding\
          \ before averaging your embedding dimensions. Here is an example of an approach\
          \ for this: https://stackoverflow.com/questions/76015844/how-to-efficiently-mean-pool-bert-embeddings-while-excluding-padding\n\
          \nUpdate:\n@DYXDAVE \nWe have now added a function to extract and plot cell\
          \ embeddings. Please see example here:\nhttps://huggingface.co/ctheodoris/Geneformer/blob/main/examples/extract_and_plot_cell_embeddings.ipynb"
        updatedAt: '2023-07-28T07:14:08.476Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64b83a3749bde5d948136f4d
    id: 64b83a3749bde5d948136f48
    type: comment
  author: ctheodoris
  content: "Thank you for your question! Yes, 2048 is the input size of the model\
    \ and 256 is the embedding dimensions parameter. So yes, the genes would be averaged\
    \ for each cell to generate a 256-dimension embedding vector for each cell. If\
    \ you have padding, you could consider removing the padding before averaging your\
    \ embedding dimensions. Here is an example of an approach for this: https://stackoverflow.com/questions/76015844/how-to-efficiently-mean-pool-bert-embeddings-while-excluding-padding\n\
    \nUpdate:\n@DYXDAVE \nWe have now added a function to extract and plot cell embeddings.\
    \ Please see example here:\nhttps://huggingface.co/ctheodoris/Geneformer/blob/main/examples/extract_and_plot_cell_embeddings.ipynb"
  created_at: 2023-07-19 18:32:07+00:00
  edited: true
  hidden: false
  id: 64b83a3749bde5d948136f48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-19T19:32:07.000Z'
    data:
      status: closed
    id: 64b83a3749bde5d948136f4d
    type: status-change
  author: ctheodoris
  created_at: 2023-07-19 18:32:07+00:00
  id: 64b83a3749bde5d948136f4d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58015c33c4c76a3882ea428ae3c3d399.svg
      fullname: Yuxuan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DYXDAVE
      type: user
    createdAt: '2023-07-20T01:30:24.000Z'
    data:
      edited: false
      editors:
      - DYXDAVE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616773128509521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58015c33c4c76a3882ea428ae3c3d399.svg
          fullname: Yuxuan Du
          isHf: false
          isPro: false
          name: DYXDAVE
          type: user
        html: '<p>So, from my observation, those matrix for padding token in the last
          layers are not all zero, so we should delete them to exclude their effect?
          </p>

          '
        raw: 'So, from my observation, those matrix for padding token in the last
          layers are not all zero, so we should delete them to exclude their effect? '
        updatedAt: '2023-07-20T01:30:24.974Z'
      numEdits: 0
      reactions: []
    id: 64b88e309ebb69a79f0eb8dc
    type: comment
  author: DYXDAVE
  content: 'So, from my observation, those matrix for padding token in the last layers
    are not all zero, so we should delete them to exclude their effect? '
  created_at: 2023-07-20 00:30:24+00:00
  edited: false
  hidden: false
  id: 64b88e309ebb69a79f0eb8dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
      fullname: Christina Theodoris
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ctheodoris
      type: user
    createdAt: '2023-07-20T03:21:28.000Z'
    data:
      edited: false
      editors:
      - ctheodoris
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8708193898200989
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671502872617-622d085c8d04fd29a9ccf169.png?w=200&h=200&f=face
          fullname: Christina Theodoris
          isHf: false
          isPro: false
          name: ctheodoris
          type: user
        html: '<p>We exclude the padding tokens from the embeddings for the in silico
          perturbation analysis as well. So, yes, you could remove the padding token
          embeddings before averaging to exclude them from the cell embedding for
          your application too.</p>

          '
        raw: We exclude the padding tokens from the embeddings for the in silico perturbation
          analysis as well. So, yes, you could remove the padding token embeddings
          before averaging to exclude them from the cell embedding for your application
          too.
        updatedAt: '2023-07-20T03:21:28.162Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - DYXDAVE
    id: 64b8a838126cfeb8fdd0765a
    type: comment
  author: ctheodoris
  content: We exclude the padding tokens from the embeddings for the in silico perturbation
    analysis as well. So, yes, you could remove the padding token embeddings before
    averaging to exclude them from the cell embedding for your application too.
  created_at: 2023-07-20 02:21:28+00:00
  edited: false
  hidden: false
  id: 64b8a838126cfeb8fdd0765a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 126
repo_id: ctheodoris/Geneformer
repo_type: model
status: closed
target_branch: null
title: Cell embedding extraction
