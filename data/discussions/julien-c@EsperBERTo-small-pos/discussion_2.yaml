!!python/object:huggingface_hub.community.DiscussionWithDetails
author: model-sizer-bot
conflicting_files: null
created_at: 2023-08-25 10:18:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
      fullname: Model Sizer Bot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: model-sizer-bot
      type: user
    createdAt: '2023-08-25T11:18:18.000Z'
    data:
      edited: false
      editors:
      - model-sizer-bot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7965067625045776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
          fullname: Model Sizer Bot
          isHf: false
          isPro: false
          name: model-sizer-bot
          type: user
        html: "<h1 id=\"model-memory-requirements\">Model Memory Requirements</h1>\n\
          <p>These calculations were measured from the <a rel=\"nofollow\" href=\"\
          https://hf.co/spaces/hf-accelerate/model-memory-utility\">Model Memory Utility\
          \ Space</a> on the Hub.</p>\n<p>The minimum recommended vRAM needed for\
          \ this model to be loaded into memory via <a href=\"https://huggingface.co/docs/accelerate/usage_guides/big_modeling\"\
          >Accelerate or <code>device_map=\"auto\"</code></a> is denoted by the size\
          \ of the \"largest layer\".<br>When performing inference, expect to add\
          \ up to an additional 20% to this, as found by <a rel=\"nofollow\" href=\"\
          https://blog.eleuther.ai/transformer-math/\">EleutherAI</a>. More tests\
          \ will be performed in the future to get a more accurate benchmark for each\
          \ model.</p>\n<p>When training with <code>Adam</code>, you can expect roughly\
          \ 4x the reported results to be used. (1x for the model, 1x for the gradients,\
          \ and 2x for the optimizer).</p>\n<h2 id=\"results\">Results:</h2>\n<div\
          \ class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th align=\"\
          left\">dtype</th>\n<th align=\"left\">Largest Layer or Residual Group</th>\n\
          <th align=\"left\">Total Size</th>\n<th align=\"left\">Training using Adam</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td align=\"left\">fp32</td>\n<td align=\"\
          left\">153.87 MB</td>\n<td align=\"left\">318.35 MB</td>\n<td align=\"left\"\
          >1.24 GB</td>\n</tr>\n<tr>\n<td align=\"left\">fp16</td>\n<td align=\"left\"\
          >153.87 MB</td>\n<td align=\"left\">318.35 MB</td>\n<td align=\"left\">1.24\
          \ GB</td>\n</tr>\n<tr>\n<td align=\"left\">int8</td>\n<td align=\"left\"\
          >38.47 MB</td>\n<td align=\"left\">79.59 MB</td>\n<td align=\"left\">318.35\
          \ MB</td>\n</tr>\n<tr>\n<td align=\"left\">int4</td>\n<td align=\"left\"\
          >19.23 MB</td>\n<td align=\"left\">39.79 MB</td>\n<td align=\"left\">159.17\
          \ MB</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: "# Model Memory Requirements\n\n    \nThese calculations were measured\
          \ from the [Model Memory Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
          \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model\
          \ to be loaded into memory via [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
          \ is denoted by the size of the \"largest layer\". \nWhen performing inference,\
          \ expect to add up to an additional 20% to this, as found by [EleutherAI](https://blog.eleuther.ai/transformer-math/).\
          \ More tests will be performed in the future to get a more accurate benchmark\
          \ for each model.\n\nWhen training with `Adam`, you can expect roughly 4x\
          \ the reported results to be used. (1x for the model, 1x for the gradients,\
          \ and 2x for the optimizer).\n\n## Results:\n\n| dtype   | Largest Layer\
          \ or Residual Group   | Total Size   | Training using Adam   |\n|:--------|:----------------------------------|:-------------|:----------------------|\n\
          | fp32    | 153.87 MB                         | 318.35 MB    | 1.24 GB \
          \              |\n| fp16    | 153.87 MB                         | 318.35\
          \ MB    | 1.24 GB               |\n| int8    | 38.47 MB                \
          \          | 79.59 MB     | 318.35 MB             |\n| int4    | 19.23 MB\
          \                          | 39.79 MB     | 159.17 MB             |"
        updatedAt: '2023-08-25T11:18:18.699Z'
      numEdits: 0
      reactions: []
    id: 64e88dfa9a928410953abf1f
    type: comment
  author: model-sizer-bot
  content: "# Model Memory Requirements\n\n    \nThese calculations were measured\
    \ from the [Model Memory Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
    \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model to be\
    \ loaded into memory via [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
    \ is denoted by the size of the \"largest layer\". \nWhen performing inference,\
    \ expect to add up to an additional 20% to this, as found by [EleutherAI](https://blog.eleuther.ai/transformer-math/).\
    \ More tests will be performed in the future to get a more accurate benchmark\
    \ for each model.\n\nWhen training with `Adam`, you can expect roughly 4x the\
    \ reported results to be used. (1x for the model, 1x for the gradients, and 2x\
    \ for the optimizer).\n\n## Results:\n\n| dtype   | Largest Layer or Residual\
    \ Group   | Total Size   | Training using Adam   |\n|:--------|:----------------------------------|:-------------|:----------------------|\n\
    | fp32    | 153.87 MB                         | 318.35 MB    | 1.24 GB       \
    \        |\n| fp16    | 153.87 MB                         | 318.35 MB    | 1.24\
    \ GB               |\n| int8    | 38.47 MB                          | 79.59 MB\
    \     | 318.35 MB             |\n| int4    | 19.23 MB                        \
    \  | 39.79 MB     | 159.17 MB             |"
  created_at: 2023-08-25 10:18:18+00:00
  edited: false
  hidden: false
  id: 64e88dfa9a928410953abf1f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: julien-c/EsperBERTo-small-pos
repo_type: model
status: open
target_branch: null
title: '[AUTOMATED] Model Memory Requirements'
