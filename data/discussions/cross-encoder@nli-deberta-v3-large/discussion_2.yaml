!!python/object:huggingface_hub.community.DiscussionWithDetails
author: navins
conflicting_files: null
created_at: 2023-05-08 10:14:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dba22f3c90cf2fa870174ae4ad723013.svg
      fullname: Navin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navins
      type: user
    createdAt: '2023-05-08T11:14:11.000Z'
    data:
      edited: false
      editors:
      - navins
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dba22f3c90cf2fa870174ae4ad723013.svg
          fullname: Navin
          isHf: false
          isPro: false
          name: navins
          type: user
        html: '<p>I''m trying to use CrossEncoder(''cross-encoder/nli-deberta-v3-xsmall'')
          but I''m getting </p>

          <hr>

          <p>TypeError                                 Traceback (most recent call
          last)<br>Cell In[8], line 2<br>      1 from sentence_transformers import
          CrossEncoder<br>----&gt; 2 model = CrossEncoder(''cross-encoder/nli-deberta-v3-xsmall'')<br>      3
          scores = model.predict([(''A man is eating pizza'', ''A man eats something''),
          (''A black race car starts up in front of a crowd of people.'', ''A man
          is driving down a lonely road.'')])<br>      5 #Convert scores to labels</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\sentence_transformers\cross_encoder\CrossEncoder.py:51,
          in CrossEncoder.<strong>init</strong>(self, model_name, num_labels, max_length,
          device, tokenizer_args, automodel_args, default_activation_function)<br>     48     self.config.num_labels
          = num_labels<br>     50 self.model = AutoModelForSequenceClassification.from_pretrained(model_name,
          config=self.config, **automodel_args)<br>---&gt; 51 self.tokenizer = AutoTokenizer.from_pretrained(model_name,
          **tokenizer_args)<br>     52 self.max_length = max_length<br>     54 if
          device is None:</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\transformers\models\auto\tokenization_auto.py:702,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    698     if tokenizer_class is None:<br>    699         raise
          ValueError(<br>    700             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    701         )<br>--&gt;
          702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    704 # Otherwise we have to be creative.<br>    705
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    706 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\transformers\tokenization_utils_base.py:1811,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          *init_inputs, **kwargs)<br>   1808     else:<br>   1809         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1811 return cls._from_pretrained(<br>   1812     resolved_vocab_files,<br>   1813     pretrained_model_name_or_path,<br>   1814     init_configuration,<br>   1815     *init_inputs,<br>   1816     use_auth_token=use_auth_token,<br>   1817     cache_dir=cache_dir,<br>   1818     local_files_only=local_files_only,<br>   1819     _commit_hash=commit_hash,<br>   1820     **kwargs,<br>   1821
          )</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\transformers\tokenization_utils_base.py:1841,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          *init_inputs, **kwargs)<br>   1839 has_tokenizer_file = resolved_vocab_files.get("tokenizer_file",
          None) is not None<br>   1840 if (from_slow or not has_tokenizer_file) and
          cls.slow_tokenizer_class is not None:<br>-&gt; 1841     slow_tokenizer =
          (cls.slow_tokenizer_class)._from_pretrained(<br>   1842         copy.deepcopy(resolved_vocab_files),<br>   1843         pretrained_model_name_or_path,<br>   1844         copy.deepcopy(init_configuration),<br>   1845         *init_inputs,<br>   1846         use_auth_token=use_auth_token,<br>   1847         cache_dir=cache_dir,<br>   1848         local_files_only=local_files_only,<br>   1849         _commit_hash=_commit_hash,<br>   1850         **(copy.deepcopy(kwargs)),<br>   1851     )<br>   1852
          else:<br>   1853     slow_tokenizer = None</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\transformers\tokenization_utils_base.py:1965,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          *init_inputs, **kwargs)<br>   1963 # Instantiate tokenizer.<br>   1964 try:<br>-&gt;
          1965     tokenizer = cls(*init_inputs, **init_kwargs)<br>   1966 except
          OSError:<br>   1967     raise OSError(<br>   1968         "Unable to load
          vocabulary from file. "<br>   1969         "Please check that the provided
          vocabulary is accessible and not corrupted."<br>   1970     )</p>

          <p>File ~\AppData\Roaming\Python\Python311\site-packages\transformers\models\deberta_v2\tokenization_deberta_v2.py:142,
          in DebertaV2Tokenizer.<strong>init</strong>(self, vocab_file, do_lower_case,
          split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token,
          mask_token, sp_model_kwargs, **kwargs)<br>    126 self.sp_model_kwargs =
          {} if sp_model_kwargs is None else sp_model_kwargs<br>    128 super().<strong>init</strong>(<br>    129     do_lower_case=do_lower_case,<br>    130     bos_token=bos_token,<br>   (...)<br>    139     **kwargs,<br>    140
          )<br>--&gt; 142 if not os.path.isfile(vocab_file):<br>    143     raise
          ValueError(<br>    144         f"Can''t find a vocabulary file at path ''{vocab_file}''.
          To load the vocabulary from a Google pretrained"<br>    145         " model
          use <code>tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)</code>"<br>    146     )<br>    147
          self.do_lower_case = do_lower_case</p>

          <p>File :30, in isfile(path)</p>

          <p>TypeError: stat: path should be string, bytes, os.PathLike or integer,
          not NoneType</p>

          <p>kindly support ASAP.</p>

          '
        raw: "I'm trying to use CrossEncoder('cross-encoder/nli-deberta-v3-xsmall')\
          \ but I'm getting \r\n\r\n---------------------------------------------------------------------------\r\
          \nTypeError                                 Traceback (most recent call\
          \ last)\r\nCell In[8], line 2\r\n      1 from sentence_transformers import\
          \ CrossEncoder\r\n----> 2 model = CrossEncoder('cross-encoder/nli-deberta-v3-xsmall')\r\
          \n      3 scores = model.predict([('A man is eating pizza', 'A man eats\
          \ something'), ('A black race car starts up in front of a crowd of people.',\
          \ 'A man is driving down a lonely road.')])\r\n      5 #Convert scores to\
          \ labels\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\\
          sentence_transformers\\cross_encoder\\CrossEncoder.py:51, in CrossEncoder.__init__(self,\
          \ model_name, num_labels, max_length, device, tokenizer_args, automodel_args,\
          \ default_activation_function)\r\n     48     self.config.num_labels = num_labels\r\
          \n     50 self.model = AutoModelForSequenceClassification.from_pretrained(model_name,\
          \ config=self.config, **automodel_args)\r\n---> 51 self.tokenizer = AutoTokenizer.from_pretrained(model_name,\
          \ **tokenizer_args)\r\n     52 self.max_length = max_length\r\n     54 if\
          \ device is None:\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py:702, in AutoTokenizer.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *inputs, **kwargs)\r\n    698     if tokenizer_class\
          \ is None:\r\n    699         raise ValueError(\r\n    700             f\"\
          Tokenizer class {tokenizer_class_candidate} does not exist or is not currently\
          \ imported.\"\r\n    701         )\r\n--> 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    704 # Otherwise we have to be creative.\r\n\
          \    705 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    706 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\\
          tokenization_utils_base.py:1811, in PreTrainedTokenizerBase.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *init_inputs, **kwargs)\r\n   1808    \
          \ else:\r\n   1809         logger.info(f\"loading file {file_path} from\
          \ cache at {resolved_vocab_files[file_id]}\")\r\n-> 1811 return cls._from_pretrained(\r\
          \n   1812     resolved_vocab_files,\r\n   1813     pretrained_model_name_or_path,\r\
          \n   1814     init_configuration,\r\n   1815     *init_inputs,\r\n   1816\
          \     use_auth_token=use_auth_token,\r\n   1817     cache_dir=cache_dir,\r\
          \n   1818     local_files_only=local_files_only,\r\n   1819     _commit_hash=commit_hash,\r\
          \n   1820     **kwargs,\r\n   1821 )\r\n\r\nFile ~\\AppData\\Roaming\\Python\\\
          Python311\\site-packages\\transformers\\tokenization_utils_base.py:1841,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, *init_inputs, **kwargs)\r\n   1839 has_tokenizer_file\
          \ = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\r\n \
          \  1840 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class\
          \ is not None:\r\n-> 1841     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
          \n   1842         copy.deepcopy(resolved_vocab_files),\r\n   1843      \
          \   pretrained_model_name_or_path,\r\n   1844         copy.deepcopy(init_configuration),\r\
          \n   1845         *init_inputs,\r\n   1846         use_auth_token=use_auth_token,\r\
          \n   1847         cache_dir=cache_dir,\r\n   1848         local_files_only=local_files_only,\r\
          \n   1849         _commit_hash=_commit_hash,\r\n   1850         **(copy.deepcopy(kwargs)),\r\
          \n   1851     )\r\n   1852 else:\r\n   1853     slow_tokenizer = None\r\n\
          \r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\\
          tokenization_utils_base.py:1965, in PreTrainedTokenizerBase._from_pretrained(cls,\
          \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration,\
          \ use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs,\
          \ **kwargs)\r\n   1963 # Instantiate tokenizer.\r\n   1964 try:\r\n-> 1965\
          \     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   1966 except OSError:\r\
          \n   1967     raise OSError(\r\n   1968         \"Unable to load vocabulary\
          \ from file. \"\r\n   1969         \"Please check that the provided vocabulary\
          \ is accessible and not corrupted.\"\r\n   1970     )\r\n\r\nFile ~\\AppData\\\
          Roaming\\Python\\Python311\\site-packages\\transformers\\models\\deberta_v2\\\
          tokenization_deberta_v2.py:142, in DebertaV2Tokenizer.__init__(self, vocab_file,\
          \ do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token,\
          \ pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)\r\n    126\
          \ self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\r\
          \n    128 super().__init__(\r\n    129     do_lower_case=do_lower_case,\r\
          \n    130     bos_token=bos_token,\r\n   (...)\r\n    139     **kwargs,\r\
          \n    140 )\r\n--> 142 if not os.path.isfile(vocab_file):\r\n    143   \
          \  raise ValueError(\r\n    144         f\"Can't find a vocabulary file\
          \ at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\
          \r\n    145         \" model use `tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\
          \r\n    146     )\r\n    147 self.do_lower_case = do_lower_case\r\n\r\n\
          File <frozen genericpath>:30, in isfile(path)\r\n\r\nTypeError: stat: path\
          \ should be string, bytes, os.PathLike or integer, not NoneType\r\n\r\n\
          kindly support ASAP.\r\n\r\n"
        updatedAt: '2023-05-08T11:14:11.830Z'
      numEdits: 0
      reactions: []
    id: 6458d9836fa580137af5f50c
    type: comment
  author: navins
  content: "I'm trying to use CrossEncoder('cross-encoder/nli-deberta-v3-xsmall')\
    \ but I'm getting \r\n\r\n---------------------------------------------------------------------------\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \nCell In[8], line 2\r\n      1 from sentence_transformers import CrossEncoder\r\
    \n----> 2 model = CrossEncoder('cross-encoder/nli-deberta-v3-xsmall')\r\n    \
    \  3 scores = model.predict([('A man is eating pizza', 'A man eats something'),\
    \ ('A black race car starts up in front of a crowd of people.', 'A man is driving\
    \ down a lonely road.')])\r\n      5 #Convert scores to labels\r\n\r\nFile ~\\\
    AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\\
    CrossEncoder.py:51, in CrossEncoder.__init__(self, model_name, num_labels, max_length,\
    \ device, tokenizer_args, automodel_args, default_activation_function)\r\n   \
    \  48     self.config.num_labels = num_labels\r\n     50 self.model = AutoModelForSequenceClassification.from_pretrained(model_name,\
    \ config=self.config, **automodel_args)\r\n---> 51 self.tokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ **tokenizer_args)\r\n     52 self.max_length = max_length\r\n     54 if device\
    \ is None:\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\\
    transformers\\models\\auto\\tokenization_auto.py:702, in AutoTokenizer.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *inputs, **kwargs)\r\n    698     if tokenizer_class\
    \ is None:\r\n    699         raise ValueError(\r\n    700             f\"Tokenizer\
    \ class {tokenizer_class_candidate} does not exist or is not currently imported.\"\
    \r\n    701         )\r\n--> 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    704 # Otherwise we have to be creative.\r\n    705\
    \ # if model is an encoder decoder, the encoder tokenizer class is used by default\r\
    \n    706 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile ~\\AppData\\\
    Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1811,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ *init_inputs, **kwargs)\r\n   1808     else:\r\n   1809         logger.info(f\"\
    loading file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
    -> 1811 return cls._from_pretrained(\r\n   1812     resolved_vocab_files,\r\n\
    \   1813     pretrained_model_name_or_path,\r\n   1814     init_configuration,\r\
    \n   1815     *init_inputs,\r\n   1816     use_auth_token=use_auth_token,\r\n\
    \   1817     cache_dir=cache_dir,\r\n   1818     local_files_only=local_files_only,\r\
    \n   1819     _commit_hash=commit_hash,\r\n   1820     **kwargs,\r\n   1821 )\r\
    \n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\\
    tokenization_utils_base.py:1841, in PreTrainedTokenizerBase._from_pretrained(cls,\
    \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token,\
    \ cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\r\n   1839\
    \ has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None) is not\
    \ None\r\n   1840 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class\
    \ is not None:\r\n-> 1841     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
    \n   1842         copy.deepcopy(resolved_vocab_files),\r\n   1843         pretrained_model_name_or_path,\r\
    \n   1844         copy.deepcopy(init_configuration),\r\n   1845         *init_inputs,\r\
    \n   1846         use_auth_token=use_auth_token,\r\n   1847         cache_dir=cache_dir,\r\
    \n   1848         local_files_only=local_files_only,\r\n   1849         _commit_hash=_commit_hash,\r\
    \n   1850         **(copy.deepcopy(kwargs)),\r\n   1851     )\r\n   1852 else:\r\
    \n   1853     slow_tokenizer = None\r\n\r\nFile ~\\AppData\\Roaming\\Python\\\
    Python311\\site-packages\\transformers\\tokenization_utils_base.py:1965, in PreTrainedTokenizerBase._from_pretrained(cls,\
    \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token,\
    \ cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\r\n   1963\
    \ # Instantiate tokenizer.\r\n   1964 try:\r\n-> 1965     tokenizer = cls(*init_inputs,\
    \ **init_kwargs)\r\n   1966 except OSError:\r\n   1967     raise OSError(\r\n\
    \   1968         \"Unable to load vocabulary from file. \"\r\n   1969        \
    \ \"Please check that the provided vocabulary is accessible and not corrupted.\"\
    \r\n   1970     )\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\\
    transformers\\models\\deberta_v2\\tokenization_deberta_v2.py:142, in DebertaV2Tokenizer.__init__(self,\
    \ vocab_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token,\
    \ sep_token, pad_token, cls_token, mask_token, sp_model_kwargs, **kwargs)\r\n\
    \    126 self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\r\
    \n    128 super().__init__(\r\n    129     do_lower_case=do_lower_case,\r\n  \
    \  130     bos_token=bos_token,\r\n   (...)\r\n    139     **kwargs,\r\n    140\
    \ )\r\n--> 142 if not os.path.isfile(vocab_file):\r\n    143     raise ValueError(\r\
    \n    144         f\"Can't find a vocabulary file at path '{vocab_file}'. To load\
    \ the vocabulary from a Google pretrained\"\r\n    145         \" model use `tokenizer\
    \ = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\r\n    146     )\r\
    \n    147 self.do_lower_case = do_lower_case\r\n\r\nFile <frozen genericpath>:30,\
    \ in isfile(path)\r\n\r\nTypeError: stat: path should be string, bytes, os.PathLike\
    \ or integer, not NoneType\r\n\r\nkindly support ASAP.\r\n\r\n"
  created_at: 2023-05-08 10:14:11+00:00
  edited: false
  hidden: false
  id: 6458d9836fa580137af5f50c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cross-encoder/nli-deberta-v3-large
repo_type: model
status: open
target_branch: null
title: ' "Unable to load vocabulary from file. " '
