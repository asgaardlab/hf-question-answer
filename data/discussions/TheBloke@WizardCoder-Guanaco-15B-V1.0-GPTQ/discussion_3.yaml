!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shikhardadhich
conflicting_files: null
created_at: 2023-09-04 09:33:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ccfd6fb63eca695adfaf7d25b367d265.svg
      fullname: 'Shikhar '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shikhardadhich
      type: user
    createdAt: '2023-09-04T10:33:34.000Z'
    data:
      edited: true
      editors:
      - shikhardadhich
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5741766095161438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ccfd6fb63eca695adfaf7d25b367d265.svg
          fullname: 'Shikhar '
          isHf: false
          isPro: false
          name: shikhardadhich
          type: user
        html: '<p>I m getting this error while running the python code  (copied from
          example)   </p>

          <p>File "/home/ubuntu/wizar.py", line 12, in <br>    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>  File
          "/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/auto.py",
          line 108, in from_quantized<br>    return quant_func(<br>  File "/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/_base.py",
          line 791, in from_quantized<br>    raise FileNotFoundError(f"Could not find
          model in {model_name_or_path}")<br>FileNotFoundError: Could not find model
          in TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ</p>

          <p>Code:<br>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse</p>

          <p>model_name_or_path = "TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ"<br>model_basename
          = "wizardcoder-guanaco-15b-v1.0-GPTQ-4bit-128g.no-act.order"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=False,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''<br>Below is an
          instruction that describes a task. Write a response that appropriately completes
          the request.</p>

          <h3 id="instruction-prompt">Instruction: PROMPT</h3>

          <h3 id="response">Response:</h3>

          <p>''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>input_ids = tokenizer(prompt_template, return_tensors=''pt'').input_ids.cuda()<br>output
          = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>

          <h1 id="inference-can-also-be-done-using-transformers-pipeline">Inference
          can also be done using transformers'' pipeline</h1>

          <h1 id="prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq">Prevent
          printing spurious transformers error when using pipeline with AutoGPTQ</h1>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          '
        raw: "I m getting this error while running the python code  (copied from example)\
          \   \n\nFile \"/home/ubuntu/wizar.py\", line 12, in <module>\n    model\
          \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n  File \"/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 108, in from_quantized\n    return quant_func(\n  File \"/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 791, in from_quantized\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\nFileNotFoundError: Could not find\
          \ model in TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ\n\n\nCode:\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ\"\
          \nmodel_basename = \"wizardcoder-guanaco-15b-v1.0-GPTQ-4bit-128g.no-act.order\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=False,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''\n\
          Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n\n### Instruction: PROMPT\n\n### Response:\n'''\n\
          \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n# Inference can also be done using transformers' pipeline\n\n# Prevent\
          \ printing spurious transformers error when using pipeline with AutoGPTQ\n\
          logging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\")\npipe\
          \ = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n"
        updatedAt: '2023-09-04T10:44:13.637Z'
      numEdits: 2
      reactions: []
    id: 64f5b27eef89693f70cdbd4b
    type: comment
  author: shikhardadhich
  content: "I m getting this error while running the python code  (copied from example)\
    \   \n\nFile \"/home/ubuntu/wizar.py\", line 12, in <module>\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \  File \"/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
    , line 108, in from_quantized\n    return quant_func(\n  File \"/home/ubuntu/Wiz/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 791, in from_quantized\n    raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")\nFileNotFoundError: Could not find model in TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ\n\
    \n\nCode:\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path\
    \ = \"TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ\"\nmodel_basename = \"wizardcoder-guanaco-15b-v1.0-GPTQ-4bit-128g.no-act.order\"\
    \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=False,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''\n\
    Below is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\n\n### Instruction: PROMPT\n\n### Response:\n'''\n\n\
    print(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\n# Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
    )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n"
  created_at: 2023-09-04 09:33:34+00:00
  edited: true
  hidden: false
  id: 64f5b27eef89693f70cdbd4b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: 'I m getting this error while running the python code  : FileNotFoundError:
  Could not find model in TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ'
