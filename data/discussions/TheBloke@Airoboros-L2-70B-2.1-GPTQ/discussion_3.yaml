!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mullerse
conflicting_files: null
created_at: 2023-09-04 08:03:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
      fullname: "M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mullerse
      type: user
    createdAt: '2023-09-04T09:03:27.000Z'
    data:
      edited: true
      editors:
      - mullerse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41868534684181213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
          fullname: "M\xFCller"
          isHf: false
          isPro: false
          name: mullerse
          type: user
        html: "<p>Hey there :)<br>I have a problem with the following error message\
          \ and hope you can help me :)<br>\"Cuda Extension not installed\"<br>Actual\
          \ systemconfig (python -m torch.utils.collect_env)<br><code>Sammeln von\
          \ Umgebungsinformationen...<br>PyTorch Version: 2.1.0.dev20230902+cu121<br>Ist\
          \ Debug-Build: Falsch<br>CUDA verwendet, um PyTorch zu erstellen: 12.1<br>ROCM\
          \ verwendet, um PyTorch zu erstellen: N/A         </code></p><code>\n<p>Betriebssystem:\
          \ Microsoft Windows 10 Pro<br>GCC-Version: Konnte nicht ermittelt werden<br>Clang-Version:\
          \ Konnte nicht gesammelt werden<br>CMake-Version: Konnte nicht gesammelt\
          \ werden<br>Libc-Version: N/A</p>\n<p>Python-Version: 3.10.11 (tags/v3.10.11:7d4cc5a,\
          \ Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64-Bit-Laufzeit)<br>Python-Plattform:\
          \ Windows-10-10.0.19045-SP0<br>Ist CUDA verf\xFCgbar: Wahr<br>CUDA-Laufzeitversion:\
          \ Konnte nicht gesammelt werden<br>CUDA_MODULE_LOADING eingestellt auf:\
          \ LAZY<br>GPU-Modelle und Konfiguration:<br>Grafikkarte 0: NVIDIA GeForce\
          \ RTX 4090<br>Grafikkarte 1: NVIDIA GeForce RTX 4090</p>\n</code><p><code>Nvidia-Treiber\
          \ Version: 536.23<br>cuDNN-Version: Konnte nicht gesammelt werden<br>HIP-Laufzeitversion:\
          \ N/A<br>MIOpen-Laufzeitversion: N/A<br>Ist XNNPACK verf\xFCgbar: True</code></p>\n\
          <p><b>Variant 1:</b><br>Current Nvidia driver installed. The Nvidia Cuda\
          \ Toolkit is not installed.<br>auto-gptq v0.4.2<br>torch 2.1.0.dev<br>torch.cuda.is_available()\
          \ = true</p>\n<p>The model Airoboros-L2 is loaded into the GPU, but the\
          \ calculations are very slow and seem to run over the CPU (see TaskManager),\
          \ also the error message always appears:<br>CUDA extension not installed.\
          \ But i can load and use the model<br>I read the following post about this:\
          \ <a href=\"https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/discussions/5\"\
          >https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/discussions/5</a><br>So\
          \ when I do pip install . from the autogptq version the following error\
          \ appears:<br><code>  error: subprocess-exited-with-error</code></p><code>\n\
          </code><p><code>  \xD7 Getting requirements to build wheel did not run successfully.<br>\
          \  \u2502 exit code: 4294967295<br>  \u2570\u2500&gt; [1 lines of output]<br>\
          \      Building cuda extension requires PyTorch(&gt;=1.13.0) been installed,\
          \ please install PyTorch first!<br>      [end of output]</code></p>\n<p><b>That's\
          \ why I tested version 2:</b><br>Current Nvidia Studio driver installed<br>Installed\
          \ Nvidia Cuda Toolkit v11.8<br>(previously deleted torch with <code>pip3\
          \ uninstall torch torchvision torchaudio</code>)<br><code>pip3 install torch\
          \ torchvision torchaudio --index-url <a rel=\"nofollow\" href=\"https://download.pytorch.org/whl/cu118\"\
          >https://download.pytorch.org/whl/cu118</a></code><br><code>pip install\
          \ auto-gptq --extra-index-url <a rel=\"nofollow\" href=\"https://huggingface.github.io/autogptq-index/whl/cu118/\"\
          >https://huggingface.github.io/autogptq-index/whl/cu118/</a></code><br>When\
          \ I load the model Airoboros-L2 now, the error message does not appear anymore.\
          \ But I get the following error:<br>\"Embedding dimension 768 does not match\
          \ collection dimensionality 1024\"<br>Using the model is not possible<br>:(</p>\n\
          <p>Is it correct to downgrade to Cuda 11.8?<br>A version for torch 2.1.0\
          \ compatible auto-gptq version I have not found</p>\n<p>I hope I could describe\
          \ my problem exactly enough and thank you very much for the help :)</p>\n"
        raw: "Hey there :) \nI have a problem with the following error message and\
          \ hope you can help me :) \n\"Cuda Extension not installed\"\nActual systemconfig\
          \ (python -m torch.utils.collect_env)\n<code>Sammeln von Umgebungsinformationen...\n\
          PyTorch Version: 2.1.0.dev20230902+cu121\nIst Debug-Build: Falsch      \
          \             \nCUDA verwendet, um PyTorch zu erstellen: 12.1        \n\
          ROCM verwendet, um PyTorch zu erstellen: N/A         \n\nBetriebssystem:\
          \ Microsoft Windows 10 Pro\nGCC-Version: Konnte nicht ermittelt werden\n\
          Clang-Version: Konnte nicht gesammelt werden\nCMake-Version: Konnte nicht\
          \ gesammelt werden\nLibc-Version: N/A\n\nPython-Version: 3.10.11 (tags/v3.10.11:7d4cc5a,\
          \ Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64-Bit-Laufzeit)\n\
          Python-Plattform: Windows-10-10.0.19045-SP0\nIst CUDA verf\xFCgbar: Wahr\n\
          CUDA-Laufzeitversion: Konnte nicht gesammelt werden\nCUDA_MODULE_LOADING\
          \ eingestellt auf: LAZY\nGPU-Modelle und Konfiguration:\nGrafikkarte 0:\
          \ NVIDIA GeForce RTX 4090\nGrafikkarte 1: NVIDIA GeForce RTX 4090\n\nNvidia-Treiber\
          \ Version: 536.23\ncuDNN-Version: Konnte nicht gesammelt werden\nHIP-Laufzeitversion:\
          \ N/A\nMIOpen-Laufzeitversion: N/A\nIst XNNPACK verf\xFCgbar: True</code>\n\
          \n<b>Variant 1:</b>\nCurrent Nvidia driver installed. The Nvidia Cuda Toolkit\
          \ is not installed.\nauto-gptq v0.4.2\ntorch 2.1.0.dev\ntorch.cuda.is_available()\
          \ = true\n\nThe model Airoboros-L2 is loaded into the GPU, but the calculations\
          \ are very slow and seem to run over the CPU (see TaskManager), also the\
          \ error message always appears: \nCUDA extension not installed. But i can\
          \ load and use the model\nI read the following post about this: https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/discussions/5\n\
          So when I do pip install . from the autogptq version the following error\
          \ appears:\n<code>  error: subprocess-exited-with-error\n\n  \xD7 Getting\
          \ requirements to build wheel did not run successfully.\n  \u2502 exit code:\
          \ 4294967295\n  \u2570\u2500> [1 lines of output]\n      Building cuda extension\
          \ requires PyTorch(>=1.13.0) been installed, please install PyTorch first!\n\
          \      [end of output]</code>\n\n<b>That's why I tested version 2:</b>\n\
          Current Nvidia Studio driver installed\nInstalled Nvidia Cuda Toolkit v11.8\n\
          (previously deleted torch with <code>pip3 uninstall torch torchvision torchaudio</code>)\n\
          <code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code>\n\
          <code>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/</code>\n\
          When I load the model Airoboros-L2 now, the error message does not appear\
          \ anymore. But I get the following error: \n\"Embedding dimension 768 does\
          \ not match collection dimensionality 1024\"\nUsing the model is not possible\n\
          :(\n\nIs it correct to downgrade to Cuda 11.8?\nA version for torch 2.1.0\
          \ compatible auto-gptq version I have not found\n\nI hope I could describe\
          \ my problem exactly enough and thank you very much for the help :)"
        updatedAt: '2023-09-04T09:04:09.178Z'
      numEdits: 1
      reactions: []
    id: 64f59d5f8b8cab6e3c231fe5
    type: comment
  author: mullerse
  content: "Hey there :) \nI have a problem with the following error message and hope\
    \ you can help me :) \n\"Cuda Extension not installed\"\nActual systemconfig (python\
    \ -m torch.utils.collect_env)\n<code>Sammeln von Umgebungsinformationen...\nPyTorch\
    \ Version: 2.1.0.dev20230902+cu121\nIst Debug-Build: Falsch                  \
    \ \nCUDA verwendet, um PyTorch zu erstellen: 12.1        \nROCM verwendet, um\
    \ PyTorch zu erstellen: N/A         \n\nBetriebssystem: Microsoft Windows 10 Pro\n\
    GCC-Version: Konnte nicht ermittelt werden\nClang-Version: Konnte nicht gesammelt\
    \ werden\nCMake-Version: Konnte nicht gesammelt werden\nLibc-Version: N/A\n\n\
    Python-Version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr 5 2023, 00:38:17) [MSC v.1929\
    \ 64 bit (AMD64)] (64-Bit-Laufzeit)\nPython-Plattform: Windows-10-10.0.19045-SP0\n\
    Ist CUDA verf\xFCgbar: Wahr\nCUDA-Laufzeitversion: Konnte nicht gesammelt werden\n\
    CUDA_MODULE_LOADING eingestellt auf: LAZY\nGPU-Modelle und Konfiguration:\nGrafikkarte\
    \ 0: NVIDIA GeForce RTX 4090\nGrafikkarte 1: NVIDIA GeForce RTX 4090\n\nNvidia-Treiber\
    \ Version: 536.23\ncuDNN-Version: Konnte nicht gesammelt werden\nHIP-Laufzeitversion:\
    \ N/A\nMIOpen-Laufzeitversion: N/A\nIst XNNPACK verf\xFCgbar: True</code>\n\n\
    <b>Variant 1:</b>\nCurrent Nvidia driver installed. The Nvidia Cuda Toolkit is\
    \ not installed.\nauto-gptq v0.4.2\ntorch 2.1.0.dev\ntorch.cuda.is_available()\
    \ = true\n\nThe model Airoboros-L2 is loaded into the GPU, but the calculations\
    \ are very slow and seem to run over the CPU (see TaskManager), also the error\
    \ message always appears: \nCUDA extension not installed. But i can load and use\
    \ the model\nI read the following post about this: https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/discussions/5\n\
    So when I do pip install . from the autogptq version the following error appears:\n\
    <code>  error: subprocess-exited-with-error\n\n  \xD7 Getting requirements to\
    \ build wheel did not run successfully.\n  \u2502 exit code: 4294967295\n  \u2570\
    \u2500> [1 lines of output]\n      Building cuda extension requires PyTorch(>=1.13.0)\
    \ been installed, please install PyTorch first!\n      [end of output]</code>\n\
    \n<b>That's why I tested version 2:</b>\nCurrent Nvidia Studio driver installed\n\
    Installed Nvidia Cuda Toolkit v11.8\n(previously deleted torch with <code>pip3\
    \ uninstall torch torchvision torchaudio</code>)\n<code>pip3 install torch torchvision\
    \ torchaudio --index-url https://download.pytorch.org/whl/cu118</code>\n<code>pip\
    \ install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/</code>\n\
    When I load the model Airoboros-L2 now, the error message does not appear anymore.\
    \ But I get the following error: \n\"Embedding dimension 768 does not match collection\
    \ dimensionality 1024\"\nUsing the model is not possible\n:(\n\nIs it correct\
    \ to downgrade to Cuda 11.8?\nA version for torch 2.1.0 compatible auto-gptq version\
    \ I have not found\n\nI hope I could describe my problem exactly enough and thank\
    \ you very much for the help :)"
  created_at: 2023-09-04 08:03:27+00:00
  edited: true
  hidden: false
  id: 64f59d5f8b8cab6e3c231fe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
      fullname: "M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mullerse
      type: user
    createdAt: '2023-09-05T05:09:37.000Z'
    data:
      edited: true
      editors:
      - mullerse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44129934906959534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
          fullname: "M\xFCller"
          isHf: false
          isPro: false
          name: mullerse
          type: user
        html: '<p>Okay, Update :)<br>Version 2 runs. The printed failure is not from
          loading the Model</p>

          <p>This is how I solved it:</p>

          <ul>

          <li>latest Nvidia Studio driver installed</li>

          <li><code>pip3 uninstall torch torchvision torchaudio auto-gptq</code></li>

          <li><code>pip3 install torch torchvision torchaudio --index-url <a rel="nofollow"
          href="https://download.pytorch.org/whl/cu118">https://download.pytorch.org/whl/cu118</a></code></li>

          <li><code>pip install auto-gptq --extra-index-url <a rel="nofollow" href="https://huggingface.github.io/autogptq-index/whl/cu118/">https://huggingface.github.io/autogptq-index/whl/cu118/</a></code></li>

          </ul>

          '
        raw: "Okay, Update :) \nVersion 2 runs. The printed failure is not from loading\
          \ the Model\n\nThis is how I solved it:\n- latest Nvidia Studio driver installed\n\
          - <code>pip3 uninstall torch torchvision torchaudio auto-gptq</code>\n-\
          \ <code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code>\n\
          - <code>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/</code>"
        updatedAt: '2023-09-05T05:12:56.714Z'
      numEdits: 1
      reactions: []
    id: 64f6b8117146133f1504f600
    type: comment
  author: mullerse
  content: "Okay, Update :) \nVersion 2 runs. The printed failure is not from loading\
    \ the Model\n\nThis is how I solved it:\n- latest Nvidia Studio driver installed\n\
    - <code>pip3 uninstall torch torchvision torchaudio auto-gptq</code>\n- <code>pip3\
    \ install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code>\n\
    - <code>pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/</code>"
  created_at: 2023-09-05 04:09:37+00:00
  edited: true
  hidden: false
  id: 64f6b8117146133f1504f600
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Airoboros-L2-70B-2.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Cuda extension not installed. i tried several things
