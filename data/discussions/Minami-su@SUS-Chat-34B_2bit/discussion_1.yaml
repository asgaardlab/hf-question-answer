!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-12-22 16:39:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-22T16:39:38.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953544557094574
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>At what context length have you computed the hessians?</p>

          '
        raw: At what context length have you computed the hessians?
        updatedAt: '2023-12-22T16:39:38.425Z'
      numEdits: 0
      reactions: []
    id: 6585bbcaaf9de4b12a7a3ed2
    type: comment
  author: KnutJaegersberg
  content: At what context length have you computed the hessians?
  created_at: 2023-12-22 16:39:38+00:00
  edited: false
  hidden: false
  id: 6585bbcaaf9de4b12a7a3ed2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-12-22T23:36:10.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: ru
        probability: 0.126681387424469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>500</p>

          '
        raw: '500'
        updatedAt: '2023-12-22T23:36:10.572Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 65861d6a595e2582184292f3
    type: comment
  author: Minami-su
  content: '500'
  created_at: 2023-12-22 23:36:10+00:00
  edited: false
  hidden: false
  id: 65861d6a595e2582184292f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T12:31:54.000Z'
    data:
      status: closed
    id: 6586d33ae878be571b0f9cc8
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-12-23 12:31:54+00:00
  id: 6586d33ae878be571b0f9cc8
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T13:11:31.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9625773429870605
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>how well does it work if you do inference over longer contexts than
          that? </p>

          '
        raw: 'how well does it work if you do inference over longer contexts than
          that? '
        updatedAt: '2023-12-23T13:11:31.208Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6586dc837959448ef5da7eea
    id: 6586dc837959448ef5da7ee5
    type: comment
  author: KnutJaegersberg
  content: 'how well does it work if you do inference over longer contexts than that? '
  created_at: 2023-12-23 13:11:31+00:00
  edited: false
  hidden: false
  id: 6586dc837959448ef5da7ee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T13:11:31.000Z'
    data:
      status: open
    id: 6586dc837959448ef5da7eea
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-12-23 13:11:31+00:00
  id: 6586dc837959448ef5da7eea
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T13:12:31.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9670858979225159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I ask because I''m currently doing a hessian over 8k context, wondering
          what the impact of less context than what you want to do inference over
          is.<br>Nobody seems to know this at this moment. </p>

          '
        raw: "I ask because I'm currently doing a hessian over 8k context, wondering\
          \ what the impact of less context than what you want to do inference over\
          \ is. \nNobody seems to know this at this moment. "
        updatedAt: '2023-12-23T13:12:31.628Z'
      numEdits: 0
      reactions: []
    id: 6586dcbf7959448ef5da86a8
    type: comment
  author: KnutJaegersberg
  content: "I ask because I'm currently doing a hessian over 8k context, wondering\
    \ what the impact of less context than what you want to do inference over is.\
    \ \nNobody seems to know this at this moment. "
  created_at: 2023-12-23 13:12:31+00:00
  edited: false
  hidden: false
  id: 6586dcbf7959448ef5da86a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-12-23T13:17:29.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772962331771851
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>I don''t know because I haven''t tried it. The perplexity of the
          model, after quantization with a sequence length of 500, increased from
          5.3 in the original model to 5.5.</p>

          '
        raw: I don't know because I haven't tried it. The perplexity of the model,
          after quantization with a sequence length of 500, increased from 5.3 in
          the original model to 5.5.
        updatedAt: '2023-12-23T13:17:29.977Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 6586dde9d1331d552bb2e7f0
    type: comment
  author: Minami-su
  content: I don't know because I haven't tried it. The perplexity of the model, after
    quantization with a sequence length of 500, increased from 5.3 in the original
    model to 5.5.
  created_at: 2023-12-23 13:17:29+00:00
  edited: false
  hidden: false
  id: 6586dde9d1331d552bb2e7f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T13:25:00.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9971967935562134
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I''m currently quantizing a model. I don''t think this is small
          enough to try on colab, is it? </p>

          '
        raw: 'I''m currently quantizing a model. I don''t think this is small enough
          to try on colab, is it? '
        updatedAt: '2023-12-23T13:25:00.022Z'
      numEdits: 0
      reactions: []
    id: 6586dfac65df457a5589a680
    type: comment
  author: KnutJaegersberg
  content: 'I''m currently quantizing a model. I don''t think this is small enough
    to try on colab, is it? '
  created_at: 2023-12-23 13:25:00+00:00
  edited: false
  hidden: false
  id: 6586dfac65df457a5589a680
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-12-23T13:27:40.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9898285269737244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>A40 with 80GB of CPU memory is enough</p>

          '
        raw: A40 with 80GB of CPU memory is enough
        updatedAt: '2023-12-23T13:27:40.466Z'
      numEdits: 0
      reactions: []
    id: 6586e04c6cf9325ae68adc47
    type: comment
  author: Minami-su
  content: A40 with 80GB of CPU memory is enough
  created_at: 2023-12-23 13:27:40+00:00
  edited: false
  hidden: false
  id: 6586e04c6cf9325ae68adc47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-12-23T13:30:07.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9906958341598511
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>but not sure 8k length, I''m trying</p>

          '
        raw: but not sure 8k length, I'm trying
        updatedAt: '2023-12-23T13:30:07.247Z'
      numEdits: 0
      reactions: []
    id: 6586e0dfd861072dc5d1c061
    type: comment
  author: Minami-su
  content: but not sure 8k length, I'm trying
  created_at: 2023-12-23 13:30:07+00:00
  edited: false
  hidden: false
  id: 6586e0dfd861072dc5d1c061
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-12-23T13:59:29.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9829955697059631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>A40 with 80GB of CPU memory might be sufficient for a length of
          8k. I tried one layer, and there were no abnormalities.</p>

          '
        raw: A40 with 80GB of CPU memory might be sufficient for a length of 8k. I
          tried one layer, and there were no abnormalities.
        updatedAt: '2023-12-23T13:59:29.151Z'
      numEdits: 0
      reactions: []
    id: 6586e7c1bbb04840e35c70ec
    type: comment
  author: Minami-su
  content: A40 with 80GB of CPU memory might be sufficient for a length of 8k. I tried
    one layer, and there were no abnormalities.
  created_at: 2023-12-23 13:59:29+00:00
  edited: false
  hidden: false
  id: 6586e7c1bbb04840e35c70ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T16:06:22.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9078185558319092
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Yep, I''m doing another 34b model on 48gb with context length 8k,
          still a little vram left.<br>I meant something different: Have you tried
          your model with a longer context length than 500? How does it behave beyond
          the context length used for the hessian at inference?<br>If you do queries
          over context length of 2k tokens, does performance collapse? </p>

          '
        raw: "Yep, I'm doing another 34b model on 48gb with context length 8k, still\
          \ a little vram left. \nI meant something different: Have you tried your\
          \ model with a longer context length than 500? How does it behave beyond\
          \ the context length used for the hessian at inference? \nIf you do queries\
          \ over context length of 2k tokens, does performance collapse? "
        updatedAt: '2023-12-23T16:06:22.990Z'
      numEdits: 0
      reactions: []
    id: 6587057e126b8d7eae960590
    type: comment
  author: KnutJaegersberg
  content: "Yep, I'm doing another 34b model on 48gb with context length 8k, still\
    \ a little vram left. \nI meant something different: Have you tried your model\
    \ with a longer context length than 500? How does it behave beyond the context\
    \ length used for the hessian at inference? \nIf you do queries over context length\
    \ of 2k tokens, does performance collapse? "
  created_at: 2023-12-23 16:06:22+00:00
  edited: false
  hidden: false
  id: 6587057e126b8d7eae960590
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-23T16:11:16.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8803936243057251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I''m asking because I wonder if one can use more context than done
          for the hessian.<br>My basic idea is to use quantization so we can use long
          context LLMs over longer context given a certain vram budget (locally).<br>Actually
          it''s simpler: Given my hardware, what is the longest query I can run on
          my system for any performance tier?<br>I suspect that yi might be the sweet
          spot. Properly allows queries over a long context in theory, being a high
          performance model.<br>Of cause there are also numerous smaller models with
          long context, but my simple question is, how can we squeeze out the best
          long context performance out of our hardware? </p>

          '
        raw: "I'm asking because I wonder if one can use more context than done for\
          \ the hessian. \nMy basic idea is to use quantization so we can use long\
          \ context LLMs over longer context given a certain vram budget (locally).\
          \ \nActually it's simpler: Given my hardware, what is the longest query\
          \ I can run on my system for any performance tier? \nI suspect that yi might\
          \ be the sweet spot. Properly allows queries over a long context in theory,\
          \ being a high performance model. \nOf cause there are also numerous smaller\
          \ models with long context, but my simple question is, how can we squeeze\
          \ out the best long context performance out of our hardware? "
        updatedAt: '2023-12-23T16:11:16.850Z'
      numEdits: 0
      reactions: []
    id: 658706a47959448ef5dffca8
    type: comment
  author: KnutJaegersberg
  content: "I'm asking because I wonder if one can use more context than done for\
    \ the hessian. \nMy basic idea is to use quantization so we can use long context\
    \ LLMs over longer context given a certain vram budget (locally). \nActually it's\
    \ simpler: Given my hardware, what is the longest query I can run on my system\
    \ for any performance tier? \nI suspect that yi might be the sweet spot. Properly\
    \ allows queries over a long context in theory, being a high performance model.\
    \ \nOf cause there are also numerous smaller models with long context, but my\
    \ simple question is, how can we squeeze out the best long context performance\
    \ out of our hardware? "
  created_at: 2023-12-23 16:11:16+00:00
  edited: false
  hidden: false
  id: 658706a47959448ef5dffca8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Minami-su/SUS-Chat-34B_2bit
repo_type: model
status: open
target_branch: null
title: Hessian context length?
