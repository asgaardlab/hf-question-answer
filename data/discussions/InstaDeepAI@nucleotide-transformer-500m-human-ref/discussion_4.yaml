!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Palaash
conflicting_files: null
created_at: 2023-05-11 00:47:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1366272a52bca7ca034c945794bf0af7.svg
      fullname: Palash Sethi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Palaash
      type: user
    createdAt: '2023-05-11T01:47:49.000Z'
    data:
      edited: false
      editors:
      - Palaash
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1366272a52bca7ca034c945794bf0af7.svg
          fullname: Palash Sethi
          isHf: false
          isPro: false
          name: Palaash
          type: user
        html: '<p>Hi,</p>

          <p>I recently observed that the embeddings generated by the models provided
          in the GitHub repo and the Hugging Face models are different for the same
          input sequence.</p>

          <p>In the Hugging Face model, embeddings are generated after layer normalization
          (for example, layer 23 for my model). However, in the model from the GitHub
          repo, embeddings are generated before layer normalization.</p>

          <p>This discrepancy might lead to differences in the downstream applications.
          I''d appreciate it if you could clarify this issue and suggest a way to
          make both models consistent in terms of the generated embeddings.</p>

          <p>Thank you!</p>

          '
        raw: "Hi,\r\n\r\nI recently observed that the embeddings generated by the\
          \ models provided in the GitHub repo and the Hugging Face models are different\
          \ for the same input sequence.\r\n\r\nIn the Hugging Face model, embeddings\
          \ are generated after layer normalization (for example, layer 23 for my\
          \ model). However, in the model from the GitHub repo, embeddings are generated\
          \ before layer normalization.\r\n\r\nThis discrepancy might lead to differences\
          \ in the downstream applications. I'd appreciate it if you could clarify\
          \ this issue and suggest a way to make both models consistent in terms of\
          \ the generated embeddings.\r\n\r\nThank you!"
        updatedAt: '2023-05-11T01:47:49.305Z'
      numEdits: 0
      reactions: []
    id: 645c49456ff46332e75614e5
    type: comment
  author: Palaash
  content: "Hi,\r\n\r\nI recently observed that the embeddings generated by the models\
    \ provided in the GitHub repo and the Hugging Face models are different for the\
    \ same input sequence.\r\n\r\nIn the Hugging Face model, embeddings are generated\
    \ after layer normalization (for example, layer 23 for my model). However, in\
    \ the model from the GitHub repo, embeddings are generated before layer normalization.\r\
    \n\r\nThis discrepancy might lead to differences in the downstream applications.\
    \ I'd appreciate it if you could clarify this issue and suggest a way to make\
    \ both models consistent in terms of the generated embeddings.\r\n\r\nThank you!"
  created_at: 2023-05-11 00:47:49+00:00
  edited: false
  hidden: false
  id: 645c49456ff46332e75614e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426908d7a0d3f02acd4cfb8/Qv-oeJQ6MaOmmRqzknH3G.png?w=200&h=200&f=face
      fullname: Hugo Dalla-torre
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hdallatorre
      type: user
    createdAt: '2023-06-23T08:24:40.000Z'
    data:
      edited: false
      editors:
      - hdallatorre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8275010585784912
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426908d7a0d3f02acd4cfb8/Qv-oeJQ6MaOmmRqzknH3G.png?w=200&h=200&f=face
          fullname: Hugo Dalla-torre
          isHf: false
          isPro: false
          name: hdallatorre
          type: user
        html: '<p>Hello Palaash, </p>

          <p>The model has 24 transformer layers.</p>

          <p>If you look at the <code>hidden_states</code> retrieved after inference
          with the HuggingFace <code>nucleotide-transformer-500m-human-ref</code>,
          you''ll see that it is a tuple of 25 tensors. That is one per layer, and
          the final one being extracted after the first layer norm of the Robert LM
          head. </p>

          <p>If you use the GitHub JAX repository, you can also retrieve 25 different
          embeddings. The layers are 0-indexed, so if you are calling <code>get_pretrained_model</code>
          with the argument <code>embeddings_layers_to_save=(23,)</code>, you will
          get the embeddings after the last transformer layer of the model, which
          correspond to the second last embeddings in Huggingface''s <code>hidden_states</code>.
          However, if you call  <code>embeddings_layers_to_save=(24,)</code>, you
          will get the embeddings after the first layer norm of the Roberta LM head,
          which correspong to the last embeddings returned in Huggingface''s <code>hidden_states</code>.
          </p>

          <p>I will add some indication in the <a rel="nofollow" href="https://github.com/instadeepai/nucleotide-transformer/tree/main">nucleotide-transformer</a>
          GitHub repository to clarify how to retrieve the embeddings in the same
          way as in the HuggingFace model.</p>

          <p>Hope that solves everything on your end ! </p>

          '
        raw: "Hello Palaash, \n\nThe model has 24 transformer layers.\n\nIf you look\
          \ at the `hidden_states` retrieved after inference with the HuggingFace\
          \ `nucleotide-transformer-500m-human-ref`, you'll see that it is a tuple\
          \ of 25 tensors. That is one per layer, and the final one being extracted\
          \ after the first layer norm of the Robert LM head. \n\nIf you use the GitHub\
          \ JAX repository, you can also retrieve 25 different embeddings. The layers\
          \ are 0-indexed, so if you are calling `get_pretrained_model` with the argument\
          \ `embeddings_layers_to_save=(23,)`, you will get the embeddings after the\
          \ last transformer layer of the model, which correspond to the second last\
          \ embeddings in Huggingface's `hidden_states`. However, if you call  `embeddings_layers_to_save=(24,)`,\
          \ you will get the embeddings after the first layer norm of the Roberta\
          \ LM head, which correspong to the last embeddings returned in Huggingface's\
          \ `hidden_states`. \n\nI will add some indication in the [nucleotide-transformer](https://github.com/instadeepai/nucleotide-transformer/tree/main)\
          \ GitHub repository to clarify how to retrieve the embeddings in the same\
          \ way as in the HuggingFace model.\n\nHope that solves everything on your\
          \ end ! "
        updatedAt: '2023-06-23T08:24:40.278Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Palaash
    id: 649556c8f02a9f75bfc7ea93
    type: comment
  author: hdallatorre
  content: "Hello Palaash, \n\nThe model has 24 transformer layers.\n\nIf you look\
    \ at the `hidden_states` retrieved after inference with the HuggingFace `nucleotide-transformer-500m-human-ref`,\
    \ you'll see that it is a tuple of 25 tensors. That is one per layer, and the\
    \ final one being extracted after the first layer norm of the Robert LM head.\
    \ \n\nIf you use the GitHub JAX repository, you can also retrieve 25 different\
    \ embeddings. The layers are 0-indexed, so if you are calling `get_pretrained_model`\
    \ with the argument `embeddings_layers_to_save=(23,)`, you will get the embeddings\
    \ after the last transformer layer of the model, which correspond to the second\
    \ last embeddings in Huggingface's `hidden_states`. However, if you call  `embeddings_layers_to_save=(24,)`,\
    \ you will get the embeddings after the first layer norm of the Roberta LM head,\
    \ which correspong to the last embeddings returned in Huggingface's `hidden_states`.\
    \ \n\nI will add some indication in the [nucleotide-transformer](https://github.com/instadeepai/nucleotide-transformer/tree/main)\
    \ GitHub repository to clarify how to retrieve the embeddings in the same way\
    \ as in the HuggingFace model.\n\nHope that solves everything on your end ! "
  created_at: 2023-06-23 07:24:40+00:00
  edited: false
  hidden: false
  id: 649556c8f02a9f75bfc7ea93
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: InstaDeepAI/nucleotide-transformer-500m-human-ref
repo_type: model
status: open
target_branch: null
title: Embeddings generated are different between GitHub repo and Hugging Face models
