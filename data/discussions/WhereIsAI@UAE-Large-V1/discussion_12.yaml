!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MZurk
conflicting_files: null
created_at: 2024-01-22 22:15:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9da4832aae30489de241a820d0512253.svg
      fullname: Malik Zurkiyeh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MZurk
      type: user
    createdAt: '2024-01-22T22:15:27.000Z'
    data:
      edited: true
      editors:
      - MZurk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7114226222038269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9da4832aae30489de241a820d0512253.svg
          fullname: Malik Zurkiyeh
          isHf: false
          isPro: false
          name: MZurk
          type: user
        html: '<p>Everytime I try to run this model it keeps printing "killed". when
          I debugged it, it prints out </p>

          <p>"Out of memory: Killed process 1221 (python3) total-vm:42467440kB, anon-rss:7566620kB,
          file-rss:0kB, shmem-rss:4kB, UID:1000 pgtables:27940kB oom_score_adj:0"</p>

          <p>Keep in mind: im testing it out using the sample code</p>

          <pre><code>from angle_emb import AnglE


          angle = AnglE.from_pretrained(''WhereIsAI/UAE-Large-V1'', pooling_strategy=''cls'').cuda()

          vec = angle.encode(''hello world'', to_numpy=True)

          print(vec)

          vecs = angle.encode([''hello world1'', ''hello world2''], to_numpy=True)

          print(vecs)

          </code></pre>

          <p>and I made sure to install everything as specified by its requirements.txt
          file</p>

          <p>I am running this on vscode using a WSL terminal. Any solutions?</p>

          '
        raw: "Everytime I try to run this model it keeps printing \"killed\". when\
          \ I debugged it, it prints out \n\n\"Out of memory: Killed process 1221\
          \ (python3) total-vm:42467440kB, anon-rss:7566620kB, file-rss:0kB, shmem-rss:4kB,\
          \ UID:1000 pgtables:27940kB oom_score_adj:0\"\n\nKeep in mind: im testing\
          \ it out using the sample code\n\n```\nfrom angle_emb import AnglE\n\nangle\
          \ = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n\
          vec = angle.encode('hello world', to_numpy=True)\nprint(vec)\nvecs = angle.encode(['hello\
          \ world1', 'hello world2'], to_numpy=True)\nprint(vecs)\n\n```\nand I made\
          \ sure to install everything as specified by its requirements.txt file\n\
          \nI am running this on vscode using a WSL terminal. Any solutions?"
        updatedAt: '2024-01-22T22:16:29.941Z'
      numEdits: 1
      reactions: []
    id: 65aee8ff77381188bd4d3aa8
    type: comment
  author: MZurk
  content: "Everytime I try to run this model it keeps printing \"killed\". when I\
    \ debugged it, it prints out \n\n\"Out of memory: Killed process 1221 (python3)\
    \ total-vm:42467440kB, anon-rss:7566620kB, file-rss:0kB, shmem-rss:4kB, UID:1000\
    \ pgtables:27940kB oom_score_adj:0\"\n\nKeep in mind: im testing it out using\
    \ the sample code\n\n```\nfrom angle_emb import AnglE\n\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
    \ pooling_strategy='cls').cuda()\nvec = angle.encode('hello world', to_numpy=True)\n\
    print(vec)\nvecs = angle.encode(['hello world1', 'hello world2'], to_numpy=True)\n\
    print(vecs)\n\n```\nand I made sure to install everything as specified by its\
    \ requirements.txt file\n\nI am running this on vscode using a WSL terminal. Any\
    \ solutions?"
  created_at: 2024-01-22 22:15:27+00:00
  edited: true
  hidden: false
  id: 65aee8ff77381188bd4d3aa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2024-01-23T03:08:05.000Z'
    data:
      edited: true
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6043553352355957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: "<p>i didn't test it on WSL.  So I am not sure whether it is a problem\
          \ of <code>transformers</code>. </p>\n<p>Could you test it using transformers?\
          \ as follows</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModel, AutoTokenizer\n\n\
          model_id = <span class=\"hljs-string\">'WhereIsAI/UAE-Large-V1'</span>\n\
          tokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModel.from_pretrained(model_id).cuda()\n\
          \ninputs = <span class=\"hljs-string\">'hello world!'</span>\ntok = tokenizer([inputs],\
          \ return_tensors=<span class=\"hljs-string\">'pt'</span>)\n<span class=\"\
          hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> tok.items():\n\
          \    tok[k] = v.cuda()\nhidden_state = model(**tok).last_hidden_state\n\
          vec = hidden_state[:, <span class=\"hljs-number\">0</span>]\n<span class=\"\
          hljs-built_in\">print</span>(vec)\n</code></pre>\n"
        raw: "i didn't test it on WSL.  So I am not sure whether it is a problem of\
          \ `transformers`. \n\nCould you test it using transformers? as follows\n\
          \n```python\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\
          \nmodel_id = 'WhereIsAI/UAE-Large-V1'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModel.from_pretrained(model_id).cuda()\n\ninputs = 'hello world!'\n\
          tok = tokenizer([inputs], return_tensors='pt')\nfor k, v in tok.items():\n\
          \    tok[k] = v.cuda()\nhidden_state = model(**tok).last_hidden_state\n\
          vec = hidden_state[:, 0]\nprint(vec)\n```"
        updatedAt: '2024-01-23T03:08:33.035Z'
      numEdits: 2
      reactions: []
    id: 65af2d957af20f8e929076d1
    type: comment
  author: SeanLee97
  content: "i didn't test it on WSL.  So I am not sure whether it is a problem of\
    \ `transformers`. \n\nCould you test it using transformers? as follows\n\n```python\n\
    import torch\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_id =\
    \ 'WhereIsAI/UAE-Large-V1'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
    model = AutoModel.from_pretrained(model_id).cuda()\n\ninputs = 'hello world!'\n\
    tok = tokenizer([inputs], return_tensors='pt')\nfor k, v in tok.items():\n   \
    \ tok[k] = v.cuda()\nhidden_state = model(**tok).last_hidden_state\nvec = hidden_state[:,\
    \ 0]\nprint(vec)\n```"
  created_at: 2024-01-23 03:08:05+00:00
  edited: true
  hidden: false
  id: 65af2d957af20f8e929076d1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: WhereIsAI/UAE-Large-V1
repo_type: model
status: open
target_branch: null
title: Resource Requirement
