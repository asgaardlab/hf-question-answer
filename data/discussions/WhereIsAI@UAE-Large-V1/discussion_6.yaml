!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ABHINAYY
conflicting_files: null
created_at: 2023-12-24 19:38:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6b7d4e2f016ba187a11e596d6bbacd9.svg
      fullname: YADAV
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ABHINAYY
      type: user
    createdAt: '2023-12-24T19:38:11.000Z'
    data:
      edited: false
      editors:
      - ABHINAYY
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49521008133888245
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6b7d4e2f016ba187a11e596d6bbacd9.svg
          fullname: YADAV
          isHf: false
          isPro: false
          name: ABHINAYY
          type: user
        html: "<p>import streamlit as st<br>from transformers import AutoTokenizer,\
          \ AutoModelForSeq2SeqLM<br>from transformers import pipeline<br>import torch<br>import\
          \ base64<br>import textwrap<br>from langchain.embeddings import SentenceTransformerEmbeddings<br>from\
          \ langchain.vectorstores import Chroma<br>from langchain.llms import HuggingFacePipeline<br>from\
          \ langchain.chains import RetrievalQA<br>from constants import CHROMA_SETTINGS</p>\n\
          <p>checkpoint = \"LaMini-T5-738M\"<br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>base_model\
          \ = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='cpu',\
          \ torch_dtype=torch.float32) #device_map=cuda/cpu</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;st&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/st\">@<span class=\"underline\">st</span></a></span>\n\
          \n\t</span></span>.cache_resource<br>def llm_pipeline():<br>    pipe = pipeline(<br>\
          \        'text2text-generation',<br>        model = base_model,<br>    \
          \    tokenizer = tokenizer,<br>        max_length = 256,<br>        do_sample=True,<br>\
          \        temperature = 0.3,<br>        top_p = 0.95<br>    )<br>    local_llm\
          \ = HuggingFacePipeline(pipeline=pipe)<br>    return local_llm</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;st&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/st\">@<span class=\"underline\">st</span></a></span>\n\
          \n\t</span></span>.cache_resource<br>def qa_llm():<br>    llm = llm_pipeline()<br>\
          \    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\"\
          )<br>    db = Chroma(persist_directory=\"db\", embedding_function=embeddings,\
          \ client_settings=CHROMA_SETTINGS)<br>    retriever = db.as_retriever()<br>\
          \    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,\
          \ return_source_documents=True)<br>    return qa</p>\n<p>def process_answer(instruction):<br>\
          \    response = ''<br>    instruction = instruction<br>    qa = qa_llm()<br>\
          \    generated_text = qa(instruction)<br>    answer = generated_text['result']</p>\n\
          <pre><code>return answer,generated_text\n</code></pre>\n<p>def main():<br>\
          \    st.title(\"Search From your PDF\U0001F4C4\")<br>    with st.expander(\"\
          About the App\"):<br>        st.markdown(<br>            \"\"\"<br>    \
          \        This is a Generative AI powered Question and Answering app that\
          \ responds to questions about your PDF File.<br>            \"\"\"<br> \
          \       )<br>    question = st.text_area(\"Enter your Question\")<br>  \
          \  if st.button(\"Ask\"):<br>        st.info(\"Your Question: \" + question)</p>\n\
          <pre><code>    st.info(\"Your Answer\")\n    answer, metadata = process_answer(question)\n\
          \    st.write(answer)\n    st.write(metadata)\n</code></pre>\n<p>if <strong>name</strong>\
          \ == '<strong>main</strong>':<br>    main()</p>\n"
        raw: "import streamlit as st \r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\r\
          \nfrom transformers import pipeline\r\nimport torch\r\nimport base64\r\n\
          import textwrap\r\nfrom langchain.embeddings import SentenceTransformerEmbeddings\r\
          \nfrom langchain.vectorstores import Chroma \r\nfrom langchain.llms import\
          \ HuggingFacePipeline\r\nfrom langchain.chains import RetrievalQA\r\nfrom\
          \ constants import CHROMA_SETTINGS\r\n\r\ncheckpoint = \"LaMini-T5-738M\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\nbase_model\
          \ = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='cpu',\
          \ torch_dtype=torch.float32) #device_map=cuda/cpu\r\n\r\n@st.cache_resource\r\
          \ndef llm_pipeline():\r\n    pipe = pipeline(\r\n        'text2text-generation',\r\
          \n        model = base_model,\r\n        tokenizer = tokenizer,\r\n    \
          \    max_length = 256,\r\n        do_sample=True,\r\n        temperature\
          \ = 0.3,\r\n        top_p = 0.95\r\n    )\r\n    local_llm = HuggingFacePipeline(pipeline=pipe)\r\
          \n    return local_llm\r\n\r\n@st.cache_resource\r\ndef qa_llm():\r\n  \
          \  llm = llm_pipeline()\r\n    embeddings = SentenceTransformerEmbeddings(model_name=\"\
          all-MiniLM-L6-v2\")\r\n    db = Chroma(persist_directory=\"db\", embedding_function=embeddings,\
          \ client_settings=CHROMA_SETTINGS)\r\n    retriever = db.as_retriever()\r\
          \n    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,\
          \ return_source_documents=True)\r\n    return qa\r\n\r\ndef process_answer(instruction):\r\
          \n    response = ''\r\n    instruction = instruction\r\n    qa = qa_llm()\r\
          \n    generated_text = qa(instruction)\r\n    answer = generated_text['result']\r\
          \n\r\n    return answer,generated_text\r\n\r\ndef main():\r\n    st.title(\"\
          Search From your PDF\U0001F4C4\")\r\n    with st.expander(\"About the App\"\
          ):\r\n        st.markdown(\r\n            \"\"\"\r\n            This is\
          \ a Generative AI powered Question and Answering app that responds to questions\
          \ about your PDF File.\r\n            \"\"\"\r\n        )\r\n    question\
          \ = st.text_area(\"Enter your Question\")\r\n    if st.button(\"Ask\"):\r\
          \n        st.info(\"Your Question: \" + question)\r\n\r\n        st.info(\"\
          Your Answer\")\r\n        answer, metadata = process_answer(question)\r\n\
          \        st.write(answer)\r\n        st.write(metadata)\r\n\r\n\r\nif __name__\
          \ == '__main__':\r\n    main()\r\n"
        updatedAt: '2023-12-24T19:38:11.589Z'
      numEdits: 0
      reactions: []
    id: 658888a361f2dd8f667ce062
    type: comment
  author: ABHINAYY
  content: "import streamlit as st \r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\r\
    \nfrom transformers import pipeline\r\nimport torch\r\nimport base64\r\nimport\
    \ textwrap\r\nfrom langchain.embeddings import SentenceTransformerEmbeddings\r\
    \nfrom langchain.vectorstores import Chroma \r\nfrom langchain.llms import HuggingFacePipeline\r\
    \nfrom langchain.chains import RetrievalQA\r\nfrom constants import CHROMA_SETTINGS\r\
    \n\r\ncheckpoint = \"LaMini-T5-738M\"\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\
    \nbase_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='cpu',\
    \ torch_dtype=torch.float32) #device_map=cuda/cpu\r\n\r\n@st.cache_resource\r\n\
    def llm_pipeline():\r\n    pipe = pipeline(\r\n        'text2text-generation',\r\
    \n        model = base_model,\r\n        tokenizer = tokenizer,\r\n        max_length\
    \ = 256,\r\n        do_sample=True,\r\n        temperature = 0.3,\r\n        top_p\
    \ = 0.95\r\n    )\r\n    local_llm = HuggingFacePipeline(pipeline=pipe)\r\n  \
    \  return local_llm\r\n\r\n@st.cache_resource\r\ndef qa_llm():\r\n    llm = llm_pipeline()\r\
    \n    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\"\
    )\r\n    db = Chroma(persist_directory=\"db\", embedding_function=embeddings,\
    \ client_settings=CHROMA_SETTINGS)\r\n    retriever = db.as_retriever()\r\n  \
    \  qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,\
    \ return_source_documents=True)\r\n    return qa\r\n\r\ndef process_answer(instruction):\r\
    \n    response = ''\r\n    instruction = instruction\r\n    qa = qa_llm()\r\n\
    \    generated_text = qa(instruction)\r\n    answer = generated_text['result']\r\
    \n\r\n    return answer,generated_text\r\n\r\ndef main():\r\n    st.title(\"Search\
    \ From your PDF\U0001F4C4\")\r\n    with st.expander(\"About the App\"):\r\n \
    \       st.markdown(\r\n            \"\"\"\r\n            This is a Generative\
    \ AI powered Question and Answering app that responds to questions about your\
    \ PDF File.\r\n            \"\"\"\r\n        )\r\n    question = st.text_area(\"\
    Enter your Question\")\r\n    if st.button(\"Ask\"):\r\n        st.info(\"Your\
    \ Question: \" + question)\r\n\r\n        st.info(\"Your Answer\")\r\n       \
    \ answer, metadata = process_answer(question)\r\n        st.write(answer)\r\n\
    \        st.write(metadata)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\
    \n"
  created_at: 2023-12-24 19:38:11+00:00
  edited: false
  hidden: false
  id: 658888a361f2dd8f667ce062
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-25T02:39:49.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8952215909957886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;ABHINAYY&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ABHINAYY\"\
          >@<span class=\"underline\">ABHINAYY</span></a></span>\n\n\t</span></span>\
          \ , thanks for following our work!</p>\n<p>Currently, UAE cannot be used\
          \ in langchain.<br>But we have created a PR to support UAE in langchain:\
          \ <a rel=\"nofollow\" href=\"https://github.com/langchain-ai/langchain/pull/15134\"\
          >https://github.com/langchain-ai/langchain/pull/15134</a>.<br>When it is\
          \ merged, I will inform you.<br>Stay tune:)</p>\n"
        raw: "hi @ABHINAYY , thanks for following our work!\n\nCurrently, UAE cannot\
          \ be used in langchain. \nBut we have created a PR to support UAE in langchain:\
          \ https://github.com/langchain-ai/langchain/pull/15134. \nWhen it is merged,\
          \ I will inform you. \nStay tune:)"
        updatedAt: '2023-12-25T02:39:49.705Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - existme
    id: 6588eb7508f83845fc4307f8
    type: comment
  author: SeanLee97
  content: "hi @ABHINAYY , thanks for following our work!\n\nCurrently, UAE cannot\
    \ be used in langchain. \nBut we have created a PR to support UAE in langchain:\
    \ https://github.com/langchain-ai/langchain/pull/15134. \nWhen it is merged, I\
    \ will inform you. \nStay tune:)"
  created_at: 2023-12-25 02:39:49+00:00
  edited: false
  hidden: false
  id: 6588eb7508f83845fc4307f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/949dc23b75c9a73e696b8f5ba25244da.svg
      fullname: rudy chip
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pribadihcr
      type: user
    createdAt: '2023-12-25T03:36:57.000Z'
    data:
      edited: false
      editors:
      - pribadihcr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48518964648246765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/949dc23b75c9a73e696b8f5ba25244da.svg
          fullname: rudy chip
          isHf: false
          isPro: false
          name: pribadihcr
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;SeanLee97&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SeanLee97\"\
          >@<span class=\"underline\">SeanLee97</span></a></span>\n\n\t</span></span>\
          \ How to finetune UAE-LARGE -V1? any tutorial? thanks</p>\n"
        raw: Hi @SeanLee97 How to finetune UAE-LARGE -V1? any tutorial? thanks
        updatedAt: '2023-12-25T03:36:57.115Z'
      numEdits: 0
      reactions: []
    id: 6588f8d9991d8e7fb226747f
    type: comment
  author: pribadihcr
  content: Hi @SeanLee97 How to finetune UAE-LARGE -V1? any tutorial? thanks
  created_at: 2023-12-25 03:36:57+00:00
  edited: false
  hidden: false
  id: 6588f8d9991d8e7fb226747f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-25T03:48:42.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5776457786560059
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pribadihcr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pribadihcr\">@<span class=\"\
          underline\">pribadihcr</span></a></span>\n\n\t</span></span>  Due to time\
          \ limitations, we haven't provided a tutorial yet.<br>You can follow this\
          \ script to fine-tune your model: <a rel=\"nofollow\" href=\"https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\"\
          >https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py</a></p>\n\
          <p>There are two steps:</p>\n<ol>\n<li>you need to prepare your data into\
          \ <code>jsonl</code>, as follows:</li>\n</ol>\n<pre><code>{\"text1\": \"\
          here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n{\"text1\"\
          : \"here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n</code></pre>\n\
          <ol start=\"2\">\n<li>start to fine-tune your model, as follows:</li>\n\
          </ol>\n<pre><code class=\"language-bash\">CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun\
          \ --nproc_per_node=4 --master_port=1234 train.py \\\n--train_path your_custom_data.jsonl\
          \ --save_dir ./ckpts/your-custom-model \\\n--model_name WhereIsAI/UAE-Large-V1\
          \ \\\n--w2 35 --learning_rate 5e-8 --maxlen 128 \\\n--epochs 2 \\\n--batch_size\
          \ 32 \\\n--apply_lora 0 \\\n--save_steps 1000 --seed -1 --gradient_accumulation_steps\
          \ 4 --fp16 1\n</code></pre>\n<p>Please set a small <code>learning_rate</code>\
          \ if you fine-tune based on <code>WhereIsAI/UAE-Large-V1</code></p>\n"
        raw: "@pribadihcr  Due to time limitations, we haven't provided a tutorial\
          \ yet. \nYou can follow this script to fine-tune your model: https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\n\
          \nThere are two steps:\n\n1) you need to prepare your data into `jsonl`,\
          \ as follows:\n```\n{\"text1\": \"here is text1\", \"text2\": \"here is\
          \ text2\", \"label\": 0/1}\n{\"text1\": \"here is text1\", \"text2\": \"\
          here is text2\", \"label\": 0/1}\n```\n\n2) start to fine-tune your model,\
          \ as follows:\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4\
          \ --master_port=1234 train.py \\\n--train_path your_custom_data.jsonl --save_dir\
          \ ./ckpts/your-custom-model \\\n--model_name WhereIsAI/UAE-Large-V1 \\\n\
          --w2 35 --learning_rate 5e-8 --maxlen 128 \\\n--epochs 2 \\\n--batch_size\
          \ 32 \\\n--apply_lora 0 \\\n--save_steps 1000 --seed -1 --gradient_accumulation_steps\
          \ 4 --fp16 1\n```\n\nPlease set a small `learning_rate` if you fine-tune\
          \ based on `WhereIsAI/UAE-Large-V1`\n"
        updatedAt: '2023-12-25T03:48:42.676Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pribadihcr
    id: 6588fb9aa66bd1cdb27fbd55
    type: comment
  author: SeanLee97
  content: "@pribadihcr  Due to time limitations, we haven't provided a tutorial yet.\
    \ \nYou can follow this script to fine-tune your model: https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\n\
    \nThere are two steps:\n\n1) you need to prepare your data into `jsonl`, as follows:\n\
    ```\n{\"text1\": \"here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n\
    {\"text1\": \"here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n\
    ```\n\n2) start to fine-tune your model, as follows:\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3\
    \ torchrun --nproc_per_node=4 --master_port=1234 train.py \\\n--train_path your_custom_data.jsonl\
    \ --save_dir ./ckpts/your-custom-model \\\n--model_name WhereIsAI/UAE-Large-V1\
    \ \\\n--w2 35 --learning_rate 5e-8 --maxlen 128 \\\n--epochs 2 \\\n--batch_size\
    \ 32 \\\n--apply_lora 0 \\\n--save_steps 1000 --seed -1 --gradient_accumulation_steps\
    \ 4 --fp16 1\n```\n\nPlease set a small `learning_rate` if you fine-tune based\
    \ on `WhereIsAI/UAE-Large-V1`\n"
  created_at: 2023-12-25 03:48:42+00:00
  edited: false
  hidden: false
  id: 6588fb9aa66bd1cdb27fbd55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-25T03:50:39.000Z'
    data:
      edited: true
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5877608060836792
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;pribadihcr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pribadihcr\"\
          >@<span class=\"underline\">pribadihcr</span></a></span>\n\n\t</span></span>\
          \  Due to time limitations, we haven't provided a tutorial yet.<br>You can\
          \ follow this script to fine-tune your model: <a rel=\"nofollow\" href=\"\
          https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\"\
          >https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py</a></p>\n\
          <p>There are two steps:</p>\n<ol>\n<li>you need to prepare your data into\
          \ <code>jsonl</code>, as follows:</li>\n</ol>\n<pre><code>{\"text1\": \"\
          here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n{\"text1\"\
          : \"here is text1\", \"text2\": \"here is text2\", \"label\": 0/1}\n</code></pre>\n\
          <ol start=\"2\">\n<li>start to fine-tune your model, as follows:</li>\n\
          </ol>\n<pre><code class=\"language-bash\">CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun\
          \ --nproc_per_node=4 --master_port=1234 train.py \\\n--train_path your_custom_data.jsonl\
          \ --save_dir ./ckpts/your-custom-model \\\n--model_name WhereIsAI/UAE-Large-V1\
          \ \\\n--w2 35 --learning_rate 5e-8 --maxlen 128 \\\n--epochs 2 \\\n--batch_size\
          \ 32 \\\n--apply_lora 0 \\\n--save_steps 1000 --seed -1 --gradient_accumulation_steps\
          \ 4 --fp16 1\n</code></pre>\n<p>Please set a small <code>learning_rate</code>\
          \ if you fine-tune based on <code>WhereIsAI/UAE-Large-V1</code></p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;pribadihcr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pribadihcr\">@<span class=\"\
          underline\">pribadihcr</span></a></span>\n\n\t</span></span>  BTW, if you\
          \ would like to fine-tune a retrieval-based model, please specify <code>--prompt\
          \ \"Represent this sentence for searching relevant passages: {text}\"</code></p>\n"
        raw: "> @pribadihcr  Due to time limitations, we haven't provided a tutorial\
          \ yet. \n> You can follow this script to fine-tune your model: https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\n\
          > \n> There are two steps:\n> \n> 1) you need to prepare your data into\
          \ `jsonl`, as follows:\n> ```\n> {\"text1\": \"here is text1\", \"text2\"\
          : \"here is text2\", \"label\": 0/1}\n> {\"text1\": \"here is text1\", \"\
          text2\": \"here is text2\", \"label\": 0/1}\n> ```\n> \n> 2) start to fine-tune\
          \ your model, as follows:\n> \n> ```bash\n> CUDA_VISIBLE_DEVICES=0,1,2,3\
          \ torchrun --nproc_per_node=4 --master_port=1234 train.py \\\n> --train_path\
          \ your_custom_data.jsonl --save_dir ./ckpts/your-custom-model \\\n> --model_name\
          \ WhereIsAI/UAE-Large-V1 \\\n> --w2 35 --learning_rate 5e-8 --maxlen 128\
          \ \\\n> --epochs 2 \\\n> --batch_size 32 \\\n> --apply_lora 0 \\\n> --save_steps\
          \ 1000 --seed -1 --gradient_accumulation_steps 4 --fp16 1\n> ```\n> \n>\
          \ Please set a small `learning_rate` if you fine-tune based on `WhereIsAI/UAE-Large-V1`\n\
          \n@pribadihcr  BTW, if you would like to fine-tune a retrieval-based model,\
          \ please specify `--prompt \"Represent this sentence for searching relevant\
          \ passages: {text}\"`\n"
        updatedAt: '2023-12-25T03:51:01.553Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pribadihcr
    id: 6588fc0fbfdd9f441099031e
    type: comment
  author: SeanLee97
  content: "> @pribadihcr  Due to time limitations, we haven't provided a tutorial\
    \ yet. \n> You can follow this script to fine-tune your model: https://github.com/SeanLee97/AnglE/blob/angle-bellm/examples/UAE/train.py\n\
    > \n> There are two steps:\n> \n> 1) you need to prepare your data into `jsonl`,\
    \ as follows:\n> ```\n> {\"text1\": \"here is text1\", \"text2\": \"here is text2\"\
    , \"label\": 0/1}\n> {\"text1\": \"here is text1\", \"text2\": \"here is text2\"\
    , \"label\": 0/1}\n> ```\n> \n> 2) start to fine-tune your model, as follows:\n\
    > \n> ```bash\n> CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=1234\
    \ train.py \\\n> --train_path your_custom_data.jsonl --save_dir ./ckpts/your-custom-model\
    \ \\\n> --model_name WhereIsAI/UAE-Large-V1 \\\n> --w2 35 --learning_rate 5e-8\
    \ --maxlen 128 \\\n> --epochs 2 \\\n> --batch_size 32 \\\n> --apply_lora 0 \\\n\
    > --save_steps 1000 --seed -1 --gradient_accumulation_steps 4 --fp16 1\n> ```\n\
    > \n> Please set a small `learning_rate` if you fine-tune based on `WhereIsAI/UAE-Large-V1`\n\
    \n@pribadihcr  BTW, if you would like to fine-tune a retrieval-based model, please\
    \ specify `--prompt \"Represent this sentence for searching relevant passages:\
    \ {text}\"`\n"
  created_at: 2023-12-25 03:50:39+00:00
  edited: true
  hidden: false
  id: 6588fc0fbfdd9f441099031e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: WhereIsAI/UAE-Large-V1
repo_type: model
status: open
target_branch: null
title: wants to use UAE-LARGE -V1 instead of all-MiniLM-L6-v2 . CODE is given below
  please correct it
