!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Reggie
conflicting_files: null
created_at: 2024-01-24 12:26:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2024-01-24T12:26:50.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7128944993019104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Hi,<br>Great model here. Been looking for something to replace my
          current e5-base-v2 and this looks like it!<br>Was wondering if the model
          has a token limit of 512 by default when I load it via</p>

          <pre><code>angle = AnglE.from_pretrained(''WhereIsAI/UAE-Large-V1'', pooling_strategy=''cls'').cuda()

          angle.set_prompt(prompt=Prompts.C)

          </code></pre>

          <p>or do I have to explicity set the token limit?</p>

          <p>For eg in Sentence Transformers I do</p>

          <pre><code>model = SentenceTransformer(''intfloat/e5-base-v2'')

          model.max_seq_length = 512

          </code></pre>

          <p>because the default token limit is 128.</p>

          <p>Another thing that confuses me is the Prompts. I''m using the model for
          Retrieval and Summarizing. Do I have to apply the prompts?<br>The example
          in Huggingface gives this:</p>

          <pre><code>angle = AnglE.from_pretrained(''WhereIsAI/UAE-Large-V1'', pooling_strategy=''cls'').cuda()

          angle.set_prompt(prompt=Prompts.C)

          </code></pre>

          <p>However this <a rel="nofollow" href="https://colab.research.google.com/drive/1WOYD6f8gb_wpkUm_57K8pEDgjlGJd6oB?usp=drive_link">colab</a>
          from your Github does not have the code using a prompt, so I''m a bit confused.
          </p>

          <p>Also do I have to in some way explicity define what prompt I should use
          for summarization or retrieval or does the model somehow figure out what
          is needed and apply the right prompt? </p>

          <p>For eg in e5, we specifically prepend query: or passage: while embedding
          the text.</p>

          <p>Last question: How do I use the onnx format. Will the angle-emb package
          support it directly? Right now it loads the safetensors version from HF.
          Or do I have to pip install onnx, in which case how do I configure it for
          Prompts.C</p>

          '
        raw: "Hi,\r\nGreat model here. Been looking for something to replace my current\
          \ e5-base-v2 and this looks like it!\r\nWas wondering if the model has a\
          \ token limit of 512 by default when I load it via\r\n```\r\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ pooling_strategy='cls').cuda()\r\nangle.set_prompt(prompt=Prompts.C)\r\
          \n```\r\n\r\nor do I have to explicity set the token limit?\r\n\r\nFor eg\
          \ in Sentence Transformers I do\r\n\r\n```\r\nmodel = SentenceTransformer('intfloat/e5-base-v2')\r\
          \nmodel.max_seq_length = 512\r\n```\r\nbecause the default token limit is\
          \ 128.\r\n\r\nAnother thing that confuses me is the Prompts. I'm using the\
          \ model for Retrieval and Summarizing. Do I have to apply the prompts?\r\
          \nThe example in Huggingface gives this:\r\n```\r\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ pooling_strategy='cls').cuda()\r\nangle.set_prompt(prompt=Prompts.C)\r\
          \n```\r\nHowever this [colab](https://colab.research.google.com/drive/1WOYD6f8gb_wpkUm_57K8pEDgjlGJd6oB?usp=drive_link)\
          \ from your Github does not have the code using a prompt, so I'm a bit confused.\
          \ \r\n\r\nAlso do I have to in some way explicity define what prompt I should\
          \ use for summarization or retrieval or does the model somehow figure out\
          \ what is needed and apply the right prompt? \r\n\r\nFor eg in e5, we specifically\
          \ prepend query: or passage: while embedding the text.\r\n\r\nLast question:\
          \ How do I use the onnx format. Will the angle-emb package support it directly?\
          \ Right now it loads the safetensors version from HF. Or do I have to pip\
          \ install onnx, in which case how do I configure it for Prompts.C\r\n"
        updatedAt: '2024-01-24T12:26:50.620Z'
      numEdits: 0
      reactions: []
    id: 65b1020a7ac1fb9fc718f86c
    type: comment
  author: Reggie
  content: "Hi,\r\nGreat model here. Been looking for something to replace my current\
    \ e5-base-v2 and this looks like it!\r\nWas wondering if the model has a token\
    \ limit of 512 by default when I load it via\r\n```\r\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
    \ pooling_strategy='cls').cuda()\r\nangle.set_prompt(prompt=Prompts.C)\r\n```\r\
    \n\r\nor do I have to explicity set the token limit?\r\n\r\nFor eg in Sentence\
    \ Transformers I do\r\n\r\n```\r\nmodel = SentenceTransformer('intfloat/e5-base-v2')\r\
    \nmodel.max_seq_length = 512\r\n```\r\nbecause the default token limit is 128.\r\
    \n\r\nAnother thing that confuses me is the Prompts. I'm using the model for Retrieval\
    \ and Summarizing. Do I have to apply the prompts?\r\nThe example in Huggingface\
    \ gives this:\r\n```\r\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
    \ pooling_strategy='cls').cuda()\r\nangle.set_prompt(prompt=Prompts.C)\r\n```\r\
    \nHowever this [colab](https://colab.research.google.com/drive/1WOYD6f8gb_wpkUm_57K8pEDgjlGJd6oB?usp=drive_link)\
    \ from your Github does not have the code using a prompt, so I'm a bit confused.\
    \ \r\n\r\nAlso do I have to in some way explicity define what prompt I should\
    \ use for summarization or retrieval or does the model somehow figure out what\
    \ is needed and apply the right prompt? \r\n\r\nFor eg in e5, we specifically\
    \ prepend query: or passage: while embedding the text.\r\n\r\nLast question: How\
    \ do I use the onnx format. Will the angle-emb package support it directly? Right\
    \ now it loads the safetensors version from HF. Or do I have to pip install onnx,\
    \ in which case how do I configure it for Prompts.C\r\n"
  created_at: 2024-01-24 12:26:50+00:00
  edited: false
  hidden: false
  id: 65b1020a7ac1fb9fc718f86c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2024-01-24T14:53:27.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7382027506828308
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: '<p>Thanks for following our work.</p>

          <p>First, our model has a token limit of 512, and you don''t need to specify
          the max token explicitly. </p>

          <p>Second, if you use it for retrieval, you need to set a prompt for the
          query (no need for documents). There are two ways:</p>

          <ul>

          <li><code>angle.set_prompt(Prompts.C); angle.encode(query)</code></li>

          <li>manually apply the prompt to the query text when calling the <code>encode()</code>
          function. For example <code>angle.encode(Prompts.C.format(text=query))</code>.</li>

          </ul>

          <p>For other non-retrieval scenarios, there is no need to set the prompt.</p>

          <p><strong>If you use it for retrieval and summarization at the same time,
          I suggest manually setting the prompt when calling <code>encode()</code>,
          i.e., the second way.</strong></p>

          <p>Third, currently, <code>angle_emb</code> does not support onnx inference.
          You can use HuggingFace <code>optimum</code> for inference.<br>Here is the
          usage: <a href="https://huggingface.co/WhereIsAI/UAE-Large-V1/discussions/10#659df9c060736ff2a6dc04e7">https://huggingface.co/WhereIsAI/UAE-Large-V1/discussions/10#659df9c060736ff2a6dc04e7</a></p>

          '
        raw: "Thanks for following our work.\n\nFirst, our model has a token limit\
          \ of 512, and you don't need to specify the max token explicitly. \n\nSecond,\
          \ if you use it for retrieval, you need to set a prompt for the query (no\
          \ need for documents). There are two ways:\n- `angle.set_prompt(Prompts.C);\
          \ angle.encode(query)`\n- manually apply the prompt to the query text when\
          \ calling the `encode()` function. For example `angle.encode(Prompts.C.format(text=query))`.\
          \ \n\nFor other non-retrieval scenarios, there is no need to set the prompt.\n\
          \n**If you use it for retrieval and summarization at the same time, I suggest\
          \ manually setting the prompt when calling `encode()`, i.e., the second\
          \ way.**\n\nThird, currently, `angle_emb` does not support onnx inference.\
          \ You can use HuggingFace `optimum` for inference. \nHere is the usage:\
          \ https://huggingface.co/WhereIsAI/UAE-Large-V1/discussions/10#659df9c060736ff2a6dc04e7"
        updatedAt: '2024-01-24T14:53:27.673Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Reggie
    id: 65b12467313a2943c79346cc
    type: comment
  author: SeanLee97
  content: "Thanks for following our work.\n\nFirst, our model has a token limit of\
    \ 512, and you don't need to specify the max token explicitly. \n\nSecond, if\
    \ you use it for retrieval, you need to set a prompt for the query (no need for\
    \ documents). There are two ways:\n- `angle.set_prompt(Prompts.C); angle.encode(query)`\n\
    - manually apply the prompt to the query text when calling the `encode()` function.\
    \ For example `angle.encode(Prompts.C.format(text=query))`. \n\nFor other non-retrieval\
    \ scenarios, there is no need to set the prompt.\n\n**If you use it for retrieval\
    \ and summarization at the same time, I suggest manually setting the prompt when\
    \ calling `encode()`, i.e., the second way.**\n\nThird, currently, `angle_emb`\
    \ does not support onnx inference. You can use HuggingFace `optimum` for inference.\
    \ \nHere is the usage: https://huggingface.co/WhereIsAI/UAE-Large-V1/discussions/10#659df9c060736ff2a6dc04e7"
  created_at: 2024-01-24 14:53:27+00:00
  edited: false
  hidden: false
  id: 65b12467313a2943c79346cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2024-01-24T15:57:27.000Z'
    data:
      edited: true
      editors:
      - Reggie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47156816720962524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: "<p>Thanks.<br>Regarding onnx, I'm getting some unexpected results.<br>The\
          \ onnx is almost 4X slower than the regular embed.</p>\n<p>Here's my code.\
          \ Any idea what might be wrong? I'm running this on a T4 in colab.</p>\n\
          <pre><code>!pip install --upgrade optimum[onnxruntime] onnxruntime-gpu onnx\n\
          \nfrom optimum.onnxruntime import ORTModelForFeatureExtraction\nfrom optimum.pipelines\
          \ import pipeline\n\nmodel = ORTModelForFeatureExtraction.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ file_name=\"onnx/model.onnx\")\n\ndef onnx_embed(query):\n  extractor\
          \ = pipeline('feature-extraction', model=model)\n  output = extractor(query)\n\
          \  return output\n\nimport time\nquery = \" some test\"\ntimh = time.time()\n\
          avec = onnx_embed(query)[0][0]\nprint(time.time()-timh) # prints 0.28627943992614746\n\
          print(avec)\n</code></pre>\n<pre><code>!pip install -U angle-emb --quiet\n\
          from angle_emb import AnglE, Prompts\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ pooling_strategy='cls').cuda()\n\ndef uae_embed(query):\n vec = angle.encode(query,\
          \ to_numpy=True)\n  return vec.flatten().tolist()\n\nimport time\nquery\
          \ = \" some test\"\ntimh = time.time()\nvec = uae_embed(query)\nprint(vec)\n\
          print(time.time()-timh) # prints 0.07494688034057617\n</code></pre>\n<p>Also\
          \ wanted a bit of advice on one particular use case. I'm trying to do extractive\
          \ summarization in a passage and find the sentence in the passage that is\
          \ most similar to the whole passage.<br>So for example, in the passage:\
          \ The ship set out to sail around the world. It was manned by sailors from\
          \ around the world. They set out to sail yesterday.\"<br>It would choose\
          \ let's say \"The ship set out to sail around the world\" as the sentence\
          \ most similar to the whole passage.</p>\n<p>In this case, I am essentially\
          \ comparing 1 passage to 3 individual sentences. Would this be considered\
          \ retrieval as well? Do I have to add the query prompt to the passage embedding\
          \ here, but not add them to the individual sentence embeddings?</p>\n"
        raw: "Thanks.\nRegarding onnx, I'm getting some unexpected results.\nThe onnx\
          \ is almost 4X slower than the regular embed.\n\nHere's my code. Any idea\
          \ what might be wrong? I'm running this on a T4 in colab.\n\n```\n!pip install\
          \ --upgrade optimum[onnxruntime] onnxruntime-gpu onnx\n\nfrom optimum.onnxruntime\
          \ import ORTModelForFeatureExtraction\nfrom optimum.pipelines import pipeline\n\
          \nmodel = ORTModelForFeatureExtraction.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ file_name=\"onnx/model.onnx\")\n\ndef onnx_embed(query):\n  extractor\
          \ = pipeline('feature-extraction', model=model)\n  output = extractor(query)\n\
          \  return output\n\nimport time\nquery = \" some test\"\ntimh = time.time()\n\
          avec = onnx_embed(query)[0][0]\nprint(time.time()-timh) # prints 0.28627943992614746\n\
          print(avec)\n\n```\n\n```\n!pip install -U angle-emb --quiet\nfrom angle_emb\
          \ import AnglE, Prompts\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
          \ pooling_strategy='cls').cuda()\n\ndef uae_embed(query):\n vec = angle.encode(query,\
          \ to_numpy=True)\n  return vec.flatten().tolist()\n\nimport time\nquery\
          \ = \" some test\"\ntimh = time.time()\nvec = uae_embed(query)\nprint(vec)\n\
          print(time.time()-timh) # prints 0.07494688034057617\n```\n\nAlso wanted\
          \ a bit of advice on one particular use case. I'm trying to do extractive\
          \ summarization in a passage and find the sentence in the passage that is\
          \ most similar to the whole passage.\nSo for example, in the passage: The\
          \ ship set out to sail around the world. It was manned by sailors from around\
          \ the world. They set out to sail yesterday.\"\nIt would choose let's say\
          \ \"The ship set out to sail around the world\" as the sentence most similar\
          \ to the whole passage.\n\nIn this case, I am essentially comparing 1 passage\
          \ to 3 individual sentences. Would this be considered retrieval as well?\
          \ Do I have to add the query prompt to the passage embedding here, but not\
          \ add them to the individual sentence embeddings?\n"
        updatedAt: '2024-01-24T16:35:23.253Z'
      numEdits: 1
      reactions: []
    id: 65b1336754fa188a2f7b5704
    type: comment
  author: Reggie
  content: "Thanks.\nRegarding onnx, I'm getting some unexpected results.\nThe onnx\
    \ is almost 4X slower than the regular embed.\n\nHere's my code. Any idea what\
    \ might be wrong? I'm running this on a T4 in colab.\n\n```\n!pip install --upgrade\
    \ optimum[onnxruntime] onnxruntime-gpu onnx\n\nfrom optimum.onnxruntime import\
    \ ORTModelForFeatureExtraction\nfrom optimum.pipelines import pipeline\n\nmodel\
    \ = ORTModelForFeatureExtraction.from_pretrained('WhereIsAI/UAE-Large-V1', file_name=\"\
    onnx/model.onnx\")\n\ndef onnx_embed(query):\n  extractor = pipeline('feature-extraction',\
    \ model=model)\n  output = extractor(query)\n  return output\n\nimport time\n\
    query = \" some test\"\ntimh = time.time()\navec = onnx_embed(query)[0][0]\nprint(time.time()-timh)\
    \ # prints 0.28627943992614746\nprint(avec)\n\n```\n\n```\n!pip install -U angle-emb\
    \ --quiet\nfrom angle_emb import AnglE, Prompts\nangle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1',\
    \ pooling_strategy='cls').cuda()\n\ndef uae_embed(query):\n vec = angle.encode(query,\
    \ to_numpy=True)\n  return vec.flatten().tolist()\n\nimport time\nquery = \" some\
    \ test\"\ntimh = time.time()\nvec = uae_embed(query)\nprint(vec)\nprint(time.time()-timh)\
    \ # prints 0.07494688034057617\n```\n\nAlso wanted a bit of advice on one particular\
    \ use case. I'm trying to do extractive summarization in a passage and find the\
    \ sentence in the passage that is most similar to the whole passage.\nSo for example,\
    \ in the passage: The ship set out to sail around the world. It was manned by\
    \ sailors from around the world. They set out to sail yesterday.\"\nIt would choose\
    \ let's say \"The ship set out to sail around the world\" as the sentence most\
    \ similar to the whole passage.\n\nIn this case, I am essentially comparing 1\
    \ passage to 3 individual sentences. Would this be considered retrieval as well?\
    \ Do I have to add the query prompt to the passage embedding here, but not add\
    \ them to the individual sentence embeddings?\n"
  created_at: 2024-01-24 15:57:27+00:00
  edited: true
  hidden: false
  id: 65b1336754fa188a2f7b5704
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: WhereIsAI/UAE-Large-V1
repo_type: model
status: open
target_branch: null
title: Default token limit?
