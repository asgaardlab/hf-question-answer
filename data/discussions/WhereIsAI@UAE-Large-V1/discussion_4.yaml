!!python/object:huggingface_hub.community.DiscussionWithDetails
author: terilias
conflicting_files: null
created_at: 2023-12-14 10:43:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2023-12-14T10:43:55.000Z'
    data:
      edited: false
      editors:
      - terilias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.907414436340332
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
          fullname: Terzis
          isHf: false
          isPro: false
          name: terilias
          type: user
        html: '<p>Hello,</p>

          <p>I was reading the README file on the GitHub repository of the models
          and noticed that on the "English results" page (<a rel="nofollow" href="https://github.com/SeanLee97/AnglE#english-sts-results">https://github.com/SeanLee97/AnglE#english-sts-results</a>),
          certain Angle models include the name of the Llama LLM, such as "SeanLee97/angle-llama-7b-nli-v2".
          I''m curious about the significance of this. I attempted to glean insights
          from the associated paper, but I find myself confused. If I interpret correctly,
          the authors utilized Llama 2 to generate the training text dataset. Is this
          the only reason of the LLM''s name inclusion in the name? Furthermore, does
          this imply that these AnglE models are more effective when paired specifically
          with Llama 2 as the LLM in RAG applications?</p>

          '
        raw: "Hello,\r\n\r\nI was reading the README file on the GitHub repository\
          \ of the models and noticed that on the \"English results\" page (https://github.com/SeanLee97/AnglE#english-sts-results),\
          \ certain Angle models include the name of the Llama LLM, such as \"SeanLee97/angle-llama-7b-nli-v2\"\
          . I'm curious about the significance of this. I attempted to glean insights\
          \ from the associated paper, but I find myself confused. If I interpret\
          \ correctly, the authors utilized Llama 2 to generate the training text\
          \ dataset. Is this the only reason of the LLM's name inclusion in the name?\
          \ Furthermore, does this imply that these AnglE models are more effective\
          \ when paired specifically with Llama 2 as the LLM in RAG applications?"
        updatedAt: '2023-12-14T10:43:55.175Z'
      numEdits: 0
      reactions: []
    id: 657adc6b8d360b690d2e36dc
    type: comment
  author: terilias
  content: "Hello,\r\n\r\nI was reading the README file on the GitHub repository of\
    \ the models and noticed that on the \"English results\" page (https://github.com/SeanLee97/AnglE#english-sts-results),\
    \ certain Angle models include the name of the Llama LLM, such as \"SeanLee97/angle-llama-7b-nli-v2\"\
    . I'm curious about the significance of this. I attempted to glean insights from\
    \ the associated paper, but I find myself confused. If I interpret correctly,\
    \ the authors utilized Llama 2 to generate the training text dataset. Is this\
    \ the only reason of the LLM's name inclusion in the name? Furthermore, does this\
    \ imply that these AnglE models are more effective when paired specifically with\
    \ Llama 2 as the LLM in RAG applications?"
  created_at: 2023-12-14 10:43:55+00:00
  edited: false
  hidden: false
  id: 657adc6b8d360b690d2e36dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-14T11:05:11.000Z'
    data:
      edited: true
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8959988951683044
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;terilias&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/terilias\"\
          >@<span class=\"underline\">terilias</span></a></span>\n\n\t</span></span>,\
          \  there are two different usages of LLaMA in our paper, as follows:</p>\n\
          <ol>\n<li><p>First usage: we use the <strong>LLaMA as a feature extractor</strong>\
          \ to encode text and obtain corresponding text embedding. Subsequently,\
          \ we fine-tune the text embeddings using AnglE optimization. <strong>This\
          \ is the primary usage of LLaMA in our paper.</strong> The models listed\
          \ in <a rel=\"nofollow\" href=\"https://github.com/SeanLee97/AnglE#english-sts-results\"\
          >English STS Results</a> belong to this setting. In this setting, LLaMA\
          \ parameters will be optimized. </p>\n</li>\n<li><p>Second usage: we maintain\
          \ the original function of LLaMA and <strong>utilize it as an annotator</strong>\
          \ to generate supervised training datasets through prompt engineering. We\
          \ refer to this approach as LLM-supervised. <strong>In this setting, the\
          \ parameters of LLaMA remain fixed as we solely use it for text generation.</strong>\
          \ We introduce this method because AnglE is a supervised approach. However,\
          \ in domain-specific applications, supervised data is often limited. The\
          \ use of LLM-supervised can help address this issue by providing additional\
          \ labeled data.</p>\n</li>\n</ol>\n"
        raw: "Hi @terilias,  there are two different usages of LLaMA in our paper,\
          \ as follows:\n\n1) First usage: we use the **LLaMA as a feature extractor**\
          \ to encode text and obtain corresponding text embedding. Subsequently,\
          \ we fine-tune the text embeddings using AnglE optimization. **This is the\
          \ primary usage of LLaMA in our paper.** The models listed in [English STS\
          \ Results](https://github.com/SeanLee97/AnglE#english-sts-results) belong\
          \ to this setting. In this setting, LLaMA parameters will be optimized.\
          \ \n\n\n2) Second usage: we maintain the original function of LLaMA and\
          \ **utilize it as an annotator** to generate supervised training datasets\
          \ through prompt engineering. We refer to this approach as LLM-supervised.\
          \ **In this setting, the parameters of LLaMA remain fixed as we solely use\
          \ it for text generation.** We introduce this method because AnglE is a\
          \ supervised approach. However, in domain-specific applications, supervised\
          \ data is often limited. The use of LLM-supervised can help address this\
          \ issue by providing additional labeled data.\n"
        updatedAt: '2023-12-14T11:11:59.045Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - terilias
    id: 657ae167b8d76880f8703924
    type: comment
  author: SeanLee97
  content: "Hi @terilias,  there are two different usages of LLaMA in our paper, as\
    \ follows:\n\n1) First usage: we use the **LLaMA as a feature extractor** to encode\
    \ text and obtain corresponding text embedding. Subsequently, we fine-tune the\
    \ text embeddings using AnglE optimization. **This is the primary usage of LLaMA\
    \ in our paper.** The models listed in [English STS Results](https://github.com/SeanLee97/AnglE#english-sts-results)\
    \ belong to this setting. In this setting, LLaMA parameters will be optimized.\
    \ \n\n\n2) Second usage: we maintain the original function of LLaMA and **utilize\
    \ it as an annotator** to generate supervised training datasets through prompt\
    \ engineering. We refer to this approach as LLM-supervised. **In this setting,\
    \ the parameters of LLaMA remain fixed as we solely use it for text generation.**\
    \ We introduce this method because AnglE is a supervised approach. However, in\
    \ domain-specific applications, supervised data is often limited. The use of LLM-supervised\
    \ can help address this issue by providing additional labeled data.\n"
  created_at: 2023-12-14 11:05:11+00:00
  edited: true
  hidden: false
  id: 657ae167b8d76880f8703924
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-14T11:10:08.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9180062413215637
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: '<blockquote>

          <p>Hello,</p>

          <p>I was reading the README file on the GitHub repository of the models
          and noticed that on the "English results" page (<a rel="nofollow" href="https://github.com/SeanLee97/AnglE#english-sts-results">https://github.com/SeanLee97/AnglE#english-sts-results</a>),
          certain Angle models include the name of the Llama LLM, such as "SeanLee97/angle-llama-7b-nli-v2".
          I''m curious about the significance of this. I attempted to glean insights
          from the associated paper, but I find myself confused. If I interpret correctly,
          the authors utilized Llama 2 to generate the training text dataset. Is this
          the only reason of the LLM''s name inclusion in the name? Furthermore, does
          this imply that these AnglE models are more effective when paired specifically
          with Llama 2 as the LLM in RAG applications?</p>

          </blockquote>

          <p>B.T.W., the models like <code>SeanLee97/angle-llama-xxx</code> were only
          trained on NLI datasets. Although it can achieve SOTA on STS tasks, we cannot
          guarantee that it generalizes well to other tasks. Therefore, if you want
          to use it for real applications, it is recommended to use WhereIsAI/UAE-Large-V1.</p>

          '
        raw: "> Hello,\n> \n> I was reading the README file on the GitHub repository\
          \ of the models and noticed that on the \"English results\" page (https://github.com/SeanLee97/AnglE#english-sts-results),\
          \ certain Angle models include the name of the Llama LLM, such as \"SeanLee97/angle-llama-7b-nli-v2\"\
          . I'm curious about the significance of this. I attempted to glean insights\
          \ from the associated paper, but I find myself confused. If I interpret\
          \ correctly, the authors utilized Llama 2 to generate the training text\
          \ dataset. Is this the only reason of the LLM's name inclusion in the name?\
          \ Furthermore, does this imply that these AnglE models are more effective\
          \ when paired specifically with Llama 2 as the LLM in RAG applications?\n\
          \nB.T.W., the models like `SeanLee97/angle-llama-xxx` were only trained\
          \ on NLI datasets. Although it can achieve SOTA on STS tasks, we cannot\
          \ guarantee that it generalizes well to other tasks. Therefore, if you want\
          \ to use it for real applications, it is recommended to use WhereIsAI/UAE-Large-V1."
        updatedAt: '2023-12-14T11:10:08.498Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - terilias
    id: 657ae290429a20edb5e375b9
    type: comment
  author: SeanLee97
  content: "> Hello,\n> \n> I was reading the README file on the GitHub repository\
    \ of the models and noticed that on the \"English results\" page (https://github.com/SeanLee97/AnglE#english-sts-results),\
    \ certain Angle models include the name of the Llama LLM, such as \"SeanLee97/angle-llama-7b-nli-v2\"\
    . I'm curious about the significance of this. I attempted to glean insights from\
    \ the associated paper, but I find myself confused. If I interpret correctly,\
    \ the authors utilized Llama 2 to generate the training text dataset. Is this\
    \ the only reason of the LLM's name inclusion in the name? Furthermore, does this\
    \ imply that these AnglE models are more effective when paired specifically with\
    \ Llama 2 as the LLM in RAG applications?\n\nB.T.W., the models like `SeanLee97/angle-llama-xxx`\
    \ were only trained on NLI datasets. Although it can achieve SOTA on STS tasks,\
    \ we cannot guarantee that it generalizes well to other tasks. Therefore, if you\
    \ want to use it for real applications, it is recommended to use WhereIsAI/UAE-Large-V1."
  created_at: 2023-12-14 11:10:08+00:00
  edited: false
  hidden: false
  id: 657ae290429a20edb5e375b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2023-12-14T11:29:04.000Z'
    data:
      edited: true
      editors:
      - terilias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9433634281158447
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
          fullname: Terzis
          isHf: false
          isPro: false
          name: terilias
          type: user
        html: "<p>Thank you for your response and your time, <span data-props=\"{&quot;user&quot;:&quot;SeanLee97&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SeanLee97\"\
          >@<span class=\"underline\">SeanLee97</span></a></span>\n\n\t</span></span>!\
          \ Based on your feedback, it seems that my assumption, suggesting that it's\
          \ preferable to use Llama 2 as the text generator (when I use these AnglE\
          \ models) instead of other LLMs  in RAG applications is incorrect, am I\
          \ right?</p>\n"
        raw: Thank you for your response and your time, @SeanLee97! Based on your
          feedback, it seems that my assumption, suggesting that it's preferable to
          use Llama 2 as the text generator (when I use these AnglE models) instead
          of other LLMs  in RAG applications is incorrect, am I right?
        updatedAt: '2023-12-14T11:31:53.889Z'
      numEdits: 2
      reactions: []
    id: 657ae7001dc93d753c3075ef
    type: comment
  author: terilias
  content: Thank you for your response and your time, @SeanLee97! Based on your feedback,
    it seems that my assumption, suggesting that it's preferable to use Llama 2 as
    the text generator (when I use these AnglE models) instead of other LLMs  in RAG
    applications is incorrect, am I right?
  created_at: 2023-12-14 11:29:04+00:00
  edited: true
  hidden: false
  id: 657ae7001dc93d753c3075ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-14T11:33:17.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9268400073051453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: '<p>I think it is incorrect.<br>Our models like<code>SeanLee97/angle-llama-xxx</code>
          can be used as feature generators for producing sentence embeddings for
          vector search.<br><strong>It cannot be used as LLM for text generation.</strong></p>

          '
        raw: "I think it is incorrect. \nOur models like`SeanLee97/angle-llama-xxx`\
          \ can be used as feature generators for producing sentence embeddings for\
          \ vector search. \n**It cannot be used as LLM for text generation.**"
        updatedAt: '2023-12-14T11:33:17.058Z'
      numEdits: 0
      reactions: []
    id: 657ae7fd429a20edb5e4368c
    type: comment
  author: SeanLee97
  content: "I think it is incorrect. \nOur models like`SeanLee97/angle-llama-xxx`\
    \ can be used as feature generators for producing sentence embeddings for vector\
    \ search. \n**It cannot be used as LLM for text generation.**"
  created_at: 2023-12-14 11:33:17+00:00
  edited: false
  hidden: false
  id: 657ae7fd429a20edb5e4368c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
      fullname: Terzis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terilias
      type: user
    createdAt: '2023-12-14T11:39:19.000Z'
    data:
      edited: false
      editors:
      - terilias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9661818742752075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93d41fa72c968382a28b18f37b96a469.svg
          fullname: Terzis
          isHf: false
          isPro: false
          name: terilias
          type: user
        html: '<p>Ok, I understand. I thought there was a connection between the selected
          embeddings model for vector search and the Language Model (LLM) for text
          generation. However, I now realize that these two components are entirely
          independent, despite the LLM being utilized in the training of the embeddings
          model. Thanks a lot for clarifying!</p>

          '
        raw: Ok, I understand. I thought there was a connection between the selected
          embeddings model for vector search and the Language Model (LLM) for text
          generation. However, I now realize that these two components are entirely
          independent, despite the LLM being utilized in the training of the embeddings
          model. Thanks a lot for clarifying!
        updatedAt: '2023-12-14T11:39:19.005Z'
      numEdits: 0
      reactions: []
    id: 657ae9671953a4194a9a456e
    type: comment
  author: terilias
  content: Ok, I understand. I thought there was a connection between the selected
    embeddings model for vector search and the Language Model (LLM) for text generation.
    However, I now realize that these two components are entirely independent, despite
    the LLM being utilized in the training of the embeddings model. Thanks a lot for
    clarifying!
  created_at: 2023-12-14 11:39:19+00:00
  edited: false
  hidden: false
  id: 657ae9671953a4194a9a456e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-14T11:49:58.000Z'
    data:
      edited: false
      editors:
      - SeanLee97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9552797675132751
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
          fullname: SeanLee
          isHf: false
          isPro: false
          name: SeanLee97
          type: user
        html: '<p>Thanks again for following our work!</p>

          '
        raw: Thanks again for following our work!
        updatedAt: '2023-12-14T11:49:58.481Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657aebe61953a4194a9ab37d
    id: 657aebe61953a4194a9ab379
    type: comment
  author: SeanLee97
  content: Thanks again for following our work!
  created_at: 2023-12-14 11:49:58+00:00
  edited: false
  hidden: false
  id: 657aebe61953a4194a9ab379
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635cc29de7aef2358a9b03ee/SVHL_mTCiOfmBamzSucb0.jpeg?w=200&h=200&f=face
      fullname: SeanLee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SeanLee97
      type: user
    createdAt: '2023-12-14T11:49:58.000Z'
    data:
      status: closed
    id: 657aebe61953a4194a9ab37d
    type: status-change
  author: SeanLee97
  created_at: 2023-12-14 11:49:58+00:00
  id: 657aebe61953a4194a9ab37d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: WhereIsAI/UAE-Large-V1
repo_type: model
status: closed
target_branch: null
title: Why some embeddings models contain "llama" in their names?
