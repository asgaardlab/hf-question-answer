!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Reign2294
conflicting_files: null
created_at: 2023-08-28 00:54:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677555617172-noauth.jpeg?w=200&h=200&f=face
      fullname: Cee Gee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reign2294
      type: user
    createdAt: '2023-08-28T01:54:29.000Z'
    data:
      edited: false
      editors:
      - Reign2294
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7793870568275452
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677555617172-noauth.jpeg?w=200&h=200&f=face
          fullname: Cee Gee
          isHf: false
          isPro: false
          name: Reign2294
          type: user
        html: '<p>I am getting 0.5 tokens/s on a RTX 4090. Can anyone tell me is this
          normal or am I doing something wrong? Cheers! </p>

          <p>cmd flags: none</p>

          <p>Warnings on loading:<br>"[''do_sample](UserWarning: <code>do_sample</code>
          is set to <code>False</code>. However, <code>temperature</code> is set to
          <code>0.9</code> -- this flag is only used in sample-based generation modes.
          You should set <code>do_sample=True</code> or unset <code>temperature</code>.
          This was detected when initializing the generation config instance, which
          means the corresponding file may hold incorrect parameterization and should
          be fixed.)"</p>

          <p>"UserWarning: <code>do_sample</code> is set to <code>False</code>. However,
          <code>top_p</code> is set to <code>0.6</code> -- this flag is only used
          in sample-based generation modes. You should set <code>do_sample=True</code>
          or unset <code>top_p</code>. This was detected when initializing the generation
          config instance, which means the corresponding file may hold incorrect parameterization
          and should be fixed."</p>

          <p>"WARNING:models\Gryphe_MythoMax-L2-13b\tokenizer_config.json is different
          from the original LlamaTokenizer file. It is either customized or outdated."</p>

          <p>"WARNING:models\Gryphe_MythoMax-L2-13b\special_tokens_map.json is different
          from the original LlamaTokenizer file. It is either customized or outdated."</p>

          <p>Settings:<br>Model loader: Transformers<br>gpu-memory in MiB for device
          :0<br>cpu-memory in MiB: 0<br>load-in-4bit params:</p>

          <ul>

          <li>compute_dtype: float16</li>

          <li>quant_type nf4<br>alpha_value: 1<br>rope_freq_base: 0<br>compress_pos_emb:
          1<br>cpu: unselected<br>load-in-8bit: unselected<br>bf16:: unselected<br>auto-devices:
          unselected<br>disk: unselected<br>load-in-4bit: unselected<br>use-double-quant:
          unselected<br>trust-remote-code: unselected</li>

          </ul>

          <p>Parameters:<br>Preset:simple-1<br>max_new_tokens: 200<br>temp: 0.7<br>top_p:
          0.9<br>top_k: 20<br>typical p: 1<br>epsilon_cutoff: 0<br>eta_cutoff: 0<br>tfs:
          1<br>top_a: 0<br>repetition penalty: 1.15<br>repetitiion_penalty_range:
          0<br>encoder_rep_penalty: 1<br>no_repeat_ngram_size: 0<br>min_length: 0<br>seed:
          -1<br>do_sample: selected</p>

          <p>If it helps, I intend to use this model in SillyTavern... unsure if anyone
          has specific settings for there too.<br>Thanks!</p>

          '
        raw: "I am getting 0.5 tokens/s on a RTX 4090. Can anyone tell me is this\
          \ normal or am I doing something wrong? Cheers! \r\n\r\ncmd flags: none\r\
          \n\r\nWarnings on loading: \r\n\"['do_sample](UserWarning: `do_sample` is\
          \ set to `False`. However, `temperature` is set to `0.9` -- this flag is\
          \ only used in sample-based generation modes. You should set `do_sample=True`\
          \ or unset `temperature`. This was detected when initializing the generation\
          \ config instance, which means the corresponding file may hold incorrect\
          \ parameterization and should be fixed.)\"\r\n\r\n\"UserWarning: `do_sample`\
          \ is set to `False`. However, `top_p` is set to `0.6` -- this flag is only\
          \ used in sample-based generation modes. You should set `do_sample=True`\
          \ or unset `top_p`. This was detected when initializing the generation config\
          \ instance, which means the corresponding file may hold incorrect parameterization\
          \ and should be fixed.\"\r\n\r\n\"WARNING:models\\Gryphe_MythoMax-L2-13b\\\
          tokenizer_config.json is different from the original LlamaTokenizer file.\
          \ It is either customized or outdated.\"\r\n\r\n\"WARNING:models\\Gryphe_MythoMax-L2-13b\\\
          special_tokens_map.json is different from the original LlamaTokenizer file.\
          \ It is either customized or outdated.\"\r\n\r\nSettings: \r\nModel loader:\
          \ Transformers\r\ngpu-memory in MiB for device :0\r\ncpu-memory in MiB:\
          \ 0 \r\nload-in-4bit params:\r\n - compute_dtype: float16\r\n - quant_type\
          \ nf4\r\nalpha_value: 1\r\nrope_freq_base: 0 \r\ncompress_pos_emb: 1\r\n\
          cpu: unselected\r\nload-in-8bit: unselected\r\nbf16:: unselected\r\nauto-devices:\
          \ unselected\r\ndisk: unselected\r\nload-in-4bit: unselected\r\nuse-double-quant:\
          \ unselected\r\ntrust-remote-code: unselected\r\n\r\nParameters:\r\nPreset:simple-1\r\
          \nmax_new_tokens: 200\r\ntemp: 0.7\r\ntop_p: 0.9\r\ntop_k: 20\r\ntypical\
          \ p: 1\r\nepsilon_cutoff: 0 \r\neta_cutoff: 0 \r\ntfs: 1\r\ntop_a: 0 \r\n\
          repetition penalty: 1.15\r\nrepetitiion_penalty_range: 0 \r\nencoder_rep_penalty:\
          \ 1\r\nno_repeat_ngram_size: 0 \r\nmin_length: 0 \r\nseed: -1\r\ndo_sample:\
          \ selected\r\n\r\nIf it helps, I intend to use this model in SillyTavern...\
          \ unsure if anyone has specific settings for there too. \r\nThanks!\r\n"
        updatedAt: '2023-08-28T01:54:29.643Z'
      numEdits: 0
      reactions: []
    id: 64ebfe557e69909b1ac6ddb0
    type: comment
  author: Reign2294
  content: "I am getting 0.5 tokens/s on a RTX 4090. Can anyone tell me is this normal\
    \ or am I doing something wrong? Cheers! \r\n\r\ncmd flags: none\r\n\r\nWarnings\
    \ on loading: \r\n\"['do_sample](UserWarning: `do_sample` is set to `False`. However,\
    \ `temperature` is set to `0.9` -- this flag is only used in sample-based generation\
    \ modes. You should set `do_sample=True` or unset `temperature`. This was detected\
    \ when initializing the generation config instance, which means the corresponding\
    \ file may hold incorrect parameterization and should be fixed.)\"\r\n\r\n\"UserWarning:\
    \ `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag\
    \ is only used in sample-based generation modes. You should set `do_sample=True`\
    \ or unset `top_p`. This was detected when initializing the generation config\
    \ instance, which means the corresponding file may hold incorrect parameterization\
    \ and should be fixed.\"\r\n\r\n\"WARNING:models\\Gryphe_MythoMax-L2-13b\\tokenizer_config.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\"\r\n\r\n\"WARNING:models\\Gryphe_MythoMax-L2-13b\\special_tokens_map.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\"\r\n\r\nSettings: \r\nModel loader: Transformers\r\ngpu-memory\
    \ in MiB for device :0\r\ncpu-memory in MiB: 0 \r\nload-in-4bit params:\r\n -\
    \ compute_dtype: float16\r\n - quant_type nf4\r\nalpha_value: 1\r\nrope_freq_base:\
    \ 0 \r\ncompress_pos_emb: 1\r\ncpu: unselected\r\nload-in-8bit: unselected\r\n\
    bf16:: unselected\r\nauto-devices: unselected\r\ndisk: unselected\r\nload-in-4bit:\
    \ unselected\r\nuse-double-quant: unselected\r\ntrust-remote-code: unselected\r\
    \n\r\nParameters:\r\nPreset:simple-1\r\nmax_new_tokens: 200\r\ntemp: 0.7\r\ntop_p:\
    \ 0.9\r\ntop_k: 20\r\ntypical p: 1\r\nepsilon_cutoff: 0 \r\neta_cutoff: 0 \r\n\
    tfs: 1\r\ntop_a: 0 \r\nrepetition penalty: 1.15\r\nrepetitiion_penalty_range:\
    \ 0 \r\nencoder_rep_penalty: 1\r\nno_repeat_ngram_size: 0 \r\nmin_length: 0 \r\
    \nseed: -1\r\ndo_sample: selected\r\n\r\nIf it helps, I intend to use this model\
    \ in SillyTavern... unsure if anyone has specific settings for there too. \r\n\
    Thanks!\r\n"
  created_at: 2023-08-28 00:54:29+00:00
  edited: false
  hidden: false
  id: 64ebfe557e69909b1ac6ddb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
      fullname: Devon M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Delcos
      type: user
    createdAt: '2023-09-03T10:46:27.000Z'
    data:
      edited: false
      editors:
      - Delcos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9221450090408325
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
          fullname: Devon M
          isHf: false
          isPro: false
          name: Delcos
          type: user
        html: '<p>So it looks like th software you''re using doesn''t have sampling
          enabled, so it''s basically ignoring the settings you''re using. It should
          be a check box or something similar. Just make sure Ddo sampling is enabled
          and checked or set somewhere. oobabooga webui is a  good one to use with
          tavern.</p>

          '
        raw: So it looks like th software you're using doesn't have sampling enabled,
          so it's basically ignoring the settings you're using. It should be a check
          box or something similar. Just make sure Ddo sampling is enabled and checked
          or set somewhere. oobabooga webui is a  good one to use with tavern.
        updatedAt: '2023-09-03T10:46:27.245Z'
      numEdits: 0
      reactions: []
    id: 64f46403925e59092ea21d24
    type: comment
  author: Delcos
  content: So it looks like th software you're using doesn't have sampling enabled,
    so it's basically ignoring the settings you're using. It should be a check box
    or something similar. Just make sure Ddo sampling is enabled and checked or set
    somewhere. oobabooga webui is a  good one to use with tavern.
  created_at: 2023-09-03 09:46:27+00:00
  edited: false
  hidden: false
  id: 64f46403925e59092ea21d24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/207c64e5cb7ebf6ab6ba455dde7cdddc.svg
      fullname: Moon Shoes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robotwalrus
      type: user
    createdAt: '2023-09-05T15:46:16.000Z'
    data:
      edited: true
      editors:
      - robotwalrus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9799810647964478
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/207c64e5cb7ebf6ab6ba455dde7cdddc.svg
          fullname: Moon Shoes
          isHf: false
          isPro: false
          name: robotwalrus
          type: user
        html: '<p>I wasn''t able to get it to fit within 24GiB on the 4090 without
          checking either load-in-4-bit, or load-in-8-bit. Even if it did, as soon
          as the context started to build, it would run out of VRAM and start sharing
          with system ram, slowing things waaay down.</p>

          <p>But with load-in-4-bit, it works great. Also set the GPU memory slider
          to max, and set the float type to bfloat16, that''s one of the major benefits
          of the 4090</p>

          '
        raw: 'I wasn''t able to get it to fit within 24GiB on the 4090 without checking
          either load-in-4-bit, or load-in-8-bit. Even if it did, as soon as the context
          started to build, it would run out of VRAM and start sharing with system
          ram, slowing things waaay down.


          But with load-in-4-bit, it works great. Also set the GPU memory slider to
          max, and set the float type to bfloat16, that''s one of the major benefits
          of the 4090'
        updatedAt: '2023-09-05T15:47:57.015Z'
      numEdits: 3
      reactions: []
    id: 64f74d48ce13e4a592cb6721
    type: comment
  author: robotwalrus
  content: 'I wasn''t able to get it to fit within 24GiB on the 4090 without checking
    either load-in-4-bit, or load-in-8-bit. Even if it did, as soon as the context
    started to build, it would run out of VRAM and start sharing with system ram,
    slowing things waaay down.


    But with load-in-4-bit, it works great. Also set the GPU memory slider to max,
    and set the float type to bfloat16, that''s one of the major benefits of the 4090'
  created_at: 2023-09-05 14:46:16+00:00
  edited: true
  hidden: false
  id: 64f74d48ce13e4a592cb6721
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: Gryphe/MythoMax-L2-13b
repo_type: model
status: open
target_branch: null
title: Manual settings for best output?
