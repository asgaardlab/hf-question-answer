!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JuLuComputing
conflicting_files: null
created_at: 2023-07-16 16:35:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-07-16T17:35:28.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8923580646514893
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: '<p>Thanks for sharing this!</p>

          <p>I''m scratch building using this same dataset with Llama.cpp right now
          on only CPU.  It''s an interesting experiment.</p>

          <p>What repo is the ''qlora.py'' from?  I''m interested to see the code.</p>

          '
        raw: "Thanks for sharing this!\r\n\r\nI'm scratch building using this same\
          \ dataset with Llama.cpp right now on only CPU.  It's an interesting experiment.\r\
          \n\r\nWhat repo is the 'qlora.py' from?  I'm interested to see the code."
        updatedAt: '2023-07-16T17:35:28.637Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Not113
    id: 64b42a60689a9a23015d5a82
    type: comment
  author: JuLuComputing
  content: "Thanks for sharing this!\r\n\r\nI'm scratch building using this same dataset\
    \ with Llama.cpp right now on only CPU.  It's an interesting experiment.\r\n\r\
    \nWhat repo is the 'qlora.py' from?  I'm interested to see the code."
  created_at: 2023-07-16 16:35:28+00:00
  edited: false
  hidden: false
  id: 64b42a60689a9a23015d5a82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76c1c11e3e00ceaddd861b551e36246f.svg
      fullname: Given113
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Not113
      type: user
    createdAt: '2023-07-16T20:46:51.000Z'
    data:
      edited: true
      editors:
      - Not113
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9069541692733765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76c1c11e3e00ceaddd861b551e36246f.svg
          fullname: Given113
          isHf: false
          isPro: false
          name: Not113
          type: user
        html: '<blockquote>

          <p>Thanks for sharing this!</p>

          <p>I''m scratch building using this same dataset with Llama.cpp right now
          on only CPU.  It''s an interesting experiment.</p>

          <p>What repo is the ''qlora.py'' from?  I''m interested to see the code.</p>

          </blockquote>

          <p>Probably <a rel="nofollow" href="https://github.com/artidoro/qlora">https://github.com/artidoro/qlora</a>
          or <a rel="nofollow" href="https://github.com/mzbac/qlora-fine-tune/">https://github.com/mzbac/qlora-fine-tune/</a><br>I''d
          be interested to know also. Like you I''m just experimenting around like
          nearly everyone else.</p>

          '
        raw: "> Thanks for sharing this!\n> \n> I'm scratch building using this same\
          \ dataset with Llama.cpp right now on only CPU.  It's an interesting experiment.\n\
          > \n> What repo is the 'qlora.py' from?  I'm interested to see the code.\n\
          \nProbably https://github.com/artidoro/qlora or https://github.com/mzbac/qlora-fine-tune/\n\
          I'd be interested to know also. Like you I'm just experimenting around like\
          \ nearly everyone else."
        updatedAt: '2023-07-16T21:43:06.749Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - JuLuComputing
    id: 64b4573b4dd3e2489522a8c0
    type: comment
  author: Not113
  content: "> Thanks for sharing this!\n> \n> I'm scratch building using this same\
    \ dataset with Llama.cpp right now on only CPU.  It's an interesting experiment.\n\
    > \n> What repo is the 'qlora.py' from?  I'm interested to see the code.\n\nProbably\
    \ https://github.com/artidoro/qlora or https://github.com/mzbac/qlora-fine-tune/\n\
    I'd be interested to know also. Like you I'm just experimenting around like nearly\
    \ everyone else."
  created_at: 2023-07-16 19:46:51+00:00
  edited: true
  hidden: false
  id: 64b4573b4dd3e2489522a8c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-07-16T23:47:58.000Z'
    data:
      edited: true
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8914391994476318
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: '<p>Very cool!  Thanks for the links, I''ll check them out.</p>

          <p>Just for the sake of sharing, here is the code I''m using to build my
          tiny model with Llama.cpp:</p>

          <p>#!/bin/bash<br>./train-text-from-scratch <br>        --vocab-model ./models/ggml-vocab.bin
          <br>        --ctx 2048 --embd 1024 --head 16 --layer 24 <br>        --checkpoint-in
          ./models/chk-code-1024x24.bin <br>        --checkpoint-out ./models/chk-code-10248x24.bin
          <br>        --model-out ./models/ggml-code-1024x24-f32.bin <br>        --train-data
          ./datasets/guanaco-unchained.jsonl <br>        --threads 90 --batch 128
          --examples 1 <br>        --print-details-interval 100 --predict 128 <br>        --use-flash
          --use-scratch --seed 42 <br>        --mem-model 640 --mem-compute 640 --mem-compute0
          320 --mem-compute1 320</p>

          <p>Its being built all CPU on a Dell R820, 96 threads, 768GB RAM.  Please
          critique my settings, if you will, I am always looking for a better ways
          to do things.</p>

          <p>I did 1 epoch of Fredithefish/ShareGPT-Unfiltered-RedPajama-Chat-format-11k.jsonl
          and now guanaco-unchained.jsonl, although they are about the same size,
          quanaco is taking more than twice as long.  I assume the little LM has gained
          some communication skills that are taking a lot more processing to work
          through the dataset, it probably now gibberishes with a sprinkle of json.  lol</p>

          <p>The next couple datasets are going to be several ''how to code Python''
          books and a bunch of GitHub projects I like, all wrapped in csv format.  Then
          I''ll probably go with some datasets from the user ewof.  If the LM seems
          like it''s coming along alright after that, I''ll run everything back through
          for a 2nd epoch.</p>

          <p>I used Lamma.cpp because:</p>

          <ol>

          <li>I found a good example of scratch building with CPU only</li>

          <li>There is the possibility of expanding out into a CPU compute cluster
          with OpenMPI</li>

          <li>I don''t have any fancy V100/H100 cards, nor the money to rent them,
          and the several M40 and K80 GPUs I do have don''t go very far these days
          for training.  But I do have a big stack of these Dell servers and unlimited
          free electricity.</li>

          </ol>

          <p>Down sides to Llama.cpp seem to be, I don''t think that it''s the highest
          performance, bleeding edge, or designed to make a large LM.  Is there a
          different project out there I should look at that has good performance on
          CPU only and can allow me to build from scratch, larger context would be
          a definite plus, too?  Mosaic Composer?  Maybe some combination of Torch
          with ALiBi?  I have experimented with several things but fail because I''m
          either not understanding the options or syntax, or I just can''t find good
          documentation for CPU only.</p>

          <p>I''ll check out the qlora links above, however, please share links to
          any other projects or code you know of.</p>

          '
        raw: "Very cool!  Thanks for the links, I'll check them out.\n\nJust for the\
          \ sake of sharing, here is the code I'm using to build my tiny model with\
          \ Llama.cpp:\n\n#!/bin/bash\n./train-text-from-scratch \\\n        --vocab-model\
          \ ./models/ggml-vocab.bin \\\n        --ctx 2048 --embd 1024 --head 16 --layer\
          \ 24 \\\n        --checkpoint-in ./models/chk-code-1024x24.bin \\\n    \
          \    --checkpoint-out ./models/chk-code-10248x24.bin \\\n        --model-out\
          \ ./models/ggml-code-1024x24-f32.bin \\\n        --train-data ./datasets/guanaco-unchained.jsonl\
          \ \\\n        --threads 90 --batch 128 --examples 1 \\\n        --print-details-interval\
          \ 100 --predict 128 \\\n        --use-flash --use-scratch --seed 42 \\\n\
          \        --mem-model 640 --mem-compute 640 --mem-compute0 320 --mem-compute1\
          \ 320\n\nIts being built all CPU on a Dell R820, 96 threads, 768GB RAM.\
          \  Please critique my settings, if you will, I am always looking for a better\
          \ ways to do things.\n\nI did 1 epoch of Fredithefish/ShareGPT-Unfiltered-RedPajama-Chat-format-11k.jsonl\
          \ and now guanaco-unchained.jsonl, although they are about the same size,\
          \ quanaco is taking more than twice as long.  I assume the little LM has\
          \ gained some communication skills that are taking a lot more processing\
          \ to work through the dataset, it probably now gibberishes with a sprinkle\
          \ of json.  lol\n\nThe next couple datasets are going to be several 'how\
          \ to code Python' books and a bunch of GitHub projects I like, all wrapped\
          \ in csv format.  Then I'll probably go with some datasets from the user\
          \ ewof.  If the LM seems like it's coming along alright after that, I'll\
          \ run everything back through for a 2nd epoch.\n\nI used Lamma.cpp because:\n\
          \  1) I found a good example of scratch building with CPU only\n  2) There\
          \ is the possibility of expanding out into a CPU compute cluster with OpenMPI\n\
          \  3) I don't have any fancy V100/H100 cards, nor the money to rent them,\
          \ and the several M40 and K80 GPUs I do have don't go very far these days\
          \ for training.  But I do have a big stack of these Dell servers and unlimited\
          \ free electricity.\n\nDown sides to Llama.cpp seem to be, I don't think\
          \ that it's the highest performance, bleeding edge, or designed to make\
          \ a large LM.  Is there a different project out there I should look at that\
          \ has good performance on CPU only and can allow me to build from scratch,\
          \ larger context would be a definite plus, too?  Mosaic Composer?  Maybe\
          \ some combination of Torch with ALiBi?  I have experimented with several\
          \ things but fail because I'm either not understanding the options or syntax,\
          \ or I just can't find good documentation for CPU only.\n\nI'll check out\
          \ the qlora links above, however, please share links to any other projects\
          \ or code you know of."
        updatedAt: '2023-07-17T00:13:13.769Z'
      numEdits: 1
      reactions: []
    id: 64b481ae0eb87fa990990c7a
    type: comment
  author: JuLuComputing
  content: "Very cool!  Thanks for the links, I'll check them out.\n\nJust for the\
    \ sake of sharing, here is the code I'm using to build my tiny model with Llama.cpp:\n\
    \n#!/bin/bash\n./train-text-from-scratch \\\n        --vocab-model ./models/ggml-vocab.bin\
    \ \\\n        --ctx 2048 --embd 1024 --head 16 --layer 24 \\\n        --checkpoint-in\
    \ ./models/chk-code-1024x24.bin \\\n        --checkpoint-out ./models/chk-code-10248x24.bin\
    \ \\\n        --model-out ./models/ggml-code-1024x24-f32.bin \\\n        --train-data\
    \ ./datasets/guanaco-unchained.jsonl \\\n        --threads 90 --batch 128 --examples\
    \ 1 \\\n        --print-details-interval 100 --predict 128 \\\n        --use-flash\
    \ --use-scratch --seed 42 \\\n        --mem-model 640 --mem-compute 640 --mem-compute0\
    \ 320 --mem-compute1 320\n\nIts being built all CPU on a Dell R820, 96 threads,\
    \ 768GB RAM.  Please critique my settings, if you will, I am always looking for\
    \ a better ways to do things.\n\nI did 1 epoch of Fredithefish/ShareGPT-Unfiltered-RedPajama-Chat-format-11k.jsonl\
    \ and now guanaco-unchained.jsonl, although they are about the same size, quanaco\
    \ is taking more than twice as long.  I assume the little LM has gained some communication\
    \ skills that are taking a lot more processing to work through the dataset, it\
    \ probably now gibberishes with a sprinkle of json.  lol\n\nThe next couple datasets\
    \ are going to be several 'how to code Python' books and a bunch of GitHub projects\
    \ I like, all wrapped in csv format.  Then I'll probably go with some datasets\
    \ from the user ewof.  If the LM seems like it's coming along alright after that,\
    \ I'll run everything back through for a 2nd epoch.\n\nI used Lamma.cpp because:\n\
    \  1) I found a good example of scratch building with CPU only\n  2) There is\
    \ the possibility of expanding out into a CPU compute cluster with OpenMPI\n \
    \ 3) I don't have any fancy V100/H100 cards, nor the money to rent them, and the\
    \ several M40 and K80 GPUs I do have don't go very far these days for training.\
    \  But I do have a big stack of these Dell servers and unlimited free electricity.\n\
    \nDown sides to Llama.cpp seem to be, I don't think that it's the highest performance,\
    \ bleeding edge, or designed to make a large LM.  Is there a different project\
    \ out there I should look at that has good performance on CPU only and can allow\
    \ me to build from scratch, larger context would be a definite plus, too?  Mosaic\
    \ Composer?  Maybe some combination of Torch with ALiBi?  I have experimented\
    \ with several things but fail because I'm either not understanding the options\
    \ or syntax, or I just can't find good documentation for CPU only.\n\nI'll check\
    \ out the qlora links above, however, please share links to any other projects\
    \ or code you know of."
  created_at: 2023-07-16 22:47:58+00:00
  edited: true
  hidden: false
  id: 64b481ae0eb87fa990990c7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642babef48f67b6f21d5c917/z3nSlb-r82nECGNpkgBsv.png?w=200&h=200&f=face
      fullname: Bonanza Unthread
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Neko-Institute-of-Science
      type: user
    createdAt: '2023-07-19T04:26:29.000Z'
    data:
      edited: false
      editors:
      - Neko-Institute-of-Science
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8798398375511169
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642babef48f67b6f21d5c917/z3nSlb-r82nECGNpkgBsv.png?w=200&h=200&f=face
          fullname: Bonanza Unthread
          isHf: false
          isPro: false
          name: Neko-Institute-of-Science
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Thanks for sharing this!</p>

          <p>I''m scratch building using this same dataset with Llama.cpp right now
          on only CPU.  It''s an interesting experiment.</p>

          <p>What repo is the ''qlora.py'' from?  I''m interested to see the code.</p>

          </blockquote>

          <p>Probably <a rel="nofollow" href="https://github.com/artidoro/qlora">https://github.com/artidoro/qlora</a>
          or <a rel="nofollow" href="https://github.com/mzbac/qlora-fine-tune/">https://github.com/mzbac/qlora-fine-tune/</a><br>I''d
          be interested to know also. Like you I''m just experimenting around like
          nearly everyone else.</p>

          </blockquote>

          <p>I used artidoro but I modified it to use epochs vs steps.</p>

          '
        raw: "> > Thanks for sharing this!\n> > \n> > I'm scratch building using this\
          \ same dataset with Llama.cpp right now on only CPU.  It's an interesting\
          \ experiment.\n> > \n> > What repo is the 'qlora.py' from?  I'm interested\
          \ to see the code.\n> \n> Probably https://github.com/artidoro/qlora or\
          \ https://github.com/mzbac/qlora-fine-tune/\n> I'd be interested to know\
          \ also. Like you I'm just experimenting around like nearly everyone else.\n\
          \nI used artidoro but I modified it to use epochs vs steps."
        updatedAt: '2023-07-19T04:26:29.168Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Not113
    id: 64b765f5a8c39dc0788261ac
    type: comment
  author: Neko-Institute-of-Science
  content: "> > Thanks for sharing this!\n> > \n> > I'm scratch building using this\
    \ same dataset with Llama.cpp right now on only CPU.  It's an interesting experiment.\n\
    > > \n> > What repo is the 'qlora.py' from?  I'm interested to see the code.\n\
    > \n> Probably https://github.com/artidoro/qlora or https://github.com/mzbac/qlora-fine-tune/\n\
    > I'd be interested to know also. Like you I'm just experimenting around like\
    \ nearly everyone else.\n\nI used artidoro but I modified it to use epochs vs\
    \ steps."
  created_at: 2023-07-19 03:26:29+00:00
  edited: false
  hidden: false
  id: 64b765f5a8c39dc0788261ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Neko-Institute-of-Science/guanaco-unchained-33b-qlora
repo_type: model
status: open
target_branch: null
title: Nice project - have a question
