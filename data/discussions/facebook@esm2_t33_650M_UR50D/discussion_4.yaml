!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xigua666
conflicting_files: null
created_at: 2023-08-26 03:34:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Cq0weZaqAK5diun6Hg7Ea.jpeg?w=200&h=200&f=face
      fullname: zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xigua666
      type: user
    createdAt: '2023-08-26T04:34:14.000Z'
    data:
      edited: false
      editors:
      - xigua666
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46230176091194153
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Cq0weZaqAK5diun6Hg7Ea.jpeg?w=200&h=200&f=face
          fullname: zhu
          isHf: false
          isPro: false
          name: xigua666
          type: user
        html: "<p>Here is my usage process. I'm not sure if it's correct. Can someone\
          \ help me</p>\n<p> def forward(self,encoded_inputs):<br>        # print(seqs)<br>\
          \        # encoded_inputs = self.tokenizer(seqs, max_length=65, padding=True,\
          \ truncation=True, return_tensors='pt')<br>        embedded_data = self.model(**encoded_inputs).last_hidden_state.mean(0)\
          \ #  this  code<br>        print(\"embedded_data   shape == &gt;\",embedded_data.shape)<br>\
          \        print(embedded_data)</p>\n<pre><code>    # .mean(0) [:,0, :]\n\
          \   \n    output = torch.squeeze(self.main(embedded_data))\n    # print(output.shape)\n\
          \    return output\n</code></pre>\n"
        raw: "Here is my usage process. I'm not sure if it's correct. Can someone\
          \ help me\r\n\r\n def forward(self,encoded_inputs):\r\n        # print(seqs)\r\
          \n        # encoded_inputs = self.tokenizer(seqs, max_length=65, padding=True,\
          \ truncation=True, return_tensors='pt')\r\n        embedded_data = self.model(**encoded_inputs).last_hidden_state.mean(0)\
          \ #  this  code \r\n        print(\"embedded_data   shape == >\",embedded_data.shape)\r\
          \n        print(embedded_data)\r\n       \r\n        # .mean(0) [:,0, :]\r\
          \n       \r\n        output = torch.squeeze(self.main(embedded_data))\r\n\
          \        # print(output.shape)\r\n        return output\r\n"
        updatedAt: '2023-08-26T04:34:14.996Z'
      numEdits: 0
      reactions: []
    id: 64e980c6fb77a3eaa7d86e3c
    type: comment
  author: xigua666
  content: "Here is my usage process. I'm not sure if it's correct. Can someone help\
    \ me\r\n\r\n def forward(self,encoded_inputs):\r\n        # print(seqs)\r\n  \
    \      # encoded_inputs = self.tokenizer(seqs, max_length=65, padding=True, truncation=True,\
    \ return_tensors='pt')\r\n        embedded_data = self.model(**encoded_inputs).last_hidden_state.mean(0)\
    \ #  this  code \r\n        print(\"embedded_data   shape == >\",embedded_data.shape)\r\
    \n        print(embedded_data)\r\n       \r\n        # .mean(0) [:,0, :]\r\n \
    \      \r\n        output = torch.squeeze(self.main(embedded_data))\r\n      \
    \  # print(output.shape)\r\n        return output\r\n"
  created_at: 2023-08-26 03:34:14+00:00
  edited: false
  hidden: false
  id: 64e980c6fb77a3eaa7d86e3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: facebook/esm2_t33_650M_UR50D
repo_type: model
status: open
target_branch: null
title: "How to get esm2_t33_650M_UR50D Fixed embedding using in the downstram task\uFF1F"
