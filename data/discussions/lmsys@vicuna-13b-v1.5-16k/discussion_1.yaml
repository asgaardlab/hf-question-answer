!!python/object:huggingface_hub.community.DiscussionWithDetails
author: monuminu
conflicting_files: null
created_at: 2023-08-02 16:57:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-08-02T17:57:39.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7605924010276794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: '<p>What is the prompt template ?</p>

          <p>prompt = "USER: write a poem about sky in 300 words ASSISTANT:"</p>

          <p>Response :</p>

          <p> I''m sorry, but i can''t do that. A poem about the sky could take take
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
          a a a a a a a a a a a a a a a a a a a</p>

          '
        raw: "What is the prompt template ?\r\n\r\nprompt = \"USER: write a poem about\
          \ sky in 300 words ASSISTANT:\"\r\n\r\nResponse :\r\n\r\n I'm sorry, but\
          \ i can't do that. A poem about the sky could take take a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
          \ a a a a a a a a a a a a a a a a a"
        updatedAt: '2023-08-02T17:57:39.912Z'
      numEdits: 0
      reactions: []
    id: 64ca9913bf67d9b76e5fe2b4
    type: comment
  author: monuminu
  content: "What is the prompt template ?\r\n\r\nprompt = \"USER: write a poem about\
    \ sky in 300 words ASSISTANT:\"\r\n\r\nResponse :\r\n\r\n I'm sorry, but i can't\
    \ do that. A poem about the sky could take take a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\
    \ a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a"
  created_at: 2023-08-02 16:57:39+00:00
  edited: false
  hidden: false
  id: 64ca9913bf67d9b76e5fe2b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-08-02T18:38:50.000Z'
    data:
      edited: false
      editors:
      - lmzheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5321658253669739
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template">https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template</a><br>It
          requires transformers &gt;= 4.31.0 for the rope scaling.</p>

          '
        raw: 'https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template

          It requires transformers >= 4.31.0 for the rope scaling.'
        updatedAt: '2023-08-02T18:38:50.560Z'
      numEdits: 0
      reactions: []
    id: 64caa2ba942890af9337c172
    type: comment
  author: lmzheng
  content: 'https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template

    It requires transformers >= 4.31.0 for the rope scaling.'
  created_at: 2023-08-02 17:38:50+00:00
  edited: false
  hidden: false
  id: 64caa2ba942890af9337c172
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-08-02T19:01:23.000Z'
    data:
      edited: false
      editors:
      - lmzheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37404799461364746
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/iadKV5qq9Q69XodPiq0J4.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/iadKV5qq9Q69XodPiq0J4.png"></a></p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/iadKV5qq9Q69XodPiq0J4.png)

          '
        updatedAt: '2023-08-02T19:01:23.370Z'
      numEdits: 0
      reactions: []
    id: 64caa80391726743360a84a6
    type: comment
  author: lmzheng
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/iadKV5qq9Q69XodPiq0J4.png)

    '
  created_at: 2023-08-02 18:01:23+00:00
  edited: false
  hidden: false
  id: 64caa80391726743360a84a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-08-03T15:04:06.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9237468242645264
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>I cannot make this prompt work in TGI. It writes a little and starts\
          \ repeating everything!</p>\n<pre><code>input=\"\"\"USER: Give me a 3 day\
          \ plan to trip to Paris?\"\"\"\n\nDay 1:\n* Wake up early in the morning\
          \ and head to the Eiffel Tower for sunrise.\n* After the tower, take a stroll\
          \ around the the beautiful Champs-\xC9lys\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\n</code></pre>\n<pre><code>input=\"\"\"\
          USER: Hi\"\"\"\n\n, I'm trying to use the `get_object_or_40()` function\
          \ in my code, but I'm getting an error message that says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says\n</code></pre>\n<p>I think we are missing some special tokens\
          \ somewhere. (I tried the Llama-2 chat prompt template, it didn't work)</p>\n"
        raw: "I cannot make this prompt work in TGI. It writes a little and starts\
          \ repeating everything!\n\n\n```\ninput=\"\"\"USER: Give me a 3 day plan\
          \ to trip to Paris?\"\"\"\n\nDay 1:\n* Wake up early in the morning and\
          \ head to the Eiffel Tower for sunrise.\n* After the tower, take a stroll\
          \ around the the beautiful Champs-\xC9lys\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
          es\xE9es\xE9es\xE9es\xE9es\xE9es\n```\n\n```\ninput=\"\"\"USER: Hi\"\"\"\
          \n\n, I'm trying to use the `get_object_or_40()` function in my code, but\
          \ I'm getting an error message that says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says says says\
          \ says says says says says says says says says says says says\n```\n\nI\
          \ think we are missing some special tokens somewhere. (I tried the Llama-2\
          \ chat prompt template, it didn't work)"
        updatedAt: '2023-08-03T15:04:06.042Z'
      numEdits: 0
      reactions: []
    id: 64cbc1e600377e28488147b2
    type: comment
  author: MaziyarPanahi
  content: "I cannot make this prompt work in TGI. It writes a little and starts repeating\
    \ everything!\n\n\n```\ninput=\"\"\"USER: Give me a 3 day plan to trip to Paris?\"\
    \"\"\n\nDay 1:\n* Wake up early in the morning and head to the Eiffel Tower for\
    \ sunrise.\n* After the tower, take a stroll around the the beautiful Champs-\xC9\
    lys\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
    es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
    es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
    es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
    es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9\
    es\xE9es\xE9es\xE9es\xE9es\xE9es\xE9es\n```\n\n```\ninput=\"\"\"USER: Hi\"\"\"\
    \n\n, I'm trying to use the `get_object_or_40()` function in my code, but I'm\
    \ getting an error message that says says says says says says says says says says\
    \ says says says says says says says says says says says says says says says says\
    \ says says says says says says says says says says says says says says says says\
    \ says says says says says says says says says says says says says says says says\
    \ says says says says says says says says says says says says says says says says\
    \ says says says says says says says says says says says says says says says says\n\
    ```\n\nI think we are missing some special tokens somewhere. (I tried the Llama-2\
    \ chat prompt template, it didn't work)"
  created_at: 2023-08-03 14:04:06+00:00
  edited: false
  hidden: false
  id: 64cbc1e600377e28488147b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-08-03T16:11:32.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2852008640766144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: '<p>Same here ..</p>

          '
        raw: Same here ..
        updatedAt: '2023-08-03T16:11:32.865Z'
      numEdits: 0
      reactions: []
    id: 64cbd1b487ec96aa47371305
    type: comment
  author: monuminu
  content: Same here ..
  created_at: 2023-08-03 15:11:32+00:00
  edited: false
  hidden: false
  id: 64cbd1b487ec96aa47371305
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-08-03T16:12:14.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9998054504394531
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: '<p>I was thinking it''s just me. Thanks for reporting </p>

          '
        raw: 'I was thinking it''s just me. Thanks for reporting '
        updatedAt: '2023-08-03T16:12:14.973Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - MaziyarPanahi
        - yahma
    id: 64cbd1de2e592905f9fdca38
    type: comment
  author: monuminu
  content: 'I was thinking it''s just me. Thanks for reporting '
  created_at: 2023-08-03 15:12:14+00:00
  edited: false
  hidden: false
  id: 64cbd1de2e592905f9fdca38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afef8240fca3bf499848f0b3681c9626.svg
      fullname: Marcel Goya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelgoya
      type: user
    createdAt: '2023-08-03T16:18:07.000Z'
    data:
      edited: true
      editors:
      - marcelgoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3970775306224823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afef8240fca3bf499848f0b3681c9626.svg
          fullname: Marcel Goya
          isHf: false
          isPro: false
          name: marcelgoya
          type: user
        html: '<p>I am also experiencing the same problem with the standard HF transformer
          example:</p>

          <pre><code>from transformers import LlamaTokenizer, LlamaForCausalLM

          tokenizer = LlamaTokenizer.from_pretrained("lmsys/vicuna-13b-v1.5-16k")

          model = LlamaForCausalLM.from_pretrained("lmsys/vicuna-13b-v1.5-16k", device_map="auto")


          inputs = tokenizer("How are you?", return_tensors="pt")

          generate_ids = model.generate(inputs.input_ids.to(''cuda:0''), max_length=16000)

          tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


          print(output)

          </code></pre>

          '
        raw: 'I am also experiencing the same problem with the standard HF transformer
          example:


          ```

          from transformers import LlamaTokenizer, LlamaForCausalLM

          tokenizer = LlamaTokenizer.from_pretrained("lmsys/vicuna-13b-v1.5-16k")

          model = LlamaForCausalLM.from_pretrained("lmsys/vicuna-13b-v1.5-16k", device_map="auto")


          inputs = tokenizer("How are you?", return_tensors="pt")

          generate_ids = model.generate(inputs.input_ids.to(''cuda:0''), max_length=16000)

          tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


          print(output)

          ```'
        updatedAt: '2023-08-03T17:51:05.945Z'
      numEdits: 3
      reactions: []
    id: 64cbd33f8256a8efea591536
    type: comment
  author: marcelgoya
  content: 'I am also experiencing the same problem with the standard HF transformer
    example:


    ```

    from transformers import LlamaTokenizer, LlamaForCausalLM

    tokenizer = LlamaTokenizer.from_pretrained("lmsys/vicuna-13b-v1.5-16k")

    model = LlamaForCausalLM.from_pretrained("lmsys/vicuna-13b-v1.5-16k", device_map="auto")


    inputs = tokenizer("How are you?", return_tensors="pt")

    generate_ids = model.generate(inputs.input_ids.to(''cuda:0''), max_length=16000)

    tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


    print(output)

    ```'
  created_at: 2023-08-03 15:18:07+00:00
  edited: true
  hidden: false
  id: 64cbd33f8256a8efea591536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
      fullname: Shangming Cai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cheshire94
      type: user
    createdAt: '2023-08-04T07:16:37.000Z'
    data:
      edited: false
      editors:
      - Cheshire94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9622732400894165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
          fullname: Shangming Cai
          isHf: false
          isPro: false
          name: Cheshire94
          type: user
        html: '<p>Same here.  V1.5 works fine while V1.5-16K continues to repeat nonsense
          letters after few words.</p>

          '
        raw: Same here.  V1.5 works fine while V1.5-16K continues to repeat nonsense
          letters after few words.
        updatedAt: '2023-08-04T07:16:37.866Z'
      numEdits: 0
      reactions: []
    id: 64cca5d5e984d09bedc7e5a5
    type: comment
  author: Cheshire94
  content: Same here.  V1.5 works fine while V1.5-16K continues to repeat nonsense
    letters after few words.
  created_at: 2023-08-04 06:16:37+00:00
  edited: false
  hidden: false
  id: 64cca5d5e984d09bedc7e5a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-08-04T07:20:40.000Z'
    data:
      edited: false
      editors:
      - lmzheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7564747333526611
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: '<p>Did you use transformers &gt;= 4.31.0?</p>

          '
        raw: Did you use transformers >= 4.31.0?
        updatedAt: '2023-08-04T07:20:40.291Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jlzhou
        - manishl127
    id: 64cca6c8a34a1fab193ff0c9
    type: comment
  author: lmzheng
  content: Did you use transformers >= 4.31.0?
  created_at: 2023-08-04 06:20:40+00:00
  edited: false
  hidden: false
  id: 64cca6c8a34a1fab193ff0c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-08-04T07:30:37.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7482869029045105
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>Thansk <span data-props=\"{&quot;user&quot;:&quot;lmzheng&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lmzheng\"\
          >@<span class=\"underline\">lmzheng</span></a></span>\n\n\t</span></span><br>It\
          \ seems the latest TGI uses older <code>transformers</code>(<a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/text-generation-inference/blob/main/server/requirements.txt#L53\"\
          >https://github.com/huggingface/text-generation-inference/blob/main/server/requirements.txt#L53</a>).\
          \ Let me try a pure CasualLM and will get back to you/</p>\n"
        raw: "Thansk @lmzheng \nIt seems the latest TGI uses older `transformers`(https://github.com/huggingface/text-generation-inference/blob/main/server/requirements.txt#L53).\
          \ Let me try a pure CasualLM and will get back to you/"
        updatedAt: '2023-08-04T07:30:37.235Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jlzhou
    id: 64cca91da34a1fab194051d3
    type: comment
  author: MaziyarPanahi
  content: "Thansk @lmzheng \nIt seems the latest TGI uses older `transformers`(https://github.com/huggingface/text-generation-inference/blob/main/server/requirements.txt#L53).\
    \ Let me try a pure CasualLM and will get back to you/"
  created_at: 2023-08-04 06:30:37+00:00
  edited: false
  hidden: false
  id: 64cca91da34a1fab194051d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
      fullname: Shangming Cai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cheshire94
      type: user
    createdAt: '2023-08-04T08:01:03.000Z'
    data:
      edited: false
      editors:
      - Cheshire94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9071674346923828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6250563d6b9676db94a224e05e42ed3b.svg
          fullname: Shangming Cai
          isHf: false
          isPro: false
          name: Cheshire94
          type: user
        html: '<blockquote>

          <p>Did you use transformers &gt;= 4.31.0?</p>

          </blockquote>

          <p>Thx, it seems that the version of the transformers library is the problem,
          I upgrade it from 4.30.2 to 4.31.0, and the mumbling does not happen again.</p>

          <p>However, I start to run into OOM situations "torch.cuda.OutOfMemoryError:
          CUDA out of memory." with my GPU(Tesla V100 31.75 GiB total capacity) sometimes,
          does it related to the memory requirement of some intermediate parameters
          for 16K context? </p>

          '
        raw: '> Did you use transformers >= 4.31.0?


          Thx, it seems that the version of the transformers library is the problem,
          I upgrade it from 4.30.2 to 4.31.0, and the mumbling does not happen again.


          However, I start to run into OOM situations "torch.cuda.OutOfMemoryError:
          CUDA out of memory." with my GPU(Tesla V100 31.75 GiB total capacity) sometimes,
          does it related to the memory requirement of some intermediate parameters
          for 16K context? '
        updatedAt: '2023-08-04T08:01:03.805Z'
      numEdits: 0
      reactions: []
    id: 64ccb03f87ec96aa4753b5a2
    type: comment
  author: Cheshire94
  content: '> Did you use transformers >= 4.31.0?


    Thx, it seems that the version of the transformers library is the problem, I upgrade
    it from 4.30.2 to 4.31.0, and the mumbling does not happen again.


    However, I start to run into OOM situations "torch.cuda.OutOfMemoryError: CUDA
    out of memory." with my GPU(Tesla V100 31.75 GiB total capacity) sometimes, does
    it related to the memory requirement of some intermediate parameters for 16K context? '
  created_at: 2023-08-04 07:01:03+00:00
  edited: false
  hidden: false
  id: 64ccb03f87ec96aa4753b5a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-08-04T08:24:52.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8671243786811829
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lmzheng&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lmzheng\">@<span class=\"\
          underline\">lmzheng</span></a></span>\n\n\t</span></span> It works great\
          \ with the normal CasualLM coming from <code>4.31.0</code>. I'll wait for\
          \ TGI to start using 4.31.0.</p>\n<p>The memory usage is also pretty decent\
          \ on small text, however, once I use lots of data I also get the same error:</p>\n\
          <pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 8.85\
          \ GiB (GPU 0; 79.19 GiB total capacity; 24.32 GiB \nalready allocated; 27.62\
          \ MiB free; 28.69 GiB reserved in total by PyTorch) If reserved memory is\
          \ &gt;&gt; allocated \nmemory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n\
          </code></pre>\n<p>Not sure if there is any way around this, but would it\
          \ be possible to calculate how much memory one needs if one uses the whole\
          \ 16000 input's length? (60G, 80G?)</p>\n"
        raw: "@lmzheng It works great with the normal CasualLM coming from `4.31.0`.\
          \ I'll wait for TGI to start using 4.31.0.\n\nThe memory usage is also pretty\
          \ decent on small text, however, once I use lots of data I also get the\
          \ same error:\n\n```\nOutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 8.85 GiB (GPU 0; 79.19 GiB total capacity; 24.32 GiB \nalready allocated;\
          \ 27.62 MiB free; 28.69 GiB reserved in total by PyTorch) If reserved memory\
          \ is >> allocated \nmemory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n\
          ```\n\nNot sure if there is any way around this, but would it be possible\
          \ to calculate how much memory one needs if one uses the whole 16000 input's\
          \ length? (60G, 80G?)"
        updatedAt: '2023-08-04T08:24:52.092Z'
      numEdits: 0
      reactions: []
    id: 64ccb5d471a7bbb60c4e810d
    type: comment
  author: MaziyarPanahi
  content: "@lmzheng It works great with the normal CasualLM coming from `4.31.0`.\
    \ I'll wait for TGI to start using 4.31.0.\n\nThe memory usage is also pretty\
    \ decent on small text, however, once I use lots of data I also get the same error:\n\
    \n```\nOutOfMemoryError: CUDA out of memory. Tried to allocate 8.85 GiB (GPU 0;\
    \ 79.19 GiB total capacity; 24.32 GiB \nalready allocated; 27.62 MiB free; 28.69\
    \ GiB reserved in total by PyTorch) If reserved memory is >> allocated \nmemory\
    \ try setting max_split_size_mb to avoid fragmentation.  See documentation for\
    \ Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n```\n\nNot sure if there is\
    \ any way around this, but would it be possible to calculate how much memory one\
    \ needs if one uses the whole 16000 input's length? (60G, 80G?)"
  created_at: 2023-08-04 07:24:52+00:00
  edited: false
  hidden: false
  id: 64ccb5d471a7bbb60c4e810d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-08-05T09:58:58.000Z'
    data:
      edited: true
      editors:
      - lmzheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38665568828582764
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;marcelgoya&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/marcelgoya\">@<span class=\"\
          underline\">marcelgoya</span></a></span>\n\n\t</span></span> follow this\
          \ to use transformer api <a rel=\"nofollow\" href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\"\
          >https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py</a></p>\n"
        raw: '@marcelgoya follow this to use transformer api https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py'
        updatedAt: '2023-08-05T09:59:07.411Z'
      numEdits: 1
      reactions: []
    id: 64ce1d623c4a1b39a0ba178d
    type: comment
  author: lmzheng
  content: '@marcelgoya follow this to use transformer api https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py'
  created_at: 2023-08-05 08:58:58+00:00
  edited: true
  hidden: false
  id: 64ce1d623c4a1b39a0ba178d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afef8240fca3bf499848f0b3681c9626.svg
      fullname: Marcel Goya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcelgoya
      type: user
    createdAt: '2023-08-05T10:18:18.000Z'
    data:
      edited: true
      editors:
      - marcelgoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6387459635734558
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afef8240fca3bf499848f0b3681c9626.svg
          fullname: Marcel Goya
          isHf: false
          isPro: false
          name: marcelgoya
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;marcelgoya&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marcelgoya\"\
          >@<span class=\"underline\">marcelgoya</span></a></span>\n\n\t</span></span>\
          \ follow this to use transformer api <a rel=\"nofollow\" href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\"\
          >https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py</a></p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;lmzheng&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lmzheng\"\
          >@<span class=\"underline\">lmzheng</span></a></span>\n\n\t</span></span>\
          \ Will do that, many thanks!</p>\n"
        raw: '> @marcelgoya follow this to use transformer api https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py


          @lmzheng Will do that, many thanks!'
        updatedAt: '2023-08-05T10:18:42.658Z'
      numEdits: 1
      reactions: []
    id: 64ce21eabf39f9c8be85647a
    type: comment
  author: marcelgoya
  content: '> @marcelgoya follow this to use transformer api https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py


    @lmzheng Will do that, many thanks!'
  created_at: 2023-08-05 09:18:18+00:00
  edited: true
  hidden: false
  id: 64ce21eabf39f9c8be85647a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d611085740789f006188e60bbf402b8.svg
      fullname: ppl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: plancktree
      type: user
    createdAt: '2023-08-10T11:45:44.000Z'
    data:
      edited: false
      editors:
      - plancktree
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8947632312774658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d611085740789f006188e60bbf402b8.svg
          fullname: ppl
          isHf: false
          isPro: false
          name: plancktree
          type: user
        html: "<blockquote>\n<p>Did you use transformers &gt;= 4.31.0?</p>\n</blockquote>\n\
          <p>my transformers is 4.31.0 but i also have the same problem.how can i\
          \ fix it\uFF1FQAQ</p>\n"
        raw: "\n> Did you use transformers >= 4.31.0?\n\n\nmy transformers is 4.31.0\
          \ but i also have the same problem.how can i fix it\uFF1FQAQ\n\n"
        updatedAt: '2023-08-10T11:45:44.999Z'
      numEdits: 0
      reactions: []
    id: 64d4cde88b65d477e6925105
    type: comment
  author: plancktree
  content: "\n> Did you use transformers >= 4.31.0?\n\n\nmy transformers is 4.31.0\
    \ but i also have the same problem.how can i fix it\uFF1FQAQ\n\n"
  created_at: 2023-08-10 10:45:44+00:00
  edited: false
  hidden: false
  id: 64d4cde88b65d477e6925105
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-08-10T13:23:59.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633697271347046
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: '<p>I realized I don''t see the issue with TGI 1.0 but with other container
          like DJL. I think the issue may be related to Rope scaling not implemented
          .</p>

          '
        raw: I realized I don't see the issue with TGI 1.0 but with other container
          like DJL. I think the issue may be related to Rope scaling not implemented
          .
        updatedAt: '2023-08-10T13:23:59.497Z'
      numEdits: 0
      reactions: []
    id: 64d4e4efdfafaed03dc8d4fd
    type: comment
  author: monuminu
  content: I realized I don't see the issue with TGI 1.0 but with other container
    like DJL. I think the issue may be related to Rope scaling not implemented .
  created_at: 2023-08-10 12:23:59+00:00
  edited: false
  hidden: false
  id: 64d4e4efdfafaed03dc8d4fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645600c94095c967f9a84ffc/uZeUPFFfJ69VugNUfRHY6.jpeg?w=200&h=200&f=face
      fullname: Robert Boehme
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: rboehme86
      type: user
    createdAt: '2023-08-11T21:18:46.000Z'
    data:
      edited: false
      editors:
      - rboehme86
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593076705932617
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645600c94095c967f9a84ffc/uZeUPFFfJ69VugNUfRHY6.jpeg?w=200&h=200&f=face
          fullname: Robert Boehme
          isHf: false
          isPro: true
          name: rboehme86
          type: user
        html: '<p>I can confirm it works flawlessly with a fresh install. Just created
          a new linux user on my GPU server, installed all and it was running like
          a charm. Quality is shockingly good. I used the OpenAI API interface to
          redirect some of my existing script to this endpoint and they just worked
          even with very complex prompts and contexts :-) Well done guys!</p>

          '
        raw: I can confirm it works flawlessly with a fresh install. Just created
          a new linux user on my GPU server, installed all and it was running like
          a charm. Quality is shockingly good. I used the OpenAI API interface to
          redirect some of my existing script to this endpoint and they just worked
          even with very complex prompts and contexts :-) Well done guys!
        updatedAt: '2023-08-11T21:18:46.913Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - lmzheng
        - MaziyarPanahi
        - valdesguefa
    id: 64d6a5b6a146b1c0a675422c
    type: comment
  author: rboehme86
  content: I can confirm it works flawlessly with a fresh install. Just created a
    new linux user on my GPU server, installed all and it was running like a charm.
    Quality is shockingly good. I used the OpenAI API interface to redirect some of
    my existing script to this endpoint and they just worked even with very complex
    prompts and contexts :-) Well done guys!
  created_at: 2023-08-11 20:18:46+00:00
  edited: false
  hidden: false
  id: 64d6a5b6a146b1c0a675422c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-08-15T09:32:07.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9024761915206909
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rboehme86&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rboehme86\">@<span class=\"\
          underline\">rboehme86</span></a></span>\n\n\t</span></span><br>Just out\
          \ of curiosity, would you mind sharing your GPU specs and how much it uses\
          \ if you do feed 16k input size?</p>\n"
        raw: "@rboehme86 \nJust out of curiosity, would you mind sharing your GPU\
          \ specs and how much it uses if you do feed 16k input size?"
        updatedAt: '2023-08-15T09:32:07.211Z'
      numEdits: 0
      reactions: []
    id: 64db46171855ce11cd5312f2
    type: comment
  author: MaziyarPanahi
  content: "@rboehme86 \nJust out of curiosity, would you mind sharing your GPU specs\
    \ and how much it uses if you do feed 16k input size?"
  created_at: 2023-08-15 08:32:07+00:00
  edited: false
  hidden: false
  id: 64db46171855ce11cd5312f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a9726df6f83e14c41bbc191c6e77189.svg
      fullname: dharmam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dharmam
      type: user
    createdAt: '2023-08-25T23:35:38.000Z'
    data:
      edited: false
      editors:
      - dharmam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6078779101371765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a9726df6f83e14c41bbc191c6e77189.svg
          fullname: dharmam
          isHf: false
          isPro: false
          name: dharmam
          type: user
        html: '<p>I tried to use Vicuna-13b-16k with vllm worker(feature in Fastchat
          library). In that case, it repeats single word in output.<br>reproduce the
          error:<br>" python3 -m fastchat.serve.vllm_worker --model-names "gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002"
          --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2"</p>

          <p>however it works when I replace "vllm_worker" to "model_worker"</p>

          '
        raw: "I tried to use Vicuna-13b-16k with vllm worker(feature in Fastchat library).\
          \ In that case, it repeats single word in output. \nreproduce the error:\n\
          \" python3 -m fastchat.serve.vllm_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\"\
          \ --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2\"\n\nhowever it works\
          \ when I replace \"vllm_worker\" to \"model_worker\""
        updatedAt: '2023-08-25T23:35:38.332Z'
      numEdits: 0
      reactions: []
    id: 64e93acafb77a3eaa7d11d17
    type: comment
  author: dharmam
  content: "I tried to use Vicuna-13b-16k with vllm worker(feature in Fastchat library).\
    \ In that case, it repeats single word in output. \nreproduce the error:\n\" python3\
    \ -m fastchat.serve.vllm_worker --model-names \"gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\"\
    \ --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2\"\n\nhowever it works when\
    \ I replace \"vllm_worker\" to \"model_worker\""
  created_at: 2023-08-25 22:35:38+00:00
  edited: false
  hidden: false
  id: 64e93acafb77a3eaa7d11d17
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lmsys/vicuna-13b-v1.5-16k
repo_type: model
status: open
target_branch: null
title: Prompt template
