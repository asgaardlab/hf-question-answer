!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cjdonahoe-ms
conflicting_files: []
created_at: 2023-10-17 16:47:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd9e91a89964d415794c491d59ac116d.svg
      fullname: CJ Donahoe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cjdonahoe-ms
      type: user
    createdAt: '2023-10-17T17:47:22.000Z'
    data:
      edited: false
      editors:
      - cjdonahoe-ms
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7705783843994141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd9e91a89964d415794c491d59ac116d.svg
          fullname: CJ Donahoe
          isHf: false
          isPro: false
          name: cjdonahoe-ms
          type: user
        html: '<p>I''m running this model on a vLLM server and I am receiving the
          following error: <code>&lt;Server response: {''code'': None, ''message'':
          "This model''s maximum context length is 4096 tokens. However, you requested
          4348 tokens (4332 in the messages, 16 in the completion). Please reduce
          the length of the messages or completion.", ''object'': ''error'', ''param'':
          None, ''type'': ''invalid_request_error''}&gt;</code></p>

          <p>Since this is the 16k version of Vicuna-13b-v1.5, the maximum context
          length should be 16384. The max_sequence_length in config.json is the only
          place left where there is any mention of 4096. I''m making the assumption
          that the parameter is what is causing the server error.</p>

          '
        raw: 'I''m running this model on a vLLM server and I am receiving the following
          error: `<Server response: {''code'': None, ''message'': "This model''s maximum
          context length is 4096 tokens. However, you requested 4348 tokens (4332
          in the messages, 16 in the completion). Please reduce the length of the
          messages or completion.", ''object'': ''error'', ''param'': None, ''type'':
          ''invalid_request_error''}>`


          Since this is the 16k version of Vicuna-13b-v1.5, the maximum context length
          should be 16384. The max_sequence_length in config.json is the only place
          left where there is any mention of 4096. I''m making the assumption that
          the parameter is what is causing the server error.'
        updatedAt: '2023-10-17T17:47:22.938Z'
      numEdits: 0
      reactions: []
    id: 652ec8aaef2f7e417ac2c7b7
    type: comment
  author: cjdonahoe-ms
  content: 'I''m running this model on a vLLM server and I am receiving the following
    error: `<Server response: {''code'': None, ''message'': "This model''s maximum
    context length is 4096 tokens. However, you requested 4348 tokens (4332 in the
    messages, 16 in the completion). Please reduce the length of the messages or completion.",
    ''object'': ''error'', ''param'': None, ''type'': ''invalid_request_error''}>`


    Since this is the 16k version of Vicuna-13b-v1.5, the maximum context length should
    be 16384. The max_sequence_length in config.json is the only place left where
    there is any mention of 4096. I''m making the assumption that the parameter is
    what is causing the server error.'
  created_at: 2023-10-17 16:47:22+00:00
  edited: false
  hidden: false
  id: 652ec8aaef2f7e417ac2c7b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/fd9e91a89964d415794c491d59ac116d.svg
      fullname: CJ Donahoe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cjdonahoe-ms
      type: user
    createdAt: '2023-10-17T17:47:23.000Z'
    data:
      oid: 36fba3ba6412deca7568cf10ae034b59e142592d
      parents:
      - 17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2
      subject: Update max_position_embeddings to 16384
    id: 652ec8ab0000000000000000
    type: commit
  author: cjdonahoe-ms
  created_at: 2023-10-17 16:47:23+00:00
  id: 652ec8ab0000000000000000
  oid: 36fba3ba6412deca7568cf10ae034b59e142592d
  summary: Update max_position_embeddings to 16384
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
      fullname: Wei-Lin Chiang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: weichiang
      type: user
    createdAt: '2023-10-23T20:35:24.000Z'
    data:
      edited: true
      editors:
      - weichiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.590235710144043
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
          fullname: Wei-Lin Chiang
          isHf: false
          isPro: false
          name: weichiang
          type: user
        html: '<p>the rope scaling factor is already specified here. I think vLLM
          should probably use this to decide the context length?<br><a href="https://huggingface.co/lmsys/vicuna-13b-v1.5-16k/blob/17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2/config.json#L22">https://huggingface.co/lmsys/vicuna-13b-v1.5-16k/blob/17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2/config.json#L22</a></p>

          '
        raw: 'the rope scaling factor is already specified here. I think vLLM should
          probably use this to decide the context length?

          https://huggingface.co/lmsys/vicuna-13b-v1.5-16k/blob/17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2/config.json#L22'
        updatedAt: '2023-10-23T20:35:49.313Z'
      numEdits: 1
      reactions: []
      relatedEventId: 6536d90c445863e72153ea38
    id: 6536d90c445863e72153ea36
    type: comment
  author: weichiang
  content: 'the rope scaling factor is already specified here. I think vLLM should
    probably use this to decide the context length?

    https://huggingface.co/lmsys/vicuna-13b-v1.5-16k/blob/17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2/config.json#L22'
  created_at: 2023-10-23 19:35:24+00:00
  edited: true
  hidden: false
  id: 6536d90c445863e72153ea36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
      fullname: Wei-Lin Chiang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: weichiang
      type: user
    createdAt: '2023-10-23T20:35:24.000Z'
    data:
      status: closed
    id: 6536d90c445863e72153ea38
    type: status-change
  author: weichiang
  created_at: 2023-10-23 19:35:24+00:00
  id: 6536d90c445863e72153ea38
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 8
repo_id: lmsys/vicuna-13b-v1.5-16k
repo_type: model
status: closed
target_branch: refs/heads/main
title: Update max_position_embeddings to 16384
