!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Maverick17
conflicting_files: null
created_at: 2023-08-06 10:18:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/689cee860d8d6d6dde7e02b6dcbda5bf.svg
      fullname: Prokhorov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maverick17
      type: user
    createdAt: '2023-08-06T11:18:14.000Z'
    data:
      edited: false
      editors:
      - Maverick17
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5941725373268127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/689cee860d8d6d6dde7e02b6dcbda5bf.svg
          fullname: Prokhorov
          isHf: false
          isPro: false
          name: Maverick17
          type: user
        html: '<p>How to use this model correclty?</p>

          <p>My current code looks like the following:</p>

          <p>from transformers import AutoTokenizer, AutoModelForTokenClassification,
          AutoConfig</p>

          <p>config = AutoConfig.from_pretrained("tli8hf/robertabase-structured-tuning-srl-conll2012")<br>tokenizer
          = AutoTokenizer.from_pretrained("tli8hf/robertabase-structured-tuning-srl-conll2012")<br>model
          = AutoModelForTokenClassification.from_pretrained("tli8hf/robertabase-structured-tuning-srl-conll2012",
          device_map="auto")</p>

          <p>query = "The keys, which were needed to access the building, were locked
          in the car."<br>inputs = tokenizer(query, return_tensors="pt")<br>outputs
          = model(**inputs)<br>predicted_labels = outputs.logits.argmax(dim=2).squeeze().tolist()  #
          Get the predicted labels</p>

          <p>texts = [tokenizer.decode(token) for token in inputs.input_ids[0]]<br>tags
          = [config.label_map_inv[str(label_id)] for label_id in predicted_labels]</p>

          <p>print(texts)<br>print(tags)</p>

          '
        raw: "How to use this model correclty?\r\n\r\nMy current code looks like the\
          \ following:\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,\
          \ AutoConfig\r\n\r\nconfig = AutoConfig.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
          )\r\nmodel = AutoModelForTokenClassification.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
          , device_map=\"auto\")\r\n\r\nquery = \"The keys, which were needed to access\
          \ the building, were locked in the car.\"\r\ninputs = tokenizer(query, return_tensors=\"\
          pt\")\r\noutputs = model(**inputs)\r\npredicted_labels = outputs.logits.argmax(dim=2).squeeze().tolist()\
          \  # Get the predicted labels\r\n\r\ntexts = [tokenizer.decode(token) for\
          \ token in inputs.input_ids[0]]\r\ntags = [config.label_map_inv[str(label_id)]\
          \ for label_id in predicted_labels]\r\n\r\nprint(texts)\r\nprint(tags)\r\
          \n"
        updatedAt: '2023-08-06T11:18:14.414Z'
      numEdits: 0
      reactions: []
    id: 64cf81760fbfb00b9157ad06
    type: comment
  author: Maverick17
  content: "How to use this model correclty?\r\n\r\nMy current code looks like the\
    \ following:\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,\
    \ AutoConfig\r\n\r\nconfig = AutoConfig.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
    )\r\ntokenizer = AutoTokenizer.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
    )\r\nmodel = AutoModelForTokenClassification.from_pretrained(\"tli8hf/robertabase-structured-tuning-srl-conll2012\"\
    , device_map=\"auto\")\r\n\r\nquery = \"The keys, which were needed to access\
    \ the building, were locked in the car.\"\r\ninputs = tokenizer(query, return_tensors=\"\
    pt\")\r\noutputs = model(**inputs)\r\npredicted_labels = outputs.logits.argmax(dim=2).squeeze().tolist()\
    \  # Get the predicted labels\r\n\r\ntexts = [tokenizer.decode(token) for token\
    \ in inputs.input_ids[0]]\r\ntags = [config.label_map_inv[str(label_id)] for label_id\
    \ in predicted_labels]\r\n\r\nprint(texts)\r\nprint(tags)\r\n"
  created_at: 2023-08-06 10:18:14+00:00
  edited: false
  hidden: false
  id: 64cf81760fbfb00b9157ad06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d2dd6b1fa6e3fe805f8627/1TiIsCsE-Wui0GosZsHYh.jpeg?w=200&h=200&f=face
      fullname: Tao Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: t-li
      type: user
    createdAt: '2023-08-09T00:31:01.000Z'
    data:
      edited: false
      editors:
      - t-li
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8847618699073792
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d2dd6b1fa6e3fe805f8627/1TiIsCsE-Wui0GosZsHYh.jpeg?w=200&h=200&f=face
          fullname: Tao Li
          isHf: false
          isPro: false
          name: t-li
          type: user
        html: '<p>Hey, sorry the model is not a standalone thing. Did you try the
          "demo" section in this <a rel="nofollow" href="https://github.com/utahnlp/structured_tuning_srl">repo</a>?</p>

          <p>Let me know if you run into other errors.</p>

          <p>Tao</p>

          '
        raw: 'Hey, sorry the model is not a standalone thing. Did you try the "demo"
          section in this [repo](https://github.com/utahnlp/structured_tuning_srl)?


          Let me know if you run into other errors.


          Tao'
        updatedAt: '2023-08-09T00:31:01.531Z'
      numEdits: 0
      reactions: []
    id: 64d2de459535640681ed196f
    type: comment
  author: t-li
  content: 'Hey, sorry the model is not a standalone thing. Did you try the "demo"
    section in this [repo](https://github.com/utahnlp/structured_tuning_srl)?


    Let me know if you run into other errors.


    Tao'
  created_at: 2023-08-08 23:31:01+00:00
  edited: false
  hidden: false
  id: 64d2de459535640681ed196f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/689cee860d8d6d6dde7e02b6dcbda5bf.svg
      fullname: Prokhorov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maverick17
      type: user
    createdAt: '2023-08-10T15:17:01.000Z'
    data:
      edited: false
      editors:
      - Maverick17
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.904519259929657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/689cee860d8d6d6dde7e02b6dcbda5bf.svg
          fullname: Prokhorov
          isHf: false
          isPro: false
          name: Maverick17
          type: user
        html: '<p>Hi Tao,</p>

          <p>thanks for the info! I tried it out and it works on Linux, but not on
          Windows due to BitsAndBytes package...</p>

          <p>Why do you guys implement this SRL stuff using BERT-based models? Of
          course, this models are pretty much capable of capturing the language context
          very well, but whatabout the latest LLMs like LLaMas? Why nobody fine tune
          them in order to predict the semantic roles?</p>

          '
        raw: 'Hi Tao,


          thanks for the info! I tried it out and it works on Linux, but not on Windows
          due to BitsAndBytes package...


          Why do you guys implement this SRL stuff using BERT-based models? Of course,
          this models are pretty much capable of capturing the language context very
          well, but whatabout the latest LLMs like LLaMas? Why nobody fine tune them
          in order to predict the semantic roles?


          '
        updatedAt: '2023-08-10T15:17:01.254Z'
      numEdits: 0
      reactions: []
    id: 64d4ff6d7cfef99e3001cb10
    type: comment
  author: Maverick17
  content: 'Hi Tao,


    thanks for the info! I tried it out and it works on Linux, but not on Windows
    due to BitsAndBytes package...


    Why do you guys implement this SRL stuff using BERT-based models? Of course, this
    models are pretty much capable of capturing the language context very well, but
    whatabout the latest LLMs like LLaMas? Why nobody fine tune them in order to predict
    the semantic roles?


    '
  created_at: 2023-08-10 14:17:01+00:00
  edited: false
  hidden: false
  id: 64d4ff6d7cfef99e3001cb10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1603143637927-noauth.jpeg?w=200&h=200&f=face
      fullname: Tao Li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: tli8hf
      type: user
    createdAt: '2023-08-10T17:44:03.000Z'
    data:
      edited: false
      editors:
      - tli8hf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9823583364486694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1603143637927-noauth.jpeg?w=200&h=200&f=face
          fullname: Tao Li
          isHf: false
          isPro: false
          name: tli8hf
          type: user
        html: '<p>Great to hear that.</p>

          <p>Back then (2019-2020 which is time of our paper), RoBERTa was one of
          the best model for classification tasks. I agree that recent generative
          models could be a stronger option. I bet many folks probably already tried
          it. There is definitely something interesting there.</p>

          '
        raw: 'Great to hear that.


          Back then (2019-2020 which is time of our paper), RoBERTa was one of the
          best model for classification tasks. I agree that recent generative models
          could be a stronger option. I bet many folks probably already tried it.
          There is definitely something interesting there.'
        updatedAt: '2023-08-10T17:44:03.792Z'
      numEdits: 0
      reactions: []
    id: 64d521e3a787c9bc7b6002db
    type: comment
  author: tli8hf
  content: 'Great to hear that.


    Back then (2019-2020 which is time of our paper), RoBERTa was one of the best
    model for classification tasks. I agree that recent generative models could be
    a stronger option. I bet many folks probably already tried it. There is definitely
    something interesting there.'
  created_at: 2023-08-10 16:44:03+00:00
  edited: false
  hidden: false
  id: 64d521e3a787c9bc7b6002db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/689cee860d8d6d6dde7e02b6dcbda5bf.svg
      fullname: Prokhorov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maverick17
      type: user
    createdAt: '2023-08-11T06:35:52.000Z'
    data:
      status: closed
    id: 64d5d6c8089bc502ce8b7641
    type: status-change
  author: Maverick17
  created_at: 2023-08-11 05:35:52+00:00
  id: 64d5d6c8089bc502ce8b7641
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tli8hf/robertabase-structured-tuning-srl-conll2012
repo_type: model
status: closed
target_branch: null
title: Usage
