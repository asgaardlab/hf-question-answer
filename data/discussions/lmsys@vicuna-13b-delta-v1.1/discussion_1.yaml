!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lpy86786
conflicting_files: null
created_at: 2023-04-13 03:05:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c040a8042f6ec2d86054238d605a167.svg
      fullname: lpy86786
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lpy86786
      type: user
    createdAt: '2023-04-13T04:05:38.000Z'
    data:
      edited: false
      editors:
      - lpy86786
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c040a8042f6ec2d86054238d605a167.svg
          fullname: lpy86786
          isHf: false
          isPro: false
          name: lpy86786
          type: user
        html: '<p>I have downloaded the model and followed the instructions on <a
          rel="nofollow" href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>
          and have gone through the models from lmsys/vicuna-13b-delta-v0. But it
          won''t work for lmsys/vicuna-13b-delta-v1.1 until I add the following files
          from lmsys/vicuna-13b-delta-v0:</p>

          <p>special_tokens_map.json<br>tokenizer.model<br>tokenizer_config.json</p>

          <p>After that I got screens of messy code...I guess maybe the three correct
          corresponding files are missing?</p>

          '
        raw: "I have downloaded the model and followed the instructions on https://github.com/lm-sys/FastChat\
          \ and have gone through the models from lmsys/vicuna-13b-delta-v0. But it\
          \ won't work for lmsys/vicuna-13b-delta-v1.1 until I add the following files\
          \ from lmsys/vicuna-13b-delta-v0:\r\n\r\nspecial_tokens_map.json\r\ntokenizer.model\r\
          \ntokenizer_config.json\r\n\r\nAfter that I got screens of messy code...I\
          \ guess maybe the three correct corresponding files are missing?"
        updatedAt: '2023-04-13T04:05:38.148Z'
      numEdits: 0
      reactions: []
    id: 64377f924aacf7bf7872aad6
    type: comment
  author: lpy86786
  content: "I have downloaded the model and followed the instructions on https://github.com/lm-sys/FastChat\
    \ and have gone through the models from lmsys/vicuna-13b-delta-v0. But it won't\
    \ work for lmsys/vicuna-13b-delta-v1.1 until I add the following files from lmsys/vicuna-13b-delta-v0:\r\
    \n\r\nspecial_tokens_map.json\r\ntokenizer.model\r\ntokenizer_config.json\r\n\r\
    \nAfter that I got screens of messy code...I guess maybe the three correct corresponding\
    \ files are missing?"
  created_at: 2023-04-13 03:05:38+00:00
  edited: false
  hidden: false
  id: 64377f924aacf7bf7872aad6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
      fullname: Johnny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FanGod
      type: user
    createdAt: '2023-04-13T04:14:36.000Z'
    data:
      edited: false
      editors:
      - FanGod
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
          fullname: Johnny
          isHf: false
          isPro: false
          name: FanGod
          type: user
        html: '<p>Yes, I have the same problem.</p>

          <pre><code>OSError: Can''t load tokenizer for ''/home/lianpengcheng/models/source_models/vicuna-13b-delta-v1.1/''

          </code></pre>

          '
        raw: 'Yes, I have the same problem.

          ```

          OSError: Can''t load tokenizer for ''/home/lianpengcheng/models/source_models/vicuna-13b-delta-v1.1/''

          ```'
        updatedAt: '2023-04-13T04:14:36.479Z'
      numEdits: 0
      reactions: []
    id: 643781acb6ed89f1417e987a
    type: comment
  author: FanGod
  content: 'Yes, I have the same problem.

    ```

    OSError: Can''t load tokenizer for ''/home/lianpengcheng/models/source_models/vicuna-13b-delta-v1.1/''

    ```'
  created_at: 2023-04-13 03:14:36+00:00
  edited: false
  hidden: false
  id: 643781acb6ed89f1417e987a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-04-13T04:38:03.000Z'
    data:
      edited: false
      editors:
      - lmzheng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: '<p>Hi, the tokenizer files are omitted on purpose because we didn''t
          change any tokenizer. The tokenizer will be the same as LLaMa''s tokenizer.<br>For
          your problems, please install the latest version of FastChat and apply the
          delta again. There should be no errors.</p>

          '
        raw: 'Hi, the tokenizer files are omitted on purpose because we didn''t change
          any tokenizer. The tokenizer will be the same as LLaMa''s tokenizer.

          For your problems, please install the latest version of FastChat and apply
          the delta again. There should be no errors.'
        updatedAt: '2023-04-13T04:38:03.429Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MikeZhang0701
    id: 6437872b2c5801f1163dcde1
    type: comment
  author: lmzheng
  content: 'Hi, the tokenizer files are omitted on purpose because we didn''t change
    any tokenizer. The tokenizer will be the same as LLaMa''s tokenizer.

    For your problems, please install the latest version of FastChat and apply the
    delta again. There should be no errors.'
  created_at: 2023-04-13 03:38:03+00:00
  edited: false
  hidden: false
  id: 6437872b2c5801f1163dcde1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cfc1a484077a8d511c46392de8dee07.svg
      fullname: Zhanghao Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: michaelvll
      type: user
    createdAt: '2023-04-13T05:35:37.000Z'
    data:
      edited: false
      editors:
      - michaelvll
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cfc1a484077a8d511c46392de8dee07.svg
          fullname: Zhanghao Wu
          isHf: false
          isPro: false
          name: michaelvll
          type: user
        html: '<p>A reminder: you may want to use the llama model transformed using
          the latest huggingface transformers, as they recently updated the tokenizer.
          For example, <a href="https://huggingface.co/decapoda-research/llama-13b-hf/tree/main">this</a>
          may not work,  but <a href="https://huggingface.co/huggyllama/llama-7b/tree/main">this</a>
          should work.</p>

          '
        raw: 'A reminder: you may want to use the llama model transformed using the
          latest huggingface transformers, as they recently updated the tokenizer.
          For example, [this](https://huggingface.co/decapoda-research/llama-13b-hf/tree/main)
          may not work,  but [this](https://huggingface.co/huggyllama/llama-7b/tree/main)
          should work.'
        updatedAt: '2023-04-13T05:35:37.758Z'
      numEdits: 0
      reactions: []
    id: 643794a9b6ed89f1417f258e
    type: comment
  author: michaelvll
  content: 'A reminder: you may want to use the llama model transformed using the
    latest huggingface transformers, as they recently updated the tokenizer. For example,
    [this](https://huggingface.co/decapoda-research/llama-13b-hf/tree/main) may not
    work,  but [this](https://huggingface.co/huggyllama/llama-7b/tree/main) should
    work.'
  created_at: 2023-04-13 04:35:37+00:00
  edited: false
  hidden: false
  id: 643794a9b6ed89f1417f258e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
      fullname: Johnny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FanGod
      type: user
    createdAt: '2023-04-13T05:57:31.000Z'
    data:
      edited: true
      editors:
      - FanGod
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
          fullname: Johnny
          isHf: false
          isPro: false
          name: FanGod
          type: user
        html: '<p>Use   LLaMa''s tokenizer, but still error.</p>

          <pre><code>...

          param.data += delta.state_dict()[name]

          ...


          RuntimeError: The size of tensor a (32001) must match the size of tensor
          b (32000) at non-singleton dimension 0

          </code></pre>

          '
        raw: 'Use   LLaMa''s tokenizer, but still error.

          ```

          ...

          param.data += delta.state_dict()[name]

          ...


          RuntimeError: The size of tensor a (32001) must match the size of tensor
          b (32000) at non-singleton dimension 0

          ```'
        updatedAt: '2023-04-13T05:58:26.129Z'
      numEdits: 2
      reactions: []
    id: 643799cb369f6f907f5dbc28
    type: comment
  author: FanGod
  content: 'Use   LLaMa''s tokenizer, but still error.

    ```

    ...

    param.data += delta.state_dict()[name]

    ...


    RuntimeError: The size of tensor a (32001) must match the size of tensor b (32000)
    at non-singleton dimension 0

    ```'
  created_at: 2023-04-13 04:57:31+00:00
  edited: true
  hidden: false
  id: 643799cb369f6f907f5dbc28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/h7dT77kNV3gcVeJQ8LHxh.jpeg?w=200&h=200&f=face
      fullname: Tim Eastwood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tangles
      type: user
    createdAt: '2023-04-13T09:30:56.000Z'
    data:
      edited: false
      editors:
      - tangles
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/h7dT77kNV3gcVeJQ8LHxh.jpeg?w=200&h=200&f=face
          fullname: Tim Eastwood
          isHf: false
          isPro: false
          name: tangles
          type: user
        html: '<p>use the latest apply_delta.py from the fastchat repo</p>

          '
        raw: use the latest apply_delta.py from the fastchat repo
        updatedAt: '2023-04-13T09:30:56.048Z'
      numEdits: 0
      reactions: []
    id: 6437cbd0f8a71f96bcdcaa79
    type: comment
  author: tangles
  content: use the latest apply_delta.py from the fastchat repo
  created_at: 2023-04-13 08:30:56+00:00
  edited: false
  hidden: false
  id: 6437cbd0f8a71f96bcdcaa79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
      fullname: Johnny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FanGod
      type: user
    createdAt: '2023-04-13T10:04:13.000Z'
    data:
      edited: false
      editors:
      - FanGod
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6433930f6c6ecd587984fc63/yWXFiGlG82gAzdkfrjpoJ.png?w=200&h=200&f=face
          fullname: Johnny
          isHf: false
          isPro: false
          name: FanGod
          type: user
        html: '<p>Thanks a lot,  it works.</p>

          '
        raw: Thanks a lot,  it works.
        updatedAt: '2023-04-13T10:04:13.109Z'
      numEdits: 0
      reactions: []
    id: 6437d39d94faafc1a2de3b2c
    type: comment
  author: FanGod
  content: Thanks a lot,  it works.
  created_at: 2023-04-13 09:04:13+00:00
  edited: false
  hidden: false
  id: 6437d39d94faafc1a2de3b2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c040a8042f6ec2d86054238d605a167.svg
      fullname: lpy86786
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lpy86786
      type: user
    createdAt: '2023-04-14T12:53:15.000Z'
    data:
      edited: false
      editors:
      - lpy86786
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c040a8042f6ec2d86054238d605a167.svg
          fullname: lpy86786
          isHf: false
          isPro: false
          name: lpy86786
          type: user
        html: '<p>Thanks a lot. The tokenizer files from lmsys/vicuna-13b-delta-v0
          have no problem and can be directly used.<br>Finally I found it was my mistake
          to omit the hint "NOTE: This "delta model" cannot be used directly.".<br>My
          problem has been addressed after using the models from <a href="https://huggingface.co/eachadea/vicuna-13b-1.1">https://huggingface.co/eachadea/vicuna-13b-1.1</a>
          .That model can be directly used.</p>

          '
        raw: 'Thanks a lot. The tokenizer files from lmsys/vicuna-13b-delta-v0 have
          no problem and can be directly used.

          Finally I found it was my mistake to omit the hint "NOTE: This "delta model"
          cannot be used directly.".

          My problem has been addressed after using the models from https://huggingface.co/eachadea/vicuna-13b-1.1
          .That model can be directly used.'
        updatedAt: '2023-04-14T12:53:15.160Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - hisku
    id: 64394cbb11e9481b75e18595
    type: comment
  author: lpy86786
  content: 'Thanks a lot. The tokenizer files from lmsys/vicuna-13b-delta-v0 have
    no problem and can be directly used.

    Finally I found it was my mistake to omit the hint "NOTE: This "delta model" cannot
    be used directly.".

    My problem has been addressed after using the models from https://huggingface.co/eachadea/vicuna-13b-1.1
    .That model can be directly used.'
  created_at: 2023-04-14 11:53:15+00:00
  edited: false
  hidden: false
  id: 64394cbb11e9481b75e18595
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa6e304e98696a053ffe17a8c7b442e1.svg
      fullname: liyang31163150
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liyang31163150
      type: user
    createdAt: '2023-04-22T17:53:25.000Z'
    data:
      edited: false
      editors:
      - liyang31163150
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa6e304e98696a053ffe17a8c7b442e1.svg
          fullname: liyang31163150
          isHf: false
          isPro: false
          name: liyang31163150
          type: user
        html: "<p>Can you provide the merged version\uFF08with llama version\uFF09\
          \ instead of just the incremental version?</p>\n"
        raw: "Can you provide the merged version\uFF08with llama version\uFF09 instead\
          \ of just the incremental version?"
        updatedAt: '2023-04-22T17:53:25.406Z'
      numEdits: 0
      reactions: []
    id: 64441f155298d19c9c0160fd
    type: comment
  author: liyang31163150
  content: "Can you provide the merged version\uFF08with llama version\uFF09 instead\
    \ of just the incremental version?"
  created_at: 2023-04-22 16:53:25+00:00
  edited: false
  hidden: false
  id: 64441f155298d19c9c0160fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-22T19:20:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<p>Can you provide the merged version\uFF08with llama\
          \ version\uFF09 instead of just the incremental version?</p>\n</blockquote>\n\
          <p>They can't provide a merged version due to the Llama license terms. But\
          \ I've merged it and it's available here: <a href=\"https://huggingface.co/TheBloke/vicuna-13B-1.1-HF\"\
          >https://huggingface.co/TheBloke/vicuna-13B-1.1-HF</a></p>\n"
        raw: "> Can you provide the merged version\uFF08with llama version\uFF09 instead\
          \ of just the incremental version?\n\nThey can't provide a merged version\
          \ due to the Llama license terms. But I've merged it and it's available\
          \ here: https://huggingface.co/TheBloke/vicuna-13B-1.1-HF"
        updatedAt: '2023-04-22T19:20:15.986Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - vishaal27
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - vishaal27
    id: 6444336fc63001ae6354af56
    type: comment
  author: TheBloke
  content: "> Can you provide the merged version\uFF08with llama version\uFF09 instead\
    \ of just the incremental version?\n\nThey can't provide a merged version due\
    \ to the Llama license terms. But I've merged it and it's available here: https://huggingface.co/TheBloke/vicuna-13B-1.1-HF"
  created_at: 2023-04-22 18:20:15+00:00
  edited: false
  hidden: false
  id: 6444336fc63001ae6354af56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100ff79518943257534e20f5bc0e0952.svg
      fullname: Ricardo Ros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricardor1267
      type: user
    createdAt: '2023-04-28T01:58:14.000Z'
    data:
      edited: false
      editors:
      - ricardor1267
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/100ff79518943257534e20f5bc0e0952.svg
          fullname: Ricardo Ros
          isHf: false
          isPro: false
          name: ricardor1267
          type: user
        html: '<p>Help..  </p>

          <p> return self._apply(lambda t: t.cuda(device))<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 6.00 GiB total
          capacity; 5.27 GiB already allocated; 0 bytes free; 5.27 GiB reserved in
          total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "Help..  \n\n return self._apply(lambda t: t.cuda(device))\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 6.00 GiB total\
          \ capacity; 5.27 GiB already allocated; 0 bytes free; 5.27 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-04-28T01:58:14.048Z'
      numEdits: 0
      reactions: []
    id: 644b2836cb45734dfd4dff0c
    type: comment
  author: ricardor1267
  content: "Help..  \n\n return self._apply(lambda t: t.cuda(device))\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 6.00 GiB total capacity;\
    \ 5.27 GiB already allocated; 0 bytes free; 5.27 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-04-28 00:58:14+00:00
  edited: false
  hidden: false
  id: 644b2836cb45734dfd4dff0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-05-05T15:08:22.000Z'
    data:
      status: closed
    id: 64551be6d55525a4fee76480
    type: status-change
  author: lmzheng
  created_at: 2023-05-05 14:08:22+00:00
  id: 64551be6d55525a4fee76480
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lmsys/vicuna-13b-delta-v1.1
repo_type: model
status: closed
target_branch: null
title: Maybe some tokenizer files are missing?
