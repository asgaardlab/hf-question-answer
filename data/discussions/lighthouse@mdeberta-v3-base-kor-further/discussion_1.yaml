!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sanginoh
conflicting_files: null
created_at: 2022-09-03 11:32:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da5ab043162a3e4a1dfcf46bb7650476.svg
      fullname: sangin.oh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanginoh
      type: user
    createdAt: '2022-09-03T12:32:14.000Z'
    data:
      edited: false
      editors:
      - sanginoh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da5ab043162a3e4a1dfcf46bb7650476.svg
          fullname: sangin.oh
          isHf: false
          isPro: false
          name: sanginoh
          type: user
        html: "<p>\uC548\uB155\uD558\uC138\uC694? \uC544\uB798\uC640 \uAC19\uC774\
          \ \uCF54\uB4DC\uB97C \uC774\uC6A9\uD574\uC11C \uC0AC\uC6A9\uD574\uBCFC\uB824\
          \uACE0 \uD558\uB294\uB370</p>\n<p>input_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)<br>token_type_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)<br>attention_mask = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)</p>\n<p>mdeberta_model = TFAutoModel.from_pretrained(\"\
          lighthouse/mdeberta-v3-base-kor-further\", from_pt=True, output_hidden_states=True)<br>x\
          \ = mdeberta_model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids,\
          \ \"attention_mask\": attention_mask}, training=False)<br>outputs = x[\"\
          pooler_output\"]</p>\n<p>model = tf.keras.Model([input_ids, token_type_ids,\
          \ attention_mask], outputs)</p>\n<p>ValueError: Tried to convert 'shape'\
          \ to a tensor and failed. Error: Cannot convert a partially known TensorShape\
          \ (None, 512, 512) to a Tensor.</p>\n<pre><code>                Call arguments\
          \ received by layer \"self\" (type TFDebertaV2DisentangledSelfAttention):\n\
          \                  \u2022 hidden_states=tf.Tensor(shape=(None, 512, 768),\
          \ dtype=float32)\n                  \u2022 attention_mask=tf.Tensor(shape=(None,\
          \ 1, 512, 512), dtype=uint8)\n                  \u2022 query_states=None\n\
          \                  \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\n\
          \                  \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\n\
          \                  \u2022 output_attentions=False\n                  \u2022\
          \ training=False\n            \n            \n            Call arguments\
          \ received by layer \"attention\" (type TFDebertaV2Attention):\n       \
          \       \u2022 input_tensor=tf.Tensor(shape=(None, 512, 768), dtype=float32)\n\
          \              \u2022 attention_mask=tf.Tensor(shape=(None, 1, 512, 512),\
          \ dtype=uint8)\n              \u2022 query_states=None\n              \u2022\
          \ relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\n           \
          \   \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\n \
          \             \u2022 output_attentions=False\n              \u2022 training=False\n\
          \        \n        \n        Call arguments received by layer \"layer_._0\"\
          \ (type TFDebertaV2Layer):\n          \u2022 hidden_states=tf.Tensor(shape=(None,\
          \ 512, 768), dtype=float32)\n          \u2022 attention_mask=tf.Tensor(shape=(None,\
          \ 1, 512, 512), dtype=uint8)\n          \u2022 query_states=None\n     \
          \     \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\n\
          \          \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\n\
          \          \u2022 output_attentions=False\n          \u2022 training=False\n\
          \    \n    \n    Call arguments received by layer \"encoder\" (type TFDebertaV2Encoder):\n\
          \      \u2022 hidden_states=tf.Tensor(shape=(None, 512, 768), dtype=float32)\n\
          \      \u2022 attention_mask=tf.Tensor(shape=(None, 512), dtype=int32)\n\
          \      \u2022 query_states=None\n      \u2022 relative_pos=None\n      \u2022\
          \ output_attentions=False\n      \u2022 output_hidden_states=True\n    \
          \  \u2022 return_dict=True\n      \u2022 training=False\n\n\nCall arguments\
          \ received by layer \"deberta\" (type TFDebertaV2MainLayer):\n  \u2022 self=tf.Tensor(shape=(None,\
          \ 512), dtype=int32)\n  \u2022 input_ids=None\n  \u2022 attention_mask=tf.Tensor(shape=(None,\
          \ 512), dtype=int32)\n  \u2022 token_type_ids=tf.Tensor(shape=(None, 512),\
          \ dtype=int32)\n  \u2022 position_ids=None\n  \u2022 inputs_embeds=None\n\
          \  \u2022 output_attentions=False\n  \u2022 output_hidden_states=True\n\
          \  \u2022 return_dict=True\n  \u2022 training=False\n</code></pre>\n<p>\uC704\
          \uC640 \uAC19\uC774 \uC5D0\uB7EC\uAC00 \uB0A9\uB2C8\uB2E4. \uB2E4\uB978\
          \ \uBAA8\uB378\uB4E4\uC740 \uC624\uB958\uAC00 \uC5C6\uB294\uB370.. input_ids\
          \ = None\uC73C\uB85C \uAC12\uC774 \uB4E4\uC5B4\uAC00\uC11C \uB098\uB294\
          \ \uC624\uB958\uB85C \uBCF4\uC774\uB294\uB370 \uD639\uC2DC PyTorch \uCF54\
          \uB4DC \uC608\uC81C\uB098 \uC6D0\uC778\uC744 \uC54C\uC218 \uC5C6\uB294\uC9C0\
          \ \uAD81\uAE08\uD569\uB2C8\uB2E4.</p>\n"
        raw: "\uC548\uB155\uD558\uC138\uC694? \uC544\uB798\uC640 \uAC19\uC774 \uCF54\
          \uB4DC\uB97C \uC774\uC6A9\uD574\uC11C \uC0AC\uC6A9\uD574\uBCFC\uB824\uACE0\
          \ \uD558\uB294\uB370\r\n\r\ninput_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)\r\ntoken_type_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)\r\nattention_mask = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,),\
          \ dtype=tf.int32)\r\n\r\nmdeberta_model = TFAutoModel.from_pretrained(\"\
          lighthouse/mdeberta-v3-base-kor-further\", from_pt=True, output_hidden_states=True)\r\
          \nx = mdeberta_model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids,\
          \ \"attention_mask\": attention_mask}, training=False)\r\noutputs = x[\"\
          pooler_output\"]\r\n\r\nmodel = tf.keras.Model([input_ids, token_type_ids,\
          \ attention_mask], outputs)\r\n\r\nValueError: Tried to convert 'shape'\
          \ to a tensor and failed. Error: Cannot convert a partially known TensorShape\
          \ (None, 512, 512) to a Tensor.\r\n                    \r\n            \
          \        \r\n                    Call arguments received by layer \"self\"\
          \ (type TFDebertaV2DisentangledSelfAttention):\r\n                     \
          \ \u2022 hidden_states=tf.Tensor(shape=(None, 512, 768), dtype=float32)\r\
          \n                      \u2022 attention_mask=tf.Tensor(shape=(None, 1,\
          \ 512, 512), dtype=uint8)\r\n                      \u2022 query_states=None\r\
          \n                      \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512),\
          \ dtype=int64)\r\n                      \u2022 rel_embeddings=tf.Tensor(shape=(512,\
          \ 768), dtype=float32)\r\n                      \u2022 output_attentions=False\r\
          \n                      \u2022 training=False\r\n                \r\n  \
          \              \r\n                Call arguments received by layer \"attention\"\
          \ (type TFDebertaV2Attention):\r\n                  \u2022 input_tensor=tf.Tensor(shape=(None,\
          \ 512, 768), dtype=float32)\r\n                  \u2022 attention_mask=tf.Tensor(shape=(None,\
          \ 1, 512, 512), dtype=uint8)\r\n                  \u2022 query_states=None\r\
          \n                  \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\r\
          \n                  \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\r\
          \n                  \u2022 output_attentions=False\r\n                 \
          \ \u2022 training=False\r\n            \r\n            \r\n            Call\
          \ arguments received by layer \"layer_._0\" (type TFDebertaV2Layer):\r\n\
          \              \u2022 hidden_states=tf.Tensor(shape=(None, 512, 768), dtype=float32)\r\
          \n              \u2022 attention_mask=tf.Tensor(shape=(None, 1, 512, 512),\
          \ dtype=uint8)\r\n              \u2022 query_states=None\r\n           \
          \   \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\r\n\
          \              \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\r\
          \n              \u2022 output_attentions=False\r\n              \u2022 training=False\r\
          \n        \r\n        \r\n        Call arguments received by layer \"encoder\"\
          \ (type TFDebertaV2Encoder):\r\n          \u2022 hidden_states=tf.Tensor(shape=(None,\
          \ 512, 768), dtype=float32)\r\n          \u2022 attention_mask=tf.Tensor(shape=(None,\
          \ 512), dtype=int32)\r\n          \u2022 query_states=None\r\n         \
          \ \u2022 relative_pos=None\r\n          \u2022 output_attentions=False\r\
          \n          \u2022 output_hidden_states=True\r\n          \u2022 return_dict=True\r\
          \n          \u2022 training=False\r\n    \r\n    \r\n    Call arguments\
          \ received by layer \"deberta\" (type TFDebertaV2MainLayer):\r\n      \u2022\
          \ self=tf.Tensor(shape=(None, 512), dtype=int32)\r\n      \u2022 input_ids=None\r\
          \n      \u2022 attention_mask=tf.Tensor(shape=(None, 512), dtype=int32)\r\
          \n      \u2022 token_type_ids=tf.Tensor(shape=(None, 512), dtype=int32)\r\
          \n      \u2022 position_ids=None\r\n      \u2022 inputs_embeds=None\r\n\
          \      \u2022 output_attentions=False\r\n      \u2022 output_hidden_states=True\r\
          \n      \u2022 return_dict=True\r\n      \u2022 training=False\r\n\r\n\uC704\
          \uC640 \uAC19\uC774 \uC5D0\uB7EC\uAC00 \uB0A9\uB2C8\uB2E4. \uB2E4\uB978\
          \ \uBAA8\uB378\uB4E4\uC740 \uC624\uB958\uAC00 \uC5C6\uB294\uB370.. input_ids\
          \ = None\uC73C\uB85C \uAC12\uC774 \uB4E4\uC5B4\uAC00\uC11C \uB098\uB294\
          \ \uC624\uB958\uB85C \uBCF4\uC774\uB294\uB370 \uD639\uC2DC PyTorch \uCF54\
          \uB4DC \uC608\uC81C\uB098 \uC6D0\uC778\uC744 \uC54C\uC218 \uC5C6\uB294\uC9C0\
          \ \uAD81\uAE08\uD569\uB2C8\uB2E4."
        updatedAt: '2022-09-03T12:32:14.144Z'
      numEdits: 0
      reactions: []
    id: 6313494e54e6e5d9f0f63c0b
    type: comment
  author: sanginoh
  content: "\uC548\uB155\uD558\uC138\uC694? \uC544\uB798\uC640 \uAC19\uC774 \uCF54\
    \uB4DC\uB97C \uC774\uC6A9\uD574\uC11C \uC0AC\uC6A9\uD574\uBCFC\uB824\uACE0 \uD558\
    \uB294\uB370\r\n\r\ninput_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,), dtype=tf.int32)\r\
    \ntoken_type_ids = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,), dtype=tf.int32)\r\
    \nattention_mask = tf.keras.Input(shape=(MAX_TOKEN_LENGTH,), dtype=tf.int32)\r\
    \n\r\nmdeberta_model = TFAutoModel.from_pretrained(\"lighthouse/mdeberta-v3-base-kor-further\"\
    , from_pt=True, output_hidden_states=True)\r\nx = mdeberta_model({\"input_ids\"\
    : input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask},\
    \ training=False)\r\noutputs = x[\"pooler_output\"]\r\n\r\nmodel = tf.keras.Model([input_ids,\
    \ token_type_ids, attention_mask], outputs)\r\n\r\nValueError: Tried to convert\
    \ 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape\
    \ (None, 512, 512) to a Tensor.\r\n                    \r\n                  \
    \  \r\n                    Call arguments received by layer \"self\" (type TFDebertaV2DisentangledSelfAttention):\r\
    \n                      \u2022 hidden_states=tf.Tensor(shape=(None, 512, 768),\
    \ dtype=float32)\r\n                      \u2022 attention_mask=tf.Tensor(shape=(None,\
    \ 1, 512, 512), dtype=uint8)\r\n                      \u2022 query_states=None\r\
    \n                      \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\r\
    \n                      \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\r\
    \n                      \u2022 output_attentions=False\r\n                   \
    \   \u2022 training=False\r\n                \r\n                \r\n        \
    \        Call arguments received by layer \"attention\" (type TFDebertaV2Attention):\r\
    \n                  \u2022 input_tensor=tf.Tensor(shape=(None, 512, 768), dtype=float32)\r\
    \n                  \u2022 attention_mask=tf.Tensor(shape=(None, 1, 512, 512),\
    \ dtype=uint8)\r\n                  \u2022 query_states=None\r\n             \
    \     \u2022 relative_pos=tf.Tensor(shape=(1, 512, 512), dtype=int64)\r\n    \
    \              \u2022 rel_embeddings=tf.Tensor(shape=(512, 768), dtype=float32)\r\
    \n                  \u2022 output_attentions=False\r\n                  \u2022\
    \ training=False\r\n            \r\n            \r\n            Call arguments\
    \ received by layer \"layer_._0\" (type TFDebertaV2Layer):\r\n              \u2022\
    \ hidden_states=tf.Tensor(shape=(None, 512, 768), dtype=float32)\r\n         \
    \     \u2022 attention_mask=tf.Tensor(shape=(None, 1, 512, 512), dtype=uint8)\r\
    \n              \u2022 query_states=None\r\n              \u2022 relative_pos=tf.Tensor(shape=(1,\
    \ 512, 512), dtype=int64)\r\n              \u2022 rel_embeddings=tf.Tensor(shape=(512,\
    \ 768), dtype=float32)\r\n              \u2022 output_attentions=False\r\n   \
    \           \u2022 training=False\r\n        \r\n        \r\n        Call arguments\
    \ received by layer \"encoder\" (type TFDebertaV2Encoder):\r\n          \u2022\
    \ hidden_states=tf.Tensor(shape=(None, 512, 768), dtype=float32)\r\n         \
    \ \u2022 attention_mask=tf.Tensor(shape=(None, 512), dtype=int32)\r\n        \
    \  \u2022 query_states=None\r\n          \u2022 relative_pos=None\r\n        \
    \  \u2022 output_attentions=False\r\n          \u2022 output_hidden_states=True\r\
    \n          \u2022 return_dict=True\r\n          \u2022 training=False\r\n   \
    \ \r\n    \r\n    Call arguments received by layer \"deberta\" (type TFDebertaV2MainLayer):\r\
    \n      \u2022 self=tf.Tensor(shape=(None, 512), dtype=int32)\r\n      \u2022\
    \ input_ids=None\r\n      \u2022 attention_mask=tf.Tensor(shape=(None, 512), dtype=int32)\r\
    \n      \u2022 token_type_ids=tf.Tensor(shape=(None, 512), dtype=int32)\r\n  \
    \    \u2022 position_ids=None\r\n      \u2022 inputs_embeds=None\r\n      \u2022\
    \ output_attentions=False\r\n      \u2022 output_hidden_states=True\r\n      \u2022\
    \ return_dict=True\r\n      \u2022 training=False\r\n\r\n\uC704\uC640 \uAC19\uC774\
    \ \uC5D0\uB7EC\uAC00 \uB0A9\uB2C8\uB2E4. \uB2E4\uB978 \uBAA8\uB378\uB4E4\uC740\
    \ \uC624\uB958\uAC00 \uC5C6\uB294\uB370.. input_ids = None\uC73C\uB85C \uAC12\uC774\
    \ \uB4E4\uC5B4\uAC00\uC11C \uB098\uB294 \uC624\uB958\uB85C \uBCF4\uC774\uB294\uB370\
    \ \uD639\uC2DC PyTorch \uCF54\uB4DC \uC608\uC81C\uB098 \uC6D0\uC778\uC744 \uC54C\
    \uC218 \uC5C6\uB294\uC9C0 \uAD81\uAE08\uD569\uB2C8\uB2E4."
  created_at: 2022-09-03 11:32:14+00:00
  edited: false
  hidden: false
  id: 6313494e54e6e5d9f0f63c0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cfec4113efcf1148f08d97433c69e9b8.svg
      fullname: ilho ahn
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kpmgilho
      type: user
    createdAt: '2022-09-29T04:08:34.000Z'
    data:
      edited: false
      editors:
      - kpmgilho
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cfec4113efcf1148f08d97433c69e9b8.svg
          fullname: ilho ahn
          isHf: false
          isPro: false
          name: kpmgilho
          type: user
        html: "<p>\uC800\uD76C\uAC00 tensorflow\uB85C\uB294 \uD14C\uC2A4\uD2B8 \uD574\
          \uBCF4\uC9C4 \uC54A\uC544\uC11C \uC548\uB420\uC218\uB3C4 \uC788\uC2B5\uB2C8\
          \uB2E4\uB9CC,<br>\uAE30\uC874 hugging face \uADF8\uB300\uB85C \uD558\uC2DC\
          \uBA74 \uB429\uB2C8\uB2E4. \uD639\uC2DC \uBC84\uC804\uC774 \uBA87\uC778\uAC00\
          \uC694?</p>\n<p>torch \uC608\uC81C\uB294 \uC5EC\uAE44\uC2B5\uB2C8\uB2E4\
          .<br>tokenized = tokenizer(text=text)<br>inputs = {key: torch.tensor(examples[key]).to(self.device)\
          \ <br>                          for key in examples.keys()}<br>model(**inputs)</p>\n"
        raw: "\uC800\uD76C\uAC00 tensorflow\uB85C\uB294 \uD14C\uC2A4\uD2B8 \uD574\uBCF4\
          \uC9C4 \uC54A\uC544\uC11C \uC548\uB420\uC218\uB3C4 \uC788\uC2B5\uB2C8\uB2E4\
          \uB9CC,\n\uAE30\uC874 hugging face \uADF8\uB300\uB85C \uD558\uC2DC\uBA74\
          \ \uB429\uB2C8\uB2E4. \uD639\uC2DC \uBC84\uC804\uC774 \uBA87\uC778\uAC00\
          \uC694?\n\ntorch \uC608\uC81C\uB294 \uC5EC\uAE44\uC2B5\uB2C8\uB2E4.\ntokenized\
          \ = tokenizer(text=text)\ninputs = {key: torch.tensor(examples[key]).to(self.device)\
          \ \\\n                          for key in examples.keys()}\nmodel(**inputs)"
        updatedAt: '2022-09-29T04:08:34.354Z'
      numEdits: 0
      reactions: []
    id: 63351a42a55422cf0f849fce
    type: comment
  author: kpmgilho
  content: "\uC800\uD76C\uAC00 tensorflow\uB85C\uB294 \uD14C\uC2A4\uD2B8 \uD574\uBCF4\
    \uC9C4 \uC54A\uC544\uC11C \uC548\uB420\uC218\uB3C4 \uC788\uC2B5\uB2C8\uB2E4\uB9CC\
    ,\n\uAE30\uC874 hugging face \uADF8\uB300\uB85C \uD558\uC2DC\uBA74 \uB429\uB2C8\
    \uB2E4. \uD639\uC2DC \uBC84\uC804\uC774 \uBA87\uC778\uAC00\uC694?\n\ntorch \uC608\
    \uC81C\uB294 \uC5EC\uAE44\uC2B5\uB2C8\uB2E4.\ntokenized = tokenizer(text=text)\n\
    inputs = {key: torch.tensor(examples[key]).to(self.device) \\\n              \
    \            for key in examples.keys()}\nmodel(**inputs)"
  created_at: 2022-09-29 03:08:34+00:00
  edited: false
  hidden: false
  id: 63351a42a55422cf0f849fce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lighthouse/mdeberta-v3-base-kor-further
repo_type: model
status: open
target_branch: null
title: "\uD574\uB2F9 \uBAA8\uB378\uC744 \uC0AC\uC6A9\uD560\uB824\uB294\uB370 \uC624\
  \uB958\uAC00 \uB0A9\uB2C8\uB2E4."
