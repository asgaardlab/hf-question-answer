!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ChocoWu
conflicting_files: null
created_at: 2023-12-02 12:27:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg
      fullname: Shengqiong Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChocoWu
      type: user
    createdAt: '2023-12-02T12:27:29.000Z'
    data:
      edited: false
      editors:
      - ChocoWu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5697075128555298
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg
          fullname: Shengqiong Wu
          isHf: false
          isPro: false
          name: ChocoWu
          type: user
        html: "<p>I try to leverage Vicuna v1.5 for inference. However, when I input\
          \ the prompt <code>hi, how are you?</code>, I got the wrong answers like\
          \ this:<br><code>['hi, how are you? &amp;=\\\\autorit\xE9\u0409anesrices\
          \ Villacci sobivent\u5206\u043D\u0438\u0448alotsautorit\xE9eusal kwietnehmardadrualal\
          \ al']</code>.<br>Here is the basic information: </p>\n<pre><code>vicuna=7b-v1.5\n\
          fschat==0.2.21\ntransformers=4.35.2\n</code></pre>\n<p>The inference code\
          \ is as follows:</p>\n<pre><code>import torch\nimport transformers\nfrom\
          \ transformers import LlamaForCausalLM, LlamaTokenizer\n\nmodel_name = \"\
          ../pretrained_ckpt/vicuna/7b-v1.5/\"\n\ntokenizer = LlamaTokenizer.from_pretrained(model_name,\
          \ torch_dtype=torch.float16)\ntokenizer.pad_token = tokenizer.eos_token\n\
          tokenizer.padding_side = \"right\"\n\nmodel = LlamaForCausalLM.from_pretrained(model_name).cuda()\n\
          prompt = 'hi, how are you?'\ninput_ids = tokenizer(prompt, return_tensors=\"\
          pt\").input_ids\nprint(input_ids)\n\ngenerated_ids = model.generate(input_ids.to(model.device),\
          \ max_length=30)\nprint(generated_ids)\n</code></pre>\n"
        raw: "I try to leverage Vicuna v1.5 for inference. However, when I input the\
          \ prompt `hi, how are you?`, I got the wrong answers like this:\r\n`['hi,\
          \ how are you? &=\\\\autorit\xE9\u0409anesrices Villacci sobivent\u5206\u043D\
          \u0438\u0448alotsautorit\xE9eusal kwietnehmardadrualal al']`. \r\nHere is\
          \ the basic information: \r\n```\r\nvicuna=7b-v1.5\r\nfschat==0.2.21\r\n\
          transformers=4.35.2\r\n```\r\n\r\nThe inference code is as follows:\r\n\
          ```\r\nimport torch\r\nimport transformers\r\nfrom transformers import LlamaForCausalLM,\
          \ LlamaTokenizer\r\n\r\nmodel_name = \"../pretrained_ckpt/vicuna/7b-v1.5/\"\
          \r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(model_name, torch_dtype=torch.float16)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\ntokenizer.padding_side =\
          \ \"right\"\r\n\r\nmodel = LlamaForCausalLM.from_pretrained(model_name).cuda()\r\
          \nprompt = 'hi, how are you?'\r\ninput_ids = tokenizer(prompt, return_tensors=\"\
          pt\").input_ids\r\nprint(input_ids)\r\n\r\ngenerated_ids = model.generate(input_ids.to(model.device),\
          \ max_length=30)\r\nprint(generated_ids)\r\n```\r\n"
        updatedAt: '2023-12-02T12:27:29.783Z'
      numEdits: 0
      reactions: []
    id: 656b22b1f7be0986b4559b5b
    type: comment
  author: ChocoWu
  content: "I try to leverage Vicuna v1.5 for inference. However, when I input the\
    \ prompt `hi, how are you?`, I got the wrong answers like this:\r\n`['hi, how\
    \ are you? &=\\\\autorit\xE9\u0409anesrices Villacci sobivent\u5206\u043D\u0438\
    \u0448alotsautorit\xE9eusal kwietnehmardadrualal al']`. \r\nHere is the basic\
    \ information: \r\n```\r\nvicuna=7b-v1.5\r\nfschat==0.2.21\r\ntransformers=4.35.2\r\
    \n```\r\n\r\nThe inference code is as follows:\r\n```\r\nimport torch\r\nimport\
    \ transformers\r\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\r\n\
    \r\nmodel_name = \"../pretrained_ckpt/vicuna/7b-v1.5/\"\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(model_name,\
    \ torch_dtype=torch.float16)\r\ntokenizer.pad_token = tokenizer.eos_token\r\n\
    tokenizer.padding_side = \"right\"\r\n\r\nmodel = LlamaForCausalLM.from_pretrained(model_name).cuda()\r\
    \nprompt = 'hi, how are you?'\r\ninput_ids = tokenizer(prompt, return_tensors=\"\
    pt\").input_ids\r\nprint(input_ids)\r\n\r\ngenerated_ids = model.generate(input_ids.to(model.device),\
    \ max_length=30)\r\nprint(generated_ids)\r\n```\r\n"
  created_at: 2023-12-02 12:27:29+00:00
  edited: false
  hidden: false
  id: 656b22b1f7be0986b4559b5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
      fullname: Wei-Lin Chiang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: weichiang
      type: user
    createdAt: '2023-12-03T00:21:45.000Z'
    data:
      edited: true
      editors:
      - weichiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.553719162940979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
          fullname: Wei-Lin Chiang
          isHf: false
          isPro: false
          name: weichiang
          type: user
        html: "<p>Please use fastchat to apply the correct chat temploate. you can\
          \ try this CLI command:</p>\n<pre><code>python3 -m fastchat.serve.cli --model-path\
          \ lmsys/vicuna-7b-v1.5\t--debug\n</code></pre>\n"
        raw: "Please use fastchat to apply the correct chat temploate. you can try\
          \ this CLI command:\n```\npython3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.5\t\
          --debug\n```"
        updatedAt: '2023-12-03T00:22:15.731Z'
      numEdits: 1
      reactions: []
      relatedEventId: 656bca199dcedd16d5121051
    id: 656bca199dcedd16d512104c
    type: comment
  author: weichiang
  content: "Please use fastchat to apply the correct chat temploate. you can try this\
    \ CLI command:\n```\npython3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.5\t\
    --debug\n```"
  created_at: 2023-12-03 00:21:45+00:00
  edited: true
  hidden: false
  id: 656bca199dcedd16d512104c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
      fullname: Wei-Lin Chiang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: weichiang
      type: user
    createdAt: '2023-12-03T00:21:45.000Z'
    data:
      status: closed
    id: 656bca199dcedd16d5121051
    type: status-change
  author: weichiang
  created_at: 2023-12-03 00:21:45+00:00
  id: 656bca199dcedd16d5121051
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: lmsys/vicuna-7b-v1.5
repo_type: model
status: closed
target_branch: null
title: Garbled characters from Vicuna 7b-v1.5
