!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LeMoussel
conflicting_files: null
created_at: 2023-11-05 07:33:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2023-11-05T08:33:03.000Z'
    data:
      edited: false
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5451349020004272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: "<p>I ran the code:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ LlamaConfig, LlamaForCausalLM\n\n<span class=\"hljs-comment\"># YaRN:\
          \ Efficient Context Window Extension of Large Language Models: https://github.com/jquesnelle/yarn</span>\n\
          <span class=\"hljs-comment\"># https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k</span>\n\
          model_path = <span class=\"hljs-string\">\"NousResearch/Yarn-Llama-2-7b-64k\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nconfig\
          \ = LlamaConfig.from_pretrained(model_path, trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\nmodel = LlamaForCausalLM.from_pretrained(model_path,\
          \ torch_dtype=torch.bfloat16, device_map=<span class=\"hljs-string\">\"\
          auto\"</span>, config=config, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n<p>Then, I got the error:</p>\n<pre><code>The\
          \ argument `trust_remote_code` is to be used with Auto classes. It has no\
          \ effect here and is ignored.\n\n---------------------------------------------------------------------------\n\
          \nValueError                                Traceback (most recent call\
          \ last)\n\n&lt;ipython-input-4-cb4c3c69fc36&gt; in &lt;cell line: 9&gt;()\n\
          \      7 \n      8 tokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          ----&gt; 9 config = LlamaConfig.from_pretrained(model_path, trust_remote_code=True)\n\
          \     10 model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", config=config, trust_remote_code=True)\n\n3 frames\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/models/llama/configuration_llama.py\
          \ in _rope_scaling_validation(self)\n    174 \n    175         if not isinstance(self.rope_scaling,\
          \ dict) or len(self.rope_scaling) != 2:\n--&gt; 176             raise ValueError(\n\
          \    177                 \"`rope_scaling` must be a dictionary with with\
          \ two fields, `type` and `factor`, \"\n    178                 f\"got {self.rope_scaling}\"\
          \n\nValueError: `rope_scaling` must be a dictionary with with two fields,\
          \ `type` and `factor`, got {'factor': 16.0, 'original_max_position_embeddings':\
          \ 4096, 'type': 'yarn', 'finetuned': True}\n</code></pre>\n<p>Thanks in\
          \ advance for your help.</p>\n"
        raw: "I ran the code:\r\n```python\r\nimport torch\r\nfrom transformers import\
          \ AutoTokenizer, LlamaConfig, LlamaForCausalLM\r\n\r\n# YaRN: Efficient\
          \ Context Window Extension of Large Language Models: https://github.com/jquesnelle/yarn\r\
          \n# https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k\r\nmodel_path\
          \ = \"NousResearch/Yarn-Llama-2-7b-64k\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\
          \nconfig = LlamaConfig.from_pretrained(model_path, trust_remote_code=True)\r\
          \nmodel = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", config=config, trust_remote_code=True)\r\n```\r\n\
          Then, I got the error:\r\n```\r\nThe argument `trust_remote_code` is to\
          \ be used with Auto classes. It has no effect here and is ignored.\r\n\r\
          \n---------------------------------------------------------------------------\r\
          \n\r\nValueError                                Traceback (most recent call\
          \ last)\r\n\r\n<ipython-input-4-cb4c3c69fc36> in <cell line: 9>()\r\n  \
          \    7 \r\n      8 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\
          \n----> 9 config = LlamaConfig.from_pretrained(model_path, trust_remote_code=True)\r\
          \n     10 model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", config=config, trust_remote_code=True)\r\n\r\n3 frames\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/llama/configuration_llama.py\
          \ in _rope_scaling_validation(self)\r\n    174 \r\n    175         if not\
          \ isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\r\n\
          --> 176             raise ValueError(\r\n    177                 \"`rope_scaling`\
          \ must be a dictionary with with two fields, `type` and `factor`, \"\r\n\
          \    178                 f\"got {self.rope_scaling}\"\r\n\r\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 16.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\r\n```\r\n\r\nThanks in advance for your help."
        updatedAt: '2023-11-05T08:33:03.538Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - HardLuck
    id: 6547533f7e0f9132182ef463
    type: comment
  author: LeMoussel
  content: "I ran the code:\r\n```python\r\nimport torch\r\nfrom transformers import\
    \ AutoTokenizer, LlamaConfig, LlamaForCausalLM\r\n\r\n# YaRN: Efficient Context\
    \ Window Extension of Large Language Models: https://github.com/jquesnelle/yarn\r\
    \n# https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k\r\nmodel_path = \"\
    NousResearch/Yarn-Llama-2-7b-64k\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\
    \nconfig = LlamaConfig.from_pretrained(model_path, trust_remote_code=True)\r\n\
    model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\", config=config, trust_remote_code=True)\r\n```\r\nThen,\
    \ I got the error:\r\n```\r\nThe argument `trust_remote_code` is to be used with\
    \ Auto classes. It has no effect here and is ignored.\r\n\r\n---------------------------------------------------------------------------\r\
    \n\r\nValueError                                Traceback (most recent call last)\r\
    \n\r\n<ipython-input-4-cb4c3c69fc36> in <cell line: 9>()\r\n      7 \r\n     \
    \ 8 tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n----> 9 config =\
    \ LlamaConfig.from_pretrained(model_path, trust_remote_code=True)\r\n     10 model\
    \ = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"\
    auto\", config=config, trust_remote_code=True)\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/llama/configuration_llama.py\
    \ in _rope_scaling_validation(self)\r\n    174 \r\n    175         if not isinstance(self.rope_scaling,\
    \ dict) or len(self.rope_scaling) != 2:\r\n--> 176             raise ValueError(\r\
    \n    177                 \"`rope_scaling` must be a dictionary with with two\
    \ fields, `type` and `factor`, \"\r\n    178                 f\"got {self.rope_scaling}\"\
    \r\n\r\nValueError: `rope_scaling` must be a dictionary with with two fields,\
    \ `type` and `factor`, got {'factor': 16.0, 'original_max_position_embeddings':\
    \ 4096, 'type': 'yarn', 'finetuned': True}\r\n```\r\n\r\nThanks in advance for\
    \ your help."
  created_at: 2023-11-05 07:33:03+00:00
  edited: false
  hidden: false
  id: 6547533f7e0f9132182ef463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2023-11-05T08:37:50.000Z'
    data:
      from: 'Error: `rope_scaling`must be a dictionary with with two fields'
      to: 'Error: `rope_scaling`must be a dictionary with two fields'
    id: 6547545e65079683747c07ac
    type: title-change
  author: LeMoussel
  created_at: 2023-11-05 07:37:50+00:00
  id: 6547545e65079683747c07ac
  new_title: 'Error: `rope_scaling`must be a dictionary with two fields'
  old_title: 'Error: `rope_scaling`must be a dictionary with with two fields'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3330f40bf5649d7eeeb223b25e05a05b.svg
      fullname: jason schwartz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasonjschwartz
      type: user
    createdAt: '2024-01-09T15:12:08.000Z'
    data:
      edited: false
      editors:
      - jasonjschwartz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540833234786987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3330f40bf5649d7eeeb223b25e05a05b.svg
          fullname: jason schwartz
          isHf: false
          isPro: false
          name: jasonjschwartz
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;LeMoussel&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LeMoussel\"\
          >@<span class=\"underline\">LeMoussel</span></a></span>\n\n\t</span></span>,\
          \ did you ever get the solution to this error?</p>\n"
        raw: Hey @LeMoussel, did you ever get the solution to this error?
        updatedAt: '2024-01-09T15:12:08.274Z'
      numEdits: 0
      reactions: []
    id: 659d624850469a2326792b2e
    type: comment
  author: jasonjschwartz
  content: Hey @LeMoussel, did you ever get the solution to this error?
  created_at: 2024-01-09 15:12:08+00:00
  edited: false
  hidden: false
  id: 659d624850469a2326792b2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-09T15:20:27.000Z'
    data:
      edited: false
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8631312251091003
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>Nope.I dropped</p>

          '
        raw: Nope.I dropped
        updatedAt: '2024-01-09T15:20:27.908Z'
      numEdits: 0
      reactions: []
    id: 659d643ba333ccfbacd58a0e
    type: comment
  author: LeMoussel
  content: Nope.I dropped
  created_at: 2024-01-09 15:20:27+00:00
  edited: false
  hidden: false
  id: 659d643ba333ccfbacd58a0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35b78e198d959cbe4e0119c415bdea6d.svg
      fullname: Muhammad Osama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mosama
      type: user
    createdAt: '2024-01-09T22:48:17.000Z'
    data:
      edited: false
      editors:
      - mosama
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9113237261772156
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35b78e198d959cbe4e0119c415bdea6d.svg
          fullname: Muhammad Osama
          isHf: false
          isPro: false
          name: mosama
          type: user
        html: '<p>You are using the transformers to load the model, which using the
          default modelling_llama script. There is a script in the files provided.
          Try running the model with that.</p>

          '
        raw: You are using the transformers to load the model, which using the default
          modelling_llama script. There is a script in the files provided. Try running
          the model with that.
        updatedAt: '2024-01-09T22:48:17.027Z'
      numEdits: 0
      reactions: []
    id: 659dcd313752e021f622e34c
    type: comment
  author: mosama
  content: You are using the transformers to load the model, which using the default
    modelling_llama script. There is a script in the files provided. Try running the
    model with that.
  created_at: 2024-01-09 22:48:17+00:00
  edited: false
  hidden: false
  id: 659dcd313752e021f622e34c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-10T06:47:39.000Z'
    data:
      edited: false
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9385935068130493
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>What script are you talking about? Is it <code>configuration_llama.py</code>?<br>Do
          you have an example of how to use this script to running the model with
          that?</p>

          '
        raw: 'What script are you talking about? Is it `configuration_llama.py`?

          Do you have an example of how to use this script to running the model with
          that?'
        updatedAt: '2024-01-10T06:47:39.656Z'
      numEdits: 0
      reactions: []
    id: 659e3d8b371a74fca3b00e60
    type: comment
  author: LeMoussel
  content: 'What script are you talking about? Is it `configuration_llama.py`?

    Do you have an example of how to use this script to running the model with that?'
  created_at: 2024-01-10 06:47:39+00:00
  edited: false
  hidden: false
  id: 659e3d8b371a74fca3b00e60
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Yarn-Llama-2-7b-64k
repo_type: model
status: open
target_branch: null
title: 'Error: `rope_scaling`must be a dictionary with two fields'
