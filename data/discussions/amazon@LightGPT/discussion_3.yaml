!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spaceman7777
conflicting_files: null
created_at: 2023-06-01 20:29:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
      fullname: Space Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spaceman7777
      type: user
    createdAt: '2023-06-01T21:29:52.000Z'
    data:
      edited: false
      editors:
      - spaceman7777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
          fullname: Space Man
          isHf: false
          isPro: false
          name: spaceman7777
          type: user
        html: '<p>Noticed that the comparison of the eval scores of this model and
          the original GPT-J show LightGPT scoring lower on almost all of the posted
          tests.</p>

          <p>What is the reasoning behind this model''s release? Is there some evaluation
          that shows this model has improved at completing some function?</p>

          <p>Or is it a statement that instruct tuned models should score lower than
          their counterparts?</p>

          <p>Please explain, I''m curious.</p>

          <p>Attaching a screenshot of the scores posted in the model card<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/63cd6028374057a338e6bed6/JL2hQbXKCJqHhixiuYEp7.png"><img
          alt="Selection_1011.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cd6028374057a338e6bed6/JL2hQbXKCJqHhixiuYEp7.png"></a></p>

          '
        raw: "Noticed that the comparison of the eval scores of this model and the\
          \ original GPT-J show LightGPT scoring lower on almost all of the posted\
          \ tests.\r\n\r\nWhat is the reasoning behind this model's release? Is there\
          \ some evaluation that shows this model has improved at completing some\
          \ function?\r\n\r\nOr is it a statement that instruct tuned models should\
          \ score lower than their counterparts?\r\n\r\nPlease explain, I'm curious.\r\
          \n\r\nAttaching a screenshot of the scores posted in the model card\r\n\
          ![Selection_1011.png](https://cdn-uploads.huggingface.co/production/uploads/63cd6028374057a338e6bed6/JL2hQbXKCJqHhixiuYEp7.png)\r\
          \n"
        updatedAt: '2023-06-01T21:29:52.400Z'
      numEdits: 0
      reactions: []
    id: 64790dd03b7f8b1f624b0fd8
    type: comment
  author: spaceman7777
  content: "Noticed that the comparison of the eval scores of this model and the original\
    \ GPT-J show LightGPT scoring lower on almost all of the posted tests.\r\n\r\n\
    What is the reasoning behind this model's release? Is there some evaluation that\
    \ shows this model has improved at completing some function?\r\n\r\nOr is it a\
    \ statement that instruct tuned models should score lower than their counterparts?\r\
    \n\r\nPlease explain, I'm curious.\r\n\r\nAttaching a screenshot of the scores\
    \ posted in the model card\r\n![Selection_1011.png](https://cdn-uploads.huggingface.co/production/uploads/63cd6028374057a338e6bed6/JL2hQbXKCJqHhixiuYEp7.png)\r\
    \n"
  created_at: 2023-06-01 20:29:52+00:00
  edited: false
  hidden: false
  id: 64790dd03b7f8b1f624b0fd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f258ee558ab7383e0a38c2d7a653b062.svg
      fullname: Chen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: chenwuml
      type: user
    createdAt: '2023-06-02T03:09:48.000Z'
    data:
      edited: true
      editors:
      - chenwuml
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f258ee558ab7383e0a38c2d7a653b062.svg
          fullname: Chen Wu
          isHf: false
          isPro: false
          name: chenwuml
          type: user
        html: "<p>Thank you for your useful comment and questions!</p>\n<p>The original\
          \ GPT-J is a great pre-trained model. But it could be further fine-tuned\
          \ for practical use cases. Particularly, if you are looking for a version\
          \ better suited to taking generic instructions in a question and answering\
          \ format, we recommend take a look at LightGPT.</p>\n<p>For example, for\
          \ the same question on <em>\"How can I tell if a pomegranate is ripe?\"\
          </em> as shown in the model card, here are two sampled answers generated\
          \ from the original GPT-J:</p>\n<pre><code>----\nA: Pomes are fragile. Once\
          \ you open one, all bets are off on whether it will ripen further. Just\
          \ squeeze and see how firm the flesh feels. If it's not quite ready to eat,\
          \ then don't worry.\nQ: How do I pick the right amount of zest for a recipe?\n\
          A: Start with a wedge of citrus and taste it to make sure it doesn't need\
          \ more or less than that.&nbsp;\n----\n----\nA: Pomegranates are in season\
          \ from early fall to mid-winter. They are available year round at farmers\
          \ markets, some grocery stores and even Costco. Look for the bright red,\
          \ tight husks of the fruit.\nQ: How should you store fresh pomegranates?\n\
          ----\n</code></pre>\n<p>Since a pre-trained language model is trained to\
          \ predict next tokens, it aims to generate tokens consistent with the empirical\
          \ distributions of tokens learned over the entire training set during pre-training,\
          \ and hence its responses are not necessarily in line with the questions\
          \ asked</p>\n<p>Take another example, when asked <em>\"Where is the closest\
          \ hospital to my location?\u201D</em></p>\n<pre><code>Original GPT-J:\n\
          The nearest hospitals are located in Richmond, Virginia.\nIs there an Emergency\
          \ Room near me? Yes, we're always open for emergencies 24 hours a day, 7\
          \ days a week. If you call 911 or go to an emergency room, you'll be sent\
          \ here (or to one of our three other nearby facilities) where our ER has\
          \ been rated \"excellent\" by the American College of Surgeons.\n\nLightGPT:\n\
          The best way to find out where the nearest hospital is located is by searching\
          \ online for hospitals in your area. You can also ask friends and family\
          \ who live nearby, or look up local listings on websites like Google Maps.\n\
          </code></pre>\n<p>As you can see, there seems to be some hallucination from\
          \ the original GPT-J. LightGPT in general tries to be more helpful and conservative.</p>\n\
          <p>One of the purposes of reporting those scores is to ensure that LightGPT\
          \ has not been overfit too much during fine-tuning and has not lost LM capabilities\
          \ as measured by those core metrics. Also the benchmark that measures those\
          \ metrics does not follow LightGPT prompt template format, potentially resulting\
          \ in poorer metrics.</p>\n<p>But we are working on some application/task-specific\
          \ evaluation and benchmarks.</p>\n"
        raw: "Thank you for your useful comment and questions!\n\nThe original GPT-J\
          \ is a great pre-trained model. But it could be further fine-tuned for practical\
          \ use cases. Particularly, if you are looking for a version better suited\
          \ to taking generic instructions in a question and answering format, we\
          \ recommend take a look at LightGPT.\n\nFor example, for the same question\
          \ on *\"How can I tell if a pomegranate is ripe?\"* as shown in the model\
          \ card, here are two sampled answers generated from the original GPT-J:\n\
          ```\n----\nA: Pomes are fragile. Once you open one, all bets are off on\
          \ whether it will ripen further. Just squeeze and see how firm the flesh\
          \ feels. If it's not quite ready to eat, then don't worry.\nQ: How do I\
          \ pick the right amount of zest for a recipe?\nA: Start with a wedge of\
          \ citrus and taste it to make sure it doesn't need more or less than that.\_\
          \n----\n----\nA: Pomegranates are in season from early fall to mid-winter.\
          \ They are available year round at farmers markets, some grocery stores\
          \ and even Costco. Look for the bright red, tight husks of the fruit.\n\
          Q: How should you store fresh pomegranates?\n----\n```\nSince a pre-trained\
          \ language model is trained to predict next tokens, it aims to generate\
          \ tokens consistent with the empirical distributions of tokens learned over\
          \ the entire training set during pre-training, and hence its responses are\
          \ not necessarily in line with the questions asked\n\nTake another example,\
          \ when asked *\"Where is the closest hospital to my location?\u201D*\n```\n\
          Original GPT-J:\nThe nearest hospitals are located in Richmond, Virginia.\n\
          Is there an Emergency Room near me? Yes, we're always open for emergencies\
          \ 24 hours a day, 7 days a week. If you call 911 or go to an emergency room,\
          \ you'll be sent here (or to one of our three other nearby facilities) where\
          \ our ER has been rated \"excellent\" by the American College of Surgeons.\n\
          \nLightGPT:\nThe best way to find out where the nearest hospital is located\
          \ is by searching online for hospitals in your area. You can also ask friends\
          \ and family who live nearby, or look up local listings on websites like\
          \ Google Maps.\n```\nAs you can see, there seems to be some hallucination\
          \ from the original GPT-J. LightGPT in general tries to be more helpful\
          \ and conservative.\n\nOne of the purposes of reporting those scores is\
          \ to ensure that LightGPT has not been overfit too much during fine-tuning\
          \ and has not lost LM capabilities as measured by those core metrics. Also\
          \ the benchmark that measures those metrics does not follow LightGPT prompt\
          \ template format, potentially resulting in poorer metrics.\n\nBut we are\
          \ working on some application/task-specific evaluation and benchmarks."
        updatedAt: '2023-06-02T04:55:08.370Z'
      numEdits: 3
      reactions: []
    id: 64795d7cc68a021fbbab9795
    type: comment
  author: chenwuml
  content: "Thank you for your useful comment and questions!\n\nThe original GPT-J\
    \ is a great pre-trained model. But it could be further fine-tuned for practical\
    \ use cases. Particularly, if you are looking for a version better suited to taking\
    \ generic instructions in a question and answering format, we recommend take a\
    \ look at LightGPT.\n\nFor example, for the same question on *\"How can I tell\
    \ if a pomegranate is ripe?\"* as shown in the model card, here are two sampled\
    \ answers generated from the original GPT-J:\n```\n----\nA: Pomes are fragile.\
    \ Once you open one, all bets are off on whether it will ripen further. Just squeeze\
    \ and see how firm the flesh feels. If it's not quite ready to eat, then don't\
    \ worry.\nQ: How do I pick the right amount of zest for a recipe?\nA: Start with\
    \ a wedge of citrus and taste it to make sure it doesn't need more or less than\
    \ that.\_\n----\n----\nA: Pomegranates are in season from early fall to mid-winter.\
    \ They are available year round at farmers markets, some grocery stores and even\
    \ Costco. Look for the bright red, tight husks of the fruit.\nQ: How should you\
    \ store fresh pomegranates?\n----\n```\nSince a pre-trained language model is\
    \ trained to predict next tokens, it aims to generate tokens consistent with the\
    \ empirical distributions of tokens learned over the entire training set during\
    \ pre-training, and hence its responses are not necessarily in line with the questions\
    \ asked\n\nTake another example, when asked *\"Where is the closest hospital to\
    \ my location?\u201D*\n```\nOriginal GPT-J:\nThe nearest hospitals are located\
    \ in Richmond, Virginia.\nIs there an Emergency Room near me? Yes, we're always\
    \ open for emergencies 24 hours a day, 7 days a week. If you call 911 or go to\
    \ an emergency room, you'll be sent here (or to one of our three other nearby\
    \ facilities) where our ER has been rated \"excellent\" by the American College\
    \ of Surgeons.\n\nLightGPT:\nThe best way to find out where the nearest hospital\
    \ is located is by searching online for hospitals in your area. You can also ask\
    \ friends and family who live nearby, or look up local listings on websites like\
    \ Google Maps.\n```\nAs you can see, there seems to be some hallucination from\
    \ the original GPT-J. LightGPT in general tries to be more helpful and conservative.\n\
    \nOne of the purposes of reporting those scores is to ensure that LightGPT has\
    \ not been overfit too much during fine-tuning and has not lost LM capabilities\
    \ as measured by those core metrics. Also the benchmark that measures those metrics\
    \ does not follow LightGPT prompt template format, potentially resulting in poorer\
    \ metrics.\n\nBut we are working on some application/task-specific evaluation\
    \ and benchmarks."
  created_at: 2023-06-02 02:09:48+00:00
  edited: true
  hidden: false
  id: 64795d7cc68a021fbbab9795
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2b8bbb77f84566321800/vBdIs222m14F2HGYWkd9T.jpeg?w=200&h=200&f=face
      fullname: Don Draper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dondraper
      type: user
    createdAt: '2023-06-02T16:48:06.000Z'
    data:
      edited: false
      editors:
      - dondraper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2b8bbb77f84566321800/vBdIs222m14F2HGYWkd9T.jpeg?w=200&h=200&f=face
          fullname: Don Draper
          isHf: false
          isPro: false
          name: dondraper
          type: user
        html: '<p>Interesting, so are you working on your own pre-trained model that
          could compete with falcon and llama? And if so, will it be a community release?
          If you want to really make waves beat falcon-40B with 25B parameters or
          less and bring the title back state-side.</p>

          '
        raw: Interesting, so are you working on your own pre-trained model that could
          compete with falcon and llama? And if so, will it be a community release?
          If you want to really make waves beat falcon-40B with 25B parameters or
          less and bring the title back state-side.
        updatedAt: '2023-06-02T16:48:06.846Z'
      numEdits: 0
      reactions: []
    id: 647a1d46f518a860fbce7349
    type: comment
  author: dondraper
  content: Interesting, so are you working on your own pre-trained model that could
    compete with falcon and llama? And if so, will it be a community release? If you
    want to really make waves beat falcon-40B with 25B parameters or less and bring
    the title back state-side.
  created_at: 2023-06-02 15:48:06+00:00
  edited: false
  hidden: false
  id: 647a1d46f518a860fbce7349
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
      fullname: Space Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spaceman7777
      type: user
    createdAt: '2023-06-02T17:46:34.000Z'
    data:
      edited: false
      editors:
      - spaceman7777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
          fullname: Space Man
          isHf: false
          isPro: false
          name: spaceman7777
          type: user
        html: "<p>mm, I understand that GPT-J is pretrained, but I'm curious as to\
          \ how the model's performance was judged, if not by the listed eval scores.</p>\n\
          <p>I understand that instruct tuning the model was needed, but I don't understand\
          \ how the quality of the instruct tuning was judged. Is there another eval\
          \ suite or method by which the model was scored?<br><span data-props=\"\
          {&quot;user&quot;:&quot;chenwuml&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/chenwuml\">@<span class=\"underline\">chenwuml</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: 'mm, I understand that GPT-J is pretrained, but I''m curious as to how
          the model''s performance was judged, if not by the listed eval scores.


          I understand that instruct tuning the model was needed, but I don''t understand
          how the quality of the instruct tuning was judged. Is there another eval
          suite or method by which the model was scored?

          @chenwuml'
        updatedAt: '2023-06-02T17:46:34.046Z'
      numEdits: 0
      reactions: []
    id: 647a2afa3a17d5e00ad6ce53
    type: comment
  author: spaceman7777
  content: 'mm, I understand that GPT-J is pretrained, but I''m curious as to how
    the model''s performance was judged, if not by the listed eval scores.


    I understand that instruct tuning the model was needed, but I don''t understand
    how the quality of the instruct tuning was judged. Is there another eval suite
    or method by which the model was scored?

    @chenwuml'
  created_at: 2023-06-02 16:46:34+00:00
  edited: false
  hidden: false
  id: 647a2afa3a17d5e00ad6ce53
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: amazon/LightGPT
repo_type: model
status: open
target_branch: null
title: Posted eval results appear lower, what is the justification for this model's
  utility?
