!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AlketaR
conflicting_files: null
created_at: 2023-09-18 10:26:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/366bd23f3569170272af567350c42720.svg
      fullname: Alketa R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlketaR
      type: user
    createdAt: '2023-09-18T11:26:25.000Z'
    data:
      edited: true
      editors:
      - AlketaR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8362885117530823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/366bd23f3569170272af567350c42720.svg
          fullname: Alketa R
          isHf: false
          isPro: false
          name: AlketaR
          type: user
        html: '<p>Hi, I am trying to do inference from my fined-tuned model which
          is uploaded on my repo in hf:</p>

          <p>"from peft import PeftModel, PeftConfig<br>from transformers import AutoModelForCausalLM</p>

          <p>config = PeftConfig.from_pretrained(/ )<br>model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")<br>model
          = PeftModel.from_pretrained(model, / )""</p>

          <p>I guess the lines above combine the weights of the pretrained model with
          the weights created from qlora.<br>The resulted model must be the fine-tuned
          model and now i want to predict by using it but it seems that the resulted
          model does not have the predict function. </p>

          <p>"predictions = model.predict(test_examples)[0]" results in "''LlamaForCausalLM''
          object has no attribute ''predict''".</p>

          <p>What am i missing? Thanks in advance!</p>

          '
        raw: "Hi, I am trying to do inference from my fined-tuned model which is uploaded\
          \ on my repo in hf:\n\n\"from peft import PeftModel, PeftConfig\nfrom transformers\
          \ import AutoModelForCausalLM\n\nconfig = PeftConfig.from_pretrained(<hf_user_name>/<repo_name>\
          \ )\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\"\
          )\nmodel = PeftModel.from_pretrained(model, <hf_user_name>/<repo_name> )\"\
          \"\n\nI guess the lines above combine the weights of the pretrained model\
          \ with the weights created from qlora.\nThe resulted model must be the fine-tuned\
          \ model and now i want to predict by using it but it seems that the resulted\
          \ model does not have the predict function. \n\n\"predictions = model.predict(test_examples)[0]\"\
          \ results in \"'LlamaForCausalLM' object has no attribute 'predict'\".\n\
          \nWhat am i missing? Thanks in advance!\n\n"
        updatedAt: '2023-09-18T14:12:24.003Z'
      numEdits: 1
      reactions: []
    id: 650833e1c53e1a7f17e6cb52
    type: comment
  author: AlketaR
  content: "Hi, I am trying to do inference from my fined-tuned model which is uploaded\
    \ on my repo in hf:\n\n\"from peft import PeftModel, PeftConfig\nfrom transformers\
    \ import AutoModelForCausalLM\n\nconfig = PeftConfig.from_pretrained(<hf_user_name>/<repo_name>\
    \ )\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\"\
    )\nmodel = PeftModel.from_pretrained(model, <hf_user_name>/<repo_name> )\"\"\n\
    \nI guess the lines above combine the weights of the pretrained model with the\
    \ weights created from qlora.\nThe resulted model must be the fine-tuned model\
    \ and now i want to predict by using it but it seems that the resulted model does\
    \ not have the predict function. \n\n\"predictions = model.predict(test_examples)[0]\"\
    \ results in \"'LlamaForCausalLM' object has no attribute 'predict'\".\n\nWhat\
    \ am i missing? Thanks in advance!\n\n"
  created_at: 2023-09-18 10:26:25+00:00
  edited: true
  hidden: false
  id: 650833e1c53e1a7f17e6cb52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645db37dd90782b1a6aa3883/jtZdZzXaPNhm1pyErUs9f.png?w=200&h=200&f=face
      fullname: Arnav Garg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: arnavgrg
      type: user
    createdAt: '2023-09-19T14:47:36.000Z'
    data:
      edited: false
      editors:
      - arnavgrg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7726388573646545
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645db37dd90782b1a6aa3883/jtZdZzXaPNhm1pyErUs9f.png?w=200&h=200&f=face
          fullname: Arnav Garg
          isHf: false
          isPro: false
          name: arnavgrg
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;AlketaR&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AlketaR\">@<span class=\"\
          underline\">AlketaR</span></a></span>\n\n\t</span></span>, thanks for raising\
          \ the issue. </p>\n<p>This is right - the notion of <code>model.predict()</code>\
          \ is actually something we've internally implemented as a part of the LudwigModel\
          \ object in Ludwig. </p>\n<p>To actually do generation, you may need to\
          \ follow this guide: <a href=\"https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\"\
          >https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration</a></p>\n"
        raw: "Hi @AlketaR, thanks for raising the issue. \n\nThis is right - the notion\
          \ of `model.predict()` is actually something we've internally implemented\
          \ as a part of the LudwigModel object in Ludwig. \n\nTo actually do generation,\
          \ you may need to follow this guide: https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration"
        updatedAt: '2023-09-19T14:47:36.398Z'
      numEdits: 0
      reactions: []
    id: 6509b488b0bdfd183f78c90d
    type: comment
  author: arnavgrg
  content: "Hi @AlketaR, thanks for raising the issue. \n\nThis is right - the notion\
    \ of `model.predict()` is actually something we've internally implemented as a\
    \ part of the LudwigModel object in Ludwig. \n\nTo actually do generation, you\
    \ may need to follow this guide: https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration"
  created_at: 2023-09-19 13:47:36+00:00
  edited: false
  hidden: false
  id: 6509b488b0bdfd183f78c90d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645db37dd90782b1a6aa3883/jtZdZzXaPNhm1pyErUs9f.png?w=200&h=200&f=face
      fullname: Arnav Garg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: arnavgrg
      type: user
    createdAt: '2023-10-05T19:54:19.000Z'
    data:
      status: closed
    id: 651f146b98759d6ec3470276
    type: status-change
  author: arnavgrg
  created_at: 2023-10-05 18:54:19+00:00
  id: 651f146b98759d6ec3470276
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: arnavgrg/codealpaca-qlora
repo_type: model
status: closed
target_branch: null
title: fined-tuned model uploaded in hf is not able to predict
