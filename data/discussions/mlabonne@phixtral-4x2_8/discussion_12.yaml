!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vince62s
conflicting_files: null
created_at: 2024-01-15 19:01:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
      fullname: VincentN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vince62s
      type: user
    createdAt: '2024-01-15T19:01:20.000Z'
    data:
      edited: false
      editors:
      - vince62s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9421664476394653
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
          fullname: VincentN
          isHf: false
          isPro: false
          name: vince62s
          type: user
        html: '<p>I tried 3 experts (dropped coder) and 2 num_experts_per_tok<br>AGI_EVAL:
          33.93<br>GPT4ALL: 70.50<br>TruthfullQA: 48.78<br>BigBench: 37.84<br>Average:
          47.76</p>

          <p>So the fact that we can''t really improve beyond the smallest phixtral-2x2_8
          proves that this is not the fault of Coder. This is just by design because
          all models share the same base phi-2 that has just been finetuned. If we
          really want to improve this work we would need extra phi-2 base model pretrained
          with different seeds. However it might not work with mergekit which retain
          only one giver base model for the attention layers.</p>

          <p>If someone wants to try a DPO on the phixtral-4x2_8 we may have a good
          surprise but I doubt. Let me know if interested, I''ll pack up the additional
          code for that.</p>

          '
        raw: "I tried 3 experts (dropped coder) and 2 num_experts_per_tok\r\nAGI_EVAL:\
          \ 33.93\r\nGPT4ALL: 70.50\r\nTruthfullQA: 48.78\r\nBigBench: 37.84\r\nAverage:\
          \ 47.76\r\n\r\nSo the fact that we can't really improve beyond the smallest\
          \ phixtral-2x2_8 proves that this is not the fault of Coder. This is just\
          \ by design because all models share the same base phi-2 that has just been\
          \ finetuned. If we really want to improve this work we would need extra\
          \ phi-2 base model pretrained with different seeds. However it might not\
          \ work with mergekit which retain only one giver base model for the attention\
          \ layers.\r\n\r\nIf someone wants to try a DPO on the phixtral-4x2_8 we\
          \ may have a good surprise but I doubt. Let me know if interested, I'll\
          \ pack up the additional code for that.\r\n"
        updatedAt: '2024-01-15T19:01:20.428Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mlabonne
        - resnet151
    id: 65a58100d81b6fa6c873e94e
    type: comment
  author: vince62s
  content: "I tried 3 experts (dropped coder) and 2 num_experts_per_tok\r\nAGI_EVAL:\
    \ 33.93\r\nGPT4ALL: 70.50\r\nTruthfullQA: 48.78\r\nBigBench: 37.84\r\nAverage:\
    \ 47.76\r\n\r\nSo the fact that we can't really improve beyond the smallest phixtral-2x2_8\
    \ proves that this is not the fault of Coder. This is just by design because all\
    \ models share the same base phi-2 that has just been finetuned. If we really\
    \ want to improve this work we would need extra phi-2 base model pretrained with\
    \ different seeds. However it might not work with mergekit which retain only one\
    \ giver base model for the attention layers.\r\n\r\nIf someone wants to try a\
    \ DPO on the phixtral-4x2_8 we may have a good surprise but I doubt. Let me know\
    \ if interested, I'll pack up the additional code for that.\r\n"
  created_at: 2024-01-15 19:01:20+00:00
  edited: false
  hidden: false
  id: 65a58100d81b6fa6c873e94e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-15T19:46:21.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7360709309577942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Interesting results, thanks!</p>

          '
        raw: Interesting results, thanks!
        updatedAt: '2024-01-15T19:46:21.223Z'
      numEdits: 0
      reactions: []
    id: 65a58b8d0797583a6822f7f0
    type: comment
  author: mlabonne
  content: Interesting results, thanks!
  created_at: 2024-01-15 19:46:21+00:00
  edited: false
  hidden: false
  id: 65a58b8d0797583a6822f7f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: mlabonne/phixtral-4x2_8
repo_type: model
status: open
target_branch: null
title: phixtral-3x2_8
