!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TomPei
conflicting_files: null
created_at: 2024-01-22 09:08:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c71b27d43e4dee51a8b31a/OCIR_yABnUZZMPKFBgzJt.png?w=200&h=200&f=face
      fullname: TomPei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TomPei
      type: user
    createdAt: '2024-01-22T09:08:25.000Z'
    data:
      edited: false
      editors:
      - TomPei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249091744422913
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c71b27d43e4dee51a8b31a/OCIR_yABnUZZMPKFBgzJt.png?w=200&h=200&f=face
          fullname: TomPei
          isHf: false
          isPro: false
          name: TomPei
          type: user
        html: '<p>hello mlabonne, can you share the mergekit''s merge.yaml of phixtral-4x2_8.<br>I
          want to merge some other models for tests.<br>Thank you very much</p>

          '
        raw: "hello mlabonne, can you share the mergekit's merge.yaml of phixtral-4x2_8.\
          \ \r\nI want to merge some other models for tests.\r\nThank you very much"
        updatedAt: '2024-01-22T09:08:25.006Z'
      numEdits: 0
      reactions: []
    id: 65ae3089b68db4f26e3b54a3
    type: comment
  author: TomPei
  content: "hello mlabonne, can you share the mergekit's merge.yaml of phixtral-4x2_8.\
    \ \r\nI want to merge some other models for tests.\r\nThank you very much"
  created_at: 2024-01-22 09:08:25+00:00
  edited: false
  hidden: false
  id: 65ae3089b68db4f26e3b54a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-22T18:44:49.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9906622767448425
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>It''s in the model card</p>

          '
        raw: It's in the model card
        updatedAt: '2024-01-22T18:44:49.125Z'
      numEdits: 0
      reactions: []
    id: 65aeb7a1b64e1c2389c1e19d
    type: comment
  author: mlabonne
  content: It's in the model card
  created_at: 2024-01-22 18:44:49+00:00
  edited: false
  hidden: false
  id: 65aeb7a1b64e1c2389c1e19d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dca2263a789bcd0f58dd546b92af39e3.svg
      fullname: Nayeon Han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HanNayeoniee
      type: user
    createdAt: '2024-01-25T09:26:14.000Z'
    data:
      edited: false
      editors:
      - HanNayeoniee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8039303421974182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dca2263a789bcd0f58dd546b92af39e3.svg
          fullname: Nayeon Han
          isHf: false
          isPro: false
          name: HanNayeoniee
          type: user
        html: '<blockquote>

          <p>It''s in the model card</p>

          </blockquote>

          <p>I tried to merge model using the same yaml file in the model card, but
          I got this error.<br>It seems like it''s giving me errors since there''s
          nothing in <code>positive_prompts</code>.</p>

          <pre><code>root@ubuntu:/workspace# mergekit-moe ./my_setting/phixtral-4x2_8.yml
          ./models/moe/phixtral-4x2_8_test

          ERROR:root:Your positive and negative prompts are identical for all experts.
          This will not produce a functioning MoE.

          ERROR:root:For each expert, `positive_prompts` must contain one or more
          example prompt reflecting what should be routed to that expert.

          </code></pre>

          <p>In the yaml file of another model, <a href="https://huggingface.co/Undi95/Mixtral-8x7B-MoE-RP-Story/blob/main/config.yaml">Undi95/Mixtral-8x7B-MoE-RP-Story</a>
          for example, keywords are listed in <code>positive_prompts</code> and <code>negative_prompts</code>
          for each base model.</p>

          <p>Could you please share details of merge.yaml file?<br>Or did I do something
          wrong?</p>

          <p>Thanks in advance!</p>

          '
        raw: '> It''s in the model card


          I tried to merge model using the same yaml file in the model card, but I
          got this error.

          It seems like it''s giving me errors since there''s nothing in `positive_prompts`.


          ```

          root@ubuntu:/workspace# mergekit-moe ./my_setting/phixtral-4x2_8.yml ./models/moe/phixtral-4x2_8_test

          ERROR:root:Your positive and negative prompts are identical for all experts.
          This will not produce a functioning MoE.

          ERROR:root:For each expert, `positive_prompts` must contain one or more
          example prompt reflecting what should be routed to that expert.

          ```


          In the yaml file of another model, [Undi95/Mixtral-8x7B-MoE-RP-Story](https://huggingface.co/Undi95/Mixtral-8x7B-MoE-RP-Story/blob/main/config.yaml)
          for example, keywords are listed in `positive_prompts` and `negative_prompts`
          for each base model.


          Could you please share details of merge.yaml file?

          Or did I do something wrong?


          Thanks in advance!'
        updatedAt: '2024-01-25T09:26:14.744Z'
      numEdits: 0
      reactions: []
    id: 65b22936c8a577067dd2cde0
    type: comment
  author: HanNayeoniee
  content: '> It''s in the model card


    I tried to merge model using the same yaml file in the model card, but I got this
    error.

    It seems like it''s giving me errors since there''s nothing in `positive_prompts`.


    ```

    root@ubuntu:/workspace# mergekit-moe ./my_setting/phixtral-4x2_8.yml ./models/moe/phixtral-4x2_8_test

    ERROR:root:Your positive and negative prompts are identical for all experts. This
    will not produce a functioning MoE.

    ERROR:root:For each expert, `positive_prompts` must contain one or more example
    prompt reflecting what should be routed to that expert.

    ```


    In the yaml file of another model, [Undi95/Mixtral-8x7B-MoE-RP-Story](https://huggingface.co/Undi95/Mixtral-8x7B-MoE-RP-Story/blob/main/config.yaml)
    for example, keywords are listed in `positive_prompts` and `negative_prompts`
    for each base model.


    Could you please share details of merge.yaml file?

    Or did I do something wrong?


    Thanks in advance!'
  created_at: 2024-01-25 09:26:14+00:00
  edited: false
  hidden: false
  id: 65b22936c8a577067dd2cde0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: mlabonne/phixtral-4x2_8
repo_type: model
status: open
target_branch: null
title: hello mlabonne, can you share the mergekit's merge.yaml of phixtral-4x2_8
