!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nuke3d
conflicting_files: null
created_at: 2023-12-17 09:53:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
      fullname: nuke 3d
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nuke3d
      type: user
    createdAt: '2023-12-17T09:53:34.000Z'
    data:
      edited: false
      editors:
      - nuke3d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8973675966262817
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
          fullname: nuke 3d
          isHf: false
          isPro: false
          name: nuke3d
          type: user
        html: '<p>I tried loading this from code with exllamav2, the same way I load
          other models, but it only generates gibberish. I then tried in text generation
          webui with exllamav2 and it''s the same issue. When I use exllamav2_HF instead,
          it generates fine. How can I make this model work from my own code when
          using exllamav2?</p>

          '
        raw: I tried loading this from code with exllamav2, the same way I load other
          models, but it only generates gibberish. I then tried in text generation
          webui with exllamav2 and it's the same issue. When I use exllamav2_HF instead,
          it generates fine. How can I make this model work from my own code when
          using exllamav2?
        updatedAt: '2023-12-17T09:53:34.394Z'
      numEdits: 0
      reactions: []
    id: 657ec51e1e3e9c41a4b7dbe5
    type: comment
  author: nuke3d
  content: I tried loading this from code with exllamav2, the same way I load other
    models, but it only generates gibberish. I then tried in text generation webui
    with exllamav2 and it's the same issue. When I use exllamav2_HF instead, it generates
    fine. How can I make this model work from my own code when using exllamav2?
  created_at: 2023-12-17 09:53:34+00:00
  edited: false
  hidden: false
  id: 657ec51e1e3e9c41a4b7dbe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-17T10:55:05.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957513689994812
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>It''s going to be down to the prompt being used. Turn on verbose
          mode with ooba text gen and see what is being sent to the model. Then use
          the notebook in ooba to try to replicate the prompt. You will also need
          to add special characters like <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>
          and any other characters in the model''s config.  It''s all a bit painful
          unfortunately.  ooba and other utils can read the prompt format if available,
          but not all tools support automatic prompt reading.</p>

          '
        raw: It's going to be down to the prompt being used. Turn on verbose mode
          with ooba text gen and see what is being sent to the model. Then use the
          notebook in ooba to try to replicate the prompt. You will also need to add
          special characters like `<s>` and `</s>` and any other characters in the
          model's config.  It's all a bit painful unfortunately.  ooba and other utils
          can read the prompt format if available, but not all tools support automatic
          prompt reading.
        updatedAt: '2023-12-17T10:55:29.319Z'
      numEdits: 1
      reactions: []
    id: 657ed389112a9ca54562a3b9
    type: comment
  author: LoneStriker
  content: It's going to be down to the prompt being used. Turn on verbose mode with
    ooba text gen and see what is being sent to the model. Then use the notebook in
    ooba to try to replicate the prompt. You will also need to add special characters
    like `<s>` and `</s>` and any other characters in the model's config.  It's all
    a bit painful unfortunately.  ooba and other utils can read the prompt format
    if available, but not all tools support automatic prompt reading.
  created_at: 2023-12-17 10:55:05+00:00
  edited: true
  hidden: false
  id: 657ed389112a9ca54562a3b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
      fullname: nuke 3d
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nuke3d
      type: user
    createdAt: '2023-12-17T11:11:10.000Z'
    data:
      edited: false
      editors:
      - nuke3d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9136536121368408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
          fullname: nuke 3d
          isHf: false
          isPro: false
          name: nuke3d
          type: user
        html: "<p>Even with the wrong prompt the exllamav2_HF version works fine:</p>\n\
          <pre><code>### Instruction\nsay Hello\n\n### Response\nHello, how can I\
          \ assist you today?\n</code></pre>\n<p>But exllamav2 doesn't:</p>\n<pre><code>###\
          \ Instruction\nsay Hello\n\n### Response\nturn\vperty\v\x0E\f\n\x15\t\f\x13\
          \t\x0F\f\v\x11@\x10\x12?\x14=ngB\u0442\u043E\u043D\x11\nilt\x10 sheunctionknww'=&gt;S\n\
          </code></pre>\n<p>I also tried the correct prompt but it doesn't make any\
          \ difference, at least in terms of returning any kind of sensible text.\
          \ I tried with exllamav2 0.0.10 and 0.0.11. Must be something related to\
          \ the tokenizer for this model. Other models work fine, and as far as I\
          \ can see the only diff between exllamav2 and exllamav2_HF is the tokenizer\
          \ part.</p>\n"
        raw: "Even with the wrong prompt the exllamav2_HF version works fine:\n```\n\
          ### Instruction\nsay Hello\n\n### Response\nHello, how can I assist you\
          \ today?\n```\n\nBut exllamav2 doesn't:\n```\n### Instruction\nsay Hello\n\
          \n### Response\nturn\vperty\v\x0E\f\n\x15\t\f\x13\t\x0F\f\v\x11@\x10\x12\
          ?\x14=ngB\u0442\u043E\u043D\x11\nilt\x10 sheunctionknww'=>S\n```\n\nI also\
          \ tried the correct prompt but it doesn't make any difference, at least\
          \ in terms of returning any kind of sensible text. I tried with exllamav2\
          \ 0.0.10 and 0.0.11. Must be something related to the tokenizer for this\
          \ model. Other models work fine, and as far as I can see the only diff between\
          \ exllamav2 and exllamav2_HF is the tokenizer part."
        updatedAt: '2023-12-17T11:11:10.366Z'
      numEdits: 0
      reactions: []
    id: 657ed74e365456e362358a9b
    type: comment
  author: nuke3d
  content: "Even with the wrong prompt the exllamav2_HF version works fine:\n```\n\
    ### Instruction\nsay Hello\n\n### Response\nHello, how can I assist you today?\n\
    ```\n\nBut exllamav2 doesn't:\n```\n### Instruction\nsay Hello\n\n### Response\n\
    turn\vperty\v\x0E\f\n\x15\t\f\x13\t\x0F\f\v\x11@\x10\x12?\x14=ngB\u0442\u043E\u043D\
    \x11\nilt\x10 sheunctionknww'=>S\n```\n\nI also tried the correct prompt but it\
    \ doesn't make any difference, at least in terms of returning any kind of sensible\
    \ text. I tried with exllamav2 0.0.10 and 0.0.11. Must be something related to\
    \ the tokenizer for this model. Other models work fine, and as far as I can see\
    \ the only diff between exllamav2 and exllamav2_HF is the tokenizer part."
  created_at: 2023-12-17 11:11:10+00:00
  edited: false
  hidden: false
  id: 657ed74e365456e362358a9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-17T11:12:35.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9067372679710388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Check for the special tokens as well like I mentioned previously,
          e.g. <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> and any start/end
          token.</p>

          '
        raw: Check for the special tokens as well like I mentioned previously, e.g.
          `<s>` and `</s>` and any start/end token.
        updatedAt: '2023-12-17T11:12:35.270Z'
      numEdits: 0
      reactions: []
    id: 657ed7a317f67d5b87dfd37e
    type: comment
  author: LoneStriker
  content: Check for the special tokens as well like I mentioned previously, e.g.
    `<s>` and `</s>` and any start/end token.
  created_at: 2023-12-17 11:12:35+00:00
  edited: false
  hidden: false
  id: 657ed7a317f67d5b87dfd37e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
      fullname: nuke 3d
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nuke3d
      type: user
    createdAt: '2023-12-17T11:23:35.000Z'
    data:
      edited: false
      editors:
      - nuke3d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8918718695640564
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d45df07e4001d132cd8c34bf7655a9cc.svg
          fullname: nuke 3d
          isHf: false
          isPro: false
          name: nuke3d
          type: user
        html: '<p>This is in the notebook so there are no special tokens in either
          case, all other settings are exactly the same, except for using _HF in the
          first case.</p>

          <p>I also tried LoneStriker/Magicoder-S-CL-7B-8.0bpw-h8-exl2-2 just now,
          this works fine with both exllamav2 and exllamav2_HF.</p>

          '
        raw: 'This is in the notebook so there are no special tokens in either case,
          all other settings are exactly the same, except for using _HF in the first
          case.


          I also tried LoneStriker/Magicoder-S-CL-7B-8.0bpw-h8-exl2-2 just now, this
          works fine with both exllamav2 and exllamav2_HF.'
        updatedAt: '2023-12-17T11:23:35.764Z'
      numEdits: 0
      reactions: []
    id: 657eda3790d65df03ddc29f3
    type: comment
  author: nuke3d
  content: 'This is in the notebook so there are no special tokens in either case,
    all other settings are exactly the same, except for using _HF in the first case.


    I also tried LoneStriker/Magicoder-S-CL-7B-8.0bpw-h8-exl2-2 just now, this works
    fine with both exllamav2 and exllamav2_HF.'
  created_at: 2023-12-17 11:23:35+00:00
  edited: false
  hidden: false
  id: 657eda3790d65df03ddc29f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Magicoder-S-DS-6.7B-8.0bpw-h8-exl2
repo_type: model
status: open
target_branch: null
title: Gibberish with exllamav2 but working fine with exllamav2_HF
