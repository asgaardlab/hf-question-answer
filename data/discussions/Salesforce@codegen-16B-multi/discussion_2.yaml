!!python/object:huggingface_hub.community.DiscussionWithDetails
author: glicerico
conflicting_files: null
created_at: 2023-03-26 20:40:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
      fullname: Gli Cerico
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: glicerico
      type: user
    createdAt: '2023-03-26T21:40:38.000Z'
    data:
      edited: false
      editors:
      - glicerico
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
          fullname: Gli Cerico
          isHf: false
          isPro: false
          name: glicerico
          type: user
        html: '<p>I am trying to fine tune this model using deepspeed, as suggested
          in the model''s repo: <a rel="nofollow" href="https://github.com/salesforce/jaxformer#a100-fine-tune">https://github.com/salesforce/jaxformer#a100-fine-tune</a><br>I
          have tried on up to 4 x A100 with a total of 360GB of RAM, but every time
          my training crashes before starting, after the memory gets fully used (monitored
          with htop).<br>How much memory do I need to fine tune this?</p>

          '
        raw: "I am trying to fine tune this model using deepspeed, as suggested in\
          \ the model's repo: https://github.com/salesforce/jaxformer#a100-fine-tune\r\
          \nI have tried on up to 4 x A100 with a total of 360GB of RAM, but every\
          \ time my training crashes before starting, after the memory gets fully\
          \ used (monitored with htop).\r\nHow much memory do I need to fine tune\
          \ this?"
        updatedAt: '2023-03-26T21:40:38.281Z'
      numEdits: 0
      reactions: []
    id: 6420bbd6290342c5df8693dc
    type: comment
  author: glicerico
  content: "I am trying to fine tune this model using deepspeed, as suggested in the\
    \ model's repo: https://github.com/salesforce/jaxformer#a100-fine-tune\r\nI have\
    \ tried on up to 4 x A100 with a total of 360GB of RAM, but every time my training\
    \ crashes before starting, after the memory gets fully used (monitored with htop).\r\
    \nHow much memory do I need to fine tune this?"
  created_at: 2023-03-26 20:40:38+00:00
  edited: false
  hidden: false
  id: 6420bbd6290342c5df8693dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da865d0e5547fc9dc51b96153e5a8d6d.svg
      fullname: Erik Nijkamp
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: enijkamp
      type: user
    createdAt: '2023-03-26T21:42:24.000Z'
    data:
      edited: false
      editors:
      - enijkamp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da865d0e5547fc9dc51b96153e5a8d6d.svg
          fullname: Erik Nijkamp
          isHf: false
          isPro: false
          name: enijkamp
          type: user
        html: '<p>Here is a configuration for deepspeed, which should fit on a single
          A100 with CPU offloading, however, this may be slow:<br><a rel="nofollow"
          href="https://github.com/salesforce/jaxformer/blob/main/jaxformer/hf/train.py">https://github.com/salesforce/jaxformer/blob/main/jaxformer/hf/train.py</a></p>

          '
        raw: 'Here is a configuration for deepspeed, which should fit on a single
          A100 with CPU offloading, however, this may be slow:

          https://github.com/salesforce/jaxformer/blob/main/jaxformer/hf/train.py'
        updatedAt: '2023-03-26T21:42:24.596Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - joebnb
    id: 6420bc409f60ddafaeb30d38
    type: comment
  author: enijkamp
  content: 'Here is a configuration for deepspeed, which should fit on a single A100
    with CPU offloading, however, this may be slow:

    https://github.com/salesforce/jaxformer/blob/main/jaxformer/hf/train.py'
  created_at: 2023-03-26 20:42:24+00:00
  edited: false
  hidden: false
  id: 6420bc409f60ddafaeb30d38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
      fullname: Gli Cerico
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: glicerico
      type: user
    createdAt: '2023-03-26T22:10:18.000Z'
    data:
      edited: false
      editors:
      - glicerico
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
          fullname: Gli Cerico
          isHf: false
          isPro: false
          name: glicerico
          type: user
        html: "<p>thanks for replying <span data-props=\"{&quot;user&quot;:&quot;enijkamp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/enijkamp\"\
          >@<span class=\"underline\">enijkamp</span></a></span>\n\n\t</span></span>\
          \ . This is exactly <a rel=\"nofollow\" href=\"https://github.com/glicerico/jaxformer/blob/my-test/jaxformer/hf/train.py\"\
          >what I am trying to use</a> (with my own training data, a longer run, and\
          \ saving checkpoints), but as I say above, loading the model uses more than\
          \ 360GB of RAM.<br>I am not sure if I am activating CPU offloading, though...\
          \ I suppose the default params in that file are enough?</p>\n"
        raw: 'thanks for replying @enijkamp . This is exactly [what I am trying to
          use](https://github.com/glicerico/jaxformer/blob/my-test/jaxformer/hf/train.py)
          (with my own training data, a longer run, and saving checkpoints), but as
          I say above, loading the model uses more than 360GB of RAM.

          I am not sure if I am activating CPU offloading, though... I suppose the
          default params in that file are enough?'
        updatedAt: '2023-03-26T22:10:18.933Z'
      numEdits: 0
      reactions: []
    id: 6420c2caf7ec87e9bfb9b281
    type: comment
  author: glicerico
  content: 'thanks for replying @enijkamp . This is exactly [what I am trying to use](https://github.com/glicerico/jaxformer/blob/my-test/jaxformer/hf/train.py)
    (with my own training data, a longer run, and saving checkpoints), but as I say
    above, loading the model uses more than 360GB of RAM.

    I am not sure if I am activating CPU offloading, though... I suppose the default
    params in that file are enough?'
  created_at: 2023-03-26 21:10:18+00:00
  edited: false
  hidden: false
  id: 6420c2caf7ec87e9bfb9b281
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
      fullname: Gli Cerico
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: glicerico
      type: user
    createdAt: '2023-03-26T22:12:10.000Z'
    data:
      edited: false
      editors:
      - glicerico
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d19fc395f4a7202e250b389c9a4f313.svg
          fullname: Gli Cerico
          isHf: false
          isPro: false
          name: glicerico
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;enijkamp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/enijkamp\">@<span class=\"\
          underline\">enijkamp</span></a></span>\n\n\t</span></span> I've succeeded\
          \ fine-tuning using TPU, but unfortunately can't find the 16B model checkpoints\
          \ for this. I have read from last year issues that you haven't had time\
          \ to upload sharding patterns... Any update on this?</p>\n"
        raw: '@enijkamp I''ve succeeded fine-tuning using TPU, but unfortunately can''t
          find the 16B model checkpoints for this. I have read from last year issues
          that you haven''t had time to upload sharding patterns... Any update on
          this?'
        updatedAt: '2023-03-26T22:12:10.323Z'
      numEdits: 0
      reactions: []
    id: 6420c33a0f061ae62185d754
    type: comment
  author: glicerico
  content: '@enijkamp I''ve succeeded fine-tuning using TPU, but unfortunately can''t
    find the 16B model checkpoints for this. I have read from last year issues that
    you haven''t had time to upload sharding patterns... Any update on this?'
  created_at: 2023-03-26 21:12:10+00:00
  edited: false
  hidden: false
  id: 6420c33a0f061ae62185d754
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/codegen-16B-multi
repo_type: model
status: open
target_branch: null
title: fine tune memory?
