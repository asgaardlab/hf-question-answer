!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vobbilisettyjayadeep
conflicting_files: null
created_at: 2024-01-08 13:11:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
      fullname: Vobbilisetty Veera Venkata Jayadeep
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vobbilisettyjayadeep
      type: user
    createdAt: '2024-01-08T13:11:37.000Z'
    data:
      edited: false
      editors:
      - vobbilisettyjayadeep
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8795129656791687
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
          fullname: Vobbilisetty Veera Venkata Jayadeep
          isHf: false
          isPro: false
          name: vobbilisettyjayadeep
          type: user
        html: '<p>Hi all<br>I am trying to create embeddings for 15 lakh rows of data
          using sentence-transformers/all-MiniLM-L6-v2 for an application and upload
          embeddings to pgVector database.<br>While creating embeddings the server
          memory is getting completely exhausted and getting crashed.  </p>

          <p>Please help me here.</p>

          '
        raw: "Hi all \r\nI am trying to create embeddings for 15 lakh rows of data\
          \ using sentence-transformers/all-MiniLM-L6-v2 for an application and upload\
          \ embeddings to pgVector database. \r\nWhile creating embeddings the server\
          \ memory is getting completely exhausted and getting crashed.  \r\n\r\n\
          Please help me here.\r\n"
        updatedAt: '2024-01-08T13:11:37.981Z'
      numEdits: 0
      reactions: []
    id: 659bf489013bdae00ca99309
    type: comment
  author: vobbilisettyjayadeep
  content: "Hi all \r\nI am trying to create embeddings for 15 lakh rows of data using\
    \ sentence-transformers/all-MiniLM-L6-v2 for an application and upload embeddings\
    \ to pgVector database. \r\nWhile creating embeddings the server memory is getting\
    \ completely exhausted and getting crashed.  \r\n\r\nPlease help me here.\r\n"
  created_at: 2024-01-08 13:11:37+00:00
  edited: false
  hidden: false
  id: 659bf489013bdae00ca99309
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-08T13:14:01.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9622327089309692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>Hello!</p>

          <p>I''m aware of this issue. The gist is that as more of the texts get turned
          into embeddings, the already processed embeddings all remain in memory until
          all texts have been processed. This can lead to high memory usage. My recommendation
          at this time is to chunk your texts and only process e.g. 1 lakh sentences
          at a time, upload those embeddings, and then do the next chunk.<br>Hope
          this helps.</p>

          <ul>

          <li>Tom Aarsen</li>

          </ul>

          '
        raw: 'Hello!


          I''m aware of this issue. The gist is that as more of the texts get turned
          into embeddings, the already processed embeddings all remain in memory until
          all texts have been processed. This can lead to high memory usage. My recommendation
          at this time is to chunk your texts and only process e.g. 1 lakh sentences
          at a time, upload those embeddings, and then do the next chunk.

          Hope this helps.


          - Tom Aarsen'
        updatedAt: '2024-01-08T13:14:01.008Z'
      numEdits: 0
      reactions: []
    id: 659bf51963974276ac3c006d
    type: comment
  author: tomaarsen
  content: 'Hello!


    I''m aware of this issue. The gist is that as more of the texts get turned into
    embeddings, the already processed embeddings all remain in memory until all texts
    have been processed. This can lead to high memory usage. My recommendation at
    this time is to chunk your texts and only process e.g. 1 lakh sentences at a time,
    upload those embeddings, and then do the next chunk.

    Hope this helps.


    - Tom Aarsen'
  created_at: 2024-01-08 13:14:01+00:00
  edited: false
  hidden: false
  id: 659bf51963974276ac3c006d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
      fullname: Vobbilisetty Veera Venkata Jayadeep
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vobbilisettyjayadeep
      type: user
    createdAt: '2024-01-08T13:43:44.000Z'
    data:
      edited: false
      editors:
      - vobbilisettyjayadeep
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8570622801780701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
          fullname: Vobbilisetty Veera Venkata Jayadeep
          isHf: false
          isPro: false
          name: vobbilisettyjayadeep
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;tomaarsen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tomaarsen\"\
          >@<span class=\"underline\">tomaarsen</span></a></span>\n\n\t</span></span>\
          \ thank you for your reply,<br>For now i am just going POC. If this is successful\
          \ i will scale up the same for 5 Crore + rows of data. In that case this\
          \ way of implementing is not suggestable.<br>Is there any way to do parallel\
          \ processing for creation of embeddings. </p>\n"
        raw: "Hey @tomaarsen thank you for your reply, \nFor now i am just going POC.\
          \ If this is successful i will scale up the same for 5 Crore + rows of data.\
          \ In that case this way of implementing is not suggestable. \nIs there any\
          \ way to do parallel processing for creation of embeddings. "
        updatedAt: '2024-01-08T13:43:44.148Z'
      numEdits: 0
      reactions: []
    id: 659bfc10ef8f83e79b68827b
    type: comment
  author: vobbilisettyjayadeep
  content: "Hey @tomaarsen thank you for your reply, \nFor now i am just going POC.\
    \ If this is successful i will scale up the same for 5 Crore + rows of data. In\
    \ that case this way of implementing is not suggestable. \nIs there any way to\
    \ do parallel processing for creation of embeddings. "
  created_at: 2024-01-08 13:43:44+00:00
  edited: false
  hidden: false
  id: 659bfc10ef8f83e79b68827b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-08T13:45:16.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7669159173965454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>Yes, you can use <a rel="nofollow" href="https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode_multi_process">https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode_multi_process</a>
          for encoding on multiple processes or multiple GPUs, but the memory issue
          might still persist then. Chunking remains a good option I think.</p>

          '
        raw: Yes, you can use https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode_multi_process
          for encoding on multiple processes or multiple GPUs, but the memory issue
          might still persist then. Chunking remains a good option I think.
        updatedAt: '2024-01-08T13:45:16.627Z'
      numEdits: 0
      reactions: []
    id: 659bfc6cb6cdb38e3cbb99fc
    type: comment
  author: tomaarsen
  content: Yes, you can use https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode_multi_process
    for encoding on multiple processes or multiple GPUs, but the memory issue might
    still persist then. Chunking remains a good option I think.
  created_at: 2024-01-08 13:45:16+00:00
  edited: false
  hidden: false
  id: 659bfc6cb6cdb38e3cbb99fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
      fullname: Vobbilisetty Veera Venkata Jayadeep
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vobbilisettyjayadeep
      type: user
    createdAt: '2024-01-08T14:59:55.000Z'
    data:
      edited: false
      editors:
      - vobbilisettyjayadeep
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.797734797000885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad6871c0174778b8b17ee67013cdb412.svg
          fullname: Vobbilisetty Veera Venkata Jayadeep
          isHf: false
          isPro: false
          name: vobbilisettyjayadeep
          type: user
        html: '<p>will check the above link and get back to you asap.<br>thanks !!!</p>

          '
        raw: 'will check the above link and get back to you asap.

          thanks !!!

          '
        updatedAt: '2024-01-08T14:59:55.956Z'
      numEdits: 0
      reactions: []
    id: 659c0debe430b3799ff6c280
    type: comment
  author: vobbilisettyjayadeep
  content: 'will check the above link and get back to you asap.

    thanks !!!

    '
  created_at: 2024-01-08 14:59:55+00:00
  edited: false
  hidden: false
  id: 659c0debe430b3799ff6c280
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: sentence-transformers/all-MiniLM-L6-v2
repo_type: model
status: open
target_branch: null
title: Memory is becoming fully exhausted during the generation of embeddings, leading
  to a complete server crash.
