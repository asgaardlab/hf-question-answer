!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nsb39
conflicting_files: null
created_at: 2023-07-05 04:16:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02b2512a95987609e6dd575db98f15f3.svg
      fullname: Nsb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nsb39
      type: user
    createdAt: '2023-07-05T05:16:28.000Z'
    data:
      edited: false
      editors:
      - Nsb39
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8911823034286499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02b2512a95987609e6dd575db98f15f3.svg
          fullname: Nsb
          isHf: false
          isPro: false
          name: Nsb39
          type: user
        html: '<p>Hi,<br>Can someone please advise me upon the hardware requirements
          of using this model for a semantic similarity use-case. I had downloaded
          the model locally and am using it to generate embedding, and finally using
          util.pytorch_cos_sim to calculate similarity scores between 2 sentences.
          All was working good in my Mac Pro ( 2.4 GHz 8-Core Intel Core i9 processor
          and 32 GB memory); but after I moved the model to containers of 1 core CPU
          and 4 GB RAM, the code is taking at least 15-20 times more time to generate
          the cosine similarity score. </p>

          <p>Did someone face a similar situation? Kindly advise.<br>Thank you in
          advance for the help!</p>

          <p>N.B.: I have also shared the sample code for reference.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64a4f9a48b961fd6fa42d72a/mWpwrxVDmUbG7o8zjN5Bm.png"><img
          alt="Screen Shot 2023-07-05 at 10.43.44 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/64a4f9a48b961fd6fa42d72a/mWpwrxVDmUbG7o8zjN5Bm.png"></a></p>

          '
        raw: "Hi,\r\nCan someone please advise me upon the hardware requirements of\
          \ using this model for a semantic similarity use-case. I had downloaded\
          \ the model locally and am using it to generate embedding, and finally using\
          \ util.pytorch_cos_sim to calculate similarity scores between 2 sentences.\
          \ All was working good in my Mac Pro ( 2.4 GHz 8-Core Intel Core i9 processor\
          \ and 32 GB memory); but after I moved the model to containers of 1 core\
          \ CPU and 4 GB RAM, the code is taking at least 15-20 times more time to\
          \ generate the cosine similarity score. \r\n\r\nDid someone face a similar\
          \ situation? Kindly advise. \r\nThank you in advance for the help!\r\n\r\
          \nN.B.: I have also shared the sample code for reference.\r\n\r\n![Screen\
          \ Shot 2023-07-05 at 10.43.44 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64a4f9a48b961fd6fa42d72a/mWpwrxVDmUbG7o8zjN5Bm.png)\r\
          \n\r\n"
        updatedAt: '2023-07-05T05:16:28.622Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bendangelo
    id: 64a4fcac4c10e36754ad7628
    type: comment
  author: Nsb39
  content: "Hi,\r\nCan someone please advise me upon the hardware requirements of\
    \ using this model for a semantic similarity use-case. I had downloaded the model\
    \ locally and am using it to generate embedding, and finally using util.pytorch_cos_sim\
    \ to calculate similarity scores between 2 sentences. All was working good in\
    \ my Mac Pro ( 2.4 GHz 8-Core Intel Core i9 processor and 32 GB memory); but after\
    \ I moved the model to containers of 1 core CPU and 4 GB RAM, the code is taking\
    \ at least 15-20 times more time to generate the cosine similarity score. \r\n\
    \r\nDid someone face a similar situation? Kindly advise. \r\nThank you in advance\
    \ for the help!\r\n\r\nN.B.: I have also shared the sample code for reference.\r\
    \n\r\n![Screen Shot 2023-07-05 at 10.43.44 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64a4f9a48b961fd6fa42d72a/mWpwrxVDmUbG7o8zjN5Bm.png)\r\
    \n\r\n"
  created_at: 2023-07-05 04:16:28+00:00
  edited: false
  hidden: false
  id: 64a4fcac4c10e36754ad7628
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f3938a92b23d74f3197b536a668cbc36.svg
      fullname: Emmanuel VAIE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: evaie
      type: user
    createdAt: '2023-07-26T19:46:10.000Z'
    data:
      edited: false
      editors:
      - evaie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9584378600120544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f3938a92b23d74f3197b536a668cbc36.svg
          fullname: Emmanuel VAIE
          isHf: false
          isPro: false
          name: evaie
          type: user
        html: '<p>Hello,<br>I''m not sure but I think when you are using the transformer
          from a container, the SentenceTransformer is downloading the model each
          time you are running it. My advice is to log the loading time (the time
          to instantiate the class SentenceTransformer) in order to confirm/infirm
          the fact that you are spending more time in downloading than in executing
          the model.</p>

          '
        raw: 'Hello,

          I''m not sure but I think when you are using the transformer from a container,
          the SentenceTransformer is downloading the model each time you are running
          it. My advice is to log the loading time (the time to instantiate the class
          SentenceTransformer) in order to confirm/infirm the fact that you are spending
          more time in downloading than in executing the model.'
        updatedAt: '2023-07-26T19:46:10.845Z'
      numEdits: 0
      reactions: []
    id: 64c17802129617dbab920da1
    type: comment
  author: evaie
  content: 'Hello,

    I''m not sure but I think when you are using the transformer from a container,
    the SentenceTransformer is downloading the model each time you are running it.
    My advice is to log the loading time (the time to instantiate the class SentenceTransformer)
    in order to confirm/infirm the fact that you are spending more time in downloading
    than in executing the model.'
  created_at: 2023-07-26 18:46:10+00:00
  edited: false
  hidden: false
  id: 64c17802129617dbab920da1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: sentence-transformers/all-MiniLM-L6-v2
repo_type: model
status: open
target_branch: null
title: Hardware requirements for using  sentence-transformers/all-MiniLM-L6-v2
