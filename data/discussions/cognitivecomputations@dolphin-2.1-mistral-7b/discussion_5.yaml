!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Acrious
conflicting_files: null
created_at: 2023-10-14 01:03:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
      fullname: Nathan Bollman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrious
      type: user
    createdAt: '2023-10-14T02:03:09.000Z'
    data:
      edited: false
      editors:
      - Acrious
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824000000953674
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
          fullname: Nathan Bollman
          isHf: false
          isPro: false
          name: Acrious
          type: user
        html: '<p>I was briefly flirting with the idea of trying to train a Mistral-7b
          model with Axolotl but didn''t think it was supported yet? Isn''t listed
          as supported by the github page anyway.  Is Mistral similar enough to one
          of these to train?  llama, Pythia, cerebras, btlm, mpt, falcon, gpt-j, XGen,
          phi</p>

          '
        raw: I was briefly flirting with the idea of trying to train a Mistral-7b
          model with Axolotl but didn't think it was supported yet? Isn't listed as
          supported by the github page anyway.  Is Mistral similar enough to one of
          these to train?  llama, Pythia, cerebras, btlm, mpt, falcon, gpt-j, XGen,
          phi
        updatedAt: '2023-10-14T02:03:09.387Z'
      numEdits: 0
      reactions: []
    id: 6529f6ddb355406e2db8ca4f
    type: comment
  author: Acrious
  content: I was briefly flirting with the idea of trying to train a Mistral-7b model
    with Axolotl but didn't think it was supported yet? Isn't listed as supported
    by the github page anyway.  Is Mistral similar enough to one of these to train?  llama,
    Pythia, cerebras, btlm, mpt, falcon, gpt-j, XGen, phi
  created_at: 2023-10-14 01:03:09+00:00
  edited: false
  hidden: false
  id: 6529f6ddb355406e2db8ca4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
      fullname: Nathan Bollman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrious
      type: user
    createdAt: '2023-10-14T02:05:07.000Z'
    data:
      from: Wow! What a model. Didn't know it could be trained with Axylotyl
      to: Wow! What a model. Didn't know it could be trained with Axolotl
    id: 6529f753789d84df6ff036b9
    type: title-change
  author: Acrious
  created_at: 2023-10-14 01:05:07+00:00
  id: 6529f753789d84df6ff036b9
  new_title: Wow! What a model. Didn't know it could be trained with Axolotl
  old_title: Wow! What a model. Didn't know it could be trained with Axylotyl
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otD5RtY--xt8wSVix77-w.png?w=200&h=200&f=face
      fullname: Umar Mnaq
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: umarmnaq
      type: user
    createdAt: '2023-10-14T08:15:15.000Z'
    data:
      edited: false
      editors:
      - umarmnaq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8941267132759094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otD5RtY--xt8wSVix77-w.png?w=200&h=200&f=face
          fullname: Umar Mnaq
          isHf: false
          isPro: false
          name: umarmnaq
          type: user
        html: '<p>It does use the llama architecture, so technically it could be trained?</p>

          '
        raw: It does use the llama architecture, so technically it could be trained?
        updatedAt: '2023-10-14T08:15:15.001Z'
      numEdits: 0
      reactions: []
    id: 652a4e13b118f26df727abbe
    type: comment
  author: umarmnaq
  content: It does use the llama architecture, so technically it could be trained?
  created_at: 2023-10-14 07:15:15+00:00
  edited: false
  hidden: false
  id: 652a4e13b118f26df727abbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-10-14T13:49:07.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9756646752357483
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>Axolotl can train many things, including mistral</p>

          '
        raw: Axolotl can train many things, including mistral
        updatedAt: '2023-10-14T13:49:07.912Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652a9c53880d145ed55eed40
    id: 652a9c53880d145ed55eed3f
    type: comment
  author: ehartford
  content: Axolotl can train many things, including mistral
  created_at: 2023-10-14 12:49:07+00:00
  edited: false
  hidden: false
  id: 652a9c53880d145ed55eed3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-10-14T13:49:07.000Z'
    data:
      status: closed
    id: 652a9c53880d145ed55eed40
    type: status-change
  author: ehartford
  created_at: 2023-10-14 12:49:07+00:00
  id: 652a9c53880d145ed55eed40
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
      fullname: Nathan Bollman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrious
      type: user
    createdAt: '2023-10-14T13:55:47.000Z'
    data:
      edited: false
      editors:
      - Acrious
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8617870211601257
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
          fullname: Nathan Bollman
          isHf: false
          isPro: false
          name: Acrious
          type: user
        html: "<p>All answers are questions so far... Bing-go<br>Searching for: Mistral-7b\
          \ LLM model LLAMA architecture<br>Yes, the Mistral-7b LLM model uses the\
          \ LLAMA architecture 1. The Mistral-7b model is a 7-billion-parameter language\
          \ model that is engineered for superior performance and efficiency. It outperforms\
          \ Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning,\
          \ mathematics, and code generation 1. The model leverages grouped-query\
          \ attention (GQA) for faster inference, coupled with sliding window attention\
          \ (SWA) to effectively handle sequences of arbitrary length with a reduced\
          \ inference cost 1. The Mistral-7b \u2013 Instruct model is fine-tuned to\
          \ follow instructions and surpasses the Llama 2 13B \u2013 Chat model both\
          \ on human and automated benchmarks 1. The models are released under the\
          \ Apache 2.0 license 1.</p>\n<p>Cool!</p>\n"
        raw: "All answers are questions so far... Bing-go\nSearching for: Mistral-7b\
          \ LLM model LLAMA architecture\nYes, the Mistral-7b LLM model uses the LLAMA\
          \ architecture 1. The Mistral-7b model is a 7-billion-parameter language\
          \ model that is engineered for superior performance and efficiency. It outperforms\
          \ Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning,\
          \ mathematics, and code generation 1. The model leverages grouped-query\
          \ attention (GQA) for faster inference, coupled with sliding window attention\
          \ (SWA) to effectively handle sequences of arbitrary length with a reduced\
          \ inference cost 1. The Mistral-7b \u2013 Instruct model is fine-tuned to\
          \ follow instructions and surpasses the Llama 2 13B \u2013 Chat model both\
          \ on human and automated benchmarks 1. The models are released under the\
          \ Apache 2.0 license 1.\n\nCool!"
        updatedAt: '2023-10-14T13:55:47.676Z'
      numEdits: 0
      reactions: []
    id: 652a9de330355beba68e6900
    type: comment
  author: Acrious
  content: "All answers are questions so far... Bing-go\nSearching for: Mistral-7b\
    \ LLM model LLAMA architecture\nYes, the Mistral-7b LLM model uses the LLAMA architecture\
    \ 1. The Mistral-7b model is a 7-billion-parameter language model that is engineered\
    \ for superior performance and efficiency. It outperforms Llama 2 13B across all\
    \ evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation\
    \ 1. The model leverages grouped-query attention (GQA) for faster inference, coupled\
    \ with sliding window attention (SWA) to effectively handle sequences of arbitrary\
    \ length with a reduced inference cost 1. The Mistral-7b \u2013 Instruct model\
    \ is fine-tuned to follow instructions and surpasses the Llama 2 13B \u2013 Chat\
    \ model both on human and automated benchmarks 1. The models are released under\
    \ the Apache 2.0 license 1.\n\nCool!"
  created_at: 2023-10-14 12:55:47+00:00
  edited: false
  hidden: false
  id: 652a9de330355beba68e6900
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-10-16T03:10:24.000Z'
    data:
      edited: true
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9585834741592407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: "<p>Mistral does not use the Llama architecture, LLaMA and Llama 2 is\
          \ LlamaForCausalLM while Mistral is MistralForCausalLM. Mistral also use\
          \ SWA and GQA which Llama(At least 7B doesn't use GPA) doesn't. I think\
          \ Mistral also have slightly different layers distribution. Also no, LLaMA\
          \ or Llama 2 is not released under Apache 2.0, Mistral is though. I think\
          \ Mistral is very slightly larger than LLaMA and Llama 2 too. I'm not sure\
          \ if Mistral has been evaluated on human benchmarks, not sure where it came\
          \ from.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;umarmnaq&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/umarmnaq\"\
          >@<span class=\"underline\">umarmnaq</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;Acrious&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Acrious\">@<span class=\"\
          underline\">Acrious</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'Mistral does not use the Llama architecture, LLaMA and Llama 2 is LlamaForCausalLM
          while Mistral is MistralForCausalLM. Mistral also use SWA and GQA which
          Llama(At least 7B doesn''t use GPA) doesn''t. I think Mistral also have
          slightly different layers distribution. Also no, LLaMA or Llama 2 is not
          released under Apache 2.0, Mistral is though. I think Mistral is very slightly
          larger than LLaMA and Llama 2 too. I''m not sure if Mistral has been evaluated
          on human benchmarks, not sure where it came from.


          @umarmnaq @Acrious '
        updatedAt: '2023-10-16T03:17:23.651Z'
      numEdits: 1
      reactions: []
    id: 652ca9a00ab8936887c4f32c
    type: comment
  author: acrastt
  content: 'Mistral does not use the Llama architecture, LLaMA and Llama 2 is LlamaForCausalLM
    while Mistral is MistralForCausalLM. Mistral also use SWA and GQA which Llama(At
    least 7B doesn''t use GPA) doesn''t. I think Mistral also have slightly different
    layers distribution. Also no, LLaMA or Llama 2 is not released under Apache 2.0,
    Mistral is though. I think Mistral is very slightly larger than LLaMA and Llama
    2 too. I''m not sure if Mistral has been evaluated on human benchmarks, not sure
    where it came from.


    @umarmnaq @Acrious '
  created_at: 2023-10-16 02:10:24+00:00
  edited: true
  hidden: false
  id: 652ca9a00ab8936887c4f32c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
      fullname: Nathan Bollman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrious
      type: user
    createdAt: '2023-10-16T03:14:11.000Z'
    data:
      edited: true
      editors:
      - Acrious
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7072891592979431
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
          fullname: Nathan Bollman
          isHf: false
          isPro: false
          name: Acrious
          type: user
        html: '<p>Awww, Bing-fail... That happens alot, weird that the authors post
          wasn''t there when I commented... Thanks Hartford, Dolphin is amazing, wonder
          why Axolotl doesn''t advertise support for Mistral?</p>

          '
        raw: Awww, Bing-fail... That happens alot, weird that the authors post wasn't
          there when I commented... Thanks Hartford, Dolphin is amazing, wonder why
          Axolotl doesn't advertise support for Mistral?
        updatedAt: '2023-10-16T03:18:26.433Z'
      numEdits: 1
      reactions: []
    id: 652caa83b20d5004473bf55e
    type: comment
  author: Acrious
  content: Awww, Bing-fail... That happens alot, weird that the authors post wasn't
    there when I commented... Thanks Hartford, Dolphin is amazing, wonder why Axolotl
    doesn't advertise support for Mistral?
  created_at: 2023-10-16 02:14:11+00:00
  edited: true
  hidden: false
  id: 652caa83b20d5004473bf55e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-10-16T03:28:28.000Z'
    data:
      edited: false
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9450367093086243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: '<blockquote>

          <p>Awww, Bing-fail... That happens alot, weird that the authors post wasn''t
          there when I commented... Thanks Hartford, Dolphin is amazing, wonder why
          Axolotl doesn''t advertise support for Mistral?</p>

          </blockquote>

          <p>I guess it''s because it doesnt <em>really</em> fully suppport Mistral
          because it doesn''t support SWA yet, it just works without SWA(That''s why
          in the config file the author has, it has 8k ctx instead of 32k with SWA).</p>

          '
        raw: '> Awww, Bing-fail... That happens alot, weird that the authors post
          wasn''t there when I commented... Thanks Hartford, Dolphin is amazing, wonder
          why Axolotl doesn''t advertise support for Mistral?


          I guess it''s because it doesnt *really* fully suppport Mistral because
          it doesn''t support SWA yet, it just works without SWA(That''s why in the
          config file the author has, it has 8k ctx instead of 32k with SWA).'
        updatedAt: '2023-10-16T03:28:28.407Z'
      numEdits: 0
      reactions: []
    id: 652caddcb20d5004473c60d8
    type: comment
  author: acrastt
  content: '> Awww, Bing-fail... That happens alot, weird that the authors post wasn''t
    there when I commented... Thanks Hartford, Dolphin is amazing, wonder why Axolotl
    doesn''t advertise support for Mistral?


    I guess it''s because it doesnt *really* fully suppport Mistral because it doesn''t
    support SWA yet, it just works without SWA(That''s why in the config file the
    author has, it has 8k ctx instead of 32k with SWA).'
  created_at: 2023-10-16 02:28:28+00:00
  edited: false
  hidden: false
  id: 652caddcb20d5004473c60d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-10-25T10:28:29.000Z'
    data:
      edited: false
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3315812051296234
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: '<p>32k position embeddings?<br><a href="https://huggingface.co/ehartford/dolphin-2.1-mistral-7b/blob/main/config.json#L12">https://huggingface.co/ehartford/dolphin-2.1-mistral-7b/blob/main/config.json#L12</a></p>

          '
        raw: '32k position embeddings?

          https://huggingface.co/ehartford/dolphin-2.1-mistral-7b/blob/main/config.json#L12'
        updatedAt: '2023-10-25T10:28:29.602Z'
      numEdits: 0
      reactions: []
    id: 6538edcdf84b1361666e4546
    type: comment
  author: odellus
  content: '32k position embeddings?

    https://huggingface.co/ehartford/dolphin-2.1-mistral-7b/blob/main/config.json#L12'
  created_at: 2023-10-25 09:28:29+00:00
  edited: false
  hidden: false
  id: 6538edcdf84b1361666e4546
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: cognitivecomputations/dolphin-2.1-mistral-7b
repo_type: model
status: closed
target_branch: null
title: Wow! What a model. Didn't know it could be trained with Axolotl
