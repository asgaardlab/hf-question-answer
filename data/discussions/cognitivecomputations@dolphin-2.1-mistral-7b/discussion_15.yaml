!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rhino224
conflicting_files: null
created_at: 2023-10-26 16:19:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64befd29bd099f855ffd5ae08aa14b30.svg
      fullname: Ryan Thayer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rhino224
      type: user
    createdAt: '2023-10-26T17:19:54.000Z'
    data:
      edited: false
      editors:
      - Rhino224
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968999981880188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64befd29bd099f855ffd5ae08aa14b30.svg
          fullname: Ryan Thayer
          isHf: false
          isPro: false
          name: Rhino224
          type: user
        html: '<p>I am having a difficult time with increasing the conversation context
          length. It appears to be set at a default of 512 tokens and does start to
          fail rather quickly once that has been reached. I am confused as to whether
          this model is capable of utilizing the 8k context tokens I have seen articles
          talk about, if so what config settings do I need to modify or what generation
          method do I need to try? </p>

          <p>Any help is much appreciated!</p>

          '
        raw: "I am having a difficult time with increasing the conversation context\
          \ length. It appears to be set at a default of 512 tokens and does start\
          \ to fail rather quickly once that has been reached. I am confused as to\
          \ whether this model is capable of utilizing the 8k context tokens I have\
          \ seen articles talk about, if so what config settings do I need to modify\
          \ or what generation method do I need to try? \r\n\r\nAny help is much appreciated!"
        updatedAt: '2023-10-26T17:19:54.297Z'
      numEdits: 0
      reactions: []
    id: 653a9fba42bfd8801c35195f
    type: comment
  author: Rhino224
  content: "I am having a difficult time with increasing the conversation context\
    \ length. It appears to be set at a default of 512 tokens and does start to fail\
    \ rather quickly once that has been reached. I am confused as to whether this\
    \ model is capable of utilizing the 8k context tokens I have seen articles talk\
    \ about, if so what config settings do I need to modify or what generation method\
    \ do I need to try? \r\n\r\nAny help is much appreciated!"
  created_at: 2023-10-26 16:19:54+00:00
  edited: false
  hidden: false
  id: 653a9fba42bfd8801c35195f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-10-26T18:17:28.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9832543730735779
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I didn''t have this problem, I just used oobabooga and set max tokens
          to 3000 and it worked.<br>I am not sure what your runtime environment looks
          like.</p>

          '
        raw: 'I didn''t have this problem, I just used oobabooga and set max tokens
          to 3000 and it worked.

          I am not sure what your runtime environment looks like.'
        updatedAt: '2023-10-26T18:17:28.207Z'
      numEdits: 0
      reactions: []
    id: 653aad385cd715dafdba65a1
    type: comment
  author: ehartford
  content: 'I didn''t have this problem, I just used oobabooga and set max tokens
    to 3000 and it worked.

    I am not sure what your runtime environment looks like.'
  created_at: 2023-10-26 17:17:28+00:00
  edited: false
  hidden: false
  id: 653aad385cd715dafdba65a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64befd29bd099f855ffd5ae08aa14b30.svg
      fullname: Ryan Thayer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rhino224
      type: user
    createdAt: '2023-10-28T14:22:01.000Z'
    data:
      edited: false
      editors:
      - Rhino224
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9056500196456909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64befd29bd099f855ffd5ae08aa14b30.svg
          fullname: Ryan Thayer
          isHf: false
          isPro: false
          name: Rhino224
          type: user
        html: '<p>Ok, I will try out oobabooga''s web UI to test it out more. Running
          it in Python with ctransformers using PyCharm. I have just tried using it
          with the LLM() method with the prompt template. I haven''t ran it with pipe
          or breaking down into tokens yet, which may be the problem? If the model
          works with oobabooga, then I assume it should be able to work just fine
          in my local environment. Thank you for the response!</p>

          '
        raw: Ok, I will try out oobabooga's web UI to test it out more. Running it
          in Python with ctransformers using PyCharm. I have just tried using it with
          the LLM() method with the prompt template. I haven't ran it with pipe or
          breaking down into tokens yet, which may be the problem? If the model works
          with oobabooga, then I assume it should be able to work just fine in my
          local environment. Thank you for the response!
        updatedAt: '2023-10-28T14:22:01.609Z'
      numEdits: 0
      reactions: []
    id: 653d1909e8ed050cb3155a9f
    type: comment
  author: Rhino224
  content: Ok, I will try out oobabooga's web UI to test it out more. Running it in
    Python with ctransformers using PyCharm. I have just tried using it with the LLM()
    method with the prompt template. I haven't ran it with pipe or breaking down into
    tokens yet, which may be the problem? If the model works with oobabooga, then
    I assume it should be able to work just fine in my local environment. Thank you
    for the response!
  created_at: 2023-10-28 13:22:01+00:00
  edited: false
  hidden: false
  id: 653d1909e8ed050cb3155a9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/64befd29bd099f855ffd5ae08aa14b30.svg
      fullname: Ryan Thayer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rhino224
      type: user
    createdAt: '2023-11-04T05:29:37.000Z'
    data:
      status: closed
    id: 6545d6c18767484a0504ad9f
    type: status-change
  author: Rhino224
  created_at: 2023-11-04 04:29:37+00:00
  id: 6545d6c18767484a0504ad9f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: cognitivecomputations/dolphin-2.1-mistral-7b
repo_type: model
status: closed
target_branch: null
title: Confusion with conversation context length
