!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-10-25 16:08:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-25T17:08:16.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4573909044265747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<hr>

          <p>ValueError                                Traceback (most recent call
          last)<br>Cell In[57], line 1<br>----&gt; 1 tokenizer = AutoTokenizer.from_pretrained("ehartford/dolphin-2.1-mistral-7b")</p>

          <p>File H:\py310-venv\lib\site-packages\transformers\models\auto\tokenization_auto.py:694,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    690     if tokenizer_class is None:<br>    691         raise
          ValueError(<br>    692             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    693         )<br>--&gt;
          694     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    696 # Otherwise we have to be creative.<br>    697
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    698 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File H:\py310-venv\lib\site-packages\transformers\tokenization_utils_base.py:1812,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          *init_inputs, **kwargs)<br>   1809     else:<br>   1810         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1812 return cls._from_pretrained(<br>   1813     resolved_vocab_files,<br>   1814     pretrained_model_name_or_path,<br>   1815     init_configuration,<br>   1816     *init_inputs,<br>   1817     use_auth_token=use_auth_token,<br>   1818     cache_dir=cache_dir,<br>   1819     local_files_only=local_files_only,<br>   1820     _commit_hash=commit_hash,<br>   1821     _is_local=is_local,<br>   1822     **kwargs,<br>   1823
          )</p>

          <p>File H:\py310-venv\lib\site-packages\transformers\tokenization_utils_base.py:1844,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          _is_local, *init_inputs, **kwargs)<br>   1842 has_tokenizer_file = resolved_vocab_files.get("tokenizer_file",
          None) is not None<br>   1843 if (from_slow or not has_tokenizer_file) and
          cls.slow_tokenizer_class is not None:<br>-&gt; 1844     slow_tokenizer =
          (cls.slow_tokenizer_class)._from_pretrained(<br>   1845         copy.deepcopy(resolved_vocab_files),<br>   1846         pretrained_model_name_or_path,<br>   1847         copy.deepcopy(init_configuration),<br>   1848         *init_inputs,<br>   1849         use_auth_token=use_auth_token,<br>   1850         cache_dir=cache_dir,<br>   1851         local_files_only=local_files_only,<br>   1852         _commit_hash=_commit_hash,<br>   1853         **(copy.deepcopy(kwargs)),<br>   1854     )<br>   1855
          else:<br>   1856     slow_tokenizer = None</p>

          <p>File H:\py310-venv\lib\site-packages\transformers\tokenization_utils_base.py:2031,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          _is_local, *init_inputs, **kwargs)<br>   2024     raise ValueError(<br>   2025         f"Wrong
          index found for {token}: should be {tokenizer.convert_tokens_to_ids(token)}
          but found "<br>   2026         f"{index}."<br>   2027     )<br>   2028 elif
          not has_tokenizer_file and index != current_index:<br>   2029     # Tokenizer
          slow: added token cannot already be in the vocabulary so its index needs
          to be the<br>   2030     # current length of the tokenizer.<br>-&gt; 2031     raise
          ValueError(<br>   2032         f"Non-consecutive added token ''{token}''
          found. "<br>   2033         f"Should have index {current_index} but has
          index {index} in saved vocabulary."<br>   2034     )<br>   2036 is_special
          = bool(token in special_tokens)<br>   2037 if is_last_special is None or
          is_last_special == is_special:</p>

          <p>ValueError: Non-consecutive added token '''' found. Should have index
          32000 but has index 0 in saved vocabulary.</p>

          '
        raw: "---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[57], line 1\r\n----> 1 tokenizer = AutoTokenizer.from_pretrained(\"\
          ehartford/dolphin-2.1-mistral-7b\")\r\n\r\nFile H:\\py310-venv\\lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py:694, in AutoTokenizer.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *inputs, **kwargs)\r\n    690     if tokenizer_class\
          \ is None:\r\n    691         raise ValueError(\r\n    692             f\"\
          Tokenizer class {tokenizer_class_candidate} does not exist or is not currently\
          \ imported.\"\r\n    693         )\r\n--> 694     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    696 # Otherwise we have to be creative.\r\n\
          \    697 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    698 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile H:\\py310-venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1812,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *init_inputs, **kwargs)\r\n   1809     else:\r\n   1810         logger.info(f\"\
          loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\
          )\r\n-> 1812 return cls._from_pretrained(\r\n   1813     resolved_vocab_files,\r\
          \n   1814     pretrained_model_name_or_path,\r\n   1815     init_configuration,\r\
          \n   1816     *init_inputs,\r\n   1817     use_auth_token=use_auth_token,\r\
          \n   1818     cache_dir=cache_dir,\r\n   1819     local_files_only=local_files_only,\r\
          \n   1820     _commit_hash=commit_hash,\r\n   1821     _is_local=is_local,\r\
          \n   1822     **kwargs,\r\n   1823 )\r\n\r\nFile H:\\py310-venv\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py:1844, in PreTrainedTokenizerBase._from_pretrained(cls,\
          \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration,\
          \ use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local,\
          \ *init_inputs, **kwargs)\r\n   1842 has_tokenizer_file = resolved_vocab_files.get(\"\
          tokenizer_file\", None) is not None\r\n   1843 if (from_slow or not has_tokenizer_file)\
          \ and cls.slow_tokenizer_class is not None:\r\n-> 1844     slow_tokenizer\
          \ = (cls.slow_tokenizer_class)._from_pretrained(\r\n   1845         copy.deepcopy(resolved_vocab_files),\r\
          \n   1846         pretrained_model_name_or_path,\r\n   1847         copy.deepcopy(init_configuration),\r\
          \n   1848         *init_inputs,\r\n   1849         use_auth_token=use_auth_token,\r\
          \n   1850         cache_dir=cache_dir,\r\n   1851         local_files_only=local_files_only,\r\
          \n   1852         _commit_hash=_commit_hash,\r\n   1853         **(copy.deepcopy(kwargs)),\r\
          \n   1854     )\r\n   1855 else:\r\n   1856     slow_tokenizer = None\r\n\
          \r\nFile H:\\py310-venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2031,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\r\n\
          \   2024     raise ValueError(\r\n   2025         f\"Wrong index found for\
          \ {token}: should be {tokenizer.convert_tokens_to_ids(token)} but found\
          \ \"\r\n   2026         f\"{index}.\"\r\n   2027     )\r\n   2028 elif not\
          \ has_tokenizer_file and index != current_index:\r\n   2029     # Tokenizer\
          \ slow: added token cannot already be in the vocabulary so its index needs\
          \ to be the\r\n   2030     # current length of the tokenizer.\r\n-> 2031\
          \     raise ValueError(\r\n   2032         f\"Non-consecutive added token\
          \ '{token}' found. \"\r\n   2033         f\"Should have index {current_index}\
          \ but has index {index} in saved vocabulary.\"\r\n   2034     )\r\n   2036\
          \ is_special = bool(token in special_tokens)\r\n   2037 if is_last_special\
          \ is None or is_last_special == is_special:\r\n\r\nValueError: Non-consecutive\
          \ added token '<unk>' found. Should have index 32000 but has index 0 in\
          \ saved vocabulary."
        updatedAt: '2023-10-25T17:08:16.690Z'
      numEdits: 0
      reactions: []
    id: 65394b80690022ca51da6a19
    type: comment
  author: LaferriereJC
  content: "---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[57], line 1\r\n----> 1 tokenizer = AutoTokenizer.from_pretrained(\"\
    ehartford/dolphin-2.1-mistral-7b\")\r\n\r\nFile H:\\py310-venv\\lib\\site-packages\\\
    transformers\\models\\auto\\tokenization_auto.py:694, in AutoTokenizer.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *inputs, **kwargs)\r\n    690     if tokenizer_class\
    \ is None:\r\n    691         raise ValueError(\r\n    692             f\"Tokenizer\
    \ class {tokenizer_class_candidate} does not exist or is not currently imported.\"\
    \r\n    693         )\r\n--> 694     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    696 # Otherwise we have to be creative.\r\n    697\
    \ # if model is an encoder decoder, the encoder tokenizer class is used by default\r\
    \n    698 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile H:\\py310-venv\\\
    lib\\site-packages\\transformers\\tokenization_utils_base.py:1812, in PreTrainedTokenizerBase.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *init_inputs, **kwargs)\r\n   1809     else:\r\
    \n   1810         logger.info(f\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\
    )\r\n-> 1812 return cls._from_pretrained(\r\n   1813     resolved_vocab_files,\r\
    \n   1814     pretrained_model_name_or_path,\r\n   1815     init_configuration,\r\
    \n   1816     *init_inputs,\r\n   1817     use_auth_token=use_auth_token,\r\n\
    \   1818     cache_dir=cache_dir,\r\n   1819     local_files_only=local_files_only,\r\
    \n   1820     _commit_hash=commit_hash,\r\n   1821     _is_local=is_local,\r\n\
    \   1822     **kwargs,\r\n   1823 )\r\n\r\nFile H:\\py310-venv\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py:1844, in PreTrainedTokenizerBase._from_pretrained(cls,\
    \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token,\
    \ cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\r\
    \n   1842 has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None)\
    \ is not None\r\n   1843 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class\
    \ is not None:\r\n-> 1844     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
    \n   1845         copy.deepcopy(resolved_vocab_files),\r\n   1846         pretrained_model_name_or_path,\r\
    \n   1847         copy.deepcopy(init_configuration),\r\n   1848         *init_inputs,\r\
    \n   1849         use_auth_token=use_auth_token,\r\n   1850         cache_dir=cache_dir,\r\
    \n   1851         local_files_only=local_files_only,\r\n   1852         _commit_hash=_commit_hash,\r\
    \n   1853         **(copy.deepcopy(kwargs)),\r\n   1854     )\r\n   1855 else:\r\
    \n   1856     slow_tokenizer = None\r\n\r\nFile H:\\py310-venv\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py:2031, in PreTrainedTokenizerBase._from_pretrained(cls,\
    \ resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token,\
    \ cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\r\
    \n   2024     raise ValueError(\r\n   2025         f\"Wrong index found for {token}:\
    \ should be {tokenizer.convert_tokens_to_ids(token)} but found \"\r\n   2026 \
    \        f\"{index}.\"\r\n   2027     )\r\n   2028 elif not has_tokenizer_file\
    \ and index != current_index:\r\n   2029     # Tokenizer slow: added token cannot\
    \ already be in the vocabulary so its index needs to be the\r\n   2030     # current\
    \ length of the tokenizer.\r\n-> 2031     raise ValueError(\r\n   2032       \
    \  f\"Non-consecutive added token '{token}' found. \"\r\n   2033         f\"Should\
    \ have index {current_index} but has index {index} in saved vocabulary.\"\r\n\
    \   2034     )\r\n   2036 is_special = bool(token in special_tokens)\r\n   2037\
    \ if is_last_special is None or is_last_special == is_special:\r\n\r\nValueError:\
    \ Non-consecutive added token '<unk>' found. Should have index 32000 but has index\
    \ 0 in saved vocabulary."
  created_at: 2023-10-25 16:08:16+00:00
  edited: false
  hidden: false
  id: 65394b80690022ca51da6a19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44f4a710b4c123827a26c37fb9f3bf9b.svg
      fullname: Simon Dennis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: professor451451
      type: user
    createdAt: '2023-10-27T22:33:35.000Z'
    data:
      edited: false
      editors:
      - professor451451
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8869252800941467
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44f4a710b4c123827a26c37fb9f3bf9b.svg
          fullname: Simon Dennis
          isHf: false
          isPro: false
          name: professor451451
          type: user
        html: '<p>I get this same error trying to deploy on hugginface endpoints</p>

          '
        raw: I get this same error trying to deploy on hugginface endpoints
        updatedAt: '2023-10-27T22:33:35.323Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ConteMascetti
        - PsiPi
    id: 653c3abfe971d8e61f318a57
    type: comment
  author: professor451451
  content: I get this same error trying to deploy on hugginface endpoints
  created_at: 2023-10-27 21:33:35+00:00
  edited: false
  hidden: false
  id: 653c3abfe971d8e61f318a57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-11-20T02:40:10.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4282703697681427
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: '<p>same</p>

          '
        raw: 'same

          '
        updatedAt: '2023-11-20T02:40:10.474Z'
      numEdits: 0
      reactions: []
    id: 655ac70a8b3f76d5330fbb68
    type: comment
  author: PsiPi
  content: 'same

    '
  created_at: 2023-11-20 02:40:10+00:00
  edited: false
  hidden: false
  id: 655ac70a8b3f76d5330fbb68
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: cognitivecomputations/dolphin-2.1-mistral-7b
repo_type: model
status: open
target_branch: null
title: tokenizer = AutoTokenizer.from_pretrained("ehartford/dolphin-2.1-mistral-7b")
  results in unk error related to tokens greater than 32000
