!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andresenric
conflicting_files: null
created_at: 2023-12-08 15:20:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c5435071947b03fff2a6ad/8wih2kohn_ocB0gbqebZq.jpeg?w=200&h=200&f=face
      fullname: Andres Restrepo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andresenric
      type: user
    createdAt: '2023-12-08T15:20:34.000Z'
    data:
      edited: true
      editors:
      - andresenric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6721958518028259
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c5435071947b03fff2a6ad/8wih2kohn_ocB0gbqebZq.jpeg?w=200&h=200&f=face
          fullname: Andres Restrepo
          isHf: false
          isPro: false
          name: andresenric
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ . Firstly, thanks for incredible work! Much appreciated from me and I'm\
          \ sure the rest of the community.</p>\n<p>Onto training...<br>In our tests\
          \ we get about 80% accuracy with this model for our classification task\
          \ without training. We set out to train with the configuration below and\
          \ inspired by this <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing#scrollTo=Cmll3BZntI88\"\
          >guide</a>  and the <a href=\"https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GPTQ#prompt-template-chatml\"\
          >prompt you suggest in the model card</a>.<br>However, after training the\
          \ model:</p>\n<ol>\n<li>Inference takes a way long time</li>\n<li>The output\
          \ is not at all like the good classification labels we get without training.\
          \ With training we get parts of the prompt template, way long output, etc,\
          \ in the output.</li>\n</ol>\n<p>We would like to know if you had any ideas\
          \ or suggestions on how we can train and get an improved result, not a worse\
          \ result like we are now.</p>\n<p>Here's the config</p>\n<pre><code>  tokenizer\
          \ = AutoTokenizer.from_pretrained(model_id)\n  tokenizer.pad_token = tokenizer.eos_token\n\
          \  quantization_config_loading = GPTQConfig(\n    bits=4,\n    disable_exllama=True,\n\
          \    group_size=128,\n    desc_act=False,\n    dataset=\"c4\",\n    tokenizer=tokenizer,\n\
          \  )\n  model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n \
          \   quantization_config=quantization_config_loading,\n    device_map=\"\
          auto\",\n  )\n\n  model.gradient_checkpointing_enable()\n  model = prepare_model_for_kbit_training(model)\n\
          \n  config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"\
          k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n    lora_dropout=0.05,\n  \
          \  bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n  )\n\n  model = get_peft_model(model,\
          \ config)\n</code></pre>\n<pre><code>  args = TrainingArguments(\n    output_dir=\"\
          checkpoints\",\n    overwrite_output_dir=True,\n    evaluation_strategy=\"\
          steps\",\n    optim=\"adamw_torch\",\n    weight_decay=weight_decay,\n \
          \   per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n\
          \    learning_rate=learning_rate,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \    gradient_checkpointing=enable_gradient_checkpointing,\n  )\n  args\
          \ = args.set_lr_scheduler(\n    name=decay,\n    warmup_ratio=warmup_fraction,\n\
          \    num_epochs=num_epochs,\n  )\n  trainer = Trainer(\n    model=model,\n\
          \    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"\
          ],\n    args=args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False)\n  )\n  trainer.train()\n</code></pre>\n<p>Thank you in advance\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> \U0001F64C\U0001F64C\
          </p>\n"
        raw: "Hi @TheBloke . Firstly, thanks for incredible work! Much appreciated\
          \ from me and I'm sure the rest of the community.\n\nOnto training...\n\
          In our tests we get about 80% accuracy with this model for our classification\
          \ task without training. We set out to train with the configuration below\
          \ and inspired by this [guide](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing#scrollTo=Cmll3BZntI88)\
          \  and the [prompt you suggest in the model card](https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GPTQ#prompt-template-chatml).\n\
          However, after training the model:\n1. Inference takes a way long time\n\
          2. The output is not at all like the good classification labels we get without\
          \ training. With training we get parts of the prompt template, way long\
          \ output, etc, in the output.\n\nWe would like to know if you had any ideas\
          \ or suggestions on how we can train and get an improved result, not a worse\
          \ result like we are now.\n\nHere's the config\n\n```\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          \  tokenizer.pad_token = tokenizer.eos_token\n  quantization_config_loading\
          \ = GPTQConfig(\n    bits=4,\n    disable_exllama=True,\n    group_size=128,\n\
          \    desc_act=False,\n    dataset=\"c4\",\n    tokenizer=tokenizer,\n  )\n\
          \  model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config_loading,\n\
          \    device_map=\"auto\",\n  )\n\n  model.gradient_checkpointing_enable()\n\
          \  model = prepare_model_for_kbit_training(model)\n\n  config = LoraConfig(\n\
          \    r=8,\n    lora_alpha=16,\n    target_modules=[\"k_proj\",\"o_proj\"\
          ,\"q_proj\",\"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n \
          \   task_type=\"CAUSAL_LM\"\n  )\n\n  model = get_peft_model(model, config)\n\
          ```\n\n\n```\n  args = TrainingArguments(\n    output_dir=\"checkpoints\"\
          ,\n    overwrite_output_dir=True,\n    evaluation_strategy=\"steps\",\n\
          \    optim=\"adamw_torch\",\n    weight_decay=weight_decay,\n    per_device_train_batch_size=batch_size,\n\
          \    per_device_eval_batch_size=eval_batch_size,\n    learning_rate=learning_rate,\n\
          \    gradient_accumulation_steps=gradient_accumulation_steps,\n    gradient_checkpointing=enable_gradient_checkpointing,\n\
          \  )\n  args = args.set_lr_scheduler(\n    name=decay,\n    warmup_ratio=warmup_fraction,\n\
          \    num_epochs=num_epochs,\n  )\n  trainer = Trainer(\n    model=model,\n\
          \    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"\
          ],\n    args=args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False)\n  )\n  trainer.train()\n```\n\nThank you in advance @TheBloke\
          \ \U0001F64C\U0001F64C"
        updatedAt: '2023-12-08T18:20:08.068Z'
      numEdits: 1
      reactions: []
    id: 65733442f9898ed3ab26a3bd
    type: comment
  author: andresenric
  content: "Hi @TheBloke . Firstly, thanks for incredible work! Much appreciated from\
    \ me and I'm sure the rest of the community.\n\nOnto training...\nIn our tests\
    \ we get about 80% accuracy with this model for our classification task without\
    \ training. We set out to train with the configuration below and inspired by this\
    \ [guide](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing#scrollTo=Cmll3BZntI88)\
    \  and the [prompt you suggest in the model card](https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GPTQ#prompt-template-chatml).\n\
    However, after training the model:\n1. Inference takes a way long time\n2. The\
    \ output is not at all like the good classification labels we get without training.\
    \ With training we get parts of the prompt template, way long output, etc, in\
    \ the output.\n\nWe would like to know if you had any ideas or suggestions on\
    \ how we can train and get an improved result, not a worse result like we are\
    \ now.\n\nHere's the config\n\n```\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
    \  tokenizer.pad_token = tokenizer.eos_token\n  quantization_config_loading =\
    \ GPTQConfig(\n    bits=4,\n    disable_exllama=True,\n    group_size=128,\n \
    \   desc_act=False,\n    dataset=\"c4\",\n    tokenizer=tokenizer,\n  )\n  model\
    \ = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config_loading,\n\
    \    device_map=\"auto\",\n  )\n\n  model.gradient_checkpointing_enable()\n  model\
    \ = prepare_model_for_kbit_training(model)\n\n  config = LoraConfig(\n    r=8,\n\
    \    lora_alpha=16,\n    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"\
    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n\
    \  )\n\n  model = get_peft_model(model, config)\n```\n\n\n```\n  args = TrainingArguments(\n\
    \    output_dir=\"checkpoints\",\n    overwrite_output_dir=True,\n    evaluation_strategy=\"\
    steps\",\n    optim=\"adamw_torch\",\n    weight_decay=weight_decay,\n    per_device_train_batch_size=batch_size,\n\
    \    per_device_eval_batch_size=eval_batch_size,\n    learning_rate=learning_rate,\n\
    \    gradient_accumulation_steps=gradient_accumulation_steps,\n    gradient_checkpointing=enable_gradient_checkpointing,\n\
    \  )\n  args = args.set_lr_scheduler(\n    name=decay,\n    warmup_ratio=warmup_fraction,\n\
    \    num_epochs=num_epochs,\n  )\n  trainer = Trainer(\n    model=model,\n   \
    \ train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n   \
    \ args=args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\
    \  )\n  trainer.train()\n```\n\nThank you in advance @TheBloke \U0001F64C\U0001F64C"
  created_at: 2023-12-08 15:20:34+00:00
  edited: true
  hidden: false
  id: 65733442f9898ed3ab26a3bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Training with Lora producing wacky output
