!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pheuter
conflicting_files: null
created_at: 2023-05-23 14:26:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1syU0u1DaJe2I-hcPRmCs.jpeg?w=200&h=200&f=face
      fullname: Mark Fayngersh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pheuter
      type: user
    createdAt: '2023-05-23T15:26:18.000Z'
    data:
      edited: false
      editors:
      - pheuter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1syU0u1DaJe2I-hcPRmCs.jpeg?w=200&h=200&f=face
          fullname: Mark Fayngersh
          isHf: false
          isPro: false
          name: pheuter
          type: user
        html: '<p>The runtime appears to run out of memory when executing:</p>

          <pre><code class="language-python">model = LlamaForCausalLM.from_pretrained(<span
          class="hljs-string">"ausboss/llama-30b-supercot"</span>).to(<span class="hljs-string">"cuda"</span>)

          </code></pre>

          <p>I''m relatively new to HF and LLMs in general, am I missing something
          obvious in how I''m going about trying to use this model? If I have access
          to multiple GPUs at once, is there a way to load this model across multiple
          GPUs?</p>

          <p>Any guidance appreciated!</p>

          '
        raw: "The runtime appears to run out of memory when executing:\r\n\r\n```python\r\
          \nmodel = LlamaForCausalLM.from_pretrained(\"ausboss/llama-30b-supercot\"\
          ).to(\"cuda\")\r\n```\r\n\r\nI'm relatively new to HF and LLMs in general,\
          \ am I missing something obvious in how I'm going about trying to use this\
          \ model? If I have access to multiple GPUs at once, is there a way to load\
          \ this model across multiple GPUs?\r\n\r\nAny guidance appreciated!"
        updatedAt: '2023-05-23T15:26:18.269Z'
      numEdits: 0
      reactions: []
    id: 646cdb1a2e6b41386b487159
    type: comment
  author: pheuter
  content: "The runtime appears to run out of memory when executing:\r\n\r\n```python\r\
    \nmodel = LlamaForCausalLM.from_pretrained(\"ausboss/llama-30b-supercot\").to(\"\
    cuda\")\r\n```\r\n\r\nI'm relatively new to HF and LLMs in general, am I missing\
    \ something obvious in how I'm going about trying to use this model? If I have\
    \ access to multiple GPUs at once, is there a way to load this model across multiple\
    \ GPUs?\r\n\r\nAny guidance appreciated!"
  created_at: 2023-05-23 14:26:18+00:00
  edited: false
  hidden: false
  id: 646cdb1a2e6b41386b487159
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5299aa98e9a695222427112494e3ad29.svg
      fullname: "El Mehdi Bou\xE2mama"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ComputroniumDev
      type: user
    createdAt: '2023-05-24T12:04:08.000Z'
    data:
      edited: true
      editors:
      - ComputroniumDev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5299aa98e9a695222427112494e3ad29.svg
          fullname: "El Mehdi Bou\xE2mama"
          isHf: false
          isPro: false
          name: ComputroniumDev
          type: user
        html: '<p>Memory / Disk requirements:<br><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp#memorydisk-requirements">https://github.com/ggerganov/llama.cpp#memorydisk-requirements</a></p>

          <p>Make sure you have enough Memory for 30B parameter models, it''s 60 GB
          if the model isn''t quantized. And if it is quantized you''re gonna need
          19.5 GB.<br>Keep in mind, this is the minimum memory required for 30B parameter
          models</p>

          '
        raw: "Memory / Disk requirements: \nhttps://github.com/ggerganov/llama.cpp#memorydisk-requirements\n\
          \nMake sure you have enough Memory for 30B parameter models, it's 60 GB\
          \ if the model isn't quantized. And if it is quantized you're gonna need\
          \ 19.5 GB.\nKeep in mind, this is the minimum memory required for 30B parameter\
          \ models"
        updatedAt: '2023-05-24T12:05:49.186Z'
      numEdits: 2
      reactions: []
    id: 646dfd385c3c0df5aef54e63
    type: comment
  author: ComputroniumDev
  content: "Memory / Disk requirements: \nhttps://github.com/ggerganov/llama.cpp#memorydisk-requirements\n\
    \nMake sure you have enough Memory for 30B parameter models, it's 60 GB if the\
    \ model isn't quantized. And if it is quantized you're gonna need 19.5 GB.\nKeep\
    \ in mind, this is the minimum memory required for 30B parameter models"
  created_at: 2023-05-24 11:04:08+00:00
  edited: true
  hidden: false
  id: 646dfd385c3c0df5aef54e63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1syU0u1DaJe2I-hcPRmCs.jpeg?w=200&h=200&f=face
      fullname: Mark Fayngersh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pheuter
      type: user
    createdAt: '2023-05-24T14:59:17.000Z'
    data:
      edited: false
      editors:
      - pheuter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1syU0u1DaJe2I-hcPRmCs.jpeg?w=200&h=200&f=face
          fullname: Mark Fayngersh
          isHf: false
          isPro: false
          name: pheuter
          type: user
        html: '<p>Definitely have enough memory for it, have 80GB in fact, but that
          still appears to not be enough for this model.</p>

          '
        raw: Definitely have enough memory for it, have 80GB in fact, but that still
          appears to not be enough for this model.
        updatedAt: '2023-05-24T14:59:17.508Z'
      numEdits: 0
      reactions: []
    id: 646e26455c3c0df5aefb10ce
    type: comment
  author: pheuter
  content: Definitely have enough memory for it, have 80GB in fact, but that still
    appears to not be enough for this model.
  created_at: 2023-05-24 13:59:17+00:00
  edited: false
  hidden: false
  id: 646e26455c3c0df5aefb10ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: slotreck
      type: user
    createdAt: '2023-05-25T16:46:59.000Z'
    data:
      edited: true
      editors:
      - slotreck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: slotreck
          type: user
        html: "<p>No idea if this will be helpful in your case, but I found the other\
          \ day that I would get OOM errors if I didn't have <code>torch</code> compiled\
          \ with GPU support; once I went and re-installed torch via <a rel=\"nofollow\"\
          \ href=\"https://pytorch.org/get-started/locally/\">their instructions</a>,\
          \ those errors went away!</p>\n<p>EDIT: Also wanted to say that in case\
          \ it's truly an issue with not having enough memory, the quantization <span\
          \ data-props=\"{&quot;user&quot;:&quot;ComputroniumDev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ComputroniumDev\">@<span\
          \ class=\"underline\">ComputroniumDev</span></a></span>\n\n\t</span></span>\
          \ referred to is explained <a href=\"https://huggingface.co/docs/transformers/main/main_classes/quantization\"\
          >here</a>, and that also helped me quite a bit! There are some models I'm\
          \ only able to load in 8 bit, and can't load without it.</p>\n"
        raw: 'No idea if this will be helpful in your case, but I found the other
          day that I would get OOM errors if I didn''t have `torch` compiled with
          GPU support; once I went and re-installed torch via [their instructions](https://pytorch.org/get-started/locally/),
          those errors went away!


          EDIT: Also wanted to say that in case it''s truly an issue with not having
          enough memory, the quantization @ComputroniumDev referred to is explained
          [here](https://huggingface.co/docs/transformers/main/main_classes/quantization),
          and that also helped me quite a bit! There are some models I''m only able
          to load in 8 bit, and can''t load without it.'
        updatedAt: '2023-05-25T16:51:36.838Z'
      numEdits: 1
      reactions: []
    id: 646f9103bc42f4b00230d5a6
    type: comment
  author: slotreck
  content: 'No idea if this will be helpful in your case, but I found the other day
    that I would get OOM errors if I didn''t have `torch` compiled with GPU support;
    once I went and re-installed torch via [their instructions](https://pytorch.org/get-started/locally/),
    those errors went away!


    EDIT: Also wanted to say that in case it''s truly an issue with not having enough
    memory, the quantization @ComputroniumDev referred to is explained [here](https://huggingface.co/docs/transformers/main/main_classes/quantization),
    and that also helped me quite a bit! There are some models I''m only able to load
    in 8 bit, and can''t load without it.'
  created_at: 2023-05-25 15:46:59+00:00
  edited: true
  hidden: false
  id: 646f9103bc42f4b00230d5a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ausboss/llama-30b-supercot
repo_type: model
status: open
target_branch: null
title: Trouble running on an H100 (80GB PCIe), runs out of memory when loading to
  cuda device
