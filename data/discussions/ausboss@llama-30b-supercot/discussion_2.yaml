!!python/object:huggingface_hub.community.DiscussionWithDetails
author: slotreck
conflicting_files: null
created_at: 2023-05-25 16:05:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: slotreck
      type: user
    createdAt: '2023-05-25T17:05:59.000Z'
    data:
      edited: true
      editors:
      - slotreck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: slotreck
          type: user
        html: "<p>I'm loading and deploying my model with the following code:</p>\n\
          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import torch\n\ncheckpoint = 'ausboss/llama-30b-supercot'\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\n\
          \    torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nprompt = &lt;see\
          \ below&gt;\n\ninputs = tokenizer(prompt, return_tensors='pt')\ninputs =\
          \ inputs.to(0)\noutput = model.generate(inputs['input_ids'], max_new_tokens=500)\n\
          response = tokenizer.decode(output[0].tolist())\n</code></pre>\n<p>Using\
          \ a prompt that follows the format in the model code:</p>\n<pre><code>prompt\
          \ = \"\"\"\nBelow is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\n\n### Instruction:\nExtract\
          \ the biological relations from the following text as (Subject, Predicate,\
          \ Object) triples in the format (\"Subject\", \"predicate\", \"Object\"\
          ):\n\n### Input:\nSalmeterol is a long-acting beta2-adrenergic receptor\
          \ (beta 2AR) agonist used clinically to treat asthma.\n\n### Response:\n\
          \n\"\"\"\n</code></pre>\n<p>EDIT: Corrected the number of newlines in my\
          \ prompt</p>\n<p>However, the model response only ever contains unknown\
          \ tokens:</p>\n<pre><code>&lt;s&gt; \nBelow is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\n###\
          \ Instruction:\nExtract the biological relations from the following text\
          \ as (Subject, Predicate, Object) triples in the format\n (\"Subject\",\
          \ \"predicate\", \"Object\"):\n### Input:\nSalmeterol is a long-acting beta2-adrenergic\
          \ receptor (beta 2AR) agonist used clinically to treat asthma.\n### Response:\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          &lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;&lt;unk&gt;\n\
          </code></pre>\n<p>What's going on here, and how can I fix it?</p>\n"
        raw: "I'm loading and deploying my model with the following code:\n```\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\
          \ncheckpoint = 'ausboss/llama-30b-supercot'\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\n\
          \    torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nprompt = <see below>\n\
          \ninputs = tokenizer(prompt, return_tensors='pt')\ninputs = inputs.to(0)\n\
          output = model.generate(inputs['input_ids'], max_new_tokens=500)\nresponse\
          \ = tokenizer.decode(output[0].tolist())\n```\n\nUsing a prompt that follows\
          \ the format in the model code:\n```\nprompt = \"\"\"\nBelow is an instruction\
          \ that describes a task. Write a response that appropriately completes the\
          \ request.\n\n### Instruction:\nExtract the biological relations from the\
          \ following text as (Subject, Predicate, Object) triples in the format (\"\
          Subject\", \"predicate\", \"Object\"):\n\n### Input:\nSalmeterol is a long-acting\
          \ beta2-adrenergic receptor (beta 2AR) agonist used clinically to treat\
          \ asthma.\n\n### Response:\n\n\"\"\"\n```\nEDIT: Corrected the number of\
          \ newlines in my prompt\n\nHowever, the model response only ever contains\
          \ unknown tokens:\n```\n<s> \nBelow is an instruction that describes a task.\
          \ Write a response that appropriately completes the request.\n### Instruction:\n\
          Extract the biological relations from the following text as (Subject, Predicate,\
          \ Object) triples in the format\n (\"Subject\", \"predicate\", \"Object\"\
          ):\n### Input:\nSalmeterol is a long-acting beta2-adrenergic receptor (beta\
          \ 2AR) agonist used clinically to treat asthma.\n### Response:\n<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
          ```\n\nWhat's going on here, and how can I fix it?"
        updatedAt: '2023-05-25T17:06:58.296Z'
      numEdits: 1
      reactions: []
    id: 646f95779ba7aeaacb2536fc
    type: comment
  author: slotreck
  content: "I'm loading and deploying my model with the following code:\n```\nfrom\
    \ transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ncheckpoint\
    \ = 'ausboss/llama-30b-supercot'\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\n\
    \    torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)\ntokenizer\
    \ = AutoTokenizer.from_pretrained(checkpoint)\n\nprompt = <see below>\n\ninputs\
    \ = tokenizer(prompt, return_tensors='pt')\ninputs = inputs.to(0)\noutput = model.generate(inputs['input_ids'],\
    \ max_new_tokens=500)\nresponse = tokenizer.decode(output[0].tolist())\n```\n\n\
    Using a prompt that follows the format in the model code:\n```\nprompt = \"\"\"\
    \nBelow is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\n\n### Instruction:\nExtract the biological relations\
    \ from the following text as (Subject, Predicate, Object) triples in the format\
    \ (\"Subject\", \"predicate\", \"Object\"):\n\n### Input:\nSalmeterol is a long-acting\
    \ beta2-adrenergic receptor (beta 2AR) agonist used clinically to treat asthma.\n\
    \n### Response:\n\n\"\"\"\n```\nEDIT: Corrected the number of newlines in my prompt\n\
    \nHowever, the model response only ever contains unknown tokens:\n```\n<s> \n\
    Below is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\n### Instruction:\nExtract the biological relations from\
    \ the following text as (Subject, Predicate, Object) triples in the format\n (\"\
    Subject\", \"predicate\", \"Object\"):\n### Input:\nSalmeterol is a long-acting\
    \ beta2-adrenergic receptor (beta 2AR) agonist used clinically to treat asthma.\n\
    ### Response:\n<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n\
    ```\n\nWhat's going on here, and how can I fix it?"
  created_at: 2023-05-25 16:05:59+00:00
  edited: true
  hidden: false
  id: 646f95779ba7aeaacb2536fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
      fullname: Om
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asach
      type: user
    createdAt: '2023-05-26T12:35:20.000Z'
    data:
      edited: false
      editors:
      - asach
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
          fullname: Om
          isHf: false
          isPro: false
          name: asach
          type: user
        html: "<p>Its working properly but still no proper answers</p>\n<pre><code>&lt;s&gt;\
          \ Plan a trip to Mumbai for 5 days.\nMumbai is the capital city of the Indian\
          \ state of Maharashtra. As of 2011 it is the most populous city in India\
          \ and the ninth most populous agglomeration in the world, with an estimated\
          \ city population of 12.4 million. Along with the neighbouring regions of\
          \ the Mumbai Metropolitan Region, it is the second most populous metropolitan\
          \ area in India, with a population of 21 million. Mumbai lies on the west\
          \ coast of India and has a deep natural harbour. In 2008, Mumbai was named\
          \ an alpha world city. It is also the wealthiest city in India, and has\
          \ the highest GDP of any city in South, West, or Central Asia. Mumbai has\
          \ the highest number of billionaires and millionaires among all cities in\
          \ India. The seven islands that came to constitute Mumbai were home to fishing\
          \ colonies. In the third century BCE, the islands formed part of the Maurya\
          \ Empire, and later they were ruled by the Satavahanas of Kolhapur (2nd\
          \ century BCE). The kings of Silhara dynasty, under whom the islands were\
          \ developed as a port, called the region \"Gotham\" and \"Gotham Nagari\"\
          . The Greek historian Ptolemy (AD 90\u2013168) referred to the islands as\
          \ \"Nosala\" and \"Nosala\". In the 3rd century CE, the islands were ruled\
          \ by the Abhira dynasty. The region was later controlled by the Gupta Empire\
          \ in the 4th century CE. From the 8th century to the 14th century, the islands\
          \ came under the control of several indigenous dynasties: the Rashtrakuta\
          \ dynasty; the Chalukya dynasty; the Silhara dynasty and the Yadava dynasty\
          \ of Devagiri. After 1343, the islands were ruled by the Muslim rulers of\
          \ Gujarat Sultanate. On 23 January 1534, Bahadur Shah, the Sultan of Gujarat,\
          \ ceded the islands to the Portuguese Empire as per the Treaty of Bassein.\
          \ The islands obtained the name Mumbai, from the name of the local Goddess\
          \ Mumba Devi. The city\n</code></pre>\n"
        raw: "Its working properly but still no proper answers\n\n```\n<s> Plan a\
          \ trip to Mumbai for 5 days.\nMumbai is the capital city of the Indian state\
          \ of Maharashtra. As of 2011 it is the most populous city in India and the\
          \ ninth most populous agglomeration in the world, with an estimated city\
          \ population of 12.4 million. Along with the neighbouring regions of the\
          \ Mumbai Metropolitan Region, it is the second most populous metropolitan\
          \ area in India, with a population of 21 million. Mumbai lies on the west\
          \ coast of India and has a deep natural harbour. In 2008, Mumbai was named\
          \ an alpha world city. It is also the wealthiest city in India, and has\
          \ the highest GDP of any city in South, West, or Central Asia. Mumbai has\
          \ the highest number of billionaires and millionaires among all cities in\
          \ India. The seven islands that came to constitute Mumbai were home to fishing\
          \ colonies. In the third century BCE, the islands formed part of the Maurya\
          \ Empire, and later they were ruled by the Satavahanas of Kolhapur (2nd\
          \ century BCE). The kings of Silhara dynasty, under whom the islands were\
          \ developed as a port, called the region \"Gotham\" and \"Gotham Nagari\"\
          . The Greek historian Ptolemy (AD 90\u2013168) referred to the islands as\
          \ \"Nosala\" and \"Nosala\". In the 3rd century CE, the islands were ruled\
          \ by the Abhira dynasty. The region was later controlled by the Gupta Empire\
          \ in the 4th century CE. From the 8th century to the 14th century, the islands\
          \ came under the control of several indigenous dynasties: the Rashtrakuta\
          \ dynasty; the Chalukya dynasty; the Silhara dynasty and the Yadava dynasty\
          \ of Devagiri. After 1343, the islands were ruled by the Muslim rulers of\
          \ Gujarat Sultanate. On 23 January 1534, Bahadur Shah, the Sultan of Gujarat,\
          \ ceded the islands to the Portuguese Empire as per the Treaty of Bassein.\
          \ The islands obtained the name Mumbai, from the name of the local Goddess\
          \ Mumba Devi. The city\n```"
        updatedAt: '2023-05-26T12:35:20.222Z'
      numEdits: 0
      reactions: []
    id: 6470a7881f0e7ee7fb1a7fb1
    type: comment
  author: asach
  content: "Its working properly but still no proper answers\n\n```\n<s> Plan a trip\
    \ to Mumbai for 5 days.\nMumbai is the capital city of the Indian state of Maharashtra.\
    \ As of 2011 it is the most populous city in India and the ninth most populous\
    \ agglomeration in the world, with an estimated city population of 12.4 million.\
    \ Along with the neighbouring regions of the Mumbai Metropolitan Region, it is\
    \ the second most populous metropolitan area in India, with a population of 21\
    \ million. Mumbai lies on the west coast of India and has a deep natural harbour.\
    \ In 2008, Mumbai was named an alpha world city. It is also the wealthiest city\
    \ in India, and has the highest GDP of any city in South, West, or Central Asia.\
    \ Mumbai has the highest number of billionaires and millionaires among all cities\
    \ in India. The seven islands that came to constitute Mumbai were home to fishing\
    \ colonies. In the third century BCE, the islands formed part of the Maurya Empire,\
    \ and later they were ruled by the Satavahanas of Kolhapur (2nd century BCE).\
    \ The kings of Silhara dynasty, under whom the islands were developed as a port,\
    \ called the region \"Gotham\" and \"Gotham Nagari\". The Greek historian Ptolemy\
    \ (AD 90\u2013168) referred to the islands as \"Nosala\" and \"Nosala\". In the\
    \ 3rd century CE, the islands were ruled by the Abhira dynasty. The region was\
    \ later controlled by the Gupta Empire in the 4th century CE. From the 8th century\
    \ to the 14th century, the islands came under the control of several indigenous\
    \ dynasties: the Rashtrakuta dynasty; the Chalukya dynasty; the Silhara dynasty\
    \ and the Yadava dynasty of Devagiri. After 1343, the islands were ruled by the\
    \ Muslim rulers of Gujarat Sultanate. On 23 January 1534, Bahadur Shah, the Sultan\
    \ of Gujarat, ceded the islands to the Portuguese Empire as per the Treaty of\
    \ Bassein. The islands obtained the name Mumbai, from the name of the local Goddess\
    \ Mumba Devi. The city\n```"
  created_at: 2023-05-26 11:35:20+00:00
  edited: false
  hidden: false
  id: 6470a7881f0e7ee7fb1a7fb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: slotreck
      type: user
    createdAt: '2023-05-26T13:37:36.000Z'
    data:
      edited: false
      editors:
      - slotreck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: slotreck
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;asach&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/asach\">@<span class=\"\
          underline\">asach</span></a></span>\n\n\t</span></span> could you post the\
          \ code you used to generate that response? That's better than what I've\
          \ been getting!</p>\n"
        raw: '@asach could you post the code you used to generate that response? That''s
          better than what I''ve been getting!'
        updatedAt: '2023-05-26T13:37:36.791Z'
      numEdits: 0
      reactions: []
    id: 6470b6201f0e7ee7fb1b7251
    type: comment
  author: slotreck
  content: '@asach could you post the code you used to generate that response? That''s
    better than what I''ve been getting!'
  created_at: 2023-05-26 12:37:36+00:00
  edited: false
  hidden: false
  id: 6470b6201f0e7ee7fb1b7251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
      fullname: Om
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asach
      type: user
    createdAt: '2023-05-26T14:37:35.000Z'
    data:
      edited: false
      editors:
      - asach
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
          fullname: Om
          isHf: false
          isPro: false
          name: asach
          type: user
        html: "<p>Same your code ! 2 GPU RTX 4090 on runpod</p>\n<pre><code>from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ncheckpoint\
          \ = 'ausboss/llama-30b-supercot'\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\n\
          \    torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nprompt = \"Plan\
          \ a trip to Mumbai for 5 days\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\n\
          inputs = inputs.to(0)\noutput = model.generate(inputs['input_ids'], max_new_tokens=500)\n\
          response = tokenizer.decode(output[0].tolist())\nprint(response)\n</code></pre>\n\
          <p>Cheers !</p>\n"
        raw: "Same your code ! 2 GPU RTX 4090 on runpod\n\n```\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ncheckpoint\
          \ = 'ausboss/llama-30b-supercot'\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\n\
          \    torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nprompt = \"Plan\
          \ a trip to Mumbai for 5 days\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\n\
          inputs = inputs.to(0)\noutput = model.generate(inputs['input_ids'], max_new_tokens=500)\n\
          response = tokenizer.decode(output[0].tolist())\nprint(response)\n```\n\n\
          Cheers !"
        updatedAt: '2023-05-26T14:37:35.629Z'
      numEdits: 0
      reactions: []
    id: 6470c42f9fe78d69a8b24864
    type: comment
  author: asach
  content: "Same your code ! 2 GPU RTX 4090 on runpod\n\n```\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ncheckpoint = 'ausboss/llama-30b-supercot'\n\
    model = AutoModelForCausalLM.from_pretrained(checkpoint,\n    torch_dtype=torch.float16,\
    \ device_map='auto', load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    \nprompt = \"Plan a trip to Mumbai for 5 days\"\n\ninputs = tokenizer(prompt,\
    \ return_tensors='pt')\ninputs = inputs.to(0)\noutput = model.generate(inputs['input_ids'],\
    \ max_new_tokens=500)\nresponse = tokenizer.decode(output[0].tolist())\nprint(response)\n\
    ```\n\nCheers !"
  created_at: 2023-05-26 13:37:35+00:00
  edited: false
  hidden: false
  id: 6470c42f9fe78d69a8b24864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: slotreck
      type: user
    createdAt: '2023-05-26T18:44:17.000Z'
    data:
      edited: false
      editors:
      - slotreck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4704b8eb23121696f19d36186ba8cfc.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: slotreck
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;asach&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/asach\">@<span class=\"\
          underline\">asach</span></a></span>\n\n\t</span></span> Thank you so much,\
          \ I ran exactly our code and still got unknown tokens, which to me implies\
          \ it's something to do with my machine. What architecture are you running\
          \ on?</p>\n"
        raw: '@asach Thank you so much, I ran exactly our code and still got unknown
          tokens, which to me implies it''s something to do with my machine. What
          architecture are you running on?'
        updatedAt: '2023-05-26T18:44:17.191Z'
      numEdits: 0
      reactions: []
    id: 6470fe01a21a31bb294c8788
    type: comment
  author: slotreck
  content: '@asach Thank you so much, I ran exactly our code and still got unknown
    tokens, which to me implies it''s something to do with my machine. What architecture
    are you running on?'
  created_at: 2023-05-26 17:44:17+00:00
  edited: false
  hidden: false
  id: 6470fe01a21a31bb294c8788
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
      fullname: Om
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asach
      type: user
    createdAt: '2023-05-30T16:18:28.000Z'
    data:
      edited: true
      editors:
      - asach
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fb3c84ce54be18b0949888/AQrrK8Ooh2xKQDGhRyRI3.jpeg?w=200&h=200&f=face
          fullname: Om
          isHf: false
          isPro: false
          name: asach
          type: user
        html: "<p>Sorry i closed that instance don\u2019t remember properly, it was\
          \ ARM. Like you can try on runpod its cheap!</p>\n"
        raw: "Sorry i closed that instance don\u2019t remember properly, it was ARM.\
          \ Like you can try on runpod its cheap!"
        updatedAt: '2023-05-30T16:18:49.071Z'
      numEdits: 1
      reactions: []
    id: 647621d426985c62d78b989e
    type: comment
  author: asach
  content: "Sorry i closed that instance don\u2019t remember properly, it was ARM.\
    \ Like you can try on runpod its cheap!"
  created_at: 2023-05-30 15:18:28+00:00
  edited: true
  hidden: false
  id: 647621d426985c62d78b989e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ausboss/llama-30b-supercot
repo_type: model
status: open
target_branch: null
title: Model only returns `<unk>` tokens in response to text generation prompt
