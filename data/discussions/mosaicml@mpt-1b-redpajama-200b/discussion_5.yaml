!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zokica
conflicting_files: null
created_at: 2023-04-22 11:18:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-04-22T12:18:48.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>I tried and got error  (ImportError: This modeling file requires
          the following packages that were not found in your environment: flash_attn.
          Run <code>pip install flash_attn</code>)</p>

          <p>But flash_attn requires cuda/GPU so i want able to install it.</p>

          '
        raw: "I tried and got error  (ImportError: This modeling file requires the\
          \ following packages that were not found in your environment: flash_attn.\
          \ Run `pip install flash_attn`)\r\n \r\nBut flash_attn requires cuda/GPU\
          \ so i want able to install it.\r\n"
        updatedAt: '2023-04-22T12:18:48.176Z'
      numEdits: 0
      reactions: []
    id: 6443d0a892c17f01a16f1d0e
    type: comment
  author: zokica
  content: "I tried and got error  (ImportError: This modeling file requires the following\
    \ packages that were not found in your environment: flash_attn. Run `pip install\
    \ flash_attn`)\r\n \r\nBut flash_attn requires cuda/GPU so i want able to install\
    \ it.\r\n"
  created_at: 2023-04-22 11:18:48+00:00
  edited: false
  hidden: false
  id: 6443d0a892c17f01a16f1d0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-04-22T15:50:55.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>What command did you run to load the model?</p>

          '
        raw: What command did you run to load the model?
        updatedAt: '2023-04-22T15:50:55.184Z'
      numEdits: 0
      reactions: []
    id: 6444025fc63001ae6351a920
    type: comment
  author: sam-mosaic
  content: What command did you run to load the model?
  created_at: 2023-04-22 14:50:55+00:00
  edited: false
  hidden: false
  id: 6444025fc63001ae6351a920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-04-22T16:20:21.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>This one:</p>

          <p>import transformers<br>model = transformers.AutoModelForCausalLM.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
          trust_remote_code=True)</p>

          <p>However, I tried on a GPU and quality is pretty bad. It almost cannot
          generate anything that make sense,it is able to write just basic things,
          maybe 200B is just too short training. Hope 7B model will be better.</p>

          <p>But I appreciate effort of working on the open-source models.</p>

          '
        raw: 'This one:


          import transformers

          model = transformers.AutoModelForCausalLM.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
          trust_remote_code=True)


          However, I tried on a GPU and quality is pretty bad. It almost cannot generate
          anything that make sense,it is able to write just basic things, maybe 200B
          is just too short training. Hope 7B model will be better.


          But I appreciate effort of working on the open-source models.'
        updatedAt: '2023-04-22T16:23:24.315Z'
      numEdits: 5
      reactions: []
    id: 644409458f795c936d017590
    type: comment
  author: zokica
  content: 'This one:


    import transformers

    model = transformers.AutoModelForCausalLM.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
    trust_remote_code=True)


    However, I tried on a GPU and quality is pretty bad. It almost cannot generate
    anything that make sense,it is able to write just basic things, maybe 200B is
    just too short training. Hope 7B model will be better.


    But I appreciate effort of working on the open-source models.'
  created_at: 2023-04-22 15:20:21+00:00
  edited: true
  hidden: false
  id: 644409458f795c936d017590
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b0b7f9a596c50b1783fe9e/nzctJXoyiEwCnOHVJEx3_.png?w=200&h=200&f=face
      fullname: Jonathan Frankle
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jfrankle
      type: user
    createdAt: '2023-04-22T16:21:58.000Z'
    data:
      edited: true
      editors:
      - jfrankle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b0b7f9a596c50b1783fe9e/nzctJXoyiEwCnOHVJEx3_.png?w=200&h=200&f=face
          fullname: Jonathan Frankle
          isHf: false
          isPro: false
          name: jfrankle
          type: user
        html: '<p>I don''t expect the quality to be that good. It''s a pretty small
          model, and the underlying dataset is of unknown quality. We intend this
          model to be another way of getting to know the redpajama dataset, not something
          necessarily good that you should try to use in production. It''s possible
          that (1B, 200B) is too little, or that the dataset is of poor quality. We
          leave that analysis to the community, and we hope this model is helpful
          in making that determination.</p>

          '
        raw: I don't expect the quality to be that good. It's a pretty small model,
          and the underlying dataset is of unknown quality. We intend this model to
          be another way of getting to know the redpajama dataset, not something necessarily
          good that you should try to use in production. It's possible that (1B, 200B)
          is too little, or that the dataset is of poor quality. We leave that analysis
          to the community, and we hope this model is helpful in making that determination.
        updatedAt: '2023-04-22T16:23:02.212Z'
      numEdits: 2
      reactions: []
    id: 644409a65298d19c9c00111d
    type: comment
  author: jfrankle
  content: I don't expect the quality to be that good. It's a pretty small model,
    and the underlying dataset is of unknown quality. We intend this model to be another
    way of getting to know the redpajama dataset, not something necessarily good that
    you should try to use in production. It's possible that (1B, 200B) is too little,
    or that the dataset is of poor quality. We leave that analysis to the community,
    and we hope this model is helpful in making that determination.
  created_at: 2023-04-22 15:21:58+00:00
  edited: true
  hidden: false
  id: 644409a65298d19c9c00111d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b0b7f9a596c50b1783fe9e/nzctJXoyiEwCnOHVJEx3_.png?w=200&h=200&f=face
      fullname: Jonathan Frankle
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jfrankle
      type: user
    createdAt: '2023-04-22T16:23:23.000Z'
    data:
      edited: false
      editors:
      - jfrankle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b0b7f9a596c50b1783fe9e/nzctJXoyiEwCnOHVJEx3_.png?w=200&h=200&f=face
          fullname: Jonathan Frankle
          isHf: false
          isPro: false
          name: jfrankle
          type: user
        html: '<p>It sounds like you''ve been able to use the model, though, so I''m
          going to close this issue.</p>

          '
        raw: It sounds like you've been able to use the model, though, so I'm going
          to close this issue.
        updatedAt: '2023-04-22T16:23:23.183Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644409fb5298d19c9c001620
    id: 644409fb5298d19c9c00161f
    type: comment
  author: jfrankle
  content: It sounds like you've been able to use the model, though, so I'm going
    to close this issue.
  created_at: 2023-04-22 15:23:23+00:00
  edited: false
  hidden: false
  id: 644409fb5298d19c9c00161f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b0b7f9a596c50b1783fe9e/nzctJXoyiEwCnOHVJEx3_.png?w=200&h=200&f=face
      fullname: Jonathan Frankle
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jfrankle
      type: user
    createdAt: '2023-04-22T16:23:23.000Z'
    data:
      status: closed
    id: 644409fb5298d19c9c001620
    type: status-change
  author: jfrankle
  created_at: 2023-04-22 15:23:23+00:00
  id: 644409fb5298d19c9c001620
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-04-22T17:47:37.000Z'
    data:
      edited: true
      editors:
      - daking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>For anyone else ending up here, you should be able to run on CPU
          without installing flash/Triton. I suspect you may need a more recent transformers
          version, as they recently added skipping of try/except blocks when checking
          imports.</p>

          '
        raw: For anyone else ending up here, you should be able to run on CPU without
          installing flash/Triton. I suspect you may need a more recent transformers
          version, as they recently added skipping of try/except blocks when checking
          imports.
        updatedAt: '2023-04-22T17:47:50.171Z'
      numEdits: 1
      reactions: []
    id: 64441db9c63001ae6353645c
    type: comment
  author: daking
  content: For anyone else ending up here, you should be able to run on CPU without
    installing flash/Triton. I suspect you may need a more recent transformers version,
    as they recently added skipping of try/except blocks when checking imports.
  created_at: 2023-04-22 16:47:37+00:00
  edited: true
  hidden: false
  id: 64441db9c63001ae6353645c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-04-22T18:48:34.000Z'
    data:
      edited: true
      editors:
      - daking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>I was able to do (run on cpu)</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          transformers

          model = transformers.AutoModelForCausalLM.from_pretrained(<span class="hljs-string">''mosaicml/mpt-1b-redpajama-200b''</span>,
          trust_remote_code=<span class="hljs-literal">True</span>)

          tokenizer = transformers.AutoTokenizer.from_pretrained(<span class="hljs-string">''mosaicml/mpt-1b-redpajama-200b''</span>,
          trust_remote_code=<span class="hljs-literal">True</span>)

          <span class="hljs-built_in">print</span>(model.generate(**tokenizer(<span
          class="hljs-string">''hello''</span>, return_tensors=<span class="hljs-string">''pt''</span>),
          max_new_tokens=<span class="hljs-number">2</span>))

          </code></pre>

          <p>after <code>pip install transformers torch==1.13.1 einops</code></p>

          '
        raw: 'I was able to do (run on cpu)

          ```python

          import transformers

          model = transformers.AutoModelForCausalLM.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
          trust_remote_code=True)

          tokenizer = transformers.AutoTokenizer.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
          trust_remote_code=True)

          print(model.generate(**tokenizer(''hello'', return_tensors=''pt''), max_new_tokens=2))

          ```

          after `pip install transformers torch==1.13.1 einops`'
        updatedAt: '2023-04-22T18:48:45.730Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sam-mosaic
    id: 64442c02c63001ae63542e83
    type: comment
  author: daking
  content: 'I was able to do (run on cpu)

    ```python

    import transformers

    model = transformers.AutoModelForCausalLM.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
    trust_remote_code=True)

    tokenizer = transformers.AutoTokenizer.from_pretrained(''mosaicml/mpt-1b-redpajama-200b'',
    trust_remote_code=True)

    print(model.generate(**tokenizer(''hello'', return_tensors=''pt''), max_new_tokens=2))

    ```

    after `pip install transformers torch==1.13.1 einops`'
  created_at: 2023-04-22 17:48:34+00:00
  edited: true
  hidden: false
  id: 64442c02c63001ae63542e83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: mosaicml/mpt-1b-redpajama-200b
repo_type: model
status: closed
target_branch: null
title: How can I use this model on CPU?
