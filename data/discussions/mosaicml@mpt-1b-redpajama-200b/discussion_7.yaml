!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matorus
conflicting_files: null
created_at: 2023-07-11 05:38:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/238bd62a0edbd0522c4afa3009f79853.svg
      fullname: Matthias Geihs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matorus
      type: user
    createdAt: '2023-07-11T06:38:18.000Z'
    data:
      edited: false
      editors:
      - matorus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.13540592789649963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/238bd62a0edbd0522c4afa3009f79853.svg
          fullname: Matthias Geihs
          isHf: false
          isPro: false
          name: matorus
          type: user
        html: "<p>When trying to finetune using Mosaic's <code>composer train.py</code>\
          \ (with a more or less default finetuning yaml) I get: <code>AttributeError:\
          \ 'ModuleDict' object has no attribute 'get_input_embeddings'</code><br>Any\
          \ ideas how to circumvent the issue?</p>\n<pre><code>\u256D\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:326\
          \ in &lt;module&gt;         \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502   323 \u2502   \u2502   yaml_cfg = om.load(f)                 \
          \                                             \u2502\n\u2502   324 \u2502\
          \   cli_cfg = om.from_cli(args_list)                                   \
          \                    \u2502\n\u2502   325 \u2502   cfg = om.merge(yaml_cfg,\
          \ cli_cfg)                                                      \u2502\n\
          \u2502 \u2771 326 \u2502   main(cfg)                                   \
          \                                           \u2502\n\u2502   327       \
          \                                                                      \
          \               \u2502\n\u2502                                         \
          \                                                         \u2502\n\u2502\
          \ /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:215\
          \ in main             \u2502\n\u2502                                   \
          \                                                               \u2502\n\
          \u2502   212 \u2502   \u2502   \u2502   \u2502   cfg.model, cfg.lora, tokenizer)\
          \                                            \u2502\n\u2502   213 \u2502\
          \   \u2502   \u2502   print_trainable_parameters(model)  # should not be\
          \ 100%                        \u2502\n\u2502   214 \u2502   \u2502   else:\
          \  # standard model                                                    \
          \        \u2502\n\u2502 \u2771 215 \u2502   \u2502   \u2502   model = build_composer_model(cfg.model,\
          \ tokenizer)                             \u2502\n\u2502   216 \u2502   cfg.n_params\
          \ = sum(p.numel() for p in model.parameters())                         \
          \     \u2502\n\u2502   217 \u2502   print(f'{cfg.n_params=:.2e}')      \
          \                                                    \u2502\n\u2502   218\
          \                                                                      \
          \                      \u2502\n\u2502                                  \
          \                                                                \u2502\n\
          \u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:70\
          \ in                   \u2502\n\u2502 build_composer_model             \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502    67 \u2502   if model_cfg.name\
          \ not in COMPOSER_MODEL_REGISTRY:                                      \u2502\
          \n\u2502    68 \u2502   \u2502   raise ValueError(                     \
          \                                             \u2502\n\u2502    69 \u2502\
          \   \u2502   \u2502   f'Not sure how to build model with name={model_cfg.name}')\
          \                     \u2502\n\u2502 \u2771  70 \u2502   return COMPOSER_MODEL_REGISTRY[model_cfg.name](model_cfg,\
          \ tokenizer)                   \u2502\n\u2502    71                    \
          \                                                                      \
          \  \u2502\n\u2502    72                                                \
          \                                            \u2502\n\u2502    73 def build_composer_peft_model(\
          \                                                             \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_causal_lm.py:181\
          \ in __init__              \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   178 \u2502   \u2502   \u2502   \u2502   f'om_model_config must\
          \ be either a DictConfig, PeftModel, or PreTrainedM   \u2502\n\u2502   179\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                     \u2502\n\u2502   180 \u2502   \u2502\
          \                                                                      \
          \                \u2502\n\u2502 \u2771 181 \u2502   \u2502   composer_model\
          \ = super().__init__(model=model,                                     \u2502\
          \n\u2502   182 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502     shift_labels=True,                    \
          \           \u2502\n\u2502   183 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502     tokenizer=tokenizer,\
          \                             \u2502\n\u2502   184 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     metrics=train_metrics,\
          \                           \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/model_wrapper.py:65\
          \ in __init__              \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502    62 \u2502   \u2502                                         \
          \                                             \u2502\n\u2502    63 \u2502\
          \   \u2502   # Note: We need to add the FSDP related attributes to the model\
          \ AFTER the super    \u2502\n\u2502    64 \u2502   \u2502   # so that the\
          \ (possible) embedding resizing doesn't destroy them                   \u2502\
          \n\u2502 \u2771  65 \u2502   \u2502   prepare_hf_model_for_fsdp(self.model,\
          \ init_device)                                 \u2502\n\u2502    66 \u2502\
          \   \u2502                                                             \
          \                         \u2502\n\u2502    67 \u2502   \u2502   # This\
          \ provides support for meta initialization when using FSDP             \
          \       \u2502\n\u2502    68 \u2502   \u2502   self.model.param_init_fn\
          \ = lambda module: self.model._init_weights(                \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:118\
          \ in prepare_hf_model_for_fsdp  \u2502\n\u2502                         \
          \                                                                      \
          \   \u2502\n\u2502   115 \u2502   else:                                \
          \                                                  \u2502\n\u2502   116\
          \ \u2502   \u2502   # many common decoder-only model do not set the flag\
          \                               \u2502\n\u2502   117 \u2502   \u2502   #\
          \ model.config.is_decoder, so we can't trust it                        \
          \            \u2502\n\u2502 \u2771 118 \u2502   \u2502   prepare_hf_causal_lm_model_for_fsdp(model,\
          \ init_device)                            \u2502\n\u2502   119         \
          \                                                                      \
          \             \u2502\n\u2502   120                                     \
          \                                                       \u2502\n\u2502 \
          \  121 def prepare_hf_causal_lm_model_for_fsdp(model: PreTrainedModel, \
          \                           \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:136\
          \ in                            \u2502\n\u2502 prepare_hf_causal_lm_model_for_fsdp\
          \                                                              \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   133 \u2502   lm_head = model.get_output_embeddings()\
          \                                                \u2502\n\u2502   134 \u2502\
          \   # some models (OPT) implement .get_input_embeddings for the causal subclass\
          \            \u2502\n\u2502   135 \u2502   # but all of them implement it\
          \ for the base model                                      \u2502\n\u2502\
          \ \u2771 136 \u2502   tied_embeddings = causal_base_model.get_input_embeddings()\
          \  # type: ignore             \u2502\n\u2502   137 \u2502   modules = {\
          \                                                                      \
          \      \u2502\n\u2502   138 \u2502   \u2502   'base_model': causal_base_model,\
          \                                                   \u2502\n\u2502   139\
          \ \u2502   \u2502   'model_block': model_block,                        \
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /home/matthias/src/replit-finetune/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1\
          \ \u2502\n\u2502 614 in __getattr__                                    \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   1611 \u2502   \u2502   \u2502   modules\
          \ = self.__dict__['_modules']                                          \
          \ \u2502\n\u2502   1612 \u2502   \u2502   \u2502   if name in modules: \
          \                                                          \u2502\n\u2502\
          \   1613 \u2502   \u2502   \u2502   \u2502   return modules[name]      \
          \                                                \u2502\n\u2502 \u2771 1614\
          \ \u2502   \u2502   raise AttributeError(\"'{}' object has no attribute\
          \ '{}'\".format(                  \u2502\n\u2502   1615 \u2502   \u2502\
          \   \u2502   type(self).__name__, name))                               \
          \                    \u2502\n\u2502   1616 \u2502                      \
          \                                                                   \u2502\
          \n\u2502   1617 \u2502   def __setattr__(self, name: str, value: Union[Tensor,\
          \ 'Module']) -&gt; None:             \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nAttributeError:\
          \ 'ModuleDict' object has no attribute 'get_input_embeddings'\n</code></pre>\n"
        raw: "When trying to finetune using Mosaic's `composer train.py` (with a more\
          \ or less default finetuning yaml) I get: `AttributeError: 'ModuleDict'\
          \ object has no attribute 'get_input_embeddings'`\r\nAny ideas how to circumvent\
          \ the issue?\r\n```\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
          \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E\r\n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:326\
          \ in <module>         \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502   323 \u2502   \u2502   yaml_cfg = om.load(f)               \
          \                                               \u2502\r\n\u2502   324 \u2502\
          \   cli_cfg = om.from_cli(args_list)                                   \
          \                    \u2502\r\n\u2502   325 \u2502   cfg = om.merge(yaml_cfg,\
          \ cli_cfg)                                                      \u2502\r\
          \n\u2502 \u2771 326 \u2502   main(cfg)                                 \
          \                                             \u2502\r\n\u2502   327   \
          \                                                                      \
          \                   \u2502\r\n\u2502                                   \
          \                                                               \u2502\r\
          \n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:215\
          \ in main             \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502   212 \u2502   \u2502   \u2502   \u2502   cfg.model, cfg.lora,\
          \ tokenizer)                                            \u2502\r\n\u2502\
          \   213 \u2502   \u2502   \u2502   print_trainable_parameters(model)  #\
          \ should not be 100%                        \u2502\r\n\u2502   214 \u2502\
          \   \u2502   else:  # standard model                                   \
          \                         \u2502\r\n\u2502 \u2771 215 \u2502   \u2502  \
          \ \u2502   model = build_composer_model(cfg.model, tokenizer)          \
          \                   \u2502\r\n\u2502   216 \u2502   cfg.n_params = sum(p.numel()\
          \ for p in model.parameters())                              \u2502\r\n\u2502\
          \   217 \u2502   print(f'{cfg.n_params=:.2e}')                         \
          \                                 \u2502\r\n\u2502   218               \
          \                                                                      \
          \       \u2502\r\n\u2502                                               \
          \                                                   \u2502\r\n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:70\
          \ in                   \u2502\r\n\u2502 build_composer_model           \
          \                                                                  \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502    67 \u2502   if\
          \ model_cfg.name not in COMPOSER_MODEL_REGISTRY:                       \
          \               \u2502\r\n\u2502    68 \u2502   \u2502   raise ValueError(\
          \                                                                  \u2502\
          \r\n\u2502    69 \u2502   \u2502   \u2502   f'Not sure how to build model\
          \ with name={model_cfg.name}')                     \u2502\r\n\u2502 \u2771\
          \  70 \u2502   return COMPOSER_MODEL_REGISTRY[model_cfg.name](model_cfg,\
          \ tokenizer)                   \u2502\r\n\u2502    71                  \
          \                                                                      \
          \    \u2502\r\n\u2502    72                                            \
          \                                                \u2502\r\n\u2502    73\
          \ def build_composer_peft_model(                                       \
          \                      \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_causal_lm.py:181\
          \ in __init__              \u2502\r\n\u2502                            \
          \                                                                      \u2502\
          \r\n\u2502   178 \u2502   \u2502   \u2502   \u2502   f'om_model_config must\
          \ be either a DictConfig, PeftModel, or PreTrainedM   \u2502\r\n\u2502 \
          \  179 \u2502   \u2502   \u2502   )                                    \
          \                                          \u2502\r\n\u2502   180 \u2502\
          \   \u2502                                                             \
          \                         \u2502\r\n\u2502 \u2771 181 \u2502   \u2502  \
          \ composer_model = super().__init__(model=model,                       \
          \              \u2502\r\n\u2502   182 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     shift_labels=True,\
          \                               \u2502\r\n\u2502   183 \u2502   \u2502 \
          \  \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \     tokenizer=tokenizer,                             \u2502\r\n\u2502\
          \   184 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502     metrics=train_metrics,                         \
          \  \u2502\r\n\u2502                                                    \
          \                                              \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/model_wrapper.py:65\
          \ in __init__              \u2502\r\n\u2502                            \
          \                                                                      \u2502\
          \r\n\u2502    62 \u2502   \u2502                                       \
          \                                               \u2502\r\n\u2502    63 \u2502\
          \   \u2502   # Note: We need to add the FSDP related attributes to the model\
          \ AFTER the super    \u2502\r\n\u2502    64 \u2502   \u2502   # so that\
          \ the (possible) embedding resizing doesn't destroy them               \
          \    \u2502\r\n\u2502 \u2771  65 \u2502   \u2502   prepare_hf_model_for_fsdp(self.model,\
          \ init_device)                                 \u2502\r\n\u2502    66 \u2502\
          \   \u2502                                                             \
          \                         \u2502\r\n\u2502    67 \u2502   \u2502   # This\
          \ provides support for meta initialization when using FSDP             \
          \       \u2502\r\n\u2502    68 \u2502   \u2502   self.model.param_init_fn\
          \ = lambda module: self.model._init_weights(                \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:118\
          \ in prepare_hf_model_for_fsdp  \u2502\r\n\u2502                       \
          \                                                                      \
          \     \u2502\r\n\u2502   115 \u2502   else:                            \
          \                                                      \u2502\r\n\u2502\
          \   116 \u2502   \u2502   # many common decoder-only model do not set the\
          \ flag                               \u2502\r\n\u2502   117 \u2502   \u2502\
          \   # model.config.is_decoder, so we can't trust it                    \
          \                \u2502\r\n\u2502 \u2771 118 \u2502   \u2502   prepare_hf_causal_lm_model_for_fsdp(model,\
          \ init_device)                            \u2502\r\n\u2502   119       \
          \                                                                      \
          \               \u2502\r\n\u2502   120                                 \
          \                                                           \u2502\r\n\u2502\
          \   121 def prepare_hf_causal_lm_model_for_fsdp(model: PreTrainedModel,\
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:136\
          \ in                            \u2502\r\n\u2502 prepare_hf_causal_lm_model_for_fsdp\
          \                                                              \u2502\r\n\
          \u2502                                                                 \
          \                                 \u2502\r\n\u2502   133 \u2502   lm_head\
          \ = model.get_output_embeddings()                                      \
          \          \u2502\r\n\u2502   134 \u2502   # some models (OPT) implement\
          \ .get_input_embeddings for the causal subclass            \u2502\r\n\u2502\
          \   135 \u2502   # but all of them implement it for the base model     \
          \                                 \u2502\r\n\u2502 \u2771 136 \u2502   tied_embeddings\
          \ = causal_base_model.get_input_embeddings()  # type: ignore           \
          \  \u2502\r\n\u2502   137 \u2502   modules = {                         \
          \                                                   \u2502\r\n\u2502   138\
          \ \u2502   \u2502   'base_model': causal_base_model,                   \
          \                                \u2502\r\n\u2502   139 \u2502   \u2502\
          \   'model_block': model_block,                                        \
          \                \u2502\r\n\u2502                                      \
          \                                                            \u2502\r\n\u2502\
          \ /home/matthias/src/replit-finetune/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1\
          \ \u2502\r\n\u2502 614 in __getattr__                                  \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1611 \u2502   \u2502   \u2502   modules\
          \ = self.__dict__['_modules']                                          \
          \ \u2502\r\n\u2502   1612 \u2502   \u2502   \u2502   if name in modules:\
          \                                                           \u2502\r\n\u2502\
          \   1613 \u2502   \u2502   \u2502   \u2502   return modules[name]      \
          \                                                \u2502\r\n\u2502 \u2771\
          \ 1614 \u2502   \u2502   raise AttributeError(\"'{}' object has no attribute\
          \ '{}'\".format(                  \u2502\r\n\u2502   1615 \u2502   \u2502\
          \   \u2502   type(self).__name__, name))                               \
          \                    \u2502\r\n\u2502   1616 \u2502                    \
          \                                                                     \u2502\
          \r\n\u2502   1617 \u2502   def __setattr__(self, name: str, value: Union[Tensor,\
          \ 'Module']) -> None:             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nAttributeError:\
          \ 'ModuleDict' object has no attribute 'get_input_embeddings'\r\n```"
        updatedAt: '2023-07-11T06:38:18.651Z'
      numEdits: 0
      reactions: []
    id: 64acf8da50c5936418a5775d
    type: comment
  author: matorus
  content: "When trying to finetune using Mosaic's `composer train.py` (with a more\
    \ or less default finetuning yaml) I get: `AttributeError: 'ModuleDict' object\
    \ has no attribute 'get_input_embeddings'`\r\nAny ideas how to circumvent the\
    \ issue?\r\n```\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
    \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:326\
    \ in <module>         \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 \
    \  323 \u2502   \u2502   yaml_cfg = om.load(f)                               \
    \                               \u2502\r\n\u2502   324 \u2502   cli_cfg = om.from_cli(args_list)\
    \                                                       \u2502\r\n\u2502   325\
    \ \u2502   cfg = om.merge(yaml_cfg, cli_cfg)                                 \
    \                     \u2502\r\n\u2502 \u2771 326 \u2502   main(cfg)         \
    \                                                                     \u2502\r\
    \n\u2502   327                                                               \
    \                             \u2502\r\n\u2502                               \
    \                                                                   \u2502\r\n\
    \u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:215\
    \ in main             \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 \
    \  212 \u2502   \u2502   \u2502   \u2502   cfg.model, cfg.lora, tokenizer)   \
    \                                         \u2502\r\n\u2502   213 \u2502   \u2502\
    \   \u2502   print_trainable_parameters(model)  # should not be 100%         \
    \               \u2502\r\n\u2502   214 \u2502   \u2502   else:  # standard model\
    \                                                            \u2502\r\n\u2502\
    \ \u2771 215 \u2502   \u2502   \u2502   model = build_composer_model(cfg.model,\
    \ tokenizer)                             \u2502\r\n\u2502   216 \u2502   cfg.n_params\
    \ = sum(p.numel() for p in model.parameters())                              \u2502\
    \r\n\u2502   217 \u2502   print(f'{cfg.n_params=:.2e}')                      \
    \                                    \u2502\r\n\u2502   218                  \
    \                                                                          \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /home/matthias/src/replit-finetune/../llm-foundry/scripts/train/train.py:70\
    \ in                   \u2502\r\n\u2502 build_composer_model                 \
    \                                                            \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502    67 \u2502   if model_cfg.name not in\
    \ COMPOSER_MODEL_REGISTRY:                                      \u2502\r\n\u2502\
    \    68 \u2502   \u2502   raise ValueError(                                  \
    \                                \u2502\r\n\u2502    69 \u2502   \u2502   \u2502\
    \   f'Not sure how to build model with name={model_cfg.name}')               \
    \      \u2502\r\n\u2502 \u2771  70 \u2502   return COMPOSER_MODEL_REGISTRY[model_cfg.name](model_cfg,\
    \ tokenizer)                   \u2502\r\n\u2502    71                        \
    \                                                                    \u2502\r\n\
    \u2502    72                                                                 \
    \                           \u2502\r\n\u2502    73 def build_composer_peft_model(\
    \                                                             \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_causal_lm.py:181\
    \ in __init__              \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \   178 \u2502   \u2502   \u2502   \u2502   f'om_model_config must be either a\
    \ DictConfig, PeftModel, or PreTrainedM   \u2502\r\n\u2502   179 \u2502   \u2502\
    \   \u2502   )                                                               \
    \               \u2502\r\n\u2502   180 \u2502   \u2502                       \
    \                                                               \u2502\r\n\u2502\
    \ \u2771 181 \u2502   \u2502   composer_model = super().__init__(model=model,\
    \                                     \u2502\r\n\u2502   182 \u2502   \u2502 \
    \  \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     shift_labels=True,\
    \                               \u2502\r\n\u2502   183 \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     tokenizer=tokenizer,\
    \                             \u2502\r\n\u2502   184 \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     metrics=train_metrics,\
    \                           \u2502\r\n\u2502                                 \
    \                                                                 \u2502\r\n\u2502\
    \ /home/matthias/src/llm-foundry/llmfoundry/models/hf/model_wrapper.py:65 in __init__\
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502    62 \u2502\
    \   \u2502                                                                   \
    \                   \u2502\r\n\u2502    63 \u2502   \u2502   # Note: We need to\
    \ add the FSDP related attributes to the model AFTER the super    \u2502\r\n\u2502\
    \    64 \u2502   \u2502   # so that the (possible) embedding resizing doesn't\
    \ destroy them                   \u2502\r\n\u2502 \u2771  65 \u2502   \u2502 \
    \  prepare_hf_model_for_fsdp(self.model, init_device)                        \
    \         \u2502\r\n\u2502    66 \u2502   \u2502                             \
    \                                                         \u2502\r\n\u2502   \
    \ 67 \u2502   \u2502   # This provides support for meta initialization when using\
    \ FSDP                    \u2502\r\n\u2502    68 \u2502   \u2502   self.model.param_init_fn\
    \ = lambda module: self.model._init_weights(                \u2502\r\n\u2502 \
    \                                                                            \
    \                     \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:118\
    \ in prepare_hf_model_for_fsdp  \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502   115 \u2502   else:                                                \
    \                                  \u2502\r\n\u2502   116 \u2502   \u2502   #\
    \ many common decoder-only model do not set the flag                         \
    \      \u2502\r\n\u2502   117 \u2502   \u2502   # model.config.is_decoder, so\
    \ we can't trust it                                    \u2502\r\n\u2502 \u2771\
    \ 118 \u2502   \u2502   prepare_hf_causal_lm_model_for_fsdp(model, init_device)\
    \                            \u2502\r\n\u2502   119                          \
    \                                                                  \u2502\r\n\u2502\
    \   120                                                                      \
    \                      \u2502\r\n\u2502   121 def prepare_hf_causal_lm_model_for_fsdp(model:\
    \ PreTrainedModel,                            \u2502\r\n\u2502               \
    \                                                                            \
    \       \u2502\r\n\u2502 /home/matthias/src/llm-foundry/llmfoundry/models/hf/hf_fsdp.py:136\
    \ in                            \u2502\r\n\u2502 prepare_hf_causal_lm_model_for_fsdp\
    \                                                              \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502   133 \u2502   lm_head = model.get_output_embeddings()\
    \                                                \u2502\r\n\u2502   134 \u2502\
    \   # some models (OPT) implement .get_input_embeddings for the causal subclass\
    \            \u2502\r\n\u2502   135 \u2502   # but all of them implement it for\
    \ the base model                                      \u2502\r\n\u2502 \u2771\
    \ 136 \u2502   tied_embeddings = causal_base_model.get_input_embeddings()  # type:\
    \ ignore             \u2502\r\n\u2502   137 \u2502   modules = {             \
    \                                                               \u2502\r\n\u2502\
    \   138 \u2502   \u2502   'base_model': causal_base_model,                   \
    \                                \u2502\r\n\u2502   139 \u2502   \u2502   'model_block':\
    \ model_block,                                                        \u2502\r\
    \n\u2502                                                                     \
    \                             \u2502\r\n\u2502 /home/matthias/src/replit-finetune/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1\
    \ \u2502\r\n\u2502 614 in __getattr__                                        \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1611 \u2502   \u2502   \u2502   modules = self.__dict__['_modules']\
    \                                           \u2502\r\n\u2502   1612 \u2502   \u2502\
    \   \u2502   if name in modules:                                             \
    \              \u2502\r\n\u2502   1613 \u2502   \u2502   \u2502   \u2502   return\
    \ modules[name]                                                      \u2502\r\n\
    \u2502 \u2771 1614 \u2502   \u2502   raise AttributeError(\"'{}' object has no\
    \ attribute '{}'\".format(                  \u2502\r\n\u2502   1615 \u2502   \u2502\
    \   \u2502   type(self).__name__, name))                                     \
    \              \u2502\r\n\u2502   1616 \u2502                                \
    \                                                         \u2502\r\n\u2502   1617\
    \ \u2502   def __setattr__(self, name: str, value: Union[Tensor, 'Module']) ->\
    \ None:             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u256F\r\nAttributeError: 'ModuleDict' object has no attribute 'get_input_embeddings'\r\
    \n```"
  created_at: 2023-07-11 05:38:18+00:00
  edited: false
  hidden: false
  id: 64acf8da50c5936418a5775d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mosaicml/mpt-1b-redpajama-200b
repo_type: model
status: open
target_branch: null
title: '`AttributeError: ''ModuleDict'' object has no attribute ''get_input_embeddings''`
  when finetuning'
