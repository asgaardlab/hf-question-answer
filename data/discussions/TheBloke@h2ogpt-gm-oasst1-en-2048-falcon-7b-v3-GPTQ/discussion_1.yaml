!!python/object:huggingface_hub.community.DiscussionWithDetails
author: voxxer
conflicting_files: null
created_at: 2023-07-15 21:47:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4f0c390c86296143c62d1ba56fc14937.svg
      fullname: Daniel Y.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: voxxer
      type: user
    createdAt: '2023-07-15T22:47:19.000Z'
    data:
      edited: false
      editors:
      - voxxer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36646783351898193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4f0c390c86296143c62d1ba56fc14937.svg
          fullname: Daniel Y.
          isHf: false
          isPro: false
          name: voxxer
          type: user
        html: '<p>When you try running the model using the provided code, the following
          error appears:<br><code>---------------------------------------------------------------------------</code></p><code>

          <p>RuntimeError                              Traceback (most recent call
          last)</p>

          <p> in ()<br>      1 input_ids = tokenizer(prompt_template, return_tensors=''pt'').input_ids.cuda()<br>----&gt;
          2 output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>      3
          print(tokenizer.decode(output[0]))</p>

          <p>13 frames</p>

          <p>/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py in
          generate(self, **kwargs)<br>    421         """shortcut for model.generate"""<br>    422         with
          torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):<br>--&gt;
          423             return self.model.generate(**kwargs)<br>    424<br>    425     def
          prepare_inputs_for_generation(self, *args, **kwargs):</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py in
          decorate_context(*args, **kwargs)<br>    113     def decorate_context(*args,
          **kwargs):<br>    114         with ctx_factory():<br>--&gt; 115             return
          func(*args, **kwargs)<br>    116<br>    117     return decorate_context</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py
          in generate(self, inputs, generation_config, logits_processor, stopping_criteria,
          prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)<br>   1520<br>   1521             #
          11. run greedy search<br>-&gt; 1522             return self.greedy_search(<br>   1523                 input_ids,<br>   1524                 logits_processor=logits_processor,</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py
          in greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length,
          pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores,
          return_dict_in_generate, synced_gpus, streamer, **model_kwargs)<br>   2337<br>   2338             #
          forward pass to get next token<br>-&gt; 2339             outputs = self(<br>   2340                 **model_inputs,<br>   2341                 return_dict=True,</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py
          in forward(self, input_ids, past_key_values, attention_mask, head_mask,
          inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,
          return_dict, **deprecated_arguments)<br>    751         return_dict = return_dict
          if return_dict is not None else self.config.use_return_dict<br>    752<br>--&gt;
          753         transformer_outputs = self.transformer(<br>    754             input_ids,<br>    755             past_key_values=past_key_values,</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py
          in forward(self, input_ids, past_key_values, attention_mask, head_mask,
          inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,
          **deprecated_arguments)<br>    646                 )<br>    647             else:<br>--&gt;
          648                 outputs = block(<br>    649                     hidden_states,<br>    650                     layer_past=layer_past,</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py
          in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,
          use_cache, output_attentions)<br>    383<br>    384         # Self attention.<br>--&gt;
          385         attn_outputs = self.self_attention(<br>    386             layernorm_output,<br>    387             layer_past=layer_past,</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py
          in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,
          use_cache, output_attentions)<br>    240         output_attentions: bool
          = False,<br>    241     ):<br>--&gt; 242         fused_qkv = self.query_key_value(hidden_states)  #
          [batch_size, seq_length, 3 x hidden_size]<br>    243<br>    244         #
          3 x [batch_size, seq_length, num_heads, head_dim]</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py
          in forward(self, x)<br>    219                weight = torch.bitwise_right_shift(torch.unsqueeze(self.qweight,
          1).expand(-1, 32 // self.bits, -1), self.wf.unsqueeze(-1)).to(torch.int16
          if self.bits == 8 else torch.int8)<br>    220                torch.bitwise_and(weight,(2
          ** self.bits) - 1, out=weight)<br>--&gt; 221                weight = weight.reshape(-1,
          self.group_size, weight.shape[2])<br>    222             elif self.bits
          == 3:<br>    223                zeros = self.qzeros.reshape(self.qzeros.shape[0],
          self.qzeros.shape[1]//3, 3, 1).expand(-1, -1, -1, 12)</p>

          </code><p><code>RuntimeError: shape ''[-1, 128, 4672]'' is invalid for input
          of size 21229568 </code>.</p>

          '
        raw: "When you try running the model using the provided code, the following\
          \ error appears:\r\n<code>---------------------------------------------------------------------------\r\
          \n\r\nRuntimeError                              Traceback (most recent call\
          \ last)\r\n\r\n<ipython-input-19-7a2a3c100405> in <cell line: 2>()\r\n \
          \     1 input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
          \n----> 2 output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
          \n      3 print(tokenizer.decode(output[0]))\r\n\r\n13 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in generate(self, **kwargs)\r\n    421         \"\"\"shortcut for model.generate\"\
          \"\"\r\n    422         with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\r\
          \n--> 423             return self.model.generate(**kwargs)\r\n    424 \r\
          \n    425     def prepare_inputs_for_generation(self, *args, **kwargs):\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\
          \ in decorate_context(*args, **kwargs)\r\n    113     def decorate_context(*args,\
          \ **kwargs):\r\n    114         with ctx_factory():\r\n--> 115         \
          \    return func(*args, **kwargs)\r\n    116 \r\n    117     return decorate_context\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\
          \ in generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
          \ prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\
          \n   1520 \r\n   1521             # 11. run greedy search\r\n-> 1522   \
          \          return self.greedy_search(\r\n   1523                 input_ids,\r\
          \n   1524                 logits_processor=logits_processor,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\
          \ in greedy_search(self, input_ids, logits_processor, stopping_criteria,\
          \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
          \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\
          \n   2337 \r\n   2338             # forward pass to get next token\r\n->\
          \ 2339             outputs = self(\r\n   2340                 **model_inputs,\r\
          \n   2341                 return_dict=True,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
          \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\
          \n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\
          \n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
          \ in forward(self, input_ids, past_key_values, attention_mask, head_mask,\
          \ inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict, **deprecated_arguments)\r\n    751         return_dict =\
          \ return_dict if return_dict is not None else self.config.use_return_dict\r\
          \n    752 \r\n--> 753         transformer_outputs = self.transformer(\r\n\
          \    754             input_ids,\r\n    755             past_key_values=past_key_values,\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
          \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\
          \n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\
          \n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
          \ in forward(self, input_ids, past_key_values, attention_mask, head_mask,\
          \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
          \ **deprecated_arguments)\r\n    646                 )\r\n    647      \
          \       else:\r\n--> 648                 outputs = block(\r\n    649   \
          \                  hidden_states,\r\n    650                     layer_past=layer_past,\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
          \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\
          \n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\
          \n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
          \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
          \ use_cache, output_attentions)\r\n    383 \r\n    384         # Self attention.\r\
          \n--> 385         attn_outputs = self.self_attention(\r\n    386       \
          \      layernorm_output,\r\n    387             layer_past=layer_past,\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
          \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\
          \n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\
          \n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
          \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
          \ use_cache, output_attentions)\r\n    240         output_attentions: bool\
          \ = False,\r\n    241     ):\r\n--> 242         fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\r\n    243 \r\n    244  \
          \       # 3 x [batch_size, seq_length, num_heads, head_dim]\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
          \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\
          \n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\
          \ in forward(self, x)\r\n    219                weight = torch.bitwise_right_shift(torch.unsqueeze(self.qweight,\
          \ 1).expand(-1, 32 // self.bits, -1), self.wf.unsqueeze(-1)).to(torch.int16\
          \ if self.bits == 8 else torch.int8)\r\n    220                torch.bitwise_and(weight,(2\
          \ ** self.bits) - 1, out=weight)\r\n--> 221                weight = weight.reshape(-1,\
          \ self.group_size, weight.shape[2])\r\n    222             elif self.bits\
          \ == 3:\r\n    223                zeros = self.qzeros.reshape(self.qzeros.shape[0],\
          \ self.qzeros.shape[1]//3, 3, 1).expand(-1, -1, -1, 12)\r\n\r\nRuntimeError:\
          \ shape '[-1, 128, 4672]' is invalid for input of size 21229568 </code>."
        updatedAt: '2023-07-15T22:47:19.348Z'
      numEdits: 0
      reactions: []
    id: 64b321f734a92b848c820293
    type: comment
  author: voxxer
  content: "When you try running the model using the provided code, the following\
    \ error appears:\r\n<code>---------------------------------------------------------------------------\r\
    \n\r\nRuntimeError                              Traceback (most recent call last)\r\
    \n\r\n<ipython-input-19-7a2a3c100405> in <cell line: 2>()\r\n      1 input_ids\
    \ = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\n---->\
    \ 2 output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \n      3 print(tokenizer.decode(output[0]))\r\n\r\n13 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
    \ in generate(self, **kwargs)\r\n    421         \"\"\"shortcut for model.generate\"\
    \"\"\r\n    422         with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\r\
    \n--> 423             return self.model.generate(**kwargs)\r\n    424 \r\n   \
    \ 425     def prepare_inputs_for_generation(self, *args, **kwargs):\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\
    \ in decorate_context(*args, **kwargs)\r\n    113     def decorate_context(*args,\
    \ **kwargs):\r\n    114         with ctx_factory():\r\n--> 115             return\
    \ func(*args, **kwargs)\r\n    116 \r\n    117     return decorate_context\r\n\
    \r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py in\
    \ generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
    \ prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\
    \n   1520 \r\n   1521             # 11. run greedy search\r\n-> 1522         \
    \    return self.greedy_search(\r\n   1523                 input_ids,\r\n   1524\
    \                 logits_processor=logits_processor,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\
    \ in greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length,\
    \ pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores,\
    \ return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\n   2337 \r\
    \n   2338             # forward pass to get next token\r\n-> 2339            \
    \ outputs = self(\r\n   2340                 **model_inputs,\r\n   2341      \
    \           return_dict=True,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
    \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n  \
    \ 1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
    \ in forward(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds,\
    \ labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\r\
    \n    751         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\
    \n    752 \r\n--> 753         transformer_outputs = self.transformer(\r\n    754\
    \             input_ids,\r\n    755             past_key_values=past_key_values,\r\
    \n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self,\
    \ *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or\
    \ _global_backward_hooks\r\n   1500                 or _global_forward_hooks or\
    \ _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
    \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n  \
    \ 1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
    \ in forward(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds,\
    \ use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\r\
    \n    646                 )\r\n    647             else:\r\n--> 648          \
    \       outputs = block(\r\n    649                     hidden_states,\r\n   \
    \ 650                     layer_past=layer_past,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
    \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n  \
    \ 1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
    \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
    \ use_cache, output_attentions)\r\n    383 \r\n    384         # Self attention.\r\
    \n--> 385         attn_outputs = self.self_attention(\r\n    386             layernorm_output,\r\
    \n    387             layer_past=layer_past,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
    \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n  \
    \ 1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n~/.cache/huggingface/modules/transformers_modules/TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ/bb6a3a0b5a5a6b809877daa8397603a4467ac90d/modelling_RW.py\
    \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
    \ use_cache, output_attentions)\r\n    240         output_attentions: bool = False,\r\
    \n    241     ):\r\n--> 242         fused_qkv = self.query_key_value(hidden_states)\
    \  # [batch_size, seq_length, 3 x hidden_size]\r\n    243 \r\n    244        \
    \ # 3 x [batch_size, seq_length, num_heads, head_dim]\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500                 or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args,\
    \ **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n  \
    \ 1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\
    \ in forward(self, x)\r\n    219                weight = torch.bitwise_right_shift(torch.unsqueeze(self.qweight,\
    \ 1).expand(-1, 32 // self.bits, -1), self.wf.unsqueeze(-1)).to(torch.int16 if\
    \ self.bits == 8 else torch.int8)\r\n    220                torch.bitwise_and(weight,(2\
    \ ** self.bits) - 1, out=weight)\r\n--> 221                weight = weight.reshape(-1,\
    \ self.group_size, weight.shape[2])\r\n    222             elif self.bits == 3:\r\
    \n    223                zeros = self.qzeros.reshape(self.qzeros.shape[0], self.qzeros.shape[1]//3,\
    \ 3, 1).expand(-1, -1, -1, 12)\r\n\r\nRuntimeError: shape '[-1, 128, 4672]' is\
    \ invalid for input of size 21229568 </code>."
  created_at: 2023-07-15 21:47:19+00:00
  edited: false
  hidden: false
  id: 64b321f734a92b848c820293
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GPTQ
repo_type: model
status: open
target_branch: null
title: Error running the model
