!!python/object:huggingface_hub.community.DiscussionWithDetails
author: skroed
conflicting_files: null
created_at: 2023-11-01 20:27:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1212bec00cfacfdf579413042debf15a.svg
      fullname: "Sebastian Kr\xF6del"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skroed
      type: user
    createdAt: '2023-11-01T21:27:31.000Z'
    data:
      edited: false
      editors:
      - skroed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8693602681159973
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1212bec00cfacfdf579413042debf15a.svg
          fullname: "Sebastian Kr\xF6del"
          isHf: false
          isPro: false
          name: skroed
          type: user
        html: '<p>Is there a way to do advanced long-form generation similar as it
          is described here: <a rel="nofollow" href="https://github.com/suno-ai/bark/blob/main/notebooks/long_form_generation.ipynb">https://github.com/suno-ai/bark/blob/main/notebooks/long_form_generation.ipynb</a><br>by
          passing a <code>temperature</code> and <code>min_eos_p</code> ?</p>

          '
        raw: "Is there a way to do advanced long-form generation similar as it is\
          \ described here: https://github.com/suno-ai/bark/blob/main/notebooks/long_form_generation.ipynb\r\
          \nby passing a `temperature` and `min_eos_p` ?"
        updatedAt: '2023-11-01T21:27:31.716Z'
      numEdits: 0
      reactions: []
    id: 6542c2c3e5309ae0a1c9845e
    type: comment
  author: skroed
  content: "Is there a way to do advanced long-form generation similar as it is described\
    \ here: https://github.com/suno-ai/bark/blob/main/notebooks/long_form_generation.ipynb\r\
    \nby passing a `temperature` and `min_eos_p` ?"
  created_at: 2023-11-01 20:27:31+00:00
  edited: false
  hidden: false
  id: 6542c2c3e5309ae0a1c9845e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62611fcabbcbd1c34f1615f6/ehvQLkjaEaz0pYJwbB51L.jpeg?w=200&h=200&f=face
      fullname: Yoach Lacombe
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ylacombe
      type: user
    createdAt: '2023-11-08T07:51:45.000Z'
    data:
      edited: false
      editors:
      - ylacombe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5731407999992371
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62611fcabbcbd1c34f1615f6/ehvQLkjaEaz0pYJwbB51L.jpeg?w=200&h=200&f=face
          fullname: Yoach Lacombe
          isHf: true
          isPro: false
          name: ylacombe
          type: user
        html: "<p>You can use the same logic than the original notebook you've sent:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ nltk  <span class=\"hljs-comment\"># we'll use this to split into sentences</span>\n\
          <span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\"\
          >as</span> np\n<span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> BarkModel, AutoProcessor\n<span class=\"\
          hljs-keyword\">import</span> torch\n\nnltk.download(<span class=\"hljs-string\"\
          >'punkt'</span>)\ndevice = <span class=\"hljs-string\">\"cuda\"</span>\n\
          \n<span class=\"hljs-comment\"># Load model with optimization</span>\nmodel\
          \ = BarkModel.from_pretrained(<span class=\"hljs-string\">\"suno/bark\"\
          </span>, torch_dtype=torch.float16).to(device)\n<span class=\"hljs-comment\"\
          ># flash attention</span>\nmodel = model.to_bettertransformer()\n\nprocessor\
          \ = AutoProcessor.from_pretrained(<span class=\"hljs-string\">\"suno/bark\"\
          </span>)\n\nsampling_rate = model.generation_config.sample_rate\nsilence\
          \ = np.zeros(<span class=\"hljs-built_in\">int</span>(<span class=\"hljs-number\"\
          >0.25</span> * sampling_rate))  <span class=\"hljs-comment\"># quarter second\
          \ of silence</span>\nvoice_preset = <span class=\"hljs-string\">\"v2/en_speaker_6\"\
          </span>\n\nBATCH_SIZE = <span class=\"hljs-number\">12</span>\n\n<span class=\"\
          hljs-comment\"># split into sentences</span>\nmodel_input = nltk.sent_tokenize(TEXT_TO_GENERATE)\n\
          \npieces = []\n<span class=\"hljs-keyword\">for</span> i <span class=\"\
          hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span\
          \ class=\"hljs-number\">0</span>, <span class=\"hljs-built_in\">len</span>(model_input),\
          \ BATCH_SIZE):\n    inputs = model_input[BATCH_SIZE*i:<span class=\"hljs-built_in\"\
          >min</span>(BATCH_SIZE*(i+<span class=\"hljs-number\">1</span>), <span class=\"\
          hljs-built_in\">len</span>(model_input))]\n    \n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">len</span>(inputs) != <span class=\"\
          hljs-number\">0</span>:\n        inputs = processor(inputs, voice_preset=voice_preset)\n\
          \        \n        speech_output, output_lengths = model.generate(**inputs.to(device),\
          \ return_output_lengths=<span class=\"hljs-literal\">True</span>, min_eos_p=<span\
          \ class=\"hljs-number\">0.2</span>)\n        \n        speech_output = [output[:length].cpu().numpy()\
          \ <span class=\"hljs-keyword\">for</span> (output,length) <span class=\"\
          hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(speech_output,\
          \ output_lengths)]\n        \n        <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{i}</span>-th part\
          \ generated\"</span>)\n        <span class=\"hljs-comment\"># you can already\
          \ play `speech_output` or wait for the whole generation</span>\n       \
          \ pieces += [*speech_output, silence.copy()]\n        \n        \nwhole_ouput\
          \ = np.concatenate(pieces)\n</code></pre>\n"
        raw: "You can use the same logic than the original notebook you've sent:\n\
          \n```python\nimport nltk  # we'll use this to split into sentences\nimport\
          \ numpy as np\nfrom transformers import BarkModel, AutoProcessor\nimport\
          \ torch\n\nnltk.download('punkt')\ndevice = \"cuda\"\n\n# Load model with\
          \ optimization\nmodel = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)\n\
          # flash attention\nmodel = model.to_bettertransformer()\n\nprocessor = AutoProcessor.from_pretrained(\"\
          suno/bark\")\n\nsampling_rate = model.generation_config.sample_rate\nsilence\
          \ = np.zeros(int(0.25 * sampling_rate))  # quarter second of silence\nvoice_preset\
          \ = \"v2/en_speaker_6\"\n\nBATCH_SIZE = 12\n\n# split into sentences\nmodel_input\
          \ = nltk.sent_tokenize(TEXT_TO_GENERATE)\n\npieces = []\nfor i in range(0,\
          \ len(model_input), BATCH_SIZE):\n    inputs = model_input[BATCH_SIZE*i:min(BATCH_SIZE*(i+1),\
          \ len(model_input))]\n    \n    if len(inputs) != 0:\n        inputs = processor(inputs,\
          \ voice_preset=voice_preset)\n        \n        speech_output, output_lengths\
          \ = model.generate(**inputs.to(device), return_output_lengths=True, min_eos_p=0.2)\n\
          \        \n        speech_output = [output[:length].cpu().numpy() for (output,length)\
          \ in zip(speech_output, output_lengths)]\n        \n        print(f\"{i}-th\
          \ part generated\")\n        # you can already play `speech_output` or wait\
          \ for the whole generation\n        pieces += [*speech_output, silence.copy()]\n\
          \        \n        \nwhole_ouput = np.concatenate(pieces)\n```"
        updatedAt: '2023-11-08T07:51:45.578Z'
      numEdits: 0
      reactions: []
    id: 654b3e1111a85feec8b5b894
    type: comment
  author: ylacombe
  content: "You can use the same logic than the original notebook you've sent:\n\n\
    ```python\nimport nltk  # we'll use this to split into sentences\nimport numpy\
    \ as np\nfrom transformers import BarkModel, AutoProcessor\nimport torch\n\nnltk.download('punkt')\n\
    device = \"cuda\"\n\n# Load model with optimization\nmodel = BarkModel.from_pretrained(\"\
    suno/bark\", torch_dtype=torch.float16).to(device)\n# flash attention\nmodel =\
    \ model.to_bettertransformer()\n\nprocessor = AutoProcessor.from_pretrained(\"\
    suno/bark\")\n\nsampling_rate = model.generation_config.sample_rate\nsilence =\
    \ np.zeros(int(0.25 * sampling_rate))  # quarter second of silence\nvoice_preset\
    \ = \"v2/en_speaker_6\"\n\nBATCH_SIZE = 12\n\n# split into sentences\nmodel_input\
    \ = nltk.sent_tokenize(TEXT_TO_GENERATE)\n\npieces = []\nfor i in range(0, len(model_input),\
    \ BATCH_SIZE):\n    inputs = model_input[BATCH_SIZE*i:min(BATCH_SIZE*(i+1), len(model_input))]\n\
    \    \n    if len(inputs) != 0:\n        inputs = processor(inputs, voice_preset=voice_preset)\n\
    \        \n        speech_output, output_lengths = model.generate(**inputs.to(device),\
    \ return_output_lengths=True, min_eos_p=0.2)\n        \n        speech_output\
    \ = [output[:length].cpu().numpy() for (output,length) in zip(speech_output, output_lengths)]\n\
    \        \n        print(f\"{i}-th part generated\")\n        # you can already\
    \ play `speech_output` or wait for the whole generation\n        pieces += [*speech_output,\
    \ silence.copy()]\n        \n        \nwhole_ouput = np.concatenate(pieces)\n\
    ```"
  created_at: 2023-11-08 07:51:45+00:00
  edited: false
  hidden: false
  id: 654b3e1111a85feec8b5b894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62611fcabbcbd1c34f1615f6/ehvQLkjaEaz0pYJwbB51L.jpeg?w=200&h=200&f=face
      fullname: Yoach Lacombe
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ylacombe
      type: user
    createdAt: '2023-11-08T07:52:00.000Z'
    data:
      edited: false
      editors:
      - ylacombe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.940514862537384
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62611fcabbcbd1c34f1615f6/ehvQLkjaEaz0pYJwbB51L.jpeg?w=200&h=200&f=face
          fullname: Yoach Lacombe
          isHf: true
          isPro: false
          name: ylacombe
          type: user
        html: '<p>This one however is faster because it produces the output in batch!</p>

          '
        raw: This one however is faster because it produces the output in batch!
        updatedAt: '2023-11-08T07:52:00.447Z'
      numEdits: 0
      reactions: []
    id: 654b3e206167ff03f72b2cbf
    type: comment
  author: ylacombe
  content: This one however is faster because it produces the output in batch!
  created_at: 2023-11-08 07:52:00+00:00
  edited: false
  hidden: false
  id: 654b3e206167ff03f72b2cbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1212bec00cfacfdf579413042debf15a.svg
      fullname: "Sebastian Kr\xF6del"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skroed
      type: user
    createdAt: '2023-11-08T20:40:27.000Z'
    data:
      edited: false
      editors:
      - skroed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9912353754043579
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1212bec00cfacfdf579413042debf15a.svg
          fullname: "Sebastian Kr\xF6del"
          isHf: false
          isPro: false
          name: skroed
          type: user
        html: '<p>thanks I will give this a try.</p>

          '
        raw: 'thanks I will give this a try.

          '
        updatedAt: '2023-11-08T20:40:27.667Z'
      numEdits: 0
      reactions: []
    id: 654bf23bacfb5e4a81d3c316
    type: comment
  author: skroed
  content: 'thanks I will give this a try.

    '
  created_at: 2023-11-08 20:40:27+00:00
  edited: false
  hidden: false
  id: 654bf23bacfb5e4a81d3c316
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: suno/bark-small
repo_type: model
status: open
target_branch: null
title: Advanced long-form generation
