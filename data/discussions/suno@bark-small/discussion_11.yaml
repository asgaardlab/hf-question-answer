!!python/object:huggingface_hub.community.DiscussionWithDetails
author: packmad
conflicting_files: null
created_at: 2023-10-24 11:30:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de83168c2b5e69193426f88b8e1f37dc.svg
      fullname: "M\xE9oc"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: packmad
      type: user
    createdAt: '2023-10-24T12:30:27.000Z'
    data:
      edited: false
      editors:
      - packmad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7775421738624573
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de83168c2b5e69193426f88b8e1f37dc.svg
          fullname: "M\xE9oc"
          isHf: false
          isPro: false
          name: packmad
          type: user
        html: '<p>Hi,<br>I tried to use the bark model inference service in my code
          and  the generated audio file is just  an empty 1KB file. Also the hosted
          inference API finish with an "internal server error" (URL: <a href="https://huggingface.co/suno/bark-small">https://huggingface.co/suno/bark-small</a>)
          so overall the service seems down at the moment.<br>Can you fix it?<br>My
          code:<br> API_URL = "<a rel="nofollow" href="https://api-inference.huggingface.co/models/suno/bark-small&quot;">https://api-inference.huggingface.co/models/suno/bark-small"</a><br>  headers
          = {"Authorization": f"Bearer {HUGGINGFACEHUB_API_TOKEN}"}<br>  payload =
          {<br>      "inputs": message<br>  }<br>  response = requests.post(API_URL,
          headers=headers, json=payload)<br>  with open(''audio.flac'', ''wb'') as
          file:<br>      file.write(response.content)</p>

          '
        raw: "Hi, \r\nI tried to use the bark model inference service in my code and\
          \  the generated audio file is just  an empty 1KB file. Also the hosted\
          \ inference API finish with an \"internal server error\" (URL: https://huggingface.co/suno/bark-small)\
          \ so overall the service seems down at the moment.\r\nCan you fix it?\r\n\
          My code:\r\n API_URL = \"https://api-inference.huggingface.co/models/suno/bark-small\"\
          \r\n  headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"\
          }\r\n  payload = {\r\n      \"inputs\": message\r\n  }\r\n  response = requests.post(API_URL,\
          \ headers=headers, json=payload)\r\n  with open('audio.flac', 'wb') as file:\r\
          \n      file.write(response.content)"
        updatedAt: '2023-10-24T12:30:27.657Z'
      numEdits: 0
      reactions: []
    id: 6537b8e3605a07338d11bd03
    type: comment
  author: packmad
  content: "Hi, \r\nI tried to use the bark model inference service in my code and\
    \  the generated audio file is just  an empty 1KB file. Also the hosted inference\
    \ API finish with an \"internal server error\" (URL: https://huggingface.co/suno/bark-small)\
    \ so overall the service seems down at the moment.\r\nCan you fix it?\r\nMy code:\r\
    \n API_URL = \"https://api-inference.huggingface.co/models/suno/bark-small\"\r\
    \n  headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\r\n\
    \  payload = {\r\n      \"inputs\": message\r\n  }\r\n  response = requests.post(API_URL,\
    \ headers=headers, json=payload)\r\n  with open('audio.flac', 'wb') as file:\r\
    \n      file.write(response.content)"
  created_at: 2023-10-24 11:30:27+00:00
  edited: false
  hidden: false
  id: 6537b8e3605a07338d11bd03
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: suno/bark-small
repo_type: model
status: open
target_branch: null
title: 'internal server error: Inference API is down for the bark-small model?'
