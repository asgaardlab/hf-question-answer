!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NemesisAlm
conflicting_files: null
created_at: 2023-10-25 19:52:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28736c6c103144bef24028a7451d6ffd.svg
      fullname: jeremyk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NemesisAlm
      type: user
    createdAt: '2023-10-25T20:52:11.000Z'
    data:
      edited: false
      editors:
      - NemesisAlm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.692725658416748
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28736c6c103144bef24028a7451d6ffd.svg
          fullname: jeremyk
          isHf: false
          isPro: false
          name: NemesisAlm
          type: user
        html: '<p>Hello.</p>

          <p>First of all, thank you for this model. It looks amazing to have whisper
          large v2 in this quantized format.</p>

          <p>I am trying to run the code provided on the model card but get an error
          related to the hardware: 4b quantization not yet supported on this hardware
          platform!<br>I also get this message before the error: You are using a model
          of type whisper to instantiate a model of type . This is not supported for
          all configurations of models and can yield errors.</p>

          <p>Do you have any idea why this error?</p>

          <p>This is the code I am using:<br>from optimum.onnxruntime import ORTModelForSpeechSeq2Seq<br>from
          transformers import PretrainedConfig<br>import os<br>model_name = ''openai/whisper-large-v2''<br>model_path
          = ''./whisper-large-v2-onnx-int4''<br>model_config = PretrainedConfig.from_pretrained(model_name)<br>predictions
          = []<br>references = []<br>sessions = ORTModelForSpeechSeq2Seq.load_model(<br>            os.path.join(model_path,
          ''encoder_model.onnx''),<br>            os.path.join(model_path, ''decoder_model.onnx''),<br>            os.path.join(model_path,
          ''decoder_with_past_model.onnx''))</p>

          <p>model = ORTModelForSpeechSeq2Seq(sessions[0], sessions[1], model_config,
          model_path, sessions[2])</p>

          <p>Thank you</p>

          '
        raw: "Hello.\r\n\r\nFirst of all, thank you for this model. It looks amazing\
          \ to have whisper large v2 in this quantized format.\r\n\r\nI am trying\
          \ to run the code provided on the model card but get an error related to\
          \ the hardware: 4b quantization not yet supported on this hardware platform!\r\
          \nI also get this message before the error: You are using a model of type\
          \ whisper to instantiate a model of type . This is not supported for all\
          \ configurations of models and can yield errors.\r\n\r\nDo you have any\
          \ idea why this error?\r\n\r\nThis is the code I am using:\r\nfrom optimum.onnxruntime\
          \ import ORTModelForSpeechSeq2Seq\r\nfrom transformers import PretrainedConfig\r\
          \nimport os\r\nmodel_name = 'openai/whisper-large-v2'\r\nmodel_path = './whisper-large-v2-onnx-int4'\r\
          \nmodel_config = PretrainedConfig.from_pretrained(model_name)\r\npredictions\
          \ = []\r\nreferences = []\r\nsessions = ORTModelForSpeechSeq2Seq.load_model(\r\
          \n            os.path.join(model_path, 'encoder_model.onnx'),\r\n      \
          \      os.path.join(model_path, 'decoder_model.onnx'),\r\n            os.path.join(model_path,\
          \ 'decoder_with_past_model.onnx'))\r\n\r\nmodel = ORTModelForSpeechSeq2Seq(sessions[0],\
          \ sessions[1], model_config, model_path, sessions[2])\r\n\r\n\r\nThank you"
        updatedAt: '2023-10-25T20:52:11.539Z'
      numEdits: 0
      reactions: []
    id: 65397ffb48b006575d349207
    type: comment
  author: NemesisAlm
  content: "Hello.\r\n\r\nFirst of all, thank you for this model. It looks amazing\
    \ to have whisper large v2 in this quantized format.\r\n\r\nI am trying to run\
    \ the code provided on the model card but get an error related to the hardware:\
    \ 4b quantization not yet supported on this hardware platform!\r\nI also get this\
    \ message before the error: You are using a model of type whisper to instantiate\
    \ a model of type . This is not supported for all configurations of models and\
    \ can yield errors.\r\n\r\nDo you have any idea why this error?\r\n\r\nThis is\
    \ the code I am using:\r\nfrom optimum.onnxruntime import ORTModelForSpeechSeq2Seq\r\
    \nfrom transformers import PretrainedConfig\r\nimport os\r\nmodel_name = 'openai/whisper-large-v2'\r\
    \nmodel_path = './whisper-large-v2-onnx-int4'\r\nmodel_config = PretrainedConfig.from_pretrained(model_name)\r\
    \npredictions = []\r\nreferences = []\r\nsessions = ORTModelForSpeechSeq2Seq.load_model(\r\
    \n            os.path.join(model_path, 'encoder_model.onnx'),\r\n            os.path.join(model_path,\
    \ 'decoder_model.onnx'),\r\n            os.path.join(model_path, 'decoder_with_past_model.onnx'))\r\
    \n\r\nmodel = ORTModelForSpeechSeq2Seq(sessions[0], sessions[1], model_config,\
    \ model_path, sessions[2])\r\n\r\n\r\nThank you"
  created_at: 2023-10-25 19:52:11+00:00
  edited: false
  hidden: false
  id: 65397ffb48b006575d349207
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cfac6bd0b30496b064d5092831e65f3e.svg
      fullname: Emmanuel Schmidbauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eschmidbauer
      type: user
    createdAt: '2023-11-02T15:25:16.000Z'
    data:
      edited: true
      editors:
      - eschmidbauer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308702945709229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cfac6bd0b30496b064d5092831e65f3e.svg
          fullname: Emmanuel Schmidbauer
          isHf: false
          isPro: false
          name: eschmidbauer
          type: user
        html: '<p>I am getting the same error, i tried small &amp; large models.<br>Also
          tried on amd64 &amp; arm64 (m2)- same error<br>I did a bit of searching,
          it looks like there is little hardware support available at this type<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/61571d3bb8d28825f709ffc2/VvYaHnRHFedEzGXhCmFH2.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/61571d3bb8d28825f709ffc2/VvYaHnRHFedEzGXhCmFH2.png"></a></p>

          <p>See issue <a rel="nofollow" href="https://github.com/microsoft/onnxruntime/issues/17883">https://github.com/microsoft/onnxruntime/issues/17883</a></p>

          '
        raw: "I am getting the same error, i tried small & large models. \nAlso tried\
          \ on amd64 & arm64 (m2)- same error\nI did a bit of searching, it looks\
          \ like there is little hardware support available at this type\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61571d3bb8d28825f709ffc2/VvYaHnRHFedEzGXhCmFH2.png)\n\
          \nSee issue https://github.com/microsoft/onnxruntime/issues/17883"
        updatedAt: '2023-11-02T15:28:54.775Z'
      numEdits: 2
      reactions: []
    id: 6543bf5cfcb96b8b484b36d5
    type: comment
  author: eschmidbauer
  content: "I am getting the same error, i tried small & large models. \nAlso tried\
    \ on amd64 & arm64 (m2)- same error\nI did a bit of searching, it looks like there\
    \ is little hardware support available at this type\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/61571d3bb8d28825f709ffc2/VvYaHnRHFedEzGXhCmFH2.png)\n\
    \nSee issue https://github.com/microsoft/onnxruntime/issues/17883"
  created_at: 2023-11-02 14:25:16+00:00
  edited: true
  hidden: false
  id: 6543bf5cfcb96b8b484b36d5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Intel/whisper-large-v2-onnx-int4
repo_type: model
status: open
target_branch: null
title: Hardware not supported
