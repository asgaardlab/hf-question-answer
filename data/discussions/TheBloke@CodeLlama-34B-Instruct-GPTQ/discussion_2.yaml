!!python/object:huggingface_hub.community.DiscussionWithDetails
author: m9e
conflicting_files: null
created_at: 2023-08-25 18:52:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
      fullname: Matthew Wallace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m9e
      type: user
    createdAt: '2023-08-25T19:52:16.000Z'
    data:
      edited: false
      editors:
      - m9e
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.908089816570282
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
          fullname: Matthew Wallace
          isHf: false
          isPro: false
          name: m9e
          type: user
        html: '<p>"GS: GPTQ group size. Higher numbers use less VRAM, but have lower
          quantisation accuracy. "None" is the lowest possible value." in the description,
          but then </p>

          <p>"8-bit, with group size 128g for higher inference quality and with Act
          Order for even higher accuracy. Poor AutoGPTQ CUDA speed."</p>

          <p>Description of whether large group size vs small/no group size results
          in higher inference quality appears to contradict between the general description
          and the row-wise model-specific description.</p>

          '
        raw: "\"GS: GPTQ group size. Higher numbers use less VRAM, but have lower\
          \ quantisation accuracy. \"None\" is the lowest possible value.\" in the\
          \ description, but then \r\n\r\n\"8-bit, with group size 128g for higher\
          \ inference quality and with Act Order for even higher accuracy. Poor AutoGPTQ\
          \ CUDA speed.\"\r\n\r\nDescription of whether large group size vs small/no\
          \ group size results in higher inference quality appears to contradict between\
          \ the general description and the row-wise model-specific description."
        updatedAt: '2023-08-25T19:52:16.641Z'
      numEdits: 0
      reactions: []
    id: 64e90670e74f54587ca3b900
    type: comment
  author: m9e
  content: "\"GS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation\
    \ accuracy. \"None\" is the lowest possible value.\" in the description, but then\
    \ \r\n\r\n\"8-bit, with group size 128g for higher inference quality and with\
    \ Act Order for even higher accuracy. Poor AutoGPTQ CUDA speed.\"\r\n\r\nDescription\
    \ of whether large group size vs small/no group size results in higher inference\
    \ quality appears to contradict between the general description and the row-wise\
    \ model-specific description."
  created_at: 2023-08-25 18:52:16+00:00
  edited: false
  hidden: false
  id: 64e90670e74f54587ca3b900
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T19:58:09.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497715830802917
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s a bit confusing and I should probably try and clarify that
          more, yes.</p>

          <p>And I definitely need to clear up my table, which uses words like "higher
          inference quality" without making it clear what it''s higher than.  Some
          of that is a vestige from when I only showed a couple of options, so "128
          = higher quality" was vs None, the only other option at that time.  I''ll
          be re-doing that table soon and will make it clearer.</p>

          <p>To clarify the progression:  "small group size" and "no group size" are
          at opposite ends of the scale.  So:</p>

          <ul>

          <li>None -- Lowest quality, smallest file size, lowest VRAM usage</li>

          <li>1024 (I don''t make this one any more)</li>

          <li>128</li>

          <li>64 </li>

          <li>32 -- Highest quality, largest file size, largest VRAM usage</li>

          </ul>

          <p>Technically you could go even further into small, eg group size 16. But
          I''ve never bothered with that, or seen anyone else do so.  Likewise at
          the other end you could have 256 or 512 or, I think, 2048.  But I''ve stopped
          making 1024 even, and now just do 128, 64 and 32.</p>

          <p>In time I may drop 64 too, as I''m not sure if anyone is actually using
          it.</p>

          '
        raw: "It's a bit confusing and I should probably try and clarify that more,\
          \ yes.\n\nAnd I definitely need to clear up my table, which uses words like\
          \ \"higher inference quality\" without making it clear what it's higher\
          \ than.  Some of that is a vestige from when I only showed a couple of options,\
          \ so \"128 = higher quality\" was vs None, the only other option at that\
          \ time.  I'll be re-doing that table soon and will make it clearer.\n\n\
          To clarify the progression:  \"small group size\" and \"no group size\"\
          \ are at opposite ends of the scale.  So:\n\n- None -- Lowest quality, smallest\
          \ file size, lowest VRAM usage\n- 1024 (I don't make this one any more)\n\
          - 128\n- 64 \n- 32 -- Highest quality, largest file size, largest VRAM usage\n\
          \nTechnically you could go even further into small, eg group size 16. But\
          \ I've never bothered with that, or seen anyone else do so.  Likewise at\
          \ the other end you could have 256 or 512 or, I think, 2048.  But I've stopped\
          \ making 1024 even, and now just do 128, 64 and 32.\n\nIn time I may drop\
          \ 64 too, as I'm not sure if anyone is actually using it."
        updatedAt: '2023-08-25T19:58:34.399Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - m9e
    id: 64e907d1c6511bd192ebc35b
    type: comment
  author: TheBloke
  content: "It's a bit confusing and I should probably try and clarify that more,\
    \ yes.\n\nAnd I definitely need to clear up my table, which uses words like \"\
    higher inference quality\" without making it clear what it's higher than.  Some\
    \ of that is a vestige from when I only showed a couple of options, so \"128 =\
    \ higher quality\" was vs None, the only other option at that time.  I'll be re-doing\
    \ that table soon and will make it clearer.\n\nTo clarify the progression:  \"\
    small group size\" and \"no group size\" are at opposite ends of the scale.  So:\n\
    \n- None -- Lowest quality, smallest file size, lowest VRAM usage\n- 1024 (I don't\
    \ make this one any more)\n- 128\n- 64 \n- 32 -- Highest quality, largest file\
    \ size, largest VRAM usage\n\nTechnically you could go even further into small,\
    \ eg group size 16. But I've never bothered with that, or seen anyone else do\
    \ so.  Likewise at the other end you could have 256 or 512 or, I think, 2048.\
    \  But I've stopped making 1024 even, and now just do 128, 64 and 32.\n\nIn time\
    \ I may drop 64 too, as I'm not sure if anyone is actually using it."
  created_at: 2023-08-25 18:58:09+00:00
  edited: true
  hidden: false
  id: 64e907d1c6511bd192ebc35b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/CodeLlama-34B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Contradiction in model description
