!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viraniaman
conflicting_files: null
created_at: 2023-09-08 02:19:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/920dc85cec1f00dc503aed663d56a662.svg
      fullname: Aman Virani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viraniaman
      type: user
    createdAt: '2023-09-08T03:19:58.000Z'
    data:
      edited: false
      editors:
      - viraniaman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5831636190414429
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/920dc85cec1f00dc503aed663d56a662.svg
          fullname: Aman Virani
          isHf: false
          isPro: false
          name: viraniaman
          type: user
        html: "<p>Llama variants seem to have frequently faced these issues. Relevant\
          \ discussions:<br>Issue 2 in <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806\"\
          >https://github.com/huggingface/text-generation-inference/issues/769\n</a><br><a\
          \ href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/5\"\
          >https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/5</a></p>\n\
          <p>My input and output:</p>\n<pre><code class=\"language-shell\">docker\
          \ run --gpus all -e GPTQ_BITS=4 -e GPTQ_GROUPSIZE=32  -p 8080:80 -v $volume:/data\
          \ ghcr.io/huggingface/text-generation-inference:1.0.3 --model-id $model\
          \ --revision $revision --quantize gptq\n\n2023-09-08T02:41:23.437900Z  INFO\
          \ text_generation_launcher: Args { model_id: \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\
          , revision: Some(\"gptq-4bit-32g-actorder_True\"), validation_workers: 2,\
          \ sharded: None, num_shard: None, quantize: Some(Gptq), dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ None, max_waiting_tokens: 20, hostname: \"7c73057b8d56\", port: 80, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
          \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
          \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_edge: None, env: false }\n2023-09-08T02:41:23.438009Z  INFO\
          \ download: text_generation_launcher: Starting download process.\n2023-09-08T02:41:25.850890Z\
          \  INFO text_generation_launcher: Files are already present on the host.\
          \ Skipping download.\n\n2023-09-08T02:41:26.240869Z  INFO download: text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-09-08T02:41:26.241382Z  INFO shard-manager:\
          \ text_generation_launcher: Starting shard rank=0\n2023-09-08T02:41:30.648305Z\
          \  INFO text_generation_launcher: Using exllama kernels\n\n2023-09-08T02:41:30.654582Z\
          \ ERROR text_generation_launcher: Error when initializing model\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 195, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          <span class=\"hljs-meta prompt_\">&gt; </span><span class=\"language-bash\"\
          >File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          </span>, line 147, <span class=\"hljs-keyword\">in</span> serve_inner</span>\n\
          \    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 187, in get_model\n    return FlashLlama(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 68, in __init__\n    model = FlashLlamaForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 461, in __init__\n    self.model = FlashLlamaModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 399, in __init__\n    [\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 400, in &lt;listcomp&gt;\n    FlashLlamaLayer(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 336, in __init__\n    self.self_attn = FlashLlamaAttention(\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 218, in __init__\n    self.o_proj = TensorParallelRowLinear.load(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 361, in load\n    get_linear(weight, bias, config.quantize),\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 233, in get_linear\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
          \ g_idx, bias, bits, groupsize)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 114, in __init__\n    assert groupsize == self.groupsize\nAssertionError\n\
          \n2023-09-08T02:41:31.147356Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\n\nYou are using the default legacy\
          \ behaviour of the &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;.\
          \ If you see this, DO NOT PANIC! This is expected, and simply means that\
          \ the `legacy` (previous) behavior will be used so nothing changes for you.\
          \ If you want to use the new behaviour, set `legacy=True`. This should only\
          \ be set if you understand what it means, and thouroughly read the reason\
          \ why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n\
          You are using a model of type llama to instantiate a model of type . This\
          \ is not supported for all configurations of models and can yield errors.\n\
          Traceback (most recent call last):\n\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 195, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 147, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 187, in get_model\n    return FlashLlama(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 68, in __init__\n    model = FlashLlamaForCausalLM(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 461, in __init__\n    self.model = FlashLlamaModel(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 399, in __init__\n    [\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 400, in &lt;listcomp&gt;\n    FlashLlamaLayer(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 336, in __init__\n    self.self_attn = FlashLlamaAttention(\n\n \
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 218, in __init__\n    self.o_proj = TensorParallelRowLinear.load(\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 361, in load\n    get_linear(weight, bias, config.quantize),\n\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 233, in get_linear\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
          \ g_idx, bias, bits, groupsize)\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 114, in __init__\n    assert groupsize == self.groupsize\n\nAssertionError\n\
          \ rank=0\n2023-09-08T02:41:31.245302Z ERROR text_generation_launcher: Shard\
          \ 0 failed to start\n2023-09-08T02:41:31.245320Z  INFO text_generation_launcher:\
          \ Shutting down shards\nError: ShardCannotStart\n</code></pre>\n<p>Looks\
          \ like a change in <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/blob/0a63e9ab688cf715d31574ee5bb31025ff22ceec/server/text_generation_server/models/custom_modeling/flash_llama_modeling.py#L4\"\
          >this file</a> will fix the issue according to <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806\"\
          >this comment</a>, but I am not sure what exactly would need to be done.\
          \ If someone can point me in the right direction I can raise a PR. </p>\n\
          <p>I am a noob at loading and running LLMs, but am an SWE with 5y of exp\
          \ FWIW. Please let me know what I should do next. Thanks!</p>\n<p>Also number\
          \ of shards seems to make a difference in some cases. Is that expected to\
          \ be the case here? </p>\n"
        raw: "Llama variants seem to have frequently faced these issues. Relevant\
          \ discussions:\r\nIssue 2 in [https://github.com/huggingface/text-generation-inference/issues/769\r\
          \n](https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806)\r\
          \nhttps://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/5\r\n\
          \r\nMy input and output:\r\n```shell\r\ndocker run --gpus all -e GPTQ_BITS=4\
          \ -e GPTQ_GROUPSIZE=32  -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.0.3\
          \ --model-id $model --revision $revision --quantize gptq\r\n\r\n2023-09-08T02:41:23.437900Z\
          \  INFO text_generation_launcher: Args { model_id: \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\
          , revision: Some(\"gptq-4bit-32g-actorder_True\"), validation_workers: 2,\
          \ sharded: None, num_shard: None, quantize: Some(Gptq), dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ None, max_waiting_tokens: 20, hostname: \"7c73057b8d56\", port: 80, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
          \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
          \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_edge: None, env: false }\r\n2023-09-08T02:41:23.438009Z  INFO\
          \ download: text_generation_launcher: Starting download process.\r\n2023-09-08T02:41:25.850890Z\
          \  INFO text_generation_launcher: Files are already present on the host.\
          \ Skipping download.\r\n\r\n2023-09-08T02:41:26.240869Z  INFO download:\
          \ text_generation_launcher: Successfully downloaded weights.\r\n2023-09-08T02:41:26.241382Z\
          \  INFO shard-manager: text_generation_launcher: Starting shard rank=0\r\
          \n2023-09-08T02:41:30.648305Z  INFO text_generation_launcher: Using exllama\
          \ kernels\r\n\r\n2023-09-08T02:41:30.654582Z ERROR text_generation_launcher:\
          \ Error when initializing model\r\nTraceback (most recent call last):\r\n\
          \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 195, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 147, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 187, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 461, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 399, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 400, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 336, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 218, in __init__\r\n    self.o_proj = TensorParallelRowLinear.load(\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 361, in load\r\n    get_linear(weight, bias, config.quantize),\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 233, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros,\
          \ scales, g_idx, bias, bits, groupsize)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 114, in __init__\r\n    assert groupsize == self.groupsize\r\nAssertionError\r\
          \n\r\n2023-09-08T02:41:31.147356Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\r\n\r\nYou are using the default\
          \ legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>.\
          \ If you see this, DO NOT PANIC! This is expected, and simply means that\
          \ the `legacy` (previous) behavior will be used so nothing changes for you.\
          \ If you want to use the new behaviour, set `legacy=True`. This should only\
          \ be set if you understand what it means, and thouroughly read the reason\
          \ why this was added as explained in https://github.com/huggingface/transformers/pull/24565\r\
          \nYou are using a model of type llama to instantiate a model of type . This\
          \ is not supported for all configurations of models and can yield errors.\r\
          \nTraceback (most recent call last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 195, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 147, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 187, in get_model\r\n    return FlashLlama(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 461, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 399, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 400, in <listcomp>\r\n    FlashLlamaLayer(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 336, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n\
          \r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 218, in __init__\r\n    self.o_proj = TensorParallelRowLinear.load(\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 361, in load\r\n    get_linear(weight, bias, config.quantize),\r\n\
          \r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 233, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros,\
          \ scales, g_idx, bias, bits, groupsize)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 114, in __init__\r\n    assert groupsize == self.groupsize\r\n\r\n\
          AssertionError\r\n rank=0\r\n2023-09-08T02:41:31.245302Z ERROR text_generation_launcher:\
          \ Shard 0 failed to start\r\n2023-09-08T02:41:31.245320Z  INFO text_generation_launcher:\
          \ Shutting down shards\r\nError: ShardCannotStart\r\n```\r\n\r\nLooks like\
          \ a change in [this file](https://github.com/huggingface/text-generation-inference/blob/0a63e9ab688cf715d31574ee5bb31025ff22ceec/server/text_generation_server/models/custom_modeling/flash_llama_modeling.py#L4)\
          \ will fix the issue according to [this comment](https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806),\
          \ but I am not sure what exactly would need to be done. If someone can point\
          \ me in the right direction I can raise a PR. \r\n\r\nI am a noob at loading\
          \ and running LLMs, but am an SWE with 5y of exp FWIW. Please let me know\
          \ what I should do next. Thanks!\r\n\r\nAlso number of shards seems to make\
          \ a difference in some cases. Is that expected to be the case here? "
        updatedAt: '2023-09-08T03:19:58.813Z'
      numEdits: 0
      reactions: []
    id: 64fa92dedcc5ce730e47b11a
    type: comment
  author: viraniaman
  content: "Llama variants seem to have frequently faced these issues. Relevant discussions:\r\
    \nIssue 2 in [https://github.com/huggingface/text-generation-inference/issues/769\r\
    \n](https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806)\r\
    \nhttps://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/5\r\n\r\n\
    My input and output:\r\n```shell\r\ndocker run --gpus all -e GPTQ_BITS=4 -e GPTQ_GROUPSIZE=32\
    \  -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.0.3\
    \ --model-id $model --revision $revision --quantize gptq\r\n\r\n2023-09-08T02:41:23.437900Z\
    \  INFO text_generation_launcher: Args { model_id: \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\
    , revision: Some(\"gptq-4bit-32g-actorder_True\"), validation_workers: 2, sharded:\
    \ None, num_shard: None, quantize: Some(Gptq), dtype: None, trust_remote_code:\
    \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4,\
    \ max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio:\
    \ 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: None, max_waiting_tokens:\
    \ 20, hostname: \"7c73057b8d56\", port: 80, shard_uds_path: \"/tmp/text-generation-server\"\
    , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache: Some(\"\
    /data\"), weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction:\
    \ 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint:\
    \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok:\
    \ false, ngrok_authtoken: None, ngrok_edge: None, env: false }\r\n2023-09-08T02:41:23.438009Z\
    \  INFO download: text_generation_launcher: Starting download process.\r\n2023-09-08T02:41:25.850890Z\
    \  INFO text_generation_launcher: Files are already present on the host. Skipping\
    \ download.\r\n\r\n2023-09-08T02:41:26.240869Z  INFO download: text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2023-09-08T02:41:26.241382Z  INFO shard-manager:\
    \ text_generation_launcher: Starting shard rank=0\r\n2023-09-08T02:41:30.648305Z\
    \  INFO text_generation_launcher: Using exllama kernels\r\n\r\n2023-09-08T02:41:30.654582Z\
    \ ERROR text_generation_launcher: Error when initializing model\r\nTraceback (most\
    \ recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\", line\
    \ 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 81, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 195, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 147, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 187, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 461, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 399, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 400, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 336, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 218, in __init__\r\n    self.o_proj = TensorParallelRowLinear.load(\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 361, in load\r\n    get_linear(weight, bias, config.quantize),\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 233, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
    \ g_idx, bias, bits, groupsize)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
    , line 114, in __init__\r\n    assert groupsize == self.groupsize\r\nAssertionError\r\
    \n\r\n2023-09-08T02:41:31.147356Z ERROR shard-manager: text_generation_launcher:\
    \ Shard complete standard error output:\r\n\r\nYou are using the default legacy\
    \ behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>.\
    \ If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy`\
    \ (previous) behavior will be used so nothing changes for you. If you want to\
    \ use the new behaviour, set `legacy=True`. This should only be set if you understand\
    \ what it means, and thouroughly read the reason why this was added as explained\
    \ in https://github.com/huggingface/transformers/pull/24565\r\nYou are using a\
    \ model of type llama to instantiate a model of type . This is not supported for\
    \ all configurations of models and can yield errors.\r\nTraceback (most recent\
    \ call last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\", line 8,\
    \ in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 81, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 195, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 147, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 187, in get_model\r\n    return FlashLlama(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 461, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 399, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 400, in <listcomp>\r\n    FlashLlamaLayer(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 336, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 218, in __init__\r\n    self.o_proj = TensorParallelRowLinear.load(\r\n\
    \r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 361, in load\r\n    get_linear(weight, bias, config.quantize),\r\n\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 233, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
    \ g_idx, bias, bits, groupsize)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
    , line 114, in __init__\r\n    assert groupsize == self.groupsize\r\n\r\nAssertionError\r\
    \n rank=0\r\n2023-09-08T02:41:31.245302Z ERROR text_generation_launcher: Shard\
    \ 0 failed to start\r\n2023-09-08T02:41:31.245320Z  INFO text_generation_launcher:\
    \ Shutting down shards\r\nError: ShardCannotStart\r\n```\r\n\r\nLooks like a change\
    \ in [this file](https://github.com/huggingface/text-generation-inference/blob/0a63e9ab688cf715d31574ee5bb31025ff22ceec/server/text_generation_server/models/custom_modeling/flash_llama_modeling.py#L4)\
    \ will fix the issue according to [this comment](https://github.com/huggingface/text-generation-inference/issues/769#issuecomment-1664596806),\
    \ but I am not sure what exactly would need to be done. If someone can point me\
    \ in the right direction I can raise a PR. \r\n\r\nI am a noob at loading and\
    \ running LLMs, but am an SWE with 5y of exp FWIW. Please let me know what I should\
    \ do next. Thanks!\r\n\r\nAlso number of shards seems to make a difference in\
    \ some cases. Is that expected to be the case here? "
  created_at: 2023-09-08 02:19:58+00:00
  edited: false
  hidden: false
  id: 64fa92dedcc5ce730e47b11a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/920dc85cec1f00dc503aed663d56a662.svg
      fullname: Aman Virani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viraniaman
      type: user
    createdAt: '2023-09-08T03:22:19.000Z'
    data:
      from: Running into common issues when trying to run with TGI
      to: Running into issues when trying to run with TGI
    id: 64fa936bfa644654224ee9b6
    type: title-change
  author: viraniaman
  created_at: 2023-09-08 02:22:19+00:00
  id: 64fa936bfa644654224ee9b6
  new_title: Running into issues when trying to run with TGI
  old_title: Running into common issues when trying to run with TGI
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/920dc85cec1f00dc503aed663d56a662.svg
      fullname: Aman Virani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viraniaman
      type: user
    createdAt: '2023-09-08T04:45:40.000Z'
    data:
      edited: true
      editors:
      - viraniaman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.293050080537796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/920dc85cec1f00dc503aed663d56a662.svg
          fullname: Aman Virani
          isHf: false
          isPro: false
          name: viraniaman
          type: user
        html: '<p>After a bunch of hit and trial, this command worked:</p>

          <pre><code>docker run --gpus all -p 8080:80 -e DISABLE_EXLLAMA=True -v $volume:/data
          ghcr.io/huggingface/text-generation-inference:latest --model-id $model --revision
          $revision --quantize gptq --max-batch-prefill-tokens=1024

          </code></pre>

          <p>But the inference speed is quite slow</p>

          '
        raw: 'After a bunch of hit and trial, this command worked:


          ```

          docker run --gpus all -p 8080:80 -e DISABLE_EXLLAMA=True -v $volume:/data
          ghcr.io/huggingface/text-generation-inference:latest --model-id $model --revision
          $revision --quantize gptq --max-batch-prefill-tokens=1024

          ```


          But the inference speed is quite slow'
        updatedAt: '2023-09-08T04:45:57.353Z'
      numEdits: 1
      reactions: []
    id: 64faa6f40e486522f8083c05
    type: comment
  author: viraniaman
  content: 'After a bunch of hit and trial, this command worked:


    ```

    docker run --gpus all -p 8080:80 -e DISABLE_EXLLAMA=True -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
    --model-id $model --revision $revision --quantize gptq --max-batch-prefill-tokens=1024

    ```


    But the inference speed is quite slow'
  created_at: 2023-09-08 03:45:40+00:00
  edited: true
  hidden: false
  id: 64faa6f40e486522f8083c05
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/CodeLlama-34B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Running into issues when trying to run with TGI
