!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mzbac
conflicting_files: null
created_at: 2023-08-26 10:05:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
      fullname: 'null'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mzbac
      type: user
    createdAt: '2023-08-26T11:05:57.000Z'
    data:
      edited: false
      editors:
      - mzbac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6357935070991516
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
          fullname: 'null'
          isHf: false
          isPro: false
          name: mzbac
          type: user
        html: "<p>Somehow I keep getting an error for the fused llama attention.</p>\n\
          <pre><code>/auto_gptq/nn_modules/fused_llama_attn.py\", line 59, in forward\n\
          \    query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size,\
          \ dim=2)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>It\
          \ seems that qkv_states doesn't match</p>\n<pre><code>Shape of qkv_states:\
          \ torch.Size([1, 62, 10240])\nExpected third dimension size: 24576\nActual\
          \ third dimension size: 10240\nValue of self.hidden_size: 8192\n</code></pre>\n"
        raw: "Somehow I keep getting an error for the fused llama attention.\r\n```\r\
          \n/auto_gptq/nn_modules/fused_llama_attn.py\", line 59, in forward\r\n \
          \   query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size,\
          \ dim=2)\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\nIt seems\
          \ that qkv_states doesn't match\r\n```\r\nShape of qkv_states: torch.Size([1,\
          \ 62, 10240])\r\nExpected third dimension size: 24576\r\nActual third dimension\
          \ size: 10240\r\nValue of self.hidden_size: 8192\r\n```"
        updatedAt: '2023-08-26T11:05:57.974Z'
      numEdits: 0
      reactions: []
    id: 64e9dc9518d79efd532df864
    type: comment
  author: mzbac
  content: "Somehow I keep getting an error for the fused llama attention.\r\n```\r\
    \n/auto_gptq/nn_modules/fused_llama_attn.py\", line 59, in forward\r\n    query_states,\
    \ key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\r\
    \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\nIt seems that qkv_states\
    \ doesn't match\r\n```\r\nShape of qkv_states: torch.Size([1, 62, 10240])\r\n\
    Expected third dimension size: 24576\r\nActual third dimension size: 10240\r\n\
    Value of self.hidden_size: 8192\r\n```"
  created_at: 2023-08-26 10:05:57+00:00
  edited: false
  hidden: false
  id: 64e9dc9518d79efd532df864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
      fullname: Patrick Shechet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kajuberdut
      type: user
    createdAt: '2023-08-26T18:09:18.000Z'
    data:
      edited: false
      editors:
      - kajuberdut
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48726749420166016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
          fullname: Patrick Shechet
          isHf: false
          isPro: false
          name: kajuberdut
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/8kyrUMXkzc8UL0XoZuUHV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/8kyrUMXkzc8UL0XoZuUHV.png"></a><br>Works
          for me on the <code>1a642c12b582330a4780047461905787cc3f18c5</code> commit
          of ooga_booga with ExLlama_HF.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/PS1ztF5jVzdYVJYR_39V9.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/PS1ztF5jVzdYVJYR_39V9.png"></a></p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/8kyrUMXkzc8UL0XoZuUHV.png)

          Works for me on the `1a642c12b582330a4780047461905787cc3f18c5` commit of
          ooga_booga with ExLlama_HF.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/PS1ztF5jVzdYVJYR_39V9.png)

          '
        updatedAt: '2023-08-26T18:09:18.441Z'
      numEdits: 0
      reactions: []
    id: 64ea3fcec8193a0eef1f7aa8
    type: comment
  author: kajuberdut
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/8kyrUMXkzc8UL0XoZuUHV.png)

    Works for me on the `1a642c12b582330a4780047461905787cc3f18c5` commit of ooga_booga
    with ExLlama_HF.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/PS1ztF5jVzdYVJYR_39V9.png)

    '
  created_at: 2023-08-26 17:09:18+00:00
  edited: false
  hidden: false
  id: 64ea3fcec8193a0eef1f7aa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
      fullname: 'null'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mzbac
      type: user
    createdAt: '2023-08-27T00:44:29.000Z'
    data:
      edited: false
      editors:
      - mzbac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9362753629684448
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
          fullname: 'null'
          isHf: false
          isPro: false
          name: mzbac
          type: user
        html: '<p>@giblesnot, thanks mate, I was using AutoGPTQ to load the model.
          I will try the Exllama.</p>

          '
        raw: '@giblesnot, thanks mate, I was using AutoGPTQ to load the model. I will
          try the Exllama.'
        updatedAt: '2023-08-27T00:44:29.709Z'
      numEdits: 0
      reactions: []
    id: 64ea9c6db96ff0e175409993
    type: comment
  author: mzbac
  content: '@giblesnot, thanks mate, I was using AutoGPTQ to load the model. I will
    try the Exllama.'
  created_at: 2023-08-26 23:44:29+00:00
  edited: false
  hidden: false
  id: 64ea9c6db96ff0e175409993
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb51217d31f0a1b8a7e0e4a03b4c500b.svg
      fullname: Max Hager
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yachty66
      type: user
    createdAt: '2023-08-29T12:16:47.000Z'
    data:
      edited: false
      editors:
      - yachty66
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8728748559951782
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb51217d31f0a1b8a7e0e4a03b4c500b.svg
          fullname: Max Hager
          isHf: false
          isPro: false
          name: yachty66
          type: user
        html: '<p>same here when running not in oogabooga. i am using the code for
          running it with AutoGPTQForCausalLM from <code>huggingface</code> and it''s
          failing with the same error. please help.</p>

          '
        raw: same here when running not in oogabooga. i am using the code for running
          it with AutoGPTQForCausalLM from `huggingface` and it's failing with the
          same error. please help.
        updatedAt: '2023-08-29T12:16:47.524Z'
      numEdits: 0
      reactions: []
    id: 64ede1af94d80aa8e86f2d2e
    type: comment
  author: yachty66
  content: same here when running not in oogabooga. i am using the code for running
    it with AutoGPTQForCausalLM from `huggingface` and it's failing with the same
    error. please help.
  created_at: 2023-08-29 11:16:47+00:00
  edited: false
  hidden: false
  id: 64ede1af94d80aa8e86f2d2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-29T12:18:58.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6334988474845886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If using AutoGPTQ, please pass <code>inject_fused_attention=False</code>.</p>

          <p>The issue is that Llama 2 70B and 34B introduced a new optimisation called
          GQA (Ghost Query Attention) which is not supported by AutoGPTQ''s Fused
          Attention optimisation.</p>

          <p>This issue will be fixed in AutoGPTQ fairly soon, hopefully: <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/pull/237">https://github.com/PanQiWei/AutoGPTQ/pull/237</a></p>

          '
        raw: 'If using AutoGPTQ, please pass `inject_fused_attention=False`.


          The issue is that Llama 2 70B and 34B introduced a new optimisation called
          GQA (Ghost Query Attention) which is not supported by AutoGPTQ''s Fused
          Attention optimisation.


          This issue will be fixed in AutoGPTQ fairly soon, hopefully: https://github.com/PanQiWei/AutoGPTQ/pull/237'
        updatedAt: '2023-08-29T12:19:41.561Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - mzbac
        - yachty66
        - arogov
    id: 64ede2329ed77553c6e54bc9
    type: comment
  author: TheBloke
  content: 'If using AutoGPTQ, please pass `inject_fused_attention=False`.


    The issue is that Llama 2 70B and 34B introduced a new optimisation called GQA
    (Ghost Query Attention) which is not supported by AutoGPTQ''s Fused Attention
    optimisation.


    This issue will be fixed in AutoGPTQ fairly soon, hopefully: https://github.com/PanQiWei/AutoGPTQ/pull/237'
  created_at: 2023-08-29 11:18:58+00:00
  edited: true
  hidden: false
  id: 64ede2329ed77553c6e54bc9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/CodeLlama-34B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Is the 34B llama2 actually GPTQ working?
