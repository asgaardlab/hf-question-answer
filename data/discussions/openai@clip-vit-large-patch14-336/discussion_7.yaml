!!python/object:huggingface_hub.community.DiscussionWithDetails
author: IRedDragonICY
conflicting_files: null
created_at: 2023-05-04 04:36:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af60f0912d983102a712a17ad6f1b195.svg
      fullname: Mohammad Farid Hendianto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IRedDragonICY
      type: user
    createdAt: '2023-05-04T05:36:11.000Z'
    data:
      edited: false
      editors:
      - IRedDragonICY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af60f0912d983102a712a17ad6f1b195.svg
          fullname: Mohammad Farid Hendianto
          isHf: false
          isPro: false
          name: IRedDragonICY
          type: user
        html: '<p>how to use clip-vit-large-patch14-336 in stable diffusion web UI?</p>

          '
        raw: how to use clip-vit-large-patch14-336 in stable diffusion web UI?
        updatedAt: '2023-05-04T05:36:11.444Z'
      numEdits: 0
      reactions: []
    id: 6453444bbeba452314002bdd
    type: comment
  author: IRedDragonICY
  content: how to use clip-vit-large-patch14-336 in stable diffusion web UI?
  created_at: 2023-05-04 04:36:11+00:00
  edited: false
  hidden: false
  id: 6453444bbeba452314002bdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/471b0d72bd3bed768b5c210359b686f8.svg
      fullname: texturalnewbie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: texturalnewbie
      type: user
    createdAt: '2023-06-01T13:05:15.000Z'
    data:
      edited: true
      editors:
      - texturalnewbie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/471b0d72bd3bed768b5c210359b686f8.svg
          fullname: texturalnewbie
          isHf: false
          isPro: false
          name: texturalnewbie
          type: user
        html: '<p>To incorporate the OpenAI CLIP model within the Automatic1111 web-ui
          and SDWeb Clip Changer extension, you can follow the instructions below:</p>

          <ol>

          <li><p><strong>Install the Extension</strong>: The SDWeb Clip Changer extension
          can be downloaded from its official GitHub repository <a rel="nofollow"
          href="https://github.com/bbc-mc/sdweb-clip-changer">here</a>.</p>

          </li>

          <li><p><strong>Configure Settings</strong>: After the installation, go to
          the extension''s settings page. Navigate to <code>Clip Changer</code> and
          enter <code>openai/clip-vit-large-patch14-336</code> in the field provided
          for specifying the CLIP model.</p>

          </li>

          <li><p><strong>Enable Clip Model Changer</strong>: Locate and check the
          box for the option <code>Enable CLIP Changer</code>.</p>

          </li>

          <li><p><strong>Save and Apply Changes</strong>: Click on the "Apply Setting"
          button to ensure your settings are stored, and then apply these changes.</p>

          </li>

          <li><p><strong>Switch Models</strong>: The modifications are applied only
          upon a model change. Switch the model to initiate these changes. The model
          will be downloaded automatically; this process can be monitored in the console.</p>

          </li>

          </ol>

          <blockquote>

          <p><strong>Note</strong>: Depending on your system, you may need to modify
          the code within the <code>sdweb_clip_changer</code> script. If you encounter
          an error related to tensors being on the CPU, you might need to change <code>.to(sd_model.cond_stage_model.transformer.device)</code>
          to <code>.to(''cuda'')</code>. This alteration directs the tensors to use
          the GPU (if available) instead of the CPU. </p>

          </blockquote>

          <p>Remember to keep track of the console during the process for any relevant
          notifications or prompts.</p>

          '
        raw: "To incorporate the OpenAI CLIP model within the Automatic1111 web-ui\
          \ and SDWeb Clip Changer extension, you can follow the instructions below:\n\
          \n1. **Install the Extension**: The SDWeb Clip Changer extension can be\
          \ downloaded from its official GitHub repository [here](https://github.com/bbc-mc/sdweb-clip-changer).\n\
          \n2. **Configure Settings**: After the installation, go to the extension's\
          \ settings page. Navigate to `Clip Changer` and enter `openai/clip-vit-large-patch14-336`\
          \ in the field provided for specifying the CLIP model.\n\n3. **Enable Clip\
          \ Model Changer**: Locate and check the box for the option `Enable CLIP\
          \ Changer`.\n\n4. **Save and Apply Changes**: Click on the \"Apply Setting\"\
          \ button to ensure your settings are stored, and then apply these changes.\n\
          \n5. **Switch Models**: The modifications are applied only upon a model\
          \ change. Switch the model to initiate these changes. The model will be\
          \ downloaded automatically; this process can be monitored in the console.\n\
          \n> **Note**: Depending on your system, you may need to modify the code\
          \ within the `sdweb_clip_changer` script. If you encounter an error related\
          \ to tensors being on the CPU, you might need to change `.to(sd_model.cond_stage_model.transformer.device)`\
          \ to `.to('cuda')`. This alteration directs the tensors to use the GPU (if\
          \ available) instead of the CPU. \n\nRemember to keep track of the console\
          \ during the process for any relevant notifications or prompts."
        updatedAt: '2023-06-01T13:48:57.263Z'
      numEdits: 7
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - IRedDragonICY
    id: 6478978b1f9756aa89d21534
    type: comment
  author: texturalnewbie
  content: "To incorporate the OpenAI CLIP model within the Automatic1111 web-ui and\
    \ SDWeb Clip Changer extension, you can follow the instructions below:\n\n1. **Install\
    \ the Extension**: The SDWeb Clip Changer extension can be downloaded from its\
    \ official GitHub repository [here](https://github.com/bbc-mc/sdweb-clip-changer).\n\
    \n2. **Configure Settings**: After the installation, go to the extension's settings\
    \ page. Navigate to `Clip Changer` and enter `openai/clip-vit-large-patch14-336`\
    \ in the field provided for specifying the CLIP model.\n\n3. **Enable Clip Model\
    \ Changer**: Locate and check the box for the option `Enable CLIP Changer`.\n\n\
    4. **Save and Apply Changes**: Click on the \"Apply Setting\" button to ensure\
    \ your settings are stored, and then apply these changes.\n\n5. **Switch Models**:\
    \ The modifications are applied only upon a model change. Switch the model to\
    \ initiate these changes. The model will be downloaded automatically; this process\
    \ can be monitored in the console.\n\n> **Note**: Depending on your system, you\
    \ may need to modify the code within the `sdweb_clip_changer` script. If you encounter\
    \ an error related to tensors being on the CPU, you might need to change `.to(sd_model.cond_stage_model.transformer.device)`\
    \ to `.to('cuda')`. This alteration directs the tensors to use the GPU (if available)\
    \ instead of the CPU. \n\nRemember to keep track of the console during the process\
    \ for any relevant notifications or prompts."
  created_at: 2023-06-01 12:05:15+00:00
  edited: true
  hidden: false
  id: 6478978b1f9756aa89d21534
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af60f0912d983102a712a17ad6f1b195.svg
      fullname: Mohammad Farid Hendianto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IRedDragonICY
      type: user
    createdAt: '2023-06-02T18:00:04.000Z'
    data:
      edited: false
      editors:
      - IRedDragonICY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af60f0912d983102a712a17ad6f1b195.svg
          fullname: Mohammad Farid Hendianto
          isHf: false
          isPro: false
          name: IRedDragonICY
          type: user
        html: '<p>thank you so much</p>

          '
        raw: thank you so much
        updatedAt: '2023-06-02T18:00:04.520Z'
      numEdits: 0
      reactions: []
    id: 647a2e24f518a860fbd06027
    type: comment
  author: IRedDragonICY
  content: thank you so much
  created_at: 2023-06-02 17:00:04+00:00
  edited: false
  hidden: false
  id: 647a2e24f518a860fbd06027
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/af60f0912d983102a712a17ad6f1b195.svg
      fullname: Mohammad Farid Hendianto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IRedDragonICY
      type: user
    createdAt: '2023-06-02T18:00:07.000Z'
    data:
      status: closed
    id: 647a2e27f518a860fbd0608d
    type: status-change
  author: IRedDragonICY
  created_at: 2023-06-02 17:00:07+00:00
  id: 647a2e27f518a860fbd0608d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: openai/clip-vit-large-patch14-336
repo_type: model
status: closed
target_branch: null
title: '[Question] Use clip on Stable diffusion.'
