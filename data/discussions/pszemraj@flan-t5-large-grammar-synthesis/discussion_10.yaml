!!python/object:huggingface_hub.community.DiscussionWithDetails
author: passivebook
conflicting_files: null
created_at: 2023-06-01 16:55:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
      fullname: Passive Book
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: passivebook
      type: user
    createdAt: '2023-06-01T17:55:37.000Z'
    data:
      edited: true
      editors:
      - passivebook
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
          fullname: Passive Book
          isHf: false
          isPro: false
          name: passivebook
          type: user
        html: "<p>Random Text is generated instead of correcting the sentence. </p>\n\
          <pre><code>from transformers import pipeline\n\ncorrector = pipeline(\n\
          \              'text2text-generation',\n              'pszemraj/flan-t5-large-grammar-synthesis',\n\
          \              )\n\nraw_text = 'sky is blu.'\nresults = corrector(raw_text)\n\
          print(results)\n</code></pre>\n<p>Here is the output:</p>\n<pre><code>[{'generated_text':\
          \ 'Blue is my favorite color. I love blue.'}]\n</code></pre>\n"
        raw: "Random Text is generated instead of correcting the sentence. \n\n```\n\
          from transformers import pipeline\n\ncorrector = pipeline(\n           \
          \   'text2text-generation',\n              'pszemraj/flan-t5-large-grammar-synthesis',\n\
          \              )\n\nraw_text = 'sky is blu.'\nresults = corrector(raw_text)\n\
          print(results)\n```\n\nHere is the output:\n```\n[{'generated_text': 'Blue\
          \ is my favorite color. I love blue.'}]\n```"
        updatedAt: '2023-06-01T17:56:10.234Z'
      numEdits: 1
      reactions: []
    id: 6478db9950ff70016316c2da
    type: comment
  author: passivebook
  content: "Random Text is generated instead of correcting the sentence. \n\n```\n\
    from transformers import pipeline\n\ncorrector = pipeline(\n              'text2text-generation',\n\
    \              'pszemraj/flan-t5-large-grammar-synthesis',\n              )\n\n\
    raw_text = 'sky is blu.'\nresults = corrector(raw_text)\nprint(results)\n```\n\
    \nHere is the output:\n```\n[{'generated_text': 'Blue is my favorite color. I\
    \ love blue.'}]\n```"
  created_at: 2023-06-01 16:55:37+00:00
  edited: true
  hidden: false
  id: 6478db9950ff70016316c2da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-02T02:35:59.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Hi, thanks for sharing! This might because the model was trained
          on mostly full sentences (1++) at a time. Also, the min_length parameter
          is by default 8 iirc and I think that may be too long for this use case.
          </p>

          <p>Can you try passing min_length=1 as an additional kwarg to when you call
          the corrector object?</p>

          '
        raw: "Hi, thanks for sharing! This might because the model was trained on\
          \ mostly full sentences (1++) at a time. Also, the min_length parameter\
          \ is by default 8 iirc and I think that may be too long for this use case.\
          \ \n\nCan you try passing min_length=1 as an additional kwarg to when you\
          \ call the corrector object?"
        updatedAt: '2023-06-02T02:35:59.543Z'
      numEdits: 0
      reactions: []
    id: 6479558fba447930a6fdd69d
    type: comment
  author: pszemraj
  content: "Hi, thanks for sharing! This might because the model was trained on mostly\
    \ full sentences (1++) at a time. Also, the min_length parameter is by default\
    \ 8 iirc and I think that may be too long for this use case. \n\nCan you try passing\
    \ min_length=1 as an additional kwarg to when you call the corrector object?"
  created_at: 2023-06-02 01:35:59+00:00
  edited: false
  hidden: false
  id: 6479558fba447930a6fdd69d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
      fullname: Passive Book
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: passivebook
      type: user
    createdAt: '2023-06-02T13:16:32.000Z'
    data:
      edited: true
      editors:
      - passivebook
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
          fullname: Passive Book
          isHf: false
          isPro: false
          name: passivebook
          type: user
        html: '<p>Tried it. Here is what I am getting. The expected output is "The
          sky is blue."</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6477b2038da268f5f23f0665/9ESvEmpESYCQ9HoYG5Qc1.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6477b2038da268f5f23f0665/9ESvEmpESYCQ9HoYG5Qc1.png"></a></p>

          <p>When you say the model is trained on full sentences, should I group sentences
          together and pass it into the model?</p>

          '
        raw: 'Tried it. Here is what I am getting. The expected output is "The sky
          is blue."


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477b2038da268f5f23f0665/9ESvEmpESYCQ9HoYG5Qc1.png)


          When you say the model is trained on full sentences, should I group sentences
          together and pass it into the model?'
        updatedAt: '2023-06-02T13:18:59.062Z'
      numEdits: 1
      reactions: []
    id: 6479ebb0a84498f2af4a4a41
    type: comment
  author: passivebook
  content: 'Tried it. Here is what I am getting. The expected output is "The sky is
    blue."


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477b2038da268f5f23f0665/9ESvEmpESYCQ9HoYG5Qc1.png)


    When you say the model is trained on full sentences, should I group sentences
    together and pass it into the model?'
  created_at: 2023-06-02 12:16:32+00:00
  edited: true
  hidden: false
  id: 6479ebb0a84498f2af4a4a41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-02T20:24:51.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9071016311645508
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Gotcha, thanks for the details. I did <a rel="nofollow" href="https://colab.research.google.com/gist/pszemraj/267a20ae16dcdfb55d631dec40dc78a7/flan-t5-large-grammar-synthesis-sky-is-blue-issue.ipynb">some
          testing</a> and interestingly the large model here is not great at this,
          and instead assumes you were trying to say something else. The <a href="https://huggingface.co/pszemraj/flan-t5-xl-grammar-synthesis">grammar
          synthesis XL model</a> is capable of handling this easily though, even at
          two beams. If you''re look to max performance, I''d recommend using that
          instead, despite the higher compute cost (you can use <code>load_in_8bit=True</code>
          as I do in the notebook)</p>

          <p><a rel="nofollow" href="https://i.imgur.com/NqjhY3z.png"><img alt="xl-model"
          src="https://i.imgur.com/NqjhY3z.png"></a></p>

          <ul>

          <li>link to the XL testing colab <a rel="nofollow" href="https://colab.research.google.com/gist/pszemraj/a207c0983b2c8e41f9d3948216276f08/flan-t5-xl-grammar-synthesis-sky-is-blue-issue.ipynb">here</a></li>

          <li>If I retrain another set of grammar synthesis models, I''ll be sure
          to include this example and permutations, as well as more short-text examples.</li>

          </ul>

          <blockquote>

          <p>When you say the model is trained on full sentences, should I group sentences
          together and pass it into the model?</p>

          </blockquote>

          <p>yes, I would pass 1-6 sentences of text at a time. there is a notebook
          on this model''s card that show cases this. Hope this helps!</p>

          '
        raw: 'Gotcha, thanks for the details. I did [some testing](https://colab.research.google.com/gist/pszemraj/267a20ae16dcdfb55d631dec40dc78a7/flan-t5-large-grammar-synthesis-sky-is-blue-issue.ipynb)
          and interestingly the large model here is not great at this, and instead
          assumes you were trying to say something else. The [grammar synthesis XL
          model](https://huggingface.co/pszemraj/flan-t5-xl-grammar-synthesis) is
          capable of handling this easily though, even at two beams. If you''re look
          to max performance, I''d recommend using that instead, despite the higher
          compute cost (you can use `load_in_8bit=True` as I do in the notebook)


          ![xl-model](https://i.imgur.com/NqjhY3z.png)


          - link to the XL testing colab [here](https://colab.research.google.com/gist/pszemraj/a207c0983b2c8e41f9d3948216276f08/flan-t5-xl-grammar-synthesis-sky-is-blue-issue.ipynb)

          - If I retrain another set of grammar synthesis models, I''ll be sure to
          include this example and permutations, as well as more short-text examples.


          > When you say the model is trained on full sentences, should I group sentences
          together and pass it into the model?


          yes, I would pass 1-6 sentences of text at a time. there is a notebook on
          this model''s card that show cases this. Hope this helps!'
        updatedAt: '2023-06-02T20:24:51.243Z'
      numEdits: 0
      reactions: []
    id: 647a501342abe277475b079b
    type: comment
  author: pszemraj
  content: 'Gotcha, thanks for the details. I did [some testing](https://colab.research.google.com/gist/pszemraj/267a20ae16dcdfb55d631dec40dc78a7/flan-t5-large-grammar-synthesis-sky-is-blue-issue.ipynb)
    and interestingly the large model here is not great at this, and instead assumes
    you were trying to say something else. The [grammar synthesis XL model](https://huggingface.co/pszemraj/flan-t5-xl-grammar-synthesis)
    is capable of handling this easily though, even at two beams. If you''re look
    to max performance, I''d recommend using that instead, despite the higher compute
    cost (you can use `load_in_8bit=True` as I do in the notebook)


    ![xl-model](https://i.imgur.com/NqjhY3z.png)


    - link to the XL testing colab [here](https://colab.research.google.com/gist/pszemraj/a207c0983b2c8e41f9d3948216276f08/flan-t5-xl-grammar-synthesis-sky-is-blue-issue.ipynb)

    - If I retrain another set of grammar synthesis models, I''ll be sure to include
    this example and permutations, as well as more short-text examples.


    > When you say the model is trained on full sentences, should I group sentences
    together and pass it into the model?


    yes, I would pass 1-6 sentences of text at a time. there is a notebook on this
    model''s card that show cases this. Hope this helps!'
  created_at: 2023-06-02 19:24:51+00:00
  edited: false
  hidden: false
  id: 647a501342abe277475b079b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-02T20:31:34.000Z'
    data:
      edited: true
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7849127650260925
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<h2 id="custom-grammarsynthesizer-class">custom GrammarSynthesizer
          class</h2>

          <p>to make things hopefully a bit simpler, I made a custom class based on
          transformers anyone can use that makes it a bit easier to integrate bitsandbytes
          for the XL model in 8-bit:</p>




          <p><a rel="nofollow" href="https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915">https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915</a></p>

          '
        raw: '## custom GrammarSynthesizer class


          to make things hopefully a bit simpler, I made a custom class based on transformers
          anyone can use that makes it a bit easier to integrate bitsandbytes for
          the XL model in 8-bit:


          <script src="https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915.js"></script>



          https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915'
        updatedAt: '2023-06-02T20:32:14.783Z'
      numEdits: 1
      reactions: []
    id: 647a51a644b6a3ae9d22d20e
    type: comment
  author: pszemraj
  content: '## custom GrammarSynthesizer class


    to make things hopefully a bit simpler, I made a custom class based on transformers
    anyone can use that makes it a bit easier to integrate bitsandbytes for the XL
    model in 8-bit:


    <script src="https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915.js"></script>



    https://gist.github.com/pszemraj/14f7b13bd2d953176db2371e5d320915'
  created_at: 2023-06-02 19:31:34+00:00
  edited: true
  hidden: false
  id: 647a51a644b6a3ae9d22d20e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-02T20:34:38.000Z'
    data:
      pinned: true
    id: 647a525ec7367455fd9bf132
    type: pinning-change
  author: pszemraj
  created_at: 2023-06-02 19:34:38+00:00
  id: 647a525ec7367455fd9bf132
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
      fullname: Passive Book
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: passivebook
      type: user
    createdAt: '2023-06-03T06:55:10.000Z'
    data:
      edited: true
      editors:
      - passivebook
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9701513051986694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab67a7884d7e6889f54f9a0d95039356.svg
          fullname: Passive Book
          isHf: false
          isPro: false
          name: passivebook
          type: user
        html: '<p>Thank you for the colab link. I could have never figured it out
          myself. </p>

          <p>The issue must be with the training because even pszemraj/grammar-synthesis-small
          does a really good job with longer more complicated text that other grammar
          models fail to pick up on. </p>

          <p>Your models is the only ones that corrects this sentence "We will show
          you how to start a Financial blog today." to "We will show you how to start
          a financial blog today." All the other models returns the original sentence.
          So was surprised when the model was not performing well for shorter sentences.
          </p>

          <p>Will you be retraining the smaller models in the future?</p>

          <p>Also a dumb question. What should the max length of the input be. It
          says 256 in your colab link. Is it 256 words or is the length measured in
          number of characters?</p>

          '
        raw: "Thank you for the colab link. I could have never figured it out myself.\
          \ \n\nThe issue must be with the training because even pszemraj/grammar-synthesis-small\
          \ does a really good job with longer more complicated text that other grammar\
          \ models fail to pick up on. \n\nYour models is the only ones that corrects\
          \ this sentence \"We will show you how to start a Financial blog today.\"\
          \ to \"We will show you how to start a financial blog today.\" All the other\
          \ models returns the original sentence. So was surprised when the model\
          \ was not performing well for shorter sentences. \n\nWill you be retraining\
          \ the smaller models in the future?\n\nAlso a dumb question. What should\
          \ the max length of the input be. It says 256 in your colab link. Is it\
          \ 256 words or is the length measured in number of characters?"
        updatedAt: '2023-06-03T16:40:24.933Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
    id: 647ae3ce6dbad6ab056cd25f
    type: comment
  author: passivebook
  content: "Thank you for the colab link. I could have never figured it out myself.\
    \ \n\nThe issue must be with the training because even pszemraj/grammar-synthesis-small\
    \ does a really good job with longer more complicated text that other grammar\
    \ models fail to pick up on. \n\nYour models is the only ones that corrects this\
    \ sentence \"We will show you how to start a Financial blog today.\" to \"We will\
    \ show you how to start a financial blog today.\" All the other models returns\
    \ the original sentence. So was surprised when the model was not performing well\
    \ for shorter sentences. \n\nWill you be retraining the smaller models in the\
    \ future?\n\nAlso a dumb question. What should the max length of the input be.\
    \ It says 256 in your colab link. Is it 256 words or is the length measured in\
    \ number of characters?"
  created_at: 2023-06-03 05:55:10+00:00
  edited: true
  hidden: false
  id: 647ae3ce6dbad6ab056cd25f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-16T11:58:35.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446320533752441
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<blockquote>

          <p>Your models is the only ones that corrects this sentence ...</p>

          </blockquote>

          <p>thanks! btw if you have any feedback on specific "general failure cases"
          to improve upon let me know and that will help any future development</p>

          <blockquote>

          <p>Will you be retraining the smaller models in the future?</p>

          </blockquote>

          <p>Someday I plan to retrain all of them with a newer/better dataset. This
          is at least a few months+ out from happening.. unless someone wants to hire
          me to do that (drop me an email)</p>

          <blockquote>

          <p>Also a dumb question. What should the max length of the input be. It
          says 256 in your colab link. Is it 256 words or is the length measured in
          number of characters?</p>

          </blockquote>

          <p>it is measured in tokens. if you are new to transformers I would recommend
          checking out and working through <a href="https://huggingface.co/learn">the
          huggingface course</a> to get a better understanding of what these models
          do and how they work (<em>seriously, I wish this existed and someone told
          me when I first started!</em>)</p>

          <p>btw I am closing this for now from an issue perspective, of course feel
          free to reply and/or reopen as needed :)</p>

          '
        raw: '> Your models is the only ones that corrects this sentence ...


          thanks! btw if you have any feedback on specific "general failure cases"
          to improve upon let me know and that will help any future development


          > Will you be retraining the smaller models in the future?


          Someday I plan to retrain all of them with a newer/better dataset. This
          is at least a few months+ out from happening.. unless someone wants to hire
          me to do that (drop me an email)


          > Also a dumb question. What should the max length of the input be. It says
          256 in your colab link. Is it 256 words or is the length measured in number
          of characters?


          it is measured in tokens. if you are new to transformers I would recommend
          checking out and working through [the huggingface course](https://huggingface.co/learn)
          to get a better understanding of what these models do and how they work
          (_seriously, I wish this existed and someone told me when I first started!_)



          btw I am closing this for now from an issue perspective, of course feel
          free to reply and/or reopen as needed :)'
        updatedAt: '2023-06-16T11:58:35.260Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648c4e6bec10366d8f5bd330
    id: 648c4e6bec10366d8f5bd32f
    type: comment
  author: pszemraj
  content: '> Your models is the only ones that corrects this sentence ...


    thanks! btw if you have any feedback on specific "general failure cases" to improve
    upon let me know and that will help any future development


    > Will you be retraining the smaller models in the future?


    Someday I plan to retrain all of them with a newer/better dataset. This is at
    least a few months+ out from happening.. unless someone wants to hire me to do
    that (drop me an email)


    > Also a dumb question. What should the max length of the input be. It says 256
    in your colab link. Is it 256 words or is the length measured in number of characters?


    it is measured in tokens. if you are new to transformers I would recommend checking
    out and working through [the huggingface course](https://huggingface.co/learn)
    to get a better understanding of what these models do and how they work (_seriously,
    I wish this existed and someone told me when I first started!_)



    btw I am closing this for now from an issue perspective, of course feel free to
    reply and/or reopen as needed :)'
  created_at: 2023-06-16 10:58:35+00:00
  edited: false
  hidden: false
  id: 648c4e6bec10366d8f5bd32f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-16T11:58:35.000Z'
    data:
      status: closed
    id: 648c4e6bec10366d8f5bd330
    type: status-change
  author: pszemraj
  created_at: 2023-06-16 10:58:35+00:00
  id: 648c4e6bec10366d8f5bd330
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: pszemraj/flan-t5-large-grammar-synthesis
repo_type: model
status: closed
target_branch: null
title: Made up output
