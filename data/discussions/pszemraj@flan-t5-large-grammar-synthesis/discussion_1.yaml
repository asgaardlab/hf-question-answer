!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OccultDemonCassette
conflicting_files: null
created_at: 2022-11-29 16:51:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
      fullname: Brent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OccultDemonCassette
      type: user
    createdAt: '2022-11-29T16:51:16.000Z'
    data:
      edited: false
      editors:
      - OccultDemonCassette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
          fullname: Brent
          isHf: false
          isPro: false
          name: OccultDemonCassette
          type: user
        html: '<p>I absolutely love that this model can actually handle things longer
          than one sentence. Would there be any way to increase the total amount of
          input-text it can handle when using it in google colab so that it can process
          a multi-paragraph document of up to 2000 words (or 10,000 characters) or
          so? </p>

          '
        raw: 'I absolutely love that this model can actually handle things longer
          than one sentence. Would there be any way to increase the total amount of
          input-text it can handle when using it in google colab so that it can process
          a multi-paragraph document of up to 2000 words (or 10,000 characters) or
          so? '
        updatedAt: '2022-11-29T16:51:16.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
    id: 63863884a38ac48afd8e1b82
    type: comment
  author: OccultDemonCassette
  content: 'I absolutely love that this model can actually handle things longer than
    one sentence. Would there be any way to increase the total amount of input-text
    it can handle when using it in google colab so that it can process a multi-paragraph
    document of up to 2000 words (or 10,000 characters) or so? '
  created_at: 2022-11-29 16:51:16+00:00
  edited: false
  hidden: false
  id: 63863884a38ac48afd8e1b82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-11-30T21:13:56.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;OccultDemonCassette&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/OccultDemonCassette\"\
          >@<span class=\"underline\">OccultDemonCassette</span></a></span>\n\n\t\
          </span></span> , glad you like it \U0001F60C currently, the model should\
          \ be able to handle up to 1024 tokens at a time (though I would have to\
          \ go back and look at the training data to see <em>practically</em> how\
          \ many tokens it was looking at max during train.. this may take some time).\
          \ You can see how many tokens you have for this model via the following:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"pszemraj/flan-t5-large-grammar-synthesis\"</span>)\n\
          input_text = <span class=\"hljs-string\">\"I love that this model can handle\
          \ things longer than one sentence. Would there be any way to increase the\
          \ total amount of input-text it can handle when using it in google colab\
          \ so that it can process a multi-paragraph document of up to 2000 words\
          \ (or 10,000 characters) or so?\"</span>\nencoded_input = tokenizer(\n \
          \       input_text,\n        truncation=<span class=\"hljs-literal\">False</span>,\n\
          \        return_tensors=<span class=\"hljs-string\">\"pt\"</span>,\n   \
          \ )\nnum_tokens = <span class=\"hljs-built_in\">len</span>(encoded_input.input_ids[<span\
          \ class=\"hljs-number\">0</span>])\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"my input text is <span class=\"hljs-subst\">{num_tokens}</span>\
          \ long\"</span>)\n</code></pre>\n<p>AFAIK there should be a diminishing\
          \ importance of \"multiple sentences away\" context on correcting the grammar\
          \ of a given sentence, I think it should be fine to correct the text via\
          \ a sliding token batch window and then aggregate the outputs. You can see\
          \ how I implement this for summarization <a href=\"https://huggingface.co/spaces/pszemraj/document-summarization/blob/main/summarize.py\"\
          >here</a> in the <code>summarize_via_tokenbatches</code> function. You need\
          \ to change calling <code>summarize_and_score</code> to one that implements\
          \ this model. Another thing that might be worth investigating is ensuring\
          \ that the token batches are split in a \"smart\" way such that the model\
          \ is not fed half of the sentence on one batch and the other half on the\
          \ next.</p>\n<p>I hope that helps!</p>\n"
        raw: "Hi @OccultDemonCassette , glad you like it \U0001F60C currently, the\
          \ model should be able to handle up to 1024 tokens at a time (though I would\
          \ have to go back and look at the training data to see _practically_ how\
          \ many tokens it was looking at max during train.. this may take some time).\
          \ You can see how many tokens you have for this model via the following:\n\
          \n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"pszemraj/flan-t5-large-grammar-synthesis\"\
          )\ninput_text = \"I love that this model can handle things longer than one\
          \ sentence. Would there be any way to increase the total amount of input-text\
          \ it can handle when using it in google colab so that it can process a multi-paragraph\
          \ document of up to 2000 words (or 10,000 characters) or so?\"\nencoded_input\
          \ = tokenizer(\n        input_text,\n        truncation=False,\n       \
          \ return_tensors=\"pt\",\n    )\nnum_tokens = len(encoded_input.input_ids[0])\n\
          print(f\"my input text is {num_tokens} long\")\n```\n\nAFAIK there should\
          \ be a diminishing importance of \"multiple sentences away\" context on\
          \ correcting the grammar of a given sentence, I think it should be fine\
          \ to correct the text via a sliding token batch window and then aggregate\
          \ the outputs. You can see how I implement this for summarization [here](https://huggingface.co/spaces/pszemraj/document-summarization/blob/main/summarize.py)\
          \ in the `summarize_via_tokenbatches` function. You need to change calling\
          \ `summarize_and_score` to one that implements this model. Another thing\
          \ that might be worth investigating is ensuring that the token batches are\
          \ split in a \"smart\" way such that the model is not fed half of the sentence\
          \ on one batch and the other half on the next.\n\nI hope that helps!"
        updatedAt: '2022-11-30T21:13:56.850Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurParkerhouse
    id: 6387c7946e484fea6d9ec61b
    type: comment
  author: pszemraj
  content: "Hi @OccultDemonCassette , glad you like it \U0001F60C currently, the model\
    \ should be able to handle up to 1024 tokens at a time (though I would have to\
    \ go back and look at the training data to see _practically_ how many tokens it\
    \ was looking at max during train.. this may take some time). You can see how\
    \ many tokens you have for this model via the following:\n\n```python\nfrom transformers\
    \ import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    pszemraj/flan-t5-large-grammar-synthesis\")\ninput_text = \"I love that this model\
    \ can handle things longer than one sentence. Would there be any way to increase\
    \ the total amount of input-text it can handle when using it in google colab so\
    \ that it can process a multi-paragraph document of up to 2000 words (or 10,000\
    \ characters) or so?\"\nencoded_input = tokenizer(\n        input_text,\n    \
    \    truncation=False,\n        return_tensors=\"pt\",\n    )\nnum_tokens = len(encoded_input.input_ids[0])\n\
    print(f\"my input text is {num_tokens} long\")\n```\n\nAFAIK there should be a\
    \ diminishing importance of \"multiple sentences away\" context on correcting\
    \ the grammar of a given sentence, I think it should be fine to correct the text\
    \ via a sliding token batch window and then aggregate the outputs. You can see\
    \ how I implement this for summarization [here](https://huggingface.co/spaces/pszemraj/document-summarization/blob/main/summarize.py)\
    \ in the `summarize_via_tokenbatches` function. You need to change calling `summarize_and_score`\
    \ to one that implements this model. Another thing that might be worth investigating\
    \ is ensuring that the token batches are split in a \"smart\" way such that the\
    \ model is not fed half of the sentence on one batch and the other half on the\
    \ next.\n\nI hope that helps!"
  created_at: 2022-11-30 21:13:56+00:00
  edited: false
  hidden: false
  id: 6387c7946e484fea6d9ec61b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-04T00:07:00.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>small update: tokens in training data (t5 tokenizer)</p>

          <p>tl;dr I would use batches of 64-96 tokens at a time to ensure "in-distribution"
          correction, but it may (and probably does) work for longer sequences too.
          </p>

          <pre><code>count    180080.000000

          mean         78.201888

          std          94.282064

          min           2.000000

          25%          19.000000

          50%          40.000000

          75%          95.000000

          max         761.000000

          Name: input_token_length, dtype: float64

          </code></pre>

          <p><a rel="nofollow" href="https://i.imgur.com/Z0832Hv.png"><img alt="token-lengths"
          src="https://i.imgur.com/Z0832Hv.png"></a></p>

          '
        raw: "small update: tokens in training data (t5 tokenizer)\n\ntl;dr I would\
          \ use batches of 64-96 tokens at a time to ensure \"in-distribution\" correction,\
          \ but it may (and probably does) work for longer sequences too. \n\n```\n\
          count    180080.000000\nmean         78.201888\nstd          94.282064\n\
          min           2.000000\n25%          19.000000\n50%          40.000000\n\
          75%          95.000000\nmax         761.000000\nName: input_token_length,\
          \ dtype: float64\n```\n\n![token-lengths](https://i.imgur.com/Z0832Hv.png)"
        updatedAt: '2022-12-04T00:07:00.844Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurParkerhouse
      relatedEventId: 638be4a43bbf29e58906e739
    id: 638be4a43bbf29e58906e738
    type: comment
  author: pszemraj
  content: "small update: tokens in training data (t5 tokenizer)\n\ntl;dr I would\
    \ use batches of 64-96 tokens at a time to ensure \"in-distribution\" correction,\
    \ but it may (and probably does) work for longer sequences too. \n\n```\ncount\
    \    180080.000000\nmean         78.201888\nstd          94.282064\nmin      \
    \     2.000000\n25%          19.000000\n50%          40.000000\n75%          95.000000\n\
    max         761.000000\nName: input_token_length, dtype: float64\n```\n\n![token-lengths](https://i.imgur.com/Z0832Hv.png)"
  created_at: 2022-12-04 00:07:00+00:00
  edited: false
  hidden: false
  id: 638be4a43bbf29e58906e738
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-04T00:07:00.000Z'
    data:
      status: closed
    id: 638be4a43bbf29e58906e739
    type: status-change
  author: pszemraj
  created_at: 2022-12-04 00:07:00+00:00
  id: 638be4a43bbf29e58906e739
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
      fullname: Parkerhouse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurParkerhouse
      type: user
    createdAt: '2022-12-04T02:20:03.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5fcb82678ead0b1387544044e542dd3b.svg
          fullname: Parkerhouse
          isHf: false
          isPro: false
          name: ArthurParkerhouse
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-04T03:30:35.303Z'
      numEdits: 3
      reactions: []
    id: 638c03d3b0cc3ae137c7edab
    type: comment
  author: ArthurParkerhouse
  content: This comment has been hidden
  created_at: 2022-12-04 02:20:03+00:00
  edited: true
  hidden: true
  id: 638c03d3b0cc3ae137c7edab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
      fullname: Brent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OccultDemonCassette
      type: user
    createdAt: '2022-12-04T03:30:46.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
          fullname: Brent
          isHf: false
          isPro: false
          name: OccultDemonCassette
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-07T14:30:29.309Z'
      numEdits: 0
      reactions: []
    id: 638c146672d5e4aa0125e03a
    type: comment
  author: OccultDemonCassette
  content: This comment has been hidden
  created_at: 2022-12-04 03:30:46+00:00
  edited: true
  hidden: true
  id: 638c146672d5e4aa0125e03a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
      fullname: Brent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OccultDemonCassette
      type: user
    createdAt: '2022-12-07T16:43:35.000Z'
    data:
      edited: true
      editors:
      - OccultDemonCassette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
          fullname: Brent
          isHf: false
          isPro: false
          name: OccultDemonCassette
          type: user
        html: '<blockquote>

          <p>Another thing that might be worth investigating is ensuring that the
          token batches are split in a "smart" way such that the model is not fed
          half of the sentence on one batch and &gt;the other half on the next.</p>

          <p>I hope that helps!</p>

          </blockquote>

          <p>Hello! Sorry for the constant messages. I''m unsure if there''s a better
          way to contact you, or if you could recommend me to any forums who might
          be able to help a newcomer figure some of this stuff out. </p>

          <p>So, the following code for "def chunks"<br>def chunks(lst, n):<br>    """Yield
          successive n-sized chunks from lst."""<br>    for i in range(0, len(lst),
          n):<br>        yield lst[i : i + n]</p>

          <p>I suppose this part splits the input text into equally sized chunks?
          </p>

          <p>Do you know if there would be a way to split these chunks to just be
          line-by-line instead of equally sized batches of text? </p>

          <p>Say, if I took my text file, and I ran a regex on it to insert a new
          line after every period, questionmark, exclamation point, etc. Basically
          so that every single sentence is on its own line. Would there be a way to
          then tokenize a batch of input based specifically on each individual line
          from the text file?</p>

          '
        raw: ">Another thing that might be worth investigating is ensuring that the\
          \ token batches are split in a \"smart\" way such that the model is not\
          \ fed half of the sentence on one batch and >the other half on the next.\n\
          > \n> I hope that helps!\n\nHello! Sorry for the constant messages. I'm\
          \ unsure if there's a better way to contact you, or if you could recommend\
          \ me to any forums who might be able to help a newcomer figure some of this\
          \ stuff out. \n\nSo, the following code for \"def chunks\"\ndef chunks(lst,\
          \ n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for\
          \ i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\nI suppose\
          \ this part splits the input text into equally sized chunks? \n\nDo you\
          \ know if there would be a way to split these chunks to just be line-by-line\
          \ instead of equally sized batches of text? \n\nSay, if I took my text file,\
          \ and I ran a regex on it to insert a new line after every period, questionmark,\
          \ exclamation point, etc. Basically so that every single sentence is on\
          \ its own line. Would there be a way to then tokenize a batch of input based\
          \ specifically on each individual line from the text file?"
        updatedAt: '2022-12-07T16:43:59.983Z'
      numEdits: 1
      reactions: []
    id: 6390c2b754a9dba96f8c0d6b
    type: comment
  author: OccultDemonCassette
  content: ">Another thing that might be worth investigating is ensuring that the\
    \ token batches are split in a \"smart\" way such that the model is not fed half\
    \ of the sentence on one batch and >the other half on the next.\n> \n> I hope\
    \ that helps!\n\nHello! Sorry for the constant messages. I'm unsure if there's\
    \ a better way to contact you, or if you could recommend me to any forums who\
    \ might be able to help a newcomer figure some of this stuff out. \n\nSo, the\
    \ following code for \"def chunks\"\ndef chunks(lst, n):\n    \"\"\"Yield successive\
    \ n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n      \
    \  yield lst[i : i + n]\n\nI suppose this part splits the input text into equally\
    \ sized chunks? \n\nDo you know if there would be a way to split these chunks\
    \ to just be line-by-line instead of equally sized batches of text? \n\nSay, if\
    \ I took my text file, and I ran a regex on it to insert a new line after every\
    \ period, questionmark, exclamation point, etc. Basically so that every single\
    \ sentence is on its own line. Would there be a way to then tokenize a batch of\
    \ input based specifically on each individual line from the text file?"
  created_at: 2022-12-07 16:43:35+00:00
  edited: true
  hidden: false
  id: 6390c2b754a9dba96f8c0d6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-08T01:09:20.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>no worries! you can try discord <code>mrshadow773#0840</code> or
          the huggingface discord <a rel="nofollow" href="https://discord.com/channels/879548962464493619/1019883044724822016">channel</a>
          would be good for this :)</p>

          <p>re <code>chunks</code> it does split evenly (as possible) into token
          batches</p>

          <blockquote>

          <p>Say, if I took my text file, and I ran a regex on it to insert a new
          line after every period, questionmark, exclamation point, etc.</p>

          </blockquote>

          <p>yes, you would then call either the tokenizer on that string or the pipeline
          object. the tricky part is you want to probably maximize the amount of tokens/lines
          you pass to the model at a time to save on compute/speed.</p>

          '
        raw: 'no worries! you can try discord `mrshadow773#0840` or the huggingface
          discord [channel](https://discord.com/channels/879548962464493619/1019883044724822016)
          would be good for this :)


          re `chunks` it does split evenly (as possible) into token batches


          > Say, if I took my text file, and I ran a regex on it to insert a new line
          after every period, questionmark, exclamation point, etc.


          yes, you would then call either the tokenizer on that string or the pipeline
          object. the tricky part is you want to probably maximize the amount of tokens/lines
          you pass to the model at a time to save on compute/speed.'
        updatedAt: '2022-12-08T01:09:20.785Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - OccultDemonCassette
    id: 63913940b6b839bb6142189f
    type: comment
  author: pszemraj
  content: 'no worries! you can try discord `mrshadow773#0840` or the huggingface
    discord [channel](https://discord.com/channels/879548962464493619/1019883044724822016)
    would be good for this :)


    re `chunks` it does split evenly (as possible) into token batches


    > Say, if I took my text file, and I ran a regex on it to insert a new line after
    every period, questionmark, exclamation point, etc.


    yes, you would then call either the tokenizer on that string or the pipeline object.
    the tricky part is you want to probably maximize the amount of tokens/lines you
    pass to the model at a time to save on compute/speed.'
  created_at: 2022-12-08 01:09:20+00:00
  edited: false
  hidden: false
  id: 63913940b6b839bb6142189f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
      fullname: Brent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OccultDemonCassette
      type: user
    createdAt: '2022-12-08T15:21:18.000Z'
    data:
      edited: false
      editors:
      - OccultDemonCassette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acc1f9b30a5a146ecdec8a74e42485df.svg
          fullname: Brent
          isHf: false
          isPro: false
          name: OccultDemonCassette
          type: user
        html: '<blockquote>

          <p>no worries! you can try discord <code>mrshadow773#0840</code> or the
          huggingface discord <a rel="nofollow" href="https://discord.com/channels/879548962464493619/1019883044724822016">channel</a>
          would be good for this :)</p>

          </blockquote>

          <p>Thanks for the references! I don''t know why, but I never even thought
          about the huggingface discord, haha. I''m really loving this long-document/fiction-book/narrative-language
          summarization stuff! </p>

          <p>On a side-note: One thing that I would love to figure out is what data
          it is pulling "The Underground Man" or "The UM" from. I''ve tested out multiple
          fiction novels which are written in a mostly 1st-person narrative, and across
          all of them if it ever forgets what the narrator name is, and it doesn''t
          default to "the narrator",  it will then default to "the underground man".
          So strange, but so fun! It only happens like once in every 50 iterations
          or so, though.</p>

          <p>Some examples:<br>In this chapter, the Underground Man explains his philosophy
          of existence and how it differs from other theories of self.<br>In this
          short chapter, the Underground Man tries to make his case for why we should
          all be afraid.<br>In this chapter, the Underground Man sums up his argument
          against being a human.<br>In this chapter, the Underground Man discusses
          several other Dracula''s who have appeared throughout the novel before their
          souls goes straight to hell.</p>

          '
        raw: "> no worries! you can try discord `mrshadow773#0840` or the huggingface\
          \ discord [channel](https://discord.com/channels/879548962464493619/1019883044724822016)\
          \ would be good for this :)\n> \n\nThanks for the references! I don't know\
          \ why, but I never even thought about the huggingface discord, haha. I'm\
          \ really loving this long-document/fiction-book/narrative-language summarization\
          \ stuff! \n\n\nOn a side-note: One thing that I would love to figure out\
          \ is what data it is pulling \"The Underground Man\" or \"The UM\" from.\
          \ I've tested out multiple fiction novels which are written in a mostly\
          \ 1st-person narrative, and across all of them if it ever forgets what the\
          \ narrator name is, and it doesn't default to \"the narrator\",  it will\
          \ then default to \"the underground man\". So strange, but so fun! It only\
          \ happens like once in every 50 iterations or so, though.\n\nSome examples:\n\
          In this chapter, the Underground Man explains his philosophy of existence\
          \ and how it differs from other theories of self. \nIn this short chapter,\
          \ the Underground Man tries to make his case for why we should all be afraid.\
          \ \nIn this chapter, the Underground Man sums up his argument against being\
          \ a human. \nIn this chapter, the Underground Man discusses several other\
          \ Dracula's who have appeared throughout the novel before their souls goes\
          \ straight to hell."
        updatedAt: '2022-12-08T15:21:18.744Z'
      numEdits: 0
      reactions: []
    id: 639200ee077fee9fbca31af3
    type: comment
  author: OccultDemonCassette
  content: "> no worries! you can try discord `mrshadow773#0840` or the huggingface\
    \ discord [channel](https://discord.com/channels/879548962464493619/1019883044724822016)\
    \ would be good for this :)\n> \n\nThanks for the references! I don't know why,\
    \ but I never even thought about the huggingface discord, haha. I'm really loving\
    \ this long-document/fiction-book/narrative-language summarization stuff! \n\n\
    \nOn a side-note: One thing that I would love to figure out is what data it is\
    \ pulling \"The Underground Man\" or \"The UM\" from. I've tested out multiple\
    \ fiction novels which are written in a mostly 1st-person narrative, and across\
    \ all of them if it ever forgets what the narrator name is, and it doesn't default\
    \ to \"the narrator\",  it will then default to \"the underground man\". So strange,\
    \ but so fun! It only happens like once in every 50 iterations or so, though.\n\
    \nSome examples:\nIn this chapter, the Underground Man explains his philosophy\
    \ of existence and how it differs from other theories of self. \nIn this short\
    \ chapter, the Underground Man tries to make his case for why we should all be\
    \ afraid. \nIn this chapter, the Underground Man sums up his argument against\
    \ being a human. \nIn this chapter, the Underground Man discusses several other\
    \ Dracula's who have appeared throughout the novel before their souls goes straight\
    \ to hell."
  created_at: 2022-12-08 15:21:18+00:00
  edited: false
  hidden: false
  id: 639200ee077fee9fbca31af3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-10T00:31:51.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>hmm is that happening with this model (grammar synthesis)? if it''s
          summarization perhaps better to continue that on a thread for one of those
          models. tl;dr I would use <a rel="nofollow" href="https://www.sbert.net/docs/pretrained_models.html#semantic-search">SBERT
          to search</a> the <a href="https://huggingface.co/datasets/kmfoda/booksum">source
          data</a> if this is related to booksum.</p>

          '
        raw: hmm is that happening with this model (grammar synthesis)? if it's summarization
          perhaps better to continue that on a thread for one of those models. tl;dr
          I would use [SBERT to search](https://www.sbert.net/docs/pretrained_models.html#semantic-search)
          the [source data](https://huggingface.co/datasets/kmfoda/booksum) if this
          is related to booksum.
        updatedAt: '2022-12-10T00:31:51.437Z'
      numEdits: 0
      reactions: []
    id: 6393d377fc6c290e7da9d5fa
    type: comment
  author: pszemraj
  content: hmm is that happening with this model (grammar synthesis)? if it's summarization
    perhaps better to continue that on a thread for one of those models. tl;dr I would
    use [SBERT to search](https://www.sbert.net/docs/pretrained_models.html#semantic-search)
    the [source data](https://huggingface.co/datasets/kmfoda/booksum) if this is related
    to booksum.
  created_at: 2022-12-10 00:31:51+00:00
  edited: false
  hidden: false
  id: 6393d377fc6c290e7da9d5fa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pszemraj/flan-t5-large-grammar-synthesis
repo_type: model
status: closed
target_branch: null
title: Love it!
