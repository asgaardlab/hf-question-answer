!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Teink
conflicting_files: null
created_at: 2023-05-04 09:54:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b211c120dc65eee7862fc00e5bff163.svg
      fullname: JIngyi Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Teink
      type: user
    createdAt: '2023-05-04T10:54:07.000Z'
    data:
      edited: false
      editors:
      - Teink
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b211c120dc65eee7862fc00e5bff163.svg
          fullname: JIngyi Li
          isHf: false
          isPro: false
          name: Teink
          type: user
        html: '<p>I am trying loading this tokenizer and model using the below code:<br>''''''<br>from
          transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("<a href="https://huggingface.co/circulus/llama-13b/tree/main/tokenizer.model&quot;">https://huggingface.co/circulus/llama-13b/tree/main/tokenizer.model"</a>)<br>model
          = AutoModelForCausalLM.from_pretrained("<a href="https://huggingface.co/circulus/llama-13b/tree/main/pytorch_model-00041-of-00041.bin&quot;">https://huggingface.co/circulus/llama-13b/tree/main/pytorch_model-00041-of-00041.bin"</a>)<br>''''''</p>

          <p>However it gives me the error :<br>''''''</p>

          <hr>

          <p>KeyError                                  Traceback (most recent call
          last)<br>/tmp/ipykernel_1596897/927730692.py in <br>      8<br>      9 #
          Instantiate a text-generation pipeline<br>---&gt; 10 text_generator = pipeline("text-generation",
          model="circulus/llama-13b")<br>     11<br>     12 # Generate text using
          the model</p>

          <p>~/.local/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py
          in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,
          framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,
          trust_remote_code, model_kwargs, pipeline_class, **kwargs)<br>    690         hub_kwargs["_commit_hash"]
          = config._commit_hash<br>    691     elif config is None and isinstance(model,
          str):<br>--&gt; 692         config = AutoConfig.from_pretrained(model, _from_pipeline=task,
          **hub_kwargs, **model_kwargs)<br>    693         hub_kwargs["_commit_hash"]
          = config._commit_hash<br>    694 </p>

          <p>~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py
          in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>    915             return
          config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)<br>    916         elif
          "model_type" in config_dict:<br>--&gt; 917             config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>    918             return
          config_class.from_dict(config_dict, **unused_kwargs)<br>    919         else:</p>

          <p>~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py
          in <strong>getitem</strong>(self, key)<br>    621             return self._extra_content[key]<br>    622         if
          key not in self._mapping:<br>--&gt; 623             raise KeyError(key)<br>    624         value
          = self._mapping[key]<br>    625         module_name = model_type_to_module_name(key)</p>

          <p>KeyError: ''llama''<br>''''''<br>Which suggest it cannot find the model
          and tokenizer.<br>May I ask what is the correct way to import the model
          from hugging face and run it , without downloading it to my local machine?</p>

          <p>Many thanks!</p>

          '
        raw: "I am trying loading this tokenizer and model using the below code:\r\
          \n'''\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(\"https://huggingface.co/circulus/llama-13b/tree/main/tokenizer.model\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"https://huggingface.co/circulus/llama-13b/tree/main/pytorch_model-00041-of-00041.bin\"\
          )\r\n'''\r\n\r\nHowever it gives me the error :\r\n'''\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\n/tmp/ipykernel_1596897/927730692.py in <module>\r\n      8 \r\
          \n      9 # Instantiate a text-generation pipeline\r\n---> 10 text_generator\
          \ = pipeline(\"text-generation\", model=\"circulus/llama-13b\")\r\n    \
          \ 11 \r\n     12 # Generate text using the model\r\n\r\n~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    690\
          \         hub_kwargs[\"_commit_hash\"] = config._commit_hash\r\n    691\
          \     elif config is None and isinstance(model, str):\r\n--> 692       \
          \  config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs,\
          \ **model_kwargs)\r\n    693         hub_kwargs[\"_commit_hash\"] = config._commit_hash\r\
          \n    694 \r\n\r\n~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n \
          \   915             return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n    916         elif \"model_type\" in config_dict:\r\n-->\
          \ 917             config_class = CONFIG_MAPPING[config_dict[\"model_type\"\
          ]]\r\n    918             return config_class.from_dict(config_dict, **unused_kwargs)\r\
          \n    919         else:\r\n\r\n~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\
          \ in __getitem__(self, key)\r\n    621             return self._extra_content[key]\r\
          \n    622         if key not in self._mapping:\r\n--> 623             raise\
          \ KeyError(key)\r\n    624         value = self._mapping[key]\r\n    625\
          \         module_name = model_type_to_module_name(key)\r\n\r\nKeyError:\
          \ 'llama'\r\n'''\r\nWhich suggest it cannot find the model and tokenizer.\r\
          \nMay I ask what is the correct way to import the model from hugging face\
          \ and run it , without downloading it to my local machine?\r\n\r\nMany thanks!"
        updatedAt: '2023-05-04T10:54:07.473Z'
      numEdits: 0
      reactions: []
    id: 64538ecfbb0a44c447fe493b
    type: comment
  author: Teink
  content: "I am trying loading this tokenizer and model using the below code:\r\n\
    '''\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"https://huggingface.co/circulus/llama-13b/tree/main/tokenizer.model\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"https://huggingface.co/circulus/llama-13b/tree/main/pytorch_model-00041-of-00041.bin\"\
    )\r\n'''\r\n\r\nHowever it gives me the error :\r\n'''\r\n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \n/tmp/ipykernel_1596897/927730692.py in <module>\r\n      8 \r\n      9 # Instantiate\
    \ a text-generation pipeline\r\n---> 10 text_generator = pipeline(\"text-generation\"\
    , model=\"circulus/llama-13b\")\r\n     11 \r\n     12 # Generate text using the\
    \ model\r\n\r\n~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
    \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    690      \
    \   hub_kwargs[\"_commit_hash\"] = config._commit_hash\r\n    691     elif config\
    \ is None and isinstance(model, str):\r\n--> 692         config = AutoConfig.from_pretrained(model,\
    \ _from_pipeline=task, **hub_kwargs, **model_kwargs)\r\n    693         hub_kwargs[\"\
    _commit_hash\"] = config._commit_hash\r\n    694 \r\n\r\n~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n    915\
    \             return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    916         elif \"model_type\" in config_dict:\r\n--> 917\
    \             config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n \
    \   918             return config_class.from_dict(config_dict, **unused_kwargs)\r\
    \n    919         else:\r\n\r\n~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\
    \ in __getitem__(self, key)\r\n    621             return self._extra_content[key]\r\
    \n    622         if key not in self._mapping:\r\n--> 623             raise KeyError(key)\r\
    \n    624         value = self._mapping[key]\r\n    625         module_name =\
    \ model_type_to_module_name(key)\r\n\r\nKeyError: 'llama'\r\n'''\r\nWhich suggest\
    \ it cannot find the model and tokenizer.\r\nMay I ask what is the correct way\
    \ to import the model from hugging face and run it , without downloading it to\
    \ my local machine?\r\n\r\nMany thanks!"
  created_at: 2023-05-04 09:54:07+00:00
  edited: false
  hidden: false
  id: 64538ecfbb0a44c447fe493b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: circulus/llama-13b
repo_type: model
status: open
target_branch: null
title: Loading llama-13b using 'AutoTokenizer, AutoModelForCausalLM'
