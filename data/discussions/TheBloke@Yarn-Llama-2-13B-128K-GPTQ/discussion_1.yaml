!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ardasevinc
conflicting_files: null
created_at: 2023-09-02 20:30:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ee488da55e6e1840c091cf/sHkEOUBcFKlzxNqNmjM99.png?w=200&h=200&f=face
      fullname: "Arda Sevin\xE7"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ardasevinc
      type: user
    createdAt: '2023-09-02T21:30:07.000Z'
    data:
      edited: false
      editors:
      - ardasevinc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34753885865211487
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ee488da55e6e1840c091cf/sHkEOUBcFKlzxNqNmjM99.png?w=200&h=200&f=face
          fullname: "Arda Sevin\xE7"
          isHf: false
          isPro: false
          name: ardasevinc
          type: user
        html: "<p>Transformers, optimum, flash_attn all installed from source</p>\n\
          <pre><code>(torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$\
          \ python main.py \nTraceback (most recent call last):\n  File \"/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ/main.py\"\
          , line 7, in &lt;module&gt;\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 519, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1037, in from_pretrained\n    return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/configuration_utils.py\"\
          , line 747, in from_dict\n    config = cls(**config_dict)\n            \
          \ ^^^^^^^^^^^^^^^^^^\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 149, in __init__\n    self._rope_scaling_validation()\n  File \"\
          /data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 167, in _rope_scaling_validation\n    raise ValueError(\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\n(torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$\
          \ \n</code></pre>\n"
        raw: "Transformers, optimum, flash_attn all installed from source\r\n```\r\
          \n(torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$\
          \ python main.py \r\nTraceback (most recent call last):\r\n  File \"/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ/main.py\"\
          , line 7, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 519, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
          \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1037, in from_pretrained\r\n    return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/configuration_utils.py\"\
          , line 747, in from_dict\r\n    config = cls(**config_dict)\r\n        \
          \     ^^^^^^^^^^^^^^^^^^\r\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 149, in __init__\r\n    self._rope_scaling_validation()\r\n  File\
          \ \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 167, in _rope_scaling_validation\r\n    raise ValueError(\r\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\r\n(torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$\
          \ \r\n```"
        updatedAt: '2023-09-02T21:30:07.941Z'
      numEdits: 0
      reactions: []
    id: 64f3a95f2143033563e79888
    type: comment
  author: ardasevinc
  content: "Transformers, optimum, flash_attn all installed from source\r\n```\r\n\
    (torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$ python\
    \ main.py \r\nTraceback (most recent call last):\r\n  File \"/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ/main.py\"\
    , line 7, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
    \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 519, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
    \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 1037, in from_pretrained\r\n    return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/configuration_utils.py\"\
    , line 747, in from_dict\r\n    config = cls(**config_dict)\r\n             ^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 149, in __init__\r\n    self._rope_scaling_validation()\r\n  File \"/data/arda/envs/torch/lib/python3.11/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 167, in _rope_scaling_validation\r\n    raise ValueError(\r\nValueError:\
    \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
    \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type': 'yarn',\
    \ 'finetuned': True}\r\n(torch) arda@toprak:/data/arda/projects/test/Yarn-Llama-2-13B-128K-GPTQ$\
    \ \r\n```"
  created_at: 2023-09-02 20:30:07+00:00
  edited: false
  hidden: false
  id: 64f3a95f2143033563e79888
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-04T14:06:18.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597044587135315
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>I'm finding the same issue, including using transformers 4.33. <span\
          \ data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> are you seeing\
          \ this issue on your end?</p>\n<p>I also posted on transformers: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/issues/25957\"\
          >https://github.com/huggingface/transformers/issues/25957</a></p>\n"
        raw: 'I''m finding the same issue, including using transformers 4.33. @TheBloke
          are you seeing this issue on your end?


          I also posted on transformers: https://github.com/huggingface/transformers/issues/25957'
        updatedAt: '2023-09-04T14:29:01.260Z'
      numEdits: 1
      reactions: []
    id: 64f5e45a522c35f1b2a3f0d1
    type: comment
  author: RonanMcGovern
  content: 'I''m finding the same issue, including using transformers 4.33. @TheBloke
    are you seeing this issue on your end?


    I also posted on transformers: https://github.com/huggingface/transformers/issues/25957'
  created_at: 2023-09-04 13:06:18+00:00
  edited: true
  hidden: false
  id: 64f5e45a522c35f1b2a3f0d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-05T08:55:31.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7934200167655945
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Solution here for the 13B model <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/25957#issuecomment-1706212549">github
          issue</a></p>

          <p>The 7B model has an inf issue on generation.</p>

          '
        raw: 'Solution here for the 13B model [github issue](https://github.com/huggingface/transformers/issues/25957#issuecomment-1706212549)


          The 7B model has an inf issue on generation.'
        updatedAt: '2023-09-05T08:55:31.325Z'
      numEdits: 0
      reactions: []
    id: 64f6ed037146133f150d44cf
    type: comment
  author: RonanMcGovern
  content: 'Solution here for the 13B model [github issue](https://github.com/huggingface/transformers/issues/25957#issuecomment-1706212549)


    The 7B model has an inf issue on generation.'
  created_at: 2023-09-05 07:55:31+00:00
  edited: false
  hidden: false
  id: 64f6ed037146133f150d44cf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Yarn-Llama-2-13B-128K-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Facing the error: `rope_scaling` must be a dictionary with with two fields
  by running the example code'
