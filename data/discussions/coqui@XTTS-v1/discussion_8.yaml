!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OlegXio
conflicting_files: null
created_at: 2023-09-22 16:19:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc0952647b31921f0d28edb30f66fe07.svg
      fullname: Oleg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OlegXio
      type: user
    createdAt: '2023-09-22T17:19:33.000Z'
    data:
      edited: false
      editors:
      - OlegXio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.15178605914115906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc0952647b31921f0d28edb30f66fe07.svg
          fullname: Oleg
          isHf: false
          isPro: false
          name: OlegXio
          type: user
        html: "<p>\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
          \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2510<br>\u2502 D:\\PycharmProjects\\XioAI\\aiapi.py:2 in       \
          \                       \u2502<br>\u2502                               \
          \                                              \u2502<br>\u2502    1 from\
          \ TTS.api import TTS                                                \u2502\
          <br>\u2502 &gt;  2 tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\"\
          , gpu=True)   \u2502<br>\u2502    3                                    \
          \                                    \u2502<br>\u2502    4 def copyvoice(name):\
          \                                                   \u2502<br>\u2502   \
          \ 5 \u2502   if name =='glados':                                       \
          \         \u2502<br>\u2502                                             \
          \                                \u2502<br>\u2502 C:\\Users\\1\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\TTS\\api.py:81 in  \u2502<br>\u2502\
          \ <strong>init</strong>                                                \
          \                    \u2502<br>\u2502                                  \
          \                                           \u2502<br>\u2502    78 \u2502\
          \   \u2502                                                             \
          \    \u2502<br>\u2502    79 \u2502   \u2502   if model_name is not None:\
          \                                    \u2502<br>\u2502    80 \u2502   \u2502\
          \   \u2502   if \"tts_models\" in model_name or \"coqui_studio\" in model_\
          \ \u2502<br>\u2502 &gt;  81 \u2502   \u2502   \u2502   \u2502   self.load_tts_model_by_name(model_name,\
          \ gpu)          \u2502<br>\u2502    82 \u2502   \u2502   \u2502   elif \"\
          voice_conversion_models\" in model_name:             \u2502<br>\u2502  \
          \  83 \u2502   \u2502   \u2502   \u2502   self.load_vc_model_by_name(model_name,\
          \ gpu)           \u2502<br>\u2502    84                                \
          \                                       \u2502<br>\u2502               \
          \                                                              \u2502<br>\u2502\
          \ C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\site-packages\\TTS\\\
          api.py:185 in \u2502<br>\u2502 load_tts_model_by_name                  \
          \                                    \u2502<br>\u2502                  \
          \                                                           \u2502<br>\u2502\
          \   182 \u2502   \u2502   \u2502                                       \
          \                      \u2502<br>\u2502   183 \u2502   \u2502   \u2502 \
          \  # init synthesizer                                        \u2502<br>\u2502\
          \   184 \u2502   \u2502   \u2502   # None values are fetch from the model\
          \                    \u2502<br>\u2502 &gt; 185 \u2502   \u2502   \u2502\
          \   self.synthesizer = Synthesizer(                           \u2502<br>\u2502\
          \   186 \u2502   \u2502   \u2502   \u2502   tts_checkpoint=model_path, \
          \                           \u2502<br>\u2502   187 \u2502   \u2502   \u2502\
          \   \u2502   tts_config_path=config_path,                          \u2502\
          <br>\u2502   188 \u2502   \u2502   \u2502   \u2502   tts_speakers_file=None,\
          \                               \u2502<br>\u2502                       \
          \                                                      \u2502<br>\u2502\
          \ C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\site-packages\\TTS\\\
          utils\\synthes \u2502<br>\u2502 izer.py:109 in <strong>init</strong>   \
          \                                                  \u2502<br>\u2502    \
          \                                                                      \
          \   \u2502<br>\u2502   106 \u2502   \u2502   \u2502   \u2502   self._load_fairseq_from_dir(model_dir,\
          \ use_cuda)      \u2502<br>\u2502   107 \u2502   \u2502   \u2502   \u2502\
          \   self.output_sample_rate = self.tts_config.audio[\"samp \u2502<br>\u2502\
          \   108 \u2502   \u2502   \u2502   else:                               \
          \                      \u2502<br>\u2502 &gt; 109 \u2502   \u2502   \u2502\
          \   \u2502   self._load_tts_from_dir(model_dir, use_cuda)          \u2502\
          <br>\u2502   110 \u2502   \u2502   \u2502   \u2502   self.output_sample_rate\
          \ = self.tts_config.audio[\"outp \u2502<br>\u2502   111 \u2502         \
          \                                                            \u2502<br>\u2502\
          \   112 \u2502   @staticmethod                                         \
          \            \u2502<br>\u2502                                          \
          \                                   \u2502<br>\u2502 C:\\Users\\1\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\TTS\\utils\\synthes \u2502<br>\u2502\
          \ izer.py:164 in <em>load_tts_from_dir                                 \
          \          \u2502<br>\u2502                                            \
          \                                 \u2502<br>\u2502   161 \u2502   \u2502\
          \   config = load_config(os.path.join(model_dir, \"config.json\"))  \u2502\
          <br>\u2502   162 \u2502   \u2502   self.tts_config = config            \
          \                          \u2502<br>\u2502   163 \u2502   \u2502   self.tts_model\
          \ = setup_tts_model(config)                      \u2502<br>\u2502 &gt; 164\
          \ \u2502   \u2502   self.tts_model.load_checkpoint(config, checkpoint_dir=model_d\
          \ \u2502<br>\u2502   165 \u2502   \u2502   if use_cuda:                \
          \                                  \u2502<br>\u2502   166 \u2502   \u2502\
          \   \u2502   self.tts_model.cuda()                                     \u2502\
          <br>\u2502   167                                                       \
          \                \u2502<br>\u2502                                      \
          \                                       \u2502<br>\u2502 C:\\Users\\1\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\TTS\\tts\\models\\xt\
          \ \u2502<br>\u2502 ts.py:645 in load_checkpoint                        \
          \                        \u2502<br>\u2502                              \
          \                                               \u2502<br>\u2502   642 \u2502\
          \   \u2502   self.init_models()                                        \
          \    \u2502<br>\u2502   643 \u2502   \u2502   if eval:                 \
          \                                     \u2502<br>\u2502   644 \u2502   \u2502\
          \   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac \u2502\
          <br>\u2502 &gt; 645 \u2502   \u2502   self.load_state_dict(load_fsspec(model_path)[\"\
          model\"], strict \u2502<br>\u2502   646 \u2502   \u2502                \
          \                                                 \u2502<br>\u2502   647\
          \ \u2502   \u2502   if eval:                                           \
          \           \u2502<br>\u2502   648 \u2502   \u2502   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac\
          \ \u2502<br>\u2502                                                     \
          \                        \u2502<br>\u2502 D:\\Program                  \
          \                                                \u2502<br>\u2502 Files\\\
          Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041 in   \
          \     \u2502<br>\u2502 load_state_dict                                 \
          \                            \u2502<br>\u2502                          \
          \                                                   \u2502<br>\u2502   2038\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   ', '.join('\"{}\"\
          '.format(k) for k in missing</em> \u2502<br>\u2502   2039 \u2502   \u2502\
          \                                                                \u2502\
          <br>\u2502   2040 \u2502   \u2502   if len(error_msgs) &gt; 0:         \
          \                             \u2502<br>\u2502 &gt; 2041 \u2502   \u2502\
          \   \u2502   raise RuntimeError('Error(s) in loading state_dict for { \u2502\
          <br>\u2502   2042 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502  \
          \ \u2502      self.<strong>class</strong>.<strong>name</strong>, \"\\n\\\
          t\".join( \u2502<br>\u2502   2043 \u2502   \u2502   return _IncompatibleKeys(missing_keys,\
          \ unexpected_keys)      \u2502<br>\u2502   2044                        \
          \                                              \u2502<br>\u2514\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2518<br>RuntimeError: Error(s) in loading state_dict\
          \ for Xtts:<br>        Missing key(s) in state_dict: \"gpt.gpt.h.0.attn.bias\"\
          ,<br>\"gpt.gpt.h.0.attn.masked_bias\", \"gpt.gpt.h.1.attn.bias\",<br>\"\
          gpt.gpt.h.1.attn.masked_bias\", \"gpt.gpt.h.2.attn.bias\",<br>\"gpt.gpt.h.2.attn.masked_bias\"\
          , \"gpt.gpt.h.3.attn.bias\",<br>\"gpt.gpt.h.3.attn.masked_bias\", \"gpt.gpt.h.4.attn.bias\"\
          ,<br>\"gpt.gpt.h.4.attn.masked_bias\", \"gpt.gpt.h.5.attn.bias\",<br>\"\
          gpt.gpt.h.5.attn.masked_bias\", \"gpt.gpt.h.6.attn.bias\",<br>\"gpt.gpt.h.6.attn.masked_bias\"\
          , \"gpt.gpt.h.7.attn.bias\",<br>\"gpt.gpt.h.7.attn.masked_bias\", \"gpt.gpt.h.8.attn.bias\"\
          ,<br>\"gpt.gpt.h.8.attn.masked_bias\", \"gpt.gpt.h.9.attn.bias\",<br>\"\
          gpt.gpt.h.9.attn.masked_bias\", \"gpt.gpt.h.10.attn.bias\",<br>\"gpt.gpt.h.10.attn.masked_bias\"\
          , \"gpt.gpt.h.11.attn.bias\",<br>\"gpt.gpt.h.11.attn.masked_bias\", \"gpt.gpt.h.12.attn.bias\"\
          ,<br>\"gpt.gpt.h.12.attn.masked_bias\", \"gpt.gpt.h.13.attn.bias\",<br>\"\
          gpt.gpt.h.13.attn.masked_bias\", \"gpt.gpt.h.14.attn.bias\",<br>\"gpt.gpt.h.14.attn.masked_bias\"\
          , \"gpt.gpt.h.15.attn.bias\",<br>\"gpt.gpt.h.15.attn.masked_bias\", \"gpt.gpt.h.16.attn.bias\"\
          ,<br>\"gpt.gpt.h.16.attn.masked_bias\", \"gpt.gpt.h.17.attn.bias\",<br>\"\
          gpt.gpt.h.17.attn.masked_bias\", \"gpt.gpt.h.18.attn.bias\",<br>\"gpt.gpt.h.18.attn.masked_bias\"\
          , \"gpt.gpt.h.19.attn.bias\",<br>\"gpt.gpt.h.19.attn.masked_bias\", \"gpt.gpt.h.20.attn.bias\"\
          ,<br>\"gpt.gpt.h.20.attn.masked_bias\", \"gpt.gpt.h.21.attn.bias\",<br>\"\
          gpt.gpt.h.21.attn.masked_bias\", \"gpt.gpt.h.22.attn.bias\",<br>\"gpt.gpt.h.22.attn.masked_bias\"\
          , \"gpt.gpt.h.23.attn.bias\",<br>\"gpt.gpt.h.23.attn.masked_bias\", \"gpt.gpt.h.24.attn.bias\"\
          ,<br>\"gpt.gpt.h.24.attn.masked_bias\", \"gpt.gpt.h.25.attn.bias\",<br>\"\
          gpt.gpt.h.25.attn.masked_bias\", \"gpt.gpt.h.26.attn.bias\",<br>\"gpt.gpt.h.26.attn.masked_bias\"\
          , \"gpt.gpt.h.27.attn.bias\",<br>\"gpt.gpt.h.27.attn.masked_bias\", \"gpt.gpt.h.28.attn.bias\"\
          ,<br>\"gpt.gpt.h.28.attn.masked_bias\", \"gpt.gpt.h.29.attn.bias\",<br>\"\
          gpt.gpt.h.29.attn.masked_bias\", \"gpt.gpt_inference.transformer.h.0.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.0.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.1.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.1.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.2.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.2.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.3.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.3.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.4.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.4.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.5.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.5.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.6.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.6.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.7.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.7.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.8.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.8.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.9.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.9.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.10.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.10.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.11.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.11.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.12.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.12.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.13.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.13.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.14.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.14.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.15.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.15.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.16.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.16.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.17.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.17.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.18.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.18.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.19.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.19.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.20.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.20.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.21.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.21.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.22.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.22.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.23.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.23.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.24.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.24.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.25.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.25.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.26.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.26.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.27.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.27.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.28.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.28.attn.masked_bias\",<br>\"gpt.gpt_inference.transformer.h.29.attn.bias\"\
          ,<br>\"gpt.gpt_inference.transformer.h.29.attn.masked_bias\". </p>\n"
        raw: "\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most\
          \ recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2510\r\n\u2502 D:\\PycharmProjects\\XioAI\\aiapi.py:2 in <module>    \
          \                         \u2502\r\n\u2502                             \
          \                                                \u2502\r\n\u2502    1 from\
          \ TTS.api import TTS                                                \u2502\
          \r\n\u2502 >  2 tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\"\
          , gpu=True)   \u2502\r\n\u2502    3                                    \
          \                                    \u2502\r\n\u2502    4 def copyvoice(name):\
          \                                                   \u2502\r\n\u2502   \
          \ 5 \u2502   if name =='glados':                                       \
          \         \u2502\r\n\u2502                                             \
          \                                \u2502\r\n\u2502 C:\\Users\\1\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\TTS\\api.py:81 in  \u2502\r\n\
          \u2502 __init__                                                        \
          \            \u2502\r\n\u2502                                          \
          \                                   \u2502\r\n\u2502    78 \u2502   \u2502\
          \                                                                 \u2502\
          \r\n\u2502    79 \u2502   \u2502   if model_name is not None:          \
          \                          \u2502\r\n\u2502    80 \u2502   \u2502   \u2502\
          \   if \"tts_models\" in model_name or \"coqui_studio\" in model_ \u2502\
          \r\n\u2502 >  81 \u2502   \u2502   \u2502   \u2502   self.load_tts_model_by_name(model_name,\
          \ gpu)          \u2502\r\n\u2502    82 \u2502   \u2502   \u2502   elif \"\
          voice_conversion_models\" in model_name:             \u2502\r\n\u2502  \
          \  83 \u2502   \u2502   \u2502   \u2502   self.load_vc_model_by_name(model_name,\
          \ gpu)           \u2502\r\n\u2502    84                                \
          \                                       \u2502\r\n\u2502               \
          \                                                              \u2502\r\n\
          \u2502 C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
          TTS\\api.py:185 in \u2502\r\n\u2502 load_tts_model_by_name             \
          \                                         \u2502\r\n\u2502             \
          \                                                                \u2502\r\
          \n\u2502   182 \u2502   \u2502   \u2502                                \
          \                             \u2502\r\n\u2502   183 \u2502   \u2502   \u2502\
          \   # init synthesizer                                        \u2502\r\n\
          \u2502   184 \u2502   \u2502   \u2502   # None values are fetch from the\
          \ model                    \u2502\r\n\u2502 > 185 \u2502   \u2502   \u2502\
          \   self.synthesizer = Synthesizer(                           \u2502\r\n\
          \u2502   186 \u2502   \u2502   \u2502   \u2502   tts_checkpoint=model_path,\
          \                            \u2502\r\n\u2502   187 \u2502   \u2502   \u2502\
          \   \u2502   tts_config_path=config_path,                          \u2502\
          \r\n\u2502   188 \u2502   \u2502   \u2502   \u2502   tts_speakers_file=None,\
          \                               \u2502\r\n\u2502                       \
          \                                                      \u2502\r\n\u2502\
          \ C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\site-packages\\TTS\\\
          utils\\synthes \u2502\r\n\u2502 izer.py:109 in __init__                \
          \                                     \u2502\r\n\u2502                 \
          \                                                            \u2502\r\n\u2502\
          \   106 \u2502   \u2502   \u2502   \u2502   self._load_fairseq_from_dir(model_dir,\
          \ use_cuda)      \u2502\r\n\u2502   107 \u2502   \u2502   \u2502   \u2502\
          \   self.output_sample_rate = self.tts_config.audio[\"samp \u2502\r\n\u2502\
          \   108 \u2502   \u2502   \u2502   else:                               \
          \                      \u2502\r\n\u2502 > 109 \u2502   \u2502   \u2502 \
          \  \u2502   self._load_tts_from_dir(model_dir, use_cuda)          \u2502\
          \r\n\u2502   110 \u2502   \u2502   \u2502   \u2502   self.output_sample_rate\
          \ = self.tts_config.audio[\"outp \u2502\r\n\u2502   111 \u2502         \
          \                                                            \u2502\r\n\u2502\
          \   112 \u2502   @staticmethod                                         \
          \            \u2502\r\n\u2502                                          \
          \                                   \u2502\r\n\u2502 C:\\Users\\1\\AppData\\\
          Roaming\\Python\\Python310\\site-packages\\TTS\\utils\\synthes \u2502\r\n\
          \u2502 izer.py:164 in _load_tts_from_dir                               \
          \            \u2502\r\n\u2502                                          \
          \                                   \u2502\r\n\u2502   161 \u2502   \u2502\
          \   config = load_config(os.path.join(model_dir, \"config.json\"))  \u2502\
          \r\n\u2502   162 \u2502   \u2502   self.tts_config = config            \
          \                          \u2502\r\n\u2502   163 \u2502   \u2502   self.tts_model\
          \ = setup_tts_model(config)                      \u2502\r\n\u2502 > 164\
          \ \u2502   \u2502   self.tts_model.load_checkpoint(config, checkpoint_dir=model_d\
          \ \u2502\r\n\u2502   165 \u2502   \u2502   if use_cuda:                \
          \                                  \u2502\r\n\u2502   166 \u2502   \u2502\
          \   \u2502   self.tts_model.cuda()                                     \u2502\
          \r\n\u2502   167                                                       \
          \                \u2502\r\n\u2502                                      \
          \                                       \u2502\r\n\u2502 C:\\Users\\1\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\TTS\\tts\\models\\xt\
          \ \u2502\r\n\u2502 ts.py:645 in load_checkpoint                        \
          \                        \u2502\r\n\u2502                              \
          \                                               \u2502\r\n\u2502   642 \u2502\
          \   \u2502   self.init_models()                                        \
          \    \u2502\r\n\u2502   643 \u2502   \u2502   if eval:                 \
          \                                     \u2502\r\n\u2502   644 \u2502   \u2502\
          \   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac \u2502\
          \r\n\u2502 > 645 \u2502   \u2502   self.load_state_dict(load_fsspec(model_path)[\"\
          model\"], strict \u2502\r\n\u2502   646 \u2502   \u2502                \
          \                                                 \u2502\r\n\u2502   647\
          \ \u2502   \u2502   if eval:                                           \
          \           \u2502\r\n\u2502   648 \u2502   \u2502   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac\
          \ \u2502\r\n\u2502                                                     \
          \                        \u2502\r\n\u2502 D:\\Program                  \
          \                                                \u2502\r\n\u2502 Files\\\
          Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041 in   \
          \     \u2502\r\n\u2502 load_state_dict                                 \
          \                            \u2502\r\n\u2502                          \
          \                                                   \u2502\r\n\u2502   2038\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   ', '.join('\"{}\"\
          '.format(k) for k in missing_ \u2502\r\n\u2502   2039 \u2502   \u2502  \
          \                                                              \u2502\r\n\
          \u2502   2040 \u2502   \u2502   if len(error_msgs) > 0:                \
          \                      \u2502\r\n\u2502 > 2041 \u2502   \u2502   \u2502\
          \   raise RuntimeError('Error(s) in loading state_dict for { \u2502\r\n\u2502\
          \   2042 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502  \
          \    self.__class__.__name__, \"\\n\\t\".join( \u2502\r\n\u2502   2043 \u2502\
          \   \u2502   return _IncompatibleKeys(missing_keys, unexpected_keys)   \
          \   \u2502\r\n\u2502   2044                                            \
          \                          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2518\r\nRuntimeError: Error(s) in loading state_dict for Xtts:\r\n   \
          \     Missing key(s) in state_dict: \"gpt.gpt.h.0.attn.bias\", \r\n\"gpt.gpt.h.0.attn.masked_bias\"\
          , \"gpt.gpt.h.1.attn.bias\", \r\n\"gpt.gpt.h.1.attn.masked_bias\", \"gpt.gpt.h.2.attn.bias\"\
          , \r\n\"gpt.gpt.h.2.attn.masked_bias\", \"gpt.gpt.h.3.attn.bias\", \r\n\"\
          gpt.gpt.h.3.attn.masked_bias\", \"gpt.gpt.h.4.attn.bias\", \r\n\"gpt.gpt.h.4.attn.masked_bias\"\
          , \"gpt.gpt.h.5.attn.bias\", \r\n\"gpt.gpt.h.5.attn.masked_bias\", \"gpt.gpt.h.6.attn.bias\"\
          , \r\n\"gpt.gpt.h.6.attn.masked_bias\", \"gpt.gpt.h.7.attn.bias\", \r\n\"\
          gpt.gpt.h.7.attn.masked_bias\", \"gpt.gpt.h.8.attn.bias\", \r\n\"gpt.gpt.h.8.attn.masked_bias\"\
          , \"gpt.gpt.h.9.attn.bias\", \r\n\"gpt.gpt.h.9.attn.masked_bias\", \"gpt.gpt.h.10.attn.bias\"\
          , \r\n\"gpt.gpt.h.10.attn.masked_bias\", \"gpt.gpt.h.11.attn.bias\", \r\n\
          \"gpt.gpt.h.11.attn.masked_bias\", \"gpt.gpt.h.12.attn.bias\", \r\n\"gpt.gpt.h.12.attn.masked_bias\"\
          , \"gpt.gpt.h.13.attn.bias\", \r\n\"gpt.gpt.h.13.attn.masked_bias\", \"\
          gpt.gpt.h.14.attn.bias\", \r\n\"gpt.gpt.h.14.attn.masked_bias\", \"gpt.gpt.h.15.attn.bias\"\
          , \r\n\"gpt.gpt.h.15.attn.masked_bias\", \"gpt.gpt.h.16.attn.bias\", \r\n\
          \"gpt.gpt.h.16.attn.masked_bias\", \"gpt.gpt.h.17.attn.bias\", \r\n\"gpt.gpt.h.17.attn.masked_bias\"\
          , \"gpt.gpt.h.18.attn.bias\", \r\n\"gpt.gpt.h.18.attn.masked_bias\", \"\
          gpt.gpt.h.19.attn.bias\", \r\n\"gpt.gpt.h.19.attn.masked_bias\", \"gpt.gpt.h.20.attn.bias\"\
          , \r\n\"gpt.gpt.h.20.attn.masked_bias\", \"gpt.gpt.h.21.attn.bias\", \r\n\
          \"gpt.gpt.h.21.attn.masked_bias\", \"gpt.gpt.h.22.attn.bias\", \r\n\"gpt.gpt.h.22.attn.masked_bias\"\
          , \"gpt.gpt.h.23.attn.bias\", \r\n\"gpt.gpt.h.23.attn.masked_bias\", \"\
          gpt.gpt.h.24.attn.bias\", \r\n\"gpt.gpt.h.24.attn.masked_bias\", \"gpt.gpt.h.25.attn.bias\"\
          , \r\n\"gpt.gpt.h.25.attn.masked_bias\", \"gpt.gpt.h.26.attn.bias\", \r\n\
          \"gpt.gpt.h.26.attn.masked_bias\", \"gpt.gpt.h.27.attn.bias\", \r\n\"gpt.gpt.h.27.attn.masked_bias\"\
          , \"gpt.gpt.h.28.attn.bias\", \r\n\"gpt.gpt.h.28.attn.masked_bias\", \"\
          gpt.gpt.h.29.attn.bias\", \r\n\"gpt.gpt.h.29.attn.masked_bias\", \"gpt.gpt_inference.transformer.h.0.attn.bias\"\
          ,\r\n\"gpt.gpt_inference.transformer.h.0.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.1.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.1.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.2.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.2.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.3.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.3.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.4.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.4.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.5.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.5.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.6.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.6.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.7.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.7.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.8.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.8.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.9.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.9.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.10.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.10.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.11.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.11.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.12.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.12.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.13.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.13.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.14.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.14.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.15.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.15.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.16.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.16.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.17.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.17.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.18.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.18.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.19.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.19.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.20.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.20.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.21.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.21.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.22.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.22.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.23.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.23.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.24.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.24.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.25.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.25.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.26.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.26.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.27.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.27.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.28.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.28.attn.masked_bias\", \r\n\"gpt.gpt_inference.transformer.h.29.attn.bias\"\
          , \r\n\"gpt.gpt_inference.transformer.h.29.attn.masked_bias\". \r\n"
        updatedAt: '2023-09-22T17:19:33.343Z'
      numEdits: 0
      reactions: []
    id: 650dcca557343388e5537bb9
    type: comment
  author: OlegXio
  content: "\u250C\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
    \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 D:\\\
    PycharmProjects\\XioAI\\aiapi.py:2 in <module>                             \u2502\
    \r\n\u2502                                                                   \
    \          \u2502\r\n\u2502    1 from TTS.api import TTS                     \
    \                           \u2502\r\n\u2502 >  2 tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\"\
    , gpu=True)   \u2502\r\n\u2502    3                                          \
    \                              \u2502\r\n\u2502    4 def copyvoice(name):    \
    \                                               \u2502\r\n\u2502    5 \u2502 \
    \  if name =='glados':                                                \u2502\r\
    \n\u2502                                                                     \
    \        \u2502\r\n\u2502 C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\TTS\\api.py:81 in  \u2502\r\n\u2502 __init__                  \
    \                                                  \u2502\r\n\u2502          \
    \                                                                   \u2502\r\n\
    \u2502    78 \u2502   \u2502                                                 \
    \                \u2502\r\n\u2502    79 \u2502   \u2502   if model_name is not\
    \ None:                                    \u2502\r\n\u2502    80 \u2502   \u2502\
    \   \u2502   if \"tts_models\" in model_name or \"coqui_studio\" in model_ \u2502\
    \r\n\u2502 >  81 \u2502   \u2502   \u2502   \u2502   self.load_tts_model_by_name(model_name,\
    \ gpu)          \u2502\r\n\u2502    82 \u2502   \u2502   \u2502   elif \"voice_conversion_models\"\
    \ in model_name:             \u2502\r\n\u2502    83 \u2502   \u2502   \u2502 \
    \  \u2502   self.load_vc_model_by_name(model_name, gpu)           \u2502\r\n\u2502\
    \    84                                                                      \
    \ \u2502\r\n\u2502                                                           \
    \                  \u2502\r\n\u2502 C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\TTS\\api.py:185 in \u2502\r\n\u2502 load_tts_model_by_name    \
    \                                                  \u2502\r\n\u2502          \
    \                                                                   \u2502\r\n\
    \u2502   182 \u2502   \u2502   \u2502                                        \
    \                     \u2502\r\n\u2502   183 \u2502   \u2502   \u2502   # init\
    \ synthesizer                                        \u2502\r\n\u2502   184 \u2502\
    \   \u2502   \u2502   # None values are fetch from the model                 \
    \   \u2502\r\n\u2502 > 185 \u2502   \u2502   \u2502   self.synthesizer = Synthesizer(\
    \                           \u2502\r\n\u2502   186 \u2502   \u2502   \u2502  \
    \ \u2502   tts_checkpoint=model_path,                            \u2502\r\n\u2502\
    \   187 \u2502   \u2502   \u2502   \u2502   tts_config_path=config_path,     \
    \                     \u2502\r\n\u2502   188 \u2502   \u2502   \u2502   \u2502\
    \   tts_speakers_file=None,                               \u2502\r\n\u2502   \
    \                                                                          \u2502\
    \r\n\u2502 C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\site-packages\\\
    TTS\\utils\\synthes \u2502\r\n\u2502 izer.py:109 in __init__                 \
    \                                    \u2502\r\n\u2502                        \
    \                                                     \u2502\r\n\u2502   106 \u2502\
    \   \u2502   \u2502   \u2502   self._load_fairseq_from_dir(model_dir, use_cuda)\
    \      \u2502\r\n\u2502   107 \u2502   \u2502   \u2502   \u2502   self.output_sample_rate\
    \ = self.tts_config.audio[\"samp \u2502\r\n\u2502   108 \u2502   \u2502   \u2502\
    \   else:                                                     \u2502\r\n\u2502\
    \ > 109 \u2502   \u2502   \u2502   \u2502   self._load_tts_from_dir(model_dir,\
    \ use_cuda)          \u2502\r\n\u2502   110 \u2502   \u2502   \u2502   \u2502\
    \   self.output_sample_rate = self.tts_config.audio[\"outp \u2502\r\n\u2502  \
    \ 111 \u2502                                                                 \
    \    \u2502\r\n\u2502   112 \u2502   @staticmethod                           \
    \                          \u2502\r\n\u2502                                  \
    \                                           \u2502\r\n\u2502 C:\\Users\\1\\AppData\\\
    Roaming\\Python\\Python310\\site-packages\\TTS\\utils\\synthes \u2502\r\n\u2502\
    \ izer.py:164 in _load_tts_from_dir                                          \
    \ \u2502\r\n\u2502                                                           \
    \                  \u2502\r\n\u2502   161 \u2502   \u2502   config = load_config(os.path.join(model_dir,\
    \ \"config.json\"))  \u2502\r\n\u2502   162 \u2502   \u2502   self.tts_config\
    \ = config                                      \u2502\r\n\u2502   163 \u2502\
    \   \u2502   self.tts_model = setup_tts_model(config)                      \u2502\
    \r\n\u2502 > 164 \u2502   \u2502   self.tts_model.load_checkpoint(config, checkpoint_dir=model_d\
    \ \u2502\r\n\u2502   165 \u2502   \u2502   if use_cuda:                      \
    \                            \u2502\r\n\u2502   166 \u2502   \u2502   \u2502 \
    \  self.tts_model.cuda()                                     \u2502\r\n\u2502\
    \   167                                                                      \
    \ \u2502\r\n\u2502                                                           \
    \                  \u2502\r\n\u2502 C:\\Users\\1\\AppData\\Roaming\\Python\\Python310\\\
    site-packages\\TTS\\tts\\models\\xt \u2502\r\n\u2502 ts.py:645 in load_checkpoint\
    \                                                \u2502\r\n\u2502            \
    \                                                                 \u2502\r\n\u2502\
    \   642 \u2502   \u2502   self.init_models()                                 \
    \           \u2502\r\n\u2502   643 \u2502   \u2502   if eval:                \
    \                                      \u2502\r\n\u2502   644 \u2502   \u2502\
    \   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac \u2502\r\
    \n\u2502 > 645 \u2502   \u2502   self.load_state_dict(load_fsspec(model_path)[\"\
    model\"], strict \u2502\r\n\u2502   646 \u2502   \u2502                      \
    \                                           \u2502\r\n\u2502   647 \u2502   \u2502\
    \   if eval:                                                      \u2502\r\n\u2502\
    \   648 \u2502   \u2502   \u2502   self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cac\
    \ \u2502\r\n\u2502                                                           \
    \                  \u2502\r\n\u2502 D:\\Program                              \
    \                                    \u2502\r\n\u2502 Files\\Python310\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py:2041 in        \u2502\r\n\u2502 load_state_dict\
    \                                                             \u2502\r\n\u2502\
    \                                                                            \
    \ \u2502\r\n\u2502   2038 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   ', '.join('\"{}\"'.format(k) for k in missing_ \u2502\r\n\u2502   2039 \u2502\
    \   \u2502                                                                \u2502\
    \r\n\u2502   2040 \u2502   \u2502   if len(error_msgs) > 0:                  \
    \                    \u2502\r\n\u2502 > 2041 \u2502   \u2502   \u2502   raise\
    \ RuntimeError('Error(s) in loading state_dict for { \u2502\r\n\u2502   2042 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self.__class__.__name__,\
    \ \"\\n\\t\".join( \u2502\r\n\u2502   2043 \u2502   \u2502   return _IncompatibleKeys(missing_keys,\
    \ unexpected_keys)      \u2502\r\n\u2502   2044                              \
    \                                        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nRuntimeError: Error(s)\
    \ in loading state_dict for Xtts:\r\n        Missing key(s) in state_dict: \"\
    gpt.gpt.h.0.attn.bias\", \r\n\"gpt.gpt.h.0.attn.masked_bias\", \"gpt.gpt.h.1.attn.bias\"\
    , \r\n\"gpt.gpt.h.1.attn.masked_bias\", \"gpt.gpt.h.2.attn.bias\", \r\n\"gpt.gpt.h.2.attn.masked_bias\"\
    , \"gpt.gpt.h.3.attn.bias\", \r\n\"gpt.gpt.h.3.attn.masked_bias\", \"gpt.gpt.h.4.attn.bias\"\
    , \r\n\"gpt.gpt.h.4.attn.masked_bias\", \"gpt.gpt.h.5.attn.bias\", \r\n\"gpt.gpt.h.5.attn.masked_bias\"\
    , \"gpt.gpt.h.6.attn.bias\", \r\n\"gpt.gpt.h.6.attn.masked_bias\", \"gpt.gpt.h.7.attn.bias\"\
    , \r\n\"gpt.gpt.h.7.attn.masked_bias\", \"gpt.gpt.h.8.attn.bias\", \r\n\"gpt.gpt.h.8.attn.masked_bias\"\
    , \"gpt.gpt.h.9.attn.bias\", \r\n\"gpt.gpt.h.9.attn.masked_bias\", \"gpt.gpt.h.10.attn.bias\"\
    , \r\n\"gpt.gpt.h.10.attn.masked_bias\", \"gpt.gpt.h.11.attn.bias\", \r\n\"gpt.gpt.h.11.attn.masked_bias\"\
    , \"gpt.gpt.h.12.attn.bias\", \r\n\"gpt.gpt.h.12.attn.masked_bias\", \"gpt.gpt.h.13.attn.bias\"\
    , \r\n\"gpt.gpt.h.13.attn.masked_bias\", \"gpt.gpt.h.14.attn.bias\", \r\n\"gpt.gpt.h.14.attn.masked_bias\"\
    , \"gpt.gpt.h.15.attn.bias\", \r\n\"gpt.gpt.h.15.attn.masked_bias\", \"gpt.gpt.h.16.attn.bias\"\
    , \r\n\"gpt.gpt.h.16.attn.masked_bias\", \"gpt.gpt.h.17.attn.bias\", \r\n\"gpt.gpt.h.17.attn.masked_bias\"\
    , \"gpt.gpt.h.18.attn.bias\", \r\n\"gpt.gpt.h.18.attn.masked_bias\", \"gpt.gpt.h.19.attn.bias\"\
    , \r\n\"gpt.gpt.h.19.attn.masked_bias\", \"gpt.gpt.h.20.attn.bias\", \r\n\"gpt.gpt.h.20.attn.masked_bias\"\
    , \"gpt.gpt.h.21.attn.bias\", \r\n\"gpt.gpt.h.21.attn.masked_bias\", \"gpt.gpt.h.22.attn.bias\"\
    , \r\n\"gpt.gpt.h.22.attn.masked_bias\", \"gpt.gpt.h.23.attn.bias\", \r\n\"gpt.gpt.h.23.attn.masked_bias\"\
    , \"gpt.gpt.h.24.attn.bias\", \r\n\"gpt.gpt.h.24.attn.masked_bias\", \"gpt.gpt.h.25.attn.bias\"\
    , \r\n\"gpt.gpt.h.25.attn.masked_bias\", \"gpt.gpt.h.26.attn.bias\", \r\n\"gpt.gpt.h.26.attn.masked_bias\"\
    , \"gpt.gpt.h.27.attn.bias\", \r\n\"gpt.gpt.h.27.attn.masked_bias\", \"gpt.gpt.h.28.attn.bias\"\
    , \r\n\"gpt.gpt.h.28.attn.masked_bias\", \"gpt.gpt.h.29.attn.bias\", \r\n\"gpt.gpt.h.29.attn.masked_bias\"\
    , \"gpt.gpt_inference.transformer.h.0.attn.bias\",\r\n\"gpt.gpt_inference.transformer.h.0.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.1.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.1.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.2.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.2.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.3.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.3.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.4.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.4.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.5.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.5.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.6.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.6.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.7.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.7.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.8.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.8.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.9.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.9.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.10.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.10.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.11.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.11.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.12.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.12.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.13.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.13.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.14.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.14.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.15.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.15.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.16.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.16.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.17.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.17.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.18.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.18.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.19.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.19.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.20.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.20.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.21.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.21.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.22.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.22.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.23.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.23.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.24.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.24.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.25.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.25.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.26.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.26.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.27.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.27.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.28.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.28.attn.masked_bias\"\
    , \r\n\"gpt.gpt_inference.transformer.h.29.attn.bias\", \r\n\"gpt.gpt_inference.transformer.h.29.attn.masked_bias\"\
    . \r\n"
  created_at: 2023-09-22 16:19:33+00:00
  edited: false
  hidden: false
  id: 650dcca557343388e5537bb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
      fullname: Gorkem Goknar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gorkemgoknar
      type: user
    createdAt: '2023-09-23T05:25:27.000Z'
    data:
      edited: false
      editors:
      - gorkemgoknar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5085884928703308
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
          fullname: Gorkem Goknar
          isHf: false
          isPro: false
          name: gorkemgoknar
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/coqui-ai/TTS/issues/2976">https://github.com/coqui-ai/TTS/issues/2976</a></p>

          '
        raw: https://github.com/coqui-ai/TTS/issues/2976
        updatedAt: '2023-09-23T05:25:27.491Z'
      numEdits: 0
      reactions: []
    id: 650e76c7f2e0c7dc7ebd59e5
    type: comment
  author: gorkemgoknar
  content: https://github.com/coqui-ai/TTS/issues/2976
  created_at: 2023-09-23 04:25:27+00:00
  edited: false
  hidden: false
  id: 650e76c7f2e0c7dc7ebd59e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc0952647b31921f0d28edb30f66fe07.svg
      fullname: Oleg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OlegXio
      type: user
    createdAt: '2023-09-23T10:02:29.000Z'
    data:
      edited: false
      editors:
      - OlegXio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8616969585418701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc0952647b31921f0d28edb30f66fe07.svg
          fullname: Oleg
          isHf: false
          isPro: false
          name: OlegXio
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gorkemgoknar&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gorkemgoknar\"\
          >@<span class=\"underline\">gorkemgoknar</span></a></span>\n\n\t</span></span><br>I\
          \ received<br>ImportError: accelerate&gt;=0.20.3 is required for a normal\
          \ functioning of this module, but found accelerate==0.18.0.<br>Try: pip\
          \ install transformers -U or pip install -e '.[dev]' if you're working with\
          \ git main<br>I have entered these two commands and the error remains the\
          \ same</p>\n"
        raw: "\n@gorkemgoknar \nI received \nImportError: accelerate>=0.20.3 is required\
          \ for a normal functioning of this module, but found accelerate==0.18.0.\n\
          Try: pip install transformers -U or pip install -e '.[dev]' if you're working\
          \ with git main\nI have entered these two commands and the error remains\
          \ the same"
        updatedAt: '2023-09-23T10:02:29.114Z'
      numEdits: 0
      reactions: []
    id: 650eb7b573c96e4a4005fd07
    type: comment
  author: OlegXio
  content: "\n@gorkemgoknar \nI received \nImportError: accelerate>=0.20.3 is required\
    \ for a normal functioning of this module, but found accelerate==0.18.0.\nTry:\
    \ pip install transformers -U or pip install -e '.[dev]' if you're working with\
    \ git main\nI have entered these two commands and the error remains the same"
  created_at: 2023-09-23 09:02:29+00:00
  edited: false
  hidden: false
  id: 650eb7b573c96e4a4005fd07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
      fullname: Gorkem Goknar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gorkemgoknar
      type: user
    createdAt: '2023-10-04T20:14:54.000Z'
    data:
      edited: true
      editors:
      - gorkemgoknar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7598414421081543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
          fullname: Gorkem Goknar
          isHf: false
          isPro: false
          name: gorkemgoknar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;OlegXio&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/OlegXio\">@<span class=\"\
          underline\">OlegXio</span></a></span>\n\n\t</span></span>  maybe you did\
          \ not download model (but folder is created, it may happen if you did not\
          \ Agree TOS at first download)<br>I suggest checking on model folder </p>\n\
          <p>This should print path check inside and if there is no file in it delete\
          \ the folder  xtts_v1</p>\n<pre><code>import os \nfrom TTS.utils.generic_utils\
          \ import get_user_data_dir\nmodel_path = os.path.join(get_user_data_dir(\"\
          tts\"), \"tts_models--multilingual--multi-dataset--xtts_v1\")\nprint(model_path)\n\
          </code></pre>\n<p>Then after deleting xtts_v1 folder you can download model\
          \ within code with this</p>\n<pre><code>import os \nos.environ[\"COQUI_TOS_AGREED\"\
          ] = \"1\"\nfrom TTS.api import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\"\
          ) \n</code></pre>\n"
        raw: "@OlegXio  maybe you did not download model (but folder is created, it\
          \ may happen if you did not Agree TOS at first download)\nI suggest checking\
          \ on model folder \n\nThis should print path check inside and if there is\
          \ no file in it delete the folder  xtts_v1\n```\nimport os \nfrom TTS.utils.generic_utils\
          \ import get_user_data_dir\nmodel_path = os.path.join(get_user_data_dir(\"\
          tts\"), \"tts_models--multilingual--multi-dataset--xtts_v1\")\nprint(model_path)\n\
          ```\n\nThen after deleting xtts_v1 folder you can download model within\
          \ code with this\n```\nimport os \nos.environ[\"COQUI_TOS_AGREED\"] = \"\
          1\"\nfrom TTS.api import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\"\
          ) \n```"
        updatedAt: '2023-10-04T20:15:27.740Z'
      numEdits: 1
      reactions: []
    id: 651dc7be3cc79d62e6a9b864
    type: comment
  author: gorkemgoknar
  content: "@OlegXio  maybe you did not download model (but folder is created, it\
    \ may happen if you did not Agree TOS at first download)\nI suggest checking on\
    \ model folder \n\nThis should print path check inside and if there is no file\
    \ in it delete the folder  xtts_v1\n```\nimport os \nfrom TTS.utils.generic_utils\
    \ import get_user_data_dir\nmodel_path = os.path.join(get_user_data_dir(\"tts\"\
    ), \"tts_models--multilingual--multi-dataset--xtts_v1\")\nprint(model_path)\n\
    ```\n\nThen after deleting xtts_v1 folder you can download model within code with\
    \ this\n```\nimport os \nos.environ[\"COQUI_TOS_AGREED\"] = \"1\"\nfrom TTS.api\
    \ import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\") \n\
    ```"
  created_at: 2023-10-04 19:14:54+00:00
  edited: true
  hidden: false
  id: 651dc7be3cc79d62e6a9b864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
      fullname: Gorkem Goknar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gorkemgoknar
      type: user
    createdAt: '2023-10-13T14:41:23.000Z'
    data:
      edited: false
      editors:
      - gorkemgoknar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.785402774810791
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
          fullname: Gorkem Goknar
          isHf: false
          isPro: false
          name: gorkemgoknar
          type: user
        html: '<p>Please update your TTS==0.17.8 should fix these issues plus speed
          boosted vocoder</p>

          '
        raw: Please update your TTS==0.17.8 should fix these issues plus speed boosted
          vocoder
        updatedAt: '2023-10-13T14:41:23.067Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652957136cdea40585b9840f
    id: 652957136cdea40585b9840d
    type: comment
  author: gorkemgoknar
  content: Please update your TTS==0.17.8 should fix these issues plus speed boosted
    vocoder
  created_at: 2023-10-13 13:41:23+00:00
  edited: false
  hidden: false
  id: 652957136cdea40585b9840d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
      fullname: Gorkem Goknar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gorkemgoknar
      type: user
    createdAt: '2023-10-13T14:41:23.000Z'
    data:
      status: closed
    id: 652957136cdea40585b9840f
    type: status-change
  author: gorkemgoknar
  created_at: 2023-10-13 13:41:23+00:00
  id: 652957136cdea40585b9840f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: coqui/XTTS-v1
repo_type: model
status: closed
target_branch: null
title: Model RunTimeError
