!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sahiljain314
conflicting_files: null
created_at: 2023-07-11 00:15:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6864599407d9b75e11860e27032b706d.svg
      fullname: Sahil Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sahiljain314
      type: user
    createdAt: '2023-07-11T01:15:58.000Z'
    data:
      edited: false
      editors:
      - Sahiljain314
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8994176983833313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6864599407d9b75e11860e27032b706d.svg
          fullname: Sahil Jain
          isHf: false
          isPro: false
          name: Sahiljain314
          type: user
        html: '<p>I noticed that the config.json for the SDXL UNET contains the following:
          <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/main/unet/config.json#L59">https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/main/unet/config.json#L59</a>,
          which indicates there is 1 transformer block at the highest resolution mapping.</p>

          <p>However, when reading the SDXL paper, they make a bit point to mention
          that the actual transformer blocks are [0, 2, 10], and they have omitted
          any blocks at the highest level. </p>

          <p>Am I missing something? If not, which one is correct?</p>

          '
        raw: "I noticed that the config.json for the SDXL UNET contains the following:\
          \ https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/main/unet/config.json#L59,\
          \ which indicates there is 1 transformer block at the highest resolution\
          \ mapping.\r\n\r\nHowever, when reading the SDXL paper, they make a bit\
          \ point to mention that the actual transformer blocks are [0, 2, 10], and\
          \ they have omitted any blocks at the highest level. \r\n\r\nAm I missing\
          \ something? If not, which one is correct?"
        updatedAt: '2023-07-11T01:15:58.844Z'
      numEdits: 0
      reactions: []
    id: 64acad4eafb6aa55340bf221
    type: comment
  author: Sahiljain314
  content: "I noticed that the config.json for the SDXL UNET contains the following:\
    \ https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/main/unet/config.json#L59,\
    \ which indicates there is 1 transformer block at the highest resolution mapping.\r\
    \n\r\nHowever, when reading the SDXL paper, they make a bit point to mention that\
    \ the actual transformer blocks are [0, 2, 10], and they have omitted any blocks\
    \ at the highest level. \r\n\r\nAm I missing something? If not, which one is correct?"
  created_at: 2023-07-11 00:15:58+00:00
  edited: false
  hidden: false
  id: 64acad4eafb6aa55340bf221
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b358bb3369231a36be4013a95cb4137.svg
      fullname: a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: furusu
      type: user
    createdAt: '2023-07-12T00:30:10.000Z'
    data:
      edited: false
      editors:
      - furusu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5804267525672913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b358bb3369231a36be4013a95cb4137.svg
          fullname: a
          isHf: false
          isPro: false
          name: furusu
          type: user
        html: '<p>Since there is no Transformer layer in DownBlock2D, the first term
          is ignored.<br><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/025709258a55cc924dc47efd88959f18ae79830e/unet/config.json#L27">https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/025709258a55cc924dc47efd88959f18ae79830e/unet/config.json#L27</a></p>

          '
        raw: 'Since there is no Transformer layer in DownBlock2D, the first term is
          ignored.

          https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/025709258a55cc924dc47efd88959f18ae79830e/unet/config.json#L27


          '
        updatedAt: '2023-07-12T00:30:10.117Z'
      numEdits: 0
      reactions: []
    id: 64adf412ad6218d51a21ccde
    type: comment
  author: furusu
  content: 'Since there is no Transformer layer in DownBlock2D, the first term is
    ignored.

    https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/blob/025709258a55cc924dc47efd88959f18ae79830e/unet/config.json#L27


    '
  created_at: 2023-07-11 23:30:10+00:00
  edited: false
  hidden: false
  id: 64adf412ad6218d51a21ccde
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: stabilityai/stable-diffusion-xl-base-0.9
repo_type: model
status: open
target_branch: null
title: Discrepancy between number of transformer layers in config and paper
