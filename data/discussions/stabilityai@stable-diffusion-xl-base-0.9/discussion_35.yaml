!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xings19
conflicting_files: null
created_at: 2023-07-11 06:39:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a77bb56726bae561fecb3719c66d1b7a.svg
      fullname: XingSen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xings19
      type: user
    createdAt: '2023-07-11T07:39:23.000Z'
    data:
      edited: false
      editors:
      - xings19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7588033676147461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a77bb56726bae561fecb3719c66d1b7a.svg
          fullname: XingSen
          isHf: false
          isPro: false
          name: xings19
          type: user
        html: '<p>Does sd_xl_base_0.9.safetensors contain all weights?<br>Does it
          contain all the weights of unet, vae, text_model? if yes, can this file
          be used alone?</p>

          '
        raw: "Does sd_xl_base_0.9.safetensors contain all weights?\r\nDoes it contain\
          \ all the weights of unet, vae, text_model? if yes, can this file be used\
          \ alone?"
        updatedAt: '2023-07-11T07:39:23.190Z'
      numEdits: 0
      reactions: []
    id: 64ad072b92772101d034bc18
    type: comment
  author: xings19
  content: "Does sd_xl_base_0.9.safetensors contain all weights?\r\nDoes it contain\
    \ all the weights of unet, vae, text_model? if yes, can this file be used alone?"
  created_at: 2023-07-11 06:39:23+00:00
  edited: false
  hidden: false
  id: 64ad072b92772101d034bc18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a77bb56726bae561fecb3719c66d1b7a.svg
      fullname: XingSen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xings19
      type: user
    createdAt: '2023-07-11T08:10:30.000Z'
    data:
      edited: false
      editors:
      - xings19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1362772434949875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a77bb56726bae561fecb3719c66d1b7a.svg
          fullname: XingSen
          isHf: false
          isPro: false
          name: xings19
          type: user
        html: '<p>if i load the model like this:</p>

          <pre><code class="language-python">pipe1 = StableDiffusionXLPipeline.from_single_file(<span
          class="hljs-string">''/root/xxx/xxx/stable-diffusion-xl-base-0.9/sd_xl_base_0.9.safetensors''</span>)

          </code></pre>

          <p>The program will have the following output</p>

          <pre><code class="language-text">global_step key not found in model

          Some weights of the model checkpoint at openai/clip-vit-large-patch14 were
          not used when initializing CLIPTextModel: [''vision_model.encoder.layers.23.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.9.mlp.fc1.weight'', ''vision_model.encoder.layers.17.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.3.layer_norm1.bias'', ''vision_model.encoder.layers.18.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.16.self_attn.k_proj.weight'', ''vision_model.encoder.layers.9.layer_norm2.bias'',
          ''vision_model.encoder.layers.17.mlp.fc2.weight'', ''vision_model.encoder.layers.8.layer_norm1.weight'',
          ''vision_model.encoder.layers.21.self_attn.out_proj.bias'', ''vision_model.encoder.layers.5.mlp.fc1.bias'',
          ''vision_model.embeddings.patch_embedding.weight'', ''vision_model.encoder.layers.19.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.11.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.23.self_attn.q_proj.bias'', ''vision_model.encoder.layers.19.mlp.fc1.weight'',
          ''vision_model.encoder.layers.21.layer_norm2.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.q_proj.bias'', ''vision_model.encoder.layers.10.mlp.fc2.weight'',
          ''vision_model.encoder.layers.6.layer_norm1.bias'', ''vision_model.encoder.layers.4.mlp.fc2.bias'',
          ''vision_model.encoder.layers.14.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.self_attn.v_proj.weight'',
          ''vision_model.embeddings.class_embedding'', ''vision_model.encoder.layers.7.layer_norm1.bias'',
          ''vision_model.encoder.layers.22.layer_norm2.bias'', ''vision_model.encoder.layers.9.layer_norm1.bias'',
          ''vision_model.encoder.layers.23.mlp.fc1.weight'', ''vision_model.encoder.layers.16.mlp.fc2.bias'',
          ''vision_model.encoder.layers.20.layer_norm2.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.k_proj.weight'', ''vision_model.encoder.layers.2.layer_norm1.bias'',
          ''vision_model.encoder.layers.0.mlp.fc2.bias'', ''vision_model.encoder.layers.7.layer_norm2.weight'',
          ''vision_model.encoder.layers.7.layer_norm1.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.18.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.weight'',
          ''vision_model.encoder.layers.23.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.layer_norm1.bias'',
          ''vision_model.encoder.layers.19.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.q_proj.bias'', ''vision_model.encoder.layers.9.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.5.mlp.fc2.bias'', ''vision_model.encoder.layers.15.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.19.self_attn.k_proj.weight'', ''vision_model.encoder.layers.8.mlp.fc2.weight'',
          ''vision_model.encoder.layers.18.self_attn.k_proj.weight'', ''vision_model.encoder.layers.12.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.q_proj.weight'', ''vision_model.encoder.layers.22.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.layer_norm2.bias'', ''vision_model.encoder.layers.19.layer_norm2.weight'',
          ''vision_model.encoder.layers.15.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc2.weight'',
          ''vision_model.encoder.layers.1.mlp.fc1.bias'', ''vision_model.encoder.layers.7.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.18.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.8.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.6.mlp.fc1.bias'', ''vision_model.encoder.layers.1.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.8.self_attn.k_proj.bias'', ''vision_model.encoder.layers.18.mlp.fc2.weight'',
          ''vision_model.encoder.layers.5.mlp.fc1.weight'', ''vision_model.encoder.layers.0.layer_norm1.bias'',
          ''vision_model.encoder.layers.21.mlp.fc2.weight'', ''vision_model.encoder.layers.2.layer_norm1.weight'',
          ''vision_model.encoder.layers.13.self_attn.k_proj.weight'', ''vision_model.encoder.layers.0.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.0.mlp.fc1.bias'', ''vision_model.encoder.layers.10.mlp.fc1.bias'',
          ''vision_model.encoder.layers.21.mlp.fc1.bias'', ''vision_model.encoder.layers.6.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.layer_norm2.weight'', ''vision_model.encoder.layers.18.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.19.self_attn.out_proj.weight'', ''vision_model.encoder.layers.1.mlp.fc1.weight'',
          ''vision_model.encoder.layers.2.mlp.fc2.weight'', ''vision_model.encoder.layers.5.layer_norm1.weight'',
          ''vision_model.encoder.layers.6.layer_norm2.weight'', ''vision_model.encoder.layers.5.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.self_attn.out_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.weight'',
          ''vision_model.encoder.layers.16.self_attn.out_proj.bias'', ''vision_model.encoder.layers.9.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.16.self_attn.v_proj.weight'', ''vision_model.encoder.layers.9.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.v_proj.weight'', ''vision_model.encoder.layers.12.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.out_proj.weight'', ''vision_model.encoder.layers.21.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.k_proj.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.19.self_attn.v_proj.weight'', ''vision_model.encoder.layers.6.layer_norm1.weight'',
          ''vision_model.encoder.layers.19.layer_norm1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.22.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.mlp.fc2.weight'',
          ''vision_model.encoder.layers.2.self_attn.out_proj.weight'', ''vision_model.encoder.layers.10.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.mlp.fc1.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.12.self_attn.k_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.bias'',
          ''vision_model.encoder.layers.7.mlp.fc2.bias'', ''vision_model.encoder.layers.2.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.13.layer_norm2.bias'', ''vision_model.encoder.layers.3.layer_norm1.weight'',
          ''vision_model.encoder.layers.11.self_attn.k_proj.bias'', ''vision_model.encoder.layers.13.layer_norm1.bias'',
          ''vision_model.encoder.layers.6.self_attn.k_proj.weight'', ''vision_model.encoder.layers.1.layer_norm2.weight'',
          ''vision_model.encoder.layers.10.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.22.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.16.layer_norm2.weight'', ''vision_model.encoder.layers.23.layer_norm2.weight'',
          ''vision_model.embeddings.position_embedding.weight'', ''vision_model.encoder.layers.8.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.20.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc1.bias'',
          ''vision_model.encoder.layers.13.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.9.mlp.fc1.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.bias'',
          ''vision_model.post_layernorm.bias'', ''vision_model.encoder.layers.6.mlp.fc1.weight'',
          ''vision_model.encoder.layers.8.self_attn.out_proj.weight'', ''vision_model.encoder.layers.7.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.10.mlp.fc1.weight'', ''vision_model.encoder.layers.22.layer_norm1.bias'',
          ''vision_model.encoder.layers.23.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.4.layer_norm1.weight'', ''vision_model.encoder.layers.13.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.21.self_attn.v_proj.bias'', ''vision_model.encoder.layers.15.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.13.self_attn.q_proj.weight'', ''vision_model.encoder.layers.4.layer_norm2.weight'',
          ''vision_model.encoder.layers.15.layer_norm1.weight'', ''vision_model.encoder.layers.15.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.mlp.fc2.bias'', ''vision_model.encoder.layers.3.layer_norm2.weight'',
          ''vision_model.encoder.layers.10.self_attn.k_proj.bias'', ''vision_model.encoder.layers.17.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.18.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.0.layer_norm2.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.v_proj.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.17.mlp.fc1.bias'', ''vision_model.encoder.layers.8.mlp.fc1.weight'',
          ''vision_model.encoder.layers.14.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.12.layer_norm2.weight'', ''vision_model.encoder.layers.4.mlp.fc1.bias'',
          ''vision_model.encoder.layers.3.layer_norm2.bias'', ''vision_model.encoder.layers.11.layer_norm1.weight'',
          ''vision_model.embeddings.position_ids'', ''vision_model.encoder.layers.13.mlp.fc2.weight'',
          ''vision_model.encoder.layers.4.mlp.fc1.weight'', ''vision_model.encoder.layers.13.mlp.fc2.bias'',
          ''vision_model.encoder.layers.4.layer_norm2.bias'', ''vision_model.encoder.layers.5.layer_norm2.bias'',
          ''vision_model.encoder.layers.20.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.20.self_attn.out_proj.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.layer_norm1.weight'', ''vision_model.encoder.layers.8.layer_norm2.weight'',
          ''vision_model.encoder.layers.8.self_attn.q_proj.bias'', ''vision_model.encoder.layers.22.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.21.layer_norm1.bias'', ''vision_model.encoder.layers.23.mlp.fc2.weight'',
          ''vision_model.encoder.layers.13.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.17.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.3.self_attn.k_proj.weight'', ''vision_model.encoder.layers.13.layer_norm2.weight'',
          ''vision_model.encoder.layers.16.self_attn.v_proj.bias'', ''vision_model.encoder.layers.6.layer_norm2.bias'',
          ''vision_model.encoder.layers.11.mlp.fc2.bias'', ''vision_model.encoder.layers.9.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.1.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.22.mlp.fc2.bias'', ''vision_model.encoder.layers.3.mlp.fc1.weight'',
          ''vision_model.encoder.layers.2.self_attn.k_proj.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.11.mlp.fc2.weight'', ''vision_model.encoder.layers.7.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.20.self_attn.v_proj.bias'', ''vision_model.encoder.layers.20.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.9.layer_norm1.weight'', ''vision_model.encoder.layers.1.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.7.layer_norm2.bias'', ''vision_model.encoder.layers.12.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.18.self_attn.out_proj.bias'', ''vision_model.encoder.layers.14.mlp.fc1.weight'',
          ''vision_model.encoder.layers.4.self_attn.k_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.bias'',
          ''vision_model.encoder.layers.4.self_attn.q_proj.weight'', ''vision_model.encoder.layers.8.layer_norm1.bias'',
          ''vision_model.encoder.layers.14.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.layer_norm1.weight'',
          ''vision_model.encoder.layers.7.mlp.fc1.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.mlp.fc2.bias'', ''vision_model.encoder.layers.13.layer_norm1.weight'',
          ''vision_model.encoder.layers.10.self_attn.v_proj.weight'', ''vision_model.encoder.layers.2.mlp.fc1.weight'',
          ''vision_model.encoder.layers.0.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.mlp.fc2.bias'', ''vision_model.encoder.layers.23.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.7.mlp.fc2.weight'', ''vision_model.encoder.layers.5.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.mlp.fc2.weight'',
          ''vision_model.encoder.layers.6.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.mlp.fc2.weight'',
          ''vision_model.encoder.layers.21.mlp.fc2.bias'', ''vision_model.encoder.layers.22.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.15.mlp.fc2.bias'', ''vision_model.encoder.layers.19.mlp.fc2.bias'',
          ''vision_model.encoder.layers.8.self_attn.k_proj.weight'', ''vision_model.encoder.layers.15.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.4.self_attn.v_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.7.mlp.fc1.bias'', ''vision_model.encoder.layers.14.mlp.fc1.bias'',
          ''vision_model.encoder.layers.20.self_attn.k_proj.bias'', ''vision_model.encoder.layers.15.layer_norm2.weight'',
          ''vision_model.encoder.layers.12.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.mlp.fc1.weight'',
          ''vision_model.encoder.layers.5.layer_norm2.weight'', ''vision_model.encoder.layers.14.layer_norm1.weight'',
          ''vision_model.encoder.layers.19.layer_norm2.bias'', ''vision_model.encoder.layers.3.mlp.fc2.bias'',
          ''vision_model.encoder.layers.4.layer_norm1.bias'', ''vision_model.encoder.layers.17.mlp.fc1.weight'',
          ''vision_model.encoder.layers.3.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.layer_norm2.bias'',
          ''vision_model.encoder.layers.23.mlp.fc2.bias'', ''vision_model.encoder.layers.6.mlp.fc2.weight'',
          ''vision_model.encoder.layers.11.layer_norm2.bias'', ''vision_model.encoder.layers.10.layer_norm1.weight'',
          ''vision_model.encoder.layers.12.self_attn.out_proj.bias'', ''vision_model.encoder.layers.23.layer_norm2.bias'',
          ''vision_model.encoder.layers.17.self_attn.k_proj.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.mlp.fc1.weight'', ''vision_model.encoder.layers.17.layer_norm2.weight'',
          ''vision_model.encoder.layers.12.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.0.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.13.mlp.fc1.weight'', ''vision_model.encoder.layers.1.self_attn.k_proj.bias'',
          ''vision_model.pre_layrnorm.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.13.mlp.fc1.bias'', ''vision_model.encoder.layers.2.mlp.fc1.bias'',
          ''vision_model.encoder.layers.15.self_attn.v_proj.bias'', ''vision_model.encoder.layers.10.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.2.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.mlp.fc2.weight'',
          ''vision_model.encoder.layers.3.self_attn.k_proj.bias'', ''vision_model.encoder.layers.2.layer_norm2.weight'',
          ''vision_model.post_layernorm.weight'', ''vision_model.encoder.layers.9.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.10.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.layer_norm1.bias'',
          ''vision_model.encoder.layers.11.layer_norm2.weight'', ''vision_model.encoder.layers.16.mlp.fc2.weight'',
          ''vision_model.encoder.layers.0.mlp.fc1.weight'', ''vision_model.encoder.layers.15.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.11.mlp.fc1.weight'', ''vision_model.encoder.layers.2.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.k_proj.weight'', ''vision_model.pre_layrnorm.weight'',
          ''vision_model.encoder.layers.20.self_attn.k_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.17.layer_norm2.bias'', ''vision_model.encoder.layers.14.mlp.fc2.weight'',
          ''vision_model.encoder.layers.4.self_attn.v_proj.bias'', ''vision_model.encoder.layers.8.mlp.fc2.bias'',
          ''vision_model.encoder.layers.12.layer_norm1.weight'', ''vision_model.encoder.layers.23.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.self_attn.out_proj.bias'', ''vision_model.encoder.layers.2.mlp.fc2.bias'',
          ''vision_model.encoder.layers.6.self_attn.q_proj.weight'', ''vision_model.encoder.layers.12.mlp.fc2.weight'',
          ''vision_model.encoder.layers.18.layer_norm2.bias'', ''vision_model.encoder.layers.16.layer_norm2.bias'',
          ''vision_model.encoder.layers.9.mlp.fc2.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.7.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.15.layer_norm1.bias'', ''vision_model.encoder.layers.16.mlp.fc1.weight'',
          ''vision_model.encoder.layers.20.mlp.fc2.bias'', ''vision_model.encoder.layers.11.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.12.self_attn.q_proj.weight'', ''vision_model.encoder.layers.0.layer_norm1.weight'',
          ''vision_model.encoder.layers.2.self_attn.q_proj.bias'', ''vision_model.encoder.layers.14.layer_norm1.bias'',
          ''vision_model.encoder.layers.4.mlp.fc2.weight'', ''vision_model.encoder.layers.13.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.layer_norm2.weight'', ''vision_model.encoder.layers.9.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.mlp.fc2.bias'', ''logit_scale'', ''vision_model.encoder.layers.5.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.13.self_attn.q_proj.bias'', ''vision_model.encoder.layers.15.mlp.fc2.weight'',
          ''vision_model.encoder.layers.7.self_attn.out_proj.weight'', ''vision_model.encoder.layers.8.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.5.self_attn.k_proj.weight'', ''vision_model.encoder.layers.18.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.18.layer_norm1.bias'', ''vision_model.encoder.layers.18.layer_norm2.weight'',
          ''vision_model.encoder.layers.21.self_attn.k_proj.weight'', ''vision_model.encoder.layers.14.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.0.self_attn.q_proj.weight'', ''vision_model.encoder.layers.5.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.14.mlp.fc2.bias'', ''vision_model.encoder.layers.5.layer_norm1.bias'',
          ''vision_model.encoder.layers.11.mlp.fc1.bias'', ''vision_model.encoder.layers.20.layer_norm1.bias'',
          ''vision_model.encoder.layers.17.mlp.fc2.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.1.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.layer_norm1.bias'',
          ''vision_model.encoder.layers.5.self_attn.q_proj.bias'', ''vision_model.encoder.layers.12.layer_norm1.bias'',
          ''vision_model.encoder.layers.21.layer_norm1.weight'', ''vision_model.encoder.layers.20.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.1.layer_norm1.bias'', ''vision_model.encoder.layers.13.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.9.self_attn.out_proj.weight'', ''vision_model.encoder.layers.0.self_attn.out_proj.bias'',
          ''text_projection.weight'', ''vision_model.encoder.layers.6.mlp.fc2.bias'',
          ''vision_model.encoder.layers.3.mlp.fc1.bias'', ''vision_model.encoder.layers.8.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.mlp.fc1.weight'', ''vision_model.encoder.layers.15.layer_norm2.bias'',
          ''vision_model.encoder.layers.18.mlp.fc1.bias'', ''vision_model.encoder.layers.1.mlp.fc2.bias'',
          ''vision_model.encoder.layers.22.mlp.fc1.bias'', ''vision_model.encoder.layers.21.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.1.self_attn.out_proj.weight'', ''vision_model.encoder.layers.10.layer_norm2.weight'',
          ''vision_model.encoder.layers.0.self_attn.out_proj.weight'', ''vision_model.encoder.layers.12.mlp.fc1.weight'',
          ''vision_model.encoder.layers.17.layer_norm1.weight'', ''vision_model.encoder.layers.22.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.22.layer_norm2.weight'', ''vision_model.encoder.layers.11.self_attn.v_proj.weight'',
          ''visual_projection.weight'', ''vision_model.encoder.layers.17.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.20.mlp.fc2.weight'', ''vision_model.encoder.layers.20.layer_norm1.weight'',
          ''vision_model.encoder.layers.20.layer_norm2.weight'', ''vision_model.encoder.layers.11.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.0.self_attn.k_proj.weight'', ''vision_model.encoder.layers.8.layer_norm2.bias'',
          ''vision_model.encoder.layers.0.mlp.fc2.weight'', ''vision_model.encoder.layers.11.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.5.self_attn.v_proj.weight'', ''vision_model.encoder.layers.1.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.layer_norm2.bias'', ''vision_model.encoder.layers.21.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.self_attn.q_proj.weight'', ''vision_model.encoder.layers.8.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.2.layer_norm2.bias'']

          - This IS expected if you are initializing CLIPTextModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).

          - This IS NOT expected if you are initializing CLIPTextModel from the checkpoint
          of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
          model from a BertForSequenceClassification model).

          Downloading shards:   0%|                                                                                                                                                                                                                                                   |
          0/2 [00:00&lt;?, ?it/s^Downloading shards:   0%|                                                                                                                                                                                                                                                   |
          0/2 [00:34&lt;?, ?it/s]

          </code></pre>

          '
        raw: 'if i load the model like this:

          ```python

          pipe1 = StableDiffusionXLPipeline.from_single_file(''/root/xxx/xxx/stable-diffusion-xl-base-0.9/sd_xl_base_0.9.safetensors'')

          ```

          The program will have the following output

          ```text

          global_step key not found in model

          Some weights of the model checkpoint at openai/clip-vit-large-patch14 were
          not used when initializing CLIPTextModel: [''vision_model.encoder.layers.23.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.9.mlp.fc1.weight'', ''vision_model.encoder.layers.17.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.3.layer_norm1.bias'', ''vision_model.encoder.layers.18.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.16.self_attn.k_proj.weight'', ''vision_model.encoder.layers.9.layer_norm2.bias'',
          ''vision_model.encoder.layers.17.mlp.fc2.weight'', ''vision_model.encoder.layers.8.layer_norm1.weight'',
          ''vision_model.encoder.layers.21.self_attn.out_proj.bias'', ''vision_model.encoder.layers.5.mlp.fc1.bias'',
          ''vision_model.embeddings.patch_embedding.weight'', ''vision_model.encoder.layers.19.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.11.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.23.self_attn.q_proj.bias'', ''vision_model.encoder.layers.19.mlp.fc1.weight'',
          ''vision_model.encoder.layers.21.layer_norm2.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.q_proj.bias'', ''vision_model.encoder.layers.10.mlp.fc2.weight'',
          ''vision_model.encoder.layers.6.layer_norm1.bias'', ''vision_model.encoder.layers.4.mlp.fc2.bias'',
          ''vision_model.encoder.layers.14.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.self_attn.v_proj.weight'',
          ''vision_model.embeddings.class_embedding'', ''vision_model.encoder.layers.7.layer_norm1.bias'',
          ''vision_model.encoder.layers.22.layer_norm2.bias'', ''vision_model.encoder.layers.9.layer_norm1.bias'',
          ''vision_model.encoder.layers.23.mlp.fc1.weight'', ''vision_model.encoder.layers.16.mlp.fc2.bias'',
          ''vision_model.encoder.layers.20.layer_norm2.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.k_proj.weight'', ''vision_model.encoder.layers.2.layer_norm1.bias'',
          ''vision_model.encoder.layers.0.mlp.fc2.bias'', ''vision_model.encoder.layers.7.layer_norm2.weight'',
          ''vision_model.encoder.layers.7.layer_norm1.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.18.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.weight'',
          ''vision_model.encoder.layers.23.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.layer_norm1.bias'',
          ''vision_model.encoder.layers.19.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.q_proj.bias'', ''vision_model.encoder.layers.9.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.5.mlp.fc2.bias'', ''vision_model.encoder.layers.15.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.19.self_attn.k_proj.weight'', ''vision_model.encoder.layers.8.mlp.fc2.weight'',
          ''vision_model.encoder.layers.18.self_attn.k_proj.weight'', ''vision_model.encoder.layers.12.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.q_proj.weight'', ''vision_model.encoder.layers.22.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.layer_norm2.bias'', ''vision_model.encoder.layers.19.layer_norm2.weight'',
          ''vision_model.encoder.layers.15.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc2.weight'',
          ''vision_model.encoder.layers.1.mlp.fc1.bias'', ''vision_model.encoder.layers.7.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.18.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.8.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.6.mlp.fc1.bias'', ''vision_model.encoder.layers.1.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.8.self_attn.k_proj.bias'', ''vision_model.encoder.layers.18.mlp.fc2.weight'',
          ''vision_model.encoder.layers.5.mlp.fc1.weight'', ''vision_model.encoder.layers.0.layer_norm1.bias'',
          ''vision_model.encoder.layers.21.mlp.fc2.weight'', ''vision_model.encoder.layers.2.layer_norm1.weight'',
          ''vision_model.encoder.layers.13.self_attn.k_proj.weight'', ''vision_model.encoder.layers.0.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.0.mlp.fc1.bias'', ''vision_model.encoder.layers.10.mlp.fc1.bias'',
          ''vision_model.encoder.layers.21.mlp.fc1.bias'', ''vision_model.encoder.layers.6.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.layer_norm2.weight'', ''vision_model.encoder.layers.18.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.19.self_attn.out_proj.weight'', ''vision_model.encoder.layers.1.mlp.fc1.weight'',
          ''vision_model.encoder.layers.2.mlp.fc2.weight'', ''vision_model.encoder.layers.5.layer_norm1.weight'',
          ''vision_model.encoder.layers.6.layer_norm2.weight'', ''vision_model.encoder.layers.5.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.self_attn.out_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.weight'',
          ''vision_model.encoder.layers.16.self_attn.out_proj.bias'', ''vision_model.encoder.layers.9.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.16.self_attn.v_proj.weight'', ''vision_model.encoder.layers.9.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.7.self_attn.v_proj.weight'', ''vision_model.encoder.layers.12.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.out_proj.weight'', ''vision_model.encoder.layers.21.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.k_proj.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.19.self_attn.v_proj.weight'', ''vision_model.encoder.layers.6.layer_norm1.weight'',
          ''vision_model.encoder.layers.19.layer_norm1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.22.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.mlp.fc2.weight'',
          ''vision_model.encoder.layers.2.self_attn.out_proj.weight'', ''vision_model.encoder.layers.10.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.mlp.fc1.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.12.self_attn.k_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.bias'',
          ''vision_model.encoder.layers.7.mlp.fc2.bias'', ''vision_model.encoder.layers.2.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.13.layer_norm2.bias'', ''vision_model.encoder.layers.3.layer_norm1.weight'',
          ''vision_model.encoder.layers.11.self_attn.k_proj.bias'', ''vision_model.encoder.layers.13.layer_norm1.bias'',
          ''vision_model.encoder.layers.6.self_attn.k_proj.weight'', ''vision_model.encoder.layers.1.layer_norm2.weight'',
          ''vision_model.encoder.layers.10.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.22.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.16.layer_norm2.weight'', ''vision_model.encoder.layers.23.layer_norm2.weight'',
          ''vision_model.embeddings.position_embedding.weight'', ''vision_model.encoder.layers.8.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.20.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc1.bias'',
          ''vision_model.encoder.layers.13.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.9.mlp.fc1.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.bias'',
          ''vision_model.post_layernorm.bias'', ''vision_model.encoder.layers.6.mlp.fc1.weight'',
          ''vision_model.encoder.layers.8.self_attn.out_proj.weight'', ''vision_model.encoder.layers.7.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.10.mlp.fc1.weight'', ''vision_model.encoder.layers.22.layer_norm1.bias'',
          ''vision_model.encoder.layers.23.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.4.layer_norm1.weight'', ''vision_model.encoder.layers.13.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.21.self_attn.v_proj.bias'', ''vision_model.encoder.layers.15.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.13.self_attn.q_proj.weight'', ''vision_model.encoder.layers.4.layer_norm2.weight'',
          ''vision_model.encoder.layers.15.layer_norm1.weight'', ''vision_model.encoder.layers.15.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.mlp.fc2.bias'', ''vision_model.encoder.layers.3.layer_norm2.weight'',
          ''vision_model.encoder.layers.10.self_attn.k_proj.bias'', ''vision_model.encoder.layers.17.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.18.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.15.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.0.layer_norm2.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.v_proj.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.17.mlp.fc1.bias'', ''vision_model.encoder.layers.8.mlp.fc1.weight'',
          ''vision_model.encoder.layers.14.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.12.layer_norm2.weight'', ''vision_model.encoder.layers.4.mlp.fc1.bias'',
          ''vision_model.encoder.layers.3.layer_norm2.bias'', ''vision_model.encoder.layers.11.layer_norm1.weight'',
          ''vision_model.embeddings.position_ids'', ''vision_model.encoder.layers.13.mlp.fc2.weight'',
          ''vision_model.encoder.layers.4.mlp.fc1.weight'', ''vision_model.encoder.layers.13.mlp.fc2.bias'',
          ''vision_model.encoder.layers.4.layer_norm2.bias'', ''vision_model.encoder.layers.5.layer_norm2.bias'',
          ''vision_model.encoder.layers.20.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.20.self_attn.out_proj.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.layer_norm1.weight'', ''vision_model.encoder.layers.8.layer_norm2.weight'',
          ''vision_model.encoder.layers.8.self_attn.q_proj.bias'', ''vision_model.encoder.layers.22.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.21.layer_norm1.bias'', ''vision_model.encoder.layers.23.mlp.fc2.weight'',
          ''vision_model.encoder.layers.13.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.17.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.3.self_attn.k_proj.weight'', ''vision_model.encoder.layers.13.layer_norm2.weight'',
          ''vision_model.encoder.layers.16.self_attn.v_proj.bias'', ''vision_model.encoder.layers.6.layer_norm2.bias'',
          ''vision_model.encoder.layers.11.mlp.fc2.bias'', ''vision_model.encoder.layers.9.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.1.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.22.mlp.fc2.bias'', ''vision_model.encoder.layers.3.mlp.fc1.weight'',
          ''vision_model.encoder.layers.2.self_attn.k_proj.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.11.mlp.fc2.weight'', ''vision_model.encoder.layers.7.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.20.self_attn.v_proj.bias'', ''vision_model.encoder.layers.20.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.9.layer_norm1.weight'', ''vision_model.encoder.layers.1.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.7.layer_norm2.bias'', ''vision_model.encoder.layers.12.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.18.self_attn.out_proj.bias'', ''vision_model.encoder.layers.14.mlp.fc1.weight'',
          ''vision_model.encoder.layers.4.self_attn.k_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.bias'',
          ''vision_model.encoder.layers.4.self_attn.q_proj.weight'', ''vision_model.encoder.layers.8.layer_norm1.bias'',
          ''vision_model.encoder.layers.14.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.layer_norm1.weight'',
          ''vision_model.encoder.layers.7.mlp.fc1.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.mlp.fc2.bias'', ''vision_model.encoder.layers.13.layer_norm1.weight'',
          ''vision_model.encoder.layers.10.self_attn.v_proj.weight'', ''vision_model.encoder.layers.2.mlp.fc1.weight'',
          ''vision_model.encoder.layers.0.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.mlp.fc2.bias'', ''vision_model.encoder.layers.23.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.7.mlp.fc2.weight'', ''vision_model.encoder.layers.5.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.6.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.mlp.fc2.weight'',
          ''vision_model.encoder.layers.6.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.mlp.fc2.weight'',
          ''vision_model.encoder.layers.21.mlp.fc2.bias'', ''vision_model.encoder.layers.22.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.15.mlp.fc2.bias'', ''vision_model.encoder.layers.19.mlp.fc2.bias'',
          ''vision_model.encoder.layers.8.self_attn.k_proj.weight'', ''vision_model.encoder.layers.15.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.4.self_attn.v_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.7.mlp.fc1.bias'', ''vision_model.encoder.layers.14.mlp.fc1.bias'',
          ''vision_model.encoder.layers.20.self_attn.k_proj.bias'', ''vision_model.encoder.layers.15.layer_norm2.weight'',
          ''vision_model.encoder.layers.12.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.mlp.fc1.weight'',
          ''vision_model.encoder.layers.5.layer_norm2.weight'', ''vision_model.encoder.layers.14.layer_norm1.weight'',
          ''vision_model.encoder.layers.19.layer_norm2.bias'', ''vision_model.encoder.layers.3.mlp.fc2.bias'',
          ''vision_model.encoder.layers.4.layer_norm1.bias'', ''vision_model.encoder.layers.17.mlp.fc1.weight'',
          ''vision_model.encoder.layers.3.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.layer_norm2.bias'',
          ''vision_model.encoder.layers.23.mlp.fc2.bias'', ''vision_model.encoder.layers.6.mlp.fc2.weight'',
          ''vision_model.encoder.layers.11.layer_norm2.bias'', ''vision_model.encoder.layers.10.layer_norm1.weight'',
          ''vision_model.encoder.layers.12.self_attn.out_proj.bias'', ''vision_model.encoder.layers.23.layer_norm2.bias'',
          ''vision_model.encoder.layers.17.self_attn.k_proj.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.18.mlp.fc1.weight'', ''vision_model.encoder.layers.17.layer_norm2.weight'',
          ''vision_model.encoder.layers.12.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.0.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.13.mlp.fc1.weight'', ''vision_model.encoder.layers.1.self_attn.k_proj.bias'',
          ''vision_model.pre_layrnorm.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.13.mlp.fc1.bias'', ''vision_model.encoder.layers.2.mlp.fc1.bias'',
          ''vision_model.encoder.layers.15.self_attn.v_proj.bias'', ''vision_model.encoder.layers.10.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.2.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.mlp.fc2.weight'',
          ''vision_model.encoder.layers.3.self_attn.k_proj.bias'', ''vision_model.encoder.layers.2.layer_norm2.weight'',
          ''vision_model.post_layernorm.weight'', ''vision_model.encoder.layers.9.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.10.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.layer_norm1.bias'',
          ''vision_model.encoder.layers.11.layer_norm2.weight'', ''vision_model.encoder.layers.16.mlp.fc2.weight'',
          ''vision_model.encoder.layers.0.mlp.fc1.weight'', ''vision_model.encoder.layers.15.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.11.mlp.fc1.weight'', ''vision_model.encoder.layers.2.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.14.self_attn.k_proj.weight'', ''vision_model.pre_layrnorm.weight'',
          ''vision_model.encoder.layers.20.self_attn.k_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.17.layer_norm2.bias'', ''vision_model.encoder.layers.14.mlp.fc2.weight'',
          ''vision_model.encoder.layers.4.self_attn.v_proj.bias'', ''vision_model.encoder.layers.8.mlp.fc2.bias'',
          ''vision_model.encoder.layers.12.layer_norm1.weight'', ''vision_model.encoder.layers.23.mlp.fc1.bias'',
          ''vision_model.encoder.layers.9.self_attn.out_proj.bias'', ''vision_model.encoder.layers.2.mlp.fc2.bias'',
          ''vision_model.encoder.layers.6.self_attn.q_proj.weight'', ''vision_model.encoder.layers.12.mlp.fc2.weight'',
          ''vision_model.encoder.layers.18.layer_norm2.bias'', ''vision_model.encoder.layers.16.layer_norm2.bias'',
          ''vision_model.encoder.layers.9.mlp.fc2.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.7.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.15.layer_norm1.bias'', ''vision_model.encoder.layers.16.mlp.fc1.weight'',
          ''vision_model.encoder.layers.20.mlp.fc2.bias'', ''vision_model.encoder.layers.11.self_attn.k_proj.weight'',
          ''vision_model.encoder.layers.12.self_attn.q_proj.weight'', ''vision_model.encoder.layers.0.layer_norm1.weight'',
          ''vision_model.encoder.layers.2.self_attn.q_proj.bias'', ''vision_model.encoder.layers.14.layer_norm1.bias'',
          ''vision_model.encoder.layers.4.mlp.fc2.weight'', ''vision_model.encoder.layers.13.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.14.layer_norm2.weight'', ''vision_model.encoder.layers.9.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.12.mlp.fc2.bias'', ''logit_scale'', ''vision_model.encoder.layers.5.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.13.self_attn.q_proj.bias'', ''vision_model.encoder.layers.15.mlp.fc2.weight'',
          ''vision_model.encoder.layers.7.self_attn.out_proj.weight'', ''vision_model.encoder.layers.8.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.5.self_attn.k_proj.weight'', ''vision_model.encoder.layers.18.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.18.layer_norm1.bias'', ''vision_model.encoder.layers.18.layer_norm2.weight'',
          ''vision_model.encoder.layers.21.self_attn.k_proj.weight'', ''vision_model.encoder.layers.14.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.0.self_attn.q_proj.weight'', ''vision_model.encoder.layers.5.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.14.mlp.fc2.bias'', ''vision_model.encoder.layers.5.layer_norm1.bias'',
          ''vision_model.encoder.layers.11.mlp.fc1.bias'', ''vision_model.encoder.layers.20.layer_norm1.bias'',
          ''vision_model.encoder.layers.17.mlp.fc2.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.1.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.layer_norm1.bias'',
          ''vision_model.encoder.layers.5.self_attn.q_proj.bias'', ''vision_model.encoder.layers.12.layer_norm1.bias'',
          ''vision_model.encoder.layers.21.layer_norm1.weight'', ''vision_model.encoder.layers.20.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.1.layer_norm1.bias'', ''vision_model.encoder.layers.13.self_attn.k_proj.bias'',
          ''vision_model.encoder.layers.9.self_attn.out_proj.weight'', ''vision_model.encoder.layers.0.self_attn.out_proj.bias'',
          ''text_projection.weight'', ''vision_model.encoder.layers.6.mlp.fc2.bias'',
          ''vision_model.encoder.layers.3.mlp.fc1.bias'', ''vision_model.encoder.layers.8.self_attn.v_proj.bias'',
          ''vision_model.encoder.layers.22.mlp.fc1.weight'', ''vision_model.encoder.layers.15.layer_norm2.bias'',
          ''vision_model.encoder.layers.18.mlp.fc1.bias'', ''vision_model.encoder.layers.1.mlp.fc2.bias'',
          ''vision_model.encoder.layers.22.mlp.fc1.bias'', ''vision_model.encoder.layers.21.self_attn.out_proj.weight'',
          ''vision_model.encoder.layers.1.self_attn.out_proj.weight'', ''vision_model.encoder.layers.10.layer_norm2.weight'',
          ''vision_model.encoder.layers.0.self_attn.out_proj.weight'', ''vision_model.encoder.layers.12.mlp.fc1.weight'',
          ''vision_model.encoder.layers.17.layer_norm1.weight'', ''vision_model.encoder.layers.22.self_attn.out_proj.bias'',
          ''vision_model.encoder.layers.22.layer_norm2.weight'', ''vision_model.encoder.layers.11.self_attn.v_proj.weight'',
          ''visual_projection.weight'', ''vision_model.encoder.layers.17.self_attn.v_proj.weight'',
          ''vision_model.encoder.layers.20.mlp.fc2.weight'', ''vision_model.encoder.layers.20.layer_norm1.weight'',
          ''vision_model.encoder.layers.20.layer_norm2.weight'', ''vision_model.encoder.layers.11.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.0.self_attn.k_proj.weight'', ''vision_model.encoder.layers.8.layer_norm2.bias'',
          ''vision_model.encoder.layers.0.mlp.fc2.weight'', ''vision_model.encoder.layers.11.self_attn.q_proj.bias'',
          ''vision_model.encoder.layers.5.self_attn.v_proj.weight'', ''vision_model.encoder.layers.1.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.layer_norm2.bias'', ''vision_model.encoder.layers.21.layer_norm2.bias'',
          ''vision_model.encoder.layers.10.self_attn.q_proj.weight'', ''vision_model.encoder.layers.8.self_attn.q_proj.weight'',
          ''vision_model.encoder.layers.2.layer_norm2.bias'']

          - This IS expected if you are initializing CLIPTextModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).

          - This IS NOT expected if you are initializing CLIPTextModel from the checkpoint
          of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
          model from a BertForSequenceClassification model).

          Downloading shards:   0%|                                                                                                                                                                                                                                                   |
          0/2 [00:00<?, ?it/s^Downloading shards:   0%|                                                                                                                                                                                                                                                   |
          0/2 [00:34<?, ?it/s]

          ```

          '
        updatedAt: '2023-07-11T08:10:30.077Z'
      numEdits: 0
      reactions: []
    id: 64ad0e760d0826bdd8960093
    type: comment
  author: xings19
  content: 'if i load the model like this:

    ```python

    pipe1 = StableDiffusionXLPipeline.from_single_file(''/root/xxx/xxx/stable-diffusion-xl-base-0.9/sd_xl_base_0.9.safetensors'')

    ```

    The program will have the following output

    ```text

    global_step key not found in model

    Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not
    used when initializing CLIPTextModel: [''vision_model.encoder.layers.23.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.9.mlp.fc1.weight'', ''vision_model.encoder.layers.17.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.3.layer_norm1.bias'', ''vision_model.encoder.layers.18.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.16.self_attn.k_proj.weight'', ''vision_model.encoder.layers.9.layer_norm2.bias'',
    ''vision_model.encoder.layers.17.mlp.fc2.weight'', ''vision_model.encoder.layers.8.layer_norm1.weight'',
    ''vision_model.encoder.layers.21.self_attn.out_proj.bias'', ''vision_model.encoder.layers.5.mlp.fc1.bias'',
    ''vision_model.embeddings.patch_embedding.weight'', ''vision_model.encoder.layers.19.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.11.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.23.self_attn.q_proj.bias'', ''vision_model.encoder.layers.19.mlp.fc1.weight'',
    ''vision_model.encoder.layers.21.layer_norm2.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.6.self_attn.q_proj.bias'', ''vision_model.encoder.layers.10.mlp.fc2.weight'',
    ''vision_model.encoder.layers.6.layer_norm1.bias'', ''vision_model.encoder.layers.4.mlp.fc2.bias'',
    ''vision_model.encoder.layers.14.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.self_attn.v_proj.weight'',
    ''vision_model.embeddings.class_embedding'', ''vision_model.encoder.layers.7.layer_norm1.bias'',
    ''vision_model.encoder.layers.22.layer_norm2.bias'', ''vision_model.encoder.layers.9.layer_norm1.bias'',
    ''vision_model.encoder.layers.23.mlp.fc1.weight'', ''vision_model.encoder.layers.16.mlp.fc2.bias'',
    ''vision_model.encoder.layers.20.layer_norm2.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.7.self_attn.k_proj.weight'', ''vision_model.encoder.layers.2.layer_norm1.bias'',
    ''vision_model.encoder.layers.0.mlp.fc2.bias'', ''vision_model.encoder.layers.7.layer_norm2.weight'',
    ''vision_model.encoder.layers.7.layer_norm1.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.18.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.weight'',
    ''vision_model.encoder.layers.23.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.layer_norm1.bias'',
    ''vision_model.encoder.layers.19.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.15.self_attn.q_proj.bias'', ''vision_model.encoder.layers.9.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.5.mlp.fc2.bias'', ''vision_model.encoder.layers.15.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.19.self_attn.k_proj.weight'', ''vision_model.encoder.layers.8.mlp.fc2.weight'',
    ''vision_model.encoder.layers.18.self_attn.k_proj.weight'', ''vision_model.encoder.layers.12.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.7.self_attn.q_proj.weight'', ''vision_model.encoder.layers.22.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.12.layer_norm2.bias'', ''vision_model.encoder.layers.19.layer_norm2.weight'',
    ''vision_model.encoder.layers.15.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc2.weight'',
    ''vision_model.encoder.layers.1.mlp.fc1.bias'', ''vision_model.encoder.layers.7.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.18.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.8.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.6.mlp.fc1.bias'', ''vision_model.encoder.layers.1.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.8.self_attn.k_proj.bias'', ''vision_model.encoder.layers.18.mlp.fc2.weight'',
    ''vision_model.encoder.layers.5.mlp.fc1.weight'', ''vision_model.encoder.layers.0.layer_norm1.bias'',
    ''vision_model.encoder.layers.21.mlp.fc2.weight'', ''vision_model.encoder.layers.2.layer_norm1.weight'',
    ''vision_model.encoder.layers.13.self_attn.k_proj.weight'', ''vision_model.encoder.layers.0.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.0.mlp.fc1.bias'', ''vision_model.encoder.layers.10.mlp.fc1.bias'',
    ''vision_model.encoder.layers.21.mlp.fc1.bias'', ''vision_model.encoder.layers.6.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.14.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.mlp.fc1.bias'',
    ''vision_model.encoder.layers.9.layer_norm2.weight'', ''vision_model.encoder.layers.18.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.19.self_attn.out_proj.weight'', ''vision_model.encoder.layers.1.mlp.fc1.weight'',
    ''vision_model.encoder.layers.2.mlp.fc2.weight'', ''vision_model.encoder.layers.5.layer_norm1.weight'',
    ''vision_model.encoder.layers.6.layer_norm2.weight'', ''vision_model.encoder.layers.5.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.18.self_attn.out_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.weight'',
    ''vision_model.encoder.layers.16.self_attn.out_proj.bias'', ''vision_model.encoder.layers.9.self_attn.k_proj.weight'',
    ''vision_model.encoder.layers.16.self_attn.v_proj.weight'', ''vision_model.encoder.layers.9.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.7.self_attn.v_proj.weight'', ''vision_model.encoder.layers.12.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.14.self_attn.out_proj.weight'', ''vision_model.encoder.layers.21.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.15.self_attn.k_proj.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.19.self_attn.v_proj.weight'', ''vision_model.encoder.layers.6.layer_norm1.weight'',
    ''vision_model.encoder.layers.19.layer_norm1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.22.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.mlp.fc2.weight'',
    ''vision_model.encoder.layers.2.self_attn.out_proj.weight'', ''vision_model.encoder.layers.10.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.12.mlp.fc1.bias'', ''vision_model.encoder.layers.11.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.12.self_attn.k_proj.weight'', ''vision_model.encoder.layers.23.layer_norm1.bias'',
    ''vision_model.encoder.layers.7.mlp.fc2.bias'', ''vision_model.encoder.layers.2.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.13.layer_norm2.bias'', ''vision_model.encoder.layers.3.layer_norm1.weight'',
    ''vision_model.encoder.layers.11.self_attn.k_proj.bias'', ''vision_model.encoder.layers.13.layer_norm1.bias'',
    ''vision_model.encoder.layers.6.self_attn.k_proj.weight'', ''vision_model.encoder.layers.1.layer_norm2.weight'',
    ''vision_model.encoder.layers.10.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.22.self_attn.q_proj.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.16.layer_norm2.weight'', ''vision_model.encoder.layers.23.layer_norm2.weight'',
    ''vision_model.embeddings.position_embedding.weight'', ''vision_model.encoder.layers.8.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.20.mlp.fc1.weight'', ''vision_model.encoder.layers.19.mlp.fc1.bias'',
    ''vision_model.encoder.layers.13.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.9.mlp.fc1.bias'', ''vision_model.encoder.layers.23.self_attn.v_proj.bias'',
    ''vision_model.post_layernorm.bias'', ''vision_model.encoder.layers.6.mlp.fc1.weight'',
    ''vision_model.encoder.layers.8.self_attn.out_proj.weight'', ''vision_model.encoder.layers.7.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.10.mlp.fc1.weight'', ''vision_model.encoder.layers.22.layer_norm1.bias'',
    ''vision_model.encoder.layers.23.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.4.layer_norm1.weight'', ''vision_model.encoder.layers.13.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.21.self_attn.v_proj.bias'', ''vision_model.encoder.layers.15.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.13.self_attn.q_proj.weight'', ''vision_model.encoder.layers.4.layer_norm2.weight'',
    ''vision_model.encoder.layers.15.layer_norm1.weight'', ''vision_model.encoder.layers.15.mlp.fc1.bias'',
    ''vision_model.encoder.layers.9.mlp.fc2.bias'', ''vision_model.encoder.layers.3.layer_norm2.weight'',
    ''vision_model.encoder.layers.10.self_attn.k_proj.bias'', ''vision_model.encoder.layers.17.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.18.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.15.self_attn.v_proj.weight'', ''vision_model.encoder.layers.5.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.0.layer_norm2.weight'', ''vision_model.encoder.layers.3.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.6.self_attn.v_proj.bias'', ''vision_model.encoder.layers.17.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.17.mlp.fc1.bias'', ''vision_model.encoder.layers.8.mlp.fc1.weight'',
    ''vision_model.encoder.layers.14.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.12.layer_norm2.weight'', ''vision_model.encoder.layers.4.mlp.fc1.bias'',
    ''vision_model.encoder.layers.3.layer_norm2.bias'', ''vision_model.encoder.layers.11.layer_norm1.weight'',
    ''vision_model.embeddings.position_ids'', ''vision_model.encoder.layers.13.mlp.fc2.weight'',
    ''vision_model.encoder.layers.4.mlp.fc1.weight'', ''vision_model.encoder.layers.13.mlp.fc2.bias'',
    ''vision_model.encoder.layers.4.layer_norm2.bias'', ''vision_model.encoder.layers.5.layer_norm2.bias'',
    ''vision_model.encoder.layers.20.mlp.fc1.bias'', ''vision_model.encoder.layers.16.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.20.self_attn.out_proj.weight'', ''vision_model.encoder.layers.2.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.22.layer_norm1.weight'', ''vision_model.encoder.layers.8.layer_norm2.weight'',
    ''vision_model.encoder.layers.8.self_attn.q_proj.bias'', ''vision_model.encoder.layers.22.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.22.self_attn.k_proj.weight'', ''vision_model.encoder.layers.17.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.21.layer_norm1.bias'', ''vision_model.encoder.layers.23.mlp.fc2.weight'',
    ''vision_model.encoder.layers.13.self_attn.v_proj.weight'', ''vision_model.encoder.layers.23.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.17.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.3.self_attn.k_proj.weight'', ''vision_model.encoder.layers.13.layer_norm2.weight'',
    ''vision_model.encoder.layers.16.self_attn.v_proj.bias'', ''vision_model.encoder.layers.6.layer_norm2.bias'',
    ''vision_model.encoder.layers.11.mlp.fc2.bias'', ''vision_model.encoder.layers.9.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.1.layer_norm1.weight'', ''vision_model.encoder.layers.4.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.22.mlp.fc2.bias'', ''vision_model.encoder.layers.3.mlp.fc1.weight'',
    ''vision_model.encoder.layers.2.self_attn.k_proj.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.11.mlp.fc2.weight'', ''vision_model.encoder.layers.7.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.6.self_attn.out_proj.bias'', ''vision_model.encoder.layers.19.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.20.self_attn.v_proj.bias'', ''vision_model.encoder.layers.20.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.9.layer_norm1.weight'', ''vision_model.encoder.layers.1.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.7.layer_norm2.bias'', ''vision_model.encoder.layers.12.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.18.self_attn.out_proj.bias'', ''vision_model.encoder.layers.14.mlp.fc1.weight'',
    ''vision_model.encoder.layers.4.self_attn.k_proj.weight'', ''vision_model.encoder.layers.16.layer_norm1.bias'',
    ''vision_model.encoder.layers.4.self_attn.q_proj.weight'', ''vision_model.encoder.layers.8.layer_norm1.bias'',
    ''vision_model.encoder.layers.14.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.layer_norm1.weight'',
    ''vision_model.encoder.layers.7.mlp.fc1.weight'', ''vision_model.encoder.layers.20.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.18.mlp.fc2.bias'', ''vision_model.encoder.layers.13.layer_norm1.weight'',
    ''vision_model.encoder.layers.10.self_attn.v_proj.weight'', ''vision_model.encoder.layers.2.mlp.fc1.weight'',
    ''vision_model.encoder.layers.0.self_attn.k_proj.bias'', ''vision_model.encoder.layers.0.layer_norm2.bias'',
    ''vision_model.encoder.layers.10.mlp.fc2.bias'', ''vision_model.encoder.layers.23.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.7.mlp.fc2.weight'', ''vision_model.encoder.layers.5.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.6.self_attn.v_proj.weight'', ''vision_model.encoder.layers.22.mlp.fc2.weight'',
    ''vision_model.encoder.layers.6.self_attn.k_proj.bias'', ''vision_model.encoder.layers.1.mlp.fc2.weight'',
    ''vision_model.encoder.layers.21.mlp.fc2.bias'', ''vision_model.encoder.layers.22.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.15.mlp.fc2.bias'', ''vision_model.encoder.layers.19.mlp.fc2.bias'',
    ''vision_model.encoder.layers.8.self_attn.k_proj.weight'', ''vision_model.encoder.layers.15.self_attn.k_proj.weight'',
    ''vision_model.encoder.layers.4.self_attn.v_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.7.mlp.fc1.bias'', ''vision_model.encoder.layers.14.mlp.fc1.bias'',
    ''vision_model.encoder.layers.20.self_attn.k_proj.bias'', ''vision_model.encoder.layers.15.layer_norm2.weight'',
    ''vision_model.encoder.layers.12.self_attn.k_proj.bias'', ''vision_model.encoder.layers.21.mlp.fc1.weight'',
    ''vision_model.encoder.layers.5.layer_norm2.weight'', ''vision_model.encoder.layers.14.layer_norm1.weight'',
    ''vision_model.encoder.layers.19.layer_norm2.bias'', ''vision_model.encoder.layers.3.mlp.fc2.bias'',
    ''vision_model.encoder.layers.4.layer_norm1.bias'', ''vision_model.encoder.layers.17.mlp.fc1.weight'',
    ''vision_model.encoder.layers.3.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.layer_norm2.bias'',
    ''vision_model.encoder.layers.23.mlp.fc2.bias'', ''vision_model.encoder.layers.6.mlp.fc2.weight'',
    ''vision_model.encoder.layers.11.layer_norm2.bias'', ''vision_model.encoder.layers.10.layer_norm1.weight'',
    ''vision_model.encoder.layers.12.self_attn.out_proj.bias'', ''vision_model.encoder.layers.23.layer_norm2.bias'',
    ''vision_model.encoder.layers.17.self_attn.k_proj.weight'', ''vision_model.encoder.layers.21.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.18.mlp.fc1.weight'', ''vision_model.encoder.layers.17.layer_norm2.weight'',
    ''vision_model.encoder.layers.12.self_attn.v_proj.weight'', ''vision_model.encoder.layers.14.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.0.self_attn.v_proj.bias'', ''vision_model.encoder.layers.19.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.13.mlp.fc1.weight'', ''vision_model.encoder.layers.1.self_attn.k_proj.bias'',
    ''vision_model.pre_layrnorm.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.13.mlp.fc1.bias'', ''vision_model.encoder.layers.2.mlp.fc1.bias'',
    ''vision_model.encoder.layers.15.self_attn.v_proj.bias'', ''vision_model.encoder.layers.10.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.2.self_attn.out_proj.bias'', ''vision_model.encoder.layers.3.mlp.fc2.weight'',
    ''vision_model.encoder.layers.3.self_attn.k_proj.bias'', ''vision_model.encoder.layers.2.layer_norm2.weight'',
    ''vision_model.post_layernorm.weight'', ''vision_model.encoder.layers.9.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.10.self_attn.v_proj.bias'', ''vision_model.encoder.layers.11.layer_norm1.bias'',
    ''vision_model.encoder.layers.11.layer_norm2.weight'', ''vision_model.encoder.layers.16.mlp.fc2.weight'',
    ''vision_model.encoder.layers.0.mlp.fc1.weight'', ''vision_model.encoder.layers.15.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.11.mlp.fc1.weight'', ''vision_model.encoder.layers.2.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.14.self_attn.k_proj.weight'', ''vision_model.pre_layrnorm.weight'',
    ''vision_model.encoder.layers.20.self_attn.k_proj.weight'', ''vision_model.encoder.layers.4.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.17.layer_norm2.bias'', ''vision_model.encoder.layers.14.mlp.fc2.weight'',
    ''vision_model.encoder.layers.4.self_attn.v_proj.bias'', ''vision_model.encoder.layers.8.mlp.fc2.bias'',
    ''vision_model.encoder.layers.12.layer_norm1.weight'', ''vision_model.encoder.layers.23.mlp.fc1.bias'',
    ''vision_model.encoder.layers.9.self_attn.out_proj.bias'', ''vision_model.encoder.layers.2.mlp.fc2.bias'',
    ''vision_model.encoder.layers.6.self_attn.q_proj.weight'', ''vision_model.encoder.layers.12.mlp.fc2.weight'',
    ''vision_model.encoder.layers.18.layer_norm2.bias'', ''vision_model.encoder.layers.16.layer_norm2.bias'',
    ''vision_model.encoder.layers.9.mlp.fc2.weight'', ''vision_model.encoder.layers.3.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.7.self_attn.q_proj.bias'', ''vision_model.encoder.layers.16.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.15.layer_norm1.bias'', ''vision_model.encoder.layers.16.mlp.fc1.weight'',
    ''vision_model.encoder.layers.20.mlp.fc2.bias'', ''vision_model.encoder.layers.11.self_attn.k_proj.weight'',
    ''vision_model.encoder.layers.12.self_attn.q_proj.weight'', ''vision_model.encoder.layers.0.layer_norm1.weight'',
    ''vision_model.encoder.layers.2.self_attn.q_proj.bias'', ''vision_model.encoder.layers.14.layer_norm1.bias'',
    ''vision_model.encoder.layers.4.mlp.fc2.weight'', ''vision_model.encoder.layers.13.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.14.layer_norm2.weight'', ''vision_model.encoder.layers.9.self_attn.q_proj.bias'',
    ''vision_model.encoder.layers.12.mlp.fc2.bias'', ''logit_scale'', ''vision_model.encoder.layers.5.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.13.self_attn.q_proj.bias'', ''vision_model.encoder.layers.15.mlp.fc2.weight'',
    ''vision_model.encoder.layers.7.self_attn.out_proj.weight'', ''vision_model.encoder.layers.8.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.5.self_attn.k_proj.weight'', ''vision_model.encoder.layers.18.self_attn.v_proj.bias'',
    ''vision_model.encoder.layers.18.layer_norm1.bias'', ''vision_model.encoder.layers.18.layer_norm2.weight'',
    ''vision_model.encoder.layers.21.self_attn.k_proj.weight'', ''vision_model.encoder.layers.14.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.0.self_attn.q_proj.weight'', ''vision_model.encoder.layers.5.self_attn.out_proj.bias'',
    ''vision_model.encoder.layers.14.mlp.fc2.bias'', ''vision_model.encoder.layers.5.layer_norm1.bias'',
    ''vision_model.encoder.layers.11.mlp.fc1.bias'', ''vision_model.encoder.layers.20.layer_norm1.bias'',
    ''vision_model.encoder.layers.17.mlp.fc2.bias'', ''vision_model.encoder.layers.1.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.1.self_attn.k_proj.weight'', ''vision_model.encoder.layers.10.layer_norm1.bias'',
    ''vision_model.encoder.layers.5.self_attn.q_proj.bias'', ''vision_model.encoder.layers.12.layer_norm1.bias'',
    ''vision_model.encoder.layers.21.layer_norm1.weight'', ''vision_model.encoder.layers.20.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.1.layer_norm1.bias'', ''vision_model.encoder.layers.13.self_attn.k_proj.bias'',
    ''vision_model.encoder.layers.9.self_attn.out_proj.weight'', ''vision_model.encoder.layers.0.self_attn.out_proj.bias'',
    ''text_projection.weight'', ''vision_model.encoder.layers.6.mlp.fc2.bias'', ''vision_model.encoder.layers.3.mlp.fc1.bias'',
    ''vision_model.encoder.layers.8.self_attn.v_proj.bias'', ''vision_model.encoder.layers.22.mlp.fc1.weight'',
    ''vision_model.encoder.layers.15.layer_norm2.bias'', ''vision_model.encoder.layers.18.mlp.fc1.bias'',
    ''vision_model.encoder.layers.1.mlp.fc2.bias'', ''vision_model.encoder.layers.22.mlp.fc1.bias'',
    ''vision_model.encoder.layers.21.self_attn.out_proj.weight'', ''vision_model.encoder.layers.1.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.10.layer_norm2.weight'', ''vision_model.encoder.layers.0.self_attn.out_proj.weight'',
    ''vision_model.encoder.layers.12.mlp.fc1.weight'', ''vision_model.encoder.layers.17.layer_norm1.weight'',
    ''vision_model.encoder.layers.22.self_attn.out_proj.bias'', ''vision_model.encoder.layers.22.layer_norm2.weight'',
    ''vision_model.encoder.layers.11.self_attn.v_proj.weight'', ''visual_projection.weight'',
    ''vision_model.encoder.layers.17.self_attn.v_proj.weight'', ''vision_model.encoder.layers.20.mlp.fc2.weight'',
    ''vision_model.encoder.layers.20.layer_norm1.weight'', ''vision_model.encoder.layers.20.layer_norm2.weight'',
    ''vision_model.encoder.layers.11.self_attn.q_proj.weight'', ''vision_model.encoder.layers.0.self_attn.k_proj.weight'',
    ''vision_model.encoder.layers.8.layer_norm2.bias'', ''vision_model.encoder.layers.0.mlp.fc2.weight'',
    ''vision_model.encoder.layers.11.self_attn.q_proj.bias'', ''vision_model.encoder.layers.5.self_attn.v_proj.weight'',
    ''vision_model.encoder.layers.1.layer_norm2.bias'', ''vision_model.encoder.layers.10.layer_norm2.bias'',
    ''vision_model.encoder.layers.21.layer_norm2.bias'', ''vision_model.encoder.layers.10.self_attn.q_proj.weight'',
    ''vision_model.encoder.layers.8.self_attn.q_proj.weight'', ''vision_model.encoder.layers.2.layer_norm2.bias'']

    - This IS expected if you are initializing CLIPTextModel from the checkpoint of
    a model trained on another task or with another architecture (e.g. initializing
    a BertForSequenceClassification model from a BertForPreTraining model).

    - This IS NOT expected if you are initializing CLIPTextModel from the checkpoint
    of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
    model from a BertForSequenceClassification model).

    Downloading shards:   0%|                                                                                                                                                                                                                                                   |
    0/2 [00:00<?, ?it/s^Downloading shards:   0%|                                                                                                                                                                                                                                                   |
    0/2 [00:34<?, ?it/s]

    ```

    '
  created_at: 2023-07-11 07:10:30+00:00
  edited: false
  hidden: false
  id: 64ad0e760d0826bdd8960093
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-07-11T14:53:35.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8654975295066833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;xings19&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xings19\"\
          >@<span class=\"underline\">xings19</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>The loading of the single file format will be greatly improved with this\
          \ PR: <a rel=\"nofollow\" href=\"https://github.com/huggingface/diffusers/pull/4041\"\
          >https://github.com/huggingface/diffusers/pull/4041</a></p>\n"
        raw: 'Hey @xings19,


          The loading of the single file format will be greatly improved with this
          PR: https://github.com/huggingface/diffusers/pull/4041'
        updatedAt: '2023-07-11T14:53:35.458Z'
      numEdits: 0
      reactions: []
    id: 64ad6cef1aee69ece050cb09
    type: comment
  author: patrickvonplaten
  content: 'Hey @xings19,


    The loading of the single file format will be greatly improved with this PR: https://github.com/huggingface/diffusers/pull/4041'
  created_at: 2023-07-11 13:53:35+00:00
  edited: false
  hidden: false
  id: 64ad6cef1aee69ece050cb09
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: stabilityai/stable-diffusion-xl-base-0.9
repo_type: model
status: open
target_branch: null
title: Does sd_xl_base_0.9.safetensors contain all weights?
