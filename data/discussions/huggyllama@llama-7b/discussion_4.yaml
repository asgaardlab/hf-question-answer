!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Iamexperimenting
conflicting_files: null
created_at: 2023-07-03 19:49:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
      fullname: IamexperimentingNow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Iamexperimenting
      type: user
    createdAt: '2023-07-03T20:49:10.000Z'
    data:
      edited: false
      editors:
      - Iamexperimenting
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7497113943099976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
          fullname: IamexperimentingNow
          isHf: false
          isPro: false
          name: Iamexperimenting
          type: user
        html: '<p>Hi,</p>

          <p>Can someone tell me how to run question answering model using LLama?
          I''m trying to build QuestionAnswering model , when I run its not giving
          correct answer even though passing context along it.</p>

          <pre><code>from langchain import PromptTemplate, HugginFacePipeline, LLMChain

          from transformers import pipeline

          import torch

          template  = """Read this article and answer the below question {context}\n
          {question} \n if you don;t know the answer please say "I don''t know" don''t
          make up on your own."""

          prompt = PromptTemplate(template=template, input_variables=["context", "question"])

          model_name = "huggyllama/llama-7b"

          hf_pipe = HugginFacePipeline(pipeline=pipeline(model=model_name, device=''cuda:0'',
          torch_dtype=torch.float16, max_new_tokens=512))

          llm_chain = LLMChain(prompt=prompt, llm=hf_pipe)


          torch.cuda.empty.cache()

          result = llm_chain.run(context=input_context, question=user_question)

          </code></pre>

          <p>I''m using the above script but it is not giving correct answer even
          though answer present in the context, can someone help me here?</p>

          '
        raw: "Hi,\r\n\r\nCan someone tell me how to run question answering model using\
          \ LLama? I'm trying to build QuestionAnswering model , when I run its not\
          \ giving correct answer even though passing context along it.\r\n\r\n```\r\
          \nfrom langchain import PromptTemplate, HugginFacePipeline, LLMChain\r\n\
          from transformers import pipeline\r\nimport torch\r\ntemplate  = \"\"\"\
          Read this article and answer the below question {context}\\n {question}\
          \ \\n if you don;t know the answer please say \"I don't know\" don't make\
          \ up on your own.\"\"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"\
          context\", \"question\"])\r\nmodel_name = \"huggyllama/llama-7b\"\r\nhf_pipe\
          \ = HugginFacePipeline(pipeline=pipeline(model=model_name, device='cuda:0',\
          \ torch_dtype=torch.float16, max_new_tokens=512))\r\nllm_chain = LLMChain(prompt=prompt,\
          \ llm=hf_pipe)\r\n\r\ntorch.cuda.empty.cache()\r\nresult = llm_chain.run(context=input_context,\
          \ question=user_question)\r\n\r\n```\r\n\r\nI'm using the above script but\
          \ it is not giving correct answer even though answer present in the context,\
          \ can someone help me here?"
        updatedAt: '2023-07-03T20:49:10.937Z'
      numEdits: 0
      reactions: []
    id: 64a3344616b4a46798a22546
    type: comment
  author: Iamexperimenting
  content: "Hi,\r\n\r\nCan someone tell me how to run question answering model using\
    \ LLama? I'm trying to build QuestionAnswering model , when I run its not giving\
    \ correct answer even though passing context along it.\r\n\r\n```\r\nfrom langchain\
    \ import PromptTemplate, HugginFacePipeline, LLMChain\r\nfrom transformers import\
    \ pipeline\r\nimport torch\r\ntemplate  = \"\"\"Read this article and answer the\
    \ below question {context}\\n {question} \\n if you don;t know the answer please\
    \ say \"I don't know\" don't make up on your own.\"\"\"\r\nprompt = PromptTemplate(template=template,\
    \ input_variables=[\"context\", \"question\"])\r\nmodel_name = \"huggyllama/llama-7b\"\
    \r\nhf_pipe = HugginFacePipeline(pipeline=pipeline(model=model_name, device='cuda:0',\
    \ torch_dtype=torch.float16, max_new_tokens=512))\r\nllm_chain = LLMChain(prompt=prompt,\
    \ llm=hf_pipe)\r\n\r\ntorch.cuda.empty.cache()\r\nresult = llm_chain.run(context=input_context,\
    \ question=user_question)\r\n\r\n```\r\n\r\nI'm using the above script but it\
    \ is not giving correct answer even though answer present in the context, can\
    \ someone help me here?"
  created_at: 2023-07-03 19:49:10+00:00
  edited: false
  hidden: false
  id: 64a3344616b4a46798a22546
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/787423c51210cae2238efb24873ed21d.svg
      fullname: Beuvelet David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Beuvelet
      type: user
    createdAt: '2023-07-14T13:58:31.000Z'
    data:
      edited: false
      editors:
      - Beuvelet
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.9913285970687866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/787423c51210cae2238efb24873ed21d.svg
          fullname: Beuvelet David
          isHf: false
          isPro: false
          name: Beuvelet
          type: user
        html: "<p>Tu aura plus facile en installant ceci pour ex\xE9cut\xE9 ton mod\xE8\
          le de plus il est compl\xE8tement param\xE9trable : <a rel=\"nofollow\"\
          \ href=\"https://github.com/oobabooga/text-generation-webui/tree/main\"\
          >https://github.com/oobabooga/text-generation-webui/tree/main</a><br>Il\
          \ te faut juste l'installer sur ton pc, ensuite tu vas dans la section mod\xE8\
          le et tu l'importe, ensuite tu peux le param\xE9tr\xE9 comme bon te semble\
          \ en suivant la doc du programme ui.</p>\n<p>J'esp\xE8re que cela te serra\
          \ tr\xE8s utile !!<br>bonne journ\xE9e !</p>\n"
        raw: "Tu aura plus facile en installant ceci pour ex\xE9cut\xE9 ton mod\xE8\
          le de plus il est compl\xE8tement param\xE9trable : https://github.com/oobabooga/text-generation-webui/tree/main\n\
          Il te faut juste l'installer sur ton pc, ensuite tu vas dans la section\
          \ mod\xE8le et tu l'importe, ensuite tu peux le param\xE9tr\xE9 comme bon\
          \ te semble en suivant la doc du programme ui.\n\nJ'esp\xE8re que cela te\
          \ serra tr\xE8s utile !!\nbonne journ\xE9e !"
        updatedAt: '2023-07-14T13:58:31.228Z'
      numEdits: 0
      reactions: []
    id: 64b15487f2497130535eefbf
    type: comment
  author: Beuvelet
  content: "Tu aura plus facile en installant ceci pour ex\xE9cut\xE9 ton mod\xE8\
    le de plus il est compl\xE8tement param\xE9trable : https://github.com/oobabooga/text-generation-webui/tree/main\n\
    Il te faut juste l'installer sur ton pc, ensuite tu vas dans la section mod\xE8\
    le et tu l'importe, ensuite tu peux le param\xE9tr\xE9 comme bon te semble en\
    \ suivant la doc du programme ui.\n\nJ'esp\xE8re que cela te serra tr\xE8s utile\
    \ !!\nbonne journ\xE9e !"
  created_at: 2023-07-14 12:58:31+00:00
  edited: false
  hidden: false
  id: 64b15487f2497130535eefbf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: huggyllama/llama-7b
repo_type: model
status: open
target_branch: null
title: question answering using llama
