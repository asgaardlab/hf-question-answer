!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiyouga
conflicting_files:
- modeling_baichuan.py
created_at: 2023-07-15 13:28:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-15T14:28:56.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7790951728820801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>The previous implementation does not accept attention masks as inputs,
          so it will cause some unexpected behaviours at batched inference (commonly
          using left-padding). So I reimplemented the alibi encodings to take attention
          masks in user inputs. Note that this implementation largely depends on [1].</p>

          <p>[1] <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py</a></p>

          '
        raw: 'The previous implementation does not accept attention masks as inputs,
          so it will cause some unexpected behaviours at batched inference (commonly
          using left-padding). So I reimplemented the alibi encodings to take attention
          masks in user inputs. Note that this implementation largely depends on [1].


          [1] https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py'
        updatedAt: '2023-07-15T14:28:56.505Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - 0xDing
    id: 64b2ad28094a60c78bf16903
    type: comment
  author: hiyouga
  content: 'The previous implementation does not accept attention masks as inputs,
    so it will cause some unexpected behaviours at batched inference (commonly using
    left-padding). So I reimplemented the alibi encodings to take attention masks
    in user inputs. Note that this implementation largely depends on [1].


    [1] https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py'
  created_at: 2023-07-15 13:28:56+00:00
  edited: false
  hidden: false
  id: 64b2ad28094a60c78bf16903
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-15T14:28:56.000Z'
    data:
      oid: 1f41c4b3e064635312c9c8074e36cc0e862f94e1
      parents:
      - 43fb20e1c7d9a19118ed0ae8076061dba775273a
      subject: Take input attention masks to support left-padded sequences
    id: 64b2ad280000000000000000
    type: commit
  author: hiyouga
  created_at: 2023-07-15 13:28:56+00:00
  id: 64b2ad280000000000000000
  oid: 1f41c4b3e064635312c9c8074e36cc0e862f94e1
  summary: Take input attention masks to support left-padded sequences
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-15T14:31:26.000Z'
    data:
      edited: true
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7864975333213806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>Of course, the above implementation requires re-computing alibi
          tensors at each inference time. We cannot use cached tensors while using
          input attention masks. Thus, the inference efficiency will be slightly worse
          than the original version.</p>

          '
        raw: Of course, the above implementation requires re-computing alibi tensors
          at each inference time. We cannot use cached tensors while using input attention
          masks. Thus, the inference efficiency will be slightly worse than the original
          version.
        updatedAt: '2023-07-15T14:32:30.654Z'
      numEdits: 1
      reactions: []
    id: 64b2adbe34a92b848c7716c9
    type: comment
  author: hiyouga
  content: Of course, the above implementation requires re-computing alibi tensors
    at each inference time. We cannot use cached tensors while using input attention
    masks. Thus, the inference efficiency will be slightly worse than the original
    version.
  created_at: 2023-07-15 13:31:26+00:00
  edited: true
  hidden: false
  id: 64b2adbe34a92b848c7716c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-15T14:34:58.000Z'
    data:
      oid: 3a663f5be1a584660a17ddb5b4673eb9435bd4e4
      parents:
      - 1f41c4b3e064635312c9c8074e36cc0e862f94e1
      subject: Update modeling_baichuan.py
    id: 64b2ae920000000000000000
    type: commit
  author: hiyouga
  created_at: 2023-07-15 13:34:58+00:00
  id: 64b2ae920000000000000000
  oid: 3a663f5be1a584660a17ddb5b4673eb9435bd4e4
  summary: Update modeling_baichuan.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-07-16T07:50:44.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9414365887641907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>Could alibi fused with expanded mask and do not need to take causal
          mask into consideration? Because alibi mask is like causal mask which is
          a lower triu?</p>

          '
        raw: Could alibi fused with expanded mask and do not need to take causal mask
          into consideration? Because alibi mask is like causal mask which is a lower
          triu?
        updatedAt: '2023-07-16T07:50:44.717Z'
      numEdits: 0
      reactions: []
    id: 64b3a15425882acb62d5eedd
    type: comment
  author: wuzhiying2023
  content: Could alibi fused with expanded mask and do not need to take causal mask
    into consideration? Because alibi mask is like causal mask which is a lower triu?
  created_at: 2023-07-16 06:50:44+00:00
  edited: false
  hidden: false
  id: 64b3a15425882acb62d5eedd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-16T09:19:24.000Z'
    data:
      oid: 478dc90defc8cd69eeab31584081bcde3a86c765
      parents:
      - 3a663f5be1a584660a17ddb5b4673eb9435bd4e4
      subject: update cache format to support contrastive search and beam search
    id: 64b3b61c0000000000000000
    type: commit
  author: hiyouga
  created_at: 2023-07-16 08:19:24+00:00
  id: 64b3b61c0000000000000000
  oid: 478dc90defc8cd69eeab31584081bcde3a86c765
  summary: update cache format to support contrastive search and beam search
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-16T09:22:27.000Z'
    data:
      oid: c9d4975e2c51d112bef06a140ac0c5f8de172817
      parents:
      - 478dc90defc8cd69eeab31584081bcde3a86c765
      subject: trim whitespace
    id: 64b3b6d30000000000000000
    type: commit
  author: hiyouga
  created_at: 2023-07-16 08:22:27+00:00
  id: 64b3b6d30000000000000000
  oid: c9d4975e2c51d112bef06a140ac0c5f8de172817
  summary: trim whitespace
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-07-19T10:12:09.000Z'
    data:
      status: closed
    id: 64b7b6f9efefc8a73889ff74
    type: status-change
  author: hiyouga
  created_at: 2023-07-19 09:12:09+00:00
  id: 64b7b6f9efefc8a73889ff74
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 1
repo_id: baichuan-inc/Baichuan-13B-Base
repo_type: model
status: closed
target_branch: refs/heads/main
title: Take input attention masks to support left-padded sequences
