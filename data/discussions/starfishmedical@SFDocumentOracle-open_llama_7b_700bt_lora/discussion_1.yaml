!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flyingkiwiguy
conflicting_files: null
created_at: 2023-06-27 06:56:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-27T07:56:26.000Z'
    data:
      edited: false
      editors:
      - flyingkiwiguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9554967880249023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
          fullname: Gary Mulder
          isHf: false
          isPro: false
          name: flyingkiwiguy
          type: user
        html: '<p>In the model card you mention:</p>

          <p>"The included tokenizer is based on that of the baseline model, however
          the BOS, EOS, and UNK/PAD tokens are distinctly defined, which was not the
          case with the baseline"</p>

          <p>Can you explain a bit more the motivation behind this change to the tokenizer?
          I notice that a lot of fine tuning of llama-cpp uses tokenizers with the
          <code>add_eos_token=True</code> flag set.</p>

          '
        raw: "In the model card you mention:\r\n\r\n\"The included tokenizer is based\
          \ on that of the baseline model, however the BOS, EOS, and UNK/PAD tokens\
          \ are distinctly defined, which was not the case with the baseline\"\r\n\
          \r\nCan you explain a bit more the motivation behind this change to the\
          \ tokenizer? I notice that a lot of fine tuning of llama-cpp uses tokenizers\
          \ with the `add_eos_token=True` flag set."
        updatedAt: '2023-06-27T07:56:26.651Z'
      numEdits: 0
      reactions: []
    id: 649a962a994b2637aa55a40c
    type: comment
  author: flyingkiwiguy
  content: "In the model card you mention:\r\n\r\n\"The included tokenizer is based\
    \ on that of the baseline model, however the BOS, EOS, and UNK/PAD tokens are\
    \ distinctly defined, which was not the case with the baseline\"\r\n\r\nCan you\
    \ explain a bit more the motivation behind this change to the tokenizer? I notice\
    \ that a lot of fine tuning of llama-cpp uses tokenizers with the `add_eos_token=True`\
    \ flag set."
  created_at: 2023-06-27 06:56:26+00:00
  edited: false
  hidden: false
  id: 649a962a994b2637aa55a40c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
      fullname: Thorold Tronrud
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ttronrud
      type: user
    createdAt: '2023-06-27T17:29:03.000Z'
    data:
      edited: false
      editors:
      - ttronrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676564931869507
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
          fullname: Thorold Tronrud
          isHf: false
          isPro: false
          name: ttronrud
          type: user
        html: '<p>The choice was mostly for fine-tuning, which requires a pad token.
          I haven''t noticed any issues with text generation, so I haven''t changed
          it. </p>

          <p>The EOS() token will prevent run-on generation, and since I brace my
          fine-tuning data with <s>...</s>, the model is trained to end appropriately.</p>

          '
        raw: "The choice was mostly for fine-tuning, which requires a pad token. I\
          \ haven't noticed any issues with text generation, so I haven't changed\
          \ it. \n\nThe EOS(</s>) token will prevent run-on generation, and since\
          \ I brace my fine-tuning data with <s>...</s>, the model is trained to end\
          \ appropriately."
        updatedAt: '2023-06-27T17:29:03.020Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flyingkiwiguy
    id: 649b1c5f8bf2c38790f5626b
    type: comment
  author: ttronrud
  content: "The choice was mostly for fine-tuning, which requires a pad token. I haven't\
    \ noticed any issues with text generation, so I haven't changed it. \n\nThe EOS(</s>)\
    \ token will prevent run-on generation, and since I brace my fine-tuning data\
    \ with <s>...</s>, the model is trained to end appropriately."
  created_at: 2023-06-27 16:29:03+00:00
  edited: false
  hidden: false
  id: 649b1c5f8bf2c38790f5626b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: starfishmedical/SFDocumentOracle-open_llama_7b_700bt_lora
repo_type: model
status: open
target_branch: null
title: Questions about the BOS, EOS, and UNK/PAD tokenizer changes
