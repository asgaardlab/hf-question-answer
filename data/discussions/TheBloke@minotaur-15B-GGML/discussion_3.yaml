!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-06-22 05:48:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-22T06:48:20.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581629633903503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>It seems Ooba got some updates talking about Minotaur 8K? But I
          can''t get either the GPTQ or GGML versions of this model to run in Ooba
          even after update? I can get up to 65B models to run so it''s not a VRAM
          or RAM issue? Any ideas? Or is this model still not supported? Or special
          setup steps? </p>

          '
        raw: 'It seems Ooba got some updates talking about Minotaur 8K? But I can''t
          get either the GPTQ or GGML versions of this model to run in Ooba even after
          update? I can get up to 65B models to run so it''s not a VRAM or RAM issue?
          Any ideas? Or is this model still not supported? Or special setup steps? '
        updatedAt: '2023-06-22T06:49:06.891Z'
      numEdits: 2
      reactions: []
    id: 6493eeb4e0db08db81a049d2
    type: comment
  author: Goldenblood56
  content: 'It seems Ooba got some updates talking about Minotaur 8K? But I can''t
    get either the GPTQ or GGML versions of this model to run in Ooba even after update?
    I can get up to 65B models to run so it''s not a VRAM or RAM issue? Any ideas?
    Or is this model still not supported? Or special setup steps? '
  created_at: 2023-06-22 05:48:20+00:00
  edited: true
  hidden: false
  id: 6493eeb4e0db08db81a049d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T08:21:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8709067106246948
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Minotaur is a non-Llama model.  text-generation-webui doesn''t currently
          support non-Llama models in GGML format.  So this model just won''t work
          in text-generation-webui.</p>

          <p>The updates you''re describing would apply to the GPTQ version of Minotaur,
          which text-generation-webui does support. It supports all model types in
          GPTQ format, but only Llama models in GGML.</p>

          '
        raw: 'Minotaur is a non-Llama model.  text-generation-webui doesn''t currently
          support non-Llama models in GGML format.  So this model just won''t work
          in text-generation-webui.


          The updates you''re describing would apply to the GPTQ version of Minotaur,
          which text-generation-webui does support. It supports all model types in
          GPTQ format, but only Llama models in GGML.'
        updatedAt: '2023-06-22T08:21:08.107Z'
      numEdits: 0
      reactions: []
    id: 64940474882cb123bf4addaf
    type: comment
  author: TheBloke
  content: 'Minotaur is a non-Llama model.  text-generation-webui doesn''t currently
    support non-Llama models in GGML format.  So this model just won''t work in text-generation-webui.


    The updates you''re describing would apply to the GPTQ version of Minotaur, which
    text-generation-webui does support. It supports all model types in GPTQ format,
    but only Llama models in GGML.'
  created_at: 2023-06-22 07:21:08+00:00
  edited: false
  hidden: false
  id: 64940474882cb123bf4addaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-22T08:27:02.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9791311025619507
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks yes I tried getting it working in GPTQ and got weird errors.
          Like I downloaded it in both GPTQ and GGML. Thanks I will check into it
          more later. Do you think it''s likely or possible for Ooba to get GGML support
          for models like this? Or do you really have no opinion or idea? Thanks again.
          </p>

          '
        raw: 'Thanks yes I tried getting it working in GPTQ and got weird errors.
          Like I downloaded it in both GPTQ and GGML. Thanks I will check into it
          more later. Do you think it''s likely or possible for Ooba to get GGML support
          for models like this? Or do you really have no opinion or idea? Thanks again. '
        updatedAt: '2023-06-22T08:27:24.796Z'
      numEdits: 1
      reactions: []
    id: 649405d6f453c3a7a285cf66
    type: comment
  author: Goldenblood56
  content: 'Thanks yes I tried getting it working in GPTQ and got weird errors. Like
    I downloaded it in both GPTQ and GGML. Thanks I will check into it more later.
    Do you think it''s likely or possible for Ooba to get GGML support for models
    like this? Or do you really have no opinion or idea? Thanks again. '
  created_at: 2023-06-22 07:27:02+00:00
  edited: true
  hidden: false
  id: 649405d6f453c3a7a285cf66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T12:31:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778257608413696
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s certainly possible for text-generation-webui to support it.
          He would need to add support for another backend, one that supports all
          GGMLs. ctransformers would be a good choice for that:  <a rel="nofollow"
          href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a></p>

          <p>If someone did a PR to implement that he''d probably include it. But
          I don''t have time to do that myself at the moment.</p>

          '
        raw: 'It''s certainly possible for text-generation-webui to support it. He
          would need to add support for another backend, one that supports all GGMLs.
          ctransformers would be a good choice for that:  https://github.com/marella/ctransformers


          If someone did a PR to implement that he''d probably include it. But I don''t
          have time to do that myself at the moment.'
        updatedAt: '2023-06-22T12:31:06.629Z'
      numEdits: 0
      reactions: []
    id: 64943f0a4ee6eaa466158a95
    type: comment
  author: TheBloke
  content: 'It''s certainly possible for text-generation-webui to support it. He would
    need to add support for another backend, one that supports all GGMLs. ctransformers
    would be a good choice for that:  https://github.com/marella/ctransformers


    If someone did a PR to implement that he''d probably include it. But I don''t
    have time to do that myself at the moment.'
  created_at: 2023-06-22 11:31:06+00:00
  edited: false
  hidden: false
  id: 64943f0a4ee6eaa466158a95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-06-22T12:39:55.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861971735954285
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>That''s all the questions I have and I would never ask you to go
          submit code to Ooba when your as busy as you are. If you do it one day because
          you want to so be it. But I imagine or at least hope that someone will get
          around to it. lol Thanks for answering my questions TheBloke. Your putting
          out amazing work quantizing all these models that other people are making.
          I don''t know if that is the correct term. Nor do I want to take up any
          more of your time so just take this as a thanks you don''t need to reply.
          </p>

          '
        raw: 'That''s all the questions I have and I would never ask you to go submit
          code to Ooba when your as busy as you are. If you do it one day because
          you want to so be it. But I imagine or at least hope that someone will get
          around to it. lol Thanks for answering my questions TheBloke. Your putting
          out amazing work quantizing all these models that other people are making.
          I don''t know if that is the correct term. Nor do I want to take up any
          more of your time so just take this as a thanks you don''t need to reply. '
        updatedAt: '2023-06-22T12:39:55.330Z'
      numEdits: 0
      reactions: []
    id: 6494411be3cc730c77351f93
    type: comment
  author: Goldenblood56
  content: 'That''s all the questions I have and I would never ask you to go submit
    code to Ooba when your as busy as you are. If you do it one day because you want
    to so be it. But I imagine or at least hope that someone will get around to it.
    lol Thanks for answering my questions TheBloke. Your putting out amazing work
    quantizing all these models that other people are making. I don''t know if that
    is the correct term. Nor do I want to take up any more of your time so just take
    this as a thanks you don''t need to reply. '
  created_at: 2023-06-22 11:39:55+00:00
  edited: false
  hidden: false
  id: 6494411be3cc730c77351f93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T13:04:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9800845384597778
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yup that''s the correct term! You''re welcome, glad you''re finding
          them useful.</p>

          '
        raw: Yup that's the correct term! You're welcome, glad you're finding them
          useful.
        updatedAt: '2023-06-22T13:04:45.546Z'
      numEdits: 0
      reactions: []
    id: 649446edb14db30b0a9765b8
    type: comment
  author: TheBloke
  content: Yup that's the correct term! You're welcome, glad you're finding them useful.
  created_at: 2023-06-22 12:04:45+00:00
  edited: false
  hidden: false
  id: 649446edb14db30b0a9765b8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/minotaur-15B-GGML
repo_type: model
status: open
target_branch: null
title: 'Getting this to run in Ooba? Anyone know what settings I have to choose? '
