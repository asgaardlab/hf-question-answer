!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Apotrox
conflicting_files: null
created_at: 2023-12-21 19:17:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65ac62933452462d3ba89034bc554541.svg
      fullname: Emanon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Apotrox
      type: user
    createdAt: '2023-12-21T19:17:34.000Z'
    data:
      edited: false
      editors:
      - Apotrox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9358620643615723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65ac62933452462d3ba89034bc554541.svg
          fullname: Emanon
          isHf: false
          isPro: false
          name: Apotrox
          type: user
        html: '<p>Hello all,</p>

          <p>i am pretty new to the entire topic of neural networks and everything
          surrounding it, so pardon me if i made any rookie mistakes or ask stupid
          questions.</p>

          <p>After a lot of trouble with getting all dependencies for threestudio
          built and installed, i tried followed the instructions on the Model Card
          and tried with the "basic" command given here. My GPU (3080 10GB) immediately
          jumps to 100%, the model gets loaded and "Epoch 0" gets started, only to
          fail after a minute or two due to memory running out.</p>

          <p>Here''s the full error message: torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 10.00
          GiB of which 0 bytes is free. Of the allocated memory 21.67 GiB is allocated
          by PyTorch, and 526.52 MiB is reserved by PyTorch but unallocated. If reserved
          but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>A few questions here:</p>

          <ul>

          <li>How is it allocated roughly 22gigs even though my gpu only has 10. Does
          RAM count into that?</li>

          <li>Would changing that allocation fix the error, even though it''d slow
          down the generation? If so, where would the PyTorch configs be to change
          that?</li>

          <li>i saw that there is the keyword "train" in the command. Does it actually
          train the NN or does it make the NN produce 3d images/meshes/files from
          the 2d images provided? If it is only for training, would i have to use
          the ckpt like the Stable Diffusion one(or the safetensors) and put it in
          some kind of WebUI?</li>

          </ul>

          <p>I hope someone here can point me in the right directions :)</p>

          '
        raw: "Hello all,\r\n\r\ni am pretty new to the entire topic of neural networks\
          \ and everything surrounding it, so pardon me if i made any rookie mistakes\
          \ or ask stupid questions.\r\n\r\nAfter a lot of trouble with getting all\
          \ dependencies for threestudio built and installed, i tried followed the\
          \ instructions on the Model Card and tried with the \"basic\" command given\
          \ here. My GPU (3080 10GB) immediately jumps to 100%, the model gets loaded\
          \ and \"Epoch 0\" gets started, only to fail after a minute or two due to\
          \ memory running out.\r\n\r\nHere's the full error message: torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty\
          \ of 10.00 GiB of which 0 bytes is free. Of the allocated memory 21.67 GiB\
          \ is allocated by PyTorch, and 526.52 MiB is reserved by PyTorch but unallocated.\
          \ If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nA few questions here:\r\n- How is it allocated roughly 22gigs even\
          \ though my gpu only has 10. Does RAM count into that?\r\n- Would changing\
          \ that allocation fix the error, even though it'd slow down the generation?\
          \ If so, where would the PyTorch configs be to change that?\r\n- i saw that\
          \ there is the keyword \"train\" in the command. Does it actually train\
          \ the NN or does it make the NN produce 3d images/meshes/files from the\
          \ 2d images provided? If it is only for training, would i have to use the\
          \ ckpt like the Stable Diffusion one(or the safetensors) and put it in some\
          \ kind of WebUI?\r\n\r\nI hope someone here can point me in the right directions\
          \ :)"
        updatedAt: '2023-12-21T19:17:34.049Z'
      numEdits: 0
      reactions: []
    id: 65848f4e0292cbbde23d6ee7
    type: comment
  author: Apotrox
  content: "Hello all,\r\n\r\ni am pretty new to the entire topic of neural networks\
    \ and everything surrounding it, so pardon me if i made any rookie mistakes or\
    \ ask stupid questions.\r\n\r\nAfter a lot of trouble with getting all dependencies\
    \ for threestudio built and installed, i tried followed the instructions on the\
    \ Model Card and tried with the \"basic\" command given here. My GPU (3080 10GB)\
    \ immediately jumps to 100%, the model gets loaded and \"Epoch 0\" gets started,\
    \ only to fail after a minute or two due to memory running out.\r\n\r\nHere's\
    \ the full error message: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 192.00 MiB. GPU 0 has a total capacty of 10.00 GiB of which 0 bytes\
    \ is free. Of the allocated memory 21.67 GiB is allocated by PyTorch, and 526.52\
    \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory\
    \ is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nA few questions here:\r\
    \n- How is it allocated roughly 22gigs even though my gpu only has 10. Does RAM\
    \ count into that?\r\n- Would changing that allocation fix the error, even though\
    \ it'd slow down the generation? If so, where would the PyTorch configs be to\
    \ change that?\r\n- i saw that there is the keyword \"train\" in the command.\
    \ Does it actually train the NN or does it make the NN produce 3d images/meshes/files\
    \ from the 2d images provided? If it is only for training, would i have to use\
    \ the ckpt like the Stable Diffusion one(or the safetensors) and put it in some\
    \ kind of WebUI?\r\n\r\nI hope someone here can point me in the right directions\
    \ :)"
  created_at: 2023-12-21 19:17:34+00:00
  edited: false
  hidden: false
  id: 65848f4e0292cbbde23d6ee7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
      fullname: Valdes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramonzero343
      type: user
    createdAt: '2023-12-21T19:52:49.000Z'
    data:
      edited: false
      editors:
      - ramonzero343
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9631890654563904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
          fullname: Valdes
          isHf: false
          isPro: false
          name: ramonzero343
          type: user
        html: '<p>I will Give a better answer latter on , But I highly recommend that
          you Run it in a cloud, Docker or VM because some model''s I still struggling.
          Anyways I have all the  Errors that you can Imagine and never got the one
          you have. The Ram Doesn''t count that much since I have 4 GB Ram and it
          working properly. I will recommend until i get into your question that you
          tried to deploy it on HuggingFace.co. , Github or Docker. And you must use
          For Hugging Face.co. Streamlit. There is working  perfect but the many models
          I still getting complicated to deploy on PC local. Just check it out the
          model because some of the only can run Private since the doesn''t have Limitations;)
          Answer letter the rest. But just try what I told you and let me know. Thanks
          for you Accertive Question!</p>

          '
        raw: 'I will Give a better answer latter on , But I highly recommend that
          you Run it in a cloud, Docker or VM because some model''s I still struggling.
          Anyways I have all the  Errors that you can Imagine and never got the one
          you have. The Ram Doesn''t count that much since I have 4 GB Ram and it
          working properly. I will recommend until i get into your question that you
          tried to deploy it on HuggingFace.co. , Github or Docker. And you must use
          For Hugging Face.co. Streamlit. There is working  perfect but the many models
          I still getting complicated to deploy on PC local. Just check it out the
          model because some of the only can run Private since the doesn''t have Limitations;)
          Answer letter the rest. But just try what I told you and let me know. Thanks
          for you Accertive Question!

          '
        updatedAt: '2023-12-21T19:52:49.219Z'
      numEdits: 0
      reactions: []
    id: 65849791b0b0e7e38a184f49
    type: comment
  author: ramonzero343
  content: 'I will Give a better answer latter on , But I highly recommend that you
    Run it in a cloud, Docker or VM because some model''s I still struggling. Anyways
    I have all the  Errors that you can Imagine and never got the one you have. The
    Ram Doesn''t count that much since I have 4 GB Ram and it working properly. I
    will recommend until i get into your question that you tried to deploy it on HuggingFace.co.
    , Github or Docker. And you must use For Hugging Face.co. Streamlit. There is
    working  perfect but the many models I still getting complicated to deploy on
    PC local. Just check it out the model because some of the only can run Private
    since the doesn''t have Limitations;) Answer letter the rest. But just try what
    I told you and let me know. Thanks for you Accertive Question!

    '
  created_at: 2023-12-21 19:52:49+00:00
  edited: false
  hidden: false
  id: 65849791b0b0e7e38a184f49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
      fullname: Valdes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramonzero343
      type: user
    createdAt: '2023-12-21T19:53:53.000Z'
    data:
      edited: false
      editors:
      - ramonzero343
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8479277491569519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
          fullname: Valdes
          isHf: false
          isPro: false
          name: ramonzero343
          type: user
        html: '<p>Oww almost forget if you are in Virtual Machine just set 2024 GbRam
          and 128 for video</p>

          '
        raw: Oww almost forget if you are in Virtual Machine just set 2024 GbRam and
          128 for video
        updatedAt: '2023-12-21T19:53:53.179Z'
      numEdits: 0
      reactions: []
    id: 658497d1b48986be10bea08e
    type: comment
  author: ramonzero343
  content: Oww almost forget if you are in Virtual Machine just set 2024 GbRam and
    128 for video
  created_at: 2023-12-21 19:53:53+00:00
  edited: false
  hidden: false
  id: 658497d1b48986be10bea08e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
      fullname: Valdes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramonzero343
      type: user
    createdAt: '2023-12-21T19:56:00.000Z'
    data:
      edited: false
      editors:
      - ramonzero343
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797416925430298
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
          fullname: Valdes
          isHf: false
          isPro: false
          name: ramonzero343
          type: user
        html: '<p>Other Think is that: Go to the repo in Github and Fork it to you.
          Then try  to build and deploy</p>

          '
        raw: 'Other Think is that: Go to the repo in Github and Fork it to you. Then
          try  to build and deploy'
        updatedAt: '2023-12-21T19:56:00.999Z'
      numEdits: 0
      reactions: []
    id: 65849850bae04f62813022a1
    type: comment
  author: ramonzero343
  content: 'Other Think is that: Go to the repo in Github and Fork it to you. Then
    try  to build and deploy'
  created_at: 2023-12-21 19:56:00+00:00
  edited: false
  hidden: false
  id: 65849850bae04f62813022a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
      fullname: Valdes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramonzero343
      type: user
    createdAt: '2023-12-21T19:57:33.000Z'
    data:
      edited: false
      editors:
      - ramonzero343
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8797001838684082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8668b7dddd46bdc96fed7177976bcde1.svg
          fullname: Valdes
          isHf: false
          isPro: false
          name: ramonzero343
          type: user
        html: '<p>From you Repo in github you deploy in Hugging..... Running with
          streamlit.</p>

          '
        raw: From you Repo in github you deploy in Hugging..... Running with streamlit.
        updatedAt: '2023-12-21T19:57:33.646Z'
      numEdits: 0
      reactions: []
    id: 658498ad69342cd754c22c7a
    type: comment
  author: ramonzero343
  content: From you Repo in github you deploy in Hugging..... Running with streamlit.
  created_at: 2023-12-21 19:57:33+00:00
  edited: false
  hidden: false
  id: 658498ad69342cd754c22c7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65ac62933452462d3ba89034bc554541.svg
      fullname: Emanon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Apotrox
      type: user
    createdAt: '2023-12-22T17:08:31.000Z'
    data:
      edited: false
      editors:
      - Apotrox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9757174253463745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65ac62933452462d3ba89034bc554541.svg
          fullname: Emanon
          isHf: false
          isPro: false
          name: Apotrox
          type: user
        html: '<p>Thank you very much for your answer. It appears that deploying on
          huggingface or any other cloud services require payment, which is exactly
          what i''m trying NOT to do by running the NN locally. I will try and set
          it up all over again. Will keep this updated whenever i find something :)</p>

          '
        raw: Thank you very much for your answer. It appears that deploying on huggingface
          or any other cloud services require payment, which is exactly what i'm trying
          NOT to do by running the NN locally. I will try and set it up all over again.
          Will keep this updated whenever i find something :)
        updatedAt: '2023-12-22T17:08:31.032Z'
      numEdits: 0
      reactions: []
    id: 6585c28fe030ee881fa3b12a
    type: comment
  author: Apotrox
  content: Thank you very much for your answer. It appears that deploying on huggingface
    or any other cloud services require payment, which is exactly what i'm trying
    NOT to do by running the NN locally. I will try and set it up all over again.
    Will keep this updated whenever i find something :)
  created_at: 2023-12-22 17:08:31+00:00
  edited: false
  hidden: false
  id: 6585c28fe030ee881fa3b12a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c720d0e39536a7e49340052f464a80d.svg
      fullname: Chenxin Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XGGNet
      type: user
    createdAt: '2023-12-24T10:05:59.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/7c720d0e39536a7e49340052f464a80d.svg
          fullname: Chenxin Li
          isHf: false
          isPro: false
          name: XGGNet
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-24T10:32:05.339Z'
      numEdits: 0
      reactions: []
    id: 65880287595e25821886ff06
    type: comment
  author: XGGNet
  content: This comment has been hidden
  created_at: 2023-12-24 10:05:59+00:00
  edited: true
  hidden: true
  id: 65880287595e25821886ff06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a9aa2745dc8415c2ef2ce41c5034c8f.svg
      fullname: Emil Wandel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ewandel
      type: user
    createdAt: '2024-01-06T22:30:29.000Z'
    data:
      edited: true
      editors:
      - ewandel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7619848251342773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a9aa2745dc8415c2ef2ce41c5034c8f.svg
          fullname: Emil Wandel
          isHf: false
          isPro: true
          name: ewandel
          type: user
        html: '<p>For what it''s worth, I managed to get it running somehow with a
          4070ti (12 GB VRAM) in ubuntu 22.04 LTS with Cuda 11.8 following the tutorials
          and with the following change in  stable-zero123.yaml (copied to stable-zero123_custom.yaml):</p>

          <p>line 21 changed to: "batch_size: [3, 2, 1]"</p>

          <p>then running with prompt: python launch.py --config configs/stable-zero123_custom.yaml
          --train --gpu 0 system.cleanup_after_validation_step=true system.cleanup_after_test_step=true
          system.renderer.num_samples_per_ray=128 data.width=128 data.height=128</p>

          <p>It''s not beautiful, but it works and gives a recognizable output. </p>

          <p>I can then get an obj file with this call (adopt directories to created
          directories)<br>python launch.py --config ./outputs/zero123-sai/[64,\ 128,\
          256]_hamburger_rgba.png@2024/configs/parsed.yaml --export --gpu 0 resume=./outputs/zero123-sai/[64,\
          128,\ 256]_hamburger_rgba.png@2024/ckpts/last.ckpt system.exporter_type=mesh-exporter</p>

          <p>Works in Blender:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63d3ad82ff1384ce6c5ad545/hLU3zc-QKZ2YIrmhoNY4h.png"><img
          alt="Screenshot from 2024-01-07 00-12-06.png" src="https://cdn-uploads.huggingface.co/production/uploads/63d3ad82ff1384ce6c5ad545/hLU3zc-QKZ2YIrmhoNY4h.png"></a></p>

          <p>Same steps for custom image (adopt directories):<br>python launch.py
          --config configs/stable-zero123_custom.yaml --train --gpu 0 data.image_path=./load/images/dragon-37_rgba.png
          system.cleanup_after_validation_step=true system.cleanup_after_test_step=true
          system.renderer.num_samples_per_ray=128 data.width=128 data.height=128</p>

          <p>python launch.py --config ./outputs/zero123-sai/[64,\ 128,\ 256]_dragon-37_rgba.png@2024/configs/parsed.yaml
          --export --gpu 0 resume=./outputs/zero123-sai/[64,\ 128,\ 256]_dragon-37_rgba.png@2024/ckpts/last.ckpt
          system.exporter_type=mesh-exporter</p>

          <p>Please let us/me know if there is a better way.</p>

          '
        raw: "For what it's worth, I managed to get it running somehow with a 4070ti\
          \ (12 GB VRAM) in ubuntu 22.04 LTS with Cuda 11.8 following the tutorials\
          \ and with the following change in  stable-zero123.yaml (copied to stable-zero123_custom.yaml):\n\
          \nline 21 changed to: \"batch_size: [3, 2, 1]\"\n\nthen running with prompt:\
          \ python launch.py --config configs/stable-zero123_custom.yaml --train --gpu\
          \ 0 system.cleanup_after_validation_step=true system.cleanup_after_test_step=true\
          \ system.renderer.num_samples_per_ray=128 data.width=128 data.height=128\n\
          \nIt's not beautiful, but it works and gives a recognizable output. \n\n\
          I can then get an obj file with this call (adopt directories to created\
          \ directories)\npython launch.py --config ./outputs/zero123-sai/\\[64\\\
          ,\\ 128\\,\\ 256\\]_hamburger_rgba.png@2024/configs/parsed.yaml --export\
          \ --gpu 0 resume=./outputs/zero123-sai/\\[64\\,\\ 128\\,\\ 256\\]_hamburger_rgba.png@2024/ckpts/last.ckpt\
          \ system.exporter_type=mesh-exporter\n\nWorks in Blender:\n![Screenshot\
          \ from 2024-01-07 00-12-06.png](https://cdn-uploads.huggingface.co/production/uploads/63d3ad82ff1384ce6c5ad545/hLU3zc-QKZ2YIrmhoNY4h.png)\n\
          \nSame steps for custom image (adopt directories):\npython launch.py --config\
          \ configs/stable-zero123_custom.yaml --train --gpu 0 data.image_path=./load/images/dragon-37_rgba.png\
          \ system.cleanup_after_validation_step=true system.cleanup_after_test_step=true\
          \ system.renderer.num_samples_per_ray=128 data.width=128 data.height=128\n\
          \npython launch.py --config ./outputs/zero123-sai/\\[64\\,\\ 128\\,\\ 256\\\
          ]_dragon-37_rgba.png@2024/configs/parsed.yaml --export --gpu 0 resume=./outputs/zero123-sai/\\\
          [64\\,\\ 128\\,\\ 256\\]_dragon-37_rgba.png@2024/ckpts/last.ckpt system.exporter_type=mesh-exporter\n\
          \nPlease let us/me know if there is a better way.\n\n\n"
        updatedAt: '2024-01-06T23:12:38.637Z'
      numEdits: 3
      reactions: []
    id: 6599d485a2a70bb883c6866d
    type: comment
  author: ewandel
  content: "For what it's worth, I managed to get it running somehow with a 4070ti\
    \ (12 GB VRAM) in ubuntu 22.04 LTS with Cuda 11.8 following the tutorials and\
    \ with the following change in  stable-zero123.yaml (copied to stable-zero123_custom.yaml):\n\
    \nline 21 changed to: \"batch_size: [3, 2, 1]\"\n\nthen running with prompt: python\
    \ launch.py --config configs/stable-zero123_custom.yaml --train --gpu 0 system.cleanup_after_validation_step=true\
    \ system.cleanup_after_test_step=true system.renderer.num_samples_per_ray=128\
    \ data.width=128 data.height=128\n\nIt's not beautiful, but it works and gives\
    \ a recognizable output. \n\nI can then get an obj file with this call (adopt\
    \ directories to created directories)\npython launch.py --config ./outputs/zero123-sai/\\\
    [64\\,\\ 128\\,\\ 256\\]_hamburger_rgba.png@2024/configs/parsed.yaml --export\
    \ --gpu 0 resume=./outputs/zero123-sai/\\[64\\,\\ 128\\,\\ 256\\]_hamburger_rgba.png@2024/ckpts/last.ckpt\
    \ system.exporter_type=mesh-exporter\n\nWorks in Blender:\n![Screenshot from 2024-01-07\
    \ 00-12-06.png](https://cdn-uploads.huggingface.co/production/uploads/63d3ad82ff1384ce6c5ad545/hLU3zc-QKZ2YIrmhoNY4h.png)\n\
    \nSame steps for custom image (adopt directories):\npython launch.py --config\
    \ configs/stable-zero123_custom.yaml --train --gpu 0 data.image_path=./load/images/dragon-37_rgba.png\
    \ system.cleanup_after_validation_step=true system.cleanup_after_test_step=true\
    \ system.renderer.num_samples_per_ray=128 data.width=128 data.height=128\n\npython\
    \ launch.py --config ./outputs/zero123-sai/\\[64\\,\\ 128\\,\\ 256\\]_dragon-37_rgba.png@2024/configs/parsed.yaml\
    \ --export --gpu 0 resume=./outputs/zero123-sai/\\[64\\,\\ 128\\,\\ 256\\]_dragon-37_rgba.png@2024/ckpts/last.ckpt\
    \ system.exporter_type=mesh-exporter\n\nPlease let us/me know if there is a better\
    \ way.\n\n\n"
  created_at: 2024-01-06 22:30:29+00:00
  edited: true
  hidden: false
  id: 6599d485a2a70bb883c6866d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: stabilityai/stable-zero123
repo_type: model
status: open
target_branch: null
title: Cuda out of memory?
