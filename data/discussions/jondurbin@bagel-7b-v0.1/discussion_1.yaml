!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andysalerno
conflicting_files: null
created_at: 2023-12-15 05:26:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
      fullname: andy s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andysalerno
      type: user
    createdAt: '2023-12-15T05:26:12.000Z'
    data:
      edited: false
      editors:
      - andysalerno
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.920857846736908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
          fullname: andy s
          isHf: false
          isPro: false
          name: andysalerno
          type: user
        html: '<p>the model card says this:</p>

          <blockquote>

          <p>I don''t really understand the point of having special tokens for &lt;|im_start|&gt;
          and &lt;|im_end|&gt;, because in practice they just act as BOS and EOS tokens
          (but, please correct me if I''m wrong).</p>

          </blockquote>

          <p>I''m definitely no expert on this topic, but I have a thought to share.</p>

          <p>From the OpenChat paper, they say this:</p>

          <blockquote>

          <p>To differentiate speakers, we introduce a new &lt;|end of turn|&gt; special
          token at the end of each<br>utterance, following Zhou et al. (2023). The
          &lt;|end of turn|&gt; token functions similarly to the<br>EOS token for
          stopping generation while preventing confusion with the learned meaning
          of EOS<br>during pretraining. </p>

          </blockquote>

          <p><a rel="nofollow" href="https://arxiv.org/pdf/2309.11235.pdf">https://arxiv.org/pdf/2309.11235.pdf</a></p>

          <p>In my (non expert) opinion, it makes sense to use a dedicated token for
          the end of turn, different from EOS, if only because OpenChat and others
          do it (and OpenChat is a really, really great finetune). And if you just
          use standard ChatML, then it has the added benefit that any API, library,
          code, or caller that knows the standard ChatML format could simply consume
          the model without any changes.</p>

          '
        raw: "the model card says this:\r\n\r\n> I don't really understand the point\
          \ of having special tokens for <|im_start|> and <|im_end|>, because in practice\
          \ they just act as BOS and EOS tokens (but, please correct me if I'm wrong).\r\
          \n\r\nI'm definitely no expert on this topic, but I have a thought to share.\r\
          \n\r\nFrom the OpenChat paper, they say this:\r\n\r\n> To differentiate\
          \ speakers, we introduce a new <|end of turn|> special token at the end\
          \ of each\r\nutterance, following Zhou et al. (2023). The <|end of turn|>\
          \ token functions similarly to the\r\nEOS token for stopping generation\
          \ while preventing confusion with the learned meaning of EOS\r\nduring pretraining.\
          \ \r\n\r\nhttps://arxiv.org/pdf/2309.11235.pdf\r\n\r\nIn my (non expert)\
          \ opinion, it makes sense to use a dedicated token for the end of turn,\
          \ different from EOS, if only because OpenChat and others do it (and OpenChat\
          \ is a really, really great finetune). And if you just use standard ChatML,\
          \ then it has the added benefit that any API, library, code, or caller that\
          \ knows the standard ChatML format could simply consume the model without\
          \ any changes."
        updatedAt: '2023-12-15T05:26:12.808Z'
      numEdits: 0
      reactions: []
    id: 657be374e34a7de14b0ffac9
    type: comment
  author: andysalerno
  content: "the model card says this:\r\n\r\n> I don't really understand the point\
    \ of having special tokens for <|im_start|> and <|im_end|>, because in practice\
    \ they just act as BOS and EOS tokens (but, please correct me if I'm wrong).\r\
    \n\r\nI'm definitely no expert on this topic, but I have a thought to share.\r\
    \n\r\nFrom the OpenChat paper, they say this:\r\n\r\n> To differentiate speakers,\
    \ we introduce a new <|end of turn|> special token at the end of each\r\nutterance,\
    \ following Zhou et al. (2023). The <|end of turn|> token functions similarly\
    \ to the\r\nEOS token for stopping generation while preventing confusion with\
    \ the learned meaning of EOS\r\nduring pretraining. \r\n\r\nhttps://arxiv.org/pdf/2309.11235.pdf\r\
    \n\r\nIn my (non expert) opinion, it makes sense to use a dedicated token for\
    \ the end of turn, different from EOS, if only because OpenChat and others do\
    \ it (and OpenChat is a really, really great finetune). And if you just use standard\
    \ ChatML, then it has the added benefit that any API, library, code, or caller\
    \ that knows the standard ChatML format could simply consume the model without\
    \ any changes."
  created_at: 2023-12-15 05:26:12+00:00
  edited: false
  hidden: false
  id: 657be374e34a7de14b0ffac9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-12-15T08:24:45.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9036137461662292
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>OpenChat doesn''t use ChatML:<br><a href="https://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer_config.json#L51">https://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer_config.json#L51</a><br><a
          rel="nofollow" href="https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L542">https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L542</a></p>

          <p>Many other popular models do, e.g. OpenHermes, Dolphin, etc., but they
          are just changing the stop tokens and adding new special tokens after BOS
          for some reason, which have the exact same purpose:<br><a rel="nofollow"
          href="https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L1067">https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L1067</a></p>

          <p>The OpenChat discusses performance differences between only having a
          turn differentiator once vs after each role, but doesn''t test the difference
          between re-using the existing special tokens - I suspect the performance
          would be identical.  They say "... prevent confusion with the learned meaning
          of EOS..." but I don''t actually see evidence that there is confusion in
          this paper or some of the referenced papers, but perhaps I missed it.</p>

          '
        raw: 'OpenChat doesn''t use ChatML:

          https://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer_config.json#L51

          https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L542


          Many other popular models do, e.g. OpenHermes, Dolphin, etc., but they are
          just changing the stop tokens and adding new special tokens after BOS for
          some reason, which have the exact same purpose:

          https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L1067


          The OpenChat discusses performance differences between only having a turn
          differentiator once vs after each role, but doesn''t test the difference
          between re-using the existing special tokens - I suspect the performance
          would be identical.  They say "... prevent confusion with the learned meaning
          of EOS..." but I don''t actually see evidence that there is confusion in
          this paper or some of the referenced papers, but perhaps I missed it.'
        updatedAt: '2023-12-15T08:24:45.184Z'
      numEdits: 0
      reactions: []
    id: 657c0d4d8e7790a347f17cac
    type: comment
  author: jondurbin
  content: 'OpenChat doesn''t use ChatML:

    https://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer_config.json#L51

    https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L542


    Many other popular models do, e.g. OpenHermes, Dolphin, etc., but they are just
    changing the stop tokens and adding new special tokens after BOS for some reason,
    which have the exact same purpose:

    https://github.com/lm-sys/FastChat/blob/ec9a07ed22110e9686b51fd6ee9bf635b7ce54f8/fastchat/conversation.py#L1067


    The OpenChat discusses performance differences between only having a turn differentiator
    once vs after each role, but doesn''t test the difference between re-using the
    existing special tokens - I suspect the performance would be identical.  They
    say "... prevent confusion with the learned meaning of EOS..." but I don''t actually
    see evidence that there is confusion in this paper or some of the referenced papers,
    but perhaps I missed it.'
  created_at: 2023-12-15 08:24:45+00:00
  edited: false
  hidden: false
  id: 657c0d4d8e7790a347f17cac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/bagel-7b-v0.1
repo_type: model
status: open
target_branch: null
title: ChatML format
