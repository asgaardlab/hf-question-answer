!!python/object:huggingface_hub.community.DiscussionWithDetails
author: warfaisal
conflicting_files: null
created_at: 2023-06-07 20:11:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5c4a925d7cfa263e118b4defc1c6840.svg
      fullname: Faisal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: warfaisal
      type: user
    createdAt: '2023-06-07T21:11:36.000Z'
    data:
      edited: false
      editors:
      - warfaisal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4708332419395447
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5c4a925d7cfa263e118b4defc1c6840.svg
          fullname: Faisal
          isHf: false
          isPro: false
          name: warfaisal
          type: user
        html: '<p>TypeError: forward() got an unexpected keyword argument ''token_type_ids''</p>

          <p>TypeError                                 Traceback (most recent call
          last)<br>Cell In[6], line 1<br>----&gt; 1 answer = qa_pipeline({"question":
          question, "context": context})<br>      2 print(answer)</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:390,
          in QuestionAnsweringPipeline.<strong>call</strong>(self, *args, **kwargs)<br>    388
          examples = self._args_parser(*args, **kwargs)<br>    389 if isinstance(examples,
          (list, tuple)) and len(examples) == 1:<br>--&gt; 390     return super().<strong>call</strong>(examples[0],
          **kwargs)<br>    391 return super().<strong>call</strong>(examples, **kwargs)</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1111,
          in Pipeline.<strong>call</strong>(self, inputs, num_workers, batch_size,
          *args, **kwargs)<br>   1109     return self.iterate(inputs, preprocess_params,
          forward_params, postprocess_params)<br>   1110 elif self.framework == "pt"
          and isinstance(self, ChunkPipeline):<br>-&gt; 1111     return next(<br>   1112         iter(<br>   1113             self.get_iterator(<br>   1114                 [inputs],
          num_workers, batch_size, preprocess_params, forward_params, postprocess_params<br>   1115             )<br>   1116         )<br>   1117     )<br>   1118
          else:<br>   1119     return self.run_single(inputs, preprocess_params, forward_params,
          postprocess_params)</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:124,
          in PipelineIterator.<strong>next</strong>(self)<br>    121     return self.loader_batch_item()<br>    123
          # We''re out of items within a batch<br>--&gt; 124 item = next(self.iterator)<br>    125
          processed = self.infer(item, **self.params)<br>    126 # We now have a batch
          of "inferred things".</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:266,
          in PipelinePackIterator.<strong>next</strong>(self)<br>    263             return
          accumulator<br>    265 while not is_last:<br>--&gt; 266     processed =
          self.infer(next(self.iterator), **self.params)<br>    267     if self.loader_batch_size
          is not None:<br>    268         if isinstance(processed, torch.Tensor):</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1025,
          in Pipeline.forward(self, model_inputs, **forward_params)<br>   1023     with
          inference_context():<br>   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,
          device=self.device)<br>-&gt; 1025         model_outputs = self._forward(model_inputs,
          **forward_params)<br>   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,
          device=torch.device("cpu"))<br>   1027 else:</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:513,
          in QuestionAnsweringPipeline._forward(self, inputs)<br>    511 example =
          inputs["example"]<br>    512 model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}<br>--&gt;
          513 output = self.model(**model_inputs)<br>    514 if isinstance(output,
          dict):<br>    515     return {"start": output["start_logits"], "end": output["end_logits"],
          "example": example, **inputs}</p>

          <p>File ~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>TypeError: forward() got an unexpected keyword argument ''token_type_ids''</p>

          '
        raw: "TypeError: forward() got an unexpected keyword argument 'token_type_ids'\r\
          \n\r\nTypeError                                 Traceback (most recent call\
          \ last)\r\nCell In[6], line 1\r\n----> 1 answer = qa_pipeline({\"question\"\
          : question, \"context\": context})\r\n      2 print(answer)\r\n\r\nFile\
          \ ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:390,\
          \ in QuestionAnsweringPipeline.__call__(self, *args, **kwargs)\r\n    388\
          \ examples = self._args_parser(*args, **kwargs)\r\n    389 if isinstance(examples,\
          \ (list, tuple)) and len(examples) == 1:\r\n--> 390     return super().__call__(examples[0],\
          \ **kwargs)\r\n    391 return super().__call__(examples, **kwargs)\r\n\r\
          \nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1111,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\r\
          \n   1109     return self.iterate(inputs, preprocess_params, forward_params,\
          \ postprocess_params)\r\n   1110 elif self.framework == \"pt\" and isinstance(self,\
          \ ChunkPipeline):\r\n-> 1111     return next(\r\n   1112         iter(\r\
          \n   1113             self.get_iterator(\r\n   1114                 [inputs],\
          \ num_workers, batch_size, preprocess_params, forward_params, postprocess_params\r\
          \n   1115             )\r\n   1116         )\r\n   1117     )\r\n   1118\
          \ else:\r\n   1119     return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:124,\
          \ in PipelineIterator.__next__(self)\r\n    121     return self.loader_batch_item()\r\
          \n    123 # We're out of items within a batch\r\n--> 124 item = next(self.iterator)\r\
          \n    125 processed = self.infer(item, **self.params)\r\n    126 # We now\
          \ have a batch of \"inferred things\".\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:266,\
          \ in PipelinePackIterator.__next__(self)\r\n    263             return accumulator\r\
          \n    265 while not is_last:\r\n--> 266     processed = self.infer(next(self.iterator),\
          \ **self.params)\r\n    267     if self.loader_batch_size is not None:\r\
          \n    268         if isinstance(processed, torch.Tensor):\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1025,\
          \ in Pipeline.forward(self, model_inputs, **forward_params)\r\n   1023 \
          \    with inference_context():\r\n   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=self.device)\r\n-> 1025         model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,\
          \ device=torch.device(\"cpu\"))\r\n   1027 else:\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:513,\
          \ in QuestionAnsweringPipeline._forward(self, inputs)\r\n    511 example\
          \ = inputs[\"example\"]\r\n    512 model_inputs = {k: inputs[k] for k in\
          \ self.tokenizer.model_input_names}\r\n--> 513 output = self.model(**model_inputs)\r\
          \n    514 if isinstance(output, dict):\r\n    515     return {\"start\"\
          : output[\"start_logits\"], \"end\": output[\"end_logits\"], \"example\"\
          : example, **inputs}\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\r\n   1497 # this\
          \ function, and just call forward.\r\n   1498 if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\
          \n   1499         or _global_backward_pre_hooks or _global_backward_hooks\r\
          \n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call\
          \ functions when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nTypeError: forward() got an unexpected keyword argument\
          \ 'token_type_ids'"
        updatedAt: '2023-06-07T21:11:36.503Z'
      numEdits: 0
      reactions: []
    id: 6480f288de559d48dbb3b21c
    type: comment
  author: warfaisal
  content: "TypeError: forward() got an unexpected keyword argument 'token_type_ids'\r\
    \n\r\nTypeError                                 Traceback (most recent call last)\r\
    \nCell In[6], line 1\r\n----> 1 answer = qa_pipeline({\"question\": question,\
    \ \"context\": context})\r\n      2 print(answer)\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:390,\
    \ in QuestionAnsweringPipeline.__call__(self, *args, **kwargs)\r\n    388 examples\
    \ = self._args_parser(*args, **kwargs)\r\n    389 if isinstance(examples, (list,\
    \ tuple)) and len(examples) == 1:\r\n--> 390     return super().__call__(examples[0],\
    \ **kwargs)\r\n    391 return super().__call__(examples, **kwargs)\r\n\r\nFile\
    \ ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1111,\
    \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\r\
    \n   1109     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)\r\
    \n   1110 elif self.framework == \"pt\" and isinstance(self, ChunkPipeline):\r\
    \n-> 1111     return next(\r\n   1112         iter(\r\n   1113             self.get_iterator(\r\
    \n   1114                 [inputs], num_workers, batch_size, preprocess_params,\
    \ forward_params, postprocess_params\r\n   1115             )\r\n   1116     \
    \    )\r\n   1117     )\r\n   1118 else:\r\n   1119     return self.run_single(inputs,\
    \ preprocess_params, forward_params, postprocess_params)\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:124,\
    \ in PipelineIterator.__next__(self)\r\n    121     return self.loader_batch_item()\r\
    \n    123 # We're out of items within a batch\r\n--> 124 item = next(self.iterator)\r\
    \n    125 processed = self.infer(item, **self.params)\r\n    126 # We now have\
    \ a batch of \"inferred things\".\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:266,\
    \ in PipelinePackIterator.__next__(self)\r\n    263             return accumulator\r\
    \n    265 while not is_last:\r\n--> 266     processed = self.infer(next(self.iterator),\
    \ **self.params)\r\n    267     if self.loader_batch_size is not None:\r\n   \
    \ 268         if isinstance(processed, torch.Tensor):\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1025,\
    \ in Pipeline.forward(self, model_inputs, **forward_params)\r\n   1023     with\
    \ inference_context():\r\n   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,\
    \ device=self.device)\r\n-> 1025         model_outputs = self._forward(model_inputs,\
    \ **forward_params)\r\n   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,\
    \ device=torch.device(\"cpu\"))\r\n   1027 else:\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/question_answering.py:513,\
    \ in QuestionAnsweringPipeline._forward(self, inputs)\r\n    511 example = inputs[\"\
    example\"]\r\n    512 model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}\r\
    \n--> 513 output = self.model(**model_inputs)\r\n    514 if isinstance(output,\
    \ dict):\r\n    515     return {\"start\": output[\"start_logits\"], \"end\":\
    \ output[\"end_logits\"], \"example\": example, **inputs}\r\n\r\nFile ~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1497 # this function,\
    \ and just call forward.\r\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\r\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\
    \n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call functions\
    \ when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\r\n\r\nTypeError: forward() got an unexpected keyword argument 'token_type_ids'"
  created_at: 2023-06-07 20:11:36+00:00
  edited: false
  hidden: false
  id: 6480f288de559d48dbb3b21c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6479e9c19763c4ef86afb11e/VDSVND_9kI4yr45jeQZfR.jpeg?w=200&h=200&f=face
      fullname: Smit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dries1
      type: user
    createdAt: '2023-06-09T15:18:44.000Z'
    data:
      edited: false
      editors:
      - Dries1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9838863611221313
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6479e9c19763c4ef86afb11e/VDSVND_9kI4yr45jeQZfR.jpeg?w=200&h=200&f=face
          fullname: Smit
          isHf: false
          isPro: false
          name: Dries1
          type: user
        html: '<p>I am having the same issue.</p>

          '
        raw: I am having the same issue.
        updatedAt: '2023-06-09T15:18:44.194Z'
      numEdits: 0
      reactions: []
    id: 648342d4350d03cc30f04bd7
    type: comment
  author: Dries1
  content: I am having the same issue.
  created_at: 2023-06-09 14:18:44+00:00
  edited: false
  hidden: false
  id: 648342d4350d03cc30f04bd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WCld3v-Ssh0hJ8ws8_nri.jpeg?w=200&h=200&f=face
      fullname: CobraMamba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CobraMamba
      type: user
    createdAt: '2023-06-20T06:11:44.000Z'
    data:
      edited: false
      editors:
      - CobraMamba
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.12484581023454666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WCld3v-Ssh0hJ8ws8_nri.jpeg?w=200&h=200&f=face
          fullname: CobraMamba
          isHf: false
          isPro: false
          name: CobraMamba
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Dries1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Dries1\">@<span class=\"\
          underline\">Dries1</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;warfaisal&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/warfaisal\">@<span class=\"underline\">warfaisal</span></a></span>\n\
          \n\t</span></span> model_inputs.pop(\"token_type_ids\").</p>\n"
        raw: '@Dries1 @warfaisal model_inputs.pop("token_type_ids").

          '
        updatedAt: '2023-06-20T06:11:44.688Z'
      numEdits: 0
      reactions: []
    id: 64914320ed034b1864dfb59c
    type: comment
  author: CobraMamba
  content: '@Dries1 @warfaisal model_inputs.pop("token_type_ids").

    '
  created_at: 2023-06-20 05:11:44+00:00
  edited: false
  hidden: false
  id: 64914320ed034b1864dfb59c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2430bcd9e0356d6a55c15e5af1b96a7b.svg
      fullname: Ekaterina Lait
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katyalait
      type: user
    createdAt: '2023-07-06T20:39:41.000Z'
    data:
      edited: false
      editors:
      - katyalait
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592940807342529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2430bcd9e0356d6a55c15e5af1b96a7b.svg
          fullname: Ekaterina Lait
          isHf: false
          isPro: false
          name: katyalait
          type: user
        html: '<p>This happens because their instructions are wrong, they say to use
          ''question-answering'' pipeline but the model is only built for text gen.</p>

          '
        raw: This happens because their instructions are wrong, they say to use 'question-answering'
          pipeline but the model is only built for text gen.
        updatedAt: '2023-07-06T20:39:41.470Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - riyavsinha
    id: 64a7268d8ef353d4f8cf6d48
    type: comment
  author: katyalait
  content: This happens because their instructions are wrong, they say to use 'question-answering'
    pipeline but the model is only built for text gen.
  created_at: 2023-07-06 19:39:41+00:00
  edited: false
  hidden: false
  id: 64a7268d8ef353d4f8cf6d48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5c4a925d7cfa263e118b4defc1c6840.svg
      fullname: Faisal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: warfaisal
      type: user
    createdAt: '2023-07-07T19:58:07.000Z'
    data:
      edited: false
      editors:
      - warfaisal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6464393734931946
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5c4a925d7cfa263e118b4defc1c6840.svg
          fullname: Faisal
          isHf: false
          isPro: false
          name: warfaisal
          type: user
        html: '<p>I resolved it by using the model via langchain hugging face local
          hub.<br>its really slow on the cpu though.</p>

          <p>from langchain import HuggingFacePipeline</p>

          <p>llm = HuggingFacePipeline.from_model_id(<br>    model_id="bigscience/bloom-1b7",<br>    task="text-generation",<br>    model_kwargs={"temperature":
          0, "max_length": 64},<br>)</p>

          '
        raw: "I resolved it by using the model via langchain hugging face local hub.\n\
          its really slow on the cpu though.\n\nfrom langchain import HuggingFacePipeline\n\
          \nllm = HuggingFacePipeline.from_model_id(\n    model_id=\"bigscience/bloom-1b7\"\
          ,\n    task=\"text-generation\",\n    model_kwargs={\"temperature\": 0,\
          \ \"max_length\": 64},\n)"
        updatedAt: '2023-07-07T19:58:07.624Z'
      numEdits: 0
      reactions: []
    id: 64a86e4fcc73827ce66a2739
    type: comment
  author: warfaisal
  content: "I resolved it by using the model via langchain hugging face local hub.\n\
    its really slow on the cpu though.\n\nfrom langchain import HuggingFacePipeline\n\
    \nllm = HuggingFacePipeline.from_model_id(\n    model_id=\"bigscience/bloom-1b7\"\
    ,\n    task=\"text-generation\",\n    model_kwargs={\"temperature\": 0, \"max_length\"\
    : 64},\n)"
  created_at: 2023-07-07 18:58:07+00:00
  edited: false
  hidden: false
  id: 64a86e4fcc73827ce66a2739
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc9645246434a631ee13183f54acca53.svg
      fullname: Riya Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: riyavsinha
      type: user
    createdAt: '2023-07-08T16:46:26.000Z'
    data:
      edited: false
      editors:
      - riyavsinha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8072261810302734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc9645246434a631ee13183f54acca53.svg
          fullname: Riya Sinha
          isHf: false
          isPro: false
          name: riyavsinha
          type: user
        html: '<p>Yes, using the <code>question-answering</code> gives errors, but
          text generation works:</p>

          <pre><code>pipeline("text-generation", model="medalpaca/medalpaca-7b", tokenizer="medalpaca/medalpaca-7b")

          </code></pre>

          <p>Can the model config be updated to actually enable question answering?
          I am trying to use this for that task, and that seems to be one of the advertised
          use cases.</p>

          '
        raw: 'Yes, using the `question-answering` gives errors, but text generation
          works:


          ```

          pipeline("text-generation", model="medalpaca/medalpaca-7b", tokenizer="medalpaca/medalpaca-7b")

          ```


          Can the model config be updated to actually enable question answering? I
          am trying to use this for that task, and that seems to be one of the advertised
          use cases.'
        updatedAt: '2023-07-08T16:46:26.925Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kbressem
    id: 64a992e26cadc7aca54d989a
    type: comment
  author: riyavsinha
  content: 'Yes, using the `question-answering` gives errors, but text generation
    works:


    ```

    pipeline("text-generation", model="medalpaca/medalpaca-7b", tokenizer="medalpaca/medalpaca-7b")

    ```


    Can the model config be updated to actually enable question answering? I am trying
    to use this for that task, and that seems to be one of the advertised use cases.'
  created_at: 2023-07-08 15:46:26+00:00
  edited: false
  hidden: false
  id: 64a992e26cadc7aca54d989a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: medalpaca/medalpaca-7b
repo_type: model
status: open
target_branch: null
title: 'does anybody have solution to this issue:TypeError: forward() got an unexpected
  keyword argument ''token_type_ids'''
