!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zubinbalsara
conflicting_files: null
created_at: 2023-08-08 01:26:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d4794cbf2714cbb2b56581fa7a74d54.svg
      fullname: Zubin B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zubinbalsara
      type: user
    createdAt: '2023-08-08T02:26:57.000Z'
    data:
      edited: false
      editors:
      - Zubinbalsara
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5663138031959534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d4794cbf2714cbb2b56581fa7a74d54.svg
          fullname: Zubin B
          isHf: false
          isPro: false
          name: Zubinbalsara
          type: user
        html: "<p>I tired to run the medalpaca -7b on sagemaker lab and getting the\
          \ following errors. Can someone please help me</p>\n<p>Could not load model\
          \ medalpaca/medalpaca-7b with any of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'&gt;).<br>\u200B\
          </p>\n<p>ValueError                                Traceback (most recent\
          \ call last)<br>/tmp/ipykernel_154/4213711743.py in &lt;cell line: 1&gt;()<br>----&gt;\
          \ 1 pl = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\"\
          , tokenizer=\"medalpaca/medalpaca-7b\")<br>      2 question = \"What are\
          \ the symptoms of diabetes?\"<br>      3 context = \"Diabetes is a metabolic\
          \ disease that causes high blood sugar. The symptoms include increased thirst,\
          \ frequent urination, and unexplained weight loss.\"<br>      4 answer =\
          \ pl(f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \")<br>\
          \      5 print(answer)</p>\n<p>~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/<strong>init</strong>.py\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)<br>    786\
          \     if isinstance(model, str) or framework is None:<br>    787       \
          \  model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
          pt\"]}<br>--&gt; 788         framework, model = infer_framework_load_model(<br>\
          \    789             model,<br>    790             model_classes=model_classes,</p>\n\
          <p>~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py\
          \ in infer_framework_load_model(model, config, model_classes, task, framework,\
          \ **model_kwargs)<br>    276<br>    277         if isinstance(model, str):<br>--&gt;\
          \ 278             raise ValueError(f\"Could not load model {model} with\
          \ any of the following classes: {class_tuple}.\")<br>    279<br>    280\
          \     if framework is None:</p>\n<p>ValueError: Could not load model medalpaca/medalpaca-7b\
          \ with any of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'&gt;).</p>\n"
        raw: "I tired to run the medalpaca -7b on sagemaker lab and getting the following\
          \ errors. Can someone please help me\r\n\r\nCould not load model medalpaca/medalpaca-7b\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>).\r\
          \n\u200B\r\n\r\nValueError                                Traceback (most\
          \ recent call last)\r\n/tmp/ipykernel_154/4213711743.py in <cell line: 1>()\r\
          \n----> 1 pl = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\"\
          , tokenizer=\"medalpaca/medalpaca-7b\")\r\n      2 question = \"What are\
          \ the symptoms of diabetes?\"\r\n      3 context = \"Diabetes is a metabolic\
          \ disease that causes high blood sugar. The symptoms include increased thirst,\
          \ frequent urination, and unexplained weight loss.\"\r\n      4 answer =\
          \ pl(f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \")\r\n\
          \      5 print(answer)\r\n\r\n~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/__init__.py\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    786\
          \     if isinstance(model, str) or framework is None:\r\n    787       \
          \  model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
          pt\"]}\r\n--> 788         framework, model = infer_framework_load_model(\r\
          \n    789             model,\r\n    790             model_classes=model_classes,\r\
          \n\r\n~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py\
          \ in infer_framework_load_model(model, config, model_classes, task, framework,\
          \ **model_kwargs)\r\n    276 \r\n    277         if isinstance(model, str):\r\
          \n--> 278             raise ValueError(f\"Could not load model {model} with\
          \ any of the following classes: {class_tuple}.\")\r\n    279 \r\n    280\
          \     if framework is None:\r\n\r\nValueError: Could not load model medalpaca/medalpaca-7b\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>)."
        updatedAt: '2023-08-08T02:26:57.875Z'
      numEdits: 0
      reactions: []
    id: 64d1a7f131c655ff8aa51929
    type: comment
  author: Zubinbalsara
  content: "I tired to run the medalpaca -7b on sagemaker lab and getting the following\
    \ errors. Can someone please help me\r\n\r\nCould not load model medalpaca/medalpaca-7b\
    \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>).\r\n\u200B\
    \r\n\r\nValueError                                Traceback (most recent call\
    \ last)\r\n/tmp/ipykernel_154/4213711743.py in <cell line: 1>()\r\n----> 1 pl\
    \ = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"\
    medalpaca/medalpaca-7b\")\r\n      2 question = \"What are the symptoms of diabetes?\"\
    \r\n      3 context = \"Diabetes is a metabolic disease that causes high blood\
    \ sugar. The symptoms include increased thirst, frequent urination, and unexplained\
    \ weight loss.\"\r\n      4 answer = pl(f\"Context: {context}\\n\\nQuestion: {question}\\\
    n\\nAnswer: \")\r\n      5 print(answer)\r\n\r\n~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/__init__.py\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
    \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    786     if\
    \ isinstance(model, str) or framework is None:\r\n    787         model_classes\
    \ = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\r\n--> 788\
    \         framework, model = infer_framework_load_model(\r\n    789          \
    \   model,\r\n    790             model_classes=model_classes,\r\n\r\n~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py\
    \ in infer_framework_load_model(model, config, model_classes, task, framework,\
    \ **model_kwargs)\r\n    276 \r\n    277         if isinstance(model, str):\r\n\
    --> 278             raise ValueError(f\"Could not load model {model} with any\
    \ of the following classes: {class_tuple}.\")\r\n    279 \r\n    280     if framework\
    \ is None:\r\n\r\nValueError: Could not load model medalpaca/medalpaca-7b with\
    \ any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>)."
  created_at: 2023-08-08 01:26:57+00:00
  edited: false
  hidden: false
  id: 64d1a7f131c655ff8aa51929
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: medalpaca/medalpaca-7b
repo_type: model
status: open
target_branch: null
title: Could not load model medalpaca/medalpaca-7b with any of the following classes
