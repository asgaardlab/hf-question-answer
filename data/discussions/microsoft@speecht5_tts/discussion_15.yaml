!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dcassine
conflicting_files: null
created_at: 2023-06-26 18:23:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/608bff887dbb6c15a44896f1c82d284a.svg
      fullname: diego cassinera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dcassine
      type: user
    createdAt: '2023-06-26T19:23:34.000Z'
    data:
      edited: false
      editors:
      - dcassine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611427187919617
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/608bff887dbb6c15a44896f1c82d284a.svg
          fullname: diego cassinera
          isHf: false
          isPro: false
          name: dcassine
          type: user
        html: '<p>I been for a while trying to finetune the model with no success
          till today.  I will comment here some of the issues i ran into.<br>The  VoxPopuli
          dataset is not compatible with Windows.   The audio samples file names are
          not valid in windows,  you can rename all the files and fix the spreadsheets
          accordingly.  </p>

          <p>In my case, I created a new dataset just like  VoxPopuli but with my
          own data.<br>In my copy of S"peechT5 TTS Fine-tuning.ipynb" I added "!pip
          install sentencepiece" to the second cell.</p>

          <p>This got the ball rolling and I was able to start the training.   After
          a couple of minutes training I got an  error tensor size mismatch.<br>I
          changed my training size and test size, and i got the same error at a different
          step and with different tensor sizes.   This lead me to think my issue was
          related the dataset.</p>

          <p>My first thought was that I had samples that were too long, so I changed
          the is_not_too_long function to filter out samples with more than 100 tokens.<br>Again
          similar error with different sizes.<br>I read somewhere the issue may be
          caused by samples with had no labels, but could not find any in my dataset.<br>Just
          in case I modified is_not_too_long function again, this time to filter out
          all samples with less than 25 and more 100 than tokens.<br>This final change
          gave me two surprises.</p>

          <ol>

          <li>I managed to finetune the model</li>

          <li>After all the filtering I went from 1500 or so samples to 135 for training
          and 15 for testing.  I''m amazed how well the results sound.</li>

          </ol>

          '
        raw: "I been for a while trying to finetune the model with no success till\
          \ today.  I will comment here some of the issues i ran into.\r\nThe  VoxPopuli\
          \ dataset is not compatible with Windows.   The audio samples file names\
          \ are not valid in windows,  you can rename all the files and fix the spreadsheets\
          \ accordingly.  \r\n\r\nIn my case, I created a new dataset just like  VoxPopuli\
          \ but with my own data.\r\nIn my copy of S\"peechT5 TTS Fine-tuning.ipynb\"\
          \ I added \"!pip install sentencepiece\" to the second cell.\r\n\r\nThis\
          \ got the ball rolling and I was able to start the training.   After a couple\
          \ of minutes training I got an  error tensor size mismatch.\r\nI changed\
          \ my training size and test size, and i got the same error at a different\
          \ step and with different tensor sizes.   This lead me to think my issue\
          \ was related the dataset.\r\n\r\nMy first thought was that I had samples\
          \ that were too long, so I changed the is_not_too_long function to filter\
          \ out samples with more than 100 tokens.\r\nAgain similar error with different\
          \ sizes.\r\nI read somewhere the issue may be caused by samples with had\
          \ no labels, but could not find any in my dataset.\r\nJust in case I modified\
          \ is_not_too_long function again, this time to filter out all samples with\
          \ less than 25 and more 100 than tokens.\r\nThis final change gave me two\
          \ surprises.\r\n1) I managed to finetune the model\r\n2) After all the filtering\
          \ I went from 1500 or so samples to 135 for training and 15 for testing.\
          \  I'm amazed how well the results sound. \r\n"
        updatedAt: '2023-06-26T19:23:34.086Z'
      numEdits: 0
      reactions: []
    id: 6499e5b65c74a2125c1670dd
    type: comment
  author: dcassine
  content: "I been for a while trying to finetune the model with no success till today.\
    \  I will comment here some of the issues i ran into.\r\nThe  VoxPopuli dataset\
    \ is not compatible with Windows.   The audio samples file names are not valid\
    \ in windows,  you can rename all the files and fix the spreadsheets accordingly.\
    \  \r\n\r\nIn my case, I created a new dataset just like  VoxPopuli but with my\
    \ own data.\r\nIn my copy of S\"peechT5 TTS Fine-tuning.ipynb\" I added \"!pip\
    \ install sentencepiece\" to the second cell.\r\n\r\nThis got the ball rolling\
    \ and I was able to start the training.   After a couple of minutes training I\
    \ got an  error tensor size mismatch.\r\nI changed my training size and test size,\
    \ and i got the same error at a different step and with different tensor sizes.\
    \   This lead me to think my issue was related the dataset.\r\n\r\nMy first thought\
    \ was that I had samples that were too long, so I changed the is_not_too_long\
    \ function to filter out samples with more than 100 tokens.\r\nAgain similar error\
    \ with different sizes.\r\nI read somewhere the issue may be caused by samples\
    \ with had no labels, but could not find any in my dataset.\r\nJust in case I\
    \ modified is_not_too_long function again, this time to filter out all samples\
    \ with less than 25 and more 100 than tokens.\r\nThis final change gave me two\
    \ surprises.\r\n1) I managed to finetune the model\r\n2) After all the filtering\
    \ I went from 1500 or so samples to 135 for training and 15 for testing.  I'm\
    \ amazed how well the results sound. \r\n"
  created_at: 2023-06-26 18:23:34+00:00
  edited: false
  hidden: false
  id: 6499e5b65c74a2125c1670dd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: microsoft/speecht5_tts
repo_type: model
status: open
target_branch: null
title: 'Fine Tunning the model '
