!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BlueNipples
conflicting_files: null
created_at: 2023-09-20 04:31:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
      fullname: Matthew Andrews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueNipples
      type: user
    createdAt: '2023-09-20T05:31:06.000Z'
    data:
      edited: false
      editors:
      - BlueNipples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658262729644775
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
          fullname: Matthew Andrews
          isHf: false
          isPro: false
          name: BlueNipples
          type: user
        html: '<p>I hope you don''t mind me asking a question. I am trying to use
          this adapter in koboldcpp and not really sure what I need to do there. I
          have converted the adapter to GGML (which I believe is a necessary step),
          now I think I need a f16 ggml base llama-2 chat model, to apply it to (which
          seems insane because it will use up so much ram). </p>

          <p>I have been unable to get oogabooga to install at all, the start.bat
          install script doesn''t do anything. I''ve seen that limarp does wonders
          for 7b models in particular, so would love to try out some combos but only
          have a 6600M mobile graphics card with 8gb vram (32 system tho). </p>

          <p>Should I try and merge the adapter instead? What advice could you offer
          me on trying to use this adapter on some 7b models? </p>

          '
        raw: "I hope you don't mind me asking a question. I am trying to use this\
          \ adapter in koboldcpp and not really sure what I need to do there. I have\
          \ converted the adapter to GGML (which I believe is a necessary step), now\
          \ I think I need a f16 ggml base llama-2 chat model, to apply it to (which\
          \ seems insane because it will use up so much ram). \r\n\r\nI have been\
          \ unable to get oogabooga to install at all, the start.bat install script\
          \ doesn't do anything. I've seen that limarp does wonders for 7b models\
          \ in particular, so would love to try out some combos but only have a 6600M\
          \ mobile graphics card with 8gb vram (32 system tho). \r\n\r\nShould I try\
          \ and merge the adapter instead? What advice could you offer me on trying\
          \ to use this adapter on some 7b models? "
        updatedAt: '2023-09-20T05:31:06.571Z'
      numEdits: 0
      reactions: []
    id: 650a839aed23af2c5d4537a5
    type: comment
  author: BlueNipples
  content: "I hope you don't mind me asking a question. I am trying to use this adapter\
    \ in koboldcpp and not really sure what I need to do there. I have converted the\
    \ adapter to GGML (which I believe is a necessary step), now I think I need a\
    \ f16 ggml base llama-2 chat model, to apply it to (which seems insane because\
    \ it will use up so much ram). \r\n\r\nI have been unable to get oogabooga to\
    \ install at all, the start.bat install script doesn't do anything. I've seen\
    \ that limarp does wonders for 7b models in particular, so would love to try out\
    \ some combos but only have a 6600M mobile graphics card with 8gb vram (32 system\
    \ tho). \r\n\r\nShould I try and merge the adapter instead? What advice could\
    \ you offer me on trying to use this adapter on some 7b models? "
  created_at: 2023-09-20 04:31:06+00:00
  edited: false
  hidden: false
  id: 650a839aed23af2c5d4537a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
      fullname: Suikamelon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lemonilia
      type: user
    createdAt: '2023-09-20T08:51:09.000Z'
    data:
      edited: true
      editors:
      - lemonilia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797714948654175
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
          fullname: Suikamelon
          isHf: false
          isPro: false
          name: lemonilia
          type: user
        html: '<p>I haven''t really played around with GGML (now GGUF) models so I
          don''t really know how to answer. The easiest option would be using pre-merged
          quantizations from TheBloke, but I think most of them use the initial release
          of LimaRP.</p>

          <p>I believe the best procedure would be merging the adapter to the base
          non-quantized model -&gt; converting the merged model to GGUF -&gt; quantizing
          the model, but I haven''t personally attempted this recently. </p>

          '
        raw: 'I haven''t really played around with GGML (now GGUF) models so I don''t
          really know how to answer. The easiest option would be using pre-merged
          quantizations from TheBloke, but I think most of them use the initial release
          of LimaRP.


          I believe the best procedure would be merging the adapter to the base non-quantized
          model -> converting the merged model to GGUF -> quantizing the model, but
          I haven''t personally attempted this recently. '
        updatedAt: '2023-09-20T08:51:36.300Z'
      numEdits: 1
      reactions: []
    id: 650ab27d19f6f953cb5be87b
    type: comment
  author: lemonilia
  content: 'I haven''t really played around with GGML (now GGUF) models so I don''t
    really know how to answer. The easiest option would be using pre-merged quantizations
    from TheBloke, but I think most of them use the initial release of LimaRP.


    I believe the best procedure would be merging the adapter to the base non-quantized
    model -> converting the merged model to GGUF -> quantizing the model, but I haven''t
    personally attempted this recently. '
  created_at: 2023-09-20 07:51:09+00:00
  edited: true
  hidden: false
  id: 650ab27d19f6f953cb5be87b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
      fullname: Matthew Andrews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueNipples
      type: user
    createdAt: '2023-09-20T12:35:58.000Z'
    data:
      edited: false
      editors:
      - BlueNipples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9721561670303345
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
          fullname: Matthew Andrews
          isHf: false
          isPro: false
          name: BlueNipples
          type: user
        html: '<p>Thanks for taking the time to respond! Yeah I have pretty basic
          hardware, nothing super fancy, so perhaps I''ll have to stick with other
          peoples merges. Maybe have another go at installing ooba at some point somehow.
          Thanks again!</p>

          '
        raw: Thanks for taking the time to respond! Yeah I have pretty basic hardware,
          nothing super fancy, so perhaps I'll have to stick with other peoples merges.
          Maybe have another go at installing ooba at some point somehow. Thanks again!
        updatedAt: '2023-09-20T12:35:58.562Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650ae72eb2a2e14eaef90560
    id: 650ae72eb2a2e14eaef9055f
    type: comment
  author: BlueNipples
  content: Thanks for taking the time to respond! Yeah I have pretty basic hardware,
    nothing super fancy, so perhaps I'll have to stick with other peoples merges.
    Maybe have another go at installing ooba at some point somehow. Thanks again!
  created_at: 2023-09-20 11:35:58+00:00
  edited: false
  hidden: false
  id: 650ae72eb2a2e14eaef9055f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
      fullname: Matthew Andrews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueNipples
      type: user
    createdAt: '2023-09-20T12:35:58.000Z'
    data:
      status: closed
    id: 650ae72eb2a2e14eaef90560
    type: status-change
  author: BlueNipples
  created_at: 2023-09-20 11:35:58+00:00
  id: 650ae72eb2a2e14eaef90560
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lemonilia/limarp-llama2-v2
repo_type: model
status: closed
target_branch: null
title: llamacpp/koboldcpp
