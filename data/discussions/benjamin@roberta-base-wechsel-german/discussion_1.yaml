!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philschmid
conflicting_files: null
created_at: 2022-11-18 09:00:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2022-11-18T09:00:59.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;benjamin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/benjamin\"\
          >@<span class=\"underline\">benjamin</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Awesome work on this! Super nice to see investments in languages other\
          \ than English. I checked out the paper and repository.<br>I couldn't find\
          \ more information about the pre-training you did after initializing the\
          \ embeddings with WECHSEL.<br>Could you share what training dataset you\
          \ used and the scripts? </p>\n"
        raw: "Hey @benjamin,\r\n\r\nAwesome work on this! Super nice to see investments\
          \ in languages other than English. I checked out the paper and repository.\
          \ \r\nI couldn't find more information about the pre-training you did after\
          \ initializing the embeddings with WECHSEL. \r\nCould you share what training\
          \ dataset you used and the scripts? "
        updatedAt: '2022-11-18T09:00:59.766Z'
      numEdits: 0
      reactions: []
    id: 637749cb7169ed9817039901
    type: comment
  author: philschmid
  content: "Hey @benjamin,\r\n\r\nAwesome work on this! Super nice to see investments\
    \ in languages other than English. I checked out the paper and repository. \r\n\
    I couldn't find more information about the pre-training you did after initializing\
    \ the embeddings with WECHSEL. \r\nCould you share what training dataset you used\
    \ and the scripts? "
  created_at: 2022-11-18 09:00:59+00:00
  edited: false
  hidden: false
  id: 637749cb7169ed9817039901
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
      fullname: Benjamin Minixhofer
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: benjamin
      type: user
    createdAt: '2022-11-18T10:10:59.000Z'
    data:
      edited: false
      editors:
      - benjamin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
          fullname: Benjamin Minixhofer
          isHf: false
          isPro: false
          name: benjamin
          type: user
        html: '<p>Hi, thanks for your interest!</p>

          <p>Sure, the training data was a subset of the first 4GB of the OSCAR corpus
          of each respective language. There are some more details here: <a rel="nofollow"
          href="https://github.com/CPJKU/wechsel/issues/4">https://github.com/CPJKU/wechsel/issues/4</a>.
          If I were to repeat training now, I''d recommend to use a larger subset
          and mC4 instead.</p>

          <p>Training script are just HF''s <code>run_clm.py</code> and <code>run_mlm.py</code>
          with hyperparameters as specified in the paper. There are also some training
          logs here: <a rel="nofollow" href="https://wandb.ai/llms-transfer-learning/main/runs/3300nggh">https://wandb.ai/llms-transfer-learning/main/runs/3300nggh</a>.
          Hope that helps!</p>

          '
        raw: 'Hi, thanks for your interest!


          Sure, the training data was a subset of the first 4GB of the OSCAR corpus
          of each respective language. There are some more details here: https://github.com/CPJKU/wechsel/issues/4.
          If I were to repeat training now, I''d recommend to use a larger subset
          and mC4 instead.


          Training script are just HF''s `run_clm.py` and `run_mlm.py` with hyperparameters
          as specified in the paper. There are also some training logs here: https://wandb.ai/llms-transfer-learning/main/runs/3300nggh.
          Hope that helps!'
        updatedAt: '2022-11-18T10:10:59.434Z'
      numEdits: 0
      reactions: []
    id: 63775a33fc2207aa11e22d47
    type: comment
  author: benjamin
  content: 'Hi, thanks for your interest!


    Sure, the training data was a subset of the first 4GB of the OSCAR corpus of each
    respective language. There are some more details here: https://github.com/CPJKU/wechsel/issues/4.
    If I were to repeat training now, I''d recommend to use a larger subset and mC4
    instead.


    Training script are just HF''s `run_clm.py` and `run_mlm.py` with hyperparameters
    as specified in the paper. There are also some training logs here: https://wandb.ai/llms-transfer-learning/main/runs/3300nggh.
    Hope that helps!'
  created_at: 2022-11-18 10:10:59+00:00
  edited: false
  hidden: false
  id: 63775a33fc2207aa11e22d47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2022-11-18T12:44:38.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: "<p>Thank you for sharing <span data-props=\"{&quot;user&quot;:&quot;benjamin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/benjamin\"\
          >@<span class=\"underline\">benjamin</span></a></span>\n\n\t</span></span>!\
          \ Super helpful! </p>\n<blockquote>\n<p>If I were to repeat training now,\
          \ I'd recommend to use a larger subset and mC4 instead.</p>\n</blockquote>\n\
          <p>Would you only use a bigger dataset with the same number of steps or\
          \ also more steps?<br>Have you run any experiments to see if performance\
          \ improves or decrease with a bigger dataset and \"less\" iteration over\
          \ that dataset?</p>\n"
        raw: "Thank you for sharing @benjamin! Super helpful! \n\n> If I were to repeat\
          \ training now, I'd recommend to use a larger subset and mC4 instead.\n\n\
          Would you only use a bigger dataset with the same number of steps or also\
          \ more steps? \nHave you run any experiments to see if performance improves\
          \ or decrease with a bigger dataset and \"less\" iteration over that dataset?"
        updatedAt: '2022-11-18T12:44:38.118Z'
      numEdits: 0
      reactions: []
    id: 63777e363a63a2983ffd69fd
    type: comment
  author: philschmid
  content: "Thank you for sharing @benjamin! Super helpful! \n\n> If I were to repeat\
    \ training now, I'd recommend to use a larger subset and mC4 instead.\n\nWould\
    \ you only use a bigger dataset with the same number of steps or also more steps?\
    \ \nHave you run any experiments to see if performance improves or decrease with\
    \ a bigger dataset and \"less\" iteration over that dataset?"
  created_at: 2022-11-18 12:44:38+00:00
  edited: false
  hidden: false
  id: 63777e363a63a2983ffd69fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
      fullname: Benjamin Minixhofer
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: benjamin
      type: user
    createdAt: '2022-11-18T13:05:18.000Z'
    data:
      edited: true
      editors:
      - benjamin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
          fullname: Benjamin Minixhofer
          isHf: false
          isPro: false
          name: benjamin
          type: user
        html: '<blockquote>

          <p>Would you only use a bigger dataset with the same number of steps or
          also more steps?</p>

          </blockquote>

          <p>For this model size, I believe the number of steps we used is sufficient.
          Although models trained from scratch at this size usually train longer (Table
          1 in the paper). Unfortunately we didn''t have a chance to check scaling
          laws because we didn''t have the compute. If you go larger, you should probably
          increase the number of steps, although maybe less than e.g. Chinchilla would
          suggest since there is some transfer from the source model.</p>

          <blockquote>

          <p>Have you run any experiments to see if performance improves or decrease
          with a bigger dataset and "less" iteration over that dataset?</p>

          </blockquote>

          <p>For encoder models like the German RoBERTa, it will probably not make
          a huge difference since the same training sample is never seen twice anyway
          due to the stochastic masking, except maybe for knowledge-intensive tasks
          like NER. The <a rel="nofollow" href="https://arxiv.org/pdf/1911.03894.pdf">CamemBERT
          paper</a> has some interesting insight on that in section 6.2.</p>

          <p>For decoder models, I think it makes more of a difference since there
          you do see the exact same sample more than once if you have &gt; 1 epoch.</p>

          <p>I don''t have any experiments to back this up though.</p>

          '
        raw: '> Would you only use a bigger dataset with the same number of steps
          or also more steps?


          For this model size, I believe the number of steps we used is sufficient.
          Although models trained from scratch at this size usually train longer (Table
          1 in the paper). Unfortunately we didn''t have a chance to check scaling
          laws because we didn''t have the compute. If you go larger, you should probably
          increase the number of steps, although maybe less than e.g. Chinchilla would
          suggest since there is some transfer from the source model.


          > Have you run any experiments to see if performance improves or decrease
          with a bigger dataset and "less" iteration over that dataset?


          For encoder models like the German RoBERTa, it will probably not make a
          huge difference since the same training sample is never seen twice anyway
          due to the stochastic masking, except maybe for knowledge-intensive tasks
          like NER. The [CamemBERT paper](https://arxiv.org/pdf/1911.03894.pdf) has
          some interesting insight on that in section 6.2.


          For decoder models, I think it makes more of a difference since there you
          do see the exact same sample more than once if you have > 1 epoch.


          I don''t have any experiments to back this up though.'
        updatedAt: '2022-11-18T13:06:30.581Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - philschmid
    id: 6377830e80c7d5df289b6ec9
    type: comment
  author: benjamin
  content: '> Would you only use a bigger dataset with the same number of steps or
    also more steps?


    For this model size, I believe the number of steps we used is sufficient. Although
    models trained from scratch at this size usually train longer (Table 1 in the
    paper). Unfortunately we didn''t have a chance to check scaling laws because we
    didn''t have the compute. If you go larger, you should probably increase the number
    of steps, although maybe less than e.g. Chinchilla would suggest since there is
    some transfer from the source model.


    > Have you run any experiments to see if performance improves or decrease with
    a bigger dataset and "less" iteration over that dataset?


    For encoder models like the German RoBERTa, it will probably not make a huge difference
    since the same training sample is never seen twice anyway due to the stochastic
    masking, except maybe for knowledge-intensive tasks like NER. The [CamemBERT paper](https://arxiv.org/pdf/1911.03894.pdf)
    has some interesting insight on that in section 6.2.


    For decoder models, I think it makes more of a difference since there you do see
    the exact same sample more than once if you have > 1 epoch.


    I don''t have any experiments to back this up though.'
  created_at: 2022-11-18 13:05:18+00:00
  edited: true
  hidden: false
  id: 6377830e80c7d5df289b6ec9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
      fullname: Benjamin Minixhofer
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: benjamin
      type: user
    createdAt: '2022-11-18T13:11:24.000Z'
    data:
      edited: false
      editors:
      - benjamin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
          fullname: Benjamin Minixhofer
          isHf: false
          isPro: false
          name: benjamin
          type: user
        html: '<p>I remembered that we actually do have somewhat related experiments
          in appendix F in <a rel="nofollow" href="https://aclanthology.org/2022.naacl-main.293.pdf">the
          paper</a>. That was mainly to remove confounding from different training
          dataset sizes on the performance improvement from WECHSEL, it does also
          give some insight in how dataset size affects performance though, at least
          for decoder models.</p>

          '
        raw: I remembered that we actually do have somewhat related experiments in
          appendix F in [the paper](https://aclanthology.org/2022.naacl-main.293.pdf).
          That was mainly to remove confounding from different training dataset sizes
          on the performance improvement from WECHSEL, it does also give some insight
          in how dataset size affects performance though, at least for decoder models.
        updatedAt: '2022-11-18T13:11:24.819Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - philschmid
    id: 6377847c06241efce1e844f2
    type: comment
  author: benjamin
  content: I remembered that we actually do have somewhat related experiments in appendix
    F in [the paper](https://aclanthology.org/2022.naacl-main.293.pdf). That was mainly
    to remove confounding from different training dataset sizes on the performance
    improvement from WECHSEL, it does also give some insight in how dataset size affects
    performance though, at least for decoder models.
  created_at: 2022-11-18 13:11:24+00:00
  edited: false
  hidden: false
  id: 6377847c06241efce1e844f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2022-11-18T14:19:12.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: "<p>Thank you for the comments! Super helpful! \U0001F917</p>\n"
        raw: "Thank you for the comments! Super helpful! \U0001F917"
        updatedAt: '2022-11-18T14:19:12.113Z'
      numEdits: 0
      reactions: []
    id: 637794607169ed981705b375
    type: comment
  author: philschmid
  content: "Thank you for the comments! Super helpful! \U0001F917"
  created_at: 2022-11-18 14:19:12+00:00
  edited: false
  hidden: false
  id: 637794607169ed981705b375
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
      fullname: Benjamin Minixhofer
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: benjamin
      type: user
    createdAt: '2022-11-19T16:42:56.000Z'
    data:
      edited: false
      editors:
      - benjamin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648291574509-5fbe9404dd6e953c5a521e13.jpeg?w=200&h=200&f=face
          fullname: Benjamin Minixhofer
          isHf: false
          isPro: false
          name: benjamin
          type: user
        html: '<p>You''re welcome! Just out of curiosity, are you planning to train
          anything specific with WECHSEL?</p>

          '
        raw: You're welcome! Just out of curiosity, are you planning to train anything
          specific with WECHSEL?
        updatedAt: '2022-11-19T16:42:56.086Z'
      numEdits: 0
      reactions: []
    id: 637907907df2fefdcaf0cbcd
    type: comment
  author: benjamin
  content: You're welcome! Just out of curiosity, are you planning to train anything
    specific with WECHSEL?
  created_at: 2022-11-19 16:42:56+00:00
  edited: false
  hidden: false
  id: 637907907df2fefdcaf0cbcd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: benjamin/roberta-base-wechsel-german
repo_type: model
status: open
target_branch: null
title: Pre-training script
