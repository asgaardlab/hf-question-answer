!!python/object:huggingface_hub.community.DiscussionWithDetails
author: macleginn
conflicting_files: null
created_at: 2023-07-05 07:15:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636650842701-noauth.jpeg?w=200&h=200&f=face
      fullname: Dmitry Nikolaev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macleginn
      type: user
    createdAt: '2023-07-05T08:15:03.000Z'
    data:
      edited: false
      editors:
      - macleginn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5337028503417969
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636650842701-noauth.jpeg?w=200&h=200&f=face
          fullname: Dmitry Nikolaev
          isHf: false
          isPro: false
          name: macleginn
          type: user
        html: "<p>When asked for hidden states, causal models usually provide embeddings\
          \ for all tokens in the input sentence. E.g., given the input \"one two\
          \ three\", GPT-2 will return a tensor of size [1, 3, 768] for each layer.\
          \ This model, surprisingly, returns tensors of size [1, <strong>4</strong>,\
          \  4096], and the extra embedding seems to correspond to the initial <code>&lt;s&gt;</code>\
          \ token. Its embeddings is therefore always the same:</p>\n<pre><code class=\"\
          language-ipython\">In [<span class=\"hljs-number\">4</span>]: tokenisation\
          \ = tok(<span class=\"hljs-string\">\"one two three\"</span>, return_tensors=<span\
          \ class=\"hljs-string\">'pt'</span>)\n\nIn [<span class=\"hljs-number\"\
          >5</span>]: outputs = model(**tokenisation, output_hidden_states=<span class=\"\
          hljs-literal\">True</span>).hidden_states\n\nIn [<span class=\"hljs-number\"\
          >6</span>]: <span class=\"hljs-built_in\">len</span>(outputs)\nOut[<span\
          \ class=\"hljs-number\">6</span>]: <span class=\"hljs-number\">33</span>\n\
          \nIn [<span class=\"hljs-number\">7</span>]: outputs[-<span class=\"hljs-number\"\
          >1</span>].size()\nOut[<span class=\"hljs-number\">7</span>]: torch.Size([<span\
          \ class=\"hljs-number\">1</span>, <span class=\"hljs-number\">4</span>,\
          \ <span class=\"hljs-number\">4096</span>])\n\nIn [<span class=\"hljs-number\"\
          >8</span>]: tok.tokenize(<span class=\"hljs-string\">\"one two three\"</span>)\n\
          Out[<span class=\"hljs-number\">8</span>]: [<span class=\"hljs-string\"\
          >'\u2581one'</span>, <span class=\"hljs-string\">'\u2581two'</span>, <span\
          \ class=\"hljs-string\">'\u2581three'</span>]\n\nIn [<span class=\"hljs-number\"\
          >9</span>]: tokenisation.input_ids[<span class=\"hljs-number\">0</span>]\n\
          Out[<span class=\"hljs-number\">9</span>]: tensor([   <span class=\"hljs-number\"\
          >1</span>,  <span class=\"hljs-number\">551</span>,  <span class=\"hljs-number\"\
          >753</span>, <span class=\"hljs-number\">1166</span>])\n\nIn [<span class=\"\
          hljs-number\">10</span>]: tok.decode(tokenisation.input_ids[<span class=\"\
          hljs-number\">0</span>])\nOut[<span class=\"hljs-number\">10</span>]: <span\
          \ class=\"hljs-string\">'&lt;s&gt;one two three'</span>\n\nIn [<span class=\"\
          hljs-number\">11</span>]: outputs[-<span class=\"hljs-number\">1</span>][<span\
          \ class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>]\n\
          Out[<span class=\"hljs-number\">11</span>]:\ntensor([ <span class=\"hljs-number\"\
          >0.0468</span>,  <span class=\"hljs-number\">0.2356</span>,  <span class=\"\
          hljs-number\">0.5536</span>,  ...,  <span class=\"hljs-number\">0.3180</span>,\
          \ -<span class=\"hljs-number\">0.2200</span>,  <span class=\"hljs-number\"\
          >0.5274</span>],\n       grad_fn=&lt;SelectBackward0&gt;)\n\nIn [<span class=\"\
          hljs-number\">12</span>]: tokenisation = tok(<span class=\"hljs-string\"\
          >\"five six seven\"</span>, return_tensors=<span class=\"hljs-string\">'pt'</span>)\n\
          \nIn [<span class=\"hljs-number\">13</span>]: outputs = model(**tokenisation,\
          \ output_hidden_states=<span class=\"hljs-literal\">True</span>).hidden_states\n\
          \nIn [<span class=\"hljs-number\">14</span>]: outputs[-<span class=\"hljs-number\"\
          >1</span>][<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\"\
          >0</span>]\nOut[<span class=\"hljs-number\">14</span>]:\ntensor([ <span\
          \ class=\"hljs-number\">0.0468</span>,  <span class=\"hljs-number\">0.2356</span>,\
          \  <span class=\"hljs-number\">0.5536</span>,  ...,  <span class=\"hljs-number\"\
          >0.3180</span>, -<span class=\"hljs-number\">0.2200</span>,  <span class=\"\
          hljs-number\">0.5274</span>],\n       grad_fn=&lt;SelectBackward0&gt;)\n\
          </code></pre>\n<p>Was this done by design or is it an API bug?</p>\n"
        raw: "When asked for hidden states, causal models usually provide embeddings\
          \ for all tokens in the input sentence. E.g., given the input \"one two\
          \ three\", GPT-2 will return a tensor of size [1, 3, 768] for each layer.\
          \ This model, surprisingly, returns tensors of size [1, **4**,  4096], and\
          \ the extra embedding seems to correspond to the initial `<s>` token. Its\
          \ embeddings is therefore always the same:\r\n\r\n```ipython\r\nIn [4]:\
          \ tokenisation = tok(\"one two three\", return_tensors='pt')\r\n\r\nIn [5]:\
          \ outputs = model(**tokenisation, output_hidden_states=True).hidden_states\r\
          \n\r\nIn [6]: len(outputs)\r\nOut[6]: 33\r\n\r\nIn [7]: outputs[-1].size()\r\
          \nOut[7]: torch.Size([1, 4, 4096])\r\n\r\nIn [8]: tok.tokenize(\"one two\
          \ three\")\r\nOut[8]: ['\u2581one', '\u2581two', '\u2581three']\r\n\r\n\
          In [9]: tokenisation.input_ids[0]\r\nOut[9]: tensor([   1,  551,  753, 1166])\r\
          \n\r\nIn [10]: tok.decode(tokenisation.input_ids[0])\r\nOut[10]: '<s>one\
          \ two three'\r\n\r\nIn [11]: outputs[-1][0, 0]\r\nOut[11]:\r\ntensor([ 0.0468,\
          \  0.2356,  0.5536,  ...,  0.3180, -0.2200,  0.5274],\r\n       grad_fn=<SelectBackward0>)\r\
          \n\r\nIn [12]: tokenisation = tok(\"five six seven\", return_tensors='pt')\r\
          \n\r\nIn [13]: outputs = model(**tokenisation, output_hidden_states=True).hidden_states\r\
          \n\r\nIn [14]: outputs[-1][0, 0]\r\nOut[14]:\r\ntensor([ 0.0468,  0.2356,\
          \  0.5536,  ...,  0.3180, -0.2200,  0.5274],\r\n       grad_fn=<SelectBackward0>)\r\
          \n```\r\n\r\nWas this done by design or is it an API bug?"
        updatedAt: '2023-07-05T08:15:03.307Z'
      numEdits: 0
      reactions: []
    id: 64a5268783df62427330aaaf
    type: comment
  author: macleginn
  content: "When asked for hidden states, causal models usually provide embeddings\
    \ for all tokens in the input sentence. E.g., given the input \"one two three\"\
    , GPT-2 will return a tensor of size [1, 3, 768] for each layer. This model, surprisingly,\
    \ returns tensors of size [1, **4**,  4096], and the extra embedding seems to\
    \ correspond to the initial `<s>` token. Its embeddings is therefore always the\
    \ same:\r\n\r\n```ipython\r\nIn [4]: tokenisation = tok(\"one two three\", return_tensors='pt')\r\
    \n\r\nIn [5]: outputs = model(**tokenisation, output_hidden_states=True).hidden_states\r\
    \n\r\nIn [6]: len(outputs)\r\nOut[6]: 33\r\n\r\nIn [7]: outputs[-1].size()\r\n\
    Out[7]: torch.Size([1, 4, 4096])\r\n\r\nIn [8]: tok.tokenize(\"one two three\"\
    )\r\nOut[8]: ['\u2581one', '\u2581two', '\u2581three']\r\n\r\nIn [9]: tokenisation.input_ids[0]\r\
    \nOut[9]: tensor([   1,  551,  753, 1166])\r\n\r\nIn [10]: tok.decode(tokenisation.input_ids[0])\r\
    \nOut[10]: '<s>one two three'\r\n\r\nIn [11]: outputs[-1][0, 0]\r\nOut[11]:\r\n\
    tensor([ 0.0468,  0.2356,  0.5536,  ...,  0.3180, -0.2200,  0.5274],\r\n     \
    \  grad_fn=<SelectBackward0>)\r\n\r\nIn [12]: tokenisation = tok(\"five six seven\"\
    , return_tensors='pt')\r\n\r\nIn [13]: outputs = model(**tokenisation, output_hidden_states=True).hidden_states\r\
    \n\r\nIn [14]: outputs[-1][0, 0]\r\nOut[14]:\r\ntensor([ 0.0468,  0.2356,  0.5536,\
    \  ...,  0.3180, -0.2200,  0.5274],\r\n       grad_fn=<SelectBackward0>)\r\n```\r\
    \n\r\nWas this done by design or is it an API bug?"
  created_at: 2023-07-05 07:15:03+00:00
  edited: false
  hidden: false
  id: 64a5268783df62427330aaaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677373824479-63036143fc783bfc743ecf50.jpeg?w=200&h=200&f=face
      fullname: Xinyang (Young) Geng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: young-geng
      type: user
    createdAt: '2023-07-10T04:48:49.000Z'
    data:
      edited: false
      editors:
      - young-geng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9799484014511108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677373824479-63036143fc783bfc743ecf50.jpeg?w=200&h=200&f=face
          fullname: Xinyang (Young) Geng
          isHf: false
          isPro: false
          name: young-geng
          type: user
        html: '<p>This is done by design. You can also turn off the BOS token during
          tokenization if you want.</p>

          '
        raw: This is done by design. You can also turn off the BOS token during tokenization
          if you want.
        updatedAt: '2023-07-10T04:48:49.990Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64ab8db2fb9cf6afdb78376f
    id: 64ab8db1fb9cf6afdb78376d
    type: comment
  author: young-geng
  content: This is done by design. You can also turn off the BOS token during tokenization
    if you want.
  created_at: 2023-07-10 03:48:49+00:00
  edited: false
  hidden: false
  id: 64ab8db1fb9cf6afdb78376d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677373824479-63036143fc783bfc743ecf50.jpeg?w=200&h=200&f=face
      fullname: Xinyang (Young) Geng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: young-geng
      type: user
    createdAt: '2023-07-10T04:48:50.000Z'
    data:
      status: closed
    id: 64ab8db2fb9cf6afdb78376f
    type: status-change
  author: young-geng
  created_at: 2023-07-10 03:48:50+00:00
  id: 64ab8db2fb9cf6afdb78376f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: openlm-research/open_llama_7b
repo_type: model
status: closed
target_branch: null
title: Why does the model output the embedding for the <s> token?
