!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jbochi
conflicting_files: []
created_at: 2023-10-04 20:09:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-10-04T21:09:47.000Z'
    data:
      edited: true
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: "<p>This PR is similar to <a href=\"https://huggingface.co/grammarly/coedit-large/discussions/4\"\
          >https://huggingface.co/grammarly/coedit-large/discussions/4</a>, which\
          \ was merged in coedit-large.</p>\n<p>This new file is equivalent to <code>pytorch_model.bin</code>\
          \ but safe in the sense that no arbitrary code can be put into it.</p>\n\
          <p>These files also happen to load much faster than their pytorch counterpart:<br><a\
          \ rel=\"nofollow\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb\"\
          >https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb</a></p>\n\
          <p>The model is too large for <a href=\"https://huggingface.co/spaces/safetensors/convert\"\
          >https://huggingface.co/spaces/safetensors/convert</a> , so I created the\
          \ file manually:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"grammarly/coedit-xxl\"</span>)\nmodel.save_pretrained(<span\
          \ class=\"hljs-string\">\"coedit-xxl/\"</span>, safe_serialization=<span\
          \ class=\"hljs-literal\">True</span>, max_shard_size=<span class=\"hljs-string\"\
          >\"100GB\"</span>)\n</code></pre>\n<p>To test the safetensors file was correct,\
          \ I ran this code:</p>\n<pre><code class=\"language-python\">model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=<span\
          \ class=\"hljs-string\">\"./coedit-xxl/\"</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"grammarly/coedit-xxl\"</span>)\ninput_text = <span\
          \ class=\"hljs-string\">'Fix grammatical errors in this sentence: When I\
          \ grow up, I start to understand what he said is quite right.'</span>\n\
          input_ids = tokenizer(input_text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).input_ids\noutputs = model.generate(input_ids, max_length=<span\
          \ class=\"hljs-number\">256</span>)\nedited_text = tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>)\n<span class=\"hljs-built_in\">print</span>(edited_text)\n\
          <span class=\"hljs-comment\"># When I grow up, I will start to understand\
          \ that what he said is quite right</span>\n</code></pre>\n<p>Finally, this\
          \ PR was created with this code:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"\
          hljs-keyword\">import</span> HfApi\napi = HfApi()\napi.upload_file(\n  path_or_fileobj=<span\
          \ class=\"hljs-string\">\"coedit-xxl/model.safetensors\"</span>,\n  path_in_repo=<span\
          \ class=\"hljs-string\">\"model.safetensors\"</span>,\n  repo_id=<span class=\"\
          hljs-string\">\"grammarly/coedit-xxl\"</span>,\n  create_pr=<span class=\"\
          hljs-literal\">True</span>)\n</code></pre>\n"
        raw: "This PR is similar to https://huggingface.co/grammarly/coedit-large/discussions/4,\
          \ which was merged in coedit-large.\n\nThis new file is equivalent to `pytorch_model.bin`\
          \ but safe in the sense that no arbitrary code can be put into it.\n\nThese\
          \ files also happen to load much faster than their pytorch counterpart:\n\
          https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb\n\
          \nThe model is too large for https://huggingface.co/spaces/safetensors/convert\
          \ , so I created the file manually:\n\n```python\nfrom transformers import\
          \ AutoTokenizer, T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          grammarly/coedit-xxl\")\nmodel.save_pretrained(\"coedit-xxl/\", safe_serialization=True,\
          \ max_shard_size=\"100GB\")\n```\n\nTo test the safetensors file was correct,\
          \ I ran this code:\n\n```python\nmodel = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=\"\
          ./coedit-xxl/\")\ntokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xxl\"\
          )\ninput_text = 'Fix grammatical errors in this sentence: When I grow up,\
          \ I start to understand what he said is quite right.'\ninput_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids,\
          \ max_length=256)\nedited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\
          print(edited_text)\n# When I grow up, I will start to understand that what\
          \ he said is quite right\n```\n\nFinally, this PR was created with this\
          \ code:\n\n```python\nfrom huggingface_hub import HfApi\napi = HfApi()\n\
          api.upload_file(\n  path_or_fileobj=\"coedit-xxl/model.safetensors\",\n\
          \  path_in_repo=\"model.safetensors\",\n  repo_id=\"grammarly/coedit-xxl\"\
          ,\n  create_pr=True)\n```"
        updatedAt: '2023-10-04T21:32:17.551Z'
      numEdits: 1
      reactions: []
    id: 651dd49b3a7bb5530359b27c
    type: comment
  author: jbochi
  content: "This PR is similar to https://huggingface.co/grammarly/coedit-large/discussions/4,\
    \ which was merged in coedit-large.\n\nThis new file is equivalent to `pytorch_model.bin`\
    \ but safe in the sense that no arbitrary code can be put into it.\n\nThese files\
    \ also happen to load much faster than their pytorch counterpart:\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb\n\
    \nThe model is too large for https://huggingface.co/spaces/safetensors/convert\
    \ , so I created the file manually:\n\n```python\nfrom transformers import AutoTokenizer,\
    \ T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    grammarly/coedit-xxl\")\nmodel.save_pretrained(\"coedit-xxl/\", safe_serialization=True,\
    \ max_shard_size=\"100GB\")\n```\n\nTo test the safetensors file was correct,\
    \ I ran this code:\n\n```python\nmodel = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=\"\
    ./coedit-xxl/\")\ntokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xxl\"\
    )\ninput_text = 'Fix grammatical errors in this sentence: When I grow up, I start\
    \ to understand what he said is quite right.'\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_length=256)\n\
    edited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(edited_text)\n\
    # When I grow up, I will start to understand that what he said is quite right\n\
    ```\n\nFinally, this PR was created with this code:\n\n```python\nfrom huggingface_hub\
    \ import HfApi\napi = HfApi()\napi.upload_file(\n  path_or_fileobj=\"coedit-xxl/model.safetensors\"\
    ,\n  path_in_repo=\"model.safetensors\",\n  repo_id=\"grammarly/coedit-xxl\",\n\
    \  create_pr=True)\n```"
  created_at: 2023-10-04 20:09:47+00:00
  edited: true
  hidden: false
  id: 651dd49b3a7bb5530359b27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-10-04T21:09:47.000Z'
    data:
      oid: 39eda090572ae0af53e0fcb9fcf7d927b4452d3e
      parents:
      - 44b2a96d9802ebd7fb71c89468b70b5408d07704
      subject: Upload model.safetensors with huggingface_hub
    id: 651dd49b0000000000000000
    type: commit
  author: jbochi
  created_at: 2023-10-04 20:09:47+00:00
  id: 651dd49b0000000000000000
  oid: 39eda090572ae0af53e0fcb9fcf7d927b4452d3e
  summary: Upload model.safetensors with huggingface_hub
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg
      fullname: Vipul Raheja
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: machineteacher
      type: user
    createdAt: '2023-10-11T00:30:05.000Z'
    data:
      edited: false
      editors:
      - machineteacher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6999276280403137
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg
          fullname: Vipul Raheja
          isHf: false
          isPro: false
          name: machineteacher
          type: user
        html: "<p>Thanks for the extensive documentation and for the fix, <span data-props=\"\
          {&quot;user&quot;:&quot;jbochi&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/jbochi\">@<span class=\"underline\">jbochi</span></a></span>\n\
          \n\t</span></span>! </p>\n"
        raw: 'Thanks for the extensive documentation and for the fix, @jbochi! '
        updatedAt: '2023-10-11T00:30:05.968Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jbochi
      relatedEventId: 6525ec8e8a21d8f14acc5690
    id: 6525ec8d8a21d8f14acc5689
    type: comment
  author: machineteacher
  content: 'Thanks for the extensive documentation and for the fix, @jbochi! '
  created_at: 2023-10-10 23:30:05+00:00
  edited: false
  hidden: false
  id: 6525ec8d8a21d8f14acc5689
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg
      fullname: Vipul Raheja
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: machineteacher
      type: user
    createdAt: '2023-10-11T00:30:06.000Z'
    data:
      status: merged
    id: 6525ec8e8a21d8f14acc5690
    type: status-change
  author: machineteacher
  created_at: 2023-10-10 23:30:06+00:00
  id: 6525ec8e8a21d8f14acc5690
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: c562637fdd418a4bdf46f0ad841d364599ee98a8
num: 3
repo_id: grammarly/coedit-xxl
repo_type: model
status: merged
target_branch: refs/heads/main
title: Upload model.safetensors with huggingface_hub
