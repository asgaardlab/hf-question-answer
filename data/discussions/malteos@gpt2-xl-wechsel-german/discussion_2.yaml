!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WANGYIWEI
conflicting_files: null
created_at: 2023-09-22 10:08:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
      fullname: WYW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WANGYIWEI
      type: user
    createdAt: '2023-09-22T11:08:54.000Z'
    data:
      edited: true
      editors:
      - WANGYIWEI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9556064009666443
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
          fullname: WYW
          isHf: false
          isPro: false
          name: WANGYIWEI
          type: user
        html: '<p>Dear Malte,</p>

          <p>thanks very much for your excellent work on adapting GPT2-XL to the German
          language.</p>

          <p>I have been trying to find a powerful enough pre-trained (fine-tuned)
          model but also not too heavy to use for my German Chatbot''s NLU algorithm.
          Your GPT2-XL-Wechsel-German has provided significant improvement to my project
          compared to other auto-encoding/ encoder-only models, which are adapted
          to German. On the private NLU intent classification dataset, I achieved
          99.5% F1 with your model ;)</p>

          <p>Now I am wondering if you are interested in replicating the fine-tuning
          process for a slightly larger model on the German dataset, but the model
          should not be as heavy as Llama2-7b, which imposes a lot of challenge to
          full-scale fine-tuning on my private customer-level GPU, an RTX 4090.</p>

          <p>I found a great base model, which is called <a href="https://huggingface.co/cerebras/btlm-3b-8k-base">BTLM-3B-8k-base</a>.
          It might not be as good as Llama2-7B, but compared to GPT2-XL from 2019,
          it will definitely be an upgrade.</p>

          <p>I am glad to hear your feedback and I will be interested to dive into
          further discussion regarding this :)</p>

          '
        raw: 'Dear Malte,


          thanks very much for your excellent work on adapting GPT2-XL to the German
          language.


          I have been trying to find a powerful enough pre-trained (fine-tuned) model
          but also not too heavy to use for my German Chatbot''s NLU algorithm. Your
          GPT2-XL-Wechsel-German has provided significant improvement to my project
          compared to other auto-encoding/ encoder-only models, which are adapted
          to German. On the private NLU intent classification dataset, I achieved
          99.5% F1 with your model ;)


          Now I am wondering if you are interested in replicating the fine-tuning
          process for a slightly larger model on the German dataset, but the model
          should not be as heavy as Llama2-7b, which imposes a lot of challenge to
          full-scale fine-tuning on my private customer-level GPU, an RTX 4090.


          I found a great base model, which is called [BTLM-3B-8k-base](https://huggingface.co/cerebras/btlm-3b-8k-base).
          It might not be as good as Llama2-7B, but compared to GPT2-XL from 2019,
          it will definitely be an upgrade.


          I am glad to hear your feedback and I will be interested to dive into further
          discussion regarding this :)'
        updatedAt: '2023-09-22T11:10:19.153Z'
      numEdits: 1
      reactions: []
    id: 650d75c6fb7a5108875d14c8
    type: comment
  author: WANGYIWEI
  content: 'Dear Malte,


    thanks very much for your excellent work on adapting GPT2-XL to the German language.


    I have been trying to find a powerful enough pre-trained (fine-tuned) model but
    also not too heavy to use for my German Chatbot''s NLU algorithm. Your GPT2-XL-Wechsel-German
    has provided significant improvement to my project compared to other auto-encoding/
    encoder-only models, which are adapted to German. On the private NLU intent classification
    dataset, I achieved 99.5% F1 with your model ;)


    Now I am wondering if you are interested in replicating the fine-tuning process
    for a slightly larger model on the German dataset, but the model should not be
    as heavy as Llama2-7b, which imposes a lot of challenge to full-scale fine-tuning
    on my private customer-level GPU, an RTX 4090.


    I found a great base model, which is called [BTLM-3B-8k-base](https://huggingface.co/cerebras/btlm-3b-8k-base).
    It might not be as good as Llama2-7B, but compared to GPT2-XL from 2019, it will
    definitely be an upgrade.


    I am glad to hear your feedback and I will be interested to dive into further
    discussion regarding this :)'
  created_at: 2023-09-22 10:08:54+00:00
  edited: true
  hidden: false
  id: 650d75c6fb7a5108875d14c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655902177104-5efda656ff69163f6f59e5d2.png?w=200&h=200&f=face
      fullname: malteos
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: malteos
      type: user
    createdAt: '2023-09-22T13:16:27.000Z'
    data:
      edited: false
      editors:
      - malteos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8938694596290588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655902177104-5efda656ff69163f6f59e5d2.png?w=200&h=200&f=face
          fullname: malteos
          isHf: false
          isPro: false
          name: malteos
          type: user
        html: '<p>There are already bigger models as the ones below (we will also
          release additional models in the near future):</p>

          <ul>

          <li><a href="https://huggingface.co/malteos/bloom-6b4-clp-german">https://huggingface.co/malteos/bloom-6b4-clp-german</a></li>

          <li><a href="https://huggingface.co/malteos/bloom-6b4-clp-german-oasst-v0.1">https://huggingface.co/malteos/bloom-6b4-clp-german-oasst-v0.1</a></li>

          </ul>

          '
        raw: 'There are already bigger models as the ones below (we will also release
          additional models in the near future):

          - https://huggingface.co/malteos/bloom-6b4-clp-german

          - https://huggingface.co/malteos/bloom-6b4-clp-german-oasst-v0.1'
        updatedAt: '2023-09-22T13:16:27.674Z'
      numEdits: 0
      reactions: []
    id: 650d93ab23adc4b7a99669e5
    type: comment
  author: malteos
  content: 'There are already bigger models as the ones below (we will also release
    additional models in the near future):

    - https://huggingface.co/malteos/bloom-6b4-clp-german

    - https://huggingface.co/malteos/bloom-6b4-clp-german-oasst-v0.1'
  created_at: 2023-09-22 12:16:27+00:00
  edited: false
  hidden: false
  id: 650d93ab23adc4b7a99669e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
      fullname: WYW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WANGYIWEI
      type: user
    createdAt: '2023-09-27T21:34:48.000Z'
    data:
      edited: false
      editors:
      - WANGYIWEI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.965960681438446
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
          fullname: WYW
          isHf: false
          isPro: false
          name: WANGYIWEI
          type: user
        html: '<p>Hey dear Malte,</p>

          <p>sorry for the late reply. I have actually tested the bloom-based German
          LLMs and compared the performance with the GPT-2-based German models.</p>

          <p>Actually, on the same scale, let''s determine it at 1.x B parameters,
          I have found the older model, GPT-2-XL has way much better generation ability
          in terms of topic consistency and text coherence. </p>

          <p>Also, I have discussed this with my supervisor, since he and his PhD
          students have done some work before using Bloom-175B as the base model for
          entity processing. From their feedback, the bloom family really doesn''t
          deliver a satisfactory level of performance.</p>

          <p>I also tested some other llama-based German models (7b). I did a small
          causal generation evaluation with the 6.4B bloom-based German model you
          provided, and the difference is still quite noticeable. However, due to
          computation resource limitations, I am not able to fine-tune these models
          on my private data, therefore, they are unfortunately not in my current
          plan.</p>

          <p>I am excited to hear that you will be releasing some new German models
          in the near future using CLP-Transfer/ Wechsel. I am looking forward to
          it :)) Also if it doesn''t hurt, some 3B models are also very nice options
          (BTLM-3B-8k-base), since most developers can run or even train it on custom-level
          GPUs. </p>

          <p>Best regards,<br>Yiwei Wang</p>

          '
        raw: "Hey dear Malte,\n\nsorry for the late reply. I have actually tested\
          \ the bloom-based German LLMs and compared the performance with the GPT-2-based\
          \ German models.\n\nActually, on the same scale, let's determine it at 1.x\
          \ B parameters, I have found the older model, GPT-2-XL has way much better\
          \ generation ability in terms of topic consistency and text coherence. \n\
          \nAlso, I have discussed this with my supervisor, since he and his PhD students\
          \ have done some work before using Bloom-175B as the base model for entity\
          \ processing. From their feedback, the bloom family really doesn't deliver\
          \ a satisfactory level of performance.\n\nI also tested some other llama-based\
          \ German models (7b). I did a small causal generation evaluation with the\
          \ 6.4B bloom-based German model you provided, and the difference is still\
          \ quite noticeable. However, due to computation resource limitations, I\
          \ am not able to fine-tune these models on my private data, therefore, they\
          \ are unfortunately not in my current plan.\n\nI am excited to hear that\
          \ you will be releasing some new German models in the near future using\
          \ CLP-Transfer/ Wechsel. I am looking forward to it :)) Also if it doesn't\
          \ hurt, some 3B models are also very nice options (BTLM-3B-8k-base), since\
          \ most developers can run or even train it on custom-level GPUs. \n\nBest\
          \ regards,\nYiwei Wang"
        updatedAt: '2023-09-27T21:34:48.586Z'
      numEdits: 0
      reactions: []
    id: 65149ff8c75a3d4c44eb85ea
    type: comment
  author: WANGYIWEI
  content: "Hey dear Malte,\n\nsorry for the late reply. I have actually tested the\
    \ bloom-based German LLMs and compared the performance with the GPT-2-based German\
    \ models.\n\nActually, on the same scale, let's determine it at 1.x B parameters,\
    \ I have found the older model, GPT-2-XL has way much better generation ability\
    \ in terms of topic consistency and text coherence. \n\nAlso, I have discussed\
    \ this with my supervisor, since he and his PhD students have done some work before\
    \ using Bloom-175B as the base model for entity processing. From their feedback,\
    \ the bloom family really doesn't deliver a satisfactory level of performance.\n\
    \nI also tested some other llama-based German models (7b). I did a small causal\
    \ generation evaluation with the 6.4B bloom-based German model you provided, and\
    \ the difference is still quite noticeable. However, due to computation resource\
    \ limitations, I am not able to fine-tune these models on my private data, therefore,\
    \ they are unfortunately not in my current plan.\n\nI am excited to hear that\
    \ you will be releasing some new German models in the near future using CLP-Transfer/\
    \ Wechsel. I am looking forward to it :)) Also if it doesn't hurt, some 3B models\
    \ are also very nice options (BTLM-3B-8k-base), since most developers can run\
    \ or even train it on custom-level GPUs. \n\nBest regards,\nYiwei Wang"
  created_at: 2023-09-27 20:34:48+00:00
  edited: false
  hidden: false
  id: 65149ff8c75a3d4c44eb85ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: malteos/gpt2-xl-wechsel-german
repo_type: model
status: open
target_branch: null
title: Scale up base model for German?
