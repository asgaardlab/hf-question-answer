!!python/object:huggingface_hub.community.DiscussionWithDetails
author: telehan
conflicting_files: null
created_at: 2024-01-15 14:10:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2871adc147d2ead81d57f55e6dc6584a.svg
      fullname: han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: telehan
      type: user
    createdAt: '2024-01-15T14:10:08.000Z'
    data:
      edited: false
      editors:
      - telehan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2541610300540924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2871adc147d2ead81d57f55e6dc6584a.svg
          fullname: han
          isHf: false
          isPro: false
          name: telehan
          type: user
        html: "<p>Log start<br>main: build = 1839 (5537d9d3)<br>main: built with Apple\
          \ clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0<br>main:\
          \ seed  = 1705327660<br>llama_model_loader: loaded meta data with 21 key-value\
          \ pairs and 453 tensors from phi-2-electrical-engineering.Q8_0.gguf (version\
          \ GGUF V3 (latest))<br>llama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.<br>llama_model_loader:\
          \ - kv   0:                       general.architecture str             \
          \ = phi2<br>llama_model_loader: - kv   1:                              \
          \ general.name str              = Phi2<br>llama_model_loader: - kv   2:\
          \                        phi2.context_length u32              = 2048<br>llama_model_loader:\
          \ - kv   3:                      phi2.embedding_length u32             \
          \ = 2560<br>llama_model_loader: - kv   4:                   phi2.feed_forward_length\
          \ u32              = 10240<br>llama_model_loader: - kv   5:            \
          \               phi2.block_count u32              = 32<br>llama_model_loader:\
          \ - kv   6:                  phi2.attention.head_count u32             \
          \ = 32<br>llama_model_loader: - kv   7:               phi2.attention.head_count_kv\
          \ u32              = 32<br>llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon\
          \ f32              = 0.000010<br>llama_model_loader: - kv   9:         \
          \         phi2.rope.dimension_count u32              = 32<br>llama_model_loader:\
          \ - kv  10:                          general.file_type u32             \
          \ = 7<br>llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token\
          \ bool             = false<br>llama_model_loader: - kv  12:            \
          \           tokenizer.ggml.model str              = gpt2<br>llama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr[str,51200]  \
          \ = [\"!\", \"\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...<br>llama_model_loader:\
          \ - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]  \
          \ = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...<br>llama_model_loader: - kv\
          \  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"\
          \u0120 t\", \"\u0120 a\", \"h e\", \"i n\", \"r e\",...<br>llama_model_loader:\
          \ - kv  16:                tokenizer.ggml.bos_token_id u32             \
          \ = 50256<br>llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
          \ u32              = 50256<br>llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
          \ u32              = 50256<br>llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 0<br>llama_model_loader: - kv  20:               general.quantization_version\
          \ u32              = 2<br>llama_model_loader: - type  f32:  259 tensors<br>llama_model_loader:\
          \ - type q8_0:  194 tensors<br>llm_load_vocab: mismatch in special tokens\
          \ definition ( 910/51200 vs 944/51200 ).<br>llm_load_print_meta: format\
          \           = GGUF V3 (latest)<br>llm_load_print_meta: arch            \
          \ = phi2<br>llm_load_print_meta: vocab type       = BPE<br>llm_load_print_meta:\
          \ n_vocab          = 51200<br>llm_load_print_meta: n_merges         = 50000<br>llm_load_print_meta:\
          \ n_ctx_train      = 2048<br>llm_load_print_meta: n_embd           = 2560<br>llm_load_print_meta:\
          \ n_head           = 32<br>llm_load_print_meta: n_head_kv        = 32<br>llm_load_print_meta:\
          \ n_layer          = 32<br>llm_load_print_meta: n_rot            = 32<br>llm_load_print_meta:\
          \ n_embd_head_k    = 80<br>llm_load_print_meta: n_embd_head_v    = 80<br>llm_load_print_meta:\
          \ n_gqa            = 1<br>llm_load_print_meta: n_embd_k_gqa     = 2560<br>llm_load_print_meta:\
          \ n_embd_v_gqa     = 2560<br>llm_load_print_meta: f_norm_eps       = 1.0e-05<br>llm_load_print_meta:\
          \ f_norm_rms_eps   = 0.0e+00<br>llm_load_print_meta: f_clamp_kqv      =\
          \ 0.0e+00<br>llm_load_print_meta: f_max_alibi_bias = 0.0e+00<br>llm_load_print_meta:\
          \ n_ff             = 10240<br>llm_load_print_meta: n_expert         = 0<br>llm_load_print_meta:\
          \ n_expert_used    = 0<br>llm_load_print_meta: rope scaling     = linear<br>llm_load_print_meta:\
          \ freq_base_train  = 10000.0<br>llm_load_print_meta: freq_scale_train =\
          \ 1<br>llm_load_print_meta: n_yarn_orig_ctx  = 2048<br>llm_load_print_meta:\
          \ rope_finetuned   = unknown<br>llm_load_print_meta: model type       =\
          \ 3B<br>llm_load_print_meta: model ftype      = Q8_0<br>llm_load_print_meta:\
          \ model params     = 2.78 B<br>llm_load_print_meta: model size       = 2.75\
          \ GiB (8.51 BPW)<br>llm_load_print_meta: general.name     = Phi2<br>llm_load_print_meta:\
          \ BOS token        = 50256 '&lt;|endoftext|&gt;'<br>llm_load_print_meta:\
          \ EOS token        = 50256 '&lt;|endoftext|&gt;'<br>llm_load_print_meta:\
          \ UNK token        = 50256 '&lt;|endoftext|&gt;'<br>llm_load_print_meta:\
          \ PAD token        = 0 '!'<br>llm_load_print_meta: LF token         = 128\
          \ '\xC4'<br>llm_load_tensors: ggml ctx size       =    0.17 MiB<br>error\
          \ loading model: create_tensor: tensor 'blk.0.attn_qkv.weight' not found<br>llama_load_model_from_file:\
          \ failed to load model<br>llama_init_from_gpt_params: error: failed to load\
          \ model 'phi-2-electrical-engineering.Q8_0.gguf'<br>main: error: unable\
          \ to load model</p>\n"
        raw: "Log start\r\nmain: build = 1839 (5537d9d3)\r\nmain: built with Apple\
          \ clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\r\
          \nmain: seed  = 1705327660\r\nllama_model_loader: loaded meta data with\
          \ 21 key-value pairs and 453 tensors from phi-2-electrical-engineering.Q8_0.gguf\
          \ (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\r\nllama_model_loader:\
          \ - kv   0:                       general.architecture str             \
          \ = phi2\r\nllama_model_loader: - kv   1:                              \
          \ general.name str              = Phi2\r\nllama_model_loader: - kv   2:\
          \                        phi2.context_length u32              = 2048\r\n\
          llama_model_loader: - kv   3:                      phi2.embedding_length\
          \ u32              = 2560\r\nllama_model_loader: - kv   4:             \
          \      phi2.feed_forward_length u32              = 10240\r\nllama_model_loader:\
          \ - kv   5:                           phi2.block_count u32             \
          \ = 32\r\nllama_model_loader: - kv   6:                  phi2.attention.head_count\
          \ u32              = 32\r\nllama_model_loader: - kv   7:               phi2.attention.head_count_kv\
          \ u32              = 32\r\nllama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon\
          \ f32              = 0.000010\r\nllama_model_loader: - kv   9:         \
          \         phi2.rope.dimension_count u32              = 32\r\nllama_model_loader:\
          \ - kv  10:                          general.file_type u32             \
          \ = 7\r\nllama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token\
          \ bool             = false\r\nllama_model_loader: - kv  12:            \
          \           tokenizer.ggml.model str              = gpt2\r\nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr[str,51200]  \
          \ = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader:\
          \ - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]  \
          \ = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv\
          \  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"\
          \u0120 t\", \"\u0120 a\", \"h e\", \"i n\", \"r e\",...\r\nllama_model_loader:\
          \ - kv  16:                tokenizer.ggml.bos_token_id u32             \
          \ = 50256\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
          \ u32              = 50256\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
          \ u32              = 50256\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 0\r\nllama_model_loader: - kv  20:               general.quantization_version\
          \ u32              = 2\r\nllama_model_loader: - type  f32:  259 tensors\r\
          \nllama_model_loader: - type q8_0:  194 tensors\r\nllm_load_vocab: mismatch\
          \ in special tokens definition ( 910/51200 vs 944/51200 ).\r\nllm_load_print_meta:\
          \ format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch     \
          \        = phi2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta:\
          \ n_vocab          = 51200\r\nllm_load_print_meta: n_merges         = 50000\r\
          \nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd\
          \           = 2560\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta:\
          \ n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\n\
          llm_load_print_meta: n_rot            = 32\r\nllm_load_print_meta: n_embd_head_k\
          \    = 80\r\nllm_load_print_meta: n_embd_head_v    = 80\r\nllm_load_print_meta:\
          \ n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2560\r\
          \nllm_load_print_meta: n_embd_v_gqa     = 2560\r\nllm_load_print_meta: f_norm_eps\
          \       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             =\
          \ 10240\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta:\
          \ n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\
          \nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta:\
          \ freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\
          \nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta:\
          \ model type       = 3B\r\nllm_load_print_meta: model ftype      = Q8_0\r\
          \nllm_load_print_meta: model params     = 2.78 B\r\nllm_load_print_meta:\
          \ model size       = 2.75 GiB (8.51 BPW)\r\nllm_load_print_meta: general.name\
          \     = Phi2\r\nllm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\r\
          \nllm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta:\
          \ UNK token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta: PAD token\
          \        = 0 '!'\r\nllm_load_print_meta: LF token         = 128 '\xC4'\r\
          \nllm_load_tensors: ggml ctx size       =    0.17 MiB\r\nerror loading model:\
          \ create_tensor: tensor 'blk.0.attn_qkv.weight' not found\r\nllama_load_model_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ model 'phi-2-electrical-engineering.Q8_0.gguf'\r\nmain: error: unable\
          \ to load model"
        updatedAt: '2024-01-15T14:10:08.417Z'
      numEdits: 0
      reactions: []
    id: 65a53cc04fecc6144c057022
    type: comment
  author: telehan
  content: "Log start\r\nmain: build = 1839 (5537d9d3)\r\nmain: built with Apple clang\
    \ version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\r\nmain: seed\
    \  = 1705327660\r\nllama_model_loader: loaded meta data with 21 key-value pairs\
    \ and 453 tensors from phi-2-electrical-engineering.Q8_0.gguf (version GGUF V3\
    \ (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides\
    \ do not apply in this output.\r\nllama_model_loader: - kv   0:              \
    \         general.architecture str              = phi2\r\nllama_model_loader:\
    \ - kv   1:                               general.name str              = Phi2\r\
    \nllama_model_loader: - kv   2:                        phi2.context_length u32\
    \              = 2048\r\nllama_model_loader: - kv   3:                      phi2.embedding_length\
    \ u32              = 2560\r\nllama_model_loader: - kv   4:                   phi2.feed_forward_length\
    \ u32              = 10240\r\nllama_model_loader: - kv   5:                  \
    \         phi2.block_count u32              = 32\r\nllama_model_loader: - kv \
    \  6:                  phi2.attention.head_count u32              = 32\r\nllama_model_loader:\
    \ - kv   7:               phi2.attention.head_count_kv u32              = 32\r\
    \nllama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32\
    \              = 0.000010\r\nllama_model_loader: - kv   9:                  phi2.rope.dimension_count\
    \ u32              = 32\r\nllama_model_loader: - kv  10:                     \
    \     general.file_type u32              = 7\r\nllama_model_loader: - kv  11:\
    \               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader:\
    \ - kv  12:                       tokenizer.ggml.model str              = gpt2\r\
    \nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]\
    \   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader:\
    \ - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1,\
    \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:      \
    \                tokenizer.ggml.merges arr[str,50000]   = [\"\u0120 t\", \"\u0120\
    \ a\", \"h e\", \"i n\", \"r e\",...\r\nllama_model_loader: - kv  16:        \
    \        tokenizer.ggml.bos_token_id u32              = 50256\r\nllama_model_loader:\
    \ - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\r\
    \nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32\
    \              = 50256\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
    \ u32              = 0\r\nllama_model_loader: - kv  20:               general.quantization_version\
    \ u32              = 2\r\nllama_model_loader: - type  f32:  259 tensors\r\nllama_model_loader:\
    \ - type q8_0:  194 tensors\r\nllm_load_vocab: mismatch in special tokens definition\
    \ ( 910/51200 vs 944/51200 ).\r\nllm_load_print_meta: format           = GGUF\
    \ V3 (latest)\r\nllm_load_print_meta: arch             = phi2\r\nllm_load_print_meta:\
    \ vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 51200\r\n\
    llm_load_print_meta: n_merges         = 50000\r\nllm_load_print_meta: n_ctx_train\
    \      = 2048\r\nllm_load_print_meta: n_embd           = 2560\r\nllm_load_print_meta:\
    \ n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta:\
    \ n_layer          = 32\r\nllm_load_print_meta: n_rot            = 32\r\nllm_load_print_meta:\
    \ n_embd_head_k    = 80\r\nllm_load_print_meta: n_embd_head_v    = 80\r\nllm_load_print_meta:\
    \ n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2560\r\nllm_load_print_meta:\
    \ n_embd_v_gqa     = 2560\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\
    \nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv\
    \      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta:\
    \ n_ff             = 10240\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta:\
    \ n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta:\
    \ freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\n\
    llm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned\
    \   = unknown\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta:\
    \ model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 2.78 B\r\n\
    llm_load_print_meta: model size       = 2.75 GiB (8.51 BPW)\r\nllm_load_print_meta:\
    \ general.name     = Phi2\r\nllm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\r\
    \nllm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta:\
    \ UNK token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta: PAD token \
    \       = 0 '!'\r\nllm_load_print_meta: LF token         = 128 '\xC4'\r\nllm_load_tensors:\
    \ ggml ctx size       =    0.17 MiB\r\nerror loading model: create_tensor: tensor\
    \ 'blk.0.attn_qkv.weight' not found\r\nllama_load_model_from_file: failed to load\
    \ model\r\nllama_init_from_gpt_params: error: failed to load model 'phi-2-electrical-engineering.Q8_0.gguf'\r\
    \nmain: error: unable to load model"
  created_at: 2024-01-15 14:10:08+00:00
  edited: false
  hidden: false
  id: 65a53cc04fecc6144c057022
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2871adc147d2ead81d57f55e6dc6584a.svg
      fullname: han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: telehan
      type: user
    createdAt: '2024-01-15T14:13:55.000Z'
    data:
      edited: false
      editors:
      - telehan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36127275228500366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2871adc147d2ead81d57f55e6dc6584a.svg
          fullname: han
          isHf: false
          isPro: false
          name: telehan
          type: user
        html: '<p>$ shasum -a 256 phi-2-electrical-engineering.Q8_0.gguf<br>8fc078112e297422e36020a389c8c36fa2ac500a9306a577b47a39be569e8af1  phi-2-electrical-engineering.Q8_0.gguf<br>llama.cpp$
          ./main --version<br>version: 1839 (5537d9d3)<br>built with Apple clang version
          15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0</p>

          '
        raw: '$ shasum -a 256 phi-2-electrical-engineering.Q8_0.gguf

          8fc078112e297422e36020a389c8c36fa2ac500a9306a577b47a39be569e8af1  phi-2-electrical-engineering.Q8_0.gguf

          llama.cpp$ ./main --version

          version: 1839 (5537d9d3)

          built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0'
        updatedAt: '2024-01-15T14:13:55.203Z'
      numEdits: 0
      reactions: []
    id: 65a53da3534e60db992d3a14
    type: comment
  author: telehan
  content: '$ shasum -a 256 phi-2-electrical-engineering.Q8_0.gguf

    8fc078112e297422e36020a389c8c36fa2ac500a9306a577b47a39be569e8af1  phi-2-electrical-engineering.Q8_0.gguf

    llama.cpp$ ./main --version

    version: 1839 (5537d9d3)

    built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0'
  created_at: 2024-01-15 14:13:55+00:00
  edited: false
  hidden: false
  id: 65a53da3534e60db992d3a14
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/phi-2-electrical-engineering-GGUF
repo_type: model
status: open
target_branch: null
title: 'error loading model: create_tensor: tensor ''blk.0.attn_qkv.weight'' not found'
