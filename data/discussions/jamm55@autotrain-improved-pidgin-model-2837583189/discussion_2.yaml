!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sapwavino
conflicting_files: null
created_at: 2023-03-26 10:10:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/254b64fd25555e6b3da9adf3b5896e92.svg
      fullname: Simeon Samari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sapwavino
      type: user
    createdAt: '2023-03-26T11:10:15.000Z'
    data:
      edited: true
      editors:
      - sapwavino
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/254b64fd25555e6b3da9adf3b5896e92.svg
          fullname: Simeon Samari
          isHf: false
          isPro: false
          name: sapwavino
          type: user
        html: "<p>I was just about to embark on the same journey when i thought to\
          \ check \U0001F917 first and fortunately i came across your project. Great\
          \ job so far. I just have a few questions. What does your training data\
          \ look like? I have a .tsv of translations of reviews and random questions\
          \ from freebase_qa and amazon_reviews_multi  and i'd love to go into training/finetuning.\
          \ I was thinking of generating more datasets with this model translating\
          \ the questions/reviews  and i was wondering if that's the approach you've\
          \ taken to diversify your training datasets. Also i can't seem to access\
          \ your tokenizer. Is that by design?<br>Thank you for such amazing work.</p>\n"
        raw: "I was just about to embark on the same journey when i thought to check\
          \ \U0001F917 first and fortunately i came across your project. Great job\
          \ so far. I just have a few questions. What does your training data look\
          \ like? I have a .tsv of translations of reviews and random questions from\
          \ freebase_qa and amazon_reviews_multi  and i'd love to go into training/finetuning.\
          \ I was thinking of generating more datasets with this model translating\
          \ the questions/reviews  and i was wondering if that's the approach you've\
          \ taken to diversify your training datasets. Also i can't seem to access\
          \ your tokenizer. Is that by design?\nThank you for such amazing work."
        updatedAt: '2023-03-26T12:43:00.864Z'
      numEdits: 2
      reactions: []
    id: 6420281769a2c293387ca6ab
    type: comment
  author: sapwavino
  content: "I was just about to embark on the same journey when i thought to check\
    \ \U0001F917 first and fortunately i came across your project. Great job so far.\
    \ I just have a few questions. What does your training data look like? I have\
    \ a .tsv of translations of reviews and random questions from freebase_qa and\
    \ amazon_reviews_multi  and i'd love to go into training/finetuning. I was thinking\
    \ of generating more datasets with this model translating the questions/reviews\
    \  and i was wondering if that's the approach you've taken to diversify your training\
    \ datasets. Also i can't seem to access your tokenizer. Is that by design?\nThank\
    \ you for such amazing work."
  created_at: 2023-03-26 10:10:15+00:00
  edited: true
  hidden: false
  id: 6420281769a2c293387ca6ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jamm55/autotrain-improved-pidgin-model-2837583189
repo_type: model
status: open
target_branch: null
title: Great job. Quick inquiries.
