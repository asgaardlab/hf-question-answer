!!python/object:huggingface_hub.community.DiscussionWithDetails
author: josete89
conflicting_files: null
created_at: 2024-01-22 16:45:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
      fullname: Jose Luis Alcala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josete89
      type: user
    createdAt: '2024-01-22T16:45:40.000Z'
    data:
      edited: false
      editors:
      - josete89
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9530357122421265
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
          fullname: Jose Luis Alcala
          isHf: false
          isPro: false
          name: josete89
          type: user
        html: '<p>I''ve tried to deploy this with Sagemaker LMI but is not possible.
          Seems the model should follow this layout: </p>

          <ul>

          <li>compiled: neff files</li>

          <li>checkpoint: pytorch weights compiled</li>

          <li>tokenizer...<br>Is it possible to get something like that? Or least
          some code snippet how to deploy this as an endpoint? I''ve tried but still
          no luck</li>

          </ul>

          '
        raw: "I've tried to deploy this with Sagemaker LMI but is not possible. Seems\
          \ the model should follow this layout: \r\n* compiled: neff files\r\n* checkpoint:\
          \ pytorch weights compiled\r\n* tokenizer...\r\nIs it possible to get something\
          \ like that? Or least some code snippet how to deploy this as an endpoint?\
          \ I've tried but still no luck"
        updatedAt: '2024-01-22T16:45:40.676Z'
      numEdits: 0
      reactions: []
    id: 65ae9bb45c9e7ad7bbe11205
    type: comment
  author: josete89
  content: "I've tried to deploy this with Sagemaker LMI but is not possible. Seems\
    \ the model should follow this layout: \r\n* compiled: neff files\r\n* checkpoint:\
    \ pytorch weights compiled\r\n* tokenizer...\r\nIs it possible to get something\
    \ like that? Or least some code snippet how to deploy this as an endpoint? I've\
    \ tried but still no luck"
  created_at: 2024-01-22 16:45:40+00:00
  edited: false
  hidden: false
  id: 65ae9bb45c9e7ad7bbe11205
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-22T17:42:39.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9878813028335571
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;josete89&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/josete89\"\
          >@<span class=\"underline\">josete89</span></a></span>\n\n\t</span></span>.\
          \  What you are describing is the layout for the Optimum library.   This\
          \ example was originally built with Transformers because Optimum didn't\
          \ have the support for Mistral, but I saw a PR went through last week with\
          \ it.  We should be able to update it to work. Reach out to me.</p>\n"
        raw: 'Hey @josete89.  What you are describing is the layout for the Optimum
          library.   This example was originally built with Transformers because Optimum
          didn''t have the support for Mistral, but I saw a PR went through last week
          with it.  We should be able to update it to work. Reach out to me.

          '
        updatedAt: '2024-01-22T17:42:39.404Z'
      numEdits: 0
      reactions: []
    id: 65aea90f317cc62a3ab4d36b
    type: comment
  author: jburtoft
  content: 'Hey @josete89.  What you are describing is the layout for the Optimum
    library.   This example was originally built with Transformers because Optimum
    didn''t have the support for Mistral, but I saw a PR went through last week with
    it.  We should be able to update it to work. Reach out to me.

    '
  created_at: 2024-01-22 17:42:39+00:00
  edited: false
  hidden: false
  id: 65aea90f317cc62a3ab4d36b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-23T08:59:00.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9439994096755981
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p><code>optimum-neuron &gt;= 0.0.17</code> compatible models have
          been added for several configurations.</p>

          '
        raw: '`optimum-neuron >= 0.0.17` compatible models have been added for several
          configurations.'
        updatedAt: '2024-01-23T08:59:00.011Z'
      numEdits: 0
      reactions: []
    id: 65af7fd4d0a5cc99d5322c9c
    type: comment
  author: dacorvo
  content: '`optimum-neuron >= 0.0.17` compatible models have been added for several
    configurations.'
  created_at: 2024-01-23 08:59:00+00:00
  edited: false
  hidden: false
  id: 65af7fd4d0a5cc99d5322c9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
      fullname: Jose Luis Alcala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josete89
      type: user
    createdAt: '2024-01-23T14:02:45.000Z'
    data:
      edited: true
      editors:
      - josete89
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7472034096717834
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
          fullname: Jose Luis Alcala
          isHf: false
          isPro: false
          name: josete89
          type: user
        html: "<p>I was able to compile the model seamlessly, but when I tried to\
          \ deploy it:</p>\n<pre><code>from sagemaker.huggingface.model import HuggingFaceModel\n\
          \n# create Hugging Face Model Class\nmodel = HuggingFaceModel(\n   model_data=s3_model_uri,\
          \        # path to your model.tar.gz on s3\n   role=role,              \
          \        # iam role with permissions to create an Endpoint\n   transformers_version=\"\
          4.34.1\",  # transformers version used\n   pytorch_version=\"1.13.1\", \
          \      # pytorch version used\n   py_version='py310',             # python\
          \ version used\n   model_server_workers=1,         # number of workers for\
          \ the model server\n)\n</code></pre>\n<p>I got the following message when\
          \ I sent a request:<br>\"Pretrained model is compiled with neuronx-cc(2.12.54.0+f631c2365)\
          \ newer than current compiler (2.11.0.34+c5231f848), which may cause runtime\"\
          . </p>\n<p>I guess the base image needs to be updated.</p>\n"
        raw: "I was able to compile the model seamlessly, but when I tried to deploy\
          \ it:\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n\
          # create Hugging Face Model Class\nmodel = HuggingFaceModel(\n   model_data=s3_model_uri,\
          \        # path to your model.tar.gz on s3\n   role=role,              \
          \        # iam role with permissions to create an Endpoint\n   transformers_version=\"\
          4.34.1\",  # transformers version used\n   pytorch_version=\"1.13.1\", \
          \      # pytorch version used\n   py_version='py310',             # python\
          \ version used\n   model_server_workers=1,         # number of workers for\
          \ the model server\n)\n```\nI got the following message when I sent a request:\n\
          \"Pretrained model is compiled with neuronx-cc(2.12.54.0+f631c2365) newer\
          \ than current compiler (2.11.0.34+c5231f848), which may cause runtime\"\
          . \n\nI guess the base image needs to be updated.\n\n"
        updatedAt: '2024-01-23T14:09:26.465Z'
      numEdits: 1
      reactions: []
    id: 65afc70549a955de36d69a61
    type: comment
  author: josete89
  content: "I was able to compile the model seamlessly, but when I tried to deploy\
    \ it:\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# create\
    \ Hugging Face Model Class\nmodel = HuggingFaceModel(\n   model_data=s3_model_uri,\
    \        # path to your model.tar.gz on s3\n   role=role,                    \
    \  # iam role with permissions to create an Endpoint\n   transformers_version=\"\
    4.34.1\",  # transformers version used\n   pytorch_version=\"1.13.1\",       #\
    \ pytorch version used\n   py_version='py310',             # python version used\n\
    \   model_server_workers=1,         # number of workers for the model server\n\
    )\n```\nI got the following message when I sent a request:\n\"Pretrained model\
    \ is compiled with neuronx-cc(2.12.54.0+f631c2365) newer than current compiler\
    \ (2.11.0.34+c5231f848), which may cause runtime\". \n\nI guess the base image\
    \ needs to be updated.\n\n"
  created_at: 2024-01-23 14:02:45+00:00
  edited: true
  hidden: false
  id: 65afc70549a955de36d69a61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-23T14:53:37.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8860528469085693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;josete89&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/josete89\">@<span class=\"\
          underline\">josete89</span></a></span>\n\n\t</span></span> Yes, Mistral\
          \ requires the new 2.16 SDK.  Not all the images are updated yet.</p>\n\
          <p>As of today, you would need to use 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0</p>\n\
          <p>That may require you to repackage your model depending on what image\
          \ you were using previously.  Watch for updates at <a rel=\"nofollow\" href=\"\
          https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\"\
          >https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers</a></p>\n\
          <p>What image are you using to deploy now?  You may be able to update that\
          \ and deploy it as a custom image.</p>\n"
        raw: '@josete89 Yes, Mistral requires the new 2.16 SDK.  Not all the images
          are updated yet.


          As of today, you would need to use 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0


          That may require you to repackage your model depending on what image you
          were using previously.  Watch for updates at https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers


          What image are you using to deploy now?  You may be able to update that
          and deploy it as a custom image.'
        updatedAt: '2024-01-23T14:53:37.638Z'
      numEdits: 0
      reactions: []
    id: 65afd2f125c173a4578252f5
    type: comment
  author: jburtoft
  content: '@josete89 Yes, Mistral requires the new 2.16 SDK.  Not all the images
    are updated yet.


    As of today, you would need to use 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0


    That may require you to repackage your model depending on what image you were
    using previously.  Watch for updates at https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers


    What image are you using to deploy now?  You may be able to update that and deploy
    it as a custom image.'
  created_at: 2024-01-23 14:53:37+00:00
  edited: false
  hidden: false
  id: 65afd2f125c173a4578252f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
      fullname: Jose Luis Alcala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josete89
      type: user
    createdAt: '2024-01-23T15:40:51.000Z'
    data:
      edited: false
      editors:
      - josete89
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7114353179931641
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623664990009-60c7285ffcb103f4f6a53c64.jpeg?w=200&h=200&f=face
          fullname: Jose Luis Alcala
          isHf: false
          isPro: false
          name: josete89
          type: user
        html: '<p>Right now I''m using: <em><strong>"763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.1-transformers4.34.1-neuronx-py310-sdk2.15.0-ubuntu20.04"</strong></em>
          I guess that''s the problem :) How can I deploy it as custom image then?</p>

          '
        raw: 'Right now I''m using: ***"763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.1-transformers4.34.1-neuronx-py310-sdk2.15.0-ubuntu20.04"***
          I guess that''s the problem :) How can I deploy it as custom image then?'
        updatedAt: '2024-01-23T15:40:51.628Z'
      numEdits: 0
      reactions: []
    id: 65afde032e39165640cca599
    type: comment
  author: josete89
  content: 'Right now I''m using: ***"763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.1-transformers4.34.1-neuronx-py310-sdk2.15.0-ubuntu20.04"***
    I guess that''s the problem :) How can I deploy it as custom image then?'
  created_at: 2024-01-23 15:40:51+00:00
  edited: false
  hidden: false
  id: 65afde032e39165640cca599
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-24T16:57:37.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7370172142982483
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: '<p>You can specify a custom image by using<br>image_uri="763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:1.13.1-optimum0.0.16-neuronx-py310-ubuntu22.04-v1.0",<br>instead
          of the version settings--those are just used to find the right image automatically
          for you.<br>You can try the above--it is a Hugging Face Text Generation
          Image update, but I am not sure of the SDK version.  </p>

          <p>You can also create a sagemaker compatible image and upload it to your
          private ECR repository.  </p>

          <pre><code>git clone https://github.com/huggingface/optimum-neuron

          cd optimum-neuron

          make neuronx-tgi-sagemaker

          </code></pre>

          <p>It is extra steps, but you can specify the exact version of the SDK in
          the Docker file:<br><a rel="nofollow" href="https://github.com/huggingface/optimum-neuron/blob/main/text-generation-inference/Dockerfile">https://github.com/huggingface/optimum-neuron/blob/main/text-generation-inference/Dockerfile</a></p>

          <p>It is less steps if you can wait for the SageMaker team to release an
          updated image.</p>

          '
        raw: "You can specify a custom image by using \nimage_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:1.13.1-optimum0.0.16-neuronx-py310-ubuntu22.04-v1.0\"\
          ,\ninstead of the version settings--those are just used to find the right\
          \ image automatically for you.\nYou can try the above--it is a Hugging Face\
          \ Text Generation Image update, but I am not sure of the SDK version.  \n\
          \nYou can also create a sagemaker compatible image and upload it to your\
          \ private ECR repository.  \n```\ngit clone https://github.com/huggingface/optimum-neuron\n\
          cd optimum-neuron\nmake neuronx-tgi-sagemaker\n```\nIt is extra steps, but\
          \ you can specify the exact version of the SDK in the Docker file:\nhttps://github.com/huggingface/optimum-neuron/blob/main/text-generation-inference/Dockerfile\n\
          \nIt is less steps if you can wait for the SageMaker team to release an\
          \ updated image."
        updatedAt: '2024-01-24T16:57:37.729Z'
      numEdits: 0
      reactions: []
    id: 65b1418142f2ba223123e039
    type: comment
  author: jburtoft
  content: "You can specify a custom image by using \nimage_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:1.13.1-optimum0.0.16-neuronx-py310-ubuntu22.04-v1.0\"\
    ,\ninstead of the version settings--those are just used to find the right image\
    \ automatically for you.\nYou can try the above--it is a Hugging Face Text Generation\
    \ Image update, but I am not sure of the SDK version.  \n\nYou can also create\
    \ a sagemaker compatible image and upload it to your private ECR repository. \
    \ \n```\ngit clone https://github.com/huggingface/optimum-neuron\ncd optimum-neuron\n\
    make neuronx-tgi-sagemaker\n```\nIt is extra steps, but you can specify the exact\
    \ version of the SDK in the Docker file:\nhttps://github.com/huggingface/optimum-neuron/blob/main/text-generation-inference/Dockerfile\n\
    \nIt is less steps if you can wait for the SageMaker team to release an updated\
    \ image."
  created_at: 2024-01-24 16:57:37+00:00
  edited: false
  hidden: false
  id: 65b1418142f2ba223123e039
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: aws-neuron/Mistral-neuron
repo_type: model
status: open
target_branch: null
title: Deploy with Sagemaker LMI
