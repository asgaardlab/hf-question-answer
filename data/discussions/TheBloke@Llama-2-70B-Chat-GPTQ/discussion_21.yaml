!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nashid
conflicting_files: null
created_at: 2023-07-25 22:22:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bab6dd8e8cd71a3e564d68189721571.svg
      fullname: nashid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nashid
      type: user
    createdAt: '2023-07-25T23:22:53.000Z'
    data:
      edited: false
      editors:
      - nashid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9187471866607666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bab6dd8e8cd71a3e564d68189721571.svg
          fullname: nashid
          isHf: false
          isPro: false
          name: nashid
          type: user
        html: '<p>Are two A5000s with 24GB each enough for handling 70TB?</p>

          '
        raw: Are two A5000s with 24GB each enough for handling 70TB?
        updatedAt: '2023-07-25T23:22:53.750Z'
      numEdits: 0
      reactions: []
    id: 64c0594de56520a63d2d4c52
    type: comment
  author: nashid
  content: Are two A5000s with 24GB each enough for handling 70TB?
  created_at: 2023-07-25 22:22:53+00:00
  edited: false
  hidden: false
  id: 64c0594de56520a63d2d4c52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T23:23:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195992350578308
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, that will work. Recommended to use ExLlama for maximum performance.  You
          need to load less of the model on GPU1 - a recommended split is 17.2GB on
          GPU1, 24GB on GPU 2. This leaves room for context on GPU1.</p>

          '
        raw: Yes, that will work. Recommended to use ExLlama for maximum performance.  You
          need to load less of the model on GPU1 - a recommended split is 17.2GB on
          GPU1, 24GB on GPU 2. This leaves room for context on GPU1.
        updatedAt: '2023-07-25T23:23:49.129Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nashid
    id: 64c0598552874207b2ce172a
    type: comment
  author: TheBloke
  content: Yes, that will work. Recommended to use ExLlama for maximum performance.  You
    need to load less of the model on GPU1 - a recommended split is 17.2GB on GPU1,
    24GB on GPU 2. This leaves room for context on GPU1.
  created_at: 2023-07-25 22:23:49+00:00
  edited: false
  hidden: false
  id: 64c0598552874207b2ce172a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neo-benjamin
      type: user
    createdAt: '2023-07-27T00:02:36.000Z'
    data:
      edited: true
      editors:
      - neo-benjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5213955640792847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
          fullname: neo
          isHf: false
          isPro: false
          name: neo-benjamin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> how to spread\
          \ workload to multiple GPU? Default example is:</p>\n<pre><code>model =\
          \ AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
          \        inject_fused_attention=False, # Required for Llama 2 70B model\
          \ at this time.\n        use_safetensors=True,\n        trust_remote_code=False,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          </code></pre>\n"
        raw: "@TheBloke how to spread workload to multiple GPU? Default example is:\n\
          \n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        inject_fused_attention=False,\
          \ # Required for Llama 2 70B model at this time.\n        use_safetensors=True,\n\
          \        trust_remote_code=False,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n```\n"
        updatedAt: '2023-07-27T00:13:07.557Z'
      numEdits: 1
      reactions: []
    id: 64c1b41cc1d1f89163c44d2e
    type: comment
  author: neo-benjamin
  content: "@TheBloke how to spread workload to multiple GPU? Default example is:\n\
    \n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n      \
    \  model_basename=model_basename,\n        inject_fused_attention=False, # Required\
    \ for Llama 2 70B model at this time.\n        use_safetensors=True,\n       \
    \ trust_remote_code=False,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n```\n"
  created_at: 2023-07-26 23:02:36+00:00
  edited: true
  hidden: false
  id: 64c1b41cc1d1f89163c44d2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neo-benjamin
      type: user
    createdAt: '2023-07-27T06:14:26.000Z'
    data:
      edited: false
      editors:
      - neo-benjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4516064524650574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
          fullname: neo
          isHf: false
          isPro: false
          name: neo-benjamin
          type: user
        html: '<p>how to define this spilt?</p>

          '
        raw: how to define this spilt?
        updatedAt: '2023-07-27T06:14:26.462Z'
      numEdits: 0
      reactions: []
    id: 64c20b4272ac4f58ddd43a87
    type: comment
  author: neo-benjamin
  content: how to define this spilt?
  created_at: 2023-07-27 05:14:26+00:00
  edited: false
  hidden: false
  id: 64c20b4272ac4f58ddd43a87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-07-28T01:36:03.000Z'
    data:
      edited: true
      editors:
      - Squeezitgirdle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9486026167869568
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: '<blockquote>

          <p>Yes, that will work. Recommended to use ExLlama for maximum performance.  You
          need to load less of the model on GPU1 - a recommended split is 17.2GB on
          GPU1, 24GB on GPU 2. This leaves room for context on GPU1.</p>

          </blockquote>

          <p>This is probably a dumb question, but using ExLlama or ExLlama HF isn''t
          enough to run this on a 4090, is it?<br>Maybe if I can split it with my
          11900k, but I don''t know how to do that. </p>

          '
        raw: "> Yes, that will work. Recommended to use ExLlama for maximum performance.\
          \  You need to load less of the model on GPU1 - a recommended split is 17.2GB\
          \ on GPU1, 24GB on GPU 2. This leaves room for context on GPU1.\n\nThis\
          \ is probably a dumb question, but using ExLlama or ExLlama HF isn't enough\
          \ to run this on a 4090, is it? \nMaybe if I can split it with my 11900k,\
          \ but I don't know how to do that. "
        updatedAt: '2023-07-28T01:36:22.387Z'
      numEdits: 1
      reactions: []
    id: 64c31b83c370f29a10321dd2
    type: comment
  author: Squeezitgirdle
  content: "> Yes, that will work. Recommended to use ExLlama for maximum performance.\
    \  You need to load less of the model on GPU1 - a recommended split is 17.2GB\
    \ on GPU1, 24GB on GPU 2. This leaves room for context on GPU1.\n\nThis is probably\
    \ a dumb question, but using ExLlama or ExLlama HF isn't enough to run this on\
    \ a 4090, is it? \nMaybe if I can split it with my 11900k, but I don't know how\
    \ to do that. "
  created_at: 2023-07-28 00:36:03+00:00
  edited: true
  hidden: false
  id: 64c31b83c370f29a10321dd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neo-benjamin
      type: user
    createdAt: '2023-07-31T22:50:09.000Z'
    data:
      edited: true
      editors:
      - neo-benjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9599551558494568
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
          fullname: neo
          isHf: false
          isPro: false
          name: neo-benjamin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> can you please\
          \ help with this?</p>\n"
        raw: '@TheBloke can you please help with this?

          '
        updatedAt: '2023-07-31T22:50:19.466Z'
      numEdits: 1
      reactions: []
    id: 64c83aa10b2ba05b2f455018
    type: comment
  author: neo-benjamin
  content: '@TheBloke can you please help with this?

    '
  created_at: 2023-07-31 21:50:09+00:00
  edited: true
  hidden: false
  id: 64c83aa10b2ba05b2f455018
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/949dc23b75c9a73e696b8f5ba25244da.svg
      fullname: rudy chip
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pribadihcr
      type: user
    createdAt: '2023-08-22T08:09:43.000Z'
    data:
      edited: false
      editors:
      - pribadihcr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3184327185153961
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/949dc23b75c9a73e696b8f5ba25244da.svg
          fullname: rudy chip
          isHf: false
          isPro: false
          name: pribadihcr
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;neo-benjamin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/neo-benjamin\"\
          >@<span class=\"underline\">neo-benjamin</span></a></span>\n\n\t</span></span><br>add\
          \ max_memory parameter.<br>reference: <a href=\"https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/9\"\
          >https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/9</a></p>\n"
        raw: "@neo-benjamin \nadd max_memory parameter.\nreference: https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/9"
        updatedAt: '2023-08-22T08:09:43.669Z'
      numEdits: 0
      reactions: []
    id: 64e46d470195913c7fa316f0
    type: comment
  author: pribadihcr
  content: "@neo-benjamin \nadd max_memory parameter.\nreference: https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/9"
  created_at: 2023-08-22 07:09:43+00:00
  edited: false
  hidden: false
  id: 64e46d470195913c7fa316f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 70TB with multiple A5000
