!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hugginglaoda
conflicting_files: null
created_at: 2023-07-20 15:37:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-20T16:37:12.000Z'
    data:
      edited: true
      editors:
      - hugginglaoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.50996994972229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: '<p>I''m using autogptq to quantize the 70b model and I have a 8*2080ti(22g)
          server<br>oom in  GPU1 with :</p>

          <p> model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config,max_memory
          = {0: "4GIB",1: "8GIB",2: "8GIB",3: "8GIB",4: "8GIB",5: "8GIB",6: "8GIB",7:
          "8GIB", "cpu": "200GIB"})<br>  model.quantize(traindataset, use_triton=True,
          cache_examples_on_gpu=False)</p>

          <p>can u share your setting for quantizing? thanks</p>

          '
        raw: "I'm using autogptq to quantize the 70b model and I have a 8*2080ti(22g)\
          \ server\noom in  GPU1 with :\n\n model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config,max_memory = {0: \"4GIB\",1: \"8GIB\",2: \"8GIB\",3: \"\
          8GIB\",4: \"8GIB\",5: \"8GIB\",6: \"8GIB\",7: \"8GIB\", \"cpu\": \"200GIB\"\
          })\n  model.quantize(traindataset, use_triton=True, cache_examples_on_gpu=False)\n\
          \ncan u share your setting for quantizing? thanks\n"
        updatedAt: '2023-07-20T16:38:26.450Z'
      numEdits: 1
      reactions: []
    id: 64b962b835c815492d448842
    type: comment
  author: hugginglaoda
  content: "I'm using autogptq to quantize the 70b model and I have a 8*2080ti(22g)\
    \ server\noom in  GPU1 with :\n\n model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
    \ quantize_config,max_memory = {0: \"4GIB\",1: \"8GIB\",2: \"8GIB\",3: \"8GIB\"\
    ,4: \"8GIB\",5: \"8GIB\",6: \"8GIB\",7: \"8GIB\", \"cpu\": \"200GIB\"})\n  model.quantize(traindataset,\
    \ use_triton=True, cache_examples_on_gpu=False)\n\ncan u share your setting for\
    \ quantizing? thanks\n"
  created_at: 2023-07-20 15:37:12+00:00
  edited: true
  hidden: false
  id: 64b962b835c815492d448842
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-20T17:05:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581835865974426
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I''m afraid that''s not going to work.  AutoGPTQ can split
          the model weights across multiple GPUs (though I never do that myself, and
          don''t recommend it unless you''re really short on CPU RAM), but it can''t
          split the VRAM required for quantisation.  That always goes on GPU0, and
          a 2080Ti is not big enough for 70B.  You''ll need a GPU with around 30GB
          VRAM, meaning you need an A100 40GB or a 48GB card like an A6000 or L40.</p>

          <p>Out of interest, why did you want to quantise it yourself? Are you doing
          a set of parameters I''ve not done, or a different dataset?</p>

          '
        raw: 'Yeah I''m afraid that''s not going to work.  AutoGPTQ can split the
          model weights across multiple GPUs (though I never do that myself, and don''t
          recommend it unless you''re really short on CPU RAM), but it can''t split
          the VRAM required for quantisation.  That always goes on GPU0, and a 2080Ti
          is not big enough for 70B.  You''ll need a GPU with around 30GB VRAM, meaning
          you need an A100 40GB or a 48GB card like an A6000 or L40.


          Out of interest, why did you want to quantise it yourself? Are you doing
          a set of parameters I''ve not done, or a different dataset?'
        updatedAt: '2023-07-20T17:05:56.943Z'
      numEdits: 0
      reactions: []
    id: 64b9697462c0bfc5738363be
    type: comment
  author: TheBloke
  content: 'Yeah I''m afraid that''s not going to work.  AutoGPTQ can split the model
    weights across multiple GPUs (though I never do that myself, and don''t recommend
    it unless you''re really short on CPU RAM), but it can''t split the VRAM required
    for quantisation.  That always goes on GPU0, and a 2080Ti is not big enough for
    70B.  You''ll need a GPU with around 30GB VRAM, meaning you need an A100 40GB
    or a 48GB card like an A6000 or L40.


    Out of interest, why did you want to quantise it yourself? Are you doing a set
    of parameters I''ve not done, or a different dataset?'
  created_at: 2023-07-20 16:05:56+00:00
  edited: false
  hidden: false
  id: 64b9697462c0bfc5738363be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-21T04:28:45.000Z'
    data:
      edited: false
      editors:
      - hugginglaoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8241443634033203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: '<p>because I want to use a dataset contains some Chinese data.<br>both
          autogptq or gptq for llama use pure english dataset for quantise by default.<br>This
          might cause the model lose more precise in Chinese, I guess?</p>

          '
        raw: 'because I want to use a dataset contains some Chinese data.

          both autogptq or gptq for llama use pure english dataset for quantise by
          default.

          This might cause the model lose more precise in Chinese, I guess?'
        updatedAt: '2023-07-21T04:28:45.881Z'
      numEdits: 0
      reactions: []
    id: 64ba097d3e62024f557ce370
    type: comment
  author: hugginglaoda
  content: 'because I want to use a dataset contains some Chinese data.

    both autogptq or gptq for llama use pure english dataset for quantise by default.

    This might cause the model lose more precise in Chinese, I guess?'
  created_at: 2023-07-21 03:28:45+00:00
  edited: false
  hidden: false
  id: 64ba097d3e62024f557ce370
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-21T04:31:57.000Z'
    data:
      edited: false
      editors:
      - hugginglaoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7484151124954224
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/640aa76e96aae649741bd09b/Gf-oHutBoLK2m_aUygesr.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/640aa76e96aae649741bd09b/Gf-oHutBoLK2m_aUygesr.png"></a></p>

          <p>the max vram cost is not stable while for llama1 65b, the cost is stable.<br>Still
          in progress, hope it can be done successfully...</p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/640aa76e96aae649741bd09b/Gf-oHutBoLK2m_aUygesr.png)


          the max vram cost is not stable while for llama1 65b, the cost is stable.

          Still in progress, hope it can be done successfully...'
        updatedAt: '2023-07-21T04:31:57.544Z'
      numEdits: 0
      reactions: []
    id: 64ba0a3d82975e7c6029c6f3
    type: comment
  author: hugginglaoda
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/640aa76e96aae649741bd09b/Gf-oHutBoLK2m_aUygesr.png)


    the max vram cost is not stable while for llama1 65b, the cost is stable.

    Still in progress, hope it can be done successfully...'
  created_at: 2023-07-21 03:31:57+00:00
  edited: false
  hidden: false
  id: 64ba0a3d82975e7c6029c6f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T08:29:31.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9237467646598816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is the script I use for quantizing - it uses the wikitext or
          c4 datasets: <a rel="nofollow" href="https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py">https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a></p>

          <p>Yes I agree for Chinese language, it would be better to use a Chinese
          dataset.</p>

          <p>I just re-read your first message. You say you have 22GB on the 2080Ti?  The
          2080Ti only has 11GB - have you modded it?</p>

          <p>Maybe 22GB would be enough, I don''t know.  I used to be able to quantise
          65B on 1 x 24GB GPU.  But 70B is a bit bigger,  and has a max sequence length
          of 4096.</p>

          <p>I have never had success getting AutoGPTQ to load the model across multiple
          GPUs.  You must not load any model weights on GPU0 else it will definitely
          OOM.  So that would be:<br><code>max_memory = { 0: ''0Gib'', 1: ''22GiB'',
          2: ''22GiB'', 3: ''22GiB'', 4: ''22GiB'', 5: ''22GiB'', 6: ''22GiB'', 7:
          ''22GiB'', ''cpu'': ''200GiB'' } </code></p>

          <p>But when I have tried that config before, I got a CUDA error about GPU0
          not being initialised.</p>

          <p>The only setup I have had success with is the default, where the model
          is loaded 100% into RAM, and then GPU0 is used automatically for quantising
          it.  For 70B this will require about 165GB RAM.</p>

          '
        raw: 'This is the script I use for quantizing - it uses the wikitext or c4
          datasets: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


          Yes I agree for Chinese language, it would be better to use a Chinese dataset.


          I just re-read your first message. You say you have 22GB on the 2080Ti?  The
          2080Ti only has 11GB - have you modded it?


          Maybe 22GB would be enough, I don''t know.  I used to be able to quantise
          65B on 1 x 24GB GPU.  But 70B is a bit bigger,  and has a max sequence length
          of 4096.


          I have never had success getting AutoGPTQ to load the model across multiple
          GPUs.  You must not load any model weights on GPU0 else it will definitely
          OOM.  So that would be:

          `max_memory = { 0: ''0Gib'', 1: ''22GiB'', 2: ''22GiB'', 3: ''22GiB'', 4:
          ''22GiB'', 5: ''22GiB'', 6: ''22GiB'', 7: ''22GiB'', ''cpu'': ''200GiB''
          } `


          But when I have tried that config before, I got a CUDA error about GPU0
          not being initialised.


          The only setup I have had success with is the default, where the model is
          loaded 100% into RAM, and then GPU0 is used automatically for quantising
          it.  For 70B this will require about 165GB RAM.'
        updatedAt: '2023-07-21T08:30:23.009Z'
      numEdits: 2
      reactions: []
    id: 64ba41ebd48f9126df46ef62
    type: comment
  author: TheBloke
  content: 'This is the script I use for quantizing - it uses the wikitext or c4 datasets:
    https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


    Yes I agree for Chinese language, it would be better to use a Chinese dataset.


    I just re-read your first message. You say you have 22GB on the 2080Ti?  The 2080Ti
    only has 11GB - have you modded it?


    Maybe 22GB would be enough, I don''t know.  I used to be able to quantise 65B
    on 1 x 24GB GPU.  But 70B is a bit bigger,  and has a max sequence length of 4096.


    I have never had success getting AutoGPTQ to load the model across multiple GPUs.  You
    must not load any model weights on GPU0 else it will definitely OOM.  So that
    would be:

    `max_memory = { 0: ''0Gib'', 1: ''22GiB'', 2: ''22GiB'', 3: ''22GiB'', 4: ''22GiB'',
    5: ''22GiB'', 6: ''22GiB'', 7: ''22GiB'', ''cpu'': ''200GiB'' } `


    But when I have tried that config before, I got a CUDA error about GPU0 not being
    initialised.


    The only setup I have had success with is the default, where the model is loaded
    100% into RAM, and then GPU0 is used automatically for quantising it.  For 70B
    this will require about 165GB RAM.'
  created_at: 2023-07-21 07:29:31+00:00
  edited: true
  hidden: false
  id: 64ba41ebd48f9126df46ef62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T09:10:48.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7823116779327393
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I am quantizing 70B right now on a 48GB card, and with seqlen =
          4096 it is using up to 34GB VRAM:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png"></a></p>

          <p>If you use seqlen=2048 it will be a bit less, and you can also save a
          little VRAM by specifying <code>cache_examples_on_gpu=False</code> in <code>.quantize()</code>.  But
          I am not confident you are going to be able to do this in 22GB.</p>

          <p>I suggest you rent a GPU. Runpod have L40 48GB systems with 250GB RAM
          for $1.14/hr.</p>

          '
        raw: 'I am quantizing 70B right now on a 48GB card, and with seqlen = 4096
          it is using up to 34GB VRAM:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png)


          If you use seqlen=2048 it will be a bit less, and you can also save a little
          VRAM by specifying `cache_examples_on_gpu=False` in `.quantize()`.  But
          I am not confident you are going to be able to do this in 22GB.


          I suggest you rent a GPU. Runpod have L40 48GB systems with 250GB RAM for
          $1.14/hr.'
        updatedAt: '2023-07-21T09:11:09.069Z'
      numEdits: 1
      reactions: []
    id: 64ba4b980daef1522b0e5f78
    type: comment
  author: TheBloke
  content: 'I am quantizing 70B right now on a 48GB card, and with seqlen = 4096 it
    is using up to 34GB VRAM:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png)


    If you use seqlen=2048 it will be a bit less, and you can also save a little VRAM
    by specifying `cache_examples_on_gpu=False` in `.quantize()`.  But I am not confident
    you are going to be able to do this in 22GB.


    I suggest you rent a GPU. Runpod have L40 48GB systems with 250GB RAM for $1.14/hr.'
  created_at: 2023-07-21 08:10:48+00:00
  edited: true
  hidden: false
  id: 64ba4b980daef1522b0e5f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-22T08:03:46.000Z'
    data:
      edited: false
      editors:
      - hugginglaoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8608567118644714
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: "<blockquote>\n<p>I am quantizing 70B right now on a 48GB card, and\
          \ with seqlen = 4096 it is using up to 34GB VRAM:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png\"\
          ></a></p>\n<p>If you use seqlen=2048 it will be a bit less, and you can\
          \ also save a little VRAM by specifying <code>cache_examples_on_gpu=False</code>\
          \ in <code>.quantize()</code>.  But I am not confident you are going to\
          \ be able to do this in 22GB.</p>\n<p>I suggest you rent a GPU. Runpod have\
          \ L40 48GB systems with 250GB RAM for $1.14/hr.</p>\n</blockquote>\n<p>much\
          \ thanks\uFF01<br>I successfully quantize the model with 2048 in my machine.\
          \ But seems impossible for 4096...hhhh, for renting GPU, would it cost a\
          \ long time in to downloading the modle?</p>\n<p>May I ask another question\
          \ that...<br>Can you lora the 4bit model now? I can train and save the lora\
          \ file with fintune.py in alpaca_lora_4bit.<br>But when I loaded the saved\
          \ lora file and do inference, the output is broken within alpaca_lora_4bit\
          \ and throw error within exllama  </p>\n"
        raw: "> I am quantizing 70B right now on a 48GB card, and with seqlen = 4096\
          \ it is using up to 34GB VRAM:\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png)\n\
          > \n> If you use seqlen=2048 it will be a bit less, and you can also save\
          \ a little VRAM by specifying `cache_examples_on_gpu=False` in `.quantize()`.\
          \  But I am not confident you are going to be able to do this in 22GB.\n\
          > \n> I suggest you rent a GPU. Runpod have L40 48GB systems with 250GB\
          \ RAM for $1.14/hr.\n\nmuch thanks\uFF01\nI successfully quantize the model\
          \ with 2048 in my machine. But seems impossible for 4096...hhhh, for renting\
          \ GPU, would it cost a long time in to downloading the modle?\n\nMay I ask\
          \ another question that...\nCan you lora the 4bit model now? I can train\
          \ and save the lora file with fintune.py in alpaca_lora_4bit.\nBut when\
          \ I loaded the saved lora file and do inference, the output is broken within\
          \ alpaca_lora_4bit and throw error within exllama  "
        updatedAt: '2023-07-22T08:03:46.420Z'
      numEdits: 0
      reactions: []
    id: 64bb8d6265b648b2df7f3967
    type: comment
  author: hugginglaoda
  content: "> I am quantizing 70B right now on a 48GB card, and with seqlen = 4096\
    \ it is using up to 34GB VRAM:\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zTNk25gXpSy54JFe6pplj.png)\n\
    > \n> If you use seqlen=2048 it will be a bit less, and you can also save a little\
    \ VRAM by specifying `cache_examples_on_gpu=False` in `.quantize()`.  But I am\
    \ not confident you are going to be able to do this in 22GB.\n> \n> I suggest\
    \ you rent a GPU. Runpod have L40 48GB systems with 250GB RAM for $1.14/hr.\n\n\
    much thanks\uFF01\nI successfully quantize the model with 2048 in my machine.\
    \ But seems impossible for 4096...hhhh, for renting GPU, would it cost a long\
    \ time in to downloading the modle?\n\nMay I ask another question that...\nCan\
    \ you lora the 4bit model now? I can train and save the lora file with fintune.py\
    \ in alpaca_lora_4bit.\nBut when I loaded the saved lora file and do inference,\
    \ the output is broken within alpaca_lora_4bit and throw error within exllama\
    \  "
  created_at: 2023-07-22 07:03:46+00:00
  edited: false
  hidden: false
  id: 64bb8d6265b648b2df7f3967
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:13:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9472528100013733
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Good to hear!</p>

          <p>No it shouldn''t take a huge amount of time to download the model. It
          depends on the exact server of course, some have 1Gbit/s, some have 10Gbit/s.
          But even if it''s only 1Gbit/s, to download Llama 2 130GB should only take
          20-30 minutes.  And then when you''ve made the quantisation you can upload
          it to Hugging Face Hub and that will be much quicker because the quantisation
          will be much smaller, only around 35GB.</p>

          <p>I don''t know how fast your internet is, but if you downloaded Llama
          2 at 130GB presumably you can also download 35GB no problem.</p>

          '
        raw: 'Good to hear!


          No it shouldn''t take a huge amount of time to download the model. It depends
          on the exact server of course, some have 1Gbit/s, some have 10Gbit/s. But
          even if it''s only 1Gbit/s, to download Llama 2 130GB should only take 20-30
          minutes.  And then when you''ve made the quantisation you can upload it
          to Hugging Face Hub and that will be much quicker because the quantisation
          will be much smaller, only around 35GB.


          I don''t know how fast your internet is, but if you downloaded Llama 2 at
          130GB presumably you can also download 35GB no problem.'
        updatedAt: '2023-07-22T08:13:21.541Z'
      numEdits: 0
      reactions: []
    id: 64bb8fa15c457ccaa4ea33c8
    type: comment
  author: TheBloke
  content: 'Good to hear!


    No it shouldn''t take a huge amount of time to download the model. It depends
    on the exact server of course, some have 1Gbit/s, some have 10Gbit/s. But even
    if it''s only 1Gbit/s, to download Llama 2 130GB should only take 20-30 minutes.  And
    then when you''ve made the quantisation you can upload it to Hugging Face Hub
    and that will be much quicker because the quantisation will be much smaller, only
    around 35GB.


    I don''t know how fast your internet is, but if you downloaded Llama 2 at 130GB
    presumably you can also download 35GB no problem.'
  created_at: 2023-07-22 07:13:21+00:00
  edited: false
  hidden: false
  id: 64bb8fa15c457ccaa4ea33c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-22T10:12:31.000Z'
    data:
      edited: false
      editors:
      - hugginglaoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: '<p>yeah, I am planning to do it in colab.</p>

          <p>Any idea with lora?</p>

          '
        raw: 'yeah, I am planning to do it in colab.



          Any idea with lora?'
        updatedAt: '2023-07-22T10:12:31.018Z'
      numEdits: 0
      reactions: []
    id: 64bbab8f4b4ff0d5091ff769
    type: comment
  author: hugginglaoda
  content: 'yeah, I am planning to do it in colab.



    Any idea with lora?'
  created_at: 2023-07-22 09:12:31+00:00
  edited: false
  hidden: false
  id: 64bbab8f4b4ff0d5091ff769
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: can u show the settings for quantizing the model?
