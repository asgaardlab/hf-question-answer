!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joycejiang
conflicting_files: null
created_at: 2023-07-27 03:46:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f3dbc6715171644f9a6063640ac07478.svg
      fullname: Joyce Jiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joycejiang
      type: user
    createdAt: '2023-07-27T04:46:50.000Z'
    data:
      edited: false
      editors:
      - joycejiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6140326857566833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f3dbc6715171644f9a6063640ac07478.svg
          fullname: Joyce Jiang
          isHf: false
          isPro: false
          name: joycejiang
          type: user
        html: "<p>The model is giving me a bunch of nonsense output \"\xEDliaa enemies\
          \ enemiesclicclic Sue SueUob pil Silvererde wc Sept\", and if I ran it again\
          \ it broke<br>'RuntimeError: CUDA error: device-side assert triggered CUDA\
          \ kernel errors might be asynchronously reported at some other API call,\
          \ so the stacktrace below might be incorrect.'</p>\n<pre><code>  model =\
          \ AutoGPTQForCausalLM.from_quantized(model_name, \n  model_basename=\"gptq_model-4bit--1g\"\
          ,\n  device_map=\"auto\", \n  use_safetensors=True, \n  use_triton=False,\
          \ \n  trust_remote_code=True, \n  quantize_config=None,\n  inject_fused_attention=False)\n\
          </code></pre>\n"
        raw: "The model is giving me a bunch of nonsense output \"\xEDliaa enemies\
          \ enemiesclicclic Sue SueUob pil Silvererde wc Sept\", and if I ran it again\
          \ it broke \r\n'RuntimeError: CUDA error: device-side assert triggered CUDA\
          \ kernel errors might be asynchronously reported at some other API call,\
          \ so the stacktrace below might be incorrect.'\r\n\r\n      model = AutoGPTQForCausalLM.from_quantized(model_name,\
          \ \r\n      model_basename=\"gptq_model-4bit--1g\",\r\n      device_map=\"\
          auto\", \r\n      use_safetensors=True, \r\n      use_triton=False, \r\n\
          \      trust_remote_code=True, \r\n      quantize_config=None,\r\n     \
          \ inject_fused_attention=False)"
        updatedAt: '2023-07-27T04:46:50.492Z'
      numEdits: 0
      reactions: []
    id: 64c1f6ba2bac49787a9bc51e
    type: comment
  author: joycejiang
  content: "The model is giving me a bunch of nonsense output \"\xEDliaa enemies enemiesclicclic\
    \ Sue SueUob pil Silvererde wc Sept\", and if I ran it again it broke \r\n'RuntimeError:\
    \ CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously\
    \ reported at some other API call, so the stacktrace below might be incorrect.'\r\
    \n\r\n      model = AutoGPTQForCausalLM.from_quantized(model_name, \r\n      model_basename=\"\
    gptq_model-4bit--1g\",\r\n      device_map=\"auto\", \r\n      use_safetensors=True,\
    \ \r\n      use_triton=False, \r\n      trust_remote_code=True, \r\n      quantize_config=None,\r\
    \n      inject_fused_attention=False)"
  created_at: 2023-07-27 03:46:50+00:00
  edited: false
  hidden: false
  id: 64c1f6ba2bac49787a9bc51e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T08:07:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9073602557182312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There was a bug in AutoGPTQ 0.3.0 that caused gibberish in some
          cases. Please update to AutoGPTQ 0.3.2 which was released yesterday and
          fixes this issue:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          pip3 install auto-gptq==0.3.2

          </code></pre>

          '
        raw: 'There was a bug in AutoGPTQ 0.3.0 that caused gibberish in some cases.
          Please update to AutoGPTQ 0.3.2 which was released yesterday and fixes this
          issue:

          ```

          pip3 uninstall -y auto-gptq

          pip3 install auto-gptq==0.3.2

          ```'
        updatedAt: '2023-07-27T08:07:10.840Z'
      numEdits: 0
      reactions: []
    id: 64c225ae5b329de1b1e2a65b
    type: comment
  author: TheBloke
  content: 'There was a bug in AutoGPTQ 0.3.0 that caused gibberish in some cases.
    Please update to AutoGPTQ 0.3.2 which was released yesterday and fixes this issue:

    ```

    pip3 uninstall -y auto-gptq

    pip3 install auto-gptq==0.3.2

    ```'
  created_at: 2023-07-27 07:07:10+00:00
  edited: false
  hidden: false
  id: 64c225ae5b329de1b1e2a65b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Generating nonsense output and then broke
