!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zeal666
conflicting_files: null
created_at: 2023-10-02 19:21:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a6da3f7009ce9f108121ede330f94f8.svg
      fullname: Hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zeal666
      type: user
    createdAt: '2023-10-02T20:21:26.000Z'
    data:
      edited: false
      editors:
      - Zeal666
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5010582208633423
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a6da3f7009ce9f108121ede330f94f8.svg
          fullname: Hua
          isHf: false
          isPro: false
          name: Zeal666
          type: user
        html: '<p>I''m using <a rel="nofollow" href="https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md">https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md</a>.
          And I got the error below. Does anyone know on how to resolve this? Appreciate!<br>+++++++++++++++++++++++++++++++++++<br>2023-10-03
          03:53:39 | INFO | model_worker | args: Namespace(host=''localhost'', port=21002,
          worker_address=''<a rel="nofollow" href="http://localhost:21002''">http://localhost:21002''</a>,
          controller_address=''<a rel="nofollow" href="http://localhost:21001''">http://localhost:21001''</a>,
          model_path=''models/Llama-2-70B-chat-GPTQ'', revision=''main'', device=''cuda'',
          gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False,
          cpu_offloading=False, gptq_ckpt=''models/Llama-2-70B-chat-GPTQ/model.safetensors'',
          gptq_wbits=4, gptq_groupsize=128, gptq_act_order=True, awq_ckpt=None, awq_wbits=16,
          awq_groupsize=-1, model_names=None, conv_template=None, embed_in_truncate=False,
          limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None)<br>2023-10-03
          03:53:39 | INFO | model_worker | Loading the model [''Llama-2-70B-chat-GPTQ'']
          on worker 9bf7fc38 ...<br>2023-10-03 03:53:39 | INFO | stdout | Loading
          GPTQ quantized model...<br>2023-10-03 03:53:43 | INFO | stdout | Loading
          model ...<br>2023-10-03 03:53:47 | ERROR | stderr | Traceback (most recent
          call last):<br>2023-10-03 03:53:47 | ERROR | stderr |   File "/usr/lib/python3.9/runpy.py",
          line 197, in _run_module_as_main<br>2023-10-03 03:53:47 | ERROR | stderr
          |     return _run_code(code, main_globals, None,<br>2023-10-03 03:53:47
          | ERROR | stderr |   File "/usr/lib/python3.9/runpy.py", line 87, in _run_code<br>2023-10-03
          03:53:47 | ERROR | stderr |     exec(code, run_globals)<br>2023-10-03 03:53:47
          | ERROR | stderr |   File "/home/zeal/FastChat/fastchat/serve/model_worker.py",
          line 543, in <br>2023-10-03 03:53:47 | ERROR | stderr |     args, worker
          = create_model_worker()<br>2023-10-03 03:53:47 | ERROR | stderr |   File
          "/home/zeal/FastChat/fastchat/serve/model_worker.py", line 518, in create_model_worker<br>2023-10-03
          03:53:47 | ERROR | stderr |     worker = ModelWorker(<br>2023-10-03 03:53:47
          | ERROR | stderr |   File "/home/zeal/FastChat/fastchat/serve/model_worker.py",
          line 221, in <strong>init</strong><br>2023-10-03 03:53:47 | ERROR | stderr
          |     self.model, self.tokenizer = load_model(<br>2023-10-03 03:53:47 |
          ERROR | stderr |   File "/home/zeal/FastChat/fastchat/model/model_adapter.py",
          line 269, in load_model<br>2023-10-03 03:53:47 | ERROR | stderr |     model,
          tokenizer = load_gptq_quantized(model_path, gptq_config)<br>2023-10-03 03:53:47
          | ERROR | stderr |   File "/home/zeal/FastChat/fastchat/modules/gptq.py",
          line 46, in load_gptq_quantized<br>2023-10-03 03:53:47 | ERROR | stderr
          |     model = load_quant(<br>2023-10-03 03:53:47 | ERROR | stderr |   File
          "/home/zeal/FastChat/fastchat/../repositories/GPTQ-for-LLaMa/llama.py",
          line 308, in load_quant<br>2023-10-03 03:53:47 | ERROR | stderr |     model.load_state_dict(safe_load(checkpoint))<br>2023-10-03
          03:53:47 | ERROR | stderr |   File "/home/zeal/venv/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 2041, in load_state_dict<br>2023-10-03 03:53:47 | ERROR | stderr |     raise
          RuntimeError(''Error(s) in loading state_dict for {}:\n\t{}''.format(<br>2023-10-03
          03:53:47 | ERROR | stderr | RuntimeError: Error(s) in loading state_dict
          for LlamaForCausalLM:</p>

          '
        raw: "I'm using https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md.\
          \ And I got the error below. Does anyone know on how to resolve this? Appreciate!\r\
          \n+++++++++++++++++++++++++++++++++++\r\n2023-10-03 03:53:39 | INFO | model_worker\
          \ | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002',\
          \ controller_address='http://localhost:21001', model_path='models/Llama-2-70B-chat-GPTQ',\
          \ revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None,\
          \ dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt='models/Llama-2-70B-chat-GPTQ/model.safetensors',\
          \ gptq_wbits=4, gptq_groupsize=128, gptq_act_order=True, awq_ckpt=None,\
          \ awq_wbits=16, awq_groupsize=-1, model_names=None, conv_template=None,\
          \ embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2,\
          \ no_register=False, seed=None)\r\n2023-10-03 03:53:39 | INFO | model_worker\
          \ | Loading the model ['Llama-2-70B-chat-GPTQ'] on worker 9bf7fc38 ...\r\
          \n2023-10-03 03:53:39 | INFO | stdout | Loading GPTQ quantized model...\r\
          \n2023-10-03 03:53:43 | INFO | stdout | Loading model ...\r\n2023-10-03\
          \ 03:53:47 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-03\
          \ 03:53:47 | ERROR | stderr |   File \"/usr/lib/python3.9/runpy.py\", line\
          \ 197, in _run_module_as_main\r\n2023-10-03 03:53:47 | ERROR | stderr |\
          \     return _run_code(code, main_globals, None,\r\n2023-10-03 03:53:47\
          \ | ERROR | stderr |   File \"/usr/lib/python3.9/runpy.py\", line 87, in\
          \ _run_code\r\n2023-10-03 03:53:47 | ERROR | stderr |     exec(code, run_globals)\r\
          \n2023-10-03 03:53:47 | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/serve/model_worker.py\"\
          , line 543, in <module>\r\n2023-10-03 03:53:47 | ERROR | stderr |     args,\
          \ worker = create_model_worker()\r\n2023-10-03 03:53:47 | ERROR | stderr\
          \ |   File \"/home/zeal/FastChat/fastchat/serve/model_worker.py\", line\
          \ 518, in create_model_worker\r\n2023-10-03 03:53:47 | ERROR | stderr |\
          \     worker = ModelWorker(\r\n2023-10-03 03:53:47 | ERROR | stderr |  \
          \ File \"/home/zeal/FastChat/fastchat/serve/model_worker.py\", line 221,\
          \ in __init__\r\n2023-10-03 03:53:47 | ERROR | stderr |     self.model,\
          \ self.tokenizer = load_model(\r\n2023-10-03 03:53:47 | ERROR | stderr |\
          \   File \"/home/zeal/FastChat/fastchat/model/model_adapter.py\", line 269,\
          \ in load_model\r\n2023-10-03 03:53:47 | ERROR | stderr |     model, tokenizer\
          \ = load_gptq_quantized(model_path, gptq_config)\r\n2023-10-03 03:53:47\
          \ | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/modules/gptq.py\"\
          , line 46, in load_gptq_quantized\r\n2023-10-03 03:53:47 | ERROR | stderr\
          \ |     model = load_quant(\r\n2023-10-03 03:53:47 | ERROR | stderr |  \
          \ File \"/home/zeal/FastChat/fastchat/../repositories/GPTQ-for-LLaMa/llama.py\"\
          , line 308, in load_quant\r\n2023-10-03 03:53:47 | ERROR | stderr |    \
          \ model.load_state_dict(safe_load(checkpoint))\r\n2023-10-03 03:53:47 |\
          \ ERROR | stderr |   File \"/home/zeal/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\r\n2023-10-03 03:53:47 | ERROR | stderr\
          \ |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\\
          t{}'.format(\r\n2023-10-03 03:53:47 | ERROR | stderr | RuntimeError: Error(s)\
          \ in loading state_dict for LlamaForCausalLM:"
        updatedAt: '2023-10-02T20:21:26.566Z'
      numEdits: 0
      reactions: []
    id: 651b264654134dcaad00ec90
    type: comment
  author: Zeal666
  content: "I'm using https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md. And\
    \ I got the error below. Does anyone know on how to resolve this? Appreciate!\r\
    \n+++++++++++++++++++++++++++++++++++\r\n2023-10-03 03:53:39 | INFO | model_worker\
    \ | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002',\
    \ controller_address='http://localhost:21001', model_path='models/Llama-2-70B-chat-GPTQ',\
    \ revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None,\
    \ dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt='models/Llama-2-70B-chat-GPTQ/model.safetensors',\
    \ gptq_wbits=4, gptq_groupsize=128, gptq_act_order=True, awq_ckpt=None, awq_wbits=16,\
    \ awq_groupsize=-1, model_names=None, conv_template=None, embed_in_truncate=False,\
    \ limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None)\r\
    \n2023-10-03 03:53:39 | INFO | model_worker | Loading the model ['Llama-2-70B-chat-GPTQ']\
    \ on worker 9bf7fc38 ...\r\n2023-10-03 03:53:39 | INFO | stdout | Loading GPTQ\
    \ quantized model...\r\n2023-10-03 03:53:43 | INFO | stdout | Loading model ...\r\
    \n2023-10-03 03:53:47 | ERROR | stderr | Traceback (most recent call last):\r\n\
    2023-10-03 03:53:47 | ERROR | stderr |   File \"/usr/lib/python3.9/runpy.py\"\
    , line 197, in _run_module_as_main\r\n2023-10-03 03:53:47 | ERROR | stderr | \
    \    return _run_code(code, main_globals, None,\r\n2023-10-03 03:53:47 | ERROR\
    \ | stderr |   File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n\
    2023-10-03 03:53:47 | ERROR | stderr |     exec(code, run_globals)\r\n2023-10-03\
    \ 03:53:47 | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/serve/model_worker.py\"\
    , line 543, in <module>\r\n2023-10-03 03:53:47 | ERROR | stderr |     args, worker\
    \ = create_model_worker()\r\n2023-10-03 03:53:47 | ERROR | stderr |   File \"\
    /home/zeal/FastChat/fastchat/serve/model_worker.py\", line 518, in create_model_worker\r\
    \n2023-10-03 03:53:47 | ERROR | stderr |     worker = ModelWorker(\r\n2023-10-03\
    \ 03:53:47 | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/serve/model_worker.py\"\
    , line 221, in __init__\r\n2023-10-03 03:53:47 | ERROR | stderr |     self.model,\
    \ self.tokenizer = load_model(\r\n2023-10-03 03:53:47 | ERROR | stderr |   File\
    \ \"/home/zeal/FastChat/fastchat/model/model_adapter.py\", line 269, in load_model\r\
    \n2023-10-03 03:53:47 | ERROR | stderr |     model, tokenizer = load_gptq_quantized(model_path,\
    \ gptq_config)\r\n2023-10-03 03:53:47 | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/modules/gptq.py\"\
    , line 46, in load_gptq_quantized\r\n2023-10-03 03:53:47 | ERROR | stderr |  \
    \   model = load_quant(\r\n2023-10-03 03:53:47 | ERROR | stderr |   File \"/home/zeal/FastChat/fastchat/../repositories/GPTQ-for-LLaMa/llama.py\"\
    , line 308, in load_quant\r\n2023-10-03 03:53:47 | ERROR | stderr |     model.load_state_dict(safe_load(checkpoint))\r\
    \n2023-10-03 03:53:47 | ERROR | stderr |   File \"/home/zeal/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\r\n2023-10-03 03:53:47 | ERROR | stderr |    \
    \ raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\
    \n2023-10-03 03:53:47 | ERROR | stderr | RuntimeError: Error(s) in loading state_dict\
    \ for LlamaForCausalLM:"
  created_at: 2023-10-02 19:21:26+00:00
  edited: false
  hidden: false
  id: 651b264654134dcaad00ec90
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM @fschat
  0.2.29, torch 2.0.1+cu118, transformers 4.33.3   '
