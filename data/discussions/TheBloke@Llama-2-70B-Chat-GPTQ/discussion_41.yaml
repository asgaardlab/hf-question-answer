!!python/object:huggingface_hub.community.DiscussionWithDetails
author: simonesartoni1
conflicting_files: null
created_at: 2023-09-19 07:31:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aebadf49ae06742aee352d42f98db55a.svg
      fullname: Simone Sartoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: simonesartoni1
      type: user
    createdAt: '2023-09-19T08:31:57.000Z'
    data:
      edited: false
      editors:
      - simonesartoni1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6448554992675781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aebadf49ae06742aee352d42f98db55a.svg
          fullname: Simone Sartoni
          isHf: false
          isPro: false
          name: simonesartoni1
          type: user
        html: '<p>I have just deployed this model on a g5.12x AWS instance (with 4
          A10G GPUs, each one with 24GB) using this setting: </p>

          <p>" GPTQ_BITS=4 GPTQ_GROUPSIZE=32 sudo docker run  --gpus all --shm-size
          1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id TheBloke/Llama-2-70B-chat-GPTQ --num-shard 4 --quantize gptq
          --revision gptq-4bit-32g-actorder_True"</p>

          <p>From the documentation, it should take 40.66 GB, but my current GPU memory
          is 17GB for each GPU, in total 68GB.</p>

          <p>Can someone explain the reason behind the higher GPU consumption?</p>

          '
        raw: "I have just deployed this model on a g5.12x AWS instance (with 4 A10G\
          \ GPUs, each one with 24GB) using this setting: \r\n\r\n\" GPTQ_BITS=4 GPTQ_GROUPSIZE=32\
          \ sudo docker run  --gpus all --shm-size 1g -p 8080:80 -v $volume:/data\
          \ ghcr.io/huggingface/text-generation-inference:latest --model-id TheBloke/Llama-2-70B-chat-GPTQ\
          \ --num-shard 4 --quantize gptq --revision gptq-4bit-32g-actorder_True\"\
          \r\n\r\nFrom the documentation, it should take 40.66 GB, but my current\
          \ GPU memory is 17GB for each GPU, in total 68GB.\r\n\r\nCan someone explain\
          \ the reason behind the higher GPU consumption?"
        updatedAt: '2023-09-19T08:31:57.346Z'
      numEdits: 0
      reactions: []
    id: 65095c7de0850b3ff030f474
    type: comment
  author: simonesartoni1
  content: "I have just deployed this model on a g5.12x AWS instance (with 4 A10G\
    \ GPUs, each one with 24GB) using this setting: \r\n\r\n\" GPTQ_BITS=4 GPTQ_GROUPSIZE=32\
    \ sudo docker run  --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id TheBloke/Llama-2-70B-chat-GPTQ --num-shard 4 --quantize gptq --revision\
    \ gptq-4bit-32g-actorder_True\"\r\n\r\nFrom the documentation, it should take\
    \ 40.66 GB, but my current GPU memory is 17GB for each GPU, in total 68GB.\r\n\
    \r\nCan someone explain the reason behind the higher GPU consumption?"
  created_at: 2023-09-19 07:31:57+00:00
  edited: false
  hidden: false
  id: 65095c7de0850b3ff030f474
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-09-19T16:08:35.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8412343859672546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>If I guess correctly, from my experience, textgen-webui use AutoGPTQ
          by default with several techniques that increases VRAM usage for sake of
          inferencing speed. Just checkout the "model" page of textgen-webui and AutoGPTQ
          loader for deatils.</p>

          <p>And still AutGPTQ is a bit slower than ExLLaMAv2_hf loader. With ExLLaMAv2_hf,
          I can confirm on my local 2x3090 rig, this model consume about 21G/17G after
          serveral rounds, where as my split is 21G/21G. Would you try that loader
          instead? There are startup arguments in textgen-webui readme for switching
          these loaders.</p>

          '
        raw: 'If I guess correctly, from my experience, textgen-webui use AutoGPTQ
          by default with several techniques that increases VRAM usage for sake of
          inferencing speed. Just checkout the "model" page of textgen-webui and AutoGPTQ
          loader for deatils.


          And still AutGPTQ is a bit slower than ExLLaMAv2_hf loader. With ExLLaMAv2_hf,
          I can confirm on my local 2x3090 rig, this model consume about 21G/17G after
          serveral rounds, where as my split is 21G/21G. Would you try that loader
          instead? There are startup arguments in textgen-webui readme for switching
          these loaders.'
        updatedAt: '2023-09-19T16:09:12.585Z'
      numEdits: 1
      reactions: []
    id: 6509c783e13ff96230f97a47
    type: comment
  author: Yhyu13
  content: 'If I guess correctly, from my experience, textgen-webui use AutoGPTQ by
    default with several techniques that increases VRAM usage for sake of inferencing
    speed. Just checkout the "model" page of textgen-webui and AutoGPTQ loader for
    deatils.


    And still AutGPTQ is a bit slower than ExLLaMAv2_hf loader. With ExLLaMAv2_hf,
    I can confirm on my local 2x3090 rig, this model consume about 21G/17G after serveral
    rounds, where as my split is 21G/21G. Would you try that loader instead? There
    are startup arguments in textgen-webui readme for switching these loaders.'
  created_at: 2023-09-19 15:08:35+00:00
  edited: true
  hidden: false
  id: 6509c783e13ff96230f97a47
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Memory consumption much higher on multi-GPU setup
