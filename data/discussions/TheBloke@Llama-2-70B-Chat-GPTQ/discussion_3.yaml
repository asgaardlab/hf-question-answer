!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-07-19 08:03:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-19T09:03:14.000Z'
    data:
      edited: true
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4648521840572357
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>Thanks in advance.</p>\n<p>Source (llama2gptq70b.py) :</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, pipeline, logging\n\
          <span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path\
          \ = <span class=\"hljs-string\">\"TheBloke/Llama-2-70B-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"gptq_model-4bit--1g\"</span>\n\nuse_triton\
          \ = <span class=\"hljs-literal\">False</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n\n<span class=\"hljs-string\">\"\"\"</span>\n\
          <span class=\"hljs-string\">To download from a specific branch, use the\
          \ revision parameter, as in this example:</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,</span>\n\
          <span class=\"hljs-string\">        revision=\"gptq-4bit-32g-actorder_True\"\
          ,</span>\n<span class=\"hljs-string\">        model_basename=model_basename,</span>\n\
          <span class=\"hljs-string\">        use_safetensors=True,</span>\n<span\
          \ class=\"hljs-string\">        trust_remote_code=True,</span>\n<span class=\"\
          hljs-string\">        device=\"cuda:0\",</span>\n<span class=\"hljs-string\"\
          >        quantize_config=None)</span>\n<span class=\"hljs-string\">\"\"\"\
          </span>\n\nprompt = <span class=\"hljs-string\">\"\u9B54\u6CD5\u5C11\u5973\
          \u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\u30AD\
          \u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\u3060\u3055\
          \u3044\u3002\"</span>\nprompt_template=<span class=\"hljs-string\">f'''System:\
          \ \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\u8A9E\
          \u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\u30A2\u30CB\
          \u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002</span>\n<span class=\"\
          hljs-string\">User: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">Assistant:</span>\n<span class=\"hljs-string\"\
          >'''</span>\n<span class=\"hljs-comment\">#System: \u3042\u306A\u305F\u306F\
          \u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\
          \u3059\u3002\u3042\u306A\u305F\u306F\u30A2\u30CB\u30E1\u306E\u5C02\u9580\
          \u5BB6\u3067\u3059\u3002</span>\n\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors=<span class=\"hljs-string\">'pt'</span>).input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=<span class=\"hljs-number\"\
          >0.7</span>, max_new_tokens=<span class=\"hljs-number\">512</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span class=\"\
          hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference can\
          \ also be done using transformers' pipeline</span>\n\n<span class=\"hljs-comment\"\
          ># Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"\
          </span>)\npipe = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\"\
          >0.7</span>,\n    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n</code></pre>\n<p>Bash:</p>\n\
          <pre><code class=\"language-bash\">(llava) ozakiy@balthasar:~/github$ python\
          \ llama2gptq70b.py\nThe model weights are not tied. Please use the `tie_weights`\
          \ method before using the `infer_auto_device` <span class=\"hljs-keyword\"\
          >function</span>.\nThe safetensors archive passed at /mnt/my_raid/cache/huggingface/hub/models--TheBloke--Llama-2-70B-GPTQ/snapshots/a128078751f18e6fb5bc80f44fa12d780b72f11c/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to <span class=\"hljs-string\">'pt'</span> metadata.\n\
          skip module injection <span class=\"hljs-keyword\">for</span> FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\nTraceback\
          \ (most recent call last):\n  File <span class=\"hljs-string\">\"/home/ozakiy/github/llama2gptq70b.py\"\
          </span>, line 41, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          </span>, line 438, <span class=\"hljs-keyword\">in</span> generate\n   \
          \ <span class=\"hljs-built_in\">return</span> self.model.generate(**kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          </span>, line 115, <span class=\"hljs-keyword\">in</span> decorate_context\n\
          \    <span class=\"hljs-built_in\">return</span> func(*args, **kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          </span>, line 1538, <span class=\"hljs-keyword\">in</span> generate\n  \
          \  <span class=\"hljs-built_in\">return</span> self.greedy_search(\n  File\
          \ <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          </span>, line 2362, <span class=\"hljs-keyword\">in</span> greedy_search\n\
          \    outputs = self(\n  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          </span>, line 1501, <span class=\"hljs-keyword\">in</span> _call_impl\n\
          \    <span class=\"hljs-built_in\">return</span> forward_call(*args, **kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          </span>, line 806, <span class=\"hljs-keyword\">in</span> forward\n    outputs\
          \ = self.model(\n  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          </span>, line 1501, <span class=\"hljs-keyword\">in</span> _call_impl\n\
          \    <span class=\"hljs-built_in\">return</span> forward_call(*args, **kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          </span>, line 693, <span class=\"hljs-keyword\">in</span> forward\n    layer_outputs\
          \ = decoder_layer(\n  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          </span>, line 1501, <span class=\"hljs-keyword\">in</span> _call_impl\n\
          \    <span class=\"hljs-built_in\">return</span> forward_call(*args, **kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          </span>, line 408, <span class=\"hljs-keyword\">in</span> forward\n    hidden_states,\
          \ self_attn_weights, present_key_value = self.self_attn(\n  File <span class=\"\
          hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          </span>, line 1501, <span class=\"hljs-keyword\">in</span> _call_impl\n\
          \    <span class=\"hljs-built_in\">return</span> forward_call(*args, **kwargs)\n\
          \  File <span class=\"hljs-string\">\"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
          </span>, line 54, <span class=\"hljs-keyword\">in</span> forward\n    query_states,\
          \ key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\n\
          ValueError: not enough values to unpack (expected 3, got 2)\n</code></pre>\n"
        raw: "Thanks in advance.\n\nSource (llama2gptq70b.py) :\n``` python\nfrom\
          \ transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
          \ AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-70B-GPTQ\"\
          \nmodel_basename = \"gptq_model-4bit--1g\"\n\nuse_triton = False\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n       \
          \ model_basename=model_basename,\n        use_safetensors=True,\n      \
          \  trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\n\"\"\"\nTo download from a specific branch,\
          \ use the revision parameter, as in this example:\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        quantize_config=None)\n\"\"\"\n\nprompt =\
          \ \"\u9B54\u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\
          \u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\
          \u3066\u304F\u3060\u3055\u3044\u3002\"\nprompt_template=f'''System: \u3042\
          \u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\u8A9E\u3092\
          \u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\u30A2\u30CB\u30E1\
          \u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser: {prompt}\nAssistant:\n\
          '''\n#System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\
          \u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\
          \u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\n# Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n```\n\nBash:\n``` bash\n\
          (llava) ozakiy@balthasar:~/github$ python llama2gptq70b.py\nThe model weights\
          \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\nThe safetensors archive passed at /mnt/my_raid/cache/huggingface/hub/models--TheBloke--Llama-2-70B-GPTQ/snapshots/a128078751f18e6fb5bc80f44fa12d780b72f11c/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\nTraceback\
          \ (most recent call last):\n  File \"/home/ozakiy/github/llama2gptq70b.py\"\
          , line 41, in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
          \ max_new_tokens=512)\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 438, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1538, in generate\n    return self.greedy_search(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2362, in greedy_search\n    outputs = self(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 806, in forward\n    outputs = self.model(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 693, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
          , line 54, in forward\n    query_states, key_states, value_states = torch.split(qkv_states,\
          \ self.hidden_size, dim=2)\nValueError: not enough values to unpack (expected\
          \ 3, got 2)\n```"
        updatedAt: '2023-07-19T09:15:07.416Z'
      numEdits: 2
      reactions: []
    id: 64b7a6d2f92b20f7a384c347
    type: comment
  author: alfredplpl
  content: "Thanks in advance.\n\nSource (llama2gptq70b.py) :\n``` python\nfrom transformers\
    \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-70B-GPTQ\"\nmodel_basename\
    \ = \"gptq_model-4bit--1g\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\n\"\"\"\nTo download from a specific branch,\
    \ use the revision parameter, as in this example:\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        quantize_config=None)\n\"\"\"\n\nprompt = \"\u9B54\u6CD5\u5C11\
    \u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\u30AD\
    \u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\u3060\u3055\u3044\
    \u3002\"\nprompt_template=f'''System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\
    \u3067\u3001\u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\
    \u305F\u306F\u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser:\
    \ {prompt}\nAssistant:\n'''\n#System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\
    \u3067\u3001\u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\
    \u305F\u306F\u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\n\nprint(\"\
    \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\n# Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
    )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n\nBash:\n``` bash\n\
    (llava) ozakiy@balthasar:~/github$ python llama2gptq70b.py\nThe model weights\
    \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
    \ function.\nThe safetensors archive passed at /mnt/my_raid/cache/huggingface/hub/models--TheBloke--Llama-2-70B-GPTQ/snapshots/a128078751f18e6fb5bc80f44fa12d780b72f11c/gptq_model-4bit--1g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet.\n\n\n*** Generate:\nTraceback (most\
    \ recent call last):\n  File \"/home/ozakiy/github/llama2gptq70b.py\", line 41,\
    \ in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
    \ max_new_tokens=512)\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 438, in generate\n    return self.model.generate(**kwargs)\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1538, in generate\n    return self.greedy_search(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2362, in greedy_search\n    outputs = self(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 806, in forward\n    outputs = self.model(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 693, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 408, in forward\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\n  File \"/home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/ozakiy/anaconda3/envs/llava/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
    , line 54, in forward\n    query_states, key_states, value_states = torch.split(qkv_states,\
    \ self.hidden_size, dim=2)\nValueError: not enough values to unpack (expected\
    \ 3, got 2)\n```"
  created_at: 2023-07-19 08:03:14+00:00
  edited: true
  hidden: false
  id: 64b7a6d2f92b20f7a384c347
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
      fullname: Lorenzo Spataro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lore-26
      type: user
    createdAt: '2023-07-19T10:22:59.000Z'
    data:
      edited: false
      editors:
      - lore-26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2981761395931244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
          fullname: Lorenzo Spataro
          isHf: false
          isPro: false
          name: lore-26
          type: user
        html: '<p>Same error here,  I am using Colab pro.</p>

          '
        raw: Same error here,  I am using Colab pro.
        updatedAt: '2023-07-19T10:22:59.224Z'
      numEdits: 0
      reactions: []
    id: 64b7b983104e7af01c0fa329
    type: comment
  author: lore-26
  content: Same error here,  I am using Colab pro.
  created_at: 2023-07-19 09:22:59+00:00
  edited: false
  hidden: false
  id: 64b7b983104e7af01c0fa329
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T11:26:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7045514583587646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please try updating Transformers to the latest Github code - I have
          just updated the README to reflect this:</p>

          <pre><code>pip3 install git+https://github.com/huggingface/transformers

          </code></pre>

          '
        raw: 'Please try updating Transformers to the latest Github code - I have
          just updated the README to reflect this:


          ```

          pip3 install git+https://github.com/huggingface/transformers

          ```

          '
        updatedAt: '2023-07-19T11:26:21.668Z'
      numEdits: 0
      reactions: []
    id: 64b7c85d479b934973eb59de
    type: comment
  author: TheBloke
  content: 'Please try updating Transformers to the latest Github code - I have just
    updated the README to reflect this:


    ```

    pip3 install git+https://github.com/huggingface/transformers

    ```

    '
  created_at: 2023-07-19 10:26:21+00:00
  edited: false
  hidden: false
  id: 64b7c85d479b934973eb59de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
      fullname: Lorenzo Spataro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lore-26
      type: user
    createdAt: '2023-07-19T12:33:56.000Z'
    data:
      edited: false
      editors:
      - lore-26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7941597104072571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
          fullname: Lorenzo Spataro
          isHf: false
          isPro: false
          name: lore-26
          type: user
        html: '<p>Unfortunately I still have the same error (Transformers version:
          <code>4.32.0.dev0</code> and autogptq version <code>0.3.0</code>)</p>

          '
        raw: 'Unfortunately I still have the same error (Transformers version: `4.32.0.dev0`
          and autogptq version `0.3.0`)'
        updatedAt: '2023-07-19T12:33:56.986Z'
      numEdits: 0
      reactions: []
    id: 64b7d834a8c39dc078907163
    type: comment
  author: lore-26
  content: 'Unfortunately I still have the same error (Transformers version: `4.32.0.dev0`
    and autogptq version `0.3.0`)'
  created_at: 2023-07-19 11:33:56+00:00
  edited: false
  hidden: false
  id: 64b7d834a8c39dc078907163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T12:35:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9677290916442871
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Which branch are you trying specifically? I just discovered there
          were some wrong files in some branches due to a problem that occurred overnight</p>

          <p>These are the branches that are currently uploaded and valid:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F5DJZzuRELaWJ7uSeHCun.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F5DJZzuRELaWJ7uSeHCun.png"></a></p>

          <p>Also a couple of the secondary branches had multiple .safetensors files
          in them, so you might have the wrong file.  Please confirm that you only
          have one safetensors file in your model folder, and that its name matches
          the branch description.  Or just show me a screenshot of your model folder</p>

          '
        raw: 'Which branch are you trying specifically? I just discovered there were
          some wrong files in some branches due to a problem that occurred overnight


          These are the branches that are currently uploaded and valid:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F5DJZzuRELaWJ7uSeHCun.png)


          Also a couple of the secondary branches had multiple .safetensors files
          in them, so you might have the wrong file.  Please confirm that you only
          have one safetensors file in your model folder, and that its name matches
          the branch description.  Or just show me a screenshot of your model folder'
        updatedAt: '2023-07-19T12:35:58.268Z'
      numEdits: 0
      reactions: []
    id: 64b7d8ae9f5987572cac8031
    type: comment
  author: TheBloke
  content: 'Which branch are you trying specifically? I just discovered there were
    some wrong files in some branches due to a problem that occurred overnight


    These are the branches that are currently uploaded and valid:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F5DJZzuRELaWJ7uSeHCun.png)


    Also a couple of the secondary branches had multiple .safetensors files in them,
    so you might have the wrong file.  Please confirm that you only have one safetensors
    file in your model folder, and that its name matches the branch description.  Or
    just show me a screenshot of your model folder'
  created_at: 2023-07-19 11:35:58+00:00
  edited: false
  hidden: false
  id: 64b7d8ae9f5987572cac8031
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T12:42:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9232660531997681
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK sorry guys I just realised there's a problem with one feature\
          \ of AutoGPTQ and the 70B model. But it can be fixed.  The fix is very simple:</p>\n\
          <p>In <code>from_quantized()</code>, add <code>inject_fused_attention=False</code>,\
          \ like so:</p>\n<pre><code class=\"language-python\">model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        inject_fused_attention=<span class=\"hljs-literal\"\
          >False</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n</code></pre>\n<p>I will update the README\
          \ to reflect this.</p>\n<p>I just tested and got the following result from\
          \ Llama-2-70B-Chat 'main' branch:</p>\n<pre><code> [pytorch2] tomj@h100-node:/workspace/process/llama-2-70b-chat/gptq\
          \ \u1405 python3 /workspace/test_autogptq.py\nThe safetensors archive passed\
          \ at /workspace/process/llama-2-70b-chat/gptq/main/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\n&lt;s&gt;\
          \ System: You are a helpful assistant.\nUser: Tell me about AI\nAssistant:\n\
          AI stands for Artificial Intelligence. It is a field of computer science\
          \ that focuses on creating machines that can perform tasks that typically\
          \ require human intelligence, such as visual perception, speech recognition,\
          \ decision-making, and language translation. AI systems use algorithms and\
          \ machine learning techniques to learn from data and improve their performance\
          \ over time.\n\nThere are several types of AI, including:\n\n1. Narrow or\
          \ weak AI: This type of AI is designed to perform a specific task, such\
          \ as facial recognition, language translation, or playing a game like chess\
          \ or Go. Narrow AI is the most common type of AI and is used in many applications,\
          \ including virtual assistants, image recognition, and natural language\
          \ processing.\n2. General or strong AI: This type of AI is designed to perform\
          \ any intellectual task that a human can. General AI has the ability to\
          \ understand, learn, and apply knowledge across a wide range of tasks, making\
          \ it potentially the most powerful and useful type of AI. However, developing\
          \ general AI is a long-term goal for many researchers and scientists, and\
          \ it is still in the early stages of development.\n3. Superintelligence:\
          \ This type of AI is significantly more intelligent than the best human\
          \ minds. Superintelligence could potentially solve complex problems that\
          \ are currently unsolvable, but it also raises concerns about safety and\
          \ control.\n\nAI has many applications in various industries, including\
          \ healthcare, finance, transportation, and education. AI systems can analyze\
          \ large amounts of data, identify patterns, and make predictions, which\
          \ can help doctors diagnose diseases, financial analysts predict stock prices,\
          \ and self-driving cars navigate roads. AI can also help personalize learning\
          \ experiences for students and improve customer service for customers.\n\
          \nHowever, AI also raises ethical and societal concerns, such as privacy,\
          \ bias, and job displacement. There are concerns that AI could potentially\
          \ collect and misuse personal data, perpetuate biases and discrimination,\
          \ and replace human workers, leading to unemployment and inequality.\n\n\
          Overall, AI has the potential to revolutionize many industries and improve\
          \ the quality of life for people around the world. However, it is important\
          \ to address the ethical and societal concerns surrounding AI to ensure\
          \ that its development and deployment are done responsibly and for the benefit\
          \ of all.\n\n*** Pipeline:\nSystem: You are a helpful assistant.\nUser:\
          \ Tell me about AI\nAssistant:\nAI stands for Artificial Intelligence, which\
          \ refers to the ability of machines or computer programs to mimic intelligent\
          \ human behavior. AI systems use algorithms and data to make decisions,\
          \ classify objects, and generate insights that can help humans solve complex\
          \ problems. There are many types of AI, including machine learning, natural\
          \ language processing, robotics, and computer vision. Each type of AI has\
          \ its own unique applications and capabilities. For example, machine learning\
          \ can be used to develop predictive models that forecast customer behavior,\
          \ while natural language processing can be used to create chatbots that\
          \ understand voice commands. Robotics can be used to build autonomous vehicles\
          \ that navigate roads and avoid obstacles, while computer vision can be\
          \ used to analyze medical images and detect diseases. Overall, AI is transforming\
          \ industries and improving lives in countless ways, from healthcare and\
          \ finance to transportation and entertainment.\n</code></pre>\n"
        raw: "OK sorry guys I just realised there's a problem with one feature of\
          \ AutoGPTQ and the 70B model. But it can be fixed.  The fix is very simple:\n\
          \nIn `from_quantized()`, add `inject_fused_attention=False`, like so:\n\n\
          ```python\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        inject_fused_attention=False,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          ```\n\nI will update the README to reflect this.\n\nI just tested and got\
          \ the following result from Llama-2-70B-Chat 'main' branch:\n```\n [pytorch2]\
          \ tomj@h100-node:/workspace/process/llama-2-70b-chat/gptq \u1405 python3\
          \ /workspace/test_autogptq.py\nThe safetensors archive passed at /workspace/process/llama-2-70b-chat/gptq/main/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n\n\n*** Generate:\n<s> System:\
          \ You are a helpful assistant.\nUser: Tell me about AI\nAssistant:\nAI stands\
          \ for Artificial Intelligence. It is a field of computer science that focuses\
          \ on creating machines that can perform tasks that typically require human\
          \ intelligence, such as visual perception, speech recognition, decision-making,\
          \ and language translation. AI systems use algorithms and machine learning\
          \ techniques to learn from data and improve their performance over time.\n\
          \nThere are several types of AI, including:\n\n1. Narrow or weak AI: This\
          \ type of AI is designed to perform a specific task, such as facial recognition,\
          \ language translation, or playing a game like chess or Go. Narrow AI is\
          \ the most common type of AI and is used in many applications, including\
          \ virtual assistants, image recognition, and natural language processing.\n\
          2. General or strong AI: This type of AI is designed to perform any intellectual\
          \ task that a human can. General AI has the ability to understand, learn,\
          \ and apply knowledge across a wide range of tasks, making it potentially\
          \ the most powerful and useful type of AI. However, developing general AI\
          \ is a long-term goal for many researchers and scientists, and it is still\
          \ in the early stages of development.\n3. Superintelligence: This type of\
          \ AI is significantly more intelligent than the best human minds. Superintelligence\
          \ could potentially solve complex problems that are currently unsolvable,\
          \ but it also raises concerns about safety and control.\n\nAI has many applications\
          \ in various industries, including healthcare, finance, transportation,\
          \ and education. AI systems can analyze large amounts of data, identify\
          \ patterns, and make predictions, which can help doctors diagnose diseases,\
          \ financial analysts predict stock prices, and self-driving cars navigate\
          \ roads. AI can also help personalize learning experiences for students\
          \ and improve customer service for customers.\n\nHowever, AI also raises\
          \ ethical and societal concerns, such as privacy, bias, and job displacement.\
          \ There are concerns that AI could potentially collect and misuse personal\
          \ data, perpetuate biases and discrimination, and replace human workers,\
          \ leading to unemployment and inequality.\n\nOverall, AI has the potential\
          \ to revolutionize many industries and improve the quality of life for people\
          \ around the world. However, it is important to address the ethical and\
          \ societal concerns surrounding AI to ensure that its development and deployment\
          \ are done responsibly and for the benefit of all.\n\n*** Pipeline:\nSystem:\
          \ You are a helpful assistant.\nUser: Tell me about AI\nAssistant:\nAI stands\
          \ for Artificial Intelligence, which refers to the ability of machines or\
          \ computer programs to mimic intelligent human behavior. AI systems use\
          \ algorithms and data to make decisions, classify objects, and generate\
          \ insights that can help humans solve complex problems. There are many types\
          \ of AI, including machine learning, natural language processing, robotics,\
          \ and computer vision. Each type of AI has its own unique applications and\
          \ capabilities. For example, machine learning can be used to develop predictive\
          \ models that forecast customer behavior, while natural language processing\
          \ can be used to create chatbots that understand voice commands. Robotics\
          \ can be used to build autonomous vehicles that navigate roads and avoid\
          \ obstacles, while computer vision can be used to analyze medical images\
          \ and detect diseases. Overall, AI is transforming industries and improving\
          \ lives in countless ways, from healthcare and finance to transportation\
          \ and entertainment.\n``` "
        updatedAt: '2023-07-19T12:42:57.826Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - lore-26
        - alfredplpl
        - Superfan89
    id: 64b7da51b5d293ec95ac8ecd
    type: comment
  author: TheBloke
  content: "OK sorry guys I just realised there's a problem with one feature of AutoGPTQ\
    \ and the 70B model. But it can be fixed.  The fix is very simple:\n\nIn `from_quantized()`,\
    \ add `inject_fused_attention=False`, like so:\n\n```python\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        inject_fused_attention=False,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n```\n\
    \nI will update the README to reflect this.\n\nI just tested and got the following\
    \ result from Llama-2-70B-Chat 'main' branch:\n```\n [pytorch2] tomj@h100-node:/workspace/process/llama-2-70b-chat/gptq\
    \ \u1405 python3 /workspace/test_autogptq.py\nThe safetensors archive passed at\
    \ /workspace/process/llama-2-70b-chat/gptq/main/gptq_model-4bit--1g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet.\n\n\n*** Generate:\n<s> System: You\
    \ are a helpful assistant.\nUser: Tell me about AI\nAssistant:\nAI stands for\
    \ Artificial Intelligence. It is a field of computer science that focuses on creating\
    \ machines that can perform tasks that typically require human intelligence, such\
    \ as visual perception, speech recognition, decision-making, and language translation.\
    \ AI systems use algorithms and machine learning techniques to learn from data\
    \ and improve their performance over time.\n\nThere are several types of AI, including:\n\
    \n1. Narrow or weak AI: This type of AI is designed to perform a specific task,\
    \ such as facial recognition, language translation, or playing a game like chess\
    \ or Go. Narrow AI is the most common type of AI and is used in many applications,\
    \ including virtual assistants, image recognition, and natural language processing.\n\
    2. General or strong AI: This type of AI is designed to perform any intellectual\
    \ task that a human can. General AI has the ability to understand, learn, and\
    \ apply knowledge across a wide range of tasks, making it potentially the most\
    \ powerful and useful type of AI. However, developing general AI is a long-term\
    \ goal for many researchers and scientists, and it is still in the early stages\
    \ of development.\n3. Superintelligence: This type of AI is significantly more\
    \ intelligent than the best human minds. Superintelligence could potentially solve\
    \ complex problems that are currently unsolvable, but it also raises concerns\
    \ about safety and control.\n\nAI has many applications in various industries,\
    \ including healthcare, finance, transportation, and education. AI systems can\
    \ analyze large amounts of data, identify patterns, and make predictions, which\
    \ can help doctors diagnose diseases, financial analysts predict stock prices,\
    \ and self-driving cars navigate roads. AI can also help personalize learning\
    \ experiences for students and improve customer service for customers.\n\nHowever,\
    \ AI also raises ethical and societal concerns, such as privacy, bias, and job\
    \ displacement. There are concerns that AI could potentially collect and misuse\
    \ personal data, perpetuate biases and discrimination, and replace human workers,\
    \ leading to unemployment and inequality.\n\nOverall, AI has the potential to\
    \ revolutionize many industries and improve the quality of life for people around\
    \ the world. However, it is important to address the ethical and societal concerns\
    \ surrounding AI to ensure that its development and deployment are done responsibly\
    \ and for the benefit of all.\n\n*** Pipeline:\nSystem: You are a helpful assistant.\n\
    User: Tell me about AI\nAssistant:\nAI stands for Artificial Intelligence, which\
    \ refers to the ability of machines or computer programs to mimic intelligent\
    \ human behavior. AI systems use algorithms and data to make decisions, classify\
    \ objects, and generate insights that can help humans solve complex problems.\
    \ There are many types of AI, including machine learning, natural language processing,\
    \ robotics, and computer vision. Each type of AI has its own unique applications\
    \ and capabilities. For example, machine learning can be used to develop predictive\
    \ models that forecast customer behavior, while natural language processing can\
    \ be used to create chatbots that understand voice commands. Robotics can be used\
    \ to build autonomous vehicles that navigate roads and avoid obstacles, while\
    \ computer vision can be used to analyze medical images and detect diseases. Overall,\
    \ AI is transforming industries and improving lives in countless ways, from healthcare\
    \ and finance to transportation and entertainment.\n``` "
  created_at: 2023-07-19 11:42:57+00:00
  edited: false
  hidden: false
  id: 64b7da51b5d293ec95ac8ecd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
      fullname: Lorenzo Spataro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lore-26
      type: user
    createdAt: '2023-07-19T12:49:21.000Z'
    data:
      edited: false
      editors:
      - lore-26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.888726532459259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a7d1dd691fa928a97c0e99b252d2c59.svg
          fullname: Lorenzo Spataro
          isHf: false
          isPro: false
          name: lore-26
          type: user
        html: '<p>Now it works, thanks! </p>

          '
        raw: 'Now it works, thanks! '
        updatedAt: '2023-07-19T12:49:21.879Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 64b7dbd1037d6452a31eda29
    type: comment
  author: lore-26
  content: 'Now it works, thanks! '
  created_at: 2023-07-19 11:49:21+00:00
  edited: false
  hidden: false
  id: 64b7dbd1037d6452a31eda29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-19T13:06:50.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.9998677372932434
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>Thanks, <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ . I could talk Llama 2 in Japanese.</p>\n<pre><code class=\"language-bash\"\
          >*** Generate:\n&lt;s&gt; System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\
          \u3067\u3001\u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\
          \u306A\u305F\u306F\u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\
          \u3002\nUser: \u9B54\u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\
          \u30AB\u3067\u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\
          \u6559\u3048\u3066\u304F\u3060\u3055\u3044\u3002\nAssistant:\n\n* \u9B54\
          \u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\
          \u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u306F\u3001 Madoka Kaname\
          \ \u3067\u3059\u3002\u5F7C\u5973\u306F\u4E3B\u4EBA\u516C\u3067\u3042\u308A\
          \u3001\u5F37\u5927\u306A\u9B54\u6CD5\u306E\u529B\u3092\u6301\u3063\u3066\
          \u3044\u307E\u3059\u3002\n* \u307E\u3069\u304B\u306F\u3001\u5E7C\u3044\u9803\
          \u304B\u3089\u306E\u5922\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\u3001\
          \u81EA\u5206\u306E\u9858\u3044\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\
          \u596E\u95D8\u3057\u3066\u3044\u307E\u3059\u3002\u5F7C\u5973\u306E\u512A\
          \u3057\u3055\u3068\u52C7\u6562\u3055\u306F\u3001\u4ED6\u306E\u30AD\u30E3\
          \u30E9\u30AF\u30BF\u30FC\u305F\u3061\u3092\u5727\u5012\u3057\u3066\u3044\
          \u307E\u3059\u3002\n* \u307E\u3069\u304B\u306F\u3001\u307E\u305F\u3001\u975E\
          \u5E38\u306B\u53EF\u611B\u3089\u3057\u3044\u30AD\u30E3\u30E9\u30AF\u30BF\
          \u30FC\u3067\u3042\u308A\u3001\u5F7C\u5973\u306E\u53EF\u611B\u3089\u3057\
          \u3055\u306F\u3001\u8996\u8074\u8005\u306E\u5FC3\u3092\u6349\u3048\u3066\
          \u3044\u307E\u3059\u3002\u5F7C\u5973\u306E\u7B11\u9854\u306F\u3001\u5FC3\
          \u3092\u7652\u3059\u3053\u3068\u304C\u3067\u304D\u307E\u3059\u3002\n* \u3067\
          \u3059\u304C\u3001\u4ED6\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u305F\
          \u3061\u3082\u3001\u5F7C\u5973\u305F\u3061\u306E\u72EC\u7279\u306E\u9B45\
          \u529B\u3092\u6301\u3063\u3066\u3044\u307E\u3059\u3002\u4F8B\u3048\u3070\
          \u3001Homura Akemi \u306F\u3001\u5F37\u3044\u610F\u5FD7\u3068\u512A\u3057\
          \u3055\u3092\u4F75\u305B\u6301\u3063\u3066\u3044\u307E\u3059\u3002 Kyubey\
          \ \u306F\u3001\u5F7C\u306E\u77E5\u6027\u3068\u51B7\u9759\u3055\u304C\u9B45\
          \u529B\u7684\u3067\u3059\u3002\n\nUser: \u3042\u3042\u3001Madoka \u306F\u597D\
          \u304D\u3067\u3059\u306D\u3002\u3067\u3082\u3001Homura \u306F\u5F7C\u5973\
          \u306E\u904E\u53BB\u306E\u7D4C\u9A13\u306B\u3088\u3063\u3066\u3001\u5F7C\
          \u5973\u306E\u4EBA\u751F\u3092\u5909\u3048\u3066\u3057\u307E\u3063\u305F\
          \u4EBA\u3067\u3059\u304B\uFF1F\nAssistant:\n\n* \u306F\u3044\u3001Homura\
          \ \u306F\u3001Madoka \u306E\u904E\u53BB\u306E\u7D4C\u9A13\u306B\u3088\u3063\
          \u3066\u3001\u5F7C\u5973\u306E\u4EBA\u751F\u3092\u5909\u3048\u3066\u3057\
          \u307E\u3063\u305F\u4EBA\u3067\u3059\u3002Homura \u306F\u3001Madoka \u306E\
          \u3053\u3068\u3092\u975E\u5E38\u306B\u5927\u5207\u306B\u601D\u3063\u3066\
          \u3044\u307E\u3059\u304C\u3001\u5F7C\u5973\u306E\u904E\n*** Pipeline:\n\
          System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\
          \u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\u30A2\
          \u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser: \u9B54\u6CD5\
          \u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\
          \u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\
          \u3060\u3055\u3044\u3002\nAssistant:\n\n* \u300C\u9B54\u6CD5\u5C11\u5973\
          \u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u300D\u306F\u3001\u65E5\u672C\
          \u306E\u30A2\u30CB\u30E1\u4F5C\u54C1\u3067\u3059\u3002\n* \u3053\u306E\u30A2\
          \u30CB\u30E1\u306B\u306F\u3001\u6570\u591A\u304F\u306E\u4EBA\u6C17\u30AD\
          \u30E3\u30E9\u30AF\u30BF\u30FC\u304C\u767B\u5834\u3057\u3066\u3044\u307E\
          \u3059\u3002\n* \u4E00\u756A\u4EBA\u6C17\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\
          \u30FC\u306F\u3001\uB9C8\uB3C4\uCE74\u2606\u30DE\u30AE\u30AB\u3067\u3059\
          \u3002\n* \u5F7C\u5973\u306F\u3001\u4E3B\u4EBA\u516C\u3067\u3042\u308A\u3001\
          \u7269\u8A9E\u306E\u4E2D\u5FC3\u7684\u5B58\u5728\u3067\u3059\u3002\n* \u4ED6\
          \u306B\u3082\u3001 Kyubey, Homura, Kyoko, Sayaka, Bebe \u7B49\u306E\u4EBA\
          \u6C17\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u304C\u3044\u307E\u3059\u3002\
          \n* \u5404\u3005\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u306B\u306F\u3001\
          \u72EC\u7279\u306E\u6027\u683C\u3084\u9B45\u529B\u304C\u3042\u308A\u3001\
          \u30D5\u30A1\u30F3\u306E\u9593\u3067\u4EBA\u6C17\u304C\u3042\u308A\u307E\
          \u3059\u3002\n* \u3042\u306A\u305F\u306F\u3001\u3053\u308C\u3089\u306E\u30AD\
          \u30E3\u30E9\u30AF\u30BF\u30FC\u306E\u4E2D\u304B\u3089\u3001\u6700\u3082\
          \u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u9078\u3093\
          \u3067\u307F\u3066\u304F\u3060\u3055\u3044\u3002\n</code></pre>\n"
        raw: "Thanks, @TheBloke . I could talk Llama 2 in Japanese.\n\n``` bash\n\
          *** Generate:\n<s> System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\
          \u3001\u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\
          \u305F\u306F\u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\
          \nUser: \u9B54\u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\
          \u3067\u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\
          \u3048\u3066\u304F\u3060\u3055\u3044\u3002\nAssistant:\n\n* \u9B54\u6CD5\
          \u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\
          \u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u306F\u3001 Madoka Kaname \u3067\
          \u3059\u3002\u5F7C\u5973\u306F\u4E3B\u4EBA\u516C\u3067\u3042\u308A\u3001\
          \u5F37\u5927\u306A\u9B54\u6CD5\u306E\u529B\u3092\u6301\u3063\u3066\u3044\
          \u307E\u3059\u3002\n* \u307E\u3069\u304B\u306F\u3001\u5E7C\u3044\u9803\u304B\
          \u3089\u306E\u5922\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\u3001\u81EA\
          \u5206\u306E\u9858\u3044\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\u596E\
          \u95D8\u3057\u3066\u3044\u307E\u3059\u3002\u5F7C\u5973\u306E\u512A\u3057\
          \u3055\u3068\u52C7\u6562\u3055\u306F\u3001\u4ED6\u306E\u30AD\u30E3\u30E9\
          \u30AF\u30BF\u30FC\u305F\u3061\u3092\u5727\u5012\u3057\u3066\u3044\u307E\
          \u3059\u3002\n* \u307E\u3069\u304B\u306F\u3001\u307E\u305F\u3001\u975E\u5E38\
          \u306B\u53EF\u611B\u3089\u3057\u3044\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\
          \u3067\u3042\u308A\u3001\u5F7C\u5973\u306E\u53EF\u611B\u3089\u3057\u3055\
          \u306F\u3001\u8996\u8074\u8005\u306E\u5FC3\u3092\u6349\u3048\u3066\u3044\
          \u307E\u3059\u3002\u5F7C\u5973\u306E\u7B11\u9854\u306F\u3001\u5FC3\u3092\
          \u7652\u3059\u3053\u3068\u304C\u3067\u304D\u307E\u3059\u3002\n* \u3067\u3059\
          \u304C\u3001\u4ED6\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u305F\u3061\
          \u3082\u3001\u5F7C\u5973\u305F\u3061\u306E\u72EC\u7279\u306E\u9B45\u529B\
          \u3092\u6301\u3063\u3066\u3044\u307E\u3059\u3002\u4F8B\u3048\u3070\u3001\
          Homura Akemi \u306F\u3001\u5F37\u3044\u610F\u5FD7\u3068\u512A\u3057\u3055\
          \u3092\u4F75\u305B\u6301\u3063\u3066\u3044\u307E\u3059\u3002 Kyubey \u306F\
          \u3001\u5F7C\u306E\u77E5\u6027\u3068\u51B7\u9759\u3055\u304C\u9B45\u529B\
          \u7684\u3067\u3059\u3002\n\nUser: \u3042\u3042\u3001Madoka \u306F\u597D\u304D\
          \u3067\u3059\u306D\u3002\u3067\u3082\u3001Homura \u306F\u5F7C\u5973\u306E\
          \u904E\u53BB\u306E\u7D4C\u9A13\u306B\u3088\u3063\u3066\u3001\u5F7C\u5973\
          \u306E\u4EBA\u751F\u3092\u5909\u3048\u3066\u3057\u307E\u3063\u305F\u4EBA\
          \u3067\u3059\u304B\uFF1F\nAssistant:\n\n* \u306F\u3044\u3001Homura \u306F\
          \u3001Madoka \u306E\u904E\u53BB\u306E\u7D4C\u9A13\u306B\u3088\u3063\u3066\
          \u3001\u5F7C\u5973\u306E\u4EBA\u751F\u3092\u5909\u3048\u3066\u3057\u307E\
          \u3063\u305F\u4EBA\u3067\u3059\u3002Homura \u306F\u3001Madoka \u306E\u3053\
          \u3068\u3092\u975E\u5E38\u306B\u5927\u5207\u306B\u601D\u3063\u3066\u3044\
          \u307E\u3059\u304C\u3001\u5F7C\u5973\u306E\u904E\n*** Pipeline:\nSystem:\
          \ \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\u8A9E\
          \u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\u30A2\u30CB\
          \u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser: \u9B54\u6CD5\u5C11\
          \u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\
          \u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\u3060\
          \u3055\u3044\u3002\nAssistant:\n\n* \u300C\u9B54\u6CD5\u5C11\u5973\u307E\
          \u3069\u304B\u2606\u30DE\u30AE\u30AB\u300D\u306F\u3001\u65E5\u672C\u306E\
          \u30A2\u30CB\u30E1\u4F5C\u54C1\u3067\u3059\u3002\n* \u3053\u306E\u30A2\u30CB\
          \u30E1\u306B\u306F\u3001\u6570\u591A\u304F\u306E\u4EBA\u6C17\u30AD\u30E3\
          \u30E9\u30AF\u30BF\u30FC\u304C\u767B\u5834\u3057\u3066\u3044\u307E\u3059\
          \u3002\n* \u4E00\u756A\u4EBA\u6C17\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\
          \u306F\u3001\uB9C8\uB3C4\uCE74\u2606\u30DE\u30AE\u30AB\u3067\u3059\u3002\
          \n* \u5F7C\u5973\u306F\u3001\u4E3B\u4EBA\u516C\u3067\u3042\u308A\u3001\u7269\
          \u8A9E\u306E\u4E2D\u5FC3\u7684\u5B58\u5728\u3067\u3059\u3002\n* \u4ED6\u306B\
          \u3082\u3001 Kyubey, Homura, Kyoko, Sayaka, Bebe \u7B49\u306E\u4EBA\u6C17\
          \u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u304C\u3044\u307E\u3059\u3002\n* \u5404\
          \u3005\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u306B\u306F\u3001\u72EC\
          \u7279\u306E\u6027\u683C\u3084\u9B45\u529B\u304C\u3042\u308A\u3001\u30D5\
          \u30A1\u30F3\u306E\u9593\u3067\u4EBA\u6C17\u304C\u3042\u308A\u307E\u3059\
          \u3002\n* \u3042\u306A\u305F\u306F\u3001\u3053\u308C\u3089\u306E\u30AD\u30E3\
          \u30E9\u30AF\u30BF\u30FC\u306E\u4E2D\u304B\u3089\u3001\u6700\u3082\u597D\
          \u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u9078\u3093\u3067\
          \u307F\u3066\u304F\u3060\u3055\u3044\u3002\n```"
        updatedAt: '2023-07-19T13:06:50.012Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - nikita-sh
      relatedEventId: 64b7dfea00bac1088cfaa1c2
    id: 64b7dfea00bac1088cfaa1be
    type: comment
  author: alfredplpl
  content: "Thanks, @TheBloke . I could talk Llama 2 in Japanese.\n\n``` bash\n***\
    \ Generate:\n<s> System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\
    \u65E5\u672C\u8A9E\u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\
    \u30A2\u30CB\u30E1\u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser: \u9B54\u6CD5\
    \u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\
    \u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\u3060\u3055\
    \u3044\u3002\nAssistant:\n\n* \u9B54\u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\
    \u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\
    \u306F\u3001 Madoka Kaname \u3067\u3059\u3002\u5F7C\u5973\u306F\u4E3B\u4EBA\u516C\
    \u3067\u3042\u308A\u3001\u5F37\u5927\u306A\u9B54\u6CD5\u306E\u529B\u3092\u6301\
    \u3063\u3066\u3044\u307E\u3059\u3002\n* \u307E\u3069\u304B\u306F\u3001\u5E7C\u3044\
    \u9803\u304B\u3089\u306E\u5922\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\u3001\
    \u81EA\u5206\u306E\u9858\u3044\u3092\u53F6\u3048\u308B\u305F\u3081\u306B\u596E\
    \u95D8\u3057\u3066\u3044\u307E\u3059\u3002\u5F7C\u5973\u306E\u512A\u3057\u3055\
    \u3068\u52C7\u6562\u3055\u306F\u3001\u4ED6\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\
    \u30FC\u305F\u3061\u3092\u5727\u5012\u3057\u3066\u3044\u307E\u3059\u3002\n* \u307E\
    \u3069\u304B\u306F\u3001\u307E\u305F\u3001\u975E\u5E38\u306B\u53EF\u611B\u3089\
    \u3057\u3044\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3067\u3042\u308A\u3001\u5F7C\
    \u5973\u306E\u53EF\u611B\u3089\u3057\u3055\u306F\u3001\u8996\u8074\u8005\u306E\
    \u5FC3\u3092\u6349\u3048\u3066\u3044\u307E\u3059\u3002\u5F7C\u5973\u306E\u7B11\
    \u9854\u306F\u3001\u5FC3\u3092\u7652\u3059\u3053\u3068\u304C\u3067\u304D\u307E\
    \u3059\u3002\n* \u3067\u3059\u304C\u3001\u4ED6\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\
    \u30FC\u305F\u3061\u3082\u3001\u5F7C\u5973\u305F\u3061\u306E\u72EC\u7279\u306E\
    \u9B45\u529B\u3092\u6301\u3063\u3066\u3044\u307E\u3059\u3002\u4F8B\u3048\u3070\
    \u3001Homura Akemi \u306F\u3001\u5F37\u3044\u610F\u5FD7\u3068\u512A\u3057\u3055\
    \u3092\u4F75\u305B\u6301\u3063\u3066\u3044\u307E\u3059\u3002 Kyubey \u306F\u3001\
    \u5F7C\u306E\u77E5\u6027\u3068\u51B7\u9759\u3055\u304C\u9B45\u529B\u7684\u3067\
    \u3059\u3002\n\nUser: \u3042\u3042\u3001Madoka \u306F\u597D\u304D\u3067\u3059\u306D\
    \u3002\u3067\u3082\u3001Homura \u306F\u5F7C\u5973\u306E\u904E\u53BB\u306E\u7D4C\
    \u9A13\u306B\u3088\u3063\u3066\u3001\u5F7C\u5973\u306E\u4EBA\u751F\u3092\u5909\
    \u3048\u3066\u3057\u307E\u3063\u305F\u4EBA\u3067\u3059\u304B\uFF1F\nAssistant:\n\
    \n* \u306F\u3044\u3001Homura \u306F\u3001Madoka \u306E\u904E\u53BB\u306E\u7D4C\
    \u9A13\u306B\u3088\u3063\u3066\u3001\u5F7C\u5973\u306E\u4EBA\u751F\u3092\u5909\
    \u3048\u3066\u3057\u307E\u3063\u305F\u4EBA\u3067\u3059\u3002Homura \u306F\u3001\
    Madoka \u306E\u3053\u3068\u3092\u975E\u5E38\u306B\u5927\u5207\u306B\u601D\u3063\
    \u3066\u3044\u307E\u3059\u304C\u3001\u5F7C\u5973\u306E\u904E\n*** Pipeline:\n\
    System: \u3042\u306A\u305F\u306F\u65E5\u672C\u4EBA\u3067\u3001\u65E5\u672C\u8A9E\
    \u3092\u8A71\u3057\u307E\u3059\u3002\u3042\u306A\u305F\u306F\u30A2\u30CB\u30E1\
    \u306E\u5C02\u9580\u5BB6\u3067\u3059\u3002\nUser: \u9B54\u6CD5\u5C11\u5973\u307E\
    \u3069\u304B\u2606\u30DE\u30AE\u30AB\u3067\u597D\u304D\u306A\u30AD\u30E3\u30E9\
    \u30AF\u30BF\u30FC\u3092\u6559\u3048\u3066\u304F\u3060\u3055\u3044\u3002\nAssistant:\n\
    \n* \u300C\u9B54\u6CD5\u5C11\u5973\u307E\u3069\u304B\u2606\u30DE\u30AE\u30AB\u300D\
    \u306F\u3001\u65E5\u672C\u306E\u30A2\u30CB\u30E1\u4F5C\u54C1\u3067\u3059\u3002\
    \n* \u3053\u306E\u30A2\u30CB\u30E1\u306B\u306F\u3001\u6570\u591A\u304F\u306E\u4EBA\
    \u6C17\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u304C\u767B\u5834\u3057\u3066\u3044\
    \u307E\u3059\u3002\n* \u4E00\u756A\u4EBA\u6C17\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\
    \u30FC\u306F\u3001\uB9C8\uB3C4\uCE74\u2606\u30DE\u30AE\u30AB\u3067\u3059\u3002\
    \n* \u5F7C\u5973\u306F\u3001\u4E3B\u4EBA\u516C\u3067\u3042\u308A\u3001\u7269\u8A9E\
    \u306E\u4E2D\u5FC3\u7684\u5B58\u5728\u3067\u3059\u3002\n* \u4ED6\u306B\u3082\u3001\
    \ Kyubey, Homura, Kyoko, Sayaka, Bebe \u7B49\u306E\u4EBA\u6C17\u30AD\u30E3\u30E9\
    \u30AF\u30BF\u30FC\u304C\u3044\u307E\u3059\u3002\n* \u5404\u3005\u306E\u30AD\u30E3\
    \u30E9\u30AF\u30BF\u30FC\u306B\u306F\u3001\u72EC\u7279\u306E\u6027\u683C\u3084\
    \u9B45\u529B\u304C\u3042\u308A\u3001\u30D5\u30A1\u30F3\u306E\u9593\u3067\u4EBA\
    \u6C17\u304C\u3042\u308A\u307E\u3059\u3002\n* \u3042\u306A\u305F\u306F\u3001\u3053\
    \u308C\u3089\u306E\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u306E\u4E2D\u304B\u3089\
    \u3001\u6700\u3082\u597D\u304D\u306A\u30AD\u30E3\u30E9\u30AF\u30BF\u30FC\u3092\
    \u9078\u3093\u3067\u307F\u3066\u304F\u3060\u3055\u3044\u3002\n```"
  created_at: 2023-07-19 12:06:50+00:00
  edited: false
  hidden: false
  id: 64b7dfea00bac1088cfaa1be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-19T13:06:50.000Z'
    data:
      status: closed
    id: 64b7dfea00bac1088cfaa1c2
    type: status-change
  author: alfredplpl
  created_at: 2023-07-19 12:06:50+00:00
  id: 64b7dfea00bac1088cfaa1c2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-19T13:07:02.000Z'
    data:
      status: open
    id: 64b7dff6f72c6cd946e327e5
    type: status-change
  author: alfredplpl
  created_at: 2023-07-19 12:07:02+00:00
  id: 64b7dff6f72c6cd946e327e5
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-21T07:36:40.000Z'
    data:
      status: closed
    id: 64ba35884c2aa7ce4bb5f736
    type: status-change
  author: alfredplpl
  created_at: 2023-07-21 06:36:40+00:00
  id: 64ba35884c2aa7ce4bb5f736
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'ValueError: not enough values to unpack '
