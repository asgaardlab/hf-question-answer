!!python/object:huggingface_hub.community.DiscussionWithDetails
author: charleyzhuyi
conflicting_files: null
created_at: 2023-07-19 17:09:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
      fullname: Yi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charleyzhuyi
      type: user
    createdAt: '2023-07-19T18:09:09.000Z'
    data:
      edited: false
      editors:
      - charleyzhuyi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5271196961402893
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
          fullname: Yi
          isHf: false
          isPro: false
          name: charleyzhuyi
          type: user
        html: '<p>I''ve  updated text-generation-web-UI to latest (transformers updated
          to 4.31.0) and also manually verified the exllama folder also has been updated
          and  contains this <a rel="nofollow" href="https://github.com/turboderp/exllama/commit/b3aea521859b83cfd889c4c00c05a323313b7fee">https://github.com/turboderp/exllama/commit/b3aea521859b83cfd889c4c00c05a323313b7fee</a>
          commit.</p>

          <p>Exllama is able to load the module, but when I typing, i got:</p>

          <p>Traceback (most recent call last):<br>  File "c:\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 331, in generate_reply_custom<br>    for reply in shared.model.generate_with_streaming(question,
          state):<br>  File "c:\oobabooga_windows\text-generation-webui\modules\exllama.py",
          line 98, in generate_with_streaming<br>    self.generator.gen_begin_reuse(ids)<br>  File
          "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\generator.py",
          line 186, in gen_begin_reuse<br>    self.gen_begin(in_tokens)<br>  File
          "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\generator.py",
          line 171, in gen_begin<br>    self.model.forward(self.sequence[:, :-1],
          self.cache, preprocess_only = True, lora = self.lora)<br>  File "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\model.py",
          line 887, in forward<br>    r = self._forward(input_ids[:, chunk_begin :
          chunk_end],<br>  File "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\model.py",
          line 968, in _forward<br>    hidden_states = decoder_layer.forward(hidden_states,
          cache, buffers[device], lora)<br>  File "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\model.py",
          line 471, in forward<br>    hidden_states = self.self_attn.forward(hidden_states,
          cache, buffer, lora)<br>  File "c:\oobabooga_windows\installer_files\env\lib\site-packages\exllama\model.py",
          line 389, in forward<br>    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,
          self.config.head_dim).transpose(1, 2)<br>RuntimeError: shape ''[1, 64, 64,
          128]'' is invalid for input of size 65536<br>Output generated in 0.00 seconds
          (0.00 tokens/s, 0 tokens, context 65, seed 789726404)</p>

          <p>is there anyone able to get the exllama working?</p>

          <p>Thanks</p>

          '
        raw: "I've  updated text-generation-web-UI to latest (transformers updated\
          \ to 4.31.0) and also manually verified the exllama folder also has been\
          \ updated and  contains this https://github.com/turboderp/exllama/commit/b3aea521859b83cfd889c4c00c05a323313b7fee\
          \ commit.\r\n\r\nExllama is able to load the module, but when I typing,\
          \ i got:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\oobabooga_windows\\\
          text-generation-webui\\modules\\text_generation.py\", line 331, in generate_reply_custom\r\
          \n    for reply in shared.model.generate_with_streaming(question, state):\r\
          \n  File \"c:\\oobabooga_windows\\text-generation-webui\\modules\\exllama.py\"\
          , line 98, in generate_with_streaming\r\n    self.generator.gen_begin_reuse(ids)\r\
          \n  File \"c:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\generator.py\", line 186, in gen_begin_reuse\r\n    self.gen_begin(in_tokens)\r\
          \n  File \"c:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\generator.py\", line 171, in gen_begin\r\n    self.model.forward(self.sequence[:,\
          \ :-1], self.cache, preprocess_only = True, lora = self.lora)\r\n  File\
          \ \"c:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\exllama\\\
          model.py\", line 887, in forward\r\n    r = self._forward(input_ids[:, chunk_begin\
          \ : chunk_end],\r\n  File \"c:\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\exllama\\model.py\", line 968, in _forward\r\n    hidden_states\
          \ = decoder_layer.forward(hidden_states, cache, buffers[device], lora)\r\
          \n  File \"c:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\model.py\", line 471, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\r\n  File \"c:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\exllama\\model.py\", line 389, in forward\r\n \
          \   key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\r\nRuntimeError: shape '[1, 64,\
          \ 64, 128]' is invalid for input of size 65536\r\nOutput generated in 0.00\
          \ seconds (0.00 tokens/s, 0 tokens, context 65, seed 789726404)\r\n\r\n\
          is there anyone able to get the exllama working?\r\n\r\nThanks"
        updatedAt: '2023-07-19T18:09:09.786Z'
      numEdits: 0
      reactions: []
    id: 64b826c5d6c468ac75116963
    type: comment
  author: charleyzhuyi
  content: "I've  updated text-generation-web-UI to latest (transformers updated to\
    \ 4.31.0) and also manually verified the exllama folder also has been updated\
    \ and  contains this https://github.com/turboderp/exllama/commit/b3aea521859b83cfd889c4c00c05a323313b7fee\
    \ commit.\r\n\r\nExllama is able to load the module, but when I typing, i got:\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"c:\\oobabooga_windows\\text-generation-webui\\\
    modules\\text_generation.py\", line 331, in generate_reply_custom\r\n    for reply\
    \ in shared.model.generate_with_streaming(question, state):\r\n  File \"c:\\oobabooga_windows\\\
    text-generation-webui\\modules\\exllama.py\", line 98, in generate_with_streaming\r\
    \n    self.generator.gen_begin_reuse(ids)\r\n  File \"c:\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\exllama\\generator.py\", line 186, in\
    \ gen_begin_reuse\r\n    self.gen_begin(in_tokens)\r\n  File \"c:\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\exllama\\generator.py\", line 171, in\
    \ gen_begin\r\n    self.model.forward(self.sequence[:, :-1], self.cache, preprocess_only\
    \ = True, lora = self.lora)\r\n  File \"c:\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\exllama\\model.py\", line 887, in forward\r\n    r =\
    \ self._forward(input_ids[:, chunk_begin : chunk_end],\r\n  File \"c:\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\exllama\\model.py\", line 968, in _forward\r\
    \n    hidden_states = decoder_layer.forward(hidden_states, cache, buffers[device],\
    \ lora)\r\n  File \"c:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    exllama\\model.py\", line 471, in forward\r\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\r\n  File \"c:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\exllama\\model.py\", line 389, in forward\r\n    key_states\
    \ = key_states.view(bsz, q_len, self.config.num_attention_heads, self.config.head_dim).transpose(1,\
    \ 2)\r\nRuntimeError: shape '[1, 64, 64, 128]' is invalid for input of size 65536\r\
    \nOutput generated in 0.00 seconds (0.00 tokens/s, 0 tokens, context 65, seed\
    \ 789726404)\r\n\r\nis there anyone able to get the exllama working?\r\n\r\nThanks"
  created_at: 2023-07-19 17:09:09+00:00
  edited: false
  hidden: false
  id: 64b826c5d6c468ac75116963
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
      fullname: hugginglaoda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugginglaoda
      type: user
    createdAt: '2023-07-22T08:05:46.000Z'
    data:
      edited: true
      editors:
      - hugginglaoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8145644068717957
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb3d5c212d163e6667af22ebd918f226.svg
          fullname: hugginglaoda
          isHf: false
          isPro: false
          name: hugginglaoda
          type: user
        html: '<p>use the latest version in main branch of exllama and latest released
          version of transformer</p>

          '
        raw: use the latest version in main branch of exllama and latest released
          version of transformer
        updatedAt: '2023-07-22T08:06:08.928Z'
      numEdits: 1
      reactions: []
    id: 64bb8dda06087679b84b5413
    type: comment
  author: hugginglaoda
  content: use the latest version in main branch of exllama and latest released version
    of transformer
  created_at: 2023-07-22 07:05:46+00:00
  edited: true
  hidden: false
  id: 64bb8dda06087679b84b5413
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:08:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8869118690490723
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>text-generation-webui provides its own exllama wheel, and I don''t
          know if that''s been updated yet.  Try <code>pip3 uninstall exllama</code>
          in the Python environment of text-generation-webui, then run again.  That
          will cause exllama to automatically build its kernel extension on model
          load, which will therefore definitely include the llama 70B changes</p>

          '
        raw: text-generation-webui provides its own exllama wheel, and I don't know
          if that's been updated yet.  Try `pip3 uninstall exllama` in the Python
          environment of text-generation-webui, then run again.  That will cause exllama
          to automatically build its kernel extension on model load, which will therefore
          definitely include the llama 70B changes
        updatedAt: '2023-07-22T08:08:25.785Z'
      numEdits: 0
      reactions: []
    id: 64bb8e79976343e90a337c66
    type: comment
  author: TheBloke
  content: text-generation-webui provides its own exllama wheel, and I don't know
    if that's been updated yet.  Try `pip3 uninstall exllama` in the Python environment
    of text-generation-webui, then run again.  That will cause exllama to automatically
    build its kernel extension on model load, which will therefore definitely include
    the llama 70B changes
  created_at: 2023-07-22 07:08:25+00:00
  edited: false
  hidden: false
  id: 64bb8e79976343e90a337c66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: ExLlama is not working, received  "shape '[1, 64, 64, 128]' is invalid for
  input of size 65536" error
