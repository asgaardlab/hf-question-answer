!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brendanlui
conflicting_files: null
created_at: 2023-08-18 04:13:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
      fullname: Brendan Lui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brendanlui
      type: user
    createdAt: '2023-08-18T05:13:40.000Z'
    data:
      edited: false
      editors:
      - brendanlui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.743071436882019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
          fullname: Brendan Lui
          isHf: false
          isPro: false
          name: brendanlui
          type: user
        html: "<p>According to Readme.md for the main branch (<a href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ#provided-files\"\
          >https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ#provided-files</a>),</p>\n\
          <pre><code>Branch\tBits\tGroup Size\tAct Order (desc_act)\tFile Size\tExLlama\
          \ Compatible?\tMade With\tDescription\nmain\t4\t128\tFalse\t35.33 GB\tTrue\t\
          AutoGPTQ\tMost compatible option. Good inference speed in AutoGPTQ and GPTQ-for-LLaMa.\
          \ Lower inference quality than other options.\n</code></pre>\n<p>but the\
          \ file name is  <code>gptq_model-4bit--1g.safetensors</code> rather than\
          \ <code>gptq_model-4bit--128g.safetensors</code>. Thus, which one is correct?</p>\n"
        raw: "According to Readme.md for the main branch (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ#provided-files),\r\
          \n\r\n```\r\nBranch\tBits\tGroup Size\tAct Order (desc_act)\tFile Size\t\
          ExLlama Compatible?\tMade With\tDescription\r\nmain\t4\t128\tFalse\t35.33\
          \ GB\tTrue\tAutoGPTQ\tMost compatible option. Good inference speed in AutoGPTQ\
          \ and GPTQ-for-LLaMa. Lower inference quality than other options.\r\n```\r\
          \n\r\nbut the file name is  `gptq_model-4bit--1g.safetensors` rather than\
          \ `gptq_model-4bit--128g.safetensors`. Thus, which one is correct?"
        updatedAt: '2023-08-18T05:13:40.168Z'
      numEdits: 0
      reactions: []
    id: 64defe04d27135dd566ec47c
    type: comment
  author: brendanlui
  content: "According to Readme.md for the main branch (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ#provided-files),\r\
    \n\r\n```\r\nBranch\tBits\tGroup Size\tAct Order (desc_act)\tFile Size\tExLlama\
    \ Compatible?\tMade With\tDescription\r\nmain\t4\t128\tFalse\t35.33 GB\tTrue\t\
    AutoGPTQ\tMost compatible option. Good inference speed in AutoGPTQ and GPTQ-for-LLaMa.\
    \ Lower inference quality than other options.\r\n```\r\n\r\nbut the file name\
    \ is  `gptq_model-4bit--1g.safetensors` rather than `gptq_model-4bit--128g.safetensors`.\
    \ Thus, which one is correct?"
  created_at: 2023-08-18 04:13:40+00:00
  edited: false
  hidden: false
  id: 64defe04d27135dd566ec47c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-18T08:12:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9138121604919434
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, README is wrong - main branch is groupsize -1 (no group size).  I''ll
          fix that</p>

          '
        raw: Sorry, README is wrong - main branch is groupsize -1 (no group size).  I'll
          fix that
        updatedAt: '2023-08-18T08:12:11.196Z'
      numEdits: 0
      reactions: []
    id: 64df27db3d3a7519f19853c1
    type: comment
  author: TheBloke
  content: Sorry, README is wrong - main branch is groupsize -1 (no group size).  I'll
    fix that
  created_at: 2023-08-18 07:12:11+00:00
  edited: false
  hidden: false
  id: 64df27db3d3a7519f19853c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
      fullname: Brendan Lui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brendanlui
      type: user
    createdAt: '2023-08-18T08:23:59.000Z'
    data:
      edited: false
      editors:
      - brendanlui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9462272524833679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
          fullname: Brendan Lui
          isHf: false
          isPro: false
          name: brendanlui
          type: user
        html: '<p>Could you clarify whether only the main branch supports GPTQ-for-LLaMa,
          as other branches seem to be unable? I''ve attempted to use TGI to start
          up the gptq_model-4bit--1g.safetensors, which worked fine, but using 2 GPUs
          failed due to the groupsize not being &gt;= 2. I am seeking a version with
          a groupsize &gt;= 2. However, my attempts to start other branches through
          TGI have failed.</p>

          '
        raw: Could you clarify whether only the main branch supports GPTQ-for-LLaMa,
          as other branches seem to be unable? I've attempted to use TGI to start
          up the gptq_model-4bit--1g.safetensors, which worked fine, but using 2 GPUs
          failed due to the groupsize not being >= 2. I am seeking a version with
          a groupsize >= 2. However, my attempts to start other branches through TGI
          have failed.
        updatedAt: '2023-08-18T08:23:59.954Z'
      numEdits: 0
      reactions: []
    id: 64df2a9fae3fd431a21e16ea
    type: comment
  author: brendanlui
  content: Could you clarify whether only the main branch supports GPTQ-for-LLaMa,
    as other branches seem to be unable? I've attempted to use TGI to start up the
    gptq_model-4bit--1g.safetensors, which worked fine, but using 2 GPUs failed due
    to the groupsize not being >= 2. I am seeking a version with a groupsize >= 2.
    However, my attempts to start other branches through TGI have failed.
  created_at: 2023-08-18 07:23:59+00:00
  edited: false
  hidden: false
  id: 64df2a9fae3fd431a21e16ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-18T08:32:58.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919680953025818
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s confusing. I thought it was the exact opposite - that the
          main branch wouldn''t work with TGI because for this model I used an old
          GPTQ-for-LLaMa version, and that all the others would work because they
          were made with AutoGPTQ.  Actually no, I made all these with AutoGPTQ so
          I would expect them all to work.</p>

          <p>What problems do you have with the ones in the other branches?</p>

          '
        raw: 'That''s confusing. I thought it was the exact opposite - that the main
          branch wouldn''t work with TGI because for this model I used an old GPTQ-for-LLaMa
          version, and that all the others would work because they were made with
          AutoGPTQ.  Actually no, I made all these with AutoGPTQ so I would expect
          them all to work.


          What problems do you have with the ones in the other branches?'
        updatedAt: '2023-08-18T08:33:36.366Z'
      numEdits: 2
      reactions: []
    id: 64df2cba5a8a9efea803abe4
    type: comment
  author: TheBloke
  content: 'That''s confusing. I thought it was the exact opposite - that the main
    branch wouldn''t work with TGI because for this model I used an old GPTQ-for-LLaMa
    version, and that all the others would work because they were made with AutoGPTQ.  Actually
    no, I made all these with AutoGPTQ so I would expect them all to work.


    What problems do you have with the ones in the other branches?'
  created_at: 2023-08-18 07:32:58+00:00
  edited: true
  hidden: false
  id: 64df2cba5a8a9efea803abe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
      fullname: Brendan Lui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brendanlui
      type: user
    createdAt: '2023-08-18T09:15:10.000Z'
    data:
      edited: false
      editors:
      - brendanlui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6002892255783081
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
          fullname: Brendan Lui
          isHf: false
          isPro: false
          name: brendanlui
          type: user
        html: '<p>Just to note, I''m using TGI v0.9.4.</p>

          <p>I encounter a ''ShardCannotStart'' error, yet it works fine when I initiate
          using the main branch and a single GPU.<br>However, for instance, with ''gptq-4bit-128g-actorder_True''
          and 2 GPUs:</p>

          <pre><code>{"timestamp":"2023-08-18T09:05:32.861563Z","level":"INFO","fields":{"message":"Args
          { model_id: \"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\",
          revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize:
          Some(Gptq), dtype: None, trust_remote_code: false, max_concurrent_requests:
          128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:
          8192, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 8192, max_batch_total_tokens:
          Some(8192), max_waiting_tokens: 20, hostname: \"0.0.0.0\", port: 1234, shard_uds_path:
          \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:
          29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override: Some(\"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\"),
          disable_custom_kernels: false, json_output: true, otlp_endpoint: None, cors_allow_origin:
          [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:
          None, ngrok_edge: None, env: false }"},"target":"text_generation_launcher"}

          {"timestamp":"2023-08-18T09:05:32.861603Z","level":"INFO","fields":{"message":"Sharding
          model on 2 processes"},"target":"text_generation_launcher"}

          {"timestamp":"2023-08-18T09:05:32.861714Z","level":"INFO","fields":{"message":"Starting
          download process."},"target":"text_generation_launcher","span":{"name":"download"},"spans":[{"name":"download"}]}

          {"timestamp":"2023-08-18T09:05:42.632849Z","level":"INFO","fields":{"message":"Files
          are already present on the host. Skipping download.\n"},"target":"text_generation_launcher"}

          {"timestamp":"2023-08-18T09:05:44.881424Z","level":"INFO","fields":{"message":"Successfully
          downloaded weights."},"target":"text_generation_launcher","span":{"name":"download"},"spans":[{"name":"download"}]}

          {"timestamp":"2023-08-18T09:05:44.881694Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:05:44.881742Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:08:45.037129Z","level":"INFO","fields":{"message":"Waiting
          for shard to be ready..."},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:08:45.037129Z","level":"INFO","fields":{"message":"Waiting
          for shard to be ready..."},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:08:55.044955Z","level":"INFO","fields":{"message":"Waiting
          for shard to be ready..."},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:08:55.044955Z","level":"INFO","fields":{"message":"Waiting
          for shard to be ready..."},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:09:04.654342Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nYou are using a model of type llama to
          instantiate a model of type . This is not supported for all configurations
          of models and can yield errors.\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 78, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 180, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 150, in serve_inner\n    create_exllama_buffers()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\",
          line 52, in create_exllama_buffers\n    prepare_buffers(DEVICE, temp_state,
          temp_dq)\n\nTypeError: prepare_buffers(): incompatible function arguments.
          The following argument types are supported:\n    1. (arg0: torch.device,
          arg1: torch.Tensor, arg2: torch.Tensor) -&gt; None\n\nInvoked with: None,
          tensor([[0.]], dtype=torch.float16), tensor([[0.]], dtype=torch.float16)\n"},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          {"timestamp":"2023-08-18T09:09:04.745107Z","level":"ERROR","fields":{"message":"Shard
          1 failed to start"},"target":"text_generation_launcher"}

          {"timestamp":"2023-08-18T09:09:04.745148Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}

          {"timestamp":"2023-08-18T09:09:04.986644Z","level":"INFO","fields":{"message":"Shard
          terminated"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          Error: ShardCannotStart

          </code></pre>

          '
        raw: "Just to note, I'm using TGI v0.9.4.\n\nI encounter a 'ShardCannotStart'\
          \ error, yet it works fine when I initiate using the main branch and a single\
          \ GPU. \nHowever, for instance, with 'gptq-4bit-128g-actorder_True' and\
          \ 2 GPUs:\n\n```\n{\"timestamp\":\"2023-08-18T09:05:32.861563Z\",\"level\"\
          :\"INFO\",\"fields\":{\"message\":\"Args { model_id: \\\"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\\\
          \", revision: None, validation_workers: 2, sharded: None, num_shard: None,\
          \ quantize: Some(Gptq), dtype: None, trust_remote_code: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:\
          \ 8192, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 8192, max_batch_total_tokens:\
          \ Some(8192), max_waiting_tokens: 20, hostname: \\\"0.0.0.0\\\", port: 1234,\
          \ shard_uds_path: \\\"/tmp/text-generation-server\\\", master_addr: \\\"\
          localhost\\\", master_port: 29500, huggingface_hub_cache: Some(\\\"/data\\\
          \"), weights_cache_override: Some(\\\"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\\\
          \"), disable_custom_kernels: false, json_output: true, otlp_endpoint: None,\
          \ cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok:\
          \ false, ngrok_authtoken: None, ngrok_edge: None, env: false }\"},\"target\"\
          :\"text_generation_launcher\"}\n{\"timestamp\":\"2023-08-18T09:05:32.861603Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Sharding model on 2 processes\"\
          },\"target\":\"text_generation_launcher\"}\n{\"timestamp\":\"2023-08-18T09:05:32.861714Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting download process.\"\
          },\"target\":\"text_generation_launcher\",\"span\":{\"name\":\"download\"\
          },\"spans\":[{\"name\":\"download\"}]}\n{\"timestamp\":\"2023-08-18T09:05:42.632849Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Files are already present\
          \ on the host. Skipping download.\\n\"},\"target\":\"text_generation_launcher\"\
          }\n{\"timestamp\":\"2023-08-18T09:05:44.881424Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Successfully downloaded weights.\"},\"target\":\"\
          text_generation_launcher\",\"span\":{\"name\":\"download\"},\"spans\":[{\"\
          name\":\"download\"}]}\n{\"timestamp\":\"2023-08-18T09:05:44.881694Z\",\"\
          level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\"\
          :\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"\
          },\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"\
          2023-08-18T09:05:44.881742Z\",\"level\":\"INFO\",\"fields\":{\"message\"\
          :\"Starting shard\"},\"target\":\"text_generation_launcher\",\"span\":{\"\
          rank\":1,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":1,\"name\":\"\
          shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:45.037129Z\",\"level\"\
          :\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"},\"\
          target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"\
          },\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"\
          2023-08-18T09:08:45.037129Z\",\"level\":\"INFO\",\"fields\":{\"message\"\
          :\"Waiting for shard to be ready...\"},\"target\":\"text_generation_launcher\"\
          ,\"span\":{\"rank\":1,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":1,\"\
          name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:55.044955Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"\
          },\"target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n\
          {\"timestamp\":\"2023-08-18T09:08:55.044955Z\",\"level\":\"INFO\",\"fields\"\
          :{\"message\":\"Waiting for shard to be ready...\"},\"target\":\"text_generation_launcher\"\
          ,\"span\":{\"rank\":1,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":1,\"\
          name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:09:04.654342Z\"\
          ,\"level\":\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error\
          \ output:\\n\\nYou are using a model of type llama to instantiate a model\
          \ of type . This is not supported for all configurations of models and can\
          \ yield errors.\\nTraceback (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 78, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 180, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in\
          \ run_until_complete\\n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 150, in serve_inner\\n    create_exllama_buffers()\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\\\
          \", line 52, in create_exllama_buffers\\n    prepare_buffers(DEVICE, temp_state,\
          \ temp_dq)\\n\\nTypeError: prepare_buffers(): incompatible function arguments.\
          \ The following argument types are supported:\\n    1. (arg0: torch.device,\
          \ arg1: torch.Tensor, arg2: torch.Tensor) -> None\\n\\nInvoked with: None,\
          \ tensor([[0.]], dtype=torch.float16), tensor([[0.]], dtype=torch.float16)\\\
          n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\n\
          {\"timestamp\":\"2023-08-18T09:09:04.745107Z\",\"level\":\"ERROR\",\"fields\"\
          :{\"message\":\"Shard 1 failed to start\"},\"target\":\"text_generation_launcher\"\
          }\n{\"timestamp\":\"2023-08-18T09:09:04.745148Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Shutting down shards\"},\"target\":\"text_generation_launcher\"\
          }\n{\"timestamp\":\"2023-08-18T09:09:04.986644Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Shard terminated\"},\"target\":\"text_generation_launcher\"\
          ,\"span\":{\"rank\":0,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":0,\"\
          name\":\"shard-manager\"}]}\nError: ShardCannotStart\n```"
        updatedAt: '2023-08-18T09:15:10.332Z'
      numEdits: 0
      reactions: []
    id: 64df369e5bd74c3c814dbe05
    type: comment
  author: brendanlui
  content: "Just to note, I'm using TGI v0.9.4.\n\nI encounter a 'ShardCannotStart'\
    \ error, yet it works fine when I initiate using the main branch and a single\
    \ GPU. \nHowever, for instance, with 'gptq-4bit-128g-actorder_True' and 2 GPUs:\n\
    \n```\n{\"timestamp\":\"2023-08-18T09:05:32.861563Z\",\"level\":\"INFO\",\"fields\"\
    :{\"message\":\"Args { model_id: \\\"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\\\
    \", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize:\
    \ Some(Gptq), dtype: None, trust_remote_code: false, max_concurrent_requests:\
    \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:\
    \ 8192, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 8192, max_batch_total_tokens:\
    \ Some(8192), max_waiting_tokens: 20, hostname: \\\"0.0.0.0\\\", port: 1234, shard_uds_path:\
    \ \\\"/tmp/text-generation-server\\\", master_addr: \\\"localhost\\\", master_port:\
    \ 29500, huggingface_hub_cache: Some(\\\"/data\\\"), weights_cache_override: Some(\\\
    \"/tmp/datadrive/Llama-2-70B-chat-GPTQ-gptq-4bit-128g-actorder_True\\\"), disable_custom_kernels:\
    \ false, json_output: true, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:\
    \ None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge:\
    \ None, env: false }\"},\"target\":\"text_generation_launcher\"}\n{\"timestamp\"\
    :\"2023-08-18T09:05:32.861603Z\",\"level\":\"INFO\",\"fields\":{\"message\":\"\
    Sharding model on 2 processes\"},\"target\":\"text_generation_launcher\"}\n{\"\
    timestamp\":\"2023-08-18T09:05:32.861714Z\",\"level\":\"INFO\",\"fields\":{\"\
    message\":\"Starting download process.\"},\"target\":\"text_generation_launcher\"\
    ,\"span\":{\"name\":\"download\"},\"spans\":[{\"name\":\"download\"}]}\n{\"timestamp\"\
    :\"2023-08-18T09:05:42.632849Z\",\"level\":\"INFO\",\"fields\":{\"message\":\"\
    Files are already present on the host. Skipping download.\\n\"},\"target\":\"\
    text_generation_launcher\"}\n{\"timestamp\":\"2023-08-18T09:05:44.881424Z\",\"\
    level\":\"INFO\",\"fields\":{\"message\":\"Successfully downloaded weights.\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"name\":\"download\"},\"\
    spans\":[{\"name\":\"download\"}]}\n{\"timestamp\":\"2023-08-18T09:05:44.881694Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\":\"\
    text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"},\"\
    spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:05:44.881742Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\":\"\
    text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"shard-manager\"},\"\
    spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:45.037129Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"\
    },\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:45.037129Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"shard-manager\"\
    },\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:55.044955Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"\
    },\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:08:55.044955Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Waiting for shard to be ready...\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"shard-manager\"\
    },\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:09:04.654342Z\"\
    ,\"level\":\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error output:\\\
    n\\nYou are using a model of type llama to instantiate a model of type . This\
    \ is not supported for all configurations of models and can yield errors.\\nTraceback\
    \ (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
    \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 78, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 180, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File \\\"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in run_until_complete\\\
    n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 150, in serve_inner\\n    create_exllama_buffers()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\\\
    \", line 52, in create_exllama_buffers\\n    prepare_buffers(DEVICE, temp_state,\
    \ temp_dq)\\n\\nTypeError: prepare_buffers(): incompatible function arguments.\
    \ The following argument types are supported:\\n    1. (arg0: torch.device, arg1:\
    \ torch.Tensor, arg2: torch.Tensor) -> None\\n\\nInvoked with: None, tensor([[0.]],\
    \ dtype=torch.float16), tensor([[0.]], dtype=torch.float16)\\n\"},\"target\":\"\
    text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"shard-manager\"},\"\
    spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\n{\"timestamp\":\"2023-08-18T09:09:04.745107Z\"\
    ,\"level\":\"ERROR\",\"fields\":{\"message\":\"Shard 1 failed to start\"},\"target\"\
    :\"text_generation_launcher\"}\n{\"timestamp\":\"2023-08-18T09:09:04.745148Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Shutting down shards\"},\"target\"\
    :\"text_generation_launcher\"}\n{\"timestamp\":\"2023-08-18T09:09:04.986644Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Shard terminated\"},\"target\":\"\
    text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"shard-manager\"},\"\
    spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\nError: ShardCannotStart\n\
    ```"
  created_at: 2023-08-18 08:15:10+00:00
  edited: false
  hidden: false
  id: 64df369e5bd74c3c814dbe05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-18T09:17:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9671075344085693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK if it works for one GPU then it''s not an issue with my GPTQs
          I think. I don''t know what''s required for sharding exactly.  Could you
          raise it on the TGI Github</p>

          '
        raw: OK if it works for one GPU then it's not an issue with my GPTQs I think.
          I don't know what's required for sharding exactly.  Could you raise it on
          the TGI Github
        updatedAt: '2023-08-18T09:17:52.513Z'
      numEdits: 0
      reactions: []
    id: 64df374092f3bd97867e0ce8
    type: comment
  author: TheBloke
  content: OK if it works for one GPU then it's not an issue with my GPTQs I think.
    I don't know what's required for sharding exactly.  Could you raise it on the
    TGI Github
  created_at: 2023-08-18 08:17:52+00:00
  edited: false
  hidden: false
  id: 64df374092f3bd97867e0ce8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
      fullname: Brendan Lui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brendanlui
      type: user
    createdAt: '2023-08-24T09:45:32.000Z'
    data:
      edited: false
      editors:
      - brendanlui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277730584144592
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
          fullname: Brendan Lui
          isHf: false
          isPro: false
          name: brendanlui
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , the problem is resolved after I used the latest TGI code. </p>\n<p>How\
          \ about the group size in the main branch of Llama-2-13B-chat-GPTQ? As there\
          \ is another branch called GPTQ-gptq-4bit-128g-actorder_True, is the only\
          \ difference between these two branches \"actorder\"?</p>\n"
        raw: "Thanks @TheBloke , the problem is resolved after I used the latest TGI\
          \ code. \n\nHow about the group size in the main branch of Llama-2-13B-chat-GPTQ?\
          \ As there is another branch called GPTQ-gptq-4bit-128g-actorder_True, is\
          \ the only difference between these two branches \"actorder\"?"
        updatedAt: '2023-08-24T09:45:32.827Z'
      numEdits: 0
      reactions: []
    id: 64e726bc79630927bfceb5bc
    type: comment
  author: brendanlui
  content: "Thanks @TheBloke , the problem is resolved after I used the latest TGI\
    \ code. \n\nHow about the group size in the main branch of Llama-2-13B-chat-GPTQ?\
    \ As there is another branch called GPTQ-gptq-4bit-128g-actorder_True, is the\
    \ only difference between these two branches \"actorder\"?"
  created_at: 2023-08-24 08:45:32+00:00
  edited: false
  hidden: false
  id: 64e726bc79630927bfceb5bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-24T10:29:55.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761213064193726
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes that''s correct. The model with act-order = True has higher
          quality, but in the past using act-order + group_size has caused performance
          problems for some GPTQ clients. </p>

          <p>That may now be resolved, and I don''t know if it ever affected TGI.  </p>

          <p>So try 128g + True first and only use 128g + False if performance seems
          slow.  In future I may make 128g + True the ''main'' model, or even drop
          128 + False entirely, if the performance issues are confirmed to be resolved.</p>

          '
        raw: "Yes that's correct. The model with act-order = True has higher quality,\
          \ but in the past using act-order + group_size has caused performance problems\
          \ for some GPTQ clients. \n\nThat may now be resolved, and I don't know\
          \ if it ever affected TGI.  \n\nSo try 128g + True first and only use 128g\
          \ + False if performance seems slow.  In future I may make 128g + True the\
          \ 'main' model, or even drop 128 + False entirely, if the performance issues\
          \ are confirmed to be resolved."
        updatedAt: '2023-08-24T10:31:14.090Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jlzhou
    id: 64e73123e9fc9d0475f11221
    type: comment
  author: TheBloke
  content: "Yes that's correct. The model with act-order = True has higher quality,\
    \ but in the past using act-order + group_size has caused performance problems\
    \ for some GPTQ clients. \n\nThat may now be resolved, and I don't know if it\
    \ ever affected TGI.  \n\nSo try 128g + True first and only use 128g + False if\
    \ performance seems slow.  In future I may make 128g + True the 'main' model,\
    \ or even drop 128 + False entirely, if the performance issues are confirmed to\
    \ be resolved."
  created_at: 2023-08-24 09:29:55+00:00
  edited: true
  hidden: false
  id: 64e73123e9fc9d0475f11221
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
      fullname: Brendan Lui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brendanlui
      type: user
    createdAt: '2023-08-28T06:23:49.000Z'
    data:
      edited: false
      editors:
      - brendanlui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.632742702960968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f1e23415e60ffcc4dd388dd0b44d0af.svg
          fullname: Brendan Lui
          isHf: false
          isPro: false
          name: brendanlui
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Do you have any\
          \ recommendation about which hyperparameters we should use to have the fastest\
          \ inference speed for GPTQ models? As I did an experiment on TGI with both\
          \ quantized and non-quantized LLaMa-2 models, I'm confused why the GPTQ\
          \ models always have slower inference speed for the same request body, and\
          \ the GPU memory usage is almost similar between every model on TGI. FYI,\
          \ I'm using A100 80GB for testing.</p>\n<div class=\"max-w-full overflow-auto\"\
          >\n\t<table>\n\t\t<thead><tr>\n<th>Model</th>\n<th>No. of GPU(s)</th>\n\
          <th>Parameters</th>\n<th>Quantization Method</th>\n<th>Bits</th>\n<th>GPTQ\
          \ Group Size</th>\n<th>ExLlama Compatible?</th>\n<th>Processing time / request</th>\n\
          <th>GPU Memory Used</th>\n<th>Sharded</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n\
          <td>Llama-2-7b-chat-hf</td>\n<td>2</td>\n<td>7B</td>\n<td>-</td>\n<td>16</td>\n\
          <td>-</td>\n<td>-</td>\n<td>4.00 s</td>\n<td>147.1 GB</td>\n<td>-</td>\n\
          </tr>\n<tr>\n<td>Llama-2-7b-chat-hf</td>\n<td>1</td>\n<td>7B</td>\n<td>-</td>\n\
          <td>16</td>\n<td>-</td>\n<td>-</td>\n<td>3.10 s</td>\n<td>78.7 GB</td>\n\
          <td>-</td>\n</tr>\n<tr>\n<td>Llama-2-7b-chat-hf</td>\n<td>1</td>\n<td>7B</td>\n\
          <td>-</td>\n<td>16</td>\n<td>-</td>\n<td>-</td>\n<td>11.30 s</td>\n<td>78.8\
          \ GB</td>\n<td>False</td>\n</tr>\n<tr>\n<td>Llama-2-7b-Chat-GPTQ (main)</td>\n\
          <td>1</td>\n<td>7B</td>\n<td>GPTQ</td>\n<td>4</td>\n<td>128</td>\n<td>Yes</td>\n\
          <td>4.50 s</td>\n<td>79.3 GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-7b-Chat-GPTQ\
          \ (main)</td>\n<td>1</td>\n<td>7B</td>\n<td>GPTQ</td>\n<td>4</td>\n<td>128</td>\n\
          <td>Yes</td>\n<td>11.35 s</td>\n<td>79.3 GB</td>\n<td>False</td>\n</tr>\n\
          <tr>\n<td>Llama-2-13b-chat-hf</td>\n<td>1</td>\n<td>13B</td>\n<td>-</td>\n\
          <td>16</td>\n<td>-</td>\n<td>-</td>\n<td>5.35 s</td>\n<td>78.4 GB</td>\n\
          <td>-</td>\n</tr>\n<tr>\n<td>Llama-2-13B-chat-GPTQ (main)</td>\n<td>1</td>\n\
          <td>13B</td>\n<td>GPTQ</td>\n<td>4</td>\n<td>128</td>\n<td>Yes</td>\n<td>8.80\
          \ s</td>\n<td>79.1 GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-13B-chat-GPTQ\
          \ (gptq-4bit-128g-actorder_True)</td>\n<td>1</td>\n<td>13B</td>\n<td>GPTQ</td>\n\
          <td>4</td>\n<td>128</td>\n<td>Yes</td>\n<td>-</td>\n<td>Not Enough Memory</td>\n\
          <td>-</td>\n</tr>\n<tr>\n<td>Llama-2-13B-chat-GPTQ (gptq-8bit--1g-actorder_True)</td>\n\
          <td>1</td>\n<td>13B</td>\n<td>GPTQ</td>\n<td>8</td>\n<td>-1</td>\n<td>No</td>\n\
          <td>11.35 s</td>\n<td>78.8 GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-13B-chat-GPTQ\
          \ (gptq-8bit-128g-actorder_False)</td>\n<td>1</td>\n<td>13B</td>\n<td>GPTQ</td>\n\
          <td>8</td>\n<td>128</td>\n<td>No</td>\n<td>11.75 s</td>\n<td>78.7 GB</td>\n\
          <td>-</td>\n</tr>\n<tr>\n<td>Llama-2-70b-chat-hf</td>\n<td>2</td>\n<td>70B</td>\n\
          <td>-</td>\n<td>16</td>\n<td>-</td>\n<td>-</td>\n<td>11.4 s</td>\n<td>159.5\
          \ GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-70b-chat-hf</td>\n<td>1</td>\n\
          <td>70B</td>\n<td>bitsandbytes</td>\n<td>4</td>\n<td>-</td>\n<td>-</td>\n\
          <td>35.5 s</td>\n<td>74.2 GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-70B-chat-GPTQ\
          \ (main)</td>\n<td>1</td>\n<td>70B</td>\n<td>GPTQ</td>\n<td>4</td>\n<td>-1</td>\n\
          <td>Yes</td>\n<td>23.95 s</td>\n<td>77.8 GB</td>\n<td>-</td>\n</tr>\n<tr>\n\
          <td>Llama-2-70B-chat-GPTQ (main)</td>\n<td>1</td>\n<td>70B</td>\n<td>GPTQ</td>\n\
          <td>4</td>\n<td>-1</td>\n<td>Yes</td>\n<td>23.95 s</td>\n<td>77.8 GB</td>\n\
          <td>False</td>\n</tr>\n<tr>\n<td>Llama-2-70B-chat-GPTQ (gptq-4bit-32g-actorder_True)</td>\n\
          <td>2</td>\n<td>70B</td>\n<td>GPTQ</td>\n<td>4</td>\n<td>32</td>\n<td>Yes</td>\n\
          <td>33.8 s</td>\n<td>86.12 GB</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Llama-2-70B-chat-GPTQ\
          \ (gptq-4bit-128g-actorder_True)</td>\n<td>1</td>\n<td>70B</td>\n<td>GPTQ</td>\n\
          <td>4</td>\n<td>128</td>\n<td>Yes</td>\n<td>-</td>\n<td>Not Enough Memory</td>\n\
          <td>-</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: "@TheBloke Do you have any recommendation about which hyperparameters\
          \ we should use to have the fastest inference speed for GPTQ models? As\
          \ I did an experiment on TGI with both quantized and non-quantized LLaMa-2\
          \ models, I'm confused why the GPTQ models always have slower inference\
          \ speed for the same request body, and the GPU memory usage is almost similar\
          \ between every model on TGI. FYI, I'm using A100 80GB for testing.\n\n\
          | Model | No. of GPU(s) | Parameters | Quantization Method | Bits | GPTQ\
          \ Group Size | ExLlama Compatible? | Processing time / request | GPU Memory\
          \ Used | Sharded | \n|--------|---------------|------------------|--------------|------|------------|---------|---------------------------|------------|------------|\n\
          | Llama-2-7b-chat-hf | 2 | 7B | - | 16 | - | - | 4.00 s | 147.1 GB | - |\n\
          | Llama-2-7b-chat-hf | 1 | 7B | - | 16 | - | - | 3.10 s | 78.7 GB | - |\n\
          | Llama-2-7b-chat-hf | 1 | 7B | - | 16 | - | - | 11.30 s | 78.8 GB | False\
          \ |\n| Llama-2-7b-Chat-GPTQ (main) | 1 | 7B | GPTQ | 4 | 128 | Yes | 4.50\
          \ s | 79.3 GB | - |\n| Llama-2-7b-Chat-GPTQ (main) | 1 | 7B | GPTQ | 4 |\
          \ 128 | Yes | 11.35 s | 79.3 GB | False |\n| Llama-2-13b-chat-hf | 1 | 13B\
          \ | - | 16 | - | - | 5.35 s | 78.4 GB | - |\n| Llama-2-13B-chat-GPTQ (main)\
          \ | 1 | 13B | GPTQ | 4 | 128 | Yes | 8.80 s | 79.1 GB | - |\n| Llama-2-13B-chat-GPTQ\
          \ (gptq-4bit-128g-actorder_True) | 1 | 13B | GPTQ | 4 | 128 | Yes | - |\
          \ Not Enough Memory | - |\n| Llama-2-13B-chat-GPTQ (gptq-8bit--1g-actorder_True)\
          \ | 1 | 13B | GPTQ | 8 | -1 | No | 11.35 s | 78.8 GB | - |\n| Llama-2-13B-chat-GPTQ\
          \ (gptq-8bit-128g-actorder_False) | 1 | 13B | GPTQ | 8 | 128 | No | 11.75\
          \ s | 78.7 GB | - |\n| Llama-2-70b-chat-hf | 2 | 70B | - | 16 | - | - |\
          \ 11.4 s | 159.5 GB | - |\n| Llama-2-70b-chat-hf | 1 | 70B | bitsandbytes\
          \ | 4 | - | - | 35.5 s | 74.2 GB | - |\n| Llama-2-70B-chat-GPTQ (main) |\
          \ 1 | 70B | GPTQ | 4 | -1 | Yes | 23.95 s | 77.8 GB | - |\n| Llama-2-70B-chat-GPTQ\
          \ (main) | 1 | 70B | GPTQ | 4 | -1 | Yes | 23.95 s | 77.8 GB | False |\n\
          | Llama-2-70B-chat-GPTQ (gptq-4bit-32g-actorder_True) | 2 | 70B | GPTQ |\
          \ 4 | 32 | Yes | 33.8 s | 86.12 GB | - |\n| Llama-2-70B-chat-GPTQ (gptq-4bit-128g-actorder_True)\
          \ | 1 | 70B | GPTQ | 4 | 128 | Yes | - | Not Enough Memory | - |\n"
        updatedAt: '2023-08-28T06:23:49.543Z'
      numEdits: 0
      reactions: []
    id: 64ec3d75c68ddc867b8f704c
    type: comment
  author: brendanlui
  content: "@TheBloke Do you have any recommendation about which hyperparameters we\
    \ should use to have the fastest inference speed for GPTQ models? As I did an\
    \ experiment on TGI with both quantized and non-quantized LLaMa-2 models, I'm\
    \ confused why the GPTQ models always have slower inference speed for the same\
    \ request body, and the GPU memory usage is almost similar between every model\
    \ on TGI. FYI, I'm using A100 80GB for testing.\n\n| Model | No. of GPU(s) | Parameters\
    \ | Quantization Method | Bits | GPTQ Group Size | ExLlama Compatible? | Processing\
    \ time / request | GPU Memory Used | Sharded | \n|--------|---------------|------------------|--------------|------|------------|---------|---------------------------|------------|------------|\n\
    | Llama-2-7b-chat-hf | 2 | 7B | - | 16 | - | - | 4.00 s | 147.1 GB | - |\n| Llama-2-7b-chat-hf\
    \ | 1 | 7B | - | 16 | - | - | 3.10 s | 78.7 GB | - |\n| Llama-2-7b-chat-hf | 1\
    \ | 7B | - | 16 | - | - | 11.30 s | 78.8 GB | False |\n| Llama-2-7b-Chat-GPTQ\
    \ (main) | 1 | 7B | GPTQ | 4 | 128 | Yes | 4.50 s | 79.3 GB | - |\n| Llama-2-7b-Chat-GPTQ\
    \ (main) | 1 | 7B | GPTQ | 4 | 128 | Yes | 11.35 s | 79.3 GB | False |\n| Llama-2-13b-chat-hf\
    \ | 1 | 13B | - | 16 | - | - | 5.35 s | 78.4 GB | - |\n| Llama-2-13B-chat-GPTQ\
    \ (main) | 1 | 13B | GPTQ | 4 | 128 | Yes | 8.80 s | 79.1 GB | - |\n| Llama-2-13B-chat-GPTQ\
    \ (gptq-4bit-128g-actorder_True) | 1 | 13B | GPTQ | 4 | 128 | Yes | - | Not Enough\
    \ Memory | - |\n| Llama-2-13B-chat-GPTQ (gptq-8bit--1g-actorder_True) | 1 | 13B\
    \ | GPTQ | 8 | -1 | No | 11.35 s | 78.8 GB | - |\n| Llama-2-13B-chat-GPTQ (gptq-8bit-128g-actorder_False)\
    \ | 1 | 13B | GPTQ | 8 | 128 | No | 11.75 s | 78.7 GB | - |\n| Llama-2-70b-chat-hf\
    \ | 2 | 70B | - | 16 | - | - | 11.4 s | 159.5 GB | - |\n| Llama-2-70b-chat-hf\
    \ | 1 | 70B | bitsandbytes | 4 | - | - | 35.5 s | 74.2 GB | - |\n| Llama-2-70B-chat-GPTQ\
    \ (main) | 1 | 70B | GPTQ | 4 | -1 | Yes | 23.95 s | 77.8 GB | - |\n| Llama-2-70B-chat-GPTQ\
    \ (main) | 1 | 70B | GPTQ | 4 | -1 | Yes | 23.95 s | 77.8 GB | False |\n| Llama-2-70B-chat-GPTQ\
    \ (gptq-4bit-32g-actorder_True) | 2 | 70B | GPTQ | 4 | 32 | Yes | 33.8 s | 86.12\
    \ GB | - |\n| Llama-2-70B-chat-GPTQ (gptq-4bit-128g-actorder_True) | 1 | 70B |\
    \ GPTQ | 4 | 128 | Yes | - | Not Enough Memory | - |\n"
  created_at: 2023-08-28 05:23:49+00:00
  edited: false
  hidden: false
  id: 64ec3d75c68ddc867b8f704c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 36
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Group size is 128 or 1 for main branch?
