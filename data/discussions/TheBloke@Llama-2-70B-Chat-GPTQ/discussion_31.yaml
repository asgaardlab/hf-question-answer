!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iamajithkumar
conflicting_files: null
created_at: 2023-08-10 07:34:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80ed5dc04579f16b5a3a74bc8f5bafd3.svg
      fullname: Ajithkumar M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iamajithkumar
      type: user
    createdAt: '2023-08-10T08:34:49.000Z'
    data:
      edited: false
      editors:
      - iamajithkumar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8619316816329956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80ed5dc04579f16b5a3a74bc8f5bafd3.svg
          fullname: Ajithkumar M
          isHf: false
          isPro: false
          name: iamajithkumar
          type: user
        html: '<p>Which AWS Instance is good to run Llama-2-70B</p>

          '
        raw: Which AWS Instance is good to run Llama-2-70B
        updatedAt: '2023-08-10T08:34:49.571Z'
      numEdits: 0
      reactions: []
    id: 64d4a129eba5e94cb2c5b93a
    type: comment
  author: iamajithkumar
  content: Which AWS Instance is good to run Llama-2-70B
  created_at: 2023-08-10 07:34:49+00:00
  edited: false
  hidden: false
  id: 64d4a129eba5e94cb2c5b93a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:22:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8424681425094604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m not familiar with the choice of AWS instances.  But in general
          to run a 70B GPTQ you need: </p>

          <ul>

          <li>a 48GB or 80GB GPU<ul>

          <li>Or 2 x 24GB, but 1 x 48GB or bigger is better</li>

          </ul>

          </li>

          <li>64+ GB RAM</li>

          </ul>

          '
        raw: "I'm not familiar with the choice of AWS instances.  But in general to\
          \ run a 70B GPTQ you need: \n- a 48GB or 80GB GPU\n  - Or 2 x 24GB, but\
          \ 1 x 48GB or bigger is better\n- 64+ GB RAM"
        updatedAt: '2023-08-10T09:22:57.230Z'
      numEdits: 0
      reactions: []
    id: 64d4ac711dbf36131e0b9070
    type: comment
  author: TheBloke
  content: "I'm not familiar with the choice of AWS instances.  But in general to\
    \ run a 70B GPTQ you need: \n- a 48GB or 80GB GPU\n  - Or 2 x 24GB, but 1 x 48GB\
    \ or bigger is better\n- 64+ GB RAM"
  created_at: 2023-08-10 08:22:57+00:00
  edited: false
  hidden: false
  id: 64d4ac711dbf36131e0b9070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e79b9f4daaf46989c59336448dd59f4.svg
      fullname: Ion Pop
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iongpt
      type: user
    createdAt: '2023-08-13T04:00:52.000Z'
    data:
      edited: false
      editors:
      - iongpt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.945497989654541
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e79b9f4daaf46989c59336448dd59f4.svg
          fullname: Ion Pop
          isHf: false
          isPro: false
          name: iongpt
          type: user
        html: '<blockquote>

          <p>Which AWS Instance is good to run Llama-2-70B</p>

          </blockquote>

          <p>None.</p>

          <p>I am a big AWS user (10k$+ per month) and they are lacking on good GPU
          options.</p>

          <p>You have the g5.2xlarge (8vCPU, 32Gb RAM) with 1xA10 (24Gb VRAM). With
          the savings, you can get it at 0.49$/h</p>

          <p>But from there, things gets complicated. If you want more than 1xA10,
          lowest option is g5.12xlarge that comes packed also with 48 vCPUs and 192Gb
          of RAM and 4xA10, but it costs more than 5$/h. Next option is 8xA10 in g5.48xlarge
          for about 15$/h. You also get 192vCPU and 768Gb RAM.</p>

          <p>There is no option for A6000</p>

          <p>The only option with A100 is p4.24xlarge at more than 20$/h for 8xA100</p>

          <p>The only option with H100 is p5.48xlarge at almost 100$/h</p>

          <p>I am in contact with my AWS account manager explaining that I am currently
          buying all the GPU time for my needs from other clouds and it is annoying
          to move that around and I might decide to jump ship entirely if they don''t
          fix this. Not sure if this will work, but for now AWS is unusable for anything
          requiring more than 24Gb of VRAM</p>

          '
        raw: '>Which AWS Instance is good to run Llama-2-70B


          None.


          I am a big AWS user (10k$+ per month) and they are lacking on good GPU options.


          You have the g5.2xlarge (8vCPU, 32Gb RAM) with 1xA10 (24Gb VRAM). With the
          savings, you can get it at 0.49$/h


          But from there, things gets complicated. If you want more than 1xA10, lowest
          option is g5.12xlarge that comes packed also with 48 vCPUs and 192Gb of
          RAM and 4xA10, but it costs more than 5$/h. Next option is 8xA10 in g5.48xlarge
          for about 15$/h. You also get 192vCPU and 768Gb RAM.


          There is no option for A6000


          The only option with A100 is p4.24xlarge at more than 20$/h for 8xA100


          The only option with H100 is p5.48xlarge at almost 100$/h


          I am in contact with my AWS account manager explaining that I am currently
          buying all the GPU time for my needs from other clouds and it is annoying
          to move that around and I might decide to jump ship entirely if they don''t
          fix this. Not sure if this will work, but for now AWS is unusable for anything
          requiring more than 24Gb of VRAM'
        updatedAt: '2023-08-13T04:00:52.530Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - jlzhou
        - beahr
        - iamajithkumar
        - Golaiet
        - VenugopalKS
    id: 64d855749a6a7ae9843f93b9
    type: comment
  author: iongpt
  content: '>Which AWS Instance is good to run Llama-2-70B


    None.


    I am a big AWS user (10k$+ per month) and they are lacking on good GPU options.


    You have the g5.2xlarge (8vCPU, 32Gb RAM) with 1xA10 (24Gb VRAM). With the savings,
    you can get it at 0.49$/h


    But from there, things gets complicated. If you want more than 1xA10, lowest option
    is g5.12xlarge that comes packed also with 48 vCPUs and 192Gb of RAM and 4xA10,
    but it costs more than 5$/h. Next option is 8xA10 in g5.48xlarge for about 15$/h.
    You also get 192vCPU and 768Gb RAM.


    There is no option for A6000


    The only option with A100 is p4.24xlarge at more than 20$/h for 8xA100


    The only option with H100 is p5.48xlarge at almost 100$/h


    I am in contact with my AWS account manager explaining that I am currently buying
    all the GPU time for my needs from other clouds and it is annoying to move that
    around and I might decide to jump ship entirely if they don''t fix this. Not sure
    if this will work, but for now AWS is unusable for anything requiring more than
    24Gb of VRAM'
  created_at: 2023-08-13 03:00:52+00:00
  edited: false
  hidden: false
  id: 64d855749a6a7ae9843f93b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/80ed5dc04579f16b5a3a74bc8f5bafd3.svg
      fullname: Ajithkumar M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iamajithkumar
      type: user
    createdAt: '2023-08-14T08:34:28.000Z'
    data:
      status: closed
    id: 64d9e71470891ac9b8d97af6
    type: status-change
  author: iamajithkumar
  created_at: 2023-08-14 07:34:28+00:00
  id: 64d9e71470891ac9b8d97af6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c276e57c7e6ff8119ea667853233748a.svg
      fullname: bb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vvvbb
      type: user
    createdAt: '2023-09-04T03:51:48.000Z'
    data:
      edited: false
      editors:
      - vvvbb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642545580863953
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c276e57c7e6ff8119ea667853233748a.svg
          fullname: bb
          isHf: false
          isPro: false
          name: vvvbb
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;iongpt&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/iongpt\">@<span class=\"\
          underline\">iongpt</span></a></span>\n\n\t</span></span> </p>\n<p>What other\
          \ services would you recommend?</p>\n"
        raw: "@iongpt \n\nWhat other services would you recommend?\n"
        updatedAt: '2023-09-04T03:51:48.669Z'
      numEdits: 0
      reactions: []
    id: 64f55454d0c711d89fbefa22
    type: comment
  author: vvvbb
  content: "@iongpt \n\nWhat other services would you recommend?\n"
  created_at: 2023-09-04 02:51:48+00:00
  edited: false
  hidden: false
  id: 64f55454d0c711d89fbefa22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1dc4d6f7c8b6f5148ef64be197df10db.svg
      fullname: Antoine dussarps
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adussarps
      type: user
    createdAt: '2023-09-04T15:48:34.000Z'
    data:
      edited: false
      editors:
      - adussarps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8941383361816406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1dc4d6f7c8b6f5148ef64be197df10db.svg
          fullname: Antoine dussarps
          isHf: false
          isPro: false
          name: adussarps
          type: user
        html: '<p>And what technologies (library/container/EFS) are you using to run
          LLAMA 2 on AWS?</p>

          <p>Also do you have an opinion on inf2 instances? I heard llama2 can be
          run on thoses too.</p>

          '
        raw: 'And what technologies (library/container/EFS) are you using to run LLAMA
          2 on AWS?


          Also do you have an opinion on inf2 instances? I heard llama2 can be run
          on thoses too.'
        updatedAt: '2023-09-04T15:48:34.582Z'
      numEdits: 0
      reactions: []
    id: 64f5fc52c48864c814468d16
    type: comment
  author: adussarps
  content: 'And what technologies (library/container/EFS) are you using to run LLAMA
    2 on AWS?


    Also do you have an opinion on inf2 instances? I heard llama2 can be run on thoses
    too.'
  created_at: 2023-09-04 14:48:34+00:00
  edited: false
  hidden: false
  id: 64f5fc52c48864c814468d16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08ad50ed17581665b6750fa28dfdf964.svg
      fullname: Tam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ernie
      type: user
    createdAt: '2023-11-05T02:20:40.000Z'
    data:
      edited: false
      editors:
      - Ernie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9248500466346741
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08ad50ed17581665b6750fa28dfdf964.svg
          fullname: Tam
          isHf: false
          isPro: false
          name: Ernie
          type: user
        html: '<p>I just tried cloned <a href="https://huggingface.co/meta-llama/Llama-2-70b-hf">https://huggingface.co/meta-llama/Llama-2-70b-hf</a>
          and tried it on AWS g5.8xlarge instance. This instance has 128G CPU RAM
          and seems like not enough for loading this 70 billion model.</p>

          '
        raw: I just tried cloned https://huggingface.co/meta-llama/Llama-2-70b-hf
          and tried it on AWS g5.8xlarge instance. This instance has 128G CPU RAM
          and seems like not enough for loading this 70 billion model.
        updatedAt: '2023-11-05T02:20:40.253Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - iamajithkumar
    id: 6546fbf84d1931dc93f4c19a
    type: comment
  author: Ernie
  content: I just tried cloned https://huggingface.co/meta-llama/Llama-2-70b-hf and
    tried it on AWS g5.8xlarge instance. This instance has 128G CPU RAM and seems
    like not enough for loading this 70 billion model.
  created_at: 2023-11-05 01:20:40+00:00
  edited: false
  hidden: false
  id: 6546fbf84d1931dc93f4c19a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: Which AWS Instance is good to run Llama-2-70B
