!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RageshAntony
conflicting_files: null
created_at: 2023-07-19 05:50:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dc32d9127f152b2f9bf570a3d9abcad.svg
      fullname: Ragesh Antony
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RageshAntony
      type: user
    createdAt: '2023-07-19T06:50:30.000Z'
    data:
      edited: false
      editors:
      - RageshAntony
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8305674195289612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dc32d9127f152b2f9bf570a3d9abcad.svg
          fullname: Ragesh Antony
          isHf: false
          isPro: false
          name: RageshAntony
          type: user
        html: '<p>Whether RTX A6000 48GB is enough for 70B ?</p>

          '
        raw: Whether RTX A6000 48GB is enough for 70B ?
        updatedAt: '2023-07-19T06:50:30.943Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sheaschwimmer
    id: 64b787b6a8c39dc07886539c
    type: comment
  author: RageshAntony
  content: Whether RTX A6000 48GB is enough for 70B ?
  created_at: 2023-07-19 05:50:30+00:00
  edited: false
  hidden: false
  id: 64b787b6a8c39dc07886539c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-07-19T13:04:42.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34301143884658813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>enough for me.</p>\n<pre><code class=\"language-bash\">Wed Jul 19\
          \ 22:03:09 2023       \n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA RTX A6000               Off | 00000000:01:00.0  On |     \
          \             Off |\n| 30%   44C    P8              32W / 300W |    805MiB\
          \ / 49140MiB |      0%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA RTX A6000               Off | 00000000:02:00.0 Off |     \
          \             Off |\n| 44%   76C    P2             298W / 300W |  34485MiB\
          \ / 49140MiB |    100%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \                   \n+---------------------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \                 |\n|  GPU   GI   CI        PID   Type   Process name \
          \                           GPU Memory |\n|        ID   ID             \
          \                                                Usage      |\n|=======================================================================================|\n\
          |    0   N/A  N/A      1262      G   /usr/lib/xorg/Xorg                \
          \          110MiB |\n|    0   N/A  N/A      1880      G   /usr/lib/xorg/Xorg\
          \                          430MiB |\n|    0   N/A  N/A      2009      G\
          \   /usr/bin/gnome-shell                         86MiB |\n|    0   N/A \
          \ N/A      4149      G   ...8417883,14948046860862319246,262144      151MiB\
          \ |\n|    1   N/A  N/A      1262      G   /usr/lib/xorg/Xorg           \
          \                 4MiB |\n|    1   N/A  N/A      1880      G   /usr/lib/xorg/Xorg\
          \                            4MiB |\n|    1   N/A  N/A     44687      C\
          \   python                                    34460MiB |\n+---------------------------------------------------------------------------------------+\n\
          </code></pre>\n"
        raw: "enough for me.\n``` bash\nWed Jul 19 22:03:09 2023       \n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA RTX A6000               Off | 00000000:01:00.0  On |     \
          \             Off |\n| 30%   44C    P8              32W / 300W |    805MiB\
          \ / 49140MiB |      0%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA RTX A6000               Off | 00000000:02:00.0 Off |     \
          \             Off |\n| 44%   76C    P2             298W / 300W |  34485MiB\
          \ / 49140MiB |    100%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \                   \n+---------------------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \                 |\n|  GPU   GI   CI        PID   Type   Process name \
          \                           GPU Memory |\n|        ID   ID             \
          \                                                Usage      |\n|=======================================================================================|\n\
          |    0   N/A  N/A      1262      G   /usr/lib/xorg/Xorg                \
          \          110MiB |\n|    0   N/A  N/A      1880      G   /usr/lib/xorg/Xorg\
          \                          430MiB |\n|    0   N/A  N/A      2009      G\
          \   /usr/bin/gnome-shell                         86MiB |\n|    0   N/A \
          \ N/A      4149      G   ...8417883,14948046860862319246,262144      151MiB\
          \ |\n|    1   N/A  N/A      1262      G   /usr/lib/xorg/Xorg           \
          \                 4MiB |\n|    1   N/A  N/A      1880      G   /usr/lib/xorg/Xorg\
          \                            4MiB |\n|    1   N/A  N/A     44687      C\
          \   python                                    34460MiB |\n+---------------------------------------------------------------------------------------+\n\
          \n```"
        updatedAt: '2023-07-19T13:04:42.864Z'
      numEdits: 0
      reactions:
      - count: 17
        reaction: "\U0001F92F"
        users:
        - vdruts
        - RageshAntony
        - tea-lover-418
        - viniciusarruda
        - seanmamasde
        - kepler-br
        - Elpis-wsd
        - nateshmbhat
        - truthisneverlinear
        - detakarang
        - Forbu14
        - jantxu
        - kobkrit
        - MphoKomape
        - Mazyod
        - wassie
        - nick-bae
    id: 64b7df6a037d6452a31f39f9
    type: comment
  author: alfredplpl
  content: "enough for me.\n``` bash\nWed Jul 19 22:03:09 2023       \n+---------------------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
    \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr.\
    \ ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util\
    \  Compute M. |\n|                                         |                 \
    \     |               MIG M. |\n|=========================================+======================+======================|\n\
    |   0  NVIDIA RTX A6000               Off | 00000000:01:00.0  On |           \
    \       Off |\n| 30%   44C    P8              32W / 300W |    805MiB / 49140MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
    |   1  NVIDIA RTX A6000               Off | 00000000:02:00.0 Off |           \
    \       Off |\n| 44%   76C    P2             298W / 300W |  34485MiB / 49140MiB\
    \ |    100%      Default |\n|                                         |      \
    \                |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \             \n+---------------------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \           |\n|  GPU   GI   CI        PID   Type   Process name             \
    \               GPU Memory |\n|        ID   ID                               \
    \                              Usage      |\n|=======================================================================================|\n\
    |    0   N/A  N/A      1262      G   /usr/lib/xorg/Xorg                      \
    \    110MiB |\n|    0   N/A  N/A      1880      G   /usr/lib/xorg/Xorg       \
    \                   430MiB |\n|    0   N/A  N/A      2009      G   /usr/bin/gnome-shell\
    \                         86MiB |\n|    0   N/A  N/A      4149      G   ...8417883,14948046860862319246,262144\
    \      151MiB |\n|    1   N/A  N/A      1262      G   /usr/lib/xorg/Xorg     \
    \                       4MiB |\n|    1   N/A  N/A      1880      G   /usr/lib/xorg/Xorg\
    \                            4MiB |\n|    1   N/A  N/A     44687      C   python\
    \                                    34460MiB |\n+---------------------------------------------------------------------------------------+\n\
    \n```"
  created_at: 2023-07-19 12:04:42+00:00
  edited: false
  hidden: false
  id: 64b7df6a037d6452a31f39f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
      fullname: Harper Grieve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpergrieve
      type: user
    createdAt: '2023-07-19T15:08:19.000Z'
    data:
      edited: false
      editors:
      - harpergrieve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018940329551697
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
          fullname: Harper Grieve
          isHf: false
          isPro: false
          name: harpergrieve
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alfredplpl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alfredplpl\">@<span class=\"\
          underline\">alfredplpl</span></a></span>\n\n\t</span></span> can you please\
          \ share how you started it? token lenghts? branch? have the same setup but\
          \ cant get it loaded...</p>\n"
        raw: '@alfredplpl can you please share how you started it? token lenghts?
          branch? have the same setup but cant get it loaded...'
        updatedAt: '2023-07-19T15:08:19.555Z'
      numEdits: 0
      reactions: []
    id: 64b7fc6375b23e68c5507f3e
    type: comment
  author: harpergrieve
  content: '@alfredplpl can you please share how you started it? token lenghts? branch?
    have the same setup but cant get it loaded...'
  created_at: 2023-07-19 14:08:19+00:00
  edited: false
  hidden: false
  id: 64b7fc6375b23e68c5507f3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T15:08:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6540976762771606
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah 4-bit uses around 36-38GB VRAM to load, plus context, so 48GB
          should be plenty</p>

          '
        raw: Yeah 4-bit uses around 36-38GB VRAM to load, plus context, so 48GB should
          be plenty
        updatedAt: '2023-07-19T15:08:20.558Z'
      numEdits: 0
      reactions: []
    id: 64b7fc6445f3511db2140988
    type: comment
  author: TheBloke
  content: Yeah 4-bit uses around 36-38GB VRAM to load, plus context, so 48GB should
    be plenty
  created_at: 2023-07-19 14:08:20+00:00
  edited: false
  hidden: false
  id: 64b7fc6445f3511db2140988
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T15:09:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7975175976753235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;harpergrieve&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/harpergrieve\"\
          >@<span class=\"underline\">harpergrieve</span></a></span>\n\n\t</span></span>\
          \ check the README again, I recently made updates to it to describe various\
          \ steps that are needed, eg updating Transformers, and, if you use text-generation-webui\
          \ or AutoGPTQ from Python code, making sure <code>inject_fused_attention=False</code>\
          \ is set</p>\n"
        raw: '@harpergrieve check the README again, I recently made updates to it
          to describe various steps that are needed, eg updating Transformers, and,
          if you use text-generation-webui or AutoGPTQ from Python code, making sure
          `inject_fused_attention=False` is set'
        updatedAt: '2023-07-19T15:09:06.641Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - harpergrieve
        - ganeshranganath
    id: 64b7fc92479b934973f1c152
    type: comment
  author: TheBloke
  content: '@harpergrieve check the README again, I recently made updates to it to
    describe various steps that are needed, eg updating Transformers, and, if you
    use text-generation-webui or AutoGPTQ from Python code, making sure `inject_fused_attention=False`
    is set'
  created_at: 2023-07-19 14:09:06+00:00
  edited: false
  hidden: false
  id: 64b7fc92479b934973f1c152
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
      fullname: Harper Grieve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpergrieve
      type: user
    createdAt: '2023-07-19T15:20:51.000Z'
    data:
      edited: false
      editors:
      - harpergrieve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6933063864707947
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
          fullname: Harper Grieve
          isHf: false
          isPro: false
          name: harpergrieve
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thanks for the\
          \ reply, using text gen inference and now getting the model.layers.0.self_attn.q_proj.weight\
          \ error. Ill try using one of the other branches.</p>\n"
        raw: '@TheBloke thanks for the reply, using text gen inference and now getting
          the model.layers.0.self_attn.q_proj.weight error. Ill try using one of the
          other branches.'
        updatedAt: '2023-07-19T15:20:51.750Z'
      numEdits: 0
      reactions: []
    id: 64b7ff53104e7af01c183fef
    type: comment
  author: harpergrieve
  content: '@TheBloke thanks for the reply, using text gen inference and now getting
    the model.layers.0.self_attn.q_proj.weight error. Ill try using one of the other
    branches.'
  created_at: 2023-07-19 14:20:51+00:00
  edited: false
  hidden: false
  id: 64b7ff53104e7af01c183fef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T15:23:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8682721853256226
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Did you update Transformers?  And is that with Loader: AutoGPTQ?</p>

          <p>Also try downloading hte model again (same branch, ie main), just to
          double check the download is OK</p>

          <p>Earlier today I confirmed text-gen-ui works OK with AutoGPTQ + the main
          file, using "no inject fused attention" and with Transformers updated to
          latest version - which be aware has to be done inside the Python environment
          of text-generation-webui, else it won''t see the changes.</p>

          '
        raw: 'Did you update Transformers?  And is that with Loader: AutoGPTQ?


          Also try downloading hte model again (same branch, ie main), just to double
          check the download is OK


          Earlier today I confirmed text-gen-ui works OK with AutoGPTQ + the main
          file, using "no inject fused attention" and with Transformers updated to
          latest version - which be aware has to be done inside the Python environment
          of text-generation-webui, else it won''t see the changes.'
        updatedAt: '2023-07-19T15:23:19.913Z'
      numEdits: 0
      reactions: []
    id: 64b7ffe7f53ae848e735888e
    type: comment
  author: TheBloke
  content: 'Did you update Transformers?  And is that with Loader: AutoGPTQ?


    Also try downloading hte model again (same branch, ie main), just to double check
    the download is OK


    Earlier today I confirmed text-gen-ui works OK with AutoGPTQ + the main file,
    using "no inject fused attention" and with Transformers updated to latest version
    - which be aware has to be done inside the Python environment of text-generation-webui,
    else it won''t see the changes.'
  created_at: 2023-07-19 14:23:19+00:00
  edited: false
  hidden: false
  id: 64b7ffe7f53ae848e735888e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
      fullname: Harper Grieve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpergrieve
      type: user
    createdAt: '2023-07-19T15:31:34.000Z'
    data:
      edited: false
      editors:
      - harpergrieve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7413209080696106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
          fullname: Harper Grieve
          isHf: false
          isPro: false
          name: harpergrieve
          type: user
        html: '<p>Yep, just updated transformers and it got me past the oom error.
          Now gettting that self_attn.q_proj.weight error on both main and gptq-4bit-32g-actorder_True.
          Can the inject_fused_attention=False flag be set through a env var like
          bits and groupsize?</p>

          '
        raw: 'Yep, just updated transformers and it got me past the oom error. Now
          gettting that self_attn.q_proj.weight error on both main and gptq-4bit-32g-actorder_True.
          Can the inject_fused_attention=False flag be set through a env var like
          bits and groupsize?

          '
        updatedAt: '2023-07-19T15:31:34.430Z'
      numEdits: 0
      reactions: []
    id: 64b801d645f3511db214c37c
    type: comment
  author: harpergrieve
  content: 'Yep, just updated transformers and it got me past the oom error. Now gettting
    that self_attn.q_proj.weight error on both main and gptq-4bit-32g-actorder_True.
    Can the inject_fused_attention=False flag be set through a env var like bits and
    groupsize?

    '
  created_at: 2023-07-19 14:31:34+00:00
  edited: false
  hidden: false
  id: 64b801d645f3511db214c37c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T15:43:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.977412223815918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, I misread what you said earlier.  Text Generation Inference
          doesn''t work and I don''t know of a fix at this time.</p>

          '
        raw: Sorry, I misread what you said earlier.  Text Generation Inference doesn't
          work and I don't know of a fix at this time.
        updatedAt: '2023-07-19T15:43:28.371Z'
      numEdits: 0
      reactions: []
    id: 64b804a06c169983c997fe36
    type: comment
  author: TheBloke
  content: Sorry, I misread what you said earlier.  Text Generation Inference doesn't
    work and I don't know of a fix at this time.
  created_at: 2023-07-19 14:43:28+00:00
  edited: false
  hidden: false
  id: 64b804a06c169983c997fe36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
      fullname: Harper Grieve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpergrieve
      type: user
    createdAt: '2023-07-19T15:54:05.000Z'
    data:
      edited: false
      editors:
      - harpergrieve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9383741021156311
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6467b937946476c5d217f8b9/P99Zjjt7CFLX-Ib-nqRcl.jpeg?w=200&h=200&f=face
          fullname: Harper Grieve
          isHf: false
          isPro: false
          name: harpergrieve
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Thanks for the\
          \ help, and thanks for the models! I appreciate your work. Ill try and look\
          \ into it and report back any findings if i do get it working...</p>\n"
        raw: '@TheBloke Thanks for the help, and thanks for the models! I appreciate
          your work. Ill try and look into it and report back any findings if i do
          get it working...

          '
        updatedAt: '2023-07-19T15:54:05.834Z'
      numEdits: 0
      reactions: []
    id: 64b8071d8ba7d6c922e14158
    type: comment
  author: harpergrieve
  content: '@TheBloke Thanks for the help, and thanks for the models! I appreciate
    your work. Ill try and look into it and report back any findings if i do get it
    working...

    '
  created_at: 2023-07-19 14:54:05+00:00
  edited: false
  hidden: false
  id: 64b8071d8ba7d6c922e14158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
      fullname: Ulymp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ulymp
      type: user
    createdAt: '2023-07-20T16:09:49.000Z'
    data:
      edited: false
      editors:
      - ulymp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8001382350921631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
          fullname: Ulymp
          isHf: false
          isPro: false
          name: ulymp
          type: user
        html: '<p>I guess not even the <code>gptq-3bit--1g-actorder_True</code> will
          fit into a 24 GB GPU (e.g. RTX 3090)?</p>

          '
        raw: I guess not even the `gptq-3bit--1g-actorder_True` will fit into a 24
          GB GPU (e.g. RTX 3090)?
        updatedAt: '2023-07-20T16:09:49.280Z'
      numEdits: 0
      reactions: []
    id: 64b95c4d7ac999b4a163eefc
    type: comment
  author: ulymp
  content: I guess not even the `gptq-3bit--1g-actorder_True` will fit into a 24 GB
    GPU (e.g. RTX 3090)?
  created_at: 2023-07-20 15:09:49+00:00
  edited: false
  hidden: false
  id: 64b95c4d7ac999b4a163eefc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T09:37:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9374719262123108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Sorry, I misread what you said earlier.  Text Generation Inference doesn''t
          work and I don''t know of a fix at this time.</p>

          </blockquote>

          <p>FYI TGI should now work with this model, a PR was merged the other day</p>

          <blockquote>

          <p>I guess not even the <code>gptq-3bit--1g-actorder_True</code> will fit
          into a 24 GB GPU (e.g. RTX 3090)?</p>

          </blockquote>

          <p>Yeah I don''t think it will. You will need 2 x 24GB GPU, or 1 x 48GB
          GPU.  Or an asynchronous setup like 1 x 24GB + 1 x 12GB.</p>

          <p>But 1 x 24GB won''t fit it I''m afraid.  Even the smallest file is 26GB.</p>

          '
        raw: '> Sorry, I misread what you said earlier.  Text Generation Inference
          doesn''t work and I don''t know of a fix at this time.


          FYI TGI should now work with this model, a PR was merged the other day


          > I guess not even the `gptq-3bit--1g-actorder_True` will fit into a 24
          GB GPU (e.g. RTX 3090)?


          Yeah I don''t think it will. You will need 2 x 24GB GPU, or 1 x 48GB GPU.  Or
          an asynchronous setup like 1 x 24GB + 1 x 12GB.


          But 1 x 24GB won''t fit it I''m afraid.  Even the smallest file is 26GB.'
        updatedAt: '2023-07-21T09:37:02.808Z'
      numEdits: 0
      reactions: []
    id: 64ba51be41078fd9a059c1a6
    type: comment
  author: TheBloke
  content: '> Sorry, I misread what you said earlier.  Text Generation Inference doesn''t
    work and I don''t know of a fix at this time.


    FYI TGI should now work with this model, a PR was merged the other day


    > I guess not even the `gptq-3bit--1g-actorder_True` will fit into a 24 GB GPU
    (e.g. RTX 3090)?


    Yeah I don''t think it will. You will need 2 x 24GB GPU, or 1 x 48GB GPU.  Or
    an asynchronous setup like 1 x 24GB + 1 x 12GB.


    But 1 x 24GB won''t fit it I''m afraid.  Even the smallest file is 26GB.'
  created_at: 2023-07-21 08:37:02+00:00
  edited: false
  hidden: false
  id: 64ba51be41078fd9a059c1a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-07-24T14:55:05.000Z'
    data:
      edited: false
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9229020476341248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: '<p>Try the binary of ggml.ccp (latest commit). I was able to load the
          ggmlv3 with 24 gb vram and 40gb additional ram. Got 0,83 token/second on
          4090 and i9/9900k on the non-chat version. Oobabooga is not updated / merged
          yet.</p>

          '
        raw: Try the binary of ggml.ccp (latest commit). I was able to load the ggmlv3
          with 24 gb vram and 40gb additional ram. Got 0,83 token/second on 4090 and
          i9/9900k on the non-chat version. Oobabooga is not updated / merged yet.
        updatedAt: '2023-07-24T14:55:05.304Z'
      numEdits: 0
      reactions: []
    id: 64be90c976a6e2efcccc6494
    type: comment
  author: Silverspoon7
  content: Try the binary of ggml.ccp (latest commit). I was able to load the ggmlv3
    with 24 gb vram and 40gb additional ram. Got 0,83 token/second on 4090 and i9/9900k
    on the non-chat version. Oobabooga is not updated / merged yet.
  created_at: 2023-07-24 13:55:05+00:00
  edited: false
  hidden: false
  id: 64be90c976a6e2efcccc6494
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-07-28T06:30:36.000Z'
    data:
      edited: true
      editors:
      - Squeezitgirdle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9504550099372864
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: '<p>It''s slow (0.8 - 0.9 tokens/s), but with exlammaHF I got it working
          on a 24gb 4090.</p>

          '
        raw: It's slow (0.8 - 0.9 tokens/s), but with exlammaHF I got it working on
          a 24gb 4090.
        updatedAt: '2023-07-28T06:31:06.010Z'
      numEdits: 1
      reactions: []
    id: 64c3608cd4ace3ce33132f98
    type: comment
  author: Squeezitgirdle
  content: It's slow (0.8 - 0.9 tokens/s), but with exlammaHF I got it working on
    a 24gb 4090.
  created_at: 2023-07-28 05:30:36+00:00
  edited: true
  hidden: false
  id: 64c3608cd4ace3ce33132f98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
      fullname: Ulymp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ulymp
      type: user
    createdAt: '2023-07-28T21:49:08.000Z'
    data:
      edited: false
      editors:
      - ulymp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471768140792847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
          fullname: Ulymp
          isHf: false
          isPro: false
          name: ulymp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Squeezitgirdle&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Squeezitgirdle\"\
          >@<span class=\"underline\">Squeezitgirdle</span></a></span>\n\n\t</span></span>\
          \ How did you do that? AFAIK Exllama does not support offloading to CPU\
          \ RAM. Or is that supported using the HF variant?</p>\n"
        raw: '@Squeezitgirdle How did you do that? AFAIK Exllama does not support
          offloading to CPU RAM. Or is that supported using the HF variant?'
        updatedAt: '2023-07-28T21:49:08.781Z'
      numEdits: 0
      reactions: []
    id: 64c437d4589826641b8646c0
    type: comment
  author: ulymp
  content: '@Squeezitgirdle How did you do that? AFAIK Exllama does not support offloading
    to CPU RAM. Or is that supported using the HF variant?'
  created_at: 2023-07-28 20:49:08+00:00
  edited: false
  hidden: false
  id: 64c437d4589826641b8646c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ffb952773f33861d936dc6e9d30e150.svg
      fullname: Szulc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaciejSzulc
      type: user
    createdAt: '2023-07-31T15:37:30.000Z'
    data:
      edited: false
      editors:
      - MaciejSzulc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9721164703369141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ffb952773f33861d936dc6e9d30e150.svg
          fullname: Szulc
          isHf: false
          isPro: false
          name: MaciejSzulc
          type: user
        html: '<p>How much 2x separated GPU is slower than one large vram GPU?</p>

          '
        raw: How much 2x separated GPU is slower than one large vram GPU?
        updatedAt: '2023-07-31T15:37:30.252Z'
      numEdits: 0
      reactions: []
    id: 64c7d53af3d2a59a4302910f
    type: comment
  author: MaciejSzulc
  content: How much 2x separated GPU is slower than one large vram GPU?
  created_at: 2023-07-31 14:37:30+00:00
  edited: false
  hidden: false
  id: 64c7d53af3d2a59a4302910f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-08-01T08:43:43.000Z'
    data:
      edited: true
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9355998635292053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: "<p>Depends on gpu model, electrical pci-e slots and cpu, I think. If\
          \ you have two full pci-e 16x slots (not available on consumer Mainboards)\
          \ with two rtx 3080, it will depend only on drivers and multi gpu supporting\
          \ the models loader. Some versions of autogptq may be slow or even not better\
          \ than with one gpu.<br>I figured out, that in use of private hobby, a 60-70b\
          \ model isn\u2019t worth to play with, because the difference to a good\
          \ 13 or 30b model is not that big. Sometimes, you are missing those little\
          \ amount of percentage a model does not answer in your language. In this\
          \ case, you may train it by yourself by simply training some books. Llama-2\
          \ 7b may work for you with 12GB VRAM. You will need 20-30 gpu hours and\
          \ a minimum of 50mb raw text files in high quality (no page numbers and\
          \ other garbage). Today, I did my first working Lora merge, which makes\
          \ me able to train in short blocks with 1MB text blocks. Training a 13b\
          \ llama2 model with only a few MByte of German text seems to work better\
          \ than I hoped. If you insist interfering with a 70b model, try pure llama.ccp.\
          \ It is faster because of lower prompt size, so like talking above you may\
          \ reach 0,8 tokens per second. Prompting with 4K history, you may have to\
          \ wait minutes to get a response while having 0,02 tokens per second. And\
          \ we are talking about a 4090 gpu. with full multi gpu support and running\
          \ under Linux, this should get much faster with two of these gpus.</p>\n"
        raw: "Depends on gpu model, electrical pci-e slots and cpu, I think. If you\
          \ have two full pci-e 16x slots (not available on consumer Mainboards) with\
          \ two rtx 3080, it will depend only on drivers and multi gpu supporting\
          \ the models loader. Some versions of autogptq may be slow or even not better\
          \ than with one gpu.\nI figured out, that in use of private hobby, a 60-70b\
          \ model isn\u2019t worth to play with, because the difference to a good\
          \ 13 or 30b model is not that big. Sometimes, you are missing those little\
          \ amount of percentage a model does not answer in your language. In this\
          \ case, you may train it by yourself by simply training some books. Llama-2\
          \ 7b may work for you with 12GB VRAM. You will need 20-30 gpu hours and\
          \ a minimum of 50mb raw text files in high quality (no page numbers and\
          \ other garbage). Today, I did my first working Lora merge, which makes\
          \ me able to train in short blocks with 1MB text blocks. Training a 13b\
          \ llama2 model with only a few MByte of German text seems to work better\
          \ than I hoped. If you insist interfering with a 70b model, try pure llama.ccp.\
          \ It is faster because of lower prompt size, so like talking above you may\
          \ reach 0,8 tokens per second. Prompting with 4K history, you may have to\
          \ wait minutes to get a response while having 0,02 tokens per second. And\
          \ we are talking about a 4090 gpu. with full multi gpu support and running\
          \ under Linux, this should get much faster with two of these gpus."
        updatedAt: '2023-08-01T08:44:23.375Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - antonbelousov
        - MaciejSzulc
        - hugginggair
    id: 64c8c5bf1c23fb9a2be79ff9
    type: comment
  author: Silverspoon7
  content: "Depends on gpu model, electrical pci-e slots and cpu, I think. If you\
    \ have two full pci-e 16x slots (not available on consumer Mainboards) with two\
    \ rtx 3080, it will depend only on drivers and multi gpu supporting the models\
    \ loader. Some versions of autogptq may be slow or even not better than with one\
    \ gpu.\nI figured out, that in use of private hobby, a 60-70b model isn\u2019\
    t worth to play with, because the difference to a good 13 or 30b model is not\
    \ that big. Sometimes, you are missing those little amount of percentage a model\
    \ does not answer in your language. In this case, you may train it by yourself\
    \ by simply training some books. Llama-2 7b may work for you with 12GB VRAM. You\
    \ will need 20-30 gpu hours and a minimum of 50mb raw text files in high quality\
    \ (no page numbers and other garbage). Today, I did my first working Lora merge,\
    \ which makes me able to train in short blocks with 1MB text blocks. Training\
    \ a 13b llama2 model with only a few MByte of German text seems to work better\
    \ than I hoped. If you insist interfering with a 70b model, try pure llama.ccp.\
    \ It is faster because of lower prompt size, so like talking above you may reach\
    \ 0,8 tokens per second. Prompting with 4K history, you may have to wait minutes\
    \ to get a response while having 0,02 tokens per second. And we are talking about\
    \ a 4090 gpu. with full multi gpu support and running under Linux, this should\
    \ get much faster with two of these gpus."
  created_at: 2023-08-01 07:43:43+00:00
  edited: true
  hidden: false
  id: 64c8c5bf1c23fb9a2be79ff9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2bacda194ece07696bf32274e5e05a9.svg
      fullname: yanmengxiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanmengxiang
      type: user
    createdAt: '2023-08-14T03:10:24.000Z'
    data:
      edited: false
      editors:
      - yanmengxiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6585041284561157
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2bacda194ece07696bf32274e5e05a9.svg
          fullname: yanmengxiang
          isHf: false
          isPro: false
          name: yanmengxiang
          type: user
        html: '<p>About Llama-2-70B-chat ,fp16, if I have 8*A10(24G),can I run it
          ,thanks!</p>

          '
        raw: About Llama-2-70B-chat ,fp16, if I have 8*A10(24G),can I run it ,thanks!
        updatedAt: '2023-08-14T03:10:24.916Z'
      numEdits: 0
      reactions: []
    id: 64d99b202fe2c112646c2b40
    type: comment
  author: yanmengxiang
  content: About Llama-2-70B-chat ,fp16, if I have 8*A10(24G),can I run it ,thanks!
  created_at: 2023-08-14 02:10:24+00:00
  edited: false
  hidden: false
  id: 64d99b202fe2c112646c2b40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-14T10:36:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8818485140800476
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The size of Llama 2 70B fp16 is around 130GB so no you can''t run
          Llama 2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or
          6 x 24GB GPU to run fp16.  </p>

          <p>But you can run Llama 2 70B 4-bit GPTQ on 2 x 24GB and many people are
          doing this.</p>

          '
        raw: "The size of Llama 2 70B fp16 is around 130GB so no you can't run Llama\
          \ 2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or 6\
          \ x 24GB GPU to run fp16.  \n\nBut you can run Llama 2 70B 4-bit GPTQ on\
          \ 2 x 24GB and many people are doing this."
        updatedAt: '2023-08-14T10:36:02.432Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - kobkrit
        - tixierae
        - OSK-Creative-Tech
        - tefod
        - zrzakhan
    id: 64da03923a7ab21ea7c27606
    type: comment
  author: TheBloke
  content: "The size of Llama 2 70B fp16 is around 130GB so no you can't run Llama\
    \ 2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or 6 x 24GB\
    \ GPU to run fp16.  \n\nBut you can run Llama 2 70B 4-bit GPTQ on 2 x 24GB and\
    \ many people are doing this."
  created_at: 2023-08-14 09:36:02+00:00
  edited: false
  hidden: false
  id: 64da03923a7ab21ea7c27606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68625e96c7a15297808df14d629345db.svg
      fullname: yanmengxiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanmengxiang666
      type: user
    createdAt: '2023-08-14T11:30:30.000Z'
    data:
      edited: false
      editors:
      - yanmengxiang666
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7480583190917969
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68625e96c7a15297808df14d629345db.svg
          fullname: yanmengxiang
          isHf: false
          isPro: false
          name: yanmengxiang666
          type: user
        html: "<p>hello\uFF0Cwhat's the need about the RAM in Llama 2 70B fp16\uFF1F\
          </p>\n"
        raw: "hello\uFF0Cwhat's the need about the RAM in Llama 2 70B fp16\uFF1F"
        updatedAt: '2023-08-14T11:30:30.368Z'
      numEdits: 0
      reactions: []
    id: 64da10568da011d656ed4030
    type: comment
  author: yanmengxiang666
  content: "hello\uFF0Cwhat's the need about the RAM in Llama 2 70B fp16\uFF1F"
  created_at: 2023-08-14 10:30:30+00:00
  edited: false
  hidden: false
  id: 64da10568da011d656ed4030
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-14T11:34:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.995011568069458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p> I think you only need as much RAM as the size of one shard, which
          is only about 10GB.  64GB would be fine for example. Generally you won''t
          find machines that have less RAM than VRAM anyway.  </p>

          '
        raw: ' I think you only need as much RAM as the size of one shard, which is
          only about 10GB.  64GB would be fine for example. Generally you won''t find
          machines that have less RAM than VRAM anyway.  '
        updatedAt: '2023-08-14T11:34:41.874Z'
      numEdits: 0
      reactions: []
    id: 64da1151311afacb538e4c42
    type: comment
  author: TheBloke
  content: ' I think you only need as much RAM as the size of one shard, which is
    only about 10GB.  64GB would be fine for example. Generally you won''t find machines
    that have less RAM than VRAM anyway.  '
  created_at: 2023-08-14 10:34:41+00:00
  edited: false
  hidden: false
  id: 64da1151311afacb538e4c42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68625e96c7a15297808df14d629345db.svg
      fullname: yanmengxiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yanmengxiang666
      type: user
    createdAt: '2023-08-24T06:47:03.000Z'
    data:
      edited: true
      editors:
      - yanmengxiang666
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4392023980617523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68625e96c7a15297808df14d629345db.svg
          fullname: yanmengxiang
          isHf: false
          isPro: false
          name: yanmengxiang666
          type: user
        html: "<p>my GPU is 16 * A10(16 * 24G). I ask many people to solve this problem,but\
          \ failed.<br>url\uFF1A<a rel=\"nofollow\" href=\"https://github.com/h2oai/h2ogpt/issues/692\"\
          >https://github.com/h2oai/h2ogpt/issues/692</a><br>command:CUDA_VISIBLE_DEVICES=\"\
          0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\" python generate.py --base_model=/data/model/llama2-70b-chat/\
          \ --prompt_type=llama2 --use_gpu_id=False --share=True<br>It appears a BUG\
          \ when I use GPUs &gt; 10:<br><a rel=\"nofollow\" href=\"https://user-images.githubusercontent.com/74184102/262883754-9f065f93-4e54-4708-8584-6b80ccf438ab.png\"\
          >https://user-images.githubusercontent.com/74184102/262883754-9f065f93-4e54-4708-8584-6b80ccf438ab.png</a></p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64bfbbe52add2625e35b612b/AV3iif4C_qHlB-rUuc3Zs.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64bfbbe52add2625e35b612b/AV3iif4C_qHlB-rUuc3Zs.png\"\
          ></a><br>10 gpu is ok\uFF01But more gpu is helpful!<br>When I use GPU &lt;=\
          \ 10, it can work! Like this command:CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9\
          \ python generate.py --base_model=/data/model/llama2-70b-chat/ --prompt_type=llama2\
          \ --use_gpu_id=False --share=True<br>But I need more gpu because longer\
          \ prompt need more gpu memmory.Thanks!</p>\n"
        raw: "my GPU is 16 * A10(16 * 24G). I ask many people to solve this problem,but\
          \ failed.\nurl\uFF1Ahttps://github.com/h2oai/h2ogpt/issues/692\ncommand:CUDA_VISIBLE_DEVICES=\"\
          0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\" python generate.py --base_model=/data/model/llama2-70b-chat/\
          \ --prompt_type=llama2 --use_gpu_id=False --share=True\nIt appears a BUG\
          \ when I use GPUs > 10:\nhttps://user-images.githubusercontent.com/74184102/262883754-9f065f93-4e54-4708-8584-6b80ccf438ab.png\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bfbbe52add2625e35b612b/AV3iif4C_qHlB-rUuc3Zs.png)\n\
          10 gpu is ok\uFF01But more gpu is helpful!\nWhen I use GPU <= 10, it can\
          \ work! Like this command:CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9 python\
          \ generate.py --base_model=/data/model/llama2-70b-chat/ --prompt_type=llama2\
          \ --use_gpu_id=False --share=True\nBut I need more gpu because longer prompt\
          \ need more gpu memmory.Thanks!\n"
        updatedAt: '2023-08-24T07:32:18.234Z'
      numEdits: 2
      reactions: []
    id: 64e6fce70d1cc5d5e37efc1e
    type: comment
  author: yanmengxiang666
  content: "my GPU is 16 * A10(16 * 24G). I ask many people to solve this problem,but\
    \ failed.\nurl\uFF1Ahttps://github.com/h2oai/h2ogpt/issues/692\ncommand:CUDA_VISIBLE_DEVICES=\"\
    0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\" python generate.py --base_model=/data/model/llama2-70b-chat/\
    \ --prompt_type=llama2 --use_gpu_id=False --share=True\nIt appears a BUG when\
    \ I use GPUs > 10:\nhttps://user-images.githubusercontent.com/74184102/262883754-9f065f93-4e54-4708-8584-6b80ccf438ab.png\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64bfbbe52add2625e35b612b/AV3iif4C_qHlB-rUuc3Zs.png)\n\
    10 gpu is ok\uFF01But more gpu is helpful!\nWhen I use GPU <= 10, it can work!\
    \ Like this command:CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9 python generate.py\
    \ --base_model=/data/model/llama2-70b-chat/ --prompt_type=llama2 --use_gpu_id=False\
    \ --share=True\nBut I need more gpu because longer prompt need more gpu memmory.Thanks!\n"
  created_at: 2023-08-24 05:47:03+00:00
  edited: true
  hidden: false
  id: 64e6fce70d1cc5d5e37efc1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45c8a3c692c745a5a49f3677b5084209.svg
      fullname: Mateusz Gilewicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gileneo
      type: user
    createdAt: '2023-08-28T04:17:33.000Z'
    data:
      edited: true
      editors:
      - gileneo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8598628044128418
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45c8a3c692c745a5a49f3677b5084209.svg
          fullname: Mateusz Gilewicz
          isHf: false
          isPro: false
          name: gileneo
          type: user
        html: '<blockquote>

          <p>The size of Llama 2 70B fp16 is around 130GB so no you can''t run Llama
          2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or 6 x
          24GB GPU to run fp16.  </p>

          <p>But you can run Llama 2 70B 4-bit GPTQ on 2 x 24GB and many people are
          doing this.</p>

          </blockquote>

          <p>so Mac Studio with M2 Ultra 196GB would run Llama 2 70B fp16? </p>

          '
        raw: "> The size of Llama 2 70B fp16 is around 130GB so no you can't run Llama\
          \ 2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or 6\
          \ x 24GB GPU to run fp16.  \n> \n> But you can run Llama 2 70B 4-bit GPTQ\
          \ on 2 x 24GB and many people are doing this.\n\nso Mac Studio with M2 Ultra\
          \ 196GB would run Llama 2 70B fp16? "
        updatedAt: '2023-08-28T04:17:56.216Z'
      numEdits: 1
      reactions: []
    id: 64ec1fddefdbd7cc0932ccd8
    type: comment
  author: gileneo
  content: "> The size of Llama 2 70B fp16 is around 130GB so no you can't run Llama\
    \ 2 70B fp16 with 2 x 24GB.  You need 2 x 80GB GPU or 4 x 48GB GPU or 6 x 24GB\
    \ GPU to run fp16.  \n> \n> But you can run Llama 2 70B 4-bit GPTQ on 2 x 24GB\
    \ and many people are doing this.\n\nso Mac Studio with M2 Ultra 196GB would run\
    \ Llama 2 70B fp16? "
  created_at: 2023-08-28 03:17:33+00:00
  edited: true
  hidden: false
  id: 64ec1fddefdbd7cc0932ccd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf83796635a2de2e9f8d32677b9f2eab.svg
      fullname: Alex Cib
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: axbon
      type: user
    createdAt: '2023-08-30T12:03:34.000Z'
    data:
      edited: false
      editors:
      - axbon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8851923942565918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf83796635a2de2e9f8d32677b9f2eab.svg
          fullname: Alex Cib
          isHf: false
          isPro: false
          name: axbon
          type: user
        html: '<p>Say you have a beefy setup with some 4xL40 gpus or similar, do these
          need to be connected with nvlink to get good perf or enough to just reside
          in the same physical box for llama 70b?</p>

          '
        raw: Say you have a beefy setup with some 4xL40 gpus or similar, do these
          need to be connected with nvlink to get good perf or enough to just reside
          in the same physical box for llama 70b?
        updatedAt: '2023-08-30T12:03:34.388Z'
      numEdits: 0
      reactions: []
    id: 64ef3016050ece66c1ff8748
    type: comment
  author: axbon
  content: Say you have a beefy setup with some 4xL40 gpus or similar, do these need
    to be connected with nvlink to get good perf or enough to just reside in the same
    physical box for llama 70b?
  created_at: 2023-08-30 11:03:34+00:00
  edited: false
  hidden: false
  id: 64ef3016050ece66c1ff8748
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3041a92675d91c196a156dcd763fb435.svg
      fullname: Mohamed Riyaz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: softriyaz
      type: user
    createdAt: '2023-09-05T11:18:15.000Z'
    data:
      edited: true
      editors:
      - softriyaz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9436043500900269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3041a92675d91c196a156dcd763fb435.svg
          fullname: Mohamed Riyaz
          isHf: false
          isPro: false
          name: softriyaz
          type: user
        html: '<p>I am running on Windows server with xenon processor, 4 Tesla GPUs
          with each 64 GB. Only one user is able to interact with at a time. The following
          error appears when another user asks a question or feeds with a prompt while
          the first one is still processing. Please advise.</p>

          <p>Error Encountered<br>Error occurred during text generation: {"detail":{"msg":"Server
          is busy; please try again later.","type":"service_unavailable"}}</p>

          '
        raw: 'I am running on Windows server with xenon processor, 4 Tesla GPUs with
          each 64 GB. Only one user is able to interact with at a time. The following
          error appears when another user asks a question or feeds with a prompt while
          the first one is still processing. Please advise.


          Error Encountered

          Error occurred during text generation: {"detail":{"msg":"Server is busy;
          please try again later.","type":"service_unavailable"}}'
        updatedAt: '2023-09-05T11:22:03.914Z'
      numEdits: 3
      reactions: []
    id: 64f70e7796d7e4e686356c8a
    type: comment
  author: softriyaz
  content: 'I am running on Windows server with xenon processor, 4 Tesla GPUs with
    each 64 GB. Only one user is able to interact with at a time. The following error
    appears when another user asks a question or feeds with a prompt while the first
    one is still processing. Please advise.


    Error Encountered

    Error occurred during text generation: {"detail":{"msg":"Server is busy; please
    try again later.","type":"service_unavailable"}}'
  created_at: 2023-09-05 10:18:15+00:00
  edited: true
  hidden: false
  id: 64f70e7796d7e4e686356c8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b607c6a3870170356417a2953c37891c.svg
      fullname: Saikiran Vajrapu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saikiran
      type: user
    createdAt: '2023-10-07T03:53:11.000Z'
    data:
      edited: false
      editors:
      - Saikiran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9468099474906921
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b607c6a3870170356417a2953c37891c.svg
          fullname: Saikiran Vajrapu
          isHf: false
          isPro: false
          name: Saikiran
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Sorry, I misread what you said earlier.\
          \  Text Generation Inference doesn't work and I don't know of a fix at this\
          \ time.</p>\n</blockquote>\n<p>FYI TGI should now work with this model,\
          \ a PR was merged the other day</p>\n</blockquote>\n<p>It's October and\
          \ it still does not work. The error about self_attn.q_proj.weight still\
          \ comes while loading 70b chat gptq on text generation inference <span data-props=\"\
          {&quot;user&quot;:&quot;Bloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Bloke\">@<span class=\"underline\">Bloke</span></a></span>\n\
          \n\t</span></span> anything I am missing. I am using the latest tgi version\
          \ docker and required cuda configs as well.</p>\n"
        raw: "> > Sorry, I misread what you said earlier.  Text Generation Inference\
          \ doesn't work and I don't know of a fix at this time.\n> \n> FYI TGI should\
          \ now work with this model, a PR was merged the other day\n>\n\nIt's October\
          \ and it still does not work. The error about self_attn.q_proj.weight still\
          \ comes while loading 70b chat gptq on text generation inference @Bloke\
          \ anything I am missing. I am using the latest tgi version docker and required\
          \ cuda configs as well."
        updatedAt: '2023-10-07T03:53:11.772Z'
      numEdits: 0
      reactions: []
    id: 6520d627ef06bb997515f149
    type: comment
  author: Saikiran
  content: "> > Sorry, I misread what you said earlier.  Text Generation Inference\
    \ doesn't work and I don't know of a fix at this time.\n> \n> FYI TGI should now\
    \ work with this model, a PR was merged the other day\n>\n\nIt's October and it\
    \ still does not work. The error about self_attn.q_proj.weight still comes while\
    \ loading 70b chat gptq on text generation inference @Bloke anything I am missing.\
    \ I am using the latest tgi version docker and required cuda configs as well."
  created_at: 2023-10-07 02:53:11+00:00
  edited: false
  hidden: false
  id: 6520d627ef06bb997515f149
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/095313e6d8542ea2c2013b5eb5cc21fc.svg
      fullname: Federica
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sprockif
      type: user
    createdAt: '2023-12-15T15:49:57.000Z'
    data:
      edited: false
      editors:
      - Sprockif
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6833918690681458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/095313e6d8542ea2c2013b5eb5cc21fc.svg
          fullname: Federica
          isHf: false
          isPro: false
          name: Sprockif
          type: user
        html: '<p>Hi, I have 2 GPUs of which 1 Nvidia. I want to run Llama2 7b-chat
          only using Nvidia (Linux Debian system).<br>I normally run Llama2 with those
          commands  (from this guide <a rel="nofollow" href="https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/#preparing-to-install-l-la-ma-2">https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/#preparing-to-install-l-la-ma-2</a>)<br>#conda
          activate TextGen2<br>#cd text-generation-webui<br>#python server.py<br>could
          you suggest me How to do?<br>Thanks :)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6560ac1e2bdaccfcd5a6860f/I7wkOsgBEQV9ES8MgmC-2.jpeg"><img
          alt="nvidia details.JPG" src="https://cdn-uploads.huggingface.co/production/uploads/6560ac1e2bdaccfcd5a6860f/I7wkOsgBEQV9ES8MgmC-2.jpeg"></a></p>

          '
        raw: 'Hi, I have 2 GPUs of which 1 Nvidia. I want to run Llama2 7b-chat only
          using Nvidia (Linux Debian system).

          I normally run Llama2 with those commands  (from this guide https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/#preparing-to-install-l-la-ma-2)

          #conda activate TextGen2

          #cd text-generation-webui

          #python server.py

          could you suggest me How to do?

          Thanks :)



          ![nvidia details.JPG](https://cdn-uploads.huggingface.co/production/uploads/6560ac1e2bdaccfcd5a6860f/I7wkOsgBEQV9ES8MgmC-2.jpeg)

          '
        updatedAt: '2023-12-15T15:49:57.080Z'
      numEdits: 0
      reactions: []
    id: 657c75a55e3f656ed4bd7006
    type: comment
  author: Sprockif
  content: 'Hi, I have 2 GPUs of which 1 Nvidia. I want to run Llama2 7b-chat only
    using Nvidia (Linux Debian system).

    I normally run Llama2 with those commands  (from this guide https://lachieslifestyle.com/2023/07/29/how-to-install-llama-2/#preparing-to-install-l-la-ma-2)

    #conda activate TextGen2

    #cd text-generation-webui

    #python server.py

    could you suggest me How to do?

    Thanks :)



    ![nvidia details.JPG](https://cdn-uploads.huggingface.co/production/uploads/6560ac1e2bdaccfcd5a6860f/I7wkOsgBEQV9ES8MgmC-2.jpeg)

    '
  created_at: 2023-12-15 15:49:57+00:00
  edited: false
  hidden: false
  id: 657c75a55e3f656ed4bd7006
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-12-29T20:20:32.000Z'
    data:
      edited: false
      editors:
      - Squeezitgirdle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9695746898651123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Squeezitgirdle&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Squeezitgirdle\"\
          >@<span class=\"underline\">Squeezitgirdle</span></a></span>\n\n\t</span></span>\
          \ How did you do that? AFAIK Exllama does not support offloading to CPU\
          \ RAM. Or is that supported using the HF variant?</p>\n</blockquote>\n<p>Sorry\
          \ I'm just now responding.</p>\n<p>I have absolutely no idea. I did it once\
          \ using LM Studio, but that's it. I haven't been able to do it again after\
          \ updating LM studio.</p>\n"
        raw: '> @Squeezitgirdle How did you do that? AFAIK Exllama does not support
          offloading to CPU RAM. Or is that supported using the HF variant?


          Sorry I''m just now responding.


          I have absolutely no idea. I did it once using LM Studio, but that''s it.
          I haven''t been able to do it again after updating LM studio.'
        updatedAt: '2023-12-29T20:20:32.260Z'
      numEdits: 0
      reactions: []
    id: 658f2a109e16fa7510257e2b
    type: comment
  author: Squeezitgirdle
  content: '> @Squeezitgirdle How did you do that? AFAIK Exllama does not support
    offloading to CPU RAM. Or is that supported using the HF variant?


    Sorry I''m just now responding.


    I have absolutely no idea. I did it once using LM Studio, but that''s it. I haven''t
    been able to do it again after updating LM studio.'
  created_at: 2023-12-29 20:20:32+00:00
  edited: false
  hidden: false
  id: 658f2a109e16fa7510257e2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'What GPU is needed for this 70B one? '
