!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sterogn
conflicting_files: null
created_at: 2023-07-19 08:53:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
      fullname: Steffen Rogne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sterogn
      type: user
    createdAt: '2023-07-19T09:53:41.000Z'
    data:
      edited: false
      editors:
      - sterogn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.790749728679657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
          fullname: Steffen Rogne
          isHf: false
          isPro: false
          name: sterogn
          type: user
        html: '<p>Hello, I have been trying to get an answer from this model.</p>

          <p>If I load with AutoGPTQ - I receive no error when loading, or asking.
          But the result is always:<br>Output generated in 0.42 seconds (0.00 tokens/s,
          0 tokens, context 61, seed 1544358233)<br>Output generated in 0.42 seconds
          (0.00 tokens/s, 0 tokens, context 32, seed 168168177)</p>

          <p>And if I load the model with exLlama I get no error when loading, but
          when asking:<br>RuntimeError: shape ''[1, 46, 64, 128]'' is invalid for
          input of size 47104<br>Output generated in 0.46 seconds (0.00 tokens/s,
          0 tokens, context 47, seed 2009475660)</p>

          <p>I have updated my environment today in case there was updates to these
          new models.</p>

          <p>Running Nvidia H100 cu11.8 on Ubuntu 5.15.0 kernel.</p>

          <p>Any tips on how to make this work?</p>

          '
        raw: "Hello, I have been trying to get an answer from this model.\r\n\r\n\
          If I load with AutoGPTQ - I receive no error when loading, or asking. But\
          \ the result is always:\r\nOutput generated in 0.42 seconds (0.00 tokens/s,\
          \ 0 tokens, context 61, seed 1544358233) \r\nOutput generated in 0.42 seconds\
          \ (0.00 tokens/s, 0 tokens, context 32, seed 168168177)\r\n\r\nAnd if I\
          \ load the model with exLlama I get no error when loading, but when asking:\r\
          \nRuntimeError: shape '[1, 46, 64, 128]' is invalid for input of size 47104\r\
          \nOutput generated in 0.46 seconds (0.00 tokens/s, 0 tokens, context 47,\
          \ seed 2009475660)\r\n\r\nI have updated my environment today in case there\
          \ was updates to these new models.\r\n\r\nRunning Nvidia H100 cu11.8 on\
          \ Ubuntu 5.15.0 kernel.\r\n\r\nAny tips on how to make this work?"
        updatedAt: '2023-07-19T09:53:41.734Z'
      numEdits: 0
      reactions: []
    id: 64b7b2a590154e1f1d02de90
    type: comment
  author: sterogn
  content: "Hello, I have been trying to get an answer from this model.\r\n\r\nIf\
    \ I load with AutoGPTQ - I receive no error when loading, or asking. But the result\
    \ is always:\r\nOutput generated in 0.42 seconds (0.00 tokens/s, 0 tokens, context\
    \ 61, seed 1544358233) \r\nOutput generated in 0.42 seconds (0.00 tokens/s, 0\
    \ tokens, context 32, seed 168168177)\r\n\r\nAnd if I load the model with exLlama\
    \ I get no error when loading, but when asking:\r\nRuntimeError: shape '[1, 46,\
    \ 64, 128]' is invalid for input of size 47104\r\nOutput generated in 0.46 seconds\
    \ (0.00 tokens/s, 0 tokens, context 47, seed 2009475660)\r\n\r\nI have updated\
    \ my environment today in case there was updates to these new models.\r\n\r\n\
    Running Nvidia H100 cu11.8 on Ubuntu 5.15.0 kernel.\r\n\r\nAny tips on how to\
    \ make this work?"
  created_at: 2023-07-19 08:53:41+00:00
  edited: false
  hidden: false
  id: 64b7b2a590154e1f1d02de90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T11:25:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9488867521286011
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please update to the latest Transformers Github code to fix compatibility
          with AutoGPTQ and GPTQ-for-LLaMa.  ExLlama won''t work yet I believe.</p>

          <pre><code>pip3 install git+https://github.com/huggingface/transformers

          </code></pre>

          <p>I have updated the README to reflect this. I should have added it last
          night, but I didn''t get these uploaded until 4am and I forgot.</p>

          '
        raw: 'Please update to the latest Transformers Github code to fix compatibility
          with AutoGPTQ and GPTQ-for-LLaMa.  ExLlama won''t work yet I believe.


          ```

          pip3 install git+https://github.com/huggingface/transformers

          ```



          I have updated the README to reflect this. I should have added it last night,
          but I didn''t get these uploaded until 4am and I forgot.'
        updatedAt: '2023-07-19T11:25:26.490Z'
      numEdits: 0
      reactions: []
    id: 64b7c826da8017900e841655
    type: comment
  author: TheBloke
  content: 'Please update to the latest Transformers Github code to fix compatibility
    with AutoGPTQ and GPTQ-for-LLaMa.  ExLlama won''t work yet I believe.


    ```

    pip3 install git+https://github.com/huggingface/transformers

    ```



    I have updated the README to reflect this. I should have added it last night,
    but I didn''t get these uploaded until 4am and I forgot.'
  created_at: 2023-07-19 10:25:26+00:00
  edited: false
  hidden: false
  id: 64b7c826da8017900e841655
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
      fullname: Steffen Rogne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sterogn
      type: user
    createdAt: '2023-07-19T11:48:56.000Z'
    data:
      edited: false
      editors:
      - sterogn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658675193786621
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
          fullname: Steffen Rogne
          isHf: false
          isPro: false
          name: sterogn
          type: user
        html: '<p>Thank you very much for the response, and your awesome work!</p>

          <p>This did not change anything for me.<br>For now I can load the regular
          70B-chat model converted to HF - in 4bit. (not getting it to run in 8bit)
          So I guess this is something else with my environment! I will continue testing.</p>

          '
        raw: 'Thank you very much for the response, and your awesome work!


          This did not change anything for me.

          For now I can load the regular 70B-chat model converted to HF - in 4bit.
          (not getting it to run in 8bit) So I guess this is something else with my
          environment! I will continue testing.'
        updatedAt: '2023-07-19T11:48:56.501Z'
      numEdits: 0
      reactions: []
    id: 64b7cda8efefc8a7388cbebd
    type: comment
  author: sterogn
  content: 'Thank you very much for the response, and your awesome work!


    This did not change anything for me.

    For now I can load the regular 70B-chat model converted to HF - in 4bit. (not
    getting it to run in 8bit) So I guess this is something else with my environment!
    I will continue testing.'
  created_at: 2023-07-19 10:48:56+00:00
  edited: false
  hidden: false
  id: 64b7cda8efefc8a7388cbebd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T13:10:07.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6441582441329956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Apologies, I discovered what the issue is. A special setting is\
          \ required for AutoGPTQ.  I have updated the README to reflect this.</p>\n\
          <ul>\n<li>If using text-generation-webui, please tick the box <strong>no_inject_fused_attention</strong>\
          \ in the AutoGPTQ loader settings. Then save these settings and reload the\
          \ model.</li>\n<li>If using Python code, add <code>inject_fused_attention=False</code>\
          \ into the <code>.from_quantized()</code> call, like so:</li>\n</ul>\n<pre><code\
          \ class=\"language-python\">model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >False</span>,\n        inject_fused_attention=<span class=\"hljs-literal\"\
          >False</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n</code></pre>\n<p>Now it should work.</p>\n"
        raw: "Apologies, I discovered what the issue is. A special setting is required\
          \ for AutoGPTQ.  I have updated the README to reflect this.\n\n- If using\
          \ text-generation-webui, please tick the box **no_inject_fused_attention**\
          \ in the AutoGPTQ loader settings. Then save these settings and reload the\
          \ model.\n- If using Python code, add `inject_fused_attention=False` into\
          \ the `.from_quantized()` call, like so:\n```python\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=False,\n        inject_fused_attention=False,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          ```\n\nNow it should work."
        updatedAt: '2023-07-19T13:22:22.983Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - SQHY
    id: 64b7e0afb5d293ec95ad3d2d
    type: comment
  author: TheBloke
  content: "Apologies, I discovered what the issue is. A special setting is required\
    \ for AutoGPTQ.  I have updated the README to reflect this.\n\n- If using text-generation-webui,\
    \ please tick the box **no_inject_fused_attention** in the AutoGPTQ loader settings.\
    \ Then save these settings and reload the model.\n- If using Python code, add\
    \ `inject_fused_attention=False` into the `.from_quantized()` call, like so:\n\
    ```python\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n  \
    \      model_basename=model_basename,\n        use_safetensors=True,\n       \
    \ trust_remote_code=False,\n        inject_fused_attention=False,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n```\n\
    \nNow it should work."
  created_at: 2023-07-19 12:10:07+00:00
  edited: true
  hidden: false
  id: 64b7e0afb5d293ec95ad3d2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
      fullname: Steffen Rogne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sterogn
      type: user
    createdAt: '2023-07-19T13:36:01.000Z'
    data:
      edited: false
      editors:
      - sterogn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9170578122138977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a85c9b32af66330c69e22c1debd711c.svg
          fullname: Steffen Rogne
          isHf: false
          isPro: false
          name: sterogn
          type: user
        html: '<p>There we go!</p>

          <p>Awesome, appreciate the help.</p>

          '
        raw: 'There we go!


          Awesome, appreciate the help.'
        updatedAt: '2023-07-19T13:36:01.486Z'
      numEdits: 0
      reactions: []
    id: 64b7e6c1479b934973eeb0c7
    type: comment
  author: sterogn
  content: 'There we go!


    Awesome, appreciate the help.'
  created_at: 2023-07-19 12:36:01+00:00
  edited: false
  hidden: false
  id: 64b7e6c1479b934973eeb0c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
      fullname: Shouyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shouyi987
      type: user
    createdAt: '2023-07-19T13:53:46.000Z'
    data:
      edited: true
      editors:
      - Shouyi987
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9943131804466248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
          fullname: Shouyi Wang
          isHf: false
          isPro: false
          name: Shouyi987
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span><br>I followed\
          \ your steps, but it didn't work. It did produce some output, and it didn't\
          \ crash. However, the output is just gibberish.</p>\n<p>My env:<br>text-generation-webui\
          \ + AutoGPTQ</p>\n"
        raw: "@TheBloke \nI followed your steps, but it didn't work. It did produce\
          \ some output, and it didn't crash. However, the output is just gibberish.\n\
          \nMy env: \ntext-generation-webui + AutoGPTQ"
        updatedAt: '2023-07-19T13:54:48.975Z'
      numEdits: 1
      reactions: []
    id: 64b7eaea17681d64b1b704a6
    type: comment
  author: Shouyi987
  content: "@TheBloke \nI followed your steps, but it didn't work. It did produce\
    \ some output, and it didn't crash. However, the output is just gibberish.\n\n\
    My env: \ntext-generation-webui + AutoGPTQ"
  created_at: 2023-07-19 12:53:46+00:00
  edited: true
  hidden: false
  id: 64b7eaea17681d64b1b704a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675705996569-631b6a5ef6bc4be4a64fbf04.png?w=200&h=200&f=face
      fullname: will ess
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PLGRND
      type: user
    createdAt: '2023-07-19T13:54:17.000Z'
    data:
      edited: false
      editors:
      - PLGRND
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7995157837867737
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675705996569-631b6a5ef6bc4be4a64fbf04.png?w=200&h=200&f=face
          fullname: will ess
          isHf: false
          isPro: false
          name: PLGRND
          type: user
        html: '<p>I''m having a similar issue with the 70b model. I checked the box
          no_inject_fused_attention in the AutoGPTQ loader settings.<br>Still getting
          this error: NameError: name ''autogptq_cuda_256'' is not defined</p>

          '
        raw: 'I''m having a similar issue with the 70b model. I checked the box no_inject_fused_attention
          in the AutoGPTQ loader settings.

          Still getting this error: NameError: name ''autogptq_cuda_256'' is not defined'
        updatedAt: '2023-07-19T13:54:17.860Z'
      numEdits: 0
      reactions: []
    id: 64b7eb09964f6e7bf339f4b3
    type: comment
  author: PLGRND
  content: 'I''m having a similar issue with the 70b model. I checked the box no_inject_fused_attention
    in the AutoGPTQ loader settings.

    Still getting this error: NameError: name ''autogptq_cuda_256'' is not defined'
  created_at: 2023-07-19 12:54:17+00:00
  edited: false
  hidden: false
  id: 64b7eb09964f6e7bf339f4b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T14:08:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9432737827301025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span><br>I\
          \ followed your steps, but it didn't work. It did produce some output, and\
          \ it didn't crash. However, the output is just gibberish.</p>\n<p>My env:<br>text-generation-webui\
          \ + AutoGPTQ</p>\n</blockquote>\n<p>Gibberish implies the quantisation settings\
          \ are wrong. I did have a problem this morning where my scripts had uploaded\
          \ duplicate models to some branches.  Please show a screenshot of your model\
          \ folder</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;PLGRND&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/PLGRND\"\
          >@<span class=\"underline\">PLGRND</span></a></span>\n\n\t</span></span>\
          \ that's a different problem, a local AutoGPTQ install problem. It means\
          \ that AutoGPTQ is not properly built.  Try this:</p>\n<pre><code>pip3 uninstall\
          \ -y auto-gptq\nGITHUB_ACTIONS=true pip3 install -v auto-gptq==0.2.2\n</code></pre>\n\
          <p>If you continue to have problems, please report it on the AutoGPTQ Github\
          \ as it's not specific to this model.</p>\n"
        raw: "> @TheBloke \n> I followed your steps, but it didn't work. It did produce\
          \ some output, and it didn't crash. However, the output is just gibberish.\n\
          > \n> My env: \n> text-generation-webui + AutoGPTQ\n\nGibberish implies\
          \ the quantisation settings are wrong. I did have a problem this morning\
          \ where my scripts had uploaded duplicate models to some branches.  Please\
          \ show a screenshot of your model folder\n\n@PLGRND that's a different problem,\
          \ a local AutoGPTQ install problem. It means that AutoGPTQ is not properly\
          \ built.  Try this:\n```\npip3 uninstall -y auto-gptq\nGITHUB_ACTIONS=true\
          \ pip3 install -v auto-gptq==0.2.2\n```\n\nIf you continue to have problems,\
          \ please report it on the AutoGPTQ Github as it's not specific to this model."
        updatedAt: '2023-07-19T14:08:53.038Z'
      numEdits: 0
      reactions: []
    id: 64b7ee7590b38df833915ba1
    type: comment
  author: TheBloke
  content: "> @TheBloke \n> I followed your steps, but it didn't work. It did produce\
    \ some output, and it didn't crash. However, the output is just gibberish.\n>\
    \ \n> My env: \n> text-generation-webui + AutoGPTQ\n\nGibberish implies the quantisation\
    \ settings are wrong. I did have a problem this morning where my scripts had uploaded\
    \ duplicate models to some branches.  Please show a screenshot of your model folder\n\
    \n@PLGRND that's a different problem, a local AutoGPTQ install problem. It means\
    \ that AutoGPTQ is not properly built.  Try this:\n```\npip3 uninstall -y auto-gptq\n\
    GITHUB_ACTIONS=true pip3 install -v auto-gptq==0.2.2\n```\n\nIf you continue to\
    \ have problems, please report it on the AutoGPTQ Github as it's not specific\
    \ to this model."
  created_at: 2023-07-19 13:08:53+00:00
  edited: false
  hidden: false
  id: 64b7ee7590b38df833915ba1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dc32d9127f152b2f9bf570a3d9abcad.svg
      fullname: Ragesh Antony
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RageshAntony
      type: user
    createdAt: '2023-07-19T15:30:10.000Z'
    data:
      edited: true
      editors:
      - RageshAntony
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49047040939331055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dc32d9127f152b2f9bf570a3d9abcad.svg
          fullname: Ragesh Antony
          isHf: false
          isPro: false
          name: RageshAntony
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>I cloned\
          \ the repo fresh with updated transformers version commit of Text-Gen-web-ui.\
          \  Using 'main' (just pasted the 'TheBloke/Llama-2-70B-chat-GPTQ' and clicked\
          \ \"Download\" )</p>\n<p>Also checked 'no_inject_fused_attention' in Text-gen-webui\
          \ </p>\n<p>Still getting this error:</p>\n<p>Traceback (most recent call\
          \ last):<br>  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask<br>    ret = self.mfunc(callback=_callback, *args,\
          \ **self.kwargs)<br>  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 297, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 423, in generate<br>    return self.model.generate(**kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1572, in generate<br>    return self.sample(<br>  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2619, in sample<br>    outputs = self(<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward<br>    outputs = self.model(<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward<br>    layer_outputs = decoder_layer(<br>  File \"\
          /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 195, in forward<br>    key_states = self.k_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
          , line 249, in forward<br>    out = out.half().reshape(out_shape)<br>RuntimeError:\
          \ shape '[1, 139, 8192]' is invalid for input of size 142336</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630b08e286b8b9904c2cec03/zAwxE1youpGyp6HwdCH-d.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630b08e286b8b9904c2cec03/zAwxE1youpGyp6HwdCH-d.png\"\
          ></a></p>\n<p>Please help me</p>\n<p>Note: 13B model is working fine !</p>\n"
        raw: "@TheBloke \n\nI cloned the repo fresh with updated transformers version\
          \ commit of Text-Gen-web-ui.  Using 'main' (just pasted the 'TheBloke/Llama-2-70B-chat-GPTQ'\
          \ and clicked \"Download\" )\n\nAlso checked 'no_inject_fused_attention'\
          \ in Text-gen-webui \n\nStill getting this error:\n\nTraceback (most recent\
          \ call last):\n  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
          \  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 297, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 423, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 292, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 195, in forward\n    key_states = self.k_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
          , line 249, in forward\n    out = out.half().reshape(out_shape)\nRuntimeError:\
          \ shape '[1, 139, 8192]' is invalid for input of size 142336\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630b08e286b8b9904c2cec03/zAwxE1youpGyp6HwdCH-d.png)\n\
          \nPlease help me\n\nNote: 13B model is working fine !"
        updatedAt: '2023-07-19T15:35:19.131Z'
      numEdits: 1
      reactions: []
    id: 64b80182901f417d486d63b7
    type: comment
  author: RageshAntony
  content: "@TheBloke \n\nI cloned the repo fresh with updated transformers version\
    \ commit of Text-Gen-web-ui.  Using 'main' (just pasted the 'TheBloke/Llama-2-70B-chat-GPTQ'\
    \ and clicked \"Download\" )\n\nAlso checked 'no_inject_fused_attention' in Text-gen-webui\
    \ \n\nStill getting this error:\n\nTraceback (most recent call last):\n  File\
    \ \"/workspace/text-generation-webui/modules/callbacks.py\", line 55, in gentask\n\
    \    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
    , line 297, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
    , line 423, in generate\n    return self.model.generate(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 1572, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 2619, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 688, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 292, in forward\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 195, in forward\n    key_states = self.k_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
    , line 249, in forward\n    out = out.half().reshape(out_shape)\nRuntimeError:\
    \ shape '[1, 139, 8192]' is invalid for input of size 142336\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630b08e286b8b9904c2cec03/zAwxE1youpGyp6HwdCH-d.png)\n\
    \nPlease help me\n\nNote: 13B model is working fine !"
  created_at: 2023-07-19 14:30:10+00:00
  edited: true
  hidden: false
  id: 64b80182901f417d486d63b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7a26ba42e4bfb97b7211b64d9f4f335.svg
      fullname: PaulCollingwood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulTC
      type: user
    createdAt: '2023-07-19T21:37:05.000Z'
    data:
      edited: false
      editors:
      - PaulTC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7691047787666321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7a26ba42e4bfb97b7211b64d9f4f335.svg
          fullname: PaulCollingwood
          isHf: false
          isPro: false
          name: PaulTC
          type: user
        html: '<blockquote>

          <p>I''m having a similar issue with the 70b model. I checked the box no_inject_fused_attention
          in the AutoGPTQ loader settings.<br>Still getting this error: NameError:
          name ''autogptq_cuda_256'' is not defined</p>

          </blockquote>

          <p>tick the triton box</p>

          '
        raw: '> I''m having a similar issue with the 70b model. I checked the box
          no_inject_fused_attention in the AutoGPTQ loader settings.

          > Still getting this error: NameError: name ''autogptq_cuda_256'' is not
          defined


          tick the triton box'
        updatedAt: '2023-07-19T21:37:05.114Z'
      numEdits: 0
      reactions: []
    id: 64b85781e436bbca165a8de4
    type: comment
  author: PaulTC
  content: '> I''m having a similar issue with the 70b model. I checked the box no_inject_fused_attention
    in the AutoGPTQ loader settings.

    > Still getting this error: NameError: name ''autogptq_cuda_256'' is not defined


    tick the triton box'
  created_at: 2023-07-19 20:37:05+00:00
  edited: false
  hidden: false
  id: 64b85781e436bbca165a8de4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T21:40:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9355048537254333
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>For people still having trouble with text-generation-webui - ExLlama
          is updated recently so I suggest you use that. It''s quicker and uses less
          VRAM anyway.  The README has instructions</p>

          <p>The <code>autogptq_cuda_256 is not defined</code> means that the AutoGPTQ
          CUDA extension hasn''t compiled, which is unfortunately a very common problem
          with AutoGPTQ at the moment.</p>

          <p>This might fix it:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install -v auto-gptq

          </code></pre>

          <p>But it doesn''t for everyone, and if it doesn''t work it''s beyond the
          scope of this Discussions to fix that here; please post about it on the
          AutoGPTQ Github</p>

          '
        raw: 'For people still having trouble with text-generation-webui - ExLlama
          is updated recently so I suggest you use that. It''s quicker and uses less
          VRAM anyway.  The README has instructions


          The `autogptq_cuda_256 is not defined` means that the AutoGPTQ CUDA extension
          hasn''t compiled, which is unfortunately a very common problem with AutoGPTQ
          at the moment.


          This might fix it:

          ```

          pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install -v auto-gptq

          ```


          But it doesn''t for everyone, and if it doesn''t work it''s beyond the scope
          of this Discussions to fix that here; please post about it on the AutoGPTQ
          Github'
        updatedAt: '2023-07-19T21:40:35.370Z'
      numEdits: 0
      reactions: []
    id: 64b85853894eb78e3943ffe8
    type: comment
  author: TheBloke
  content: 'For people still having trouble with text-generation-webui - ExLlama is
    updated recently so I suggest you use that. It''s quicker and uses less VRAM anyway.  The
    README has instructions


    The `autogptq_cuda_256 is not defined` means that the AutoGPTQ CUDA extension
    hasn''t compiled, which is unfortunately a very common problem with AutoGPTQ at
    the moment.


    This might fix it:

    ```

    pip3 uninstall -y auto-gptq

    GITHUB_ACTIONS=true pip3 install -v auto-gptq

    ```


    But it doesn''t for everyone, and if it doesn''t work it''s beyond the scope of
    this Discussions to fix that here; please post about it on the AutoGPTQ Github'
  created_at: 2023-07-19 20:40:35+00:00
  edited: false
  hidden: false
  id: 64b85853894eb78e3943ffe8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00bea1b7d6ba751636e7ebb6fb8cc3ac.svg
      fullname: animal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fox2048
      type: user
    createdAt: '2023-09-25T02:29:26.000Z'
    data:
      edited: false
      editors:
      - fox2048
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7824298739433289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00bea1b7d6ba751636e7ebb6fb8cc3ac.svg
          fullname: animal
          isHf: false
          isPro: false
          name: fox2048
          type: user
        html: '<p>Hello, I try to load 70B model with GPU (GTX 1080ti * 7ea) -&gt;
          capability is 6.1<br>The loader I used is autogptq and add option "--no_use_cuda_fp16"
          and "--disable_exllama".<br>Also I used oobabooga (text-generation-webui)
          -&gt; build docker image </p>

          <p>The 70B chat-GPTQ model is loaded well but when I trying to inference,
          give me 0 token output always. </p>

          <p>Do you have any recommendation? </p>

          '
        raw: "Hello, I try to load 70B model with GPU (GTX 1080ti * 7ea) -> capability\
          \ is 6.1 \nThe loader I used is autogptq and add option \"--no_use_cuda_fp16\"\
          \ and \"--disable_exllama\". \nAlso I used oobabooga (text-generation-webui)\
          \ -> build docker image \n\nThe 70B chat-GPTQ model is loaded well but when\
          \ I trying to inference, give me 0 token output always. \n\nDo you have\
          \ any recommendation? \n"
        updatedAt: '2023-09-25T02:29:26.906Z'
      numEdits: 0
      reactions: []
    id: 6510f08653b1e2d59e4aabe4
    type: comment
  author: fox2048
  content: "Hello, I try to load 70B model with GPU (GTX 1080ti * 7ea) -> capability\
    \ is 6.1 \nThe loader I used is autogptq and add option \"--no_use_cuda_fp16\"\
    \ and \"--disable_exllama\". \nAlso I used oobabooga (text-generation-webui) ->\
    \ build docker image \n\nThe 70B chat-GPTQ model is loaded well but when I trying\
    \ to inference, give me 0 token output always. \n\nDo you have any recommendation?\
    \ \n"
  created_at: 2023-09-25 01:29:26+00:00
  edited: false
  hidden: false
  id: 6510f08653b1e2d59e4aabe4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Output always 0 tokens
