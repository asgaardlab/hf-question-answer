!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jacktenyx
conflicting_files: null
created_at: 2023-07-23 02:09:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0143f1db420a2e27851abebbe425ba27.svg
      fullname: Jack Weissenberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jacktenyx
      type: user
    createdAt: '2023-07-23T03:09:06.000Z'
    data:
      edited: false
      editors:
      - jacktenyx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8102957010269165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0143f1db420a2e27851abebbe425ba27.svg
          fullname: Jack Weissenberger
          isHf: false
          isPro: false
          name: jacktenyx
          type: user
        html: "<p>Thanks for posting this model, I was able to run inference with\
          \ TGI on a single 40 GB A100 with the following command: </p>\n<pre><code\
          \ class=\"language-bash\">docker run \\\n    -p 8080:80 \\\n    -e GPTQ_BITS=4\
          \ \\\n    -e GPTQ_GROUPSIZE=1 \\\n    --gpus all \\\n    --shm-size 5g \\\
          \n    -v <span class=\"hljs-variable\">$volume</span>:/data ghcr.io/huggingface/text-generation-inference:latest\
          \ \\\n    --model-id TheBloke/Llama-2-70B-chat-GPTQ \\\n    --max-input-length\
          \ 4096 \\\n    --max-total-tokens 8192 \\\n    --quantize gptq \\\n    --sharded\
          \ <span class=\"hljs-literal\">false</span>\n</code></pre>\n<p>This was\
          \ able to generate a response at 225ms/token. However, when running the\
          \ unquantized model sharded across 4 A100s I was able to get around 45ms/token.\
          \ Am I missing a config or environment variable that would improve the inference\
          \ time or is this expected behavior with this quantization?</p>\n"
        raw: "Thanks for posting this model, I was able to run inference with TGI\
          \ on a single 40 GB A100 with the following command: \r\n```bash\r\ndocker\
          \ run \\\r\n    -p 8080:80 \\\r\n    -e GPTQ_BITS=4 \\\r\n    -e GPTQ_GROUPSIZE=1\
          \ \\\r\n    --gpus all \\\r\n    --shm-size 5g \\\r\n    -v $volume:/data\
          \ ghcr.io/huggingface/text-generation-inference:latest \\\r\n    --model-id\
          \ TheBloke/Llama-2-70B-chat-GPTQ \\\r\n    --max-input-length 4096 \\\r\n\
          \    --max-total-tokens 8192 \\\r\n    --quantize gptq \\\r\n    --sharded\
          \ false\r\n```\r\nThis was able to generate a response at 225ms/token. However,\
          \ when running the unquantized model sharded across 4 A100s I was able to\
          \ get around 45ms/token. Am I missing a config or environment variable that\
          \ would improve the inference time or is this expected behavior with this\
          \ quantization?"
        updatedAt: '2023-07-23T03:09:06.369Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - SaraQX
        - vhug
    id: 64bc99d2b9296eef62bff5c5
    type: comment
  author: jacktenyx
  content: "Thanks for posting this model, I was able to run inference with TGI on\
    \ a single 40 GB A100 with the following command: \r\n```bash\r\ndocker run \\\
    \r\n    -p 8080:80 \\\r\n    -e GPTQ_BITS=4 \\\r\n    -e GPTQ_GROUPSIZE=1 \\\r\
    \n    --gpus all \\\r\n    --shm-size 5g \\\r\n    -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ \\\r\n    --model-id TheBloke/Llama-2-70B-chat-GPTQ \\\r\n    --max-input-length\
    \ 4096 \\\r\n    --max-total-tokens 8192 \\\r\n    --quantize gptq \\\r\n    --sharded\
    \ false\r\n```\r\nThis was able to generate a response at 225ms/token. However,\
    \ when running the unquantized model sharded across 4 A100s I was able to get\
    \ around 45ms/token. Am I missing a config or environment variable that would\
    \ improve the inference time or is this expected behavior with this quantization?"
  created_at: 2023-07-23 02:09:06+00:00
  edited: false
  hidden: false
  id: 64bc99d2b9296eef62bff5c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbcfc56ce66e4cccf95ca9eecf967897.svg
      fullname: Luu Huu Phuc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phucdoitoan
      type: user
    createdAt: '2023-09-19T12:50:02.000Z'
    data:
      edited: false
      editors:
      - phucdoitoan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8773635625839233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbcfc56ce66e4cccf95ca9eecf967897.svg
          fullname: Luu Huu Phuc
          isHf: false
          isPro: false
          name: phucdoitoan
          type: user
        html: '<p>I get the same number of latency (&gt;100ms/ toekn) with half the
          length for input and total_tokens. It even slower than using quantization
          with bitsandbytes-nf4  (~51 ms/token).<br>This is weird as it is said that
          gptq is faster for inference than bitsandbytes.<br>...</p>

          '
        raw: 'I get the same number of latency (>100ms/ toekn) with half the length
          for input and total_tokens. It even slower than using quantization with
          bitsandbytes-nf4  (~51 ms/token).

          This is weird as it is said that gptq is faster for inference than bitsandbytes.

          ...'
        updatedAt: '2023-09-19T12:50:02.901Z'
      numEdits: 0
      reactions: []
    id: 650998fac9aa376f76bf3327
    type: comment
  author: phucdoitoan
  content: 'I get the same number of latency (>100ms/ toekn) with half the length
    for input and total_tokens. It even slower than using quantization with bitsandbytes-nf4  (~51
    ms/token).

    This is weird as it is said that gptq is faster for inference than bitsandbytes.

    ...'
  created_at: 2023-09-19 11:50:02+00:00
  edited: false
  hidden: false
  id: 650998fac9aa376f76bf3327
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Inference time with TGI
