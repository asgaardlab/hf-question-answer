!!python/object:huggingface_hub.community.DiscussionWithDetails
author: neo-benjamin
conflicting_files: null
created_at: 2023-07-27 07:08:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neo-benjamin
      type: user
    createdAt: '2023-07-27T08:08:12.000Z'
    data:
      edited: false
      editors:
      - neo-benjamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8024290800094604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52ab10e685aeb8f93a3314a58c04244d.svg
          fullname: neo
          isHf: false
          isPro: false
          name: neo-benjamin
          type: user
        html: "<p>When I ran the code:</p>\n<pre><code>prompt = \"Tell me about AI\"\
          \nsystem_message = \"You are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature. If a question does not make any sense, or is not\
          \ factually coherent, explain why instead of answering something not correct.\
          \ If you don't know the answer to a question, please don't share false information.\"\
          \nprompt_template=f'''[INST] &lt;&lt;SYS&gt;&gt;\n{system_message}\n&lt;&lt;/SYS&gt;&gt;\n\
          \n{prompt} [/INST]\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids =\
          \ tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput\
          \ = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n</code></pre>\n<p>output contains the\
          \ prompt along with the <code>output</code> i.e. response from the model.<br>So\
          \ how token is calculated? the output token is <code>max_new_tokens</code>\
          \ and the <code>max_new_tokens</code>+<code>output tokens</code> needs to\
          \ be less than 4096?\n  </p>\n"
        raw: "When I ran the code:\r\n\r\n```\r\nprompt = \"Tell me about AI\"\r\n\
          system_message = \"You are a helpful, respectful and honest assistant. Always\
          \ answer as helpfully as possible, while being safe.  Your answers should\
          \ not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature. If a question does not make any sense, or is not\
          \ factually coherent, explain why instead of answering something not correct.\
          \ If you don't know the answer to a question, please don't share false information.\"\
          \r\nprompt_template=f'''[INST] <<SYS>>\r\n{system_message}\r\n<</SYS>>\r\
          \n\r\n{prompt} [/INST]\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
          \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
          \nprint(tokenizer.decode(output[0]))\r\n```\r\n\r\noutput contains the prompt\
          \ along with the `output` i.e. response from the model. \r\nSo how token\
          \ is calculated? the output token is `max_new_tokens` and the `max_new_tokens`+`output\
          \ tokens` needs to be less than 4096?\r\n  "
        updatedAt: '2023-07-27T08:08:12.250Z'
      numEdits: 0
      reactions: []
    id: 64c225ec144954b4df70ef65
    type: comment
  author: neo-benjamin
  content: "When I ran the code:\r\n\r\n```\r\nprompt = \"Tell me about AI\"\r\nsystem_message\
    \ = \"You are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe.  Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature. If a question\
    \ does not make any sense, or is not factually coherent, explain why instead of\
    \ answering something not correct. If you don't know the answer to a question,\
    \ please don't share false information.\"\r\nprompt_template=f'''[INST] <<SYS>>\r\
    \n{system_message}\r\n<</SYS>>\r\n\r\n{prompt} [/INST]\r\n'''\r\n\r\nprint(\"\\\
    n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n```\r\n\r\noutput contains the prompt\
    \ along with the `output` i.e. response from the model. \r\nSo how token is calculated?\
    \ the output token is `max_new_tokens` and the `max_new_tokens`+`output tokens`\
    \ needs to be less than 4096?\r\n  "
  created_at: 2023-07-27 07:08:12+00:00
  edited: false
  hidden: false
  id: 64c225ec144954b4df70ef65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T08:17:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7402561902999878
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s how <code>model.generate()</code> works.  You could use
          <code>model.pipeline()</code> instead with <code>return_full_text=False</code></p>

          <p><code>max_new_tokens</code> is the number of tokens it will generate
          in response to your prompt.  prompt + <code>max_new_tokens</code> must be
          less than 4096.</p>

          '
        raw: 'That''s how `model.generate()` works.  You could use `model.pipeline()`
          instead with `return_full_text=False`


          `max_new_tokens` is the number of tokens it will generate in response to
          your prompt.  prompt + `max_new_tokens` must be less than 4096.'
        updatedAt: '2023-07-27T08:17:18.224Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - SathwikBairaboina
        - fame2312
        - kaifanli
    id: 64c2280eb005aab93d691dac
    type: comment
  author: TheBloke
  content: 'That''s how `model.generate()` works.  You could use `model.pipeline()`
    instead with `return_full_text=False`


    `max_new_tokens` is the number of tokens it will generate in response to your
    prompt.  prompt + `max_new_tokens` must be less than 4096.'
  created_at: 2023-07-27 07:17:18+00:00
  edited: false
  hidden: false
  id: 64c2280eb005aab93d691dac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de25b0b5af4e83f27aad89c60ed4485d.svg
      fullname: Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Michael528
      type: user
    createdAt: '2023-08-29T23:51:57.000Z'
    data:
      edited: false
      editors:
      - Michael528
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6130237579345703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de25b0b5af4e83f27aad89c60ed4485d.svg
          fullname: Li
          isHf: false
          isPro: false
          name: Michael528
          type: user
        html: '<p>Is there a way to remove the input prompt from the output? I use
          below but still gives me the input prompt.<br>out_obj[0][''generated_text'']</p>

          '
        raw: 'Is there a way to remove the input prompt from the output? I use below
          but still gives me the input prompt.

          out_obj[0][''generated_text'']'
        updatedAt: '2023-08-29T23:51:57.016Z'
      numEdits: 0
      reactions: []
    id: 64ee849dc50ab3b3399e5a69
    type: comment
  author: Michael528
  content: 'Is there a way to remove the input prompt from the output? I use below
    but still gives me the input prompt.

    out_obj[0][''generated_text'']'
  created_at: 2023-08-29 22:51:57+00:00
  edited: false
  hidden: false
  id: 64ee849dc50ab3b3399e5a69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05c6042b984f7887171d1fb8abb87f6e.svg
      fullname: XinlongLee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: which47
      type: user
    createdAt: '2023-11-26T06:39:09.000Z'
    data:
      edited: true
      editors:
      - which47
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.06997043639421463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05c6042b984f7887171d1fb8abb87f6e.svg
          fullname: XinlongLee
          isHf: false
          isPro: false
          name: which47
          type: user
        html: '<p>output_ids  = model.generate(input_ids)<br>output = tokenizer.decode(output_ids[:,
          input_ids.shape[1]:])</p>

          '
        raw: 'output_ids  = model.generate(input_ids)

          output = tokenizer.decode(output_ids[:, input_ids.shape[1]:])'
        updatedAt: '2023-11-26T06:39:44.702Z'
      numEdits: 1
      reactions: []
    id: 6562e80d6573ea07085ac3a7
    type: comment
  author: which47
  content: 'output_ids  = model.generate(input_ids)

    output = tokenizer.decode(output_ids[:, input_ids.shape[1]:])'
  created_at: 2023-11-26 06:39:09+00:00
  edited: true
  hidden: false
  id: 6562e80d6573ea07085ac3a7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Why the input prompt is part of the output?
