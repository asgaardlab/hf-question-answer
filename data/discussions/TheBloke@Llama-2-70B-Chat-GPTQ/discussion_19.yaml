!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lasalH
conflicting_files: null
created_at: 2023-07-25 08:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545bea602c3a2a065849148db68c40e3.svg
      fullname: Lasal Hettiarachchi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lasalH
      type: user
    createdAt: '2023-07-25T09:28:14.000Z'
    data:
      edited: false
      editors:
      - lasalH
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5560483932495117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545bea602c3a2a065849148db68c40e3.svg
          fullname: Lasal Hettiarachchi
          isHf: false
          isPro: false
          name: lasalH
          type: user
        html: "<p>HI everyone,  really appreciate <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ for these wonderful models</p>\n<p>Im trying to set up the TheBloke/Llama-2-70B-chat-GPTQ\
          \ for basic inferencing as python code. The steps I followed were as follows:<br>Environment:</p>\n\
          <ul>\n<li>A6000 RTX</li>\n<li>62 GB ram<br>Proccess:</li>\n<li>install auto-gptq\
          \ (GITHUB_ACTIONS=true pip3 install auto-gptq)</li>\n<li>install the latest\
          \ transformers lib (pip3 install git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers\"\
          >https://github.com/huggingface/transformers</a>)<br>Code:</li>\n</ul>\n\
          <hr>\n<p>from transformers import AutoTokenizer, pipeline, logging<br>from\
          \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>\n<p>model_name_or_path\
          \ = \"TheBloke/Llama-2-70B-chat-GPTQ\"<br>model_basename = \"gptq_model-4bit--1g\"\
          </p>\n<p>use_triton = False</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>\
          \        model_basename=model_basename,<br>        inject_fused_attention=False,\
          \ # Required for Llama 2 70B model at this time.<br>        use_safetensors=True,<br>\
          \        trust_remote_code=False,<br>        device=\"cuda:0\",<br>    \
          \    use_triton=use_triton,<br>        quantize_config=None)</p>\n<hr>\n\
          <p>Error:<br>TypeError                                 Traceback (most recent\
          \ call last)<br>Cell In[1], line 11<br>      7 use_triton = False<br>  \
          \    9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)<br>---&gt;\
          \ 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>\
          \     12         model_basename=model_basename,<br>     13         inject_fused_attention=False,\
          \ # Required for Llama 2 70B model at this time.<br>     14         use_safetensors=True,<br>\
          \     15         trust_remote_code=False,<br>     16         device=\"cuda:0\"\
          ,<br>     17         use_triton=use_triton,<br>     18         quantize_config=None)</p>\n\
          <h2 id=\"typeerror-autogptqforcausallmfrom_quantized-got-an-unexpected-keyword-argument-inject_fused_attention\"\
          >TypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected keyword\
          \ argument 'inject_fused_attention'</h2>\n<p>Any help will be appreciated.\
          \ Thank you in advance</p>\n"
        raw: "HI everyone,  really appreciate @TheBloke for these wonderful models\r\
          \n\r\nIm trying to set up the TheBloke/Llama-2-70B-chat-GPTQ for basic inferencing\
          \ as python code. The steps I followed were as follows:\r\nEnvironment:\r\
          \n- A6000 RTX\r\n- 62 GB ram\r\nProccess:\r\n- install auto-gptq (GITHUB_ACTIONS=true\
          \ pip3 install auto-gptq)\r\n- install the latest transformers lib (pip3\
          \ install git+https://github.com/huggingface/transformers)\r\nCode:\r\n\
          -------------------------------------------------------------------------------------------------\r\
          \nfrom transformers import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path\
          \ = \"TheBloke/Llama-2-70B-chat-GPTQ\"\r\nmodel_basename = \"gptq_model-4bit--1g\"\
          \r\n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        model_basename=model_basename,\r\n        inject_fused_attention=False,\
          \ # Required for Llama 2 70B model at this time.\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=False,\r\n        device=\"cuda:0\",\r\n   \
          \     use_triton=use_triton,\r\n        quantize_config=None)\r\n-------------------------------------------------------------------------------------------------\r\
          \nError:\r\nTypeError                                 Traceback (most recent\
          \ call last)\r\nCell In[1], line 11\r\n      7 use_triton = False\r\n  \
          \    9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
          \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n     12         model_basename=model_basename,\r\n     13         inject_fused_attention=False,\
          \ # Required for Llama 2 70B model at this time.\r\n     14         use_safetensors=True,\r\
          \n     15         trust_remote_code=False,\r\n     16         device=\"\
          cuda:0\",\r\n     17         use_triton=use_triton,\r\n     18         quantize_config=None)\r\
          \n\r\nTypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected\
          \ keyword argument 'inject_fused_attention'\r\n-------------------------------------------------------------------------------------------------\r\
          \nAny help will be appreciated. Thank you in advance"
        updatedAt: '2023-07-25T09:28:14.658Z'
      numEdits: 0
      reactions: []
    id: 64bf95ae01f1983a86be05d1
    type: comment
  author: lasalH
  content: "HI everyone,  really appreciate @TheBloke for these wonderful models\r\
    \n\r\nIm trying to set up the TheBloke/Llama-2-70B-chat-GPTQ for basic inferencing\
    \ as python code. The steps I followed were as follows:\r\nEnvironment:\r\n- A6000\
    \ RTX\r\n- 62 GB ram\r\nProccess:\r\n- install auto-gptq (GITHUB_ACTIONS=true\
    \ pip3 install auto-gptq)\r\n- install the latest transformers lib (pip3 install\
    \ git+https://github.com/huggingface/transformers)\r\nCode:\r\n-------------------------------------------------------------------------------------------------\r\
    \nfrom transformers import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"\
    TheBloke/Llama-2-70B-chat-GPTQ\"\r\nmodel_basename = \"gptq_model-4bit--1g\"\r\
    \n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        model_basename=model_basename,\r\n        inject_fused_attention=False,\
    \ # Required for Llama 2 70B model at this time.\r\n        use_safetensors=True,\r\
    \n        trust_remote_code=False,\r\n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\
    \n        quantize_config=None)\r\n-------------------------------------------------------------------------------------------------\r\
    \nError:\r\nTypeError                                 Traceback (most recent call\
    \ last)\r\nCell In[1], line 11\r\n      7 use_triton = False\r\n      9 tokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n---> 11\
    \ model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n     12  \
    \       model_basename=model_basename,\r\n     13         inject_fused_attention=False,\
    \ # Required for Llama 2 70B model at this time.\r\n     14         use_safetensors=True,\r\
    \n     15         trust_remote_code=False,\r\n     16         device=\"cuda:0\"\
    ,\r\n     17         use_triton=use_triton,\r\n     18         quantize_config=None)\r\
    \n\r\nTypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected keyword\
    \ argument 'inject_fused_attention'\r\n-------------------------------------------------------------------------------------------------\r\
    \nAny help will be appreciated. Thank you in advance"
  created_at: 2023-07-25 08:28:14+00:00
  edited: false
  hidden: false
  id: 64bf95ae01f1983a86be05d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db55694015cfdeaf3381428978e9bf76.svg
      fullname: "Alejandro Fern\xE1ndez Llorente"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alejandrofdz
      type: user
    createdAt: '2023-07-25T09:59:55.000Z'
    data:
      edited: false
      editors:
      - alejandrofdz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6996479034423828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db55694015cfdeaf3381428978e9bf76.svg
          fullname: "Alejandro Fern\xE1ndez Llorente"
          isHf: false
          isPro: false
          name: alejandrofdz
          type: user
        html: "<p>Its weird because in the AutoGPTQ the injection_fused_attention\
          \ is declared clearly.</p>\n<pre><code>def from_quantized(\n    cls,\n \
          \   model_name_or_path: Optional[str] = None,\n    save_dir: Optional[str]\
          \ = None,\n    device_map: Optional[Union[str, Dict[str, Union[str, int]]]]\
          \ = None,\n    max_memory: Optional[dict] = None,\n    device: Optional[Union[str,\
          \ int]] = None,\n    low_cpu_mem_usage: bool = False,\n    use_triton: bool\
          \ = False,\n    inject_fused_attention: bool = True,\n    inject_fused_mlp:\
          \ bool = True,\n    use_cuda_fp16: bool = True,\n    quantize_config: Optional[BaseQuantizeConfig]\
          \ = None,\n    model_basename: Optional[str] = None,\n    use_safetensors:\
          \ bool = False,\n    trust_remote_code: bool = False,\n    warmup_triton:\
          \ bool = False,\n    trainable: bool = False,\n    **kwargs\n) -&gt; BaseGPTQForCausalLM:\n\
          \    model_type = check_and_get_model_type(\n        save_dir or model_name_or_path,\
          \ trust_remote_code\n    )\n</code></pre>\n<p>I do not have this error I\
          \ have another one (<a href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/18#64be578976a6e2efccc31cd0\"\
          >https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/18#64be578976a6e2efccc31cd0</a>)\
          \ different but it seems later than yours. Which python version are you\
          \ using? (I use 3.8). Which version of auto-gptq? (I have 0.3.0).</p>\n"
        raw: "Its weird because in the AutoGPTQ the injection_fused_attention is declared\
          \ clearly.\n\n    def from_quantized(\n        cls,\n        model_name_or_path:\
          \ Optional[str] = None,\n        save_dir: Optional[str] = None,\n     \
          \   device_map: Optional[Union[str, Dict[str, Union[str, int]]]] = None,\n\
          \        max_memory: Optional[dict] = None,\n        device: Optional[Union[str,\
          \ int]] = None,\n        low_cpu_mem_usage: bool = False,\n        use_triton:\
          \ bool = False,\n        inject_fused_attention: bool = True,\n        inject_fused_mlp:\
          \ bool = True,\n        use_cuda_fp16: bool = True,\n        quantize_config:\
          \ Optional[BaseQuantizeConfig] = None,\n        model_basename: Optional[str]\
          \ = None,\n        use_safetensors: bool = False,\n        trust_remote_code:\
          \ bool = False,\n        warmup_triton: bool = False,\n        trainable:\
          \ bool = False,\n        **kwargs\n    ) -> BaseGPTQForCausalLM:\n     \
          \   model_type = check_and_get_model_type(\n            save_dir or model_name_or_path,\
          \ trust_remote_code\n        )\n\n\nI do not have this error I have another\
          \ one (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/18#64be578976a6e2efccc31cd0)\
          \ different but it seems later than yours. Which python version are you\
          \ using? (I use 3.8). Which version of auto-gptq? (I have 0.3.0)."
        updatedAt: '2023-07-25T09:59:55.441Z'
      numEdits: 0
      reactions: []
    id: 64bf9d1b81caff7f186b0f21
    type: comment
  author: alejandrofdz
  content: "Its weird because in the AutoGPTQ the injection_fused_attention is declared\
    \ clearly.\n\n    def from_quantized(\n        cls,\n        model_name_or_path:\
    \ Optional[str] = None,\n        save_dir: Optional[str] = None,\n        device_map:\
    \ Optional[Union[str, Dict[str, Union[str, int]]]] = None,\n        max_memory:\
    \ Optional[dict] = None,\n        device: Optional[Union[str, int]] = None,\n\
    \        low_cpu_mem_usage: bool = False,\n        use_triton: bool = False,\n\
    \        inject_fused_attention: bool = True,\n        inject_fused_mlp: bool\
    \ = True,\n        use_cuda_fp16: bool = True,\n        quantize_config: Optional[BaseQuantizeConfig]\
    \ = None,\n        model_basename: Optional[str] = None,\n        use_safetensors:\
    \ bool = False,\n        trust_remote_code: bool = False,\n        warmup_triton:\
    \ bool = False,\n        trainable: bool = False,\n        **kwargs\n    ) ->\
    \ BaseGPTQForCausalLM:\n        model_type = check_and_get_model_type(\n     \
    \       save_dir or model_name_or_path, trust_remote_code\n        )\n\n\nI do\
    \ not have this error I have another one (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/18#64be578976a6e2efccc31cd0)\
    \ different but it seems later than yours. Which python version are you using?\
    \ (I use 3.8). Which version of auto-gptq? (I have 0.3.0)."
  created_at: 2023-07-25 08:59:55+00:00
  edited: false
  hidden: false
  id: 64bf9d1b81caff7f186b0f21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T10:51:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9354385137557983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lasalH&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lasalH\">@<span class=\"\
          underline\">lasalH</span></a></span>\n\n\t</span></span> this error suggest\
          \ AutoGPTQ is on an earlier version.  I am not sure why that's happened,\
          \ but can you try:</p>\n<pre><code>pip3 uninstall -y auto-gptq\nGITHUB_ACTIONS=true\
          \ pip3 install auto-gptq==0.2.2\n</code></pre>\n<p>report if there's any\
          \ errors shown by that command, and if not, test again.</p>\n<p>I've specified\
          \ 0.2.2 as there's currently a bug in 0.3.0 which affects inference with\
          \ some of my GPTQ uploads (the ones that have act_order + group_size together).\
          \  The bug has been fixed and there should be another release soon, 0.3.1,\
          \ but for now use 0.2.2</p>\n"
        raw: '@lasalH this error suggest AutoGPTQ is on an earlier version.  I am
          not sure why that''s happened, but can you try:

          ```

          pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2

          ```


          report if there''s any errors shown by that command, and if not, test again.


          I''ve specified 0.2.2 as there''s currently a bug in 0.3.0 which affects
          inference with some of my GPTQ uploads (the ones that have act_order + group_size
          together).  The bug has been fixed and there should be another release soon,
          0.3.1, but for now use 0.2.2'
        updatedAt: '2023-07-25T10:51:33.647Z'
      numEdits: 0
      reactions: []
    id: 64bfa9358e051085ba46b9c8
    type: comment
  author: TheBloke
  content: '@lasalH this error suggest AutoGPTQ is on an earlier version.  I am not
    sure why that''s happened, but can you try:

    ```

    pip3 uninstall -y auto-gptq

    GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2

    ```


    report if there''s any errors shown by that command, and if not, test again.


    I''ve specified 0.2.2 as there''s currently a bug in 0.3.0 which affects inference
    with some of my GPTQ uploads (the ones that have act_order + group_size together).  The
    bug has been fixed and there should be another release soon, 0.3.1, but for now
    use 0.2.2'
  created_at: 2023-07-25 09:51:33+00:00
  edited: false
  hidden: false
  id: 64bfa9358e051085ba46b9c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545bea602c3a2a065849148db68c40e3.svg
      fullname: Lasal Hettiarachchi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lasalH
      type: user
    createdAt: '2023-07-25T11:09:01.000Z'
    data:
      edited: false
      editors:
      - lasalH
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47617068886756897
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545bea602c3a2a065849148db68c40e3.svg
          fullname: Lasal Hettiarachchi
          isHf: false
          isPro: false
          name: lasalH
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ . Installing auto-gptq version 0.2.2 fixed the issue.</p>\n"
        raw: Thank you @TheBloke . Installing auto-gptq version 0.2.2 fixed the issue.
        updatedAt: '2023-07-25T11:09:01.785Z'
      numEdits: 0
      reactions: []
    id: 64bfad4df671da974e7a869c
    type: comment
  author: lasalH
  content: Thank you @TheBloke . Installing auto-gptq version 0.2.2 fixed the issue.
  created_at: 2023-07-25 10:09:01+00:00
  edited: false
  hidden: false
  id: 64bfad4df671da974e7a869c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'error:  unexpected keyword argument ''inject_fused_attention'''
