!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yekta
conflicting_files: null
created_at: 2023-07-22 01:08:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
      fullname: Yekta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yekta
      type: user
    createdAt: '2023-07-22T02:08:08.000Z'
    data:
      edited: false
      editors:
      - yekta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5076280832290649
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
          fullname: Yekta
          isHf: false
          isPro: false
          name: yekta
          type: user
        html: "<p>I'm trying to launch the model with TGI but it's erroring out. Here\
          \ is the command I'm running:</p>\n<pre><code>model=TheBloke/Llama-2-70B-chat-GPTQ\n\
          num_shard=1\nvolume=$PWD/data\nquantize=gptq\n\ndocker run -d --gpus all\
          \ --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9.3\
          \ --model-id $model --num-shard $num_shard --quantize $quantize\n</code></pre>\n\
          <p>Am I doing doing something wrong? Here is the error I get:</p>\n<pre><code>2023-07-22T02:01:59.461306Z\
          \  INFO download: text_generation_launcher: Successfully downloaded weights.\n\
          2023-07-22T02:01:59.461559Z  INFO shard-manager: text_generation_launcher:\
          \ Starting shard 0 rank=0\n2023-07-22T02:02:03.443521Z ERROR text_generation_launcher:\
          \ Error when initializing model\nTraceback (most recent call last):\n  File\
          \ \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 175, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 142, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 185, in get_model\n    return FlashLlama(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 65, in __init__\n    model = FlashLlamaForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 452, in __init__\n    self.model = FlashLlamaModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 390, in __init__\n    [\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 391, in &lt;listcomp&gt;\n    FlashLlamaLayer(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 326, in __init__\n    self.self_attn = FlashLlamaAttention(\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 199, in __init__\n    self.query_key_value = _load_gqa(config, prefix,\
          \ weights)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 152, in _load_gqa\n    weights.get_sharded(f\"{prefix}.q_proj.weight\"\
          , dim=0),\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\n    filename, tensor_name = self.get_filename(tensor_name)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 49, in get_filename\n    raise RuntimeError(f\"weight {tensor_name}\
          \ does not exist\")\nRuntimeError: weight model.layers.0.self_attn.q_proj.weight\
          \ does not exist\n</code></pre>\n"
        raw: "I'm trying to launch the model with TGI but it's erroring out. Here\
          \ is the command I'm running:\r\n\r\n```\r\nmodel=TheBloke/Llama-2-70B-chat-GPTQ\r\
          \nnum_shard=1\r\nvolume=$PWD/data\r\nquantize=gptq\r\n\r\ndocker run -d\
          \ --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9.3\
          \ --model-id $model --num-shard $num_shard --quantize $quantize\r\n```\r\
          \n\r\nAm I doing doing something wrong? Here is the error I get:\r\n\r\n\
          ```\r\n2023-07-22T02:01:59.461306Z  INFO download: text_generation_launcher:\
          \ Successfully downloaded weights.\r\n2023-07-22T02:01:59.461559Z  INFO\
          \ shard-manager: text_generation_launcher: Starting shard 0 rank=0\r\n2023-07-22T02:02:03.443521Z\
          \ ERROR text_generation_launcher: Error when initializing model\r\nTraceback\
          \ (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 175, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 142, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 185, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 65, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 452, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 390, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 391, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 326, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 199, in __init__\r\n    self.query_key_value = _load_gqa(config,\
          \ prefix, weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 152, in _load_gqa\r\n    weights.get_sharded(f\"{prefix}.q_proj.weight\"\
          , dim=0),\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 98, in get_sharded\r\n    filename, tensor_name = self.get_filename(tensor_name)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
          , line 49, in get_filename\r\n    raise RuntimeError(f\"weight {tensor_name}\
          \ does not exist\")\r\nRuntimeError: weight model.layers.0.self_attn.q_proj.weight\
          \ does not exist\r\n```"
        updatedAt: '2023-07-22T02:08:08.337Z'
      numEdits: 0
      reactions: []
    id: 64bb3a08976343e90a29e98e
    type: comment
  author: yekta
  content: "I'm trying to launch the model with TGI but it's erroring out. Here is\
    \ the command I'm running:\r\n\r\n```\r\nmodel=TheBloke/Llama-2-70B-chat-GPTQ\r\
    \nnum_shard=1\r\nvolume=$PWD/data\r\nquantize=gptq\r\n\r\ndocker run -d --gpus\
    \ all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9.3\
    \ --model-id $model --num-shard $num_shard --quantize $quantize\r\n```\r\n\r\n\
    Am I doing doing something wrong? Here is the error I get:\r\n\r\n```\r\n2023-07-22T02:01:59.461306Z\
    \  INFO download: text_generation_launcher: Successfully downloaded weights.\r\
    \n2023-07-22T02:01:59.461559Z  INFO shard-manager: text_generation_launcher: Starting\
    \ shard 0 rank=0\r\n2023-07-22T02:02:03.443521Z ERROR text_generation_launcher:\
    \ Error when initializing model\r\nTraceback (most recent call last):\r\n  File\
    \ \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n    sys.exit(app())\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line 311, in\
    \ __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\n\
    \    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 78, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 175, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 142, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 185, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 65, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 452, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 390, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 391, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 326, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 199, in __init__\r\n    self.query_key_value = _load_gqa(config, prefix,\
    \ weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 152, in _load_gqa\r\n    weights.get_sharded(f\"{prefix}.q_proj.weight\"\
    , dim=0),\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 98, in get_sharded\r\n    filename, tensor_name = self.get_filename(tensor_name)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/weights.py\"\
    , line 49, in get_filename\r\n    raise RuntimeError(f\"weight {tensor_name} does\
    \ not exist\")\r\nRuntimeError: weight model.layers.0.self_attn.q_proj.weight\
    \ does not exist\r\n```"
  created_at: 2023-07-22 01:08:08+00:00
  edited: false
  hidden: false
  id: 64bb3a08976343e90a29e98e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:18:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7937096357345581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Are you using the latest TGI code? They merged a fix for this error
          recently <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/pull/648">https://github.com/huggingface/text-generation-inference/pull/648</a></p>

          '
        raw: Are you using the latest TGI code? They merged a fix for this error recently
          https://github.com/huggingface/text-generation-inference/pull/648
        updatedAt: '2023-07-22T08:18:21.595Z'
      numEdits: 0
      reactions: []
    id: 64bb90cdd05a97d7227a21b8
    type: comment
  author: TheBloke
  content: Are you using the latest TGI code? They merged a fix for this error recently
    https://github.com/huggingface/text-generation-inference/pull/648
  created_at: 2023-07-22 07:18:21+00:00
  edited: false
  hidden: false
  id: 64bb90cdd05a97d7227a21b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
      fullname: Yekta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yekta
      type: user
    createdAt: '2023-07-22T11:34:30.000Z'
    data:
      edited: true
      editors:
      - yekta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9451892971992493
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
          fullname: Yekta
          isHf: false
          isPro: false
          name: yekta
          type: user
        html: '<p>I''m using: <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/releases/tag/v0.9.3">https://github.com/huggingface/text-generation-inference/releases/tag/v0.9.3</a></p>

          <p>This image specifically: ghcr.io/huggingface/text-generation-inference:0.9.3</p>

          <p>I guess that doesn''t have the fix?</p>

          '
        raw: 'I''m using: https://github.com/huggingface/text-generation-inference/releases/tag/v0.9.3


          This image specifically: ghcr.io/huggingface/text-generation-inference:0.9.3


          I guess that doesn''t have the fix?'
        updatedAt: '2023-07-22T11:37:54.868Z'
      numEdits: 1
      reactions: []
    id: 64bbbec69f94ea2554d42edf
    type: comment
  author: yekta
  content: 'I''m using: https://github.com/huggingface/text-generation-inference/releases/tag/v0.9.3


    This image specifically: ghcr.io/huggingface/text-generation-inference:0.9.3


    I guess that doesn''t have the fix?'
  created_at: 2023-07-22 10:34:30+00:00
  edited: true
  hidden: false
  id: 64bbbec69f94ea2554d42edf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T11:35:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9962278008460999
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Correct. 0.9.3 was released 4 days ago and the PR with the fix was
          merged 2 days ago.</p>

          '
        raw: Correct. 0.9.3 was released 4 days ago and the PR with the fix was merged
          2 days ago.
        updatedAt: '2023-07-22T11:35:52.723Z'
      numEdits: 0
      reactions: []
    id: 64bbbf185b8d826146ace347
    type: comment
  author: TheBloke
  content: Correct. 0.9.3 was released 4 days ago and the PR with the fix was merged
    2 days ago.
  created_at: 2023-07-22 10:35:52+00:00
  edited: false
  hidden: false
  id: 64bbbf185b8d826146ace347
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
      fullname: Yekta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yekta
      type: user
    createdAt: '2023-07-22T13:51:30.000Z'
    data:
      edited: false
      editors:
      - yekta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806448221206665
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656123216350-noauth.png?w=200&h=200&f=face
          fullname: Yekta
          isHf: false
          isPro: false
          name: yekta
          type: user
        html: '<p>I built it myself from the latest source and it worked. Thanks for
          the help.</p>

          '
        raw: I built it myself from the latest source and it worked. Thanks for the
          help.
        updatedAt: '2023-07-22T13:51:30.472Z'
      numEdits: 0
      reactions: []
    id: 64bbdee2796f20daad4a9aed
    type: comment
  author: yekta
  content: I built it myself from the latest source and it worked. Thanks for the
    help.
  created_at: 2023-07-22 12:51:30+00:00
  edited: false
  hidden: false
  id: 64bbdee2796f20daad4a9aed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T14:06:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408519864082336
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great to hear!</p>

          '
        raw: Great to hear!
        updatedAt: '2023-07-22T14:06:02.096Z'
      numEdits: 0
      reactions: []
    id: 64bbe24a1d40292dd3c2168c
    type: comment
  author: TheBloke
  content: Great to hear!
  created_at: 2023-07-22 13:06:02+00:00
  edited: false
  hidden: false
  id: 64bbe24a1d40292dd3c2168c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2217445536d1616b80675fbf994d11cb.svg
      fullname: Rishabh Thukral
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rtspeaks360
      type: user
    createdAt: '2023-09-18T13:30:06.000Z'
    data:
      edited: false
      editors:
      - rtspeaks360
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9918463826179504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2217445536d1616b80675fbf994d11cb.svg
          fullname: Rishabh Thukral
          isHf: false
          isPro: false
          name: rtspeaks360
          type: user
        html: '<p>Has anyone been able to confirm that it''s working? I am trying
          to deploy a similar model but now getting a phantom error with it.</p>

          '
        raw: Has anyone been able to confirm that it's working? I am trying to deploy
          a similar model but now getting a phantom error with it.
        updatedAt: '2023-09-18T13:30:06.817Z'
      numEdits: 0
      reactions: []
    id: 650850de1ae953ff2f84e4a7
    type: comment
  author: rtspeaks360
  content: Has anyone been able to confirm that it's working? I am trying to deploy
    a similar model but now getting a phantom error with it.
  created_at: 2023-09-18 12:30:06+00:00
  edited: false
  hidden: false
  id: 650850de1ae953ff2f84e4a7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Can't launch with TGI
