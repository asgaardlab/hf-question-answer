!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mstachow
conflicting_files: null
created_at: 2023-08-23 13:24:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-08-23T14:24:38.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9315647482872009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>I''m trying to run this model using auto_gptq and huggingface. Other
          gptq models, for instance 13B models, load just fine. However, whenever
          I try to load this model I get weird errors. Specifically:</p>

          <p>RuntimeError: [enforce fail at ..\c10\core\impl\alloc_cpu.cpp:72] data.
          DefaultCPUAllocator: not enough memory: you tried to allocate 117440512
          bytes.</p>

          <p>There are several issues here:</p>

          <ol>

          <li>I have 128GB of RAM, and when this error happened only about 58GB were
          allocated. That number looks to be about 117 MB, so I for sure have enough
          RAM</li>

          <li>I have an RTX A6000 with 48GB of RAM. Using 4-bit quantization with
          this model I should need only 35GB or so of VRAM</li>

          </ol>

          <p>Does anyone know what''s going on?</p>

          '
        raw: "I'm trying to run this model using auto_gptq and huggingface. Other\
          \ gptq models, for instance 13B models, load just fine. However, whenever\
          \ I try to load this model I get weird errors. Specifically:\r\n\r\nRuntimeError:\
          \ [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 117440512 bytes.\r\n\r\nThere\
          \ are several issues here:\r\n\r\n1. I have 128GB of RAM, and when this\
          \ error happened only about 58GB were allocated. That number looks to be\
          \ about 117 MB, so I for sure have enough RAM\r\n2. I have an RTX A6000\
          \ with 48GB of RAM. Using 4-bit quantization with this model I should need\
          \ only 35GB or so of VRAM\r\n\r\nDoes anyone know what's going on?"
        updatedAt: '2023-08-23T14:24:38.852Z'
      numEdits: 0
      reactions: []
    id: 64e616a64a4a87396d240321
    type: comment
  author: mstachow
  content: "I'm trying to run this model using auto_gptq and huggingface. Other gptq\
    \ models, for instance 13B models, load just fine. However, whenever I try to\
    \ load this model I get weird errors. Specifically:\r\n\r\nRuntimeError: [enforce\
    \ fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not\
    \ enough memory: you tried to allocate 117440512 bytes.\r\n\r\nThere are several\
    \ issues here:\r\n\r\n1. I have 128GB of RAM, and when this error happened only\
    \ about 58GB were allocated. That number looks to be about 117 MB, so I for sure\
    \ have enough RAM\r\n2. I have an RTX A6000 with 48GB of RAM. Using 4-bit quantization\
    \ with this model I should need only 35GB or so of VRAM\r\n\r\nDoes anyone know\
    \ what's going on?"
  created_at: 2023-08-23 13:24:38+00:00
  edited: false
  hidden: false
  id: 64e616a64a4a87396d240321
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T15:11:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9827937483787537
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm. That is odd. That error does mean you don''t have enough RAM,
          but you have plenty. And yes you have enough VRAM, too.</p>

          <p>What OS?  All I can think is that you''re running on Windows? If so,
          then this is an annoyance with Windows: regardless of how much RAM you have,
          you also need a large pagefile.  I''d set it to 100GB minimum.  That often
          affects people trying to load 33B models, and would apply even more with
          70B.</p>

          <p>If you''re on Linux then I don''t know; it should work.</p>

          '
        raw: 'Hmm. That is odd. That error does mean you don''t have enough RAM, but
          you have plenty. And yes you have enough VRAM, too.


          What OS?  All I can think is that you''re running on Windows? If so, then
          this is an annoyance with Windows: regardless of how much RAM you have,
          you also need a large pagefile.  I''d set it to 100GB minimum.  That often
          affects people trying to load 33B models, and would apply even more with
          70B.


          If you''re on Linux then I don''t know; it should work.'
        updatedAt: '2023-08-23T15:11:47.944Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ineijp
        - gchesnik
        - jlzhou
        - GracieSqueaks
    id: 64e621b3a6bab0d559048163
    type: comment
  author: TheBloke
  content: 'Hmm. That is odd. That error does mean you don''t have enough RAM, but
    you have plenty. And yes you have enough VRAM, too.


    What OS?  All I can think is that you''re running on Windows? If so, then this
    is an annoyance with Windows: regardless of how much RAM you have, you also need
    a large pagefile.  I''d set it to 100GB minimum.  That often affects people trying
    to load 33B models, and would apply even more with 70B.


    If you''re on Linux then I don''t know; it should work.'
  created_at: 2023-08-23 14:11:47+00:00
  edited: false
  hidden: false
  id: 64e621b3a6bab0d559048163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-08-23T15:21:27.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9898985624313354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>You were entirely correct, thank you! I am on Windows, and I had
          previously set my page files to 16GB, but I didn''t realize they had to
          be THAT big. Also I have two drives, so I set the page files on both to
          be 100GB min, 200GB max. Whether I needed to do that for both drives I''m
          not sure, but it works now and is happily running inference.</p>

          '
        raw: You were entirely correct, thank you! I am on Windows, and I had previously
          set my page files to 16GB, but I didn't realize they had to be THAT big.
          Also I have two drives, so I set the page files on both to be 100GB min,
          200GB max. Whether I needed to do that for both drives I'm not sure, but
          it works now and is happily running inference.
        updatedAt: '2023-08-23T15:21:27.536Z'
      numEdits: 0
      reactions: []
    id: 64e623f7a27e6be064119a61
    type: comment
  author: mstachow
  content: You were entirely correct, thank you! I am on Windows, and I had previously
    set my page files to 16GB, but I didn't realize they had to be THAT big. Also
    I have two drives, so I set the page files on both to be 100GB min, 200GB max.
    Whether I needed to do that for both drives I'm not sure, but it works now and
    is happily running inference.
  created_at: 2023-08-23 14:21:27+00:00
  edited: false
  hidden: false
  id: 64e623f7a27e6be064119a61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-05T16:41:26.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195600748062134
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>Same thing I trying to run synthia-70b-v1.1.Q8_0.gguf in Obaboga
          WEB UI and it drops loading the model with MemoryError in console after
          memory usage hits 77GB. (In native Llama.cpp the model loads without problem
          and consumes 75GB) I have 128GB and it looks like I just need 1-3GB more
          to cache this model even tho it only consumed 77GB (the model wants to cache
          78-80GB) but because of how this shitty Windows OS works I can''t load the
          model with still plenty of ram available. Right now I''m trying to squeeze
          additional 3GB from the OS by disabling some services, defender.. and etc.
          The Windows system just ridiculous. P.S. and my pagefile is disabled (for
          better speed and etc) and I don''t want to have it in my current PC. The
          barebone W11 system consumes 3GB of ram in Safe Mode with 128GB installed
          but in normal mode (with some drivers and crucial launchers installed for
          GPU and etc) it consumes around 8GB with 128GB installed I managed to lower
          it to 6GB so far but I need to lower somehow on 2-3GB more.  Windows 11
          is memory hungry system in comparison to Windows XP OS that just consumes
          something like 128-256MB out of the box.<br>Or I might just wait for Obaboga
          WEB UI updates because I can load the model just fine in Llama.cpp.</p>

          <p>Update: Updated today Obaboga WEB UI to the latest version and finally
          now the model loads perfectly fine.</p>

          '
        raw: "Same thing I trying to run synthia-70b-v1.1.Q8_0.gguf in Obaboga WEB\
          \ UI and it drops loading the model with MemoryError in console after memory\
          \ usage hits 77GB. (In native Llama.cpp the model loads without problem\
          \ and consumes 75GB) I have 128GB and it looks like I just need 1-3GB more\
          \ to cache this model even tho it only consumed 77GB (the model wants to\
          \ cache 78-80GB) but because of how this shitty Windows OS works I can't\
          \ load the model with still plenty of ram available. Right now I'm trying\
          \ to squeeze additional 3GB from the OS by disabling some services, defender..\
          \ and etc. The Windows system just ridiculous. P.S. and my pagefile is disabled\
          \ (for better speed and etc) and I don't want to have it in my current PC.\
          \ The barebone W11 system consumes 3GB of ram in Safe Mode with 128GB installed\
          \ but in normal mode (with some drivers and crucial launchers installed\
          \ for GPU and etc) it consumes around 8GB with 128GB installed I managed\
          \ to lower it to 6GB so far but I need to lower somehow on 2-3GB more. \
          \ Windows 11 is memory hungry system in comparison to Windows XP OS that\
          \ just consumes something like 128-256MB out of the box. \nOr I might just\
          \ wait for Obaboga WEB UI updates because I can load the model just fine\
          \ in Llama.cpp.\n\nUpdate: Updated today Obaboga WEB UI to the latest version\
          \ and finally now the model loads perfectly fine."
        updatedAt: '2023-09-05T19:16:53.757Z'
      numEdits: 14
      reactions: []
    id: 64f75a36d04a890f5346aba4
    type: comment
  author: Flanua
  content: "Same thing I trying to run synthia-70b-v1.1.Q8_0.gguf in Obaboga WEB UI\
    \ and it drops loading the model with MemoryError in console after memory usage\
    \ hits 77GB. (In native Llama.cpp the model loads without problem and consumes\
    \ 75GB) I have 128GB and it looks like I just need 1-3GB more to cache this model\
    \ even tho it only consumed 77GB (the model wants to cache 78-80GB) but because\
    \ of how this shitty Windows OS works I can't load the model with still plenty\
    \ of ram available. Right now I'm trying to squeeze additional 3GB from the OS\
    \ by disabling some services, defender.. and etc. The Windows system just ridiculous.\
    \ P.S. and my pagefile is disabled (for better speed and etc) and I don't want\
    \ to have it in my current PC. The barebone W11 system consumes 3GB of ram in\
    \ Safe Mode with 128GB installed but in normal mode (with some drivers and crucial\
    \ launchers installed for GPU and etc) it consumes around 8GB with 128GB installed\
    \ I managed to lower it to 6GB so far but I need to lower somehow on 2-3GB more.\
    \  Windows 11 is memory hungry system in comparison to Windows XP OS that just\
    \ consumes something like 128-256MB out of the box. \nOr I might just wait for\
    \ Obaboga WEB UI updates because I can load the model just fine in Llama.cpp.\n\
    \nUpdate: Updated today Obaboga WEB UI to the latest version and finally now the\
    \ model loads perfectly fine."
  created_at: 2023-09-05 15:41:26+00:00
  edited: true
  hidden: false
  id: 64f75a36d04a890f5346aba4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7f29b06442eb4b2f2250c10add920e8.svg
      fullname: Griffin Chesnik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gchesnik
      type: user
    createdAt: '2023-09-27T19:15:02.000Z'
    data:
      edited: false
      editors:
      - gchesnik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822955131530762
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7f29b06442eb4b2f2250c10add920e8.svg
          fullname: Griffin Chesnik
          isHf: false
          isPro: false
          name: gchesnik
          type: user
        html: '<blockquote>

          <p>Hmm. That is odd. That error does mean you don''t have enough RAM, but
          you have plenty. And yes you have enough VRAM, too.</p>

          <p>What OS?  All I can think is that you''re running on Windows? If so,
          then this is an annoyance with Windows: regardless of how much RAM you have,
          you also need a large pagefile.  I''d set it to 100GB minimum.  That often
          affects people trying to load 33B models, and would apply even more with
          70B.</p>

          <p>If you''re on Linux then I don''t know; it should work.</p>

          </blockquote>

          <p>Thank you for this, I recently got an A6000 and was trying to load 70b
          and was running into the same issue. Changing the paging file corrected
          the issue right away. Took me a while to find the answer. Maybe mention
          on the pages about the paging file. First time I came across this after
          a few days of searching.</p>

          '
        raw: "> Hmm. That is odd. That error does mean you don't have enough RAM,\
          \ but you have plenty. And yes you have enough VRAM, too.\n> \n> What OS?\
          \  All I can think is that you're running on Windows? If so, then this is\
          \ an annoyance with Windows: regardless of how much RAM you have, you also\
          \ need a large pagefile.  I'd set it to 100GB minimum.  That often affects\
          \ people trying to load 33B models, and would apply even more with 70B.\n\
          > \n> If you're on Linux then I don't know; it should work.\n\nThank you\
          \ for this, I recently got an A6000 and was trying to load 70b and was running\
          \ into the same issue. Changing the paging file corrected the issue right\
          \ away. Took me a while to find the answer. Maybe mention on the pages about\
          \ the paging file. First time I came across this after a few days of searching."
        updatedAt: '2023-09-27T19:15:02.283Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 65147f3608854107699c2a74
    type: comment
  author: gchesnik
  content: "> Hmm. That is odd. That error does mean you don't have enough RAM, but\
    \ you have plenty. And yes you have enough VRAM, too.\n> \n> What OS?  All I can\
    \ think is that you're running on Windows? If so, then this is an annoyance with\
    \ Windows: regardless of how much RAM you have, you also need a large pagefile.\
    \  I'd set it to 100GB minimum.  That often affects people trying to load 33B\
    \ models, and would apply even more with 70B.\n> \n> If you're on Linux then I\
    \ don't know; it should work.\n\nThank you for this, I recently got an A6000 and\
    \ was trying to load 70b and was running into the same issue. Changing the paging\
    \ file corrected the issue right away. Took me a while to find the answer. Maybe\
    \ mention on the pages about the paging file. First time I came across this after\
    \ a few days of searching."
  created_at: 2023-09-27 18:15:02+00:00
  edited: false
  hidden: false
  id: 65147f3608854107699c2a74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-30T23:27:59.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9758256077766418
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Hmm. That is odd. That error does mean you don''t have enough RAM, but
          you have plenty. And yes you have enough VRAM, too.</p>

          <p>What OS?  All I can think is that you''re running on Windows? If so,
          then this is an annoyance with Windows: regardless of how much RAM you have,
          you also need a large pagefile.  I''d set it to 100GB minimum.  That often
          affects people trying to load 33B models, and would apply even more with
          70B.</p>

          <p>If you''re on Linux then I don''t know; it should work.</p>

          </blockquote>

          <p>Thank you for this, I recently got an A6000 and was trying to load 70b
          and was running into the same issue. Changing the paging file corrected
          the issue right away. Took me a while to find the answer. Maybe mention
          on the pages about the paging file. First time I came across this after
          a few days of searching.</p>

          </blockquote>

          <p>Actually If you have enough ram you don''t need a pagefile I have 128GB
          of ram right now and my pagefile is completely disabled. I run 70B models
          without ram issues after I updated my Obaboga WEB UI.</p>

          '
        raw: "> > Hmm. That is odd. That error does mean you don't have enough RAM,\
          \ but you have plenty. And yes you have enough VRAM, too.\n> > \n> > What\
          \ OS?  All I can think is that you're running on Windows? If so, then this\
          \ is an annoyance with Windows: regardless of how much RAM you have, you\
          \ also need a large pagefile.  I'd set it to 100GB minimum.  That often\
          \ affects people trying to load 33B models, and would apply even more with\
          \ 70B.\n> > \n> > If you're on Linux then I don't know; it should work.\n\
          > \n> Thank you for this, I recently got an A6000 and was trying to load\
          \ 70b and was running into the same issue. Changing the paging file corrected\
          \ the issue right away. Took me a while to find the answer. Maybe mention\
          \ on the pages about the paging file. First time I came across this after\
          \ a few days of searching.\n\nActually If you have enough ram you don't\
          \ need a pagefile I have 128GB of ram right now and my pagefile is completely\
          \ disabled. I run 70B models without ram issues after I updated my Obaboga\
          \ WEB UI."
        updatedAt: '2023-09-30T23:27:59.649Z'
      numEdits: 0
      reactions: []
    id: 6518aeff0e3a5553d492b3bd
    type: comment
  author: Flanua
  content: "> > Hmm. That is odd. That error does mean you don't have enough RAM,\
    \ but you have plenty. And yes you have enough VRAM, too.\n> > \n> > What OS?\
    \  All I can think is that you're running on Windows? If so, then this is an annoyance\
    \ with Windows: regardless of how much RAM you have, you also need a large pagefile.\
    \  I'd set it to 100GB minimum.  That often affects people trying to load 33B\
    \ models, and would apply even more with 70B.\n> > \n> > If you're on Linux then\
    \ I don't know; it should work.\n> \n> Thank you for this, I recently got an A6000\
    \ and was trying to load 70b and was running into the same issue. Changing the\
    \ paging file corrected the issue right away. Took me a while to find the answer.\
    \ Maybe mention on the pages about the paging file. First time I came across this\
    \ after a few days of searching.\n\nActually If you have enough ram you don't\
    \ need a pagefile I have 128GB of ram right now and my pagefile is completely\
    \ disabled. I run 70B models without ram issues after I updated my Obaboga WEB\
    \ UI."
  created_at: 2023-09-30 22:27:59+00:00
  edited: false
  hidden: false
  id: 6518aeff0e3a5553d492b3bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Out of memory error, but both system and GPU have plenty of memory
