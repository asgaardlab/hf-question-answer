!!python/object:huggingface_hub.community.DiscussionWithDetails
author: StefanStroescu
conflicting_files: null
created_at: 2023-08-16 09:30:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07ff748bc496630785e48476441e0c1d.svg
      fullname: Stefan Stroescu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StefanStroescu
      type: user
    createdAt: '2023-08-16T10:30:50.000Z'
    data:
      edited: false
      editors:
      - StefanStroescu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9480493664741516
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07ff748bc496630785e48476441e0c1d.svg
          fullname: Stefan Stroescu
          isHf: false
          isPro: false
          name: StefanStroescu
          type: user
        html: '<p>Hello,</p>

          <p>I am trying to use the model to generate me an answer from a context
          I provide, but when I get to text generation, I get this error: temp_state
          buffer is too small.<br>I think it is because my prompt is quite large in
          terms of tokens, because when I prompt the model without context it works.</p>

          <p>I checked and is not an issue of resources, GPU or RAM, and the Llama-2-13B-chat-GPTQ
          worked when prompted with context. </p>

          <p>Does anyone have any suggestions on how to solve this?</p>

          <p>Thanks,</p>

          '
        raw: "Hello,\r\n\r\nI am trying to use the model to generate me an answer\
          \ from a context I provide, but when I get to text generation, I get this\
          \ error: temp_state buffer is too small. \r\nI think it is because my prompt\
          \ is quite large in terms of tokens, because when I prompt the model without\
          \ context it works.\r\n\r\nI checked and is not an issue of resources, GPU\
          \ or RAM, and the Llama-2-13B-chat-GPTQ worked when prompted with context.\
          \ \r\n\r\nDoes anyone have any suggestions on how to solve this?\r\n\r\n\
          Thanks,\r\n"
        updatedAt: '2023-08-16T10:30:50.834Z'
      numEdits: 0
      reactions: []
    id: 64dca55a56932ae551a7a573
    type: comment
  author: StefanStroescu
  content: "Hello,\r\n\r\nI am trying to use the model to generate me an answer from\
    \ a context I provide, but when I get to text generation, I get this error: temp_state\
    \ buffer is too small. \r\nI think it is because my prompt is quite large in terms\
    \ of tokens, because when I prompt the model without context it works.\r\n\r\n\
    I checked and is not an issue of resources, GPU or RAM, and the Llama-2-13B-chat-GPTQ\
    \ worked when prompted with context. \r\n\r\nDoes anyone have any suggestions\
    \ on how to solve this?\r\n\r\nThanks,\r\n"
  created_at: 2023-08-16 09:30:50+00:00
  edited: false
  hidden: false
  id: 64dca55a56932ae551a7a573
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-16T11:22:13.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6378381848335266
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues/253">https://github.com/PanQiWei/AutoGPTQ/issues/253</a></p>

          '
        raw: https://github.com/PanQiWei/AutoGPTQ/issues/253
        updatedAt: '2023-08-16T11:22:13.429Z'
      numEdits: 0
      reactions: []
    id: 64dcb1658ce1a26b2183eb4d
    type: comment
  author: Komposter43
  content: https://github.com/PanQiWei/AutoGPTQ/issues/253
  created_at: 2023-08-16 10:22:13+00:00
  edited: false
  hidden: false
  id: 64dcb1658ce1a26b2183eb4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07ff748bc496630785e48476441e0c1d.svg
      fullname: Stefan Stroescu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StefanStroescu
      type: user
    createdAt: '2023-08-17T05:15:08.000Z'
    data:
      edited: false
      editors:
      - StefanStroescu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.887134850025177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07ff748bc496630785e48476441e0c1d.svg
          fullname: Stefan Stroescu
          isHf: false
          isPro: false
          name: StefanStroescu
          type: user
        html: '<p>Thanks, Komposter43,</p>

          <p>I don''t know if it has anything to do with this (<a href="https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/29">https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/29</a>),
          but I noticed that the model accepts only inference requests under 2048
          tokens.</p>

          '
        raw: 'Thanks, Komposter43,


          I don''t know if it has anything to do with this (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/29),
          but I noticed that the model accepts only inference requests under 2048
          tokens.

          '
        updatedAt: '2023-08-17T05:15:08.362Z'
      numEdits: 0
      reactions: []
    id: 64ddacdc6c6356f64bebb3ca
    type: comment
  author: StefanStroescu
  content: 'Thanks, Komposter43,


    I don''t know if it has anything to do with this (https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ/discussions/29),
    but I noticed that the model accepts only inference requests under 2048 tokens.

    '
  created_at: 2023-08-17 04:15:08+00:00
  edited: false
  hidden: false
  id: 64ddacdc6c6356f64bebb3ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
      fullname: Komposter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Komposter43
      type: user
    createdAt: '2023-08-25T11:04:47.000Z'
    data:
      edited: false
      editors:
      - Komposter43
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.694624662399292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4095f153b83292bc9997d792cd3853b.svg
          fullname: Komposter
          isHf: false
          isPro: false
          name: Komposter43
          type: user
        html: '<p>fixed in 0.4.2 <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/releases/tag/v0.4.2">https://github.com/PanQiWei/AutoGPTQ/releases/tag/v0.4.2</a></p>

          '
        raw: fixed in 0.4.2 https://github.com/PanQiWei/AutoGPTQ/releases/tag/v0.4.2
        updatedAt: '2023-08-25T11:04:47.152Z'
      numEdits: 0
      reactions: []
    id: 64e88acf83affd72d9432de7
    type: comment
  author: Komposter43
  content: fixed in 0.4.2 https://github.com/PanQiWei/AutoGPTQ/releases/tag/v0.4.2
  created_at: 2023-08-25 10:04:47+00:00
  edited: false
  hidden: false
  id: 64e88acf83affd72d9432de7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Error when running pipe: temp_state buffer is too small'
