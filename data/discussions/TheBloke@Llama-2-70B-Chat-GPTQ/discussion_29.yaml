!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zzzac
conflicting_files: null
created_at: 2023-07-31 20:45:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0ff4efc3ea41ffb06aebeb962762ae3.svg
      fullname: ziyuan wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zzzac
      type: user
    createdAt: '2023-07-31T21:45:44.000Z'
    data:
      edited: false
      editors:
      - zzzac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8914657831192017
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0ff4efc3ea41ffb06aebeb962762ae3.svg
          fullname: ziyuan wang
          isHf: false
          isPro: false
          name: zzzac
          type: user
        html: '<p>I saw from the config that <code>max_position_embeddings</code>
          is set to 2048, but the original llama2 model has 4096 maximum input length.
          Is there a particular reason to reduce the input length of these quantized
          model? </p>

          <p>Thanks for this great work!</p>

          '
        raw: "I saw from the config that `max_position_embeddings` is set to 2048,\
          \ but the original llama2 model has 4096 maximum input length. Is there\
          \ a particular reason to reduce the input length of these quantized model?\
          \ \r\n\r\nThanks for this great work!\r\n"
        updatedAt: '2023-07-31T21:45:44.310Z'
      numEdits: 0
      reactions: []
    id: 64c82b888b1d0044b902e898
    type: comment
  author: zzzac
  content: "I saw from the config that `max_position_embeddings` is set to 2048, but\
    \ the original llama2 model has 4096 maximum input length. Is there a particular\
    \ reason to reduce the input length of these quantized model? \r\n\r\nThanks for\
    \ this great work!\r\n"
  created_at: 2023-07-31 20:45:44+00:00
  edited: false
  hidden: false
  id: 64c82b888b1d0044b902e898
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-31T21:50:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9762850403785706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No sorry that''s just a mistake. Or rather, the original Llama 2
          config.json''s had that set to 2048 so that''s what mine were set to. Then
          they updated theirs to 4096.</p>

          <p>I did update mine too, but I see now I only did that for the main branch
          config.json, not the additional branch alternative GPTQs.  I''ll fix that
          now.</p>

          <p>To be honest it doesn''t matter for most clients, which set the length
          independently. The max_position_embeddings is more a default, not a maximum.  But
          anyway, I''ll fix it.</p>

          '
        raw: 'No sorry that''s just a mistake. Or rather, the original Llama 2 config.json''s
          had that set to 2048 so that''s what mine were set to. Then they updated
          theirs to 4096.


          I did update mine too, but I see now I only did that for the main branch
          config.json, not the additional branch alternative GPTQs.  I''ll fix that
          now.


          To be honest it doesn''t matter for most clients, which set the length independently.
          The max_position_embeddings is more a default, not a maximum.  But anyway,
          I''ll fix it.'
        updatedAt: '2023-07-31T21:50:27.422Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
        - zzzac
    id: 64c82ca34515835c4d807695
    type: comment
  author: TheBloke
  content: 'No sorry that''s just a mistake. Or rather, the original Llama 2 config.json''s
    had that set to 2048 so that''s what mine were set to. Then they updated theirs
    to 4096.


    I did update mine too, but I see now I only did that for the main branch config.json,
    not the additional branch alternative GPTQs.  I''ll fix that now.


    To be honest it doesn''t matter for most clients, which set the length independently.
    The max_position_embeddings is more a default, not a maximum.  But anyway, I''ll
    fix it.'
  created_at: 2023-07-31 20:50:27+00:00
  edited: false
  hidden: false
  id: 64c82ca34515835c4d807695
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: max_position_embeddings = 2048?
