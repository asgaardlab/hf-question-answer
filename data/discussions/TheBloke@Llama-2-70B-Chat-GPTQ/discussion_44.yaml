!!python/object:huggingface_hub.community.DiscussionWithDetails
author: peterwu00
conflicting_files: null
created_at: 2023-10-13 21:19:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
      fullname: ning wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peterwu00
      type: user
    createdAt: '2023-10-13T22:19:04.000Z'
    data:
      edited: false
      editors:
      - peterwu00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7508819103240967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
          fullname: ning wu
          isHf: false
          isPro: false
          name: peterwu00
          type: user
        html: '<p>I tried to load this model into langchain and ran into this issue
          below.  has anyone successfully used langchain with this model?  Thanks.</p>

          <p>RuntimeError: The temp_state buffer is too small in the exllama backend.
          Please call the exllama_set_max_input_length function to increase the buffer
          size. Example:<br>from auto_gptq import exllama_set_max_input_length<br>model
          = exllama_set_max_input_length(model, 4096)</p>

          '
        raw: "I tried to load this model into langchain and ran into this issue below.\
          \  has anyone successfully used langchain with this model?  Thanks.\r\n\r\
          \nRuntimeError: The temp_state buffer is too small in the exllama backend.\
          \ Please call the exllama_set_max_input_length function to increase the\
          \ buffer size. Example:\r\nfrom auto_gptq import exllama_set_max_input_length\r\
          \nmodel = exllama_set_max_input_length(model, 4096)\r\n"
        updatedAt: '2023-10-13T22:19:04.174Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - willu
    id: 6529c2584d8e7e7646c79663
    type: comment
  author: peterwu00
  content: "I tried to load this model into langchain and ran into this issue below.\
    \  has anyone successfully used langchain with this model?  Thanks.\r\n\r\nRuntimeError:\
    \ The temp_state buffer is too small in the exllama backend. Please call the exllama_set_max_input_length\
    \ function to increase the buffer size. Example:\r\nfrom auto_gptq import exllama_set_max_input_length\r\
    \nmodel = exllama_set_max_input_length(model, 4096)\r\n"
  created_at: 2023-10-13 21:19:04+00:00
  edited: false
  hidden: false
  id: 6529c2584d8e7e7646c79663
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f051d2be8c0020e6100b47b8cc0688c4.svg
      fullname: Shreayan Chaudhary
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shreayan98c
      type: user
    createdAt: '2023-10-18T00:11:54.000Z'
    data:
      edited: false
      editors:
      - shreayan98c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9747244715690613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f051d2be8c0020e6100b47b8cc0688c4.svg
          fullname: Shreayan Chaudhary
          isHf: false
          isPro: false
          name: shreayan98c
          type: user
        html: '<p>I am facing the same issue. Were you able to find a way to resolve
          it?</p>

          '
        raw: I am facing the same issue. Were you able to find a way to resolve it?
        updatedAt: '2023-10-18T00:11:54.116Z'
      numEdits: 0
      reactions: []
    id: 652f22caf75d9cc2a4cdeb44
    type: comment
  author: shreayan98c
  content: I am facing the same issue. Were you able to find a way to resolve it?
  created_at: 2023-10-17 23:11:54+00:00
  edited: false
  hidden: false
  id: 652f22caf75d9cc2a4cdeb44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
      fullname: ning wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peterwu00
      type: user
    createdAt: '2023-10-18T00:18:38.000Z'
    data:
      edited: false
      editors:
      - peterwu00
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.875447690486908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
          fullname: ning wu
          isHf: false
          isPro: false
          name: peterwu00
          type: user
        html: '<p>No</p>

          '
        raw: 'No'
        updatedAt: '2023-10-18T00:18:38.789Z'
      numEdits: 0
      reactions: []
    id: 652f245e67acb68b9fab6849
    type: comment
  author: peterwu00
  content: 'No'
  created_at: 2023-10-17 23:18:38+00:00
  edited: false
  hidden: false
  id: 652f245e67acb68b9fab6849
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
      fullname: ning wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peterwu00
      type: user
    createdAt: '2023-10-18T21:22:50.000Z'
    data:
      edited: false
      editors:
      - peterwu00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7775126695632935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
          fullname: ning wu
          isHf: false
          isPro: false
          name: peterwu00
          type: user
        html: '<blockquote>

          <p>I am facing the same issue. Were you able to find a way to resolve it?</p>

          </blockquote>

          <p>fixed the issue.  actually straightforward.  just add these to your code:
          </p>

          <p>from auto_gptq import exllama_set_max_input_length<br>model = exllama_set_max_input_length(model,
          4096)</p>

          '
        raw: "> I am facing the same issue. Were you able to find a way to resolve\
          \ it?\n\nfixed the issue.  actually straightforward.  just add these to\
          \ your code: \n\nfrom auto_gptq import exllama_set_max_input_length\nmodel\
          \ = exllama_set_max_input_length(model, 4096)"
        updatedAt: '2023-10-18T21:22:50.128Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - JoshuaCAlpuerto
        - fabriceyhc
    id: 65304caa01a1a4ee277d505d
    type: comment
  author: peterwu00
  content: "> I am facing the same issue. Were you able to find a way to resolve it?\n\
    \nfixed the issue.  actually straightforward.  just add these to your code: \n\
    \nfrom auto_gptq import exllama_set_max_input_length\nmodel = exllama_set_max_input_length(model,\
    \ 4096)"
  created_at: 2023-10-18 20:22:50+00:00
  edited: false
  hidden: false
  id: 65304caa01a1a4ee277d505d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/991fc1ea0c54ec58d75371b0ef553131.svg
      fullname: ning wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peterwu00
      type: user
    createdAt: '2023-10-18T21:22:56.000Z'
    data:
      status: closed
    id: 65304cb0cb5ad115c3863f98
    type: status-change
  author: peterwu00
  created_at: 2023-10-18 20:22:56+00:00
  id: 65304cb0cb5ad115c3863f98
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: TheBloke/Llama-2-70B-Chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: langchain issue
