!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hemangjoshi37a
conflicting_files: null
created_at: 2023-08-02 11:53:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
      fullname: Hemang Joshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hemangjoshi37a
      type: user
    createdAt: '2023-08-02T12:53:16.000Z'
    data:
      edited: false
      editors:
      - hemangjoshi37a
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43839699029922485
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
          fullname: Hemang Joshi
          isHf: false
          isPro: false
          name: hemangjoshi37a
          type: user
        html: "<p>code : </p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"red1xe/falcon-7b-codeGPT-3K\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nmodel = AutoModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"red1xe/falcon-7b-codeGPT-3K\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, device=<span class=\"hljs-string\"\
          >'cpu'</span>)\nmodel = model.<span class=\"hljs-built_in\">eval</span>()\n\
          </code></pre>\n<p>error :</p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[4], line 2\n      1 from transformers import AutoTokenizer, AutoModel\n\
          ----&gt; 2 tokenizer = AutoTokenizer.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
          , trust_remote_code=True)\n      3 model = AutoModel.from_pretrained(\"\
          red1xe/falcon-7b-codeGPT-3K\", trust_remote_code=True, device='cpu')\n \
          \     4 model = model.eval()\n\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:658,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    656 if config_tokenizer_class is None:\n    657     if\
          \ not isinstance(config, PretrainedConfig):\n--&gt; 658         config =\
          \ AutoConfig.from_pretrained(\n    659             pretrained_model_name_or_path,\
          \ trust_remote_code=trust_remote_code, **kwargs\n    660         )\n   \
          \ 661     config_tokenizer_class = config.tokenizer_class\n    662     if\
          \ hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\n\
          \nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:966,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    963         if pattern in str(pretrained_model_name_or_path):\n   \
          \ 964             return CONFIG_MAPPING[pattern].from_dict(config_dict,\
          \ **unused_kwargs)\n--&gt; 966 raise ValueError(\n    967     f\"Unrecognized\
          \ model in {pretrained_model_name_or_path}. \"\n    968     f\"Should have\
          \ a `model_type` key in its {CONFIG_NAME}, or contain one of the following\
          \ strings \"\n    969     f\"in its name: {', '.join(CONFIG_MAPPING.keys())}\"\
          \n    970 )\n\nValueError: Unrecognized model in red1xe/falcon-7b-codeGPT-3K.\
          \ Should have a `model_type` key in its config.json, or contain one of the\
          \ following strings in its name: albert, align, altclip, audio-spectrogram-transformer,\
          \ autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus,\
          \ biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower,\
          \ camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr,\
          \ convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text,\
          \ data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr,\
          \ deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer,\
          \ efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert,\
          \ flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode,\
          \ gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer,\
          \ groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2,\
          \ layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert,\
          \ m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart,\
          \ mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2,\
          \ mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer,\
          \ oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver,\
          \ pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer,\
          \ regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert,\
          \ roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text,\
          \ speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin,\
          \ swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer,\
          \ timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr,\
          \ tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder,\
          \ vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn,\
          \ wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet,\
          \ xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso\n</code></pre>\n\
          <p><a rel=\"nofollow\" href=\"https://hjlabs.in\">https://hjlabs.in</a></p>\n"
        raw: "code : \r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
          , trust_remote_code=True)\r\nmodel = AutoModel.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
          , trust_remote_code=True, device='cpu')\r\nmodel = model.eval()\r\n```\r\
          \n\r\nerror :\r\n```\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[4], line 2\r\n      1 from transformers import AutoTokenizer,\
          \ AutoModel\r\n----> 2 tokenizer = AutoTokenizer.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
          , trust_remote_code=True)\r\n      3 model = AutoModel.from_pretrained(\"\
          red1xe/falcon-7b-codeGPT-3K\", trust_remote_code=True, device='cpu')\r\n\
          \      4 model = model.eval()\r\n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:658,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    656 if config_tokenizer_class is None:\r\n    657   \
          \  if not isinstance(config, PretrainedConfig):\r\n--> 658         config\
          \ = AutoConfig.from_pretrained(\r\n    659             pretrained_model_name_or_path,\
          \ trust_remote_code=trust_remote_code, **kwargs\r\n    660         )\r\n\
          \    661     config_tokenizer_class = config.tokenizer_class\r\n    662\
          \     if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\r\
          \n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:966,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n    963         if pattern in str(pretrained_model_name_or_path):\r\n\
          \    964             return CONFIG_MAPPING[pattern].from_dict(config_dict,\
          \ **unused_kwargs)\r\n--> 966 raise ValueError(\r\n    967     f\"Unrecognized\
          \ model in {pretrained_model_name_or_path}. \"\r\n    968     f\"Should\
          \ have a `model_type` key in its {CONFIG_NAME}, or contain one of the following\
          \ strings \"\r\n    969     f\"in its name: {', '.join(CONFIG_MAPPING.keys())}\"\
          \r\n    970 )\r\n\r\nValueError: Unrecognized model in red1xe/falcon-7b-codeGPT-3K.\
          \ Should have a `model_type` key in its config.json, or contain one of the\
          \ following strings in its name: albert, align, altclip, audio-spectrogram-transformer,\
          \ autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus,\
          \ biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower,\
          \ camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr,\
          \ convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text,\
          \ data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr,\
          \ deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer,\
          \ efficientnet, electra, encoder-decoder, ernie, ernie_m, esm, flaubert,\
          \ flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode,\
          \ gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer,\
          \ groupvit, hubert, ibert, imagegpt, informer, jukebox, layoutlm, layoutlmv2,\
          \ layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert,\
          \ m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart,\
          \ mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2,\
          \ mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer,\
          \ oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver,\
          \ pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer,\
          \ regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert,\
          \ roformer, rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text,\
          \ speech_to_text_2, speecht5, splinter, squeezebert, swiftformer, swin,\
          \ swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer,\
          \ timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr,\
          \ tvlt, unispeech, unispeech-sat, upernet, van, videomae, vilt, vision-encoder-decoder,\
          \ vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn,\
          \ wav2vec2, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet,\
          \ xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso\r\n```\r\nhttps://hjlabs.in"
        updatedAt: '2023-08-02T12:53:16.531Z'
      numEdits: 0
      reactions: []
    id: 64ca51bc40ce646cc7b146da
    type: comment
  author: hemangjoshi37a
  content: "code : \r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\", trust_remote_code=True)\r\
    \nmodel = AutoModel.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\", trust_remote_code=True,\
    \ device='cpu')\r\nmodel = model.eval()\r\n```\r\n\r\nerror :\r\n```\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[4], line 2\r\n      1 from transformers import AutoTokenizer, AutoModel\r\
    \n----> 2 tokenizer = AutoTokenizer.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
    , trust_remote_code=True)\r\n      3 model = AutoModel.from_pretrained(\"red1xe/falcon-7b-codeGPT-3K\"\
    , trust_remote_code=True, device='cpu')\r\n      4 model = model.eval()\r\n\r\n\
    File ~/.local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:658,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    656 if config_tokenizer_class is None:\r\n    657     if not\
    \ isinstance(config, PretrainedConfig):\r\n--> 658         config = AutoConfig.from_pretrained(\r\
    \n    659             pretrained_model_name_or_path, trust_remote_code=trust_remote_code,\
    \ **kwargs\r\n    660         )\r\n    661     config_tokenizer_class = config.tokenizer_class\r\
    \n    662     if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\r\
    \n\r\nFile ~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:966,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    963         if pattern in str(pretrained_model_name_or_path):\r\n    964\
    \             return CONFIG_MAPPING[pattern].from_dict(config_dict, **unused_kwargs)\r\
    \n--> 966 raise ValueError(\r\n    967     f\"Unrecognized model in {pretrained_model_name_or_path}.\
    \ \"\r\n    968     f\"Should have a `model_type` key in its {CONFIG_NAME}, or\
    \ contain one of the following strings \"\r\n    969     f\"in its name: {', '.join(CONFIG_MAPPING.keys())}\"\
    \r\n    970 )\r\n\r\nValueError: Unrecognized model in red1xe/falcon-7b-codeGPT-3K.\
    \ Should have a `model_type` key in its config.json, or contain one of the following\
    \ strings in its name: albert, align, altclip, audio-spectrogram-transformer,\
    \ autoformer, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt,\
    \ bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert,\
    \ canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert,\
    \ convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision,\
    \ deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr,\
    \ dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra,\
    \ encoder-decoder, ernie, ernie_m, esm, flaubert, flava, fnet, focalnet, fsmt,\
    \ funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese,\
    \ gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer,\
    \ jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer,\
    \ longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin,\
    \ mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2,\
    \ mobilevit, mobilevitv2, mpnet, mt5, mvp, nat, nezha, nllb-moe, nystromformer,\
    \ oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver,\
    \ pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet,\
    \ rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer,\
    \ rwkv, sam, segformer, sew, sew-d, speech-encoder-decoder, speech_to_text, speech_to_text_2,\
    \ speecht5, splinter, squeezebert, swiftformer, swin, swin2sr, swinv2, switch_transformers,\
    \ t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone,\
    \ trajectory_transformer, transfo-xl, trocr, tvlt, unispeech, unispeech-sat, upernet,\
    \ van, videomae, vilt, vision-encoder-decoder, vision-text-dual-encoder, visual_bert,\
    \ vit, vit_hybrid, vit_mae, vit_msn, wav2vec2, wav2vec2-conformer, wavlm, whisper,\
    \ xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod,\
    \ yolos, yoso\r\n```\r\nhttps://hjlabs.in"
  created_at: 2023-08-02 11:53:16+00:00
  edited: false
  hidden: false
  id: 64ca51bc40ce646cc7b146da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64af9f5b6c81917fb206ee9c/KV457N7BiBtkHcMu4NAPr.png?w=200&h=200&f=face
      fullname: Selim Berat
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: red1xe
      type: user
    createdAt: '2023-08-03T07:49:59.000Z'
    data:
      edited: true
      editors:
      - red1xe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447459578514099
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64af9f5b6c81917fb206ee9c/KV457N7BiBtkHcMu4NAPr.png?w=200&h=200&f=face
          fullname: Selim Berat
          isHf: false
          isPro: false
          name: red1xe
          type: user
        html: '<p>I''m new in this field. I''m trying to figure out fix this problem.
          This model has been trained from "vilsonrodrigues/falcon-7b-instruct-sharded"
          with code_instructions dataset selecting 3K row. So the "model_type = falcon"
          but transformers library doesn''t support falcon models. Thats why my trying
          to train Llama-2 with same dataset. You can follow me to keep up to date
          with the developments.</p>

          '
        raw: I'm new in this field. I'm trying to figure out fix this problem. This
          model has been trained from "vilsonrodrigues/falcon-7b-instruct-sharded"
          with code_instructions dataset selecting 3K row. So the "model_type = falcon"
          but transformers library doesn't support falcon models. Thats why my trying
          to train Llama-2 with same dataset. You can follow me to keep up to date
          with the developments.
        updatedAt: '2023-08-03T07:50:15.858Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - red1xe
        - hemangjoshi37a
    id: 64cb5c2776200ec80ff0c5e2
    type: comment
  author: red1xe
  content: I'm new in this field. I'm trying to figure out fix this problem. This
    model has been trained from "vilsonrodrigues/falcon-7b-instruct-sharded" with
    code_instructions dataset selecting 3K row. So the "model_type = falcon" but transformers
    library doesn't support falcon models. Thats why my trying to train Llama-2 with
    same dataset. You can follow me to keep up to date with the developments.
  created_at: 2023-08-03 06:49:59+00:00
  edited: true
  hidden: false
  id: 64cb5c2776200ec80ff0c5e2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: red1xe/falcon-7b-codeGPT-3K
repo_type: model
status: open
target_branch: null
title: getting this error
