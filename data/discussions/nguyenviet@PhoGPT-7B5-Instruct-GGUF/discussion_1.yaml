!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nobita3921
conflicting_files: null
created_at: 2023-12-29 03:59:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
      fullname: Sane
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nobita3921
      type: user
    createdAt: '2023-12-29T03:59:13.000Z'
    data:
      edited: false
      editors:
      - nobita3921
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7623502016067505
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
          fullname: Sane
          isHf: false
          isPro: false
          name: nobita3921
          type: user
        html: '<p>Hi, I used <strong>ctransformer</strong> to load <strong>PhoGPT-7B5-Instruct-q5_0.gguf</strong>
          but I got the exception: <strong>RuntimeError: Failed to create LLM ''mpt''</strong>.
          I also tried using <strong>PhoGPT-7B5-Instruct-q5_k_m.gguf</strong> instead
          but got the same error. Do you know the reason and the way to fix it ? </p>

          '
        raw: 'Hi, I used **ctransformer** to load __PhoGPT-7B5-Instruct-q5_0.gguf__
          but I got the exception: **RuntimeError: Failed to create LLM ''mpt''**.
          I also tried using **PhoGPT-7B5-Instruct-q5_k_m.gguf** instead but got the
          same error. Do you know the reason and the way to fix it ? '
        updatedAt: '2023-12-29T03:59:13.872Z'
      numEdits: 0
      reactions: []
    id: 658e44119e16fa7510f97ebb
    type: comment
  author: nobita3921
  content: 'Hi, I used **ctransformer** to load __PhoGPT-7B5-Instruct-q5_0.gguf__
    but I got the exception: **RuntimeError: Failed to create LLM ''mpt''**. I also
    tried using **PhoGPT-7B5-Instruct-q5_k_m.gguf** instead but got the same error.
    Do you know the reason and the way to fix it ? '
  created_at: 2023-12-29 03:59:13+00:00
  edited: false
  hidden: false
  id: 658e44119e16fa7510f97ebb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
      fullname: Nguyen Viet
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nguyenviet
      type: user
    createdAt: '2023-12-31T14:58:55.000Z'
    data:
      edited: false
      editors:
      - nguyenviet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8878219127655029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
          fullname: Nguyen Viet
          isHf: false
          isPro: false
          name: nguyenviet
          type: user
        html: '<p>There are 2 things: (1) <em>llama.cpp</em> supports MPT model but
          does not fully support <em>phoGPT</em> yet. Some patching is required to
          make it work (I created a fork of <em>llama.cpp</em> <a rel="nofollow" href="https://github.com/nviet/llama.cpp">here</a>
          as an example) (2) <em>ctransformers</em> uses an older implementation of
          <em>llama.cpp</em> (the latest update is already 4 months ago), which making
          point <a href="/nguyenviet/PhoGPT-7B5-Instruct-GGUF/discussions/1">#1</a>
          impossible to be done correctly. So unless <em>ctransformers</em> updates
          itself, I''m not able to help you on this one. </p>

          '
        raw: 'There are 2 things: (1) *llama.cpp* supports MPT model but does not
          fully support *phoGPT* yet. Some patching is required to make it work (I
          created a fork of *llama.cpp* [here](https://github.com/nviet/llama.cpp)
          as an example) (2) *ctransformers* uses an older implementation of *llama.cpp*
          (the latest update is already 4 months ago), which making point #1 impossible
          to be done correctly. So unless *ctransformers* updates itself, I''m not
          able to help you on this one. '
        updatedAt: '2023-12-31T14:58:55.162Z'
      numEdits: 0
      reactions: []
    id: 659181af89f1ff0463a16740
    type: comment
  author: nguyenviet
  content: 'There are 2 things: (1) *llama.cpp* supports MPT model but does not fully
    support *phoGPT* yet. Some patching is required to make it work (I created a fork
    of *llama.cpp* [here](https://github.com/nviet/llama.cpp) as an example) (2) *ctransformers*
    uses an older implementation of *llama.cpp* (the latest update is already 4 months
    ago), which making point #1 impossible to be done correctly. So unless *ctransformers*
    updates itself, I''m not able to help you on this one. '
  created_at: 2023-12-31 14:58:55+00:00
  edited: false
  hidden: false
  id: 659181af89f1ff0463a16740
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
      fullname: Sane
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nobita3921
      type: user
    createdAt: '2024-01-02T04:02:06.000Z'
    data:
      edited: false
      editors:
      - nobita3921
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447922706604004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
          fullname: Sane
          isHf: false
          isPro: false
          name: nobita3921
          type: user
        html: '<p>Thank you for replying.<br>I have just used llama-cpp-python but
          it does not work too.<br>Have you ever loaded your converted phogpt ? And
          does it succeed?</p>

          '
        raw: "Thank you for replying. \nI have just used llama-cpp-python but it does\
          \ not work too.\nHave you ever loaded your converted phogpt ? And does it\
          \ succeed?"
        updatedAt: '2024-01-02T04:02:06.580Z'
      numEdits: 0
      reactions: []
    id: 65938abe77105e6e4077e4e6
    type: comment
  author: nobita3921
  content: "Thank you for replying. \nI have just used llama-cpp-python but it does\
    \ not work too.\nHave you ever loaded your converted phogpt ? And does it succeed?"
  created_at: 2024-01-02 04:02:06+00:00
  edited: false
  hidden: false
  id: 65938abe77105e6e4077e4e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
      fullname: Nguyen Viet
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nguyenviet
      type: user
    createdAt: '2024-01-04T04:32:04.000Z'
    data:
      edited: false
      editors:
      - nguyenviet
      hidden: false
      identifiedLanguage:
        language: vi
        probability: 0.9986730813980103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
          fullname: Nguyen Viet
          isHf: false
          isPro: false
          name: nguyenviet
          type: user
        html: "<p>Tested on WSL on Windows 10:</p>\n<pre><code>python3.10 -m venv\
          \ venv\nsource venv/bin/activate\ngit clone https://github.com/abetlen/llama-cpp-python/\n\
          cd llama-cpp-python\ngit checkout e9bc4c4baf3f121a178dec215770ccd0ac86c28e\n\
          cd vendor\ngit clone https://github.com/nviet/llama.cpp\ncd ..\npip install\
          \ .\n</code></pre>\n<p>The output:</p>\n<pre><code>&gt;&gt;&gt; from llama_cpp\
          \ import Llama\n&gt;&gt;&gt; llm = Llama(model_path=\"PhoGPT-7B5-Instruct-q4_0.gguf\"\
          , verbose=True)\n&gt;&gt;&gt; output = llm(\"L\xE0m th\u1EBF n\xE0o \u0111\
          \u1EC3 c\u1EA3i thi\u1EC7n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\"\
          , max_tokens=1024, echo=True)\nLlama.generate: prefix-match hit\n\nllama_print_timings:\
          \        load time =    2342.78 ms\nllama_print_timings:      sample time\
          \ =    2019.40 ms /   498 runs   (    4.06 ms per token,   246.61 tokens\
          \ per second)\nllama_print_timings: prompt eval time =       0.00 ms / \
          \    1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:\
          \        eval time =  187318.09 ms /   498 runs   (  376.14 ms per token,\
          \     2.66 tokens per second)\nllama_print_timings:       total time = \
          \ 198721.88 ms\n&gt;&gt;&gt; print(output)\n{'id': 'cmpl-8eb1eb96-86eb-43a8-8049-34ed37504371',\
          \ 'object': 'text_completion', 'created': 1704342181, 'model': 'PhoGPT-7B5-Instruct-q4_0.gguf',\
          \ 'choices': [{'text': 'L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian? H\xE3y th\u1EED nh\u1EEF\
          ng c\xE1ch sau \u0111\xE2y!\\n1. L\u1EADp k\u1EBF ho\u1EA1ch h\xE0ng ng\xE0\
          y: H\xE3y x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEAu c\u1EE5 th\u1EC3\
          \ v\xE0 thi\u1EBFt l\u1EADp m\u1ED9t l\u1ECBch tr\xECnh chi ti\u1EBFt cho\
          \ t\u1EEBng ng\xE0y trong tu\u1EA7n. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1\
          n t\u1ED5 ch\u1EE9c c\xF4ng vi\u1EC7c hi\u1EC7u qu\u1EA3 h\u01A1n v\xE0\
          \ tr\xE1nh b\u1ECB \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\
          \u1EDDng s\u1EF1 t\u1EF1 gi\xE1c.\\n\\n2. T\u1EADp trung v\xE0o nhi\u1EC7\
          m v\u1EE5 hi\u1EC7n t\u1EA1i: Thay v\xEC lo l\u1EAFng v\u1EC1 nh\u1EEFng\
          \ vi\u1EC7c ch\u01B0a ho\xE0n th\xE0nh, h\xE3y t\u1EADp trung v\xE0o c\xE1\
          c nhi\u1EC7m v\u1EE5 \u0111ang th\u1EF1c hi\u1EC7n ngay b\xE2y gi\u1EDD\
          . \u0110i\u1EC1u n\xE0y s\u1EBD gi\xFAp b\u1EA1n gi\u1EA3m thi\u1EC3u stress\
          \ do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDDng s\u1EF1\
          \ ch\u1EE7 \u0111\u1ED9ng v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\n3. S\u1EED d\u1EE5\
          ng c\xF4ng c\u1EE5 h\u1ED7 tr\u1EE3 qu\u1EA3n l\xFD th\u1EDDi gian: C\xE1\
          c c\xF4ng c\u1EE5 nh\u01B0 l\u1ECBch h\u1EB9n gi\u1EDD, danh s\xE1ch c\xF4\
          ng vi\u1EC7c ho\u1EB7c \u1EE9ng d\u1EE5ng di \u0111\u1ED9ng c\xF3 th\u1EC3\
          \ gi\xFAp b\u1EA1n theo d\xF5i v\xE0 qu\u1EA3n l\xFD th\u1EDDi gian hi\u1EC7\
          u qu\u1EA3 h\u01A1n. Ch\xFAng cung c\u1EA5p cho b\u1EA1n c\xE1c t\xEDnh\
          \ n\u0103ng t\u1EF1 \u0111\u1ED9ng, gi\xFAp b\u1EA1n \u0111\u1EB7t ra m\u1EE5\
          c ti\xEAu h\xE0ng ng\xE0y v\xE0 thi\u1EBFt l\u1EADp nh\u1EAFc nh\u1EDF cho\
          \ c\xE1c ho\u1EA1t \u0111\u1ED9ng quan tr\u1ECDng.\\n\\n4. T\u1EADn d\u1EE5\
          ng kho\u1EA3nh kh\u1EAFc r\u1EA3nh r\u1ED7i: Thay v\xEC l\xE3ng ph\xED th\u1EDD\
          i gian trong nh\u1EEFng c\xF4ng vi\u1EC7c t\u0129nh l\u1EB7ng, h\xE3y s\u1EED\
          \ d\u1EE5ng ch\xFAng \u0111\u1EC3 t\u1EA1o ra gi\xE1 tr\u1ECB cho cu\u1ED9\
          c s\u1ED1ng c\u1EE7a b\u1EA1n. H\xE3y t\xECm ki\u1EBFm c\u01A1 h\u1ED9i\
          \ \u0111\u1EC3 gi\u1EA3i quy\u1EBFt v\u1EA5n \u0111\u1EC1 nh\u1ECF, t\u1EAD\
          p trung v\xE0o \u0111i\u1EC1u quan tr\u1ECDng v\xE0 th\u01B0 gi\xE3n khi\
          \ r\u1EA3nh r\u1ED7i.\\n\\n5. H\u1ECDc c\xE1ch \u01B0u ti\xEAn c\xF4ng vi\u1EC7\
          c: H\xE3y h\u1ECDc c\xE1ch x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEA\
          u quan tr\u1ECDng v\xE0 s\u1EAFp x\u1EBFp c\xF4ng vi\u1EC7c theo m\u1EE9\
          c \u0111\u1ED9 \u01B0u ti\xEAn. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n gi\u1EA3\
          m thi\u1EC3u stress do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng\
          \ c\u01B0\u1EDDng s\u1EF1 t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\\
          n6. K\u1EBFt h\u1EE3p c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n: Thay v\xEC c\u1ED1\
          \ g\u1EAFng l\xE0m m\u1ECDi th\u1EE9 c\xF9ng m\u1ED9t l\xFAc, h\xE3y th\u1EED\
          \ k\u1EBFt h\u1EE3p c\xE1c ho\u1EA1t \u0111\u1ED9ng qu\u1EA3n l\xFD th\u1EDD\
          i gian nh\u01B0 \u0111an xen gi\u1EEFa c\xF4ng vi\u1EC7c kh\xF3 kh\u0103\
          n v\xE0 d\u1EC5 d\xE0ng \u0111\u1EC3 \u0111\u1EA1t hi\u1EC7u qu\u1EA3 cao\
          \ h\u01A1n.\\n\\n7. H\u1ECDc c\xE1ch \u0111\xE1nh gi\xE1 l\u1EA1i \u01B0\
          u ti\xEAn: H\xE3y h\u1ECDc c\xE1ch \u0111\xE1nh gi\xE1 l\u1EA1i \u01B0u\
          \ ti\xEAn c\u1EE7a b\u1EA1n sau m\u1ED9t kho\u1EA3ng th\u1EDDi gian nh\u1EA5\
          t \u0111\u1ECBnh. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n t\u1EADp trung\
          \ v\xE0o nh\u1EEFng nhi\u1EC7m v\u1EE5 quan tr\u1ECDng nh\u1EA5t v\xE0 tr\xE1\
          nh b\u1ECB \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDD\
          ng s\u1EF1 t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m. \\n\\n8. T\xECm ki\u1EBF\
          m ngu\u1ED3n \u0111\u1ED9ng l\u1EF1c b\xEAn ngo\xE0i: M\u1ED9t ngu\u1ED3\
          n \u0111\u1ED9ng l\u1EF1c m\u1EA1nh m\u1EBD c\xF3 th\u1EC3 gi\xFAp b\u1EA1\
          n qu\u1EA3n l\xFD th\u1EDDi gian hi\u1EC7u qu\u1EA3 h\u01A1n. H\xE3y t\xEC\
          m ki\u1EBFm c\xE1c ch\u01B0\u01A1ng tr\xECnh \u0111\xE0o t\u1EA1o ho\u1EB7\
          c c\u1ED9ng \u0111\u1ED3ng tr\u1EF1c tuy\u1EBFn n\u01A1i b\u1EA1n c\xF3\
          \ th\u1EC3 h\u1ECDc h\u1ECFi v\xE0 chia s\u1EBB kinh nghi\u1EC7m v\u1EDB\
          i nh\u1EEFng ng\u01B0\u1EDDi kh\xE1c.\\n\\n9. T\u1EADp luy\u1EC7n kh\u1EA3\
          \ n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian: \u0110\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian, h\xE3y t\u1EADp luy\u1EC7\
          n kh\u1EA3 n\u0103ng', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}],\
          \ 'usage': {'prompt_tokens': 14, 'completion_tokens': 498, 'total_tokens':\
          \ 512}}\n</code></pre>\n"
        raw: "Tested on WSL on Windows 10:\n\n```\npython3.10 -m venv venv\nsource\
          \ venv/bin/activate\ngit clone https://github.com/abetlen/llama-cpp-python/\n\
          cd llama-cpp-python\ngit checkout e9bc4c4baf3f121a178dec215770ccd0ac86c28e\n\
          cd vendor\ngit clone https://github.com/nviet/llama.cpp\ncd ..\npip install\
          \ .\n```\n\nThe output:\n\n```\n>>> from llama_cpp import Llama\n>>> llm\
          \ = Llama(model_path=\"PhoGPT-7B5-Instruct-q4_0.gguf\", verbose=True)\n\
          >>> output = llm(\"L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\", max_tokens=1024,\
          \ echo=True)\nLlama.generate: prefix-match hit\n\nllama_print_timings: \
          \       load time =    2342.78 ms\nllama_print_timings:      sample time\
          \ =    2019.40 ms /   498 runs   (    4.06 ms per token,   246.61 tokens\
          \ per second)\nllama_print_timings: prompt eval time =       0.00 ms / \
          \    1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:\
          \        eval time =  187318.09 ms /   498 runs   (  376.14 ms per token,\
          \     2.66 tokens per second)\nllama_print_timings:       total time = \
          \ 198721.88 ms\n>>> print(output)\n{'id': 'cmpl-8eb1eb96-86eb-43a8-8049-34ed37504371',\
          \ 'object': 'text_completion', 'created': 1704342181, 'model': 'PhoGPT-7B5-Instruct-q4_0.gguf',\
          \ 'choices': [{'text': 'L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian? H\xE3y th\u1EED nh\u1EEF\
          ng c\xE1ch sau \u0111\xE2y!\\n1. L\u1EADp k\u1EBF ho\u1EA1ch h\xE0ng ng\xE0\
          y: H\xE3y x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEAu c\u1EE5 th\u1EC3\
          \ v\xE0 thi\u1EBFt l\u1EADp m\u1ED9t l\u1ECBch tr\xECnh chi ti\u1EBFt cho\
          \ t\u1EEBng ng\xE0y trong tu\u1EA7n. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1\
          n t\u1ED5 ch\u1EE9c c\xF4ng vi\u1EC7c hi\u1EC7u qu\u1EA3 h\u01A1n v\xE0\
          \ tr\xE1nh b\u1ECB \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\
          \u1EDDng s\u1EF1 t\u1EF1 gi\xE1c.\\n\\n2. T\u1EADp trung v\xE0o nhi\u1EC7\
          m v\u1EE5 hi\u1EC7n t\u1EA1i: Thay v\xEC lo l\u1EAFng v\u1EC1 nh\u1EEFng\
          \ vi\u1EC7c ch\u01B0a ho\xE0n th\xE0nh, h\xE3y t\u1EADp trung v\xE0o c\xE1\
          c nhi\u1EC7m v\u1EE5 \u0111ang th\u1EF1c hi\u1EC7n ngay b\xE2y gi\u1EDD\
          . \u0110i\u1EC1u n\xE0y s\u1EBD gi\xFAp b\u1EA1n gi\u1EA3m thi\u1EC3u stress\
          \ do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDDng s\u1EF1\
          \ ch\u1EE7 \u0111\u1ED9ng v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\n3. S\u1EED d\u1EE5\
          ng c\xF4ng c\u1EE5 h\u1ED7 tr\u1EE3 qu\u1EA3n l\xFD th\u1EDDi gian: C\xE1\
          c c\xF4ng c\u1EE5 nh\u01B0 l\u1ECBch h\u1EB9n gi\u1EDD, danh s\xE1ch c\xF4\
          ng vi\u1EC7c ho\u1EB7c \u1EE9ng d\u1EE5ng di \u0111\u1ED9ng c\xF3 th\u1EC3\
          \ gi\xFAp b\u1EA1n theo d\xF5i v\xE0 qu\u1EA3n l\xFD th\u1EDDi gian hi\u1EC7\
          u qu\u1EA3 h\u01A1n. Ch\xFAng cung c\u1EA5p cho b\u1EA1n c\xE1c t\xEDnh\
          \ n\u0103ng t\u1EF1 \u0111\u1ED9ng, gi\xFAp b\u1EA1n \u0111\u1EB7t ra m\u1EE5\
          c ti\xEAu h\xE0ng ng\xE0y v\xE0 thi\u1EBFt l\u1EADp nh\u1EAFc nh\u1EDF cho\
          \ c\xE1c ho\u1EA1t \u0111\u1ED9ng quan tr\u1ECDng.\\n\\n4. T\u1EADn d\u1EE5\
          ng kho\u1EA3nh kh\u1EAFc r\u1EA3nh r\u1ED7i: Thay v\xEC l\xE3ng ph\xED th\u1EDD\
          i gian trong nh\u1EEFng c\xF4ng vi\u1EC7c t\u0129nh l\u1EB7ng, h\xE3y s\u1EED\
          \ d\u1EE5ng ch\xFAng \u0111\u1EC3 t\u1EA1o ra gi\xE1 tr\u1ECB cho cu\u1ED9\
          c s\u1ED1ng c\u1EE7a b\u1EA1n. H\xE3y t\xECm ki\u1EBFm c\u01A1 h\u1ED9i\
          \ \u0111\u1EC3 gi\u1EA3i quy\u1EBFt v\u1EA5n \u0111\u1EC1 nh\u1ECF, t\u1EAD\
          p trung v\xE0o \u0111i\u1EC1u quan tr\u1ECDng v\xE0 th\u01B0 gi\xE3n khi\
          \ r\u1EA3nh r\u1ED7i.\\n\\n5. H\u1ECDc c\xE1ch \u01B0u ti\xEAn c\xF4ng vi\u1EC7\
          c: H\xE3y h\u1ECDc c\xE1ch x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEA\
          u quan tr\u1ECDng v\xE0 s\u1EAFp x\u1EBFp c\xF4ng vi\u1EC7c theo m\u1EE9\
          c \u0111\u1ED9 \u01B0u ti\xEAn. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n gi\u1EA3\
          m thi\u1EC3u stress do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng\
          \ c\u01B0\u1EDDng s\u1EF1 t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\\
          n6. K\u1EBFt h\u1EE3p c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n: Thay v\xEC c\u1ED1\
          \ g\u1EAFng l\xE0m m\u1ECDi th\u1EE9 c\xF9ng m\u1ED9t l\xFAc, h\xE3y th\u1EED\
          \ k\u1EBFt h\u1EE3p c\xE1c ho\u1EA1t \u0111\u1ED9ng qu\u1EA3n l\xFD th\u1EDD\
          i gian nh\u01B0 \u0111an xen gi\u1EEFa c\xF4ng vi\u1EC7c kh\xF3 kh\u0103\
          n v\xE0 d\u1EC5 d\xE0ng \u0111\u1EC3 \u0111\u1EA1t hi\u1EC7u qu\u1EA3 cao\
          \ h\u01A1n.\\n\\n7. H\u1ECDc c\xE1ch \u0111\xE1nh gi\xE1 l\u1EA1i \u01B0\
          u ti\xEAn: H\xE3y h\u1ECDc c\xE1ch \u0111\xE1nh gi\xE1 l\u1EA1i \u01B0u\
          \ ti\xEAn c\u1EE7a b\u1EA1n sau m\u1ED9t kho\u1EA3ng th\u1EDDi gian nh\u1EA5\
          t \u0111\u1ECBnh. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n t\u1EADp trung\
          \ v\xE0o nh\u1EEFng nhi\u1EC7m v\u1EE5 quan tr\u1ECDng nh\u1EA5t v\xE0 tr\xE1\
          nh b\u1ECB \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDD\
          ng s\u1EF1 t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m. \\n\\n8. T\xECm ki\u1EBF\
          m ngu\u1ED3n \u0111\u1ED9ng l\u1EF1c b\xEAn ngo\xE0i: M\u1ED9t ngu\u1ED3\
          n \u0111\u1ED9ng l\u1EF1c m\u1EA1nh m\u1EBD c\xF3 th\u1EC3 gi\xFAp b\u1EA1\
          n qu\u1EA3n l\xFD th\u1EDDi gian hi\u1EC7u qu\u1EA3 h\u01A1n. H\xE3y t\xEC\
          m ki\u1EBFm c\xE1c ch\u01B0\u01A1ng tr\xECnh \u0111\xE0o t\u1EA1o ho\u1EB7\
          c c\u1ED9ng \u0111\u1ED3ng tr\u1EF1c tuy\u1EBFn n\u01A1i b\u1EA1n c\xF3\
          \ th\u1EC3 h\u1ECDc h\u1ECFi v\xE0 chia s\u1EBB kinh nghi\u1EC7m v\u1EDB\
          i nh\u1EEFng ng\u01B0\u1EDDi kh\xE1c.\\n\\n9. T\u1EADp luy\u1EC7n kh\u1EA3\
          \ n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian: \u0110\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian, h\xE3y t\u1EADp luy\u1EC7\
          n kh\u1EA3 n\u0103ng', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}],\
          \ 'usage': {'prompt_tokens': 14, 'completion_tokens': 498, 'total_tokens':\
          \ 512}}\n```"
        updatedAt: '2024-01-04T04:32:04.467Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nobita3921
    id: 659634c4379d7baa5af2f17b
    type: comment
  author: nguyenviet
  content: "Tested on WSL on Windows 10:\n\n```\npython3.10 -m venv venv\nsource venv/bin/activate\n\
    git clone https://github.com/abetlen/llama-cpp-python/\ncd llama-cpp-python\n\
    git checkout e9bc4c4baf3f121a178dec215770ccd0ac86c28e\ncd vendor\ngit clone https://github.com/nviet/llama.cpp\n\
    cd ..\npip install .\n```\n\nThe output:\n\n```\n>>> from llama_cpp import Llama\n\
    >>> llm = Llama(model_path=\"PhoGPT-7B5-Instruct-q4_0.gguf\", verbose=True)\n\
    >>> output = llm(\"L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7n k\u1EF9\
    \ n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\", max_tokens=1024, echo=True)\nLlama.generate:\
    \ prefix-match hit\n\nllama_print_timings:        load time =    2342.78 ms\n\
    llama_print_timings:      sample time =    2019.40 ms /   498 runs   (    4.06\
    \ ms per token,   246.61 tokens per second)\nllama_print_timings: prompt eval\
    \ time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens\
    \ per second)\nllama_print_timings:        eval time =  187318.09 ms /   498 runs\
    \   (  376.14 ms per token,     2.66 tokens per second)\nllama_print_timings:\
    \       total time =  198721.88 ms\n>>> print(output)\n{'id': 'cmpl-8eb1eb96-86eb-43a8-8049-34ed37504371',\
    \ 'object': 'text_completion', 'created': 1704342181, 'model': 'PhoGPT-7B5-Instruct-q4_0.gguf',\
    \ 'choices': [{'text': 'L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
    n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian? H\xE3y th\u1EED nh\u1EEFng\
    \ c\xE1ch sau \u0111\xE2y!\\n1. L\u1EADp k\u1EBF ho\u1EA1ch h\xE0ng ng\xE0y: H\xE3\
    y x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEAu c\u1EE5 th\u1EC3 v\xE0 thi\u1EBF\
    t l\u1EADp m\u1ED9t l\u1ECBch tr\xECnh chi ti\u1EBFt cho t\u1EEBng ng\xE0y trong\
    \ tu\u1EA7n. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n t\u1ED5 ch\u1EE9c c\xF4ng\
    \ vi\u1EC7c hi\u1EC7u qu\u1EA3 h\u01A1n v\xE0 tr\xE1nh b\u1ECB \xE1p l\u1EF1c\
    \ \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDDng s\u1EF1 t\u1EF1 gi\xE1c.\\\
    n\\n2. T\u1EADp trung v\xE0o nhi\u1EC7m v\u1EE5 hi\u1EC7n t\u1EA1i: Thay v\xEC\
    \ lo l\u1EAFng v\u1EC1 nh\u1EEFng vi\u1EC7c ch\u01B0a ho\xE0n th\xE0nh, h\xE3\
    y t\u1EADp trung v\xE0o c\xE1c nhi\u1EC7m v\u1EE5 \u0111ang th\u1EF1c hi\u1EC7\
    n ngay b\xE2y gi\u1EDD. \u0110i\u1EC1u n\xE0y s\u1EBD gi\xFAp b\u1EA1n gi\u1EA3\
    m thi\u1EC3u stress do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\
    \u1EDDng s\u1EF1 ch\u1EE7 \u0111\u1ED9ng v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\n3. S\u1EED\
    \ d\u1EE5ng c\xF4ng c\u1EE5 h\u1ED7 tr\u1EE3 qu\u1EA3n l\xFD th\u1EDDi gian: C\xE1\
    c c\xF4ng c\u1EE5 nh\u01B0 l\u1ECBch h\u1EB9n gi\u1EDD, danh s\xE1ch c\xF4ng vi\u1EC7\
    c ho\u1EB7c \u1EE9ng d\u1EE5ng di \u0111\u1ED9ng c\xF3 th\u1EC3 gi\xFAp b\u1EA1\
    n theo d\xF5i v\xE0 qu\u1EA3n l\xFD th\u1EDDi gian hi\u1EC7u qu\u1EA3 h\u01A1\
    n. Ch\xFAng cung c\u1EA5p cho b\u1EA1n c\xE1c t\xEDnh n\u0103ng t\u1EF1 \u0111\
    \u1ED9ng, gi\xFAp b\u1EA1n \u0111\u1EB7t ra m\u1EE5c ti\xEAu h\xE0ng ng\xE0y v\xE0\
    \ thi\u1EBFt l\u1EADp nh\u1EAFc nh\u1EDF cho c\xE1c ho\u1EA1t \u0111\u1ED9ng quan\
    \ tr\u1ECDng.\\n\\n4. T\u1EADn d\u1EE5ng kho\u1EA3nh kh\u1EAFc r\u1EA3nh r\u1ED7\
    i: Thay v\xEC l\xE3ng ph\xED th\u1EDDi gian trong nh\u1EEFng c\xF4ng vi\u1EC7\
    c t\u0129nh l\u1EB7ng, h\xE3y s\u1EED d\u1EE5ng ch\xFAng \u0111\u1EC3 t\u1EA1\
    o ra gi\xE1 tr\u1ECB cho cu\u1ED9c s\u1ED1ng c\u1EE7a b\u1EA1n. H\xE3y t\xECm\
    \ ki\u1EBFm c\u01A1 h\u1ED9i \u0111\u1EC3 gi\u1EA3i quy\u1EBFt v\u1EA5n \u0111\
    \u1EC1 nh\u1ECF, t\u1EADp trung v\xE0o \u0111i\u1EC1u quan tr\u1ECDng v\xE0 th\u01B0\
    \ gi\xE3n khi r\u1EA3nh r\u1ED7i.\\n\\n5. H\u1ECDc c\xE1ch \u01B0u ti\xEAn c\xF4\
    ng vi\u1EC7c: H\xE3y h\u1ECDc c\xE1ch x\xE1c \u0111\u1ECBnh c\xE1c m\u1EE5c ti\xEA\
    u quan tr\u1ECDng v\xE0 s\u1EAFp x\u1EBFp c\xF4ng vi\u1EC7c theo m\u1EE9c \u0111\
    \u1ED9 \u01B0u ti\xEAn. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n gi\u1EA3m thi\u1EC3\
    u stress do \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDDng\
    \ s\u1EF1 t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m.\\n\\n6. K\u1EBFt h\u1EE3\
    p c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n: Thay v\xEC c\u1ED1 g\u1EAFng l\xE0m m\u1ECD\
    i th\u1EE9 c\xF9ng m\u1ED9t l\xFAc, h\xE3y th\u1EED k\u1EBFt h\u1EE3p c\xE1c ho\u1EA1\
    t \u0111\u1ED9ng qu\u1EA3n l\xFD th\u1EDDi gian nh\u01B0 \u0111an xen gi\u1EEF\
    a c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n v\xE0 d\u1EC5 d\xE0ng \u0111\u1EC3 \u0111\
    \u1EA1t hi\u1EC7u qu\u1EA3 cao h\u01A1n.\\n\\n7. H\u1ECDc c\xE1ch \u0111\xE1nh\
    \ gi\xE1 l\u1EA1i \u01B0u ti\xEAn: H\xE3y h\u1ECDc c\xE1ch \u0111\xE1nh gi\xE1\
    \ l\u1EA1i \u01B0u ti\xEAn c\u1EE7a b\u1EA1n sau m\u1ED9t kho\u1EA3ng th\u1EDD\
    i gian nh\u1EA5t \u0111\u1ECBnh. \u0110i\u1EC1u n\xE0y gi\xFAp b\u1EA1n t\u1EAD\
    p trung v\xE0o nh\u1EEFng nhi\u1EC7m v\u1EE5 quan tr\u1ECDng nh\u1EA5t v\xE0 tr\xE1\
    nh b\u1ECB \xE1p l\u1EF1c \u0111\u1ED3ng th\u1EDDi t\u0103ng c\u01B0\u1EDDng s\u1EF1\
    \ t\u1EF1 gi\xE1c v\xE0 tr\xE1ch nhi\u1EC7m. \\n\\n8. T\xECm ki\u1EBFm ngu\u1ED3\
    n \u0111\u1ED9ng l\u1EF1c b\xEAn ngo\xE0i: M\u1ED9t ngu\u1ED3n \u0111\u1ED9ng\
    \ l\u1EF1c m\u1EA1nh m\u1EBD c\xF3 th\u1EC3 gi\xFAp b\u1EA1n qu\u1EA3n l\xFD th\u1EDD\
    i gian hi\u1EC7u qu\u1EA3 h\u01A1n. H\xE3y t\xECm ki\u1EBFm c\xE1c ch\u01B0\u01A1\
    ng tr\xECnh \u0111\xE0o t\u1EA1o ho\u1EB7c c\u1ED9ng \u0111\u1ED3ng tr\u1EF1c\
    \ tuy\u1EBFn n\u01A1i b\u1EA1n c\xF3 th\u1EC3 h\u1ECDc h\u1ECFi v\xE0 chia s\u1EBB\
    \ kinh nghi\u1EC7m v\u1EDBi nh\u1EEFng ng\u01B0\u1EDDi kh\xE1c.\\n\\n9. T\u1EAD\
    p luy\u1EC7n kh\u1EA3 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian: \u0110\u1EC3 c\u1EA3\
    i thi\u1EC7n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian, h\xE3y t\u1EAD\
    p luy\u1EC7n kh\u1EA3 n\u0103ng', 'index': 0, 'logprobs': None, 'finish_reason':\
    \ 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 498, 'total_tokens':\
    \ 512}}\n```"
  created_at: 2024-01-04 04:32:04+00:00
  edited: false
  hidden: false
  id: 659634c4379d7baa5af2f17b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
      fullname: Sane
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nobita3921
      type: user
    createdAt: '2024-01-04T08:10:10.000Z'
    data:
      edited: false
      editors:
      - nobita3921
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9397895932197571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba801ba11aa12e6b4535af707f6983b8.svg
          fullname: Sane
          isHf: false
          isPro: false
          name: nobita3921
          type: user
        html: '<p>I have tried two ways.<br>first, I followed your steps above and
          it has worked.<br>second, I pulled the last commit of <strong>llama-cpp-python</strong>
          and did not check out to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e  but
          It didn''t work.<br>So, it needs to check out to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e.<br>Thank
          u so much.</p>

          '
        raw: 'I have tried two ways.

          first, I followed your steps above and it has worked.

          second, I pulled the last commit of __llama-cpp-python__ and did not check
          out to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e  but It didn''t work.

          So, it needs to check out to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e.

          Thank u so much.'
        updatedAt: '2024-01-04T08:10:10.749Z'
      numEdits: 0
      reactions: []
    id: 659667e2576d6c613c62a0da
    type: comment
  author: nobita3921
  content: 'I have tried two ways.

    first, I followed your steps above and it has worked.

    second, I pulled the last commit of __llama-cpp-python__ and did not check out
    to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e  but It didn''t work.

    So, it needs to check out to commit e9bc4c4baf3f121a178dec215770ccd0ac86c28e.

    Thank u so much.'
  created_at: 2024-01-04 08:10:10+00:00
  edited: false
  hidden: false
  id: 659667e2576d6c613c62a0da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
      fullname: Nguyen Viet
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nguyenviet
      type: user
    createdAt: '2024-01-04T09:13:16.000Z'
    data:
      edited: false
      editors:
      - nguyenviet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602172374725342
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
          fullname: Nguyen Viet
          isHf: false
          isPro: false
          name: nguyenviet
          type: user
        html: '<p>Yes, because my fork of <em>llama.cpp</em> was made 1 month ago
          so it''s required to check out an older commit of <em>llama-cpp-python</em>
          for it to work. If you want to use the latest commit of <em>llama-cpp-python</em>,
          make change to <em>llama.cpp</em> in the same style that I did and it should
          be fine.</p>

          <p>Since the code is working for you now, I''m closing this issue.</p>

          '
        raw: 'Yes, because my fork of *llama.cpp* was made 1 month ago so it''s required
          to check out an older commit of *llama-cpp-python* for it to work. If you
          want to use the latest commit of *llama-cpp-python*, make change to *llama.cpp*
          in the same style that I did and it should be fine.


          Since the code is working for you now, I''m closing this issue.'
        updatedAt: '2024-01-04T09:13:16.751Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - nobita3921
      relatedEventId: 659676ac2460ad036fabfad0
    id: 659676ac2460ad036fabfacc
    type: comment
  author: nguyenviet
  content: 'Yes, because my fork of *llama.cpp* was made 1 month ago so it''s required
    to check out an older commit of *llama-cpp-python* for it to work. If you want
    to use the latest commit of *llama-cpp-python*, make change to *llama.cpp* in
    the same style that I did and it should be fine.


    Since the code is working for you now, I''m closing this issue.'
  created_at: 2024-01-04 09:13:16+00:00
  edited: false
  hidden: false
  id: 659676ac2460ad036fabfacc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/545a2a2629a69933cd59cc28576017a3.svg
      fullname: Nguyen Viet
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nguyenviet
      type: user
    createdAt: '2024-01-04T09:13:16.000Z'
    data:
      status: closed
    id: 659676ac2460ad036fabfad0
    type: status-change
  author: nguyenviet
  created_at: 2024-01-04 09:13:16+00:00
  id: 659676ac2460ad036fabfad0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nguyenviet/PhoGPT-7B5-Instruct-GGUF
repo_type: model
status: closed
target_branch: null
title: Cannot load PhoGPT-7B5-Instruct-GGUF using ctransformer
