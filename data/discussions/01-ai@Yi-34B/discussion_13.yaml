!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cailuyu
conflicting_files: null
created_at: 2023-11-07 06:43:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70870de4b74a55a9a1b976abd30bc917.svg
      fullname: cailuyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cailuyu
      type: user
    createdAt: '2023-11-07T06:43:12.000Z'
    data:
      edited: false
      editors:
      - cailuyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4498822093009949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70870de4b74a55a9a1b976abd30bc917.svg
          fullname: cailuyu
          isHf: false
          isPro: false
          name: cailuyu
          type: user
        html: '<p>Only 20% GPU (cuda 114)utilized with the following sample code:<br><code><br>from
          transformers import AutoModelForCausalLM, AutoTokenizer</code></p><code>

          <p>model = AutoModelForCausalLM.from_pretrained("01-ai/Yi-34B", device_map="auto",
          torch_dtype="auto", trust_remote_code=True)<br>tokenizer = AutoTokenizer.from_pretrained("01-ai/Yi-34B",
          trust_remote_code=True)<br>inputs = tokenizer("There''s a place where time
          stands still. A place of breath taking wonder, but also", return_tensors="pt")<br>max_length
          = 256  </p>

          </code><p><code>outputs = model.generate(<br>    inputs.input_ids.cuda(),<br>    max_length=max_length,<br>    eos_token_id=tokenizer.eos_token_id<br>)<br>print(tokenizer.decode(outputs[0],
          skip_special_tokens=True))<br></code></p>

          '
        raw: "Only 20% GPU (cuda 114)utilized with the following sample code:\r\n\
          <code>\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34B\", device_map=\"\
          auto\", torch_dtype=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          01-ai/Yi-34B\", trust_remote_code=True)\r\ninputs = tokenizer(\"There's\
          \ a place where time stands still. A place of breath taking wonder, but\
          \ also\", return_tensors=\"pt\")\r\nmax_length = 256  \r\n\r\noutputs =\
          \ model.generate(\r\n    inputs.input_ids.cuda(),\r\n    max_length=max_length,\r\
          \n    eos_token_id=tokenizer.eos_token_id \r\n)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\r\n</code>"
        updatedAt: '2023-11-07T06:43:12.664Z'
      numEdits: 0
      reactions: []
    id: 6549dc80137b7246f0454610
    type: comment
  author: cailuyu
  content: "Only 20% GPU (cuda 114)utilized with the following sample code:\r\n<code>\r\
    \nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel =\
    \ AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34B\", device_map=\"auto\",\
    \ torch_dtype=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    01-ai/Yi-34B\", trust_remote_code=True)\r\ninputs = tokenizer(\"There's a place\
    \ where time stands still. A place of breath taking wonder, but also\", return_tensors=\"\
    pt\")\r\nmax_length = 256  \r\n\r\noutputs = model.generate(\r\n    inputs.input_ids.cuda(),\r\
    \n    max_length=max_length,\r\n    eos_token_id=tokenizer.eos_token_id \r\n)\r\
    \nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\r\n</code>"
  created_at: 2023-11-07 06:43:12+00:00
  edited: false
  hidden: false
  id: 6549dc80137b7246f0454610
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64194e784939d72681882ead/JpvtmR0T1MYCNb-ugY_rM.jpeg?w=200&h=200&f=face
      fullname: Chao Li
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: reedcli
      type: user
    createdAt: '2023-11-07T08:10:05.000Z'
    data:
      edited: false
      editors:
      - reedcli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797440767288208
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64194e784939d72681882ead/JpvtmR0T1MYCNb-ugY_rM.jpeg?w=200&h=200&f=face
          fullname: Chao Li
          isHf: false
          isPro: false
          name: reedcli
          type: user
        html: '<p>Sorry, but we can''t do much about that...<br>If you want to get
          high GPU utilization, maybe you can infer with bigger batchsize?</p>

          '
        raw: 'Sorry, but we can''t do much about that...

          If you want to get high GPU utilization, maybe you can infer with bigger
          batchsize?'
        updatedAt: '2023-11-07T08:10:05.926Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - c-w
    id: 6549f0ddb8d6b1e8631f4576
    type: comment
  author: reedcli
  content: 'Sorry, but we can''t do much about that...

    If you want to get high GPU utilization, maybe you can infer with bigger batchsize?'
  created_at: 2023-11-07 08:10:05+00:00
  edited: false
  hidden: false
  id: 6549f0ddb8d6b1e8631f4576
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30e247417b27d659708d0757ab590ed4.svg
      fullname: LI MOU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lxglbk
      type: user
    createdAt: '2023-11-08T10:23:17.000Z'
    data:
      edited: false
      editors:
      - lxglbk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8509263396263123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30e247417b27d659708d0757ab590ed4.svg
          fullname: LI MOU
          isHf: false
          isPro: false
          name: lxglbk
          type: user
        html: '<p>This is just a sample code for inference, consider to switch to
          another inference engine(like vllm) if you need to run the model efficiently~</p>

          '
        raw: This is just a sample code for inference, consider to switch to another
          inference engine(like vllm) if you need to run the model efficiently~
        updatedAt: '2023-11-08T10:23:17.105Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - c-w
    id: 654b61955e16a34d9a8f48c2
    type: comment
  author: lxglbk
  content: This is just a sample code for inference, consider to switch to another
    inference engine(like vllm) if you need to run the model efficiently~
  created_at: 2023-11-08 10:23:17+00:00
  edited: false
  hidden: false
  id: 654b61955e16a34d9a8f48c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-14T13:52:38.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9099734425544739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;reedcli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/reedcli\">@<span class=\"\
          underline\">reedcli</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;lxglbk&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lxglbk\">@<span class=\"underline\">lxglbk</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;cailuyu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cailuyu\"\
          >@<span class=\"underline\">cailuyu</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I can confirm exllamav2 has reached around 90% GPU utilities\
          \ with flash-attention2 cuda 118 torch2.0.1 on A5000.</p>\n<p>The HF transformer\
          \ always poor for almost any model due to not specialized for cuda</p>\n\
          <p>Are you guys partnered with exllamav2?</p>\n"
        raw: "@reedcli @lxglbk @cailuyu \n\nI can confirm exllamav2 has reached around\
          \ 90% GPU utilities with flash-attention2 cuda 118 torch2.0.1 on A5000.\n\
          \nThe HF transformer always poor for almost any model due to not specialized\
          \ for cuda\n\nAre you guys partnered with exllamav2?"
        updatedAt: '2023-11-14T13:53:20.383Z'
      numEdits: 1
      reactions: []
    id: 65537ba64a24e2d1193fa102
    type: comment
  author: Yhyu13
  content: "@reedcli @lxglbk @cailuyu \n\nI can confirm exllamav2 has reached around\
    \ 90% GPU utilities with flash-attention2 cuda 118 torch2.0.1 on A5000.\n\nThe\
    \ HF transformer always poor for almost any model due to not specialized for cuda\n\
    \nAre you guys partnered with exllamav2?"
  created_at: 2023-11-14 13:52:38+00:00
  edited: true
  hidden: false
  id: 65537ba64a24e2d1193fa102
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-14T20:51:03.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9454377889633179
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> no lol, exllama\
          \ 2 is created by turboderp and other contributors also help as well. Exllamav2\
          \ is specifically designed for fastest single batch inference and it\u2019\
          s infact the fastest one probably.</p>\n<p>Vllm is better for batching but\
          \ most people are not gonna input tens or a hundred input prompts.</p>\n"
        raw: "@Yhyu13 no lol, exllama 2 is created by turboderp and other contributors\
          \ also help as well. Exllamav2 is specifically designed for fastest single\
          \ batch inference and it\u2019s infact the fastest one probably.\n\nVllm\
          \ is better for batching but most people are not gonna input tens or a hundred\
          \ input prompts."
        updatedAt: '2023-11-14T20:51:03.996Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 6553ddb7e149dd8e9f1c2096
    type: comment
  author: YaTharThShaRma999
  content: "@Yhyu13 no lol, exllama 2 is created by turboderp and other contributors\
    \ also help as well. Exllamav2 is specifically designed for fastest single batch\
    \ inference and it\u2019s infact the fastest one probably.\n\nVllm is better for\
    \ batching but most people are not gonna input tens or a hundred input prompts."
  created_at: 2023-11-14 20:51:03+00:00
  edited: false
  hidden: false
  id: 6553ddb7e149dd8e9f1c2096
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-16T03:40:54.000Z'
    data:
      status: closed
    id: 65558f4638985840b3689adb
    type: status-change
  author: FancyZhao
  created_at: 2023-11-16 03:40:54+00:00
  id: 65558f4638985840b3689adb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: 01-ai/Yi-34B
repo_type: model
status: closed
target_branch: null
title: GPU is not fully utilized
