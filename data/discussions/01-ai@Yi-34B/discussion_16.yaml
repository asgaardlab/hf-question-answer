!!python/object:huggingface_hub.community.DiscussionWithDetails
author: guocuimi
conflicting_files: null
created_at: 2023-11-08 16:46:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
      fullname: Michael Mi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guocuimi
      type: user
    createdAt: '2023-11-08T16:46:08.000Z'
    data:
      edited: true
      editors:
      - guocuimi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6314736008644104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
          fullname: Michael Mi
          isHf: false
          isPro: false
          name: guocuimi
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/vectorch-ai/ScaleLLM\"\
          >https://github.com/vectorch-ai/ScaleLLM</a></p>\n<p>ScaleLLM is a tool\
          \ that enables you to serve language models locally. You can find the project\
          \ and documentation here: <a rel=\"nofollow\" href=\"https://github.com/vectorch-ai/ScaleLLM\"\
          >ScaleLLM GitHub</a>. Here's how you can set it up:</p>\n<p>1: start the\
          \ model inference server<br>First, run the model inference server using\
          \ the following Docker command. This command will start a container with\
          \ GPU support (if available) and link it to your local model cache:</p>\n\
          <pre><code>docker run -it --gpus=all --net=host --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models\
          \ \\\n  -e HF_MODEL_ID=01-ai/Yi-34B \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest\
          \ --logtostderr\n</code></pre>\n<p>2: start REST API server<br>Next, start\
          \ the REST API server by running the following Docker command:</p>\n<pre><code>docker\
          \ run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n</code></pre>\n<p>you will get following running services:</p>\n\
          <p>ScaleLLM gRPC server on port 8888: localhost:8888<br>ScaleLLM HTTP server\
          \ for monitoring on port 9999: localhost:9999<br>ScaleLLM REST API server\
          \ on port 8080: localhost:8080</p>\n<p>You can now send requests to the\
          \ local REST API server to generate text completions using a command like\
          \ this:</p>\n<pre><code>curl http://localhost:8080/v1/completions   -H \"\
          Content-Type: application/json\"   -d '{\n    \"model\": \"01-ai/Yi-34B\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n</code></pre>\n<p>This command sends a POST request\
          \ to the local REST API server, specifying the model, prompt, and other\
          \ parameters to generate completions.</p>\n<p>Make sure you have Docker\
          \ installed and configured for GPU usage if you want to take advantage of\
          \ GPU acceleration. This setup allows you to efficiently run the language\
          \ model locally with ScaleLLM.</p>\n"
        raw: "https://github.com/vectorch-ai/ScaleLLM\n\nScaleLLM is a tool that enables\
          \ you to serve language models locally. You can find the project and documentation\
          \ here: [ScaleLLM GitHub](https://github.com/vectorch-ai/ScaleLLM). Here's\
          \ how you can set it up:\n\n1: start the model inference server\nFirst,\
          \ run the model inference server using the following Docker command. This\
          \ command will start a container with GPU support (if available) and link\
          \ it to your local model cache:\n```\ndocker run -it --gpus=all --net=host\
          \ --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models \\\n  -e HF_MODEL_ID=01-ai/Yi-34B\
          \ \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest --logtostderr\n\
          ```\n2: start REST API server\nNext, start the REST API server by running\
          \ the following Docker command:\n```\ndocker run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n```\nyou will get following running services:\n\nScaleLLM\
          \ gRPC server on port 8888: localhost:8888\nScaleLLM HTTP server for monitoring\
          \ on port 9999: localhost:9999\nScaleLLM REST API server on port 8080: localhost:8080\n\
          \nYou can now send requests to the local REST API server to generate text\
          \ completions using a command like this:\n```\ncurl http://localhost:8080/v1/completions\
          \   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"01-ai/Yi-34B\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n```\nThis command sends a POST request to the\
          \ local REST API server, specifying the model, prompt, and other parameters\
          \ to generate completions.\n\nMake sure you have Docker installed and configured\
          \ for GPU usage if you want to take advantage of GPU acceleration. This\
          \ setup allows you to efficiently run the language model locally with ScaleLLM."
        updatedAt: '2023-11-11T07:13:57.310Z'
      numEdits: 2
      reactions: []
    id: 654bbb5017d83697c73d14c5
    type: comment
  author: guocuimi
  content: "https://github.com/vectorch-ai/ScaleLLM\n\nScaleLLM is a tool that enables\
    \ you to serve language models locally. You can find the project and documentation\
    \ here: [ScaleLLM GitHub](https://github.com/vectorch-ai/ScaleLLM). Here's how\
    \ you can set it up:\n\n1: start the model inference server\nFirst, run the model\
    \ inference server using the following Docker command. This command will start\
    \ a container with GPU support (if available) and link it to your local model\
    \ cache:\n```\ndocker run -it --gpus=all --net=host --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models\
    \ \\\n  -e HF_MODEL_ID=01-ai/Yi-34B \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest\
    \ --logtostderr\n```\n2: start REST API server\nNext, start the REST API server\
    \ by running the following Docker command:\n```\ndocker run -it --net=host \\\n\
    \  docker.io/vectorchai/scalellm-gateway:latest --logtostderr\n```\nyou will get\
    \ following running services:\n\nScaleLLM gRPC server on port 8888: localhost:8888\n\
    ScaleLLM HTTP server for monitoring on port 9999: localhost:9999\nScaleLLM REST\
    \ API server on port 8080: localhost:8080\n\nYou can now send requests to the\
    \ local REST API server to generate text completions using a command like this:\n\
    ```\ncurl http://localhost:8080/v1/completions   -H \"Content-Type: application/json\"\
    \   -d '{\n    \"model\": \"01-ai/Yi-34B\",\n    \"prompt\": \"what is vue.js\"\
    ,\n    \"max_tokens\": 32,\n    \"temperature\": 0.7\n  }'\n```\nThis command\
    \ sends a POST request to the local REST API server, specifying the model, prompt,\
    \ and other parameters to generate completions.\n\nMake sure you have Docker installed\
    \ and configured for GPU usage if you want to take advantage of GPU acceleration.\
    \ This setup allows you to efficiently run the language model locally with ScaleLLM."
  created_at: 2023-11-08 16:46:08+00:00
  edited: true
  hidden: false
  id: 654bbb5017d83697c73d14c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
      fullname: Michael Mi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guocuimi
      type: user
    createdAt: '2023-11-08T16:46:30.000Z'
    data:
      from: Efficiently run the model locally using ScaleLLM (https://github.com/vectorch-ai/ScaleLLM)
      to: Efficiently run the model locally using ScaleLLM
    id: 654bbb66bac6e6e49877fb92
    type: title-change
  author: guocuimi
  created_at: 2023-11-08 16:46:30+00:00
  id: 654bbb66bac6e6e49877fb92
  new_title: Efficiently run the model locally using ScaleLLM
  old_title: Efficiently run the model locally using ScaleLLM (https://github.com/vectorch-ai/ScaleLLM)
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-16T03:39:56.000Z'
    data:
      edited: false
      editors:
      - FancyZhao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9529531598091125
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
          fullname: FancyZhao
          isHf: false
          isPro: false
          name: FancyZhao
          type: user
        html: '<p>I notice that you''ve added the support for the Yi series models
          in your project. That''s great!<br>I think what you posted above is better
          to be added in your project''s docs. So I''m closing this discussion now.<br>Note
          that we''ll add a <strong>Ecosystem</strong> section in our README soon
          (<a rel="nofollow" href="https://github.com/01-ai/Yi/issues/109">https://github.com/01-ai/Yi/issues/109</a>)
          And your project will be added there.</p>

          '
        raw: 'I notice that you''ve added the support for the Yi series models in
          your project. That''s great!

          I think what you posted above is better to be added in your project''s docs.
          So I''m closing this discussion now.

          Note that we''ll add a **Ecosystem** section in our README soon (https://github.com/01-ai/Yi/issues/109)
          And your project will be added there.'
        updatedAt: '2023-11-16T03:39:56.970Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65558f0d7446daf1ce2f4dc1
    id: 65558f0c7446daf1ce2f4dbf
    type: comment
  author: FancyZhao
  content: 'I notice that you''ve added the support for the Yi series models in your
    project. That''s great!

    I think what you posted above is better to be added in your project''s docs. So
    I''m closing this discussion now.

    Note that we''ll add a **Ecosystem** section in our README soon (https://github.com/01-ai/Yi/issues/109)
    And your project will be added there.'
  created_at: 2023-11-16 03:39:56+00:00
  edited: false
  hidden: false
  id: 65558f0c7446daf1ce2f4dbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-16T03:39:57.000Z'
    data:
      status: closed
    id: 65558f0d7446daf1ce2f4dc1
    type: status-change
  author: FancyZhao
  created_at: 2023-11-16 03:39:57+00:00
  id: 65558f0d7446daf1ce2f4dc1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: 01-ai/Yi-34B
repo_type: model
status: closed
target_branch: null
title: Efficiently run the model locally using ScaleLLM
