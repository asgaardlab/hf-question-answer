!!python/object:huggingface_hub.community.DiscussionWithDetails
author: angeligareta
conflicting_files: null
created_at: 2023-11-23 15:52:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
      fullname: Angel Igareta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: angeligareta
      type: user
    createdAt: '2023-11-23T15:52:36.000Z'
    data:
      edited: false
      editors:
      - angeligareta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5630509853363037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
          fullname: Angel Igareta
          isHf: false
          isPro: false
          name: angeligareta
          type: user
        html: '<p>I am trying to deploy the new quantized versions to Sagemaker to
          experiment with them. When I specify this configuration:</p>

          <p>config = {<br>    "HF_MODEL_ID": "01-ai/Yi-34B-Chat-4bits"<br>    ''SM_NUM_GPUS'':
          json.dumps(4),<br>    ''QUANTIZE'': ''awq'',<br>}</p>

          <p>I get the following error:</p>

          <blockquote>

          <p>File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 159, in serve_inner<br>    model = get_model(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 201, in get_model<br>    return FlashLlama(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py",
          line 68, in <strong>init</strong><br>    model = FlashLlamaForCausalLM(config,
          weights)<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 478, in <strong>init</strong><br>    self.model = FlashLlamaModel(config,
          weights)<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 416, in <strong>init</strong><br>    [<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 417, in <br>    FlashLlamaLayer(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 353, in <strong>init</strong><br>    self.self_attn = FlashLlamaAttention(<br>  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 233, in <strong>init</strong><br>    self.query_key_value = load_attention(config,
          prefix, weights)<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 154, in load_attention<br>    return _load_gqa(config, prefix, weights)<br>  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 195, in _load_gqa<br>    get_linear(weight, bias=None, quantize=config.quantize)<br>  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py",
          line 332, in get_linear<br>    linear = WQLinear(<br>NameError: name ''WQLinear''
          is not defined</p>

          </blockquote>

          <p>Is something else needed in the config or is there something missing
          in the huggingface image?</p>

          <p>Thank you in advance!</p>

          <p>P.S GPTQ version (01-ai/Yi-34B-Chat-8bits) worked out of the box!!</p>

          '
        raw: "I am trying to deploy the new quantized versions to Sagemaker to experiment\
          \ with them. When I specify this configuration:\r\n\r\nconfig = {\r\n  \
          \  \"HF_MODEL_ID\": \"01-ai/Yi-34B-Chat-4bits\"\r\n    'SM_NUM_GPUS': json.dumps(4),\r\
          \n    'QUANTIZE': 'awq',\r\n}\r\n\r\nI get the following error:\r\n> File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 201, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 478, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 416, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 417, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 353, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 233, in __init__\r\n    self.query_key_value = load_attention(config,\
          \ prefix, weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 154, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 195, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 332, in get_linear\r\n    linear = WQLinear( \r\nNameError: name\
          \ 'WQLinear' is not defined\r\n\r\nIs something else needed in the config\
          \ or is there something missing in the huggingface image?\r\n\r\nThank you\
          \ in advance!\r\n\r\nP.S GPTQ version (01-ai/Yi-34B-Chat-8bits) worked out\
          \ of the box!!\r\n"
        updatedAt: '2023-11-23T15:52:36.273Z'
      numEdits: 0
      reactions: []
    id: 655f7544d5c0d3db538d005b
    type: comment
  author: angeligareta
  content: "I am trying to deploy the new quantized versions to Sagemaker to experiment\
    \ with them. When I specify this configuration:\r\n\r\nconfig = {\r\n    \"HF_MODEL_ID\"\
    : \"01-ai/Yi-34B-Chat-4bits\"\r\n    'SM_NUM_GPUS': json.dumps(4),\r\n    'QUANTIZE':\
    \ 'awq',\r\n}\r\n\r\nI get the following error:\r\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 201, in get_model\r\n    return FlashLlama(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 68, in __init__\r\n    model = FlashLlamaForCausalLM(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 478, in __init__\r\n    self.model = FlashLlamaModel(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 416, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 417, in <listcomp>\r\n    FlashLlamaLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 353, in __init__\r\n    self.self_attn = FlashLlamaAttention(\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 233, in __init__\r\n    self.query_key_value = load_attention(config, prefix,\
    \ weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 154, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 195, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 332, in get_linear\r\n    linear = WQLinear( \r\nNameError: name 'WQLinear'\
    \ is not defined\r\n\r\nIs something else needed in the config or is there something\
    \ missing in the huggingface image?\r\n\r\nThank you in advance!\r\n\r\nP.S GPTQ\
    \ version (01-ai/Yi-34B-Chat-8bits) worked out of the box!!\r\n"
  created_at: 2023-11-23 15:52:36+00:00
  edited: false
  hidden: false
  id: 655f7544d5c0d3db538d005b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
      fullname: panyang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: panyang01
      type: user
    createdAt: '2023-11-24T01:23:06.000Z'
    data:
      edited: false
      editors:
      - panyang01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9132464528083801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
          fullname: panyang
          isHf: false
          isPro: false
          name: panyang01
          type: user
        html: '<p>You need to update your transformers to the latest version. We tested
          running AWQ version on transformers==4.35.2, and it worked fine.</p>

          '
        raw: You need to update your transformers to the latest version. We tested
          running AWQ version on transformers==4.35.2, and it worked fine.
        updatedAt: '2023-11-24T01:23:06.334Z'
      numEdits: 0
      reactions: []
    id: 655ffafad3e98399662a2e16
    type: comment
  author: panyang01
  content: You need to update your transformers to the latest version. We tested running
    AWQ version on transformers==4.35.2, and it worked fine.
  created_at: 2023-11-24 01:23:06+00:00
  edited: false
  hidden: false
  id: 655ffafad3e98399662a2e16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
      fullname: panyang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: panyang01
      type: user
    createdAt: '2023-11-24T02:47:59.000Z'
    data:
      edited: true
      editors:
      - panyang01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331539273262024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
          fullname: panyang
          isHf: false
          isPro: false
          name: panyang01
          type: user
        html: '<p>Oh, it seems like you''re having an issue with the TGI image. You
          can keep an eye on the progress of the issue <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/issues/1234">https://github.com/huggingface/text-generation-inference/issues/1234</a></p>

          '
        raw: Oh, it seems like you're having an issue with the TGI image. You can
          keep an eye on the progress of the issue https://github.com/huggingface/text-generation-inference/issues/1234
        updatedAt: '2023-11-24T02:51:51.783Z'
      numEdits: 1
      reactions: []
    id: 65600edf9cf1eef99d5bd9d0
    type: comment
  author: panyang01
  content: Oh, it seems like you're having an issue with the TGI image. You can keep
    an eye on the progress of the issue https://github.com/huggingface/text-generation-inference/issues/1234
  created_at: 2023-11-24 02:47:59+00:00
  edited: true
  hidden: false
  id: 65600edf9cf1eef99d5bd9d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
      fullname: Angel Igareta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: angeligareta
      type: user
    createdAt: '2023-11-24T14:12:00.000Z'
    data:
      edited: false
      editors:
      - angeligareta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9515098929405212
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
          fullname: Angel Igareta
          isHf: false
          isPro: false
          name: angeligareta
          type: user
        html: '<p>Thank you, I will keep an eye</p>

          '
        raw: Thank you, I will keep an eye
        updatedAt: '2023-11-24T14:12:00.170Z'
      numEdits: 0
      reactions: []
    id: 6560af307ff2e1b1cf6094cc
    type: comment
  author: angeligareta
  content: Thank you, I will keep an eye
  created_at: 2023-11-24 14:12:00+00:00
  edited: false
  hidden: false
  id: 6560af307ff2e1b1cf6094cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
      fullname: Angel Igareta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: angeligareta
      type: user
    createdAt: '2023-11-28T14:20:19.000Z'
    data:
      edited: false
      editors:
      - angeligareta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5240815877914429
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
          fullname: Angel Igareta
          isHf: false
          isPro: false
          name: angeligareta
          type: user
        html: '<p>Resolved, more information here: <a href="https://huggingface.co/01-ai/Yi-34B-Chat-4bits/discussions/3">https://huggingface.co/01-ai/Yi-34B-Chat-4bits/discussions/3</a></p>

          '
        raw: 'Resolved, more information here: https://huggingface.co/01-ai/Yi-34B-Chat-4bits/discussions/3'
        updatedAt: '2023-11-28T14:20:19.509Z'
      numEdits: 0
      reactions: []
    id: 6565f723d4c272e7a9f2660f
    type: comment
  author: angeligareta
  content: 'Resolved, more information here: https://huggingface.co/01-ai/Yi-34B-Chat-4bits/discussions/3'
  created_at: 2023-11-28 14:20:19+00:00
  edited: false
  hidden: false
  id: 6565f723d4c272e7a9f2660f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
      fullname: Angel Igareta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: angeligareta
      type: user
    createdAt: '2023-11-28T14:20:22.000Z'
    data:
      status: closed
    id: 6565f726890c3786d7aaed3d
    type: status-change
  author: angeligareta
  created_at: 2023-11-28 14:20:22+00:00
  id: 6565f726890c3786d7aaed3d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 36
repo_id: 01-ai/Yi-34B
repo_type: model
status: closed
target_branch: null
title: Yi-34B-Chat-4bits fails to deploy in AWS Sagemaker
