!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Xeronid
conflicting_files: null
created_at: 2023-11-26 18:53:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/I5nVfm6EFSnGpXgqOj22-.jpeg?w=200&h=200&f=face
      fullname: Rob Fox
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xeronid
      type: user
    createdAt: '2023-11-26T18:53:40.000Z'
    data:
      edited: false
      editors:
      - Xeronid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8154308199882507
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/I5nVfm6EFSnGpXgqOj22-.jpeg?w=200&h=200&f=face
          fullname: Rob Fox
          isHf: false
          isPro: false
          name: Xeronid
          type: user
        html: "<p>Some modules are dispatched on the CPU or the disk. Make sure you\
          \ have enough GPU RAM to fit</p>\n<pre><code>                the quantized\
          \ model. If you want to dispatch the model on the CPU or the disk while\
          \ keeping\n\n                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\n\n                `device_map` to `from_pretrained`.\
          \ Check\n\n                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \n                for more details.\n</code></pre>\n"
        raw: "Some modules are dispatched on the CPU or the disk. Make sure you have\
          \ enough GPU RAM to fit\r\n\r\n                    the quantized model.\
          \ If you want to dispatch the model on the CPU or the disk while keeping\r\
          \n\r\n                    these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\r\n\r\n                    `device_map` to `from_pretrained`.\
          \ Check\r\n\r\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
          \n\r\n                    for more details.\r\n"
        updatedAt: '2023-11-26T18:53:40.036Z'
      numEdits: 0
      reactions: []
    id: 65639434119e451d4aa9bbb8
    type: comment
  author: Xeronid
  content: "Some modules are dispatched on the CPU or the disk. Make sure you have\
    \ enough GPU RAM to fit\r\n\r\n                    the quantized model. If you\
    \ want to dispatch the model on the CPU or the disk while keeping\r\n\r\n    \
    \                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
    \ and pass a custom\r\n\r\n                    `device_map` to `from_pretrained`.\
    \ Check\r\n\r\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
    \n\r\n                    for more details.\r\n"
  created_at: 2023-11-26 18:53:40+00:00
  edited: false
  hidden: false
  id: 65639434119e451d4aa9bbb8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: 01-ai/Yi-34B
repo_type: model
status: open
target_branch: null
title: Unable to run this locally with oobabooga text-generation-webui
