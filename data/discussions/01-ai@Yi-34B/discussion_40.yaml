!!python/object:huggingface_hub.community.DiscussionWithDetails
author: blurjp
conflicting_files: null
created_at: 2023-11-29 23:09:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b4fce484b8a3589b13a125374c58ee0.svg
      fullname: jianping huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blurjp
      type: user
    createdAt: '2023-11-29T23:09:22.000Z'
    data:
      edited: false
      editors:
      - blurjp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9547980427742004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b4fce484b8a3589b13a125374c58ee0.svg
          fullname: jianping huang
          isHf: false
          isPro: false
          name: blurjp
          type: user
        html: '<p>I currently use a 4090, but the inference process is extremely slow.
          Is it impractical to expect this model to run efficiently on just a single
          4090?</p>

          '
        raw: I currently use a 4090, but the inference process is extremely slow.
          Is it impractical to expect this model to run efficiently on just a single
          4090?
        updatedAt: '2023-11-29T23:09:23.000Z'
      numEdits: 0
      reactions: []
    id: 6567c4a22f3ec92d7e65ff52
    type: comment
  author: blurjp
  content: I currently use a 4090, but the inference process is extremely slow. Is
    it impractical to expect this model to run efficiently on just a single 4090?
  created_at: 2023-11-29 23:09:22+00:00
  edited: false
  hidden: false
  id: 6567c4a22f3ec92d7e65ff52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59065ffdf15d72558dc1d6d0f2c8872f.svg
      fullname: Xianbin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShampX
      type: user
    createdAt: '2023-12-10T17:01:23.000Z'
    data:
      edited: false
      editors:
      - ShampX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9941093325614929
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59065ffdf15d72558dc1d6d0f2c8872f.svg
          fullname: Xianbin
          isHf: false
          isPro: false
          name: ShampX
          type: user
        html: '<p>Did you solve that? I have a same problem.</p>

          '
        raw: Did you solve that? I have a same problem.
        updatedAt: '2023-12-10T17:01:23.825Z'
      numEdits: 0
      reactions: []
    id: 6575eee3244aefdfc4cf9a7a
    type: comment
  author: ShampX
  content: Did you solve that? I have a same problem.
  created_at: 2023-12-10 17:01:23+00:00
  edited: false
  hidden: false
  id: 6575eee3244aefdfc4cf9a7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-03T09:15:40.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7054075002670288
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>You can use these 2 bit versions made with quip#. Inference is slower
          than usual but it should work on a single 4090. </p>

          <p><a href="https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit">https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit</a><br><a
          href="https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit">https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit</a></p>

          '
        raw: "You can use these 2 bit versions made with quip#. Inference is slower\
          \ than usual but it should work on a single 4090. \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit\n\
          https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit"
        updatedAt: '2024-01-03T09:15:40.093Z'
      numEdits: 0
      reactions: []
    id: 659525bc354e365237a2fba9
    type: comment
  author: KnutJaegersberg
  content: "You can use these 2 bit versions made with quip#. Inference is slower\
    \ than usual but it should work on a single 4090. \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit\n\
    https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit"
  created_at: 2024-01-03 09:15:40+00:00
  edited: false
  hidden: false
  id: 659525bc354e365237a2fba9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: 01-ai/Yi-34B
repo_type: model
status: open
target_branch: null
title: Necessary hardware for Operating the  34B Model
