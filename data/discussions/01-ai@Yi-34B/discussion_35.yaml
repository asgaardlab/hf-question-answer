!!python/object:huggingface_hub.community.DiscussionWithDetails
author: angeligareta
conflicting_files: null
created_at: 2023-11-23 15:35:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
      fullname: Angel Igareta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: angeligareta
      type: user
    createdAt: '2023-11-23T15:35:43.000Z'
    data:
      edited: false
      editors:
      - angeligareta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8850114941596985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b35649453e4383a8df92af0c05148f4d.svg
          fullname: Angel Igareta
          isHf: false
          isPro: false
          name: angeligareta
          type: user
        html: '<p>Hello, </p>

          <p>First of all thank you for the release of the quantized versions, this
          is going to change the possibilities of using Yi-34B.</p>

          <p>I would like to ask recommendations on how to improve inference time
          for the Yi-34B models. I am having a MAX_INPUT_LENGTH of 3500 tokens and
          I am setting MAX_BATCH_PREFILL_TOKENS equal to X times MAX_INPUT_LENGTH,
          trying several ones until there is no a memory overflow when launching the
          app.<br>Is this a good approach? What other <a href="https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher">TGI
          parameters</a> should I check? </p>

          <p>And in the table where you compared the quantized versions based on a
          batch size, could you elaborate how to interpret that batch_size with respect
          to the MAX_INPUT_LENGTH?</p>

          <p>Thank you in advance!</p>

          '
        raw: "Hello, \r\n\r\nFirst of all thank you for the release of the quantized\
          \ versions, this is going to change the possibilities of using Yi-34B.\r\
          \n\r\nI would like to ask recommendations on how to improve inference time\
          \ for the Yi-34B models. I am having a MAX_INPUT_LENGTH of 3500 tokens and\
          \ I am setting MAX_BATCH_PREFILL_TOKENS equal to X times MAX_INPUT_LENGTH,\
          \ trying several ones until there is no a memory overflow when launching\
          \ the app.\r\nIs this a good approach? What other [TGI parameters](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)\
          \ should I check? \r\n\r\nAnd in the table where you compared the quantized\
          \ versions based on a batch size, could you elaborate how to interpret that\
          \ batch_size with respect to the MAX_INPUT_LENGTH?\r\n\r\nThank you in advance!"
        updatedAt: '2023-11-23T15:35:43.521Z'
      numEdits: 0
      reactions: []
    id: 655f714f19fd101f14c9a694
    type: comment
  author: angeligareta
  content: "Hello, \r\n\r\nFirst of all thank you for the release of the quantized\
    \ versions, this is going to change the possibilities of using Yi-34B.\r\n\r\n\
    I would like to ask recommendations on how to improve inference time for the Yi-34B\
    \ models. I am having a MAX_INPUT_LENGTH of 3500 tokens and I am setting MAX_BATCH_PREFILL_TOKENS\
    \ equal to X times MAX_INPUT_LENGTH, trying several ones until there is no a memory\
    \ overflow when launching the app.\r\nIs this a good approach? What other [TGI\
    \ parameters](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)\
    \ should I check? \r\n\r\nAnd in the table where you compared the quantized versions\
    \ based on a batch size, could you elaborate how to interpret that batch_size\
    \ with respect to the MAX_INPUT_LENGTH?\r\n\r\nThank you in advance!"
  created_at: 2023-11-23 15:35:43+00:00
  edited: false
  hidden: false
  id: 655f714f19fd101f14c9a694
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
      fullname: panyang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: panyang01
      type: user
    createdAt: '2023-11-28T08:04:16.000Z'
    data:
      edited: false
      editors:
      - panyang01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168515801429749
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
          fullname: panyang
          isHf: false
          isPro: false
          name: panyang01
          type: user
        html: '<blockquote>

          <p>And in the table where you compared the quantized versions based on a
          batch size, could you elaborate how to interpret that batch_size with respect
          to the MAX_INPUT_LENGTH?</p>

          </blockquote>

          <p>The table might not be accurate in your scenario. It serves as a guidance
          indicating the general memory footprint usage under typical conditions.
          The testing was performed on transformers==4.35.2 with a prompt length of
          512 and a maximum generation token count of 1000.</p>

          '
        raw: '> And in the table where you compared the quantized versions based on
          a batch size, could you elaborate how to interpret that batch_size with
          respect to the MAX_INPUT_LENGTH?


          The table might not be accurate in your scenario. It serves as a guidance
          indicating the general memory footprint usage under typical conditions.
          The testing was performed on transformers==4.35.2 with a prompt length of
          512 and a maximum generation token count of 1000.'
        updatedAt: '2023-11-28T08:04:16.313Z'
      numEdits: 0
      reactions: []
    id: 65659f00f9d90924751c6b18
    type: comment
  author: panyang01
  content: '> And in the table where you compared the quantized versions based on
    a batch size, could you elaborate how to interpret that batch_size with respect
    to the MAX_INPUT_LENGTH?


    The table might not be accurate in your scenario. It serves as a guidance indicating
    the general memory footprint usage under typical conditions. The testing was performed
    on transformers==4.35.2 with a prompt length of 512 and a maximum generation token
    count of 1000.'
  created_at: 2023-11-28 08:04:16+00:00
  edited: false
  hidden: false
  id: 65659f00f9d90924751c6b18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
      fullname: panyang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: panyang01
      type: user
    createdAt: '2023-11-28T08:16:08.000Z'
    data:
      edited: false
      editors:
      - panyang01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8801629543304443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f437bdce32d436851fa6c2dcdfd808.svg
          fullname: panyang
          isHf: false
          isPro: false
          name: panyang01
          type: user
        html: '<blockquote>

          <p>I would like to ask recommendations on how to improve inference time
          for the Yi-34B models.</p>

          </blockquote>

          <p>I can''t provide any specific suggestions since it depends on your running
          case and the TGI. But the <code>MAX_INPUT_LENGTH</code> and <code>MAX_BATCH_PREFILL_TOKENS</code>
          do impact how much memory will be preserved for the KV cache at runtime.
          This will limit the batch size, aka the concurrency. I think you can try
          on decreasing <code>MAX_INPUT_LENGTH</code> while increasing <code>MAX_BATCH_PREFILL_TOKENS</code>.</p>

          '
        raw: '> I would like to ask recommendations on how to improve inference time
          for the Yi-34B models.


          I can''t provide any specific suggestions since it depends on your running
          case and the TGI. But the `MAX_INPUT_LENGTH` and `MAX_BATCH_PREFILL_TOKENS`
          do impact how much memory will be preserved for the KV cache at runtime.
          This will limit the batch size, aka the concurrency. I think you can try
          on decreasing `MAX_INPUT_LENGTH` while increasing `MAX_BATCH_PREFILL_TOKENS`.'
        updatedAt: '2023-11-28T08:16:08.086Z'
      numEdits: 0
      reactions: []
    id: 6565a1c8e3aa3156af75c67d
    type: comment
  author: panyang01
  content: '> I would like to ask recommendations on how to improve inference time
    for the Yi-34B models.


    I can''t provide any specific suggestions since it depends on your running case
    and the TGI. But the `MAX_INPUT_LENGTH` and `MAX_BATCH_PREFILL_TOKENS` do impact
    how much memory will be preserved for the KV cache at runtime. This will limit
    the batch size, aka the concurrency. I think you can try on decreasing `MAX_INPUT_LENGTH`
    while increasing `MAX_BATCH_PREFILL_TOKENS`.'
  created_at: 2023-11-28 08:16:08+00:00
  edited: false
  hidden: false
  id: 6565a1c8e3aa3156af75c67d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-28T12:10:37.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557116985321045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;angeligareta&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/angeligareta\"\
          >@<span class=\"underline\">angeligareta</span></a></span>\n\n\t</span></span>\
          \ Try install flash-attn2 with Nvidia GPU as inference device, this save\
          \ a lot of attention memory. Also try the GPTQ 4 bit version made by TheBloke,\
          \ pairing with exllamav2 (if you mostly use Yi for single batch inferencing)\
          \ or vLLM (which serves multi-batch well), all with flash-attn2</p>\n"
        raw: '@angeligareta Try install flash-attn2 with Nvidia GPU as inference device,
          this save a lot of attention memory. Also try the GPTQ 4 bit version made
          by TheBloke, pairing with exllamav2 (if you mostly use Yi for single batch
          inferencing) or vLLM (which serves multi-batch well), all with flash-attn2'
        updatedAt: '2023-11-28T12:10:37.917Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - panyang01
    id: 6565d8bd81fa36db35bd414f
    type: comment
  author: Yhyu13
  content: '@angeligareta Try install flash-attn2 with Nvidia GPU as inference device,
    this save a lot of attention memory. Also try the GPTQ 4 bit version made by TheBloke,
    pairing with exllamav2 (if you mostly use Yi for single batch inferencing) or
    vLLM (which serves multi-batch well), all with flash-attn2'
  created_at: 2023-11-28 12:10:37+00:00
  edited: false
  hidden: false
  id: 6565d8bd81fa36db35bd414f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: 01-ai/Yi-34B
repo_type: model
status: open
target_branch: null
title: Recommendations on how to improve inference time
