!!python/object:huggingface_hub.community.DiscussionWithDetails
author: godaspeg
conflicting_files: null
created_at: 2023-11-27 12:37:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/726942776665751d708e9b7f517f3e17.svg
      fullname: Michael Sieghartsleitner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: godaspeg
      type: user
    createdAt: '2023-11-27T12:37:05.000Z'
    data:
      edited: true
      editors:
      - godaspeg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8089155554771423
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/726942776665751d708e9b7f517f3e17.svg
          fullname: Michael Sieghartsleitner
          isHf: false
          isPro: false
          name: godaspeg
          type: user
        html: '<p>I am using Yi-34b-chat with config from example :<br>    max_length=2048,<br>    do_sample=True,<br>    repetition_penalty=1.3,<br>    no_repeat_ngram_size=5,<br>    temperature=0.7,<br>    top_k=40,<br>    top_p=0.8,</p>

          <p>The model does not stop generating till max_length is reached, at some
          point the model is generating random tokens. how can i prevent the model
          from doing that and automatically stop if a question is answered/task is
          fullfilled?</p>

          '
        raw: "I am using Yi-34b-chat with config from example :\n    max_length=2048,\n\
          \    do_sample=True,\n    repetition_penalty=1.3,\n    no_repeat_ngram_size=5,\n\
          \    temperature=0.7,\n    top_k=40,\n    top_p=0.8,\n\nThe model does not\
          \ stop generating till max_length is reached, at some point the model is\
          \ generating random tokens. how can i prevent the model from doing that\
          \ and automatically stop if a question is answered/task is fullfilled?"
        updatedAt: '2023-11-27T21:37:10.437Z'
      numEdits: 1
      reactions: []
    id: 65648d7150ad0a7545d3427b
    type: comment
  author: godaspeg
  content: "I am using Yi-34b-chat with config from example :\n    max_length=2048,\n\
    \    do_sample=True,\n    repetition_penalty=1.3,\n    no_repeat_ngram_size=5,\n\
    \    temperature=0.7,\n    top_k=40,\n    top_p=0.8,\n\nThe model does not stop\
    \ generating till max_length is reached, at some point the model is generating\
    \ random tokens. how can i prevent the model from doing that and automatically\
    \ stop if a question is answered/task is fullfilled?"
  created_at: 2023-11-27 12:37:05+00:00
  edited: true
  hidden: false
  id: 65648d7150ad0a7545d3427b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/111f115cfa32485b9d0aefa1d2167dab.svg
      fullname: Nirav Raiyani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niravraiyani
      type: user
    createdAt: '2023-11-27T21:33:47.000Z'
    data:
      edited: false
      editors:
      - niravraiyani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18544939160346985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/111f115cfa32485b9d0aefa1d2167dab.svg
          fullname: Nirav Raiyani
          isHf: false
          isPro: false
          name: niravraiyani
          type: user
        html: '<p>Same problem!!!</p>

          '
        raw: Same problem!!!
        updatedAt: '2023-11-27T21:33:47.853Z'
      numEdits: 0
      reactions: []
    id: 65650b3b18d648e7cca4a57e
    type: comment
  author: niravraiyani
  content: Same problem!!!
  created_at: 2023-11-27 21:33:47+00:00
  edited: false
  hidden: false
  id: 65650b3b18d648e7cca4a57e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6556d7287eaa0731c915f42e/qqgaoVkK3tj7qWQyPLWAz.png?w=200&h=200&f=face
      fullname: Kai
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Kai01ai
      type: user
    createdAt: '2023-11-28T03:33:58.000Z'
    data:
      edited: false
      editors:
      - Kai01ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8571577668190002
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6556d7287eaa0731c915f42e/qqgaoVkK3tj7qWQyPLWAz.png?w=200&h=200&f=face
          fullname: Kai
          isHf: false
          isPro: false
          name: Kai01ai
          type: user
        html: '<p>Hi,</p>

          <ol>

          <li>Could you verify the eos_token_id in the generation function? It''s
          set by default in the generation_config, and should be 7.</li>

          <li>I recommend using the default settings for the generation config, particularly
          the repetition_penalty. Given the presence of numerous special tokens such
          as stop token "&lt;|im_end|&gt;" in the prompt, they might not be generated
          anymore due to the penalty.</li>

          </ol>

          '
        raw: 'Hi,

          1. Could you verify the eos_token_id in the generation function? It''s set
          by default in the generation_config, and should be 7.

          2. I recommend using the default settings for the generation config, particularly
          the repetition_penalty. Given the presence of numerous special tokens such
          as stop token "<|im_end|>" in the prompt, they might not be generated anymore
          due to the penalty.'
        updatedAt: '2023-11-28T03:33:58.986Z'
      numEdits: 0
      reactions: []
    id: 65655fa6f483a5f0c6bfcea2
    type: comment
  author: Kai01ai
  content: 'Hi,

    1. Could you verify the eos_token_id in the generation function? It''s set by
    default in the generation_config, and should be 7.

    2. I recommend using the default settings for the generation config, particularly
    the repetition_penalty. Given the presence of numerous special tokens such as
    stop token "<|im_end|>" in the prompt, they might not be generated anymore due
    to the penalty.'
  created_at: 2023-11-28 03:33:58+00:00
  edited: false
  hidden: false
  id: 65655fa6f483a5f0c6bfcea2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/726942776665751d708e9b7f517f3e17.svg
      fullname: Michael Sieghartsleitner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: godaspeg
      type: user
    createdAt: '2023-11-28T07:07:37.000Z'
    data:
      edited: false
      editors:
      - godaspeg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7816588878631592
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/726942776665751d708e9b7f517f3e17.svg
          fullname: Michael Sieghartsleitner
          isHf: false
          isPro: false
          name: godaspeg
          type: user
        html: '<p>Hi Kai, thank you so much for your quick answer, using the settings
          from the Git Repo fixes the bug.</p>

          <p><a rel="nofollow" href="https://github.com/01-ai/Yi">https://github.com/01-ai/Yi</a></p>

          '
        raw: 'Hi Kai, thank you so much for your quick answer, using the settings
          from the Git Repo fixes the bug.


          https://github.com/01-ai/Yi




          '
        updatedAt: '2023-11-28T07:07:37.878Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656591b9216fb2d2c86d16a1
    id: 656591b9216fb2d2c86d16a0
    type: comment
  author: godaspeg
  content: 'Hi Kai, thank you so much for your quick answer, using the settings from
    the Git Repo fixes the bug.


    https://github.com/01-ai/Yi




    '
  created_at: 2023-11-28 07:07:37+00:00
  edited: false
  hidden: false
  id: 656591b9216fb2d2c86d16a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/726942776665751d708e9b7f517f3e17.svg
      fullname: Michael Sieghartsleitner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: godaspeg
      type: user
    createdAt: '2023-11-28T07:07:37.000Z'
    data:
      status: closed
    id: 656591b9216fb2d2c86d16a1
    type: status-change
  author: godaspeg
  created_at: 2023-11-28 07:07:37+00:00
  id: 656591b9216fb2d2c86d16a1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: 01-ai/Yi-34B
repo_type: model
status: closed
target_branch: null
title: Model keeps generating
