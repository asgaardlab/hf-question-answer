!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jackboot
conflicting_files: null
created_at: 2023-08-04 15:37:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-08-04T16:37:17.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.985297441482544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>The characters have attitude and give interesting replies. Maybe
          the original merged-in 33b is better? Not sure as it''s the first I''m hearing
          of it, but this has more soul than the L2 70b. Scale it up.</p>

          '
        raw: The characters have attitude and give interesting replies. Maybe the
          original merged-in 33b is better? Not sure as it's the first I'm hearing
          of it, but this has more soul than the L2 70b. Scale it up.
        updatedAt: '2023-08-04T16:37:17.239Z'
      numEdits: 0
      reactions: []
    id: 64cd293da785f2043b2dc3f0
    type: comment
  author: jackboot
  content: The characters have attitude and give interesting replies. Maybe the original
    merged-in 33b is better? Not sure as it's the first I'm hearing of it, but this
    has more soul than the L2 70b. Scale it up.
  created_at: 2023-08-04 15:37:17+00:00
  edited: false
  hidden: false
  id: 64cd293da785f2043b2dc3f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ff790eb151c7eb65abf7034ad1f7b76.svg
      fullname: Del Leet
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Envoid
      type: user
    createdAt: '2023-08-06T04:12:01.000Z'
    data:
      edited: false
      editors:
      - Envoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680972099304199
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ff790eb151c7eb65abf7034ad1f7b76.svg
          fullname: Del Leet
          isHf: false
          isPro: false
          name: Envoid
          type: user
        html: '<p>Glad you are enjoying the model! Unfortunately scaling up 1:1 is
          dependent upon meta releasing a LLaMa-2-34B.<br>I did experiment with a
          32.9B model that involved diagonal merging 65B (with Enterredaas qlora merged)
          onto llama-2-chat-13B which results in a roughly 33B model but the model
          was "oops all attention heads" and the results were unfortunately not very
          good. I could look for an alternate 33B model as a recipient model for the
          script but it''s quite likely it would not end up with the same temperament
          as Dendrite which I feel the chat model contributes a lot to. That said
          I do plan to experiment with other model combinations of finetunes to see
          what else yields usable results and if LLaMa-2-chat-34B ever comes out I
          am already planning to try to scale up the project.</p>

          '
        raw: 'Glad you are enjoying the model! Unfortunately scaling up 1:1 is dependent
          upon meta releasing a LLaMa-2-34B.

          I did experiment with a 32.9B model that involved diagonal merging 65B (with
          Enterredaas qlora merged) onto llama-2-chat-13B which results in a roughly
          33B model but the model was "oops all attention heads" and the results were
          unfortunately not very good. I could look for an alternate 33B model as
          a recipient model for the script but it''s quite likely it would not end
          up with the same temperament as Dendrite which I feel the chat model contributes
          a lot to. That said I do plan to experiment with other model combinations
          of finetunes to see what else yields usable results and if LLaMa-2-chat-34B
          ever comes out I am already planning to try to scale up the project.'
        updatedAt: '2023-08-06T04:12:01.930Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jackboot
    id: 64cf1d9144d373d7061bda2c
    type: comment
  author: Envoid
  content: 'Glad you are enjoying the model! Unfortunately scaling up 1:1 is dependent
    upon meta releasing a LLaMa-2-34B.

    I did experiment with a 32.9B model that involved diagonal merging 65B (with Enterredaas
    qlora merged) onto llama-2-chat-13B which results in a roughly 33B model but the
    model was "oops all attention heads" and the results were unfortunately not very
    good. I could look for an alternate 33B model as a recipient model for the script
    but it''s quite likely it would not end up with the same temperament as Dendrite
    which I feel the chat model contributes a lot to. That said I do plan to experiment
    with other model combinations of finetunes to see what else yields usable results
    and if LLaMa-2-chat-34B ever comes out I am already planning to try to scale up
    the project.'
  created_at: 2023-08-06 03:12:01+00:00
  edited: false
  hidden: false
  id: 64cf1d9144d373d7061bda2c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Envoid/Dendrite-II-22B
repo_type: model
status: open
target_branch: null
title: This model is lit.
