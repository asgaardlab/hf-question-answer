!!python/object:huggingface_hub.community.DiscussionWithDetails
author: waddledee
conflicting_files: null
created_at: 2022-12-01 06:56:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e5f0d65eef503e17ba25393dfe7dcbe2.svg
      fullname: michika takahashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waddledee
      type: user
    createdAt: '2022-12-01T06:56:04.000Z'
    data:
      edited: true
      editors:
      - waddledee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e5f0d65eef503e17ba25393dfe7dcbe2.svg
          fullname: michika takahashi
          isHf: false
          isPro: false
          name: waddledee
          type: user
        html: '<p>In Model card page, two ways to generate embeddings are intoduced;
          Usage (Sentence-Transformers) and Usage (HuggingFace Transformers).<br>However,
          if you input a sentence longer than certain length (like around 128), their
          outputs will not be the same.<br>The reason is that max_seq_length defined
          in sentence_bert_config.json is 128 while BERT''s max_position_embeddings
          is 512 as shown in config.json.<br>In addition, this <a rel="nofollow" href="https://www.sbert.net/examples/applications/computing-embeddings/README.html#input-sequence-length:~:text=Note%3A%20You%20cannot%20increase%20the%20length%20higher%20than%20what%20is%20maximally%20supported%20by%20the%20respective%20transformer%20model.%20Also%20note%20that%20if%20a%20model%20was%20trained%20on%20short%20texts%2C%20the%20representations%20for%20long%20texts%20might%20not%20be%20that%20good">document</a>
          says you cannot increase the length higher than what is maximally supported
          by the respective transformer model, but I think that in Usage (HuggingFace
          Transformers), max_seq_length is actually increased from 128 to 512.<br>Does
          it mean that we had better use SentenceTransformer class directly, not Usage
          (HuggingFace Transformers) ?  When should I use Usage (HuggingFace Transformers)?</p>

          '
        raw: "In Model card page, two ways to generate embeddings are intoduced; Usage\
          \ (Sentence-Transformers) and Usage (HuggingFace Transformers).\nHowever,\
          \ if you input a sentence longer than certain length (like around 128),\
          \ their outputs will not be the same.\nThe reason is that max_seq_length\
          \ defined in sentence_bert_config.json is 128 while BERT's max_position_embeddings\
          \ is 512 as shown in config.json.\nIn addition, this [document](https://www.sbert.net/examples/applications/computing-embeddings/README.html#input-sequence-length:~:text=Note%3A%20You%20cannot%20increase%20the%20length%20higher%20than%20what%20is%20maximally%20supported%20by%20the%20respective%20transformer%20model.%20Also%20note%20that%20if%20a%20model%20was%20trained%20on%20short%20texts%2C%20the%20representations%20for%20long%20texts%20might%20not%20be%20that%20good)\
          \ says you cannot increase the length higher than what is maximally supported\
          \ by the respective transformer model, but I think that in Usage (HuggingFace\
          \ Transformers), max_seq_length is actually increased from 128 to 512. \n\
          Does it mean that we had better use SentenceTransformer class directly,\
          \ not Usage (HuggingFace Transformers) ?  When should I use Usage (HuggingFace\
          \ Transformers)?"
        updatedAt: '2022-12-01T07:03:58.305Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - hsctim
        - nicotaroni
        - roeycohen
    id: 63885004a79d1656d2f38c65
    type: comment
  author: waddledee
  content: "In Model card page, two ways to generate embeddings are intoduced; Usage\
    \ (Sentence-Transformers) and Usage (HuggingFace Transformers).\nHowever, if you\
    \ input a sentence longer than certain length (like around 128), their outputs\
    \ will not be the same.\nThe reason is that max_seq_length defined in sentence_bert_config.json\
    \ is 128 while BERT's max_position_embeddings is 512 as shown in config.json.\n\
    In addition, this [document](https://www.sbert.net/examples/applications/computing-embeddings/README.html#input-sequence-length:~:text=Note%3A%20You%20cannot%20increase%20the%20length%20higher%20than%20what%20is%20maximally%20supported%20by%20the%20respective%20transformer%20model.%20Also%20note%20that%20if%20a%20model%20was%20trained%20on%20short%20texts%2C%20the%20representations%20for%20long%20texts%20might%20not%20be%20that%20good)\
    \ says you cannot increase the length higher than what is maximally supported\
    \ by the respective transformer model, but I think that in Usage (HuggingFace\
    \ Transformers), max_seq_length is actually increased from 128 to 512. \nDoes\
    \ it mean that we had better use SentenceTransformer class directly, not Usage\
    \ (HuggingFace Transformers) ?  When should I use Usage (HuggingFace Transformers)?"
  created_at: 2022-12-01 06:56:04+00:00
  edited: true
  hidden: false
  id: 63885004a79d1656d2f38c65
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
repo_type: model
status: open
target_branch: null
title: Result of Usage (Sentence-Transformers) and Usage (HuggingFace Transformers)
  is different
