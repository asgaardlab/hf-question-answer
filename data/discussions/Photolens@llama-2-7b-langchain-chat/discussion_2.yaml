!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YanaS
conflicting_files: null
created_at: 2023-10-03 06:49:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-10-03T07:49:54.000Z'
    data:
      edited: true
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8516397476196289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: '<p>Hi, I have used your model in localGPT repo and instead of just
          returning one answer, it returns something like this:<br>Question:<br>{{question}}</p>

          <p>Answer:</p>

          <p> <code>json {"action": "Final Answer", "action_input": {{answer1}}} </code>
          [INST] {{somehow rephrased question}} [/INST] <code>json {"action": "Final
          Answer", "action_input": {{answer2}}} </code> [INST] {{somehow rephrased
          question again}} [/INST] ```json<br>{"action": "Final Answer", "action_input":
          {{yet another answer}}} and so on.</p>

          <p>Is this how it is supposed to look like, or I need to make changes to
          the output?</p>

          <p>The original model used in localGPT is TheBloke/Llama-2-13b-Chat-GGUF</p>

          <p>Update: I realized that this is because of the dataset your model was
          trained on. But how should I format the prompt template so that I just get
          one answer and not this whole format above? If it is any help, I work with
          langchain.</p>

          '
        raw: "Hi, I have used your model in localGPT repo and instead of just returning\
          \ one answer, it returns something like this:\nQuestion:\n{{question}}\n\
          \nAnswer:\n ```json\n{\"action\": \"Final Answer\", \"action_input\": {{answer1}}}\n\
          ``` [INST] {{somehow rephrased question}} [/INST] ```json\n{\"action\":\
          \ \"Final Answer\", \"action_input\": {{answer2}}}\n``` [INST] {{somehow\
          \ rephrased question again}} [/INST] ```json\n{\"action\": \"Final Answer\"\
          , \"action_input\": {{yet another answer}}} and so on.\n\nIs this how it\
          \ is supposed to look like, or I need to make changes to the output?\n\n\
          The original model used in localGPT is TheBloke/Llama-2-13b-Chat-GGUF\n\n\
          Update: I realized that this is because of the dataset your model was trained\
          \ on. But how should I format the prompt template so that I just get one\
          \ answer and not this whole format above? If it is any help, I work with\
          \ langchain."
        updatedAt: '2023-10-03T08:10:44.207Z'
      numEdits: 2
      reactions: []
    id: 651bc7a28010d2458e1b868c
    type: comment
  author: YanaS
  content: "Hi, I have used your model in localGPT repo and instead of just returning\
    \ one answer, it returns something like this:\nQuestion:\n{{question}}\n\nAnswer:\n\
    \ ```json\n{\"action\": \"Final Answer\", \"action_input\": {{answer1}}}\n```\
    \ [INST] {{somehow rephrased question}} [/INST] ```json\n{\"action\": \"Final\
    \ Answer\", \"action_input\": {{answer2}}}\n``` [INST] {{somehow rephrased question\
    \ again}} [/INST] ```json\n{\"action\": \"Final Answer\", \"action_input\": {{yet\
    \ another answer}}} and so on.\n\nIs this how it is supposed to look like, or\
    \ I need to make changes to the output?\n\nThe original model used in localGPT\
    \ is TheBloke/Llama-2-13b-Chat-GGUF\n\nUpdate: I realized that this is because\
    \ of the dataset your model was trained on. But how should I format the prompt\
    \ template so that I just get one answer and not this whole format above? If it\
    \ is any help, I work with langchain."
  created_at: 2023-10-03 06:49:54+00:00
  edited: true
  hidden: false
  id: 651bc7a28010d2458e1b868c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f3e4002ce39336c411048/vMHxViuIK2QxreqQW3HOY.png?w=200&h=200&f=face
      fullname: AT&Dev
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: AtAndDev
      type: user
    createdAt: '2023-10-03T14:46:17.000Z'
    data:
      edited: false
      editors:
      - AtAndDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7056459188461304
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f3e4002ce39336c411048/vMHxViuIK2QxreqQW3HOY.png?w=200&h=200&f=face
          fullname: AT&Dev
          isHf: false
          isPro: false
          name: AtAndDev
          type: user
        html: '<p>Hi,</p>

          <p>It because the prompt template is llama-2. You shouldent use:</p>

          <pre><code>Question:

          &lt;question&gt;


          Answer:

          &lt;answer&gt;

          </code></pre>

          <p>The whole chat should look like this:</p>

          <pre><code>&lt;s&gt;[INST] question [/INST] answer &lt;/s&gt;

          </code></pre>

          '
        raw: 'Hi,


          It because the prompt template is llama-2. You shouldent use:

          ```

          Question:

          <question>


          Answer:

          <answer>

          ```


          The whole chat should look like this:

          ```

          <s>[INST] question [/INST] answer </s>

          ```'
        updatedAt: '2023-10-03T14:46:17.058Z'
      numEdits: 0
      reactions: []
    id: 651c29390e6b7fa4293f7178
    type: comment
  author: AtAndDev
  content: 'Hi,


    It because the prompt template is llama-2. You shouldent use:

    ```

    Question:

    <question>


    Answer:

    <answer>

    ```


    The whole chat should look like this:

    ```

    <s>[INST] question [/INST] answer </s>

    ```'
  created_at: 2023-10-03 13:46:17+00:00
  edited: false
  hidden: false
  id: 651c29390e6b7fa4293f7178
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-10-04T05:20:15.000Z'
    data:
      edited: true
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9956021904945374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: '<p>Thank you. I had a problem with where the system message should
          be, in [INST] or outside. It seems like outside is good. I also don''t use
          <s></s> in the template because it returns a lot of [INST][/INST] answers
          when I only need one. So, my prompt looks like this:</p>

          <p>&lt;&lt;SYS&gt;&gt;\n<br>You are a helpful assistant. Use only the given
          context to answer a question.\n<br>If you do not know the answer reply with
          ''I don''t know''. Always answer with one sentence not more than 10 words
          long.\n<br>&lt;&lt;/SYS&gt;&gt;\n\n<br>[INST]\n<br>Context: {context}\n<br>User:
          {question}<br>[/INST]</p>

          <p>Update: Although, when the model is not sure of the answer it returns
          something like this {answer}[/INST]{another answer}[/INST]{yet another answer}[/INST].
          Also, if it really do not know the answer, it still returns answer in the
          format that I have already shared.</p>

          '
        raw: "Thank you. I had a problem with where the system message should be,\
          \ in [INST] or outside. It seems like outside is good. I also don't use\
          \ <s></s> in the template because it returns a lot of [INST][/INST] answers\
          \ when I only need one. So, my prompt looks like this:\n\n\\<\\<SYS>>\\\
          n\nYou are a helpful assistant. Use only the given context to answer a question.\\\
          n\nIf you do not know the answer reply with 'I don't know'. Always answer\
          \ with one sentence not more than 10 words long.\\n\n\\<\\</SYS>>\\n\\n\n\
          [INST]\\n            \nContext: {context}\\n           \nUser: {question}\n\
          [/INST]\n\nUpdate: Although, when the model is not sure of the answer it\
          \ returns something like this {answer}[/INST]{another answer}[/INST]{yet\
          \ another answer}[/INST]. Also, if it really do not know the answer, it\
          \ still returns answer in the format that I have already shared."
        updatedAt: '2023-10-04T08:10:10.197Z'
      numEdits: 9
      reactions: []
    id: 651cf60fa095cd022bb25153
    type: comment
  author: YanaS
  content: "Thank you. I had a problem with where the system message should be, in\
    \ [INST] or outside. It seems like outside is good. I also don't use <s></s> in\
    \ the template because it returns a lot of [INST][/INST] answers when I only need\
    \ one. So, my prompt looks like this:\n\n\\<\\<SYS>>\\n\nYou are a helpful assistant.\
    \ Use only the given context to answer a question.\\n\nIf you do not know the\
    \ answer reply with 'I don't know'. Always answer with one sentence not more than\
    \ 10 words long.\\n\n\\<\\</SYS>>\\n\\n\n[INST]\\n            \nContext: {context}\\\
    n           \nUser: {question}\n[/INST]\n\nUpdate: Although, when the model is\
    \ not sure of the answer it returns something like this {answer}[/INST]{another\
    \ answer}[/INST]{yet another answer}[/INST]. Also, if it really do not know the\
    \ answer, it still returns answer in the format that I have already shared."
  created_at: 2023-10-04 04:20:15+00:00
  edited: true
  hidden: false
  id: 651cf60fa095cd022bb25153
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f3e4002ce39336c411048/vMHxViuIK2QxreqQW3HOY.png?w=200&h=200&f=face
      fullname: AT&Dev
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: AtAndDev
      type: user
    createdAt: '2023-10-05T18:29:48.000Z'
    data:
      edited: false
      editors:
      - AtAndDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8815234303474426
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f3e4002ce39336c411048/vMHxViuIK2QxreqQW3HOY.png?w=200&h=200&f=face
          fullname: AT&Dev
          isHf: false
          isPro: false
          name: AtAndDev
          type: user
        html: '<p>Yeah, I dont like llama format too. Alpaca is much better.</p>

          '
        raw: Yeah, I dont like llama format too. Alpaca is much better.
        updatedAt: '2023-10-05T18:29:48.924Z'
      numEdits: 0
      reactions: []
    id: 651f009c5ff73c3cec4a9940
    type: comment
  author: AtAndDev
  content: Yeah, I dont like llama format too. Alpaca is much better.
  created_at: 2023-10-05 17:29:48+00:00
  edited: false
  hidden: false
  id: 651f009c5ff73c3cec4a9940
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-11-08T09:02:43.000Z'
    data:
      edited: false
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.928477942943573
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: '<p>I am sorry to bother you again but I still have problems with the
          model and I really need to use it. When I use the standard llama2 template
          for this model TheBloke/Llama-2-13b-Chat-GGUF I get expected answers in
          the form of normal one, or two sentences. Then, when I use it with this
          model, I get all these outputs I have mentioned to you in the conversation
          before. So there must be some difference in the formats when you finetuned
          to optimize for langchain. Could be the order of the system message or the
          tags, I cannot figure it out.</p>

          '
        raw: I am sorry to bother you again but I still have problems with the model
          and I really need to use it. When I use the standard llama2 template for
          this model TheBloke/Llama-2-13b-Chat-GGUF I get expected answers in the
          form of normal one, or two sentences. Then, when I use it with this model,
          I get all these outputs I have mentioned to you in the conversation before.
          So there must be some difference in the formats when you finetuned to optimize
          for langchain. Could be the order of the system message or the tags, I cannot
          figure it out.
        updatedAt: '2023-11-08T09:02:43.099Z'
      numEdits: 0
      reactions: []
    id: 654b4eb3f8d368d29fd2129f
    type: comment
  author: YanaS
  content: I am sorry to bother you again but I still have problems with the model
    and I really need to use it. When I use the standard llama2 template for this
    model TheBloke/Llama-2-13b-Chat-GGUF I get expected answers in the form of normal
    one, or two sentences. Then, when I use it with this model, I get all these outputs
    I have mentioned to you in the conversation before. So there must be some difference
    in the formats when you finetuned to optimize for langchain. Could be the order
    of the system message or the tags, I cannot figure it out.
  created_at: 2023-11-08 09:02:43+00:00
  edited: false
  hidden: false
  id: 654b4eb3f8d368d29fd2129f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Photolens/llama-2-7b-langchain-chat
repo_type: model
status: open
target_branch: null
title: Strange way of returning answers
