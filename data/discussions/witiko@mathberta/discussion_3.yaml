!!python/object:huggingface_hub.community.DiscussionWithDetails
author: surya-narayanan
conflicting_files: null
created_at: 2023-06-06 17:22:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
      fullname: Surya Narayanan Hari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: surya-narayanan
      type: user
    createdAt: '2023-06-06T18:22:27.000Z'
    data:
      edited: false
      editors:
      - surya-narayanan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8947364091873169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
          fullname: Surya Narayanan Hari
          isHf: false
          isPro: false
          name: surya-narayanan
          type: user
        html: '<p>Hi, the model seems to output a tensor of size batchsize x sentence
          size x 78672 but the tokenizer vocab size is 50265. Any idea why there''s
          this discrepancy?</p>

          '
        raw: Hi, the model seems to output a tensor of size batchsize x sentence size
          x 78672 but the tokenizer vocab size is 50265. Any idea why there's this
          discrepancy?
        updatedAt: '2023-06-06T18:22:27.175Z'
      numEdits: 0
      reactions: []
    id: 647f7963aa8c04bbf9397d3f
    type: comment
  author: surya-narayanan
  content: Hi, the model seems to output a tensor of size batchsize x sentence size
    x 78672 but the tokenizer vocab size is 50265. Any idea why there's this discrepancy?
  created_at: 2023-06-06 17:22:27+00:00
  edited: false
  hidden: false
  id: 647f7963aa8c04bbf9397d3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
      fullname: "Michal \u0160tef\xE1nik"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michal-stefanik
      type: user
    createdAt: '2023-07-03T15:06:07.000Z'
    data:
      edited: false
      editors:
      - michal-stefanik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8786653876304626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
          fullname: "Michal \u0160tef\xE1nik"
          isHf: false
          isPro: false
          name: michal-stefanik
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;surya-narayanan&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/surya-narayanan\"\
          >@<span class=\"underline\">surya-narayanan</span></a></span>\n\n\t</span></span>\
          \ , MathBERTa's tokenizer was largely extended to cover the math vocabulary.\
          \ At the time of training, this was not fully supported by <code>transformers</code>,\
          \ so some inconsistencies like this can still be ocassionally found. FWIW,\
          \ later on, we opened and merged <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/16936\"\
          >related PR</a> to <code>transformers</code> library.</p>\n<p>Anyway, I've\
          \ checked for you that <a href=\"https://huggingface.co/witiko/mathberta/blob/main/config.json#L26\"\
          >model's config</a> matches the <code>len(tokenizer.vocab)</code>, so the\
          \ current model and tokenizer should be good to be used together as-is.</p>\n"
        raw: 'Hi @surya-narayanan , MathBERTa''s tokenizer was largely extended to
          cover the math vocabulary. At the time of training, this was not fully supported
          by `transformers`, so some inconsistencies like this can still be ocassionally
          found. FWIW, later on, we opened and merged [related PR](https://github.com/huggingface/transformers/issues/16936)
          to `transformers` library.


          Anyway, I''ve checked for you that [model''s config](https://huggingface.co/witiko/mathberta/blob/main/config.json#L26)
          matches the `len(tokenizer.vocab)`, so the current model and tokenizer should
          be good to be used together as-is.'
        updatedAt: '2023-07-03T15:06:07.635Z'
      numEdits: 0
      reactions: []
    id: 64a2e3dfea94f2190bf06cf2
    type: comment
  author: michal-stefanik
  content: 'Hi @surya-narayanan , MathBERTa''s tokenizer was largely extended to cover
    the math vocabulary. At the time of training, this was not fully supported by
    `transformers`, so some inconsistencies like this can still be ocassionally found.
    FWIW, later on, we opened and merged [related PR](https://github.com/huggingface/transformers/issues/16936)
    to `transformers` library.


    Anyway, I''ve checked for you that [model''s config](https://huggingface.co/witiko/mathberta/blob/main/config.json#L26)
    matches the `len(tokenizer.vocab)`, so the current model and tokenizer should
    be good to be used together as-is.'
  created_at: 2023-07-03 14:06:07+00:00
  edited: false
  hidden: false
  id: 64a2e3dfea94f2190bf06cf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
      fullname: Surya Narayanan Hari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: surya-narayanan
      type: user
    createdAt: '2023-07-19T22:19:45.000Z'
    data:
      edited: false
      editors:
      - surya-narayanan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9473460912704468
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
          fullname: Surya Narayanan Hari
          isHf: false
          isPro: false
          name: surya-narayanan
          type: user
        html: '<p>hmm, still facing an error- should i re-install transformers?</p>

          '
        raw: hmm, still facing an error- should i re-install transformers?
        updatedAt: '2023-07-19T22:19:45.240Z'
      numEdits: 0
      reactions: []
    id: 64b86181e3d41dbd6953e860
    type: comment
  author: surya-narayanan
  content: hmm, still facing an error- should i re-install transformers?
  created_at: 2023-07-19 21:19:45+00:00
  edited: false
  hidden: false
  id: 64b86181e3d41dbd6953e860
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: witiko/mathberta
repo_type: model
status: open
target_branch: null
title: Prediction size vs tokenizer size
