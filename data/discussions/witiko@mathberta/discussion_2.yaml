!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DimOgu
conflicting_files: null
created_at: 2023-03-27 14:14:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6e29f60e84d015dda6728eba6c977dc.svg
      fullname: Dmitrii Ogurtsov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DimOgu
      type: user
    createdAt: '2023-03-27T15:14:03.000Z'
    data:
      edited: false
      editors:
      - DimOgu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6e29f60e84d015dda6728eba6c977dc.svg
          fullname: Dmitrii Ogurtsov
          isHf: false
          isPro: false
          name: DimOgu
          type: user
        html: '<p>I would like to report a bug when updating the version of the transformers
          library (transormers 4.16.2 -&gt; 4.20.1), the version of the tokenizer
          library has also changed (tokenizer == 0.10.3 -&gt; 0.12.1), which entailed
          changes when applying the tokenizer.<br>Consider an example.<br>This figure
          shows the operation of the tokenizer with tokenizer version 0.10.3 </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/KR8ZZxxlcFxH6E9AeR7NN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/KR8ZZxxlcFxH6E9AeR7NN.png"></a></p>

          <p>This figure shows the operation of the tokenizer with tokenizer version
          0.12.1 </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/Gy_QgEm-cQwKwnEasWkUD.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/Gy_QgEm-cQwKwnEasWkUD.png"></a></p>

          <p>The difference in this case is the separation of "" into a separate token.</p>

          <p>There are also problems with the allocation of such latex "words" as
          "\cite" "\Omega" and so on into single tokens, in both versions of the tokenizer.</p>

          '
        raw: "I would like to report a bug when updating the version of the transformers\
          \ library (transormers 4.16.2 -> 4.20.1), the version of the tokenizer library\
          \ has also changed (tokenizer == 0.10.3 -> 0.12.1), which entailed changes\
          \ when applying the tokenizer. \r\nConsider an example.\r\nThis figure shows\
          \ the operation of the tokenizer with tokenizer version 0.10.3 \r\n\r\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/KR8ZZxxlcFxH6E9AeR7NN.png)\r\
          \n\r\nThis figure shows the operation of the tokenizer with tokenizer version\
          \ 0.12.1 \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/Gy_QgEm-cQwKwnEasWkUD.png)\r\
          \n\r\nThe difference in this case is the separation of \"\\\" into a separate\
          \ token.\r\n\r\nThere are also problems with the allocation of such latex\
          \ \"words\" as \"\\cite\" \"\\Omega\" and so on into single tokens, in both\
          \ versions of the tokenizer.\r\n\r\n"
        updatedAt: '2023-03-27T15:14:03.098Z'
      numEdits: 0
      reactions: []
    id: 6421b2bb9628d83384b27ecf
    type: comment
  author: DimOgu
  content: "I would like to report a bug when updating the version of the transformers\
    \ library (transormers 4.16.2 -> 4.20.1), the version of the tokenizer library\
    \ has also changed (tokenizer == 0.10.3 -> 0.12.1), which entailed changes when\
    \ applying the tokenizer. \r\nConsider an example.\r\nThis figure shows the operation\
    \ of the tokenizer with tokenizer version 0.10.3 \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/KR8ZZxxlcFxH6E9AeR7NN.png)\r\
    \n\r\nThis figure shows the operation of the tokenizer with tokenizer version\
    \ 0.12.1 \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62c1b3ea8b647bdc24f741c9/Gy_QgEm-cQwKwnEasWkUD.png)\r\
    \n\r\nThe difference in this case is the separation of \"\\\" into a separate\
    \ token.\r\n\r\nThere are also problems with the allocation of such latex \"words\"\
    \ as \"\\cite\" \"\\Omega\" and so on into single tokens, in both versions of\
    \ the tokenizer.\r\n\r\n"
  created_at: 2023-03-27 14:14:03+00:00
  edited: false
  hidden: false
  id: 6421b2bb9628d83384b27ecf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
      fullname: "Michal \u0160tef\xE1nik"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michal-stefanik
      type: user
    createdAt: '2023-03-29T08:34:21.000Z'
    data:
      edited: false
      editors:
      - michal-stefanik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659956308483-62f0c5bca594daba4a55e793.jpeg?w=200&h=200&f=face
          fullname: "Michal \u0160tef\xE1nik"
          isHf: false
          isPro: false
          name: michal-stefanik
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;DimOgu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/DimOgu\">@<span class=\"\
          underline\">DimOgu</span></a></span>\n\n\t</span></span> ,<br>Please note\
          \ that mathberta has been trained with <a href=\"https://huggingface.co/witiko/mathberta/blob/main/config.json#L23\"\
          >transformers==4.18.0</a>, which requires <code>tokenizers&gt;=0.11.1,!=0.11.3,&lt;0.13</code>.\
          \ Therefore, we recommend not using mathberta with older versions of transformers\
          \ than 4.18.0 and we recommend using it with <code>transformers==4.20.1</code>\
          \ due to an <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/16936\"\
          >issue</a> that we fixed in the meantime.</p>\n<p>If you still need to use\
          \ it with older version, check that the resulting wordpieces from calling\
          \ the tokenizer are the same:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"witiko/mathberta\"</span>)\ntext = <span class=\"\
          hljs-string\">\"This \\emph{Extended Patience Sorting Algorithm} is similar.\"\
          </span>\n\n<span class=\"hljs-comment\"># on transformers==4.20.1 + tokenizers==0.12.1:</span>\n\
          t(text).input_ids\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>[<span\
          \ class=\"hljs-number\">0</span>, <span class=\"hljs-number\">152</span>,\
          \ <span class=\"hljs-number\">1437</span>, <span class=\"hljs-number\">57042</span>,\
          \ <span class=\"hljs-number\">50619</span>, <span class=\"hljs-number\"\
          >1437</span>, <span class=\"hljs-number\">11483</span>, <span class=\"hljs-number\"\
          >6228</span>, <span class=\"hljs-number\">3769</span>, <span class=\"hljs-number\"\
          >11465</span>, <span class=\"hljs-number\">208</span>, <span class=\"hljs-number\"\
          >23817</span>, <span class=\"hljs-number\">83</span>, <span class=\"hljs-number\"\
          >53143</span>, <span class=\"hljs-number\">50</span>, <span class=\"hljs-number\"\
          >3432</span>, <span class=\"hljs-number\">54598</span>, <span class=\"hljs-number\"\
          >16</span>, <span class=\"hljs-number\">16207</span>, <span class=\"hljs-number\"\
          >4882</span>, <span class=\"hljs-number\">55021</span>, <span class=\"hljs-number\"\
          >2</span>]\n</code></pre>\n<p>If the <code>input_ids</code> on your desired\
          \ versions of libraries <strong>match</strong> <code>input_ids</code> in\
          \ the supported version (transformers==4.20.1), you do not have to care\
          \ about the spaces in decoding (normally done by calling <code>decode</code>\
          \ or <code>batch_decode</code>) and you can still use the model without\
          \ doubts.</p>\n"
        raw: 'Hi @DimOgu ,

          Please note that mathberta has been trained with [transformers==4.18.0](https://huggingface.co/witiko/mathberta/blob/main/config.json#L23),
          which requires `tokenizers>=0.11.1,!=0.11.3,<0.13`. Therefore, we recommend
          not using mathberta with older versions of transformers than 4.18.0 and
          we recommend using it with `transformers==4.20.1` due to an [issue](https://github.com/huggingface/transformers/issues/16936)
          that we fixed in the meantime.


          If you still need to use it with older version, check that the resulting
          wordpieces from calling the tokenizer are the same:

          ```python

          from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained("witiko/mathberta")

          text = "This \emph{Extended Patience Sorting Algorithm} is similar."


          # on transformers==4.20.1 + tokenizers==0.12.1:

          t(text).input_ids

          >>> [0, 152, 1437, 57042, 50619, 1437, 11483, 6228, 3769, 11465, 208, 23817,
          83, 53143, 50, 3432, 54598, 16, 16207, 4882, 55021, 2]

          ```

          If the `input_ids` on your desired versions of libraries **match** `input_ids`
          in the supported version (transformers==4.20.1), you do not have to care
          about the spaces in decoding (normally done by calling `decode` or `batch_decode`)
          and you can still use the model without doubts.'
        updatedAt: '2023-03-29T08:34:21.357Z'
      numEdits: 0
      reactions: []
    id: 6423f80d73bbae7a6d836002
    type: comment
  author: michal-stefanik
  content: 'Hi @DimOgu ,

    Please note that mathberta has been trained with [transformers==4.18.0](https://huggingface.co/witiko/mathberta/blob/main/config.json#L23),
    which requires `tokenizers>=0.11.1,!=0.11.3,<0.13`. Therefore, we recommend not
    using mathberta with older versions of transformers than 4.18.0 and we recommend
    using it with `transformers==4.20.1` due to an [issue](https://github.com/huggingface/transformers/issues/16936)
    that we fixed in the meantime.


    If you still need to use it with older version, check that the resulting wordpieces
    from calling the tokenizer are the same:

    ```python

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("witiko/mathberta")

    text = "This \emph{Extended Patience Sorting Algorithm} is similar."


    # on transformers==4.20.1 + tokenizers==0.12.1:

    t(text).input_ids

    >>> [0, 152, 1437, 57042, 50619, 1437, 11483, 6228, 3769, 11465, 208, 23817, 83,
    53143, 50, 3432, 54598, 16, 16207, 4882, 55021, 2]

    ```

    If the `input_ids` on your desired versions of libraries **match** `input_ids`
    in the supported version (transformers==4.20.1), you do not have to care about
    the spaces in decoding (normally done by calling `decode` or `batch_decode`) and
    you can still use the model without doubts.'
  created_at: 2023-03-29 07:34:21+00:00
  edited: false
  hidden: false
  id: 6423f80d73bbae7a6d836002
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: witiko/mathberta
repo_type: model
status: open
target_branch: null
title: Problems with latex tokenization
