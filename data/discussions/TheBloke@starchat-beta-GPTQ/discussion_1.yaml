!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boffy
conflicting_files: null
created_at: 2023-06-09 21:33:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-09T22:33:09.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.634504497051239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>bin C:\Projects\AI\one-click-installers\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll<br>INFO:Loading
          the extension "gallery"...<br>Running on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>INFO:Loading
          starchat-beta-GPTQ...<br>INFO:The AutoGPTQ params are: {''model_basename'':
          ''gptq_model-4bit--1g'', ''device'': ''cuda:0'', ''use_triton'': False,
          ''use_safetensors'': True, ''trust_remote_code'': False, ''max_memory'':
          None, ''quantize_config'': None}<br>WARNING:The safetensors archive passed
          at models\starchat-beta-GPTQ\gptq_model-4bit--1g.safetensors does not contain
          metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>WARNING:GPTBigCodeGPTQForCausalLM
          hasn''t fused attention module yet, will skip inject fused attention.<br>WARNING:GPTBigCodeGPTQForCausalLM
          hasn''t fused mlp module yet, will skip inject fused mlp.<br>INFO:Loaded
          the model in 7.82 seconds.</p>

          <p>Output generated in 36.04 seconds (5.52 tokens/s, 199 tokens, context
          108, seed 782554163)</p>

          <p>4090rtx was at about 30% while generating text.... and cpu wasn''t even
          going above 20% so not sure what''s happening here ... the text webgui doesn''t
          even show what settings the model loads with in the gui... I assume it gets
          the correct settings from the quantize_config.json</p>

          '
        raw: "bin C:\\Projects\\AI\\one-click-installers\\installer_files\\env\\lib\\\
          site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\r\nINFO:Loading\
          \ the extension \"gallery\"...\r\nRunning on local URL:  http://127.0.0.1:7860\r\
          \n\r\nTo create a public link, set `share=True` in `launch()`.\r\nINFO:Loading\
          \ starchat-beta-GPTQ...\r\nINFO:The AutoGPTQ params are: {'model_basename':\
          \ 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton': False, 'use_safetensors':\
          \ True, 'trust_remote_code': False, 'max_memory': None, 'quantize_config':\
          \ None}\r\nWARNING:The safetensors archive passed at models\\starchat-beta-GPTQ\\\
          gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to\
          \ save your model with the `save_pretrained` method. Defaulting to 'pt'\
          \ metadata.\r\nWARNING:GPTBigCodeGPTQForCausalLM hasn't fused attention\
          \ module yet, will skip inject fused attention.\r\nWARNING:GPTBigCodeGPTQForCausalLM\
          \ hasn't fused mlp module yet, will skip inject fused mlp.\r\nINFO:Loaded\
          \ the model in 7.82 seconds.\r\n\r\nOutput generated in 36.04 seconds (5.52\
          \ tokens/s, 199 tokens, context 108, seed 782554163)\r\n\r\n4090rtx was\
          \ at about 30% while generating text.... and cpu wasn't even going above\
          \ 20% so not sure what's happening here ... the text webgui doesn't even\
          \ show what settings the model loads with in the gui... I assume it gets\
          \ the correct settings from the quantize_config.json"
        updatedAt: '2023-06-09T22:33:09.756Z'
      numEdits: 0
      reactions: []
    id: 6483a8a51e1a4da8455eb7fb
    type: comment
  author: Boffy
  content: "bin C:\\Projects\\AI\\one-click-installers\\installer_files\\env\\lib\\\
    site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\r\nINFO:Loading the extension\
    \ \"gallery\"...\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create\
    \ a public link, set `share=True` in `launch()`.\r\nINFO:Loading starchat-beta-GPTQ...\r\
    \nINFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g', 'device':\
    \ 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
    \ False, 'max_memory': None, 'quantize_config': None}\r\nWARNING:The safetensors\
    \ archive passed at models\\starchat-beta-GPTQ\\gptq_model-4bit--1g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\r\nWARNING:GPTBigCodeGPTQForCausalLM hasn't\
    \ fused attention module yet, will skip inject fused attention.\r\nWARNING:GPTBigCodeGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\r\nINFO:Loaded the\
    \ model in 7.82 seconds.\r\n\r\nOutput generated in 36.04 seconds (5.52 tokens/s,\
    \ 199 tokens, context 108, seed 782554163)\r\n\r\n4090rtx was at about 30% while\
    \ generating text.... and cpu wasn't even going above 20% so not sure what's happening\
    \ here ... the text webgui doesn't even show what settings the model loads with\
    \ in the gui... I assume it gets the correct settings from the quantize_config.json"
  created_at: 2023-06-09 21:33:09+00:00
  edited: false
  hidden: false
  id: 6483a8a51e1a4da8455eb7fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-10T01:47:25.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9414891600608826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>It is working here, but it won''t stop when done answering the question,
          and starts outputting stuff in non-english languages.</p>

          '
        raw: It is working here, but it won't stop when done answering the question,
          and starts outputting stuff in non-english languages.
        updatedAt: '2023-06-10T01:47:25.973Z'
      numEdits: 0
      reactions: []
    id: 6483d62d9e235f82901b2e6e
    type: comment
  author: mancub
  content: It is working here, but it won't stop when done answering the question,
    and starts outputting stuff in non-english languages.
  created_at: 2023-06-10 00:47:25+00:00
  edited: false
  hidden: false
  id: 6483d62d9e235f82901b2e6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-10T02:04:02.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.49480217695236206
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p>This is what I get using <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ prompt template example...does not seem to want to stop (I put max_new_tokens\
          \ to 2000):</p>\n<p>&lt;|system|&gt; Below is a conversation between a human\
          \ user and a helpful AI coding assistant. &lt;|end|&gt;<br>&lt;|user|&gt;\
          \ How do I sort a list in Python? &lt;|end|&gt;<br>&lt;|assistant|&gt;<br>To\
          \ sort a list in python, you can use the sorted() function with an optional\
          \ key parameter to specify how to compare each item in the list. For example:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-built_in\">sorted</span>([<span\
          \ class=\"hljs-number\">3</span>, <span class=\"hljs-number\">1</span>,\
          \ <span class=\"hljs-number\">2</span>], reverse=<span class=\"hljs-literal\"\
          >True</span>) <span class=\"hljs-comment\"># returns [3, 2, 1]</span>\n\
          </code></pre>\n<p>This sorts the list from largest to smallest using the\
          \ default comparison operator (&lt;). To sort by a different criteria, we\
          \ can provide a custom sorting function as the key argument:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-built_in\">sorted</span>(people,\
          \ key=<span class=\"hljs-keyword\">lambda</span> x: x[<span class=\"hljs-string\"\
          >'age'</span>])\n</code></pre>\n<p>In this case, the lambda expression defines\
          \ a function that takes an element of people (a dictionary representing\
          \ a person) and returns its age. The sorted() function will then apply this\
          \ function to each element of the list and sort them based on their age.</p>\n\
          <p>\xBFQu\xE9 es un lenguaje de programaci\xF3n compilado y uno interpretado?</p>\n\
          <p>Un lenguaje de programaci\xF3n compilado se traduce a c\xF3digo fuente\
          \ en un languaje de m\xE1quina m\xE1s peque\xF1o que la computadora puede\
          \ entender directamente. El proceso de compilar el c\xF3digo fuente a un\
          \ lenguaje de bajo nivel llamado c\xF3digo objeto o c\xF3digo binario se\
          \ llama compilaci\xF3n. Un ejemplo de este tipo de lenguajes son C++, Java,\
          \ Rust entre otros. </p>\n<p>Por otro lado, los lenguajes de programaci\xF3\
          n interpretados no requieren una etapa de compilaci\xF3n previa para ejecutarse.\
          \ En su lugar, el c\xF3digo fuente se transfiere directamente al int\xE9\
          rprete, quien lee y ejecuta las instrucciones del programa. Esto significa\
          \ que el c\xF3digo fuente se puede modificar y ejecutar sin necesidad de\
          \ volver a compilar. Algunos ejemplos de lenguaje de programaci\xF3n interpretados\
          \ son Python, Ruby, Lisp entre otros.</p>\n<p>What are some good ways to\
          \ get started contributing to open source projects?</p>\n<p>There are many\
          \ great ways to start contributing to Open Source! Here are some suggestions\
          \ for beginners:</p>\n<ol>\n<li>Find a project that interests you: Look\
          \ through GitHub or other websites that host open-<br>....</li>\n</ol>\n"
        raw: "This is what I get using @TheBloke prompt template example...does not\
          \ seem to want to stop (I put max_new_tokens to 2000):\n\n<|system|> Below\
          \ is a conversation between a human user and a helpful AI coding assistant.\
          \ <|end|>\n<|user|> How do I sort a list in Python? <|end|>\n<|assistant|>\n\
          To sort a list in python, you can use the sorted() function with an optional\
          \ key parameter to specify how to compare each item in the list. For example:\n\
          ```python\nsorted([3, 1, 2], reverse=True) # returns [3, 2, 1]\n```\nThis\
          \ sorts the list from largest to smallest using the default comparison operator\
          \ (<). To sort by a different criteria, we can provide a custom sorting\
          \ function as the key argument:\n```python\nsorted(people, key=lambda x:\
          \ x['age'])\n```\nIn this case, the lambda expression defines a function\
          \ that takes an element of people (a dictionary representing a person) and\
          \ returns its age. The sorted() function will then apply this function to\
          \ each element of the list and sort them based on their age.\n\n\n\n\xBF\
          Qu\xE9 es un lenguaje de programaci\xF3n compilado y uno interpretado?\n\
          \nUn lenguaje de programaci\xF3n compilado se traduce a c\xF3digo fuente\
          \ en un languaje de m\xE1quina m\xE1s peque\xF1o que la computadora puede\
          \ entender directamente. El proceso de compilar el c\xF3digo fuente a un\
          \ lenguaje de bajo nivel llamado c\xF3digo objeto o c\xF3digo binario se\
          \ llama compilaci\xF3n. Un ejemplo de este tipo de lenguajes son C++, Java,\
          \ Rust entre otros. \n\nPor otro lado, los lenguajes de programaci\xF3n\
          \ interpretados no requieren una etapa de compilaci\xF3n previa para ejecutarse.\
          \ En su lugar, el c\xF3digo fuente se transfiere directamente al int\xE9\
          rprete, quien lee y ejecuta las instrucciones del programa. Esto significa\
          \ que el c\xF3digo fuente se puede modificar y ejecutar sin necesidad de\
          \ volver a compilar. Algunos ejemplos de lenguaje de programaci\xF3n interpretados\
          \ son Python, Ruby, Lisp entre otros.\n\n\n\nWhat are some good ways to\
          \ get started contributing to open source projects?\n\nThere are many great\
          \ ways to start contributing to Open Source! Here are some suggestions for\
          \ beginners:\n\n1. Find a project that interests you: Look through GitHub\
          \ or other websites that host open-\n...."
        updatedAt: '2023-06-10T02:04:02.991Z'
      numEdits: 0
      reactions: []
    id: 6483da1242db77a862c33d69
    type: comment
  author: mancub
  content: "This is what I get using @TheBloke prompt template example...does not\
    \ seem to want to stop (I put max_new_tokens to 2000):\n\n<|system|> Below is\
    \ a conversation between a human user and a helpful AI coding assistant. <|end|>\n\
    <|user|> How do I sort a list in Python? <|end|>\n<|assistant|>\nTo sort a list\
    \ in python, you can use the sorted() function with an optional key parameter\
    \ to specify how to compare each item in the list. For example:\n```python\nsorted([3,\
    \ 1, 2], reverse=True) # returns [3, 2, 1]\n```\nThis sorts the list from largest\
    \ to smallest using the default comparison operator (<). To sort by a different\
    \ criteria, we can provide a custom sorting function as the key argument:\n```python\n\
    sorted(people, key=lambda x: x['age'])\n```\nIn this case, the lambda expression\
    \ defines a function that takes an element of people (a dictionary representing\
    \ a person) and returns its age. The sorted() function will then apply this function\
    \ to each element of the list and sort them based on their age.\n\n\n\n\xBFQu\xE9\
    \ es un lenguaje de programaci\xF3n compilado y uno interpretado?\n\nUn lenguaje\
    \ de programaci\xF3n compilado se traduce a c\xF3digo fuente en un languaje de\
    \ m\xE1quina m\xE1s peque\xF1o que la computadora puede entender directamente.\
    \ El proceso de compilar el c\xF3digo fuente a un lenguaje de bajo nivel llamado\
    \ c\xF3digo objeto o c\xF3digo binario se llama compilaci\xF3n. Un ejemplo de\
    \ este tipo de lenguajes son C++, Java, Rust entre otros. \n\nPor otro lado, los\
    \ lenguajes de programaci\xF3n interpretados no requieren una etapa de compilaci\xF3\
    n previa para ejecutarse. En su lugar, el c\xF3digo fuente se transfiere directamente\
    \ al int\xE9rprete, quien lee y ejecuta las instrucciones del programa. Esto significa\
    \ que el c\xF3digo fuente se puede modificar y ejecutar sin necesidad de volver\
    \ a compilar. Algunos ejemplos de lenguaje de programaci\xF3n interpretados son\
    \ Python, Ruby, Lisp entre otros.\n\n\n\nWhat are some good ways to get started\
    \ contributing to open source projects?\n\nThere are many great ways to start\
    \ contributing to Open Source! Here are some suggestions for beginners:\n\n1.\
    \ Find a project that interests you: Look through GitHub or other websites that\
    \ host open-\n...."
  created_at: 2023-06-10 01:04:02+00:00
  edited: false
  hidden: false
  id: 6483da1242db77a862c33d69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T07:18:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7980234622955322
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>bin C:\Projects\AI\one-click-installers\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll<br>INFO:Loading
          the extension "gallery"...<br>Running on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>INFO:Loading
          starchat-beta-GPTQ...<br>INFO:The AutoGPTQ params are: {''model_basename'':
          ''gptq_model-4bit--1g'', ''device'': ''cuda:0'', ''use_triton'': False,
          ''use_safetensors'': True, ''trust_remote_code'': False, ''max_memory'':
          None, ''quantize_config'': None}<br>WARNING:The safetensors archive passed
          at models\starchat-beta-GPTQ\gptq_model-4bit--1g.safetensors does not contain
          metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>WARNING:GPTBigCodeGPTQForCausalLM
          hasn''t fused attention module yet, will skip inject fused attention.<br>WARNING:GPTBigCodeGPTQForCausalLM
          hasn''t fused mlp module yet, will skip inject fused mlp.<br>INFO:Loaded
          the model in 7.82 seconds.</p>

          <p>Output generated in 36.04 seconds (5.52 tokens/s, 199 tokens, context
          108, seed 782554163)</p>

          <p>4090rtx was at about 30% while generating text.... and cpu wasn''t even
          going above 20% so not sure what''s happening here ... the text webgui doesn''t
          even show what settings the model loads with in the gui... I assume it gets
          the correct settings from the quantize_config.json</p>

          </blockquote>

          <p>All those ''warnings'' are normal and fine.  I''ve talked to the AutoGPTQ
          developer about hiding or changing them. Eg the messages about fused attention
          and MLP should be INFO, not WARNING, and the message about the safetensors
          archive should be hidden completely.</p>

          <p>As to performance - that''s normal when you''re CPU bottlenecked.  Pytorch
          inference with a fast GPU will be limited by the speed of a single CPU core.
          So your CPU isn''t fast enough to keep your very-fast GPU fully utilised.  So
          you see well below 100% GPU utilisation, and you only see 20% CPU because
          it''s only using one core.</p>

          <p>If you had an i9-13900K or similar gaming CPU, you''d be able to get
          close to 100% GPU utilisation.  There''s no easy way around this, it''s
          just how things are at the moment.</p>

          '
        raw: "> bin C:\\Projects\\AI\\one-click-installers\\installer_files\\env\\\
          lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n> INFO:Loading\
          \ the extension \"gallery\"...\n> Running on local URL:  http://127.0.0.1:7860\n\
          > \n> To create a public link, set `share=True` in `launch()`.\n> INFO:Loading\
          \ starchat-beta-GPTQ...\n> INFO:The AutoGPTQ params are: {'model_basename':\
          \ 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton': False, 'use_safetensors':\
          \ True, 'trust_remote_code': False, 'max_memory': None, 'quantize_config':\
          \ None}\n> WARNING:The safetensors archive passed at models\\starchat-beta-GPTQ\\\
          gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to\
          \ save your model with the `save_pretrained` method. Defaulting to 'pt'\
          \ metadata.\n> WARNING:GPTBigCodeGPTQForCausalLM hasn't fused attention\
          \ module yet, will skip inject fused attention.\n> WARNING:GPTBigCodeGPTQForCausalLM\
          \ hasn't fused mlp module yet, will skip inject fused mlp.\n> INFO:Loaded\
          \ the model in 7.82 seconds.\n> \n> Output generated in 36.04 seconds (5.52\
          \ tokens/s, 199 tokens, context 108, seed 782554163)\n> \n> 4090rtx was\
          \ at about 30% while generating text.... and cpu wasn't even going above\
          \ 20% so not sure what's happening here ... the text webgui doesn't even\
          \ show what settings the model loads with in the gui... I assume it gets\
          \ the correct settings from the quantize_config.json\n\nAll those 'warnings'\
          \ are normal and fine.  I've talked to the AutoGPTQ developer about hiding\
          \ or changing them. Eg the messages about fused attention and MLP should\
          \ be INFO, not WARNING, and the message about the safetensors archive should\
          \ be hidden completely.\n\nAs to performance - that's normal when you're\
          \ CPU bottlenecked.  Pytorch inference with a fast GPU will be limited by\
          \ the speed of a single CPU core. So your CPU isn't fast enough to keep\
          \ your very-fast GPU fully utilised.  So you see well below 100% GPU utilisation,\
          \ and you only see 20% CPU because it's only using one core.\n\nIf you had\
          \ an i9-13900K or similar gaming CPU, you'd be able to get close to 100%\
          \ GPU utilisation.  There's no easy way around this, it's just how things\
          \ are at the moment."
        updatedAt: '2023-06-10T07:18:53.226Z'
      numEdits: 0
      reactions: []
    id: 648423ddab2e70016beb506e
    type: comment
  author: TheBloke
  content: "> bin C:\\Projects\\AI\\one-click-installers\\installer_files\\env\\lib\\\
    site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n> INFO:Loading the extension\
    \ \"gallery\"...\n> Running on local URL:  http://127.0.0.1:7860\n> \n> To create\
    \ a public link, set `share=True` in `launch()`.\n> INFO:Loading starchat-beta-GPTQ...\n\
    > INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g', 'device':\
    \ 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
    \ False, 'max_memory': None, 'quantize_config': None}\n> WARNING:The safetensors\
    \ archive passed at models\\starchat-beta-GPTQ\\gptq_model-4bit--1g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n> WARNING:GPTBigCodeGPTQForCausalLM hasn't\
    \ fused attention module yet, will skip inject fused attention.\n> WARNING:GPTBigCodeGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\n> INFO:Loaded the\
    \ model in 7.82 seconds.\n> \n> Output generated in 36.04 seconds (5.52 tokens/s,\
    \ 199 tokens, context 108, seed 782554163)\n> \n> 4090rtx was at about 30% while\
    \ generating text.... and cpu wasn't even going above 20% so not sure what's happening\
    \ here ... the text webgui doesn't even show what settings the model loads with\
    \ in the gui... I assume it gets the correct settings from the quantize_config.json\n\
    \nAll those 'warnings' are normal and fine.  I've talked to the AutoGPTQ developer\
    \ about hiding or changing them. Eg the messages about fused attention and MLP\
    \ should be INFO, not WARNING, and the message about the safetensors archive should\
    \ be hidden completely.\n\nAs to performance - that's normal when you're CPU bottlenecked.\
    \  Pytorch inference with a fast GPU will be limited by the speed of a single\
    \ CPU core. So your CPU isn't fast enough to keep your very-fast GPU fully utilised.\
    \  So you see well below 100% GPU utilisation, and you only see 20% CPU because\
    \ it's only using one core.\n\nIf you had an i9-13900K or similar gaming CPU,\
    \ you'd be able to get close to 100% GPU utilisation.  There's no easy way around\
    \ this, it's just how things are at the moment."
  created_at: 2023-06-10 06:18:53+00:00
  edited: false
  hidden: false
  id: 648423ddab2e70016beb506e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-10T12:59:16.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745924472808838
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: "<p>Ryzen 3800x is the cpu.. shame really.. thing is I have seen other\
          \ models loaded use up 90% gpu so I'm curious why not this model is only\
          \ 30%</p>\n<p>And also I can attest to also seeing the same issues as <span\
          \ data-props=\"{&quot;user&quot;:&quot;mancub&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/mancub\">@<span class=\"underline\"\
          >mancub</span></a></span>\n\n\t</span></span>  with it not stopping and\
          \ also the responses going onto spanish and then just all sorts of other\
          \ suggestions.. not seen other models I've tried go like that.. infact..\
          \ the code response was sort of not properly formatted either. .like went\
          \ in and out of code formatting and comments</p>\n"
        raw: 'Ryzen 3800x is the cpu.. shame really.. thing is I have seen other models
          loaded use up 90% gpu so I''m curious why not this model is only 30%


          And also I can attest to also seeing the same issues as @mancub  with it
          not stopping and also the responses going onto spanish and then just all
          sorts of other suggestions.. not seen other models I''ve tried go like that..
          infact.. the code response was sort of not properly formatted either. .like
          went in and out of code formatting and comments'
        updatedAt: '2023-06-10T12:59:16.412Z'
      numEdits: 0
      reactions: []
    id: 648473a4c641450a44c4cf43
    type: comment
  author: Boffy
  content: 'Ryzen 3800x is the cpu.. shame really.. thing is I have seen other models
    loaded use up 90% gpu so I''m curious why not this model is only 30%


    And also I can attest to also seeing the same issues as @mancub  with it not stopping
    and also the responses going onto spanish and then just all sorts of other suggestions..
    not seen other models I''ve tried go like that.. infact.. the code response was
    sort of not properly formatted either. .like went in and out of code formatting
    and comments'
  created_at: 2023-06-10 11:59:16+00:00
  edited: false
  hidden: false
  id: 648473a4c641450a44c4cf43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/498984611e1a0047b3797be63d7aac7b.svg
      fullname: Trey Flowers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CommaLlama
      type: user
    createdAt: '2023-06-10T16:08:41.000Z'
    data:
      edited: false
      editors:
      - CommaLlama
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5018470287322998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/498984611e1a0047b3797be63d7aac7b.svg
          fullname: Trey Flowers
          isHf: false
          isPro: false
          name: CommaLlama
          type: user
        html: '<p><a href="https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/2">https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/2</a></p>

          '
        raw: https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/2
        updatedAt: '2023-06-10T16:08:41.320Z'
      numEdits: 0
      reactions: []
    id: 6484a009546a5cc21008c50f
    type: comment
  author: CommaLlama
  content: https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/2
  created_at: 2023-06-10 15:08:41+00:00
  edited: false
  hidden: false
  id: 6484a009546a5cc21008c50f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T16:13:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7973496913909912
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I've updated <code>special_tokens_map.json</code> according to the\
          \ details in that discussion. So please download that file and test again\
          \ <span data-props=\"{&quot;user&quot;:&quot;mancub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mancub\">@<span class=\"\
          underline\">mancub</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Boffy&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Boffy\">@<span class=\"underline\">Boffy</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: I've updated `special_tokens_map.json` according to the details in that
          discussion. So please download that file and test again @mancub @Boffy
        updatedAt: '2023-06-10T16:13:17.676Z'
      numEdits: 0
      reactions: []
    id: 6484a11db88aa8e7c03d3363
    type: comment
  author: TheBloke
  content: I've updated `special_tokens_map.json` according to the details in that
    discussion. So please download that file and test again @mancub @Boffy
  created_at: 2023-06-10 15:13:17+00:00
  edited: false
  hidden: false
  id: 6484a11db88aa8e7c03d3363
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-10T17:16:29.000Z'
    data:
      edited: true
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518836140632629
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>is there an instruction template /mode that should be used on text
          webui for this model?</p>

          <p>I''ve updated that file... it still seems to mess up with what is code
          and what speech/comments outside of it...... on the plus side.. it does
          stop well (I hadn''t pressed ''Stop'' ) but before it would carry on talking
          about gibberish and then spanish.. which might aswel be gibberish to me
          anyway :D</p>

          <p>Also just as example that question.. and response isn''t really correct
          though the online demo <a href="https://huggingface.co/spaces/HuggingFaceH4/starchat-playground">https://huggingface.co/spaces/HuggingFaceH4/starchat-playground</a>  of
          this beta model(it is the same right? ) does output something that does
          work.. along with correct formatting of code/comments outside of it.. so
          not sure.. </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63bd401a106236e1c152320d/IBts0Y5vND5PgMsjViBoF.png"><img
          alt="firefox_dUK0Smjudt.png" src="https://cdn-uploads.huggingface.co/production/uploads/63bd401a106236e1c152320d/IBts0Y5vND5PgMsjViBoF.png"></a></p>

          '
        raw: "is there an instruction template /mode that should be used on text webui\
          \ for this model?\n\nI've updated that file... it still seems to mess up\
          \ with what is code and what speech/comments outside of it...... on the\
          \ plus side.. it does stop well (I hadn't pressed 'Stop' ) but before it\
          \ would carry on talking about gibberish and then spanish.. which might\
          \ aswel be gibberish to me anyway :D\n\nAlso just as example that question..\
          \ and response isn't really correct though the online demo https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\
          \  of this beta model(it is the same right? ) does output something that\
          \ does work.. along with correct formatting of code/comments outside of\
          \ it.. so not sure.. \n\n![firefox_dUK0Smjudt.png](https://cdn-uploads.huggingface.co/production/uploads/63bd401a106236e1c152320d/IBts0Y5vND5PgMsjViBoF.png)"
        updatedAt: '2023-06-10T17:16:47.506Z'
      numEdits: 1
      reactions: []
    id: 6484afedc641450a44d43d2a
    type: comment
  author: Boffy
  content: "is there an instruction template /mode that should be used on text webui\
    \ for this model?\n\nI've updated that file... it still seems to mess up with\
    \ what is code and what speech/comments outside of it...... on the plus side..\
    \ it does stop well (I hadn't pressed 'Stop' ) but before it would carry on talking\
    \ about gibberish and then spanish.. which might aswel be gibberish to me anyway\
    \ :D\n\nAlso just as example that question.. and response isn't really correct\
    \ though the online demo https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\
    \  of this beta model(it is the same right? ) does output something that does\
    \ work.. along with correct formatting of code/comments outside of it.. so not\
    \ sure.. \n\n![firefox_dUK0Smjudt.png](https://cdn-uploads.huggingface.co/production/uploads/63bd401a106236e1c152320d/IBts0Y5vND5PgMsjViBoF.png)"
  created_at: 2023-06-10 16:16:29+00:00
  edited: true
  hidden: false
  id: 6484afedc641450a44d43d2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-10T17:25:34.000Z'
    data:
      edited: true
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952547550201416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>also seems to be exact same issue as reported here: <a href="https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6">https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6</a>
          with cpu/gpu speeds</p>

          '
        raw: 'also seems to be exact same issue as reported here: https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6
          with cpu/gpu speeds'
        updatedAt: '2023-06-10T17:25:48.392Z'
      numEdits: 1
      reactions: []
    id: 6484b20e437def2ff8eecaab
    type: comment
  author: Boffy
  content: 'also seems to be exact same issue as reported here: https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6
    with cpu/gpu speeds'
  created_at: 2023-06-10 16:25:34+00:00
  edited: true
  hidden: false
  id: 6484b20e437def2ff8eecaab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-11T21:23:30.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.614480197429657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p>It stops now, neat. I used the same prompt <span data-props=\"{&quot;user&quot;:&quot;Boffy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Boffy\"\
          >@<span class=\"underline\">Boffy</span></a></span>\n\n\t</span></span>\
          \ did and this is what I got:</p>\n<p>Common sense questions and answers</p>\n\
          <p>Question: write a C# editor script for Unity that use Sirenix OdinInspector\
          \ and have a list of selectable scenes from the build list with a button\
          \ attribute to load multiple selected scenes from the list<br>Factual answer:\
          \ </p>\n<pre><code class=\"language-cs\"><span class=\"hljs-keyword\">using</span>\
          \ UnityEngine;\n<span class=\"hljs-keyword\">using</span> Sirenix.OdinInspector;\n\
          \n<span class=\"hljs-keyword\">public</span> <span class=\"hljs-keyword\"\
          >class</span> <span class=\"hljs-title\">SceneSelector</span> : <span class=\"\
          hljs-title\">MonoBehaviour</span> {\n    [<span class=\"hljs-meta\">Button</span>]\n\
          \    <span class=\"hljs-function\"><span class=\"hljs-keyword\">public</span>\
          \ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">LoadScenes</span>()</span>\
          \ \n    {\n        <span class=\"hljs-keyword\">foreach</span> (<span class=\"\
          hljs-function\"><span class=\"hljs-keyword\">var</span> scene <span class=\"\
          hljs-keyword\">in</span> <span class=\"hljs-title\">GetComponentsInChildren</span>&lt;<span\
          \ class=\"hljs-title\">SceneInfo</span>&gt;())</span> \n        {\n    \
          \        <span class=\"hljs-keyword\">if</span>(scene.IsSelected) \n   \
          \         {\n                SceneManager.LoadScene(scene.Name);\n     \
          \       }\n        }\n    }\n\n    [<span class=\"hljs-meta\">Title(<span\
          \ class=\"hljs-string\">\"Scenes\"</span>)</span>]\n    <span class=\"hljs-keyword\"\
          >public</span> List&lt;SceneInfo&gt; Scenes = <span class=\"hljs-keyword\"\
          >new</span> List&lt;SceneInfo&gt;();\n}\n\n[<span class=\"hljs-meta\">System.Serializable</span>]\n\
          <span class=\"hljs-keyword\">public</span> <span class=\"hljs-keyword\"\
          >struct</span> SceneInfo \n{\n    <span class=\"hljs-keyword\">public</span>\
          \ <span class=\"hljs-built_in\">string</span> Name;\n    [<span class=\"\
          hljs-meta\">ShowIf(<span class=\"hljs-string\">\"@nameof(ShouldDisplay),HideIf(invertCondition:\
          \ true, OnlyForOneField:true)\"</span>)</span>]\n    <span class=\"hljs-keyword\"\
          >public</span> <span class=\"hljs-built_in\">bool</span> IsSelected;\n \
          \   \n    <span class=\"hljs-keyword\">private</span> <span class=\"hljs-keyword\"\
          >static</span> <span class=\"hljs-built_in\">bool</span> ShouldDisplay =&gt;\
          \ <span class=\"hljs-literal\">false</span>; <span class=\"hljs-comment\"\
          >// your logic here</span>\n}\n</code></pre>\n"
        raw: "It stops now, neat. I used the same prompt @Boffy did and this is what\
          \ I got:\n\nCommon sense questions and answers\n\nQuestion: write a C# editor\
          \ script for Unity that use Sirenix OdinInspector and have a list of selectable\
          \ scenes from the build list with a button attribute to load multiple selected\
          \ scenes from the list\nFactual answer: \n```cs\nusing UnityEngine;\nusing\
          \ Sirenix.OdinInspector;\n\npublic class SceneSelector : MonoBehaviour {\n\
          \    [Button]\n    public void LoadScenes() \n    {\n        foreach (var\
          \ scene in GetComponentsInChildren<SceneInfo>()) \n        {\n         \
          \   if(scene.IsSelected) \n            {\n                SceneManager.LoadScene(scene.Name);\n\
          \            }\n        }\n    }\n\n    [Title(\"Scenes\")]\n    public\
          \ List<SceneInfo> Scenes = new List<SceneInfo>();\n}\n\n[System.Serializable]\n\
          public struct SceneInfo \n{\n    public string Name;\n    [ShowIf(\"@nameof(ShouldDisplay),HideIf(invertCondition:\
          \ true, OnlyForOneField:true)\")]\n    public bool IsSelected;\n    \n \
          \   private static bool ShouldDisplay => false; // your logic here\n}\n\
          ```"
        updatedAt: '2023-06-11T21:23:30.403Z'
      numEdits: 0
      reactions: []
    id: 64863b52435b19d6ae967889
    type: comment
  author: mancub
  content: "It stops now, neat. I used the same prompt @Boffy did and this is what\
    \ I got:\n\nCommon sense questions and answers\n\nQuestion: write a C# editor\
    \ script for Unity that use Sirenix OdinInspector and have a list of selectable\
    \ scenes from the build list with a button attribute to load multiple selected\
    \ scenes from the list\nFactual answer: \n```cs\nusing UnityEngine;\nusing Sirenix.OdinInspector;\n\
    \npublic class SceneSelector : MonoBehaviour {\n    [Button]\n    public void\
    \ LoadScenes() \n    {\n        foreach (var scene in GetComponentsInChildren<SceneInfo>())\
    \ \n        {\n            if(scene.IsSelected) \n            {\n            \
    \    SceneManager.LoadScene(scene.Name);\n            }\n        }\n    }\n\n\
    \    [Title(\"Scenes\")]\n    public List<SceneInfo> Scenes = new List<SceneInfo>();\n\
    }\n\n[System.Serializable]\npublic struct SceneInfo \n{\n    public string Name;\n\
    \    [ShowIf(\"@nameof(ShouldDisplay),HideIf(invertCondition: true, OnlyForOneField:true)\"\
    )]\n    public bool IsSelected;\n    \n    private static bool ShouldDisplay =>\
    \ false; // your logic here\n}\n```"
  created_at: 2023-06-11 20:23:30+00:00
  edited: false
  hidden: false
  id: 64863b52435b19d6ae967889
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T21:25:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640825390815735
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Very cool!</p>

          <p>I will have to try this model out.  I just signed up to the free trial
          of Github Copilot and I''m really impressed with it.  It''s really cool
          having it automatically integrated into VS Studio Code.  Not just the fact
          that it can do common functions, but that it intelligently guesses variable
          names and stuff like that.</p>

          <p>I''m definitely curious to see how StarChat compares.</p>

          <p>What are your impressions of StarChat versus other recent LLMs, like
          WizardLM 30B, Guanaco 30B, etc?  Is it noticeably better at coding than
          them?</p>

          '
        raw: 'Very cool!


          I will have to try this model out.  I just signed up to the free trial of
          Github Copilot and I''m really impressed with it.  It''s really cool having
          it automatically integrated into VS Studio Code.  Not just the fact that
          it can do common functions, but that it intelligently guesses variable names
          and stuff like that.


          I''m definitely curious to see how StarChat compares.


          What are your impressions of StarChat versus other recent LLMs, like WizardLM
          30B, Guanaco 30B, etc?  Is it noticeably better at coding than them?'
        updatedAt: '2023-06-11T21:25:33.273Z'
      numEdits: 0
      reactions: []
    id: 64863bcdbac7613d07998bdb
    type: comment
  author: TheBloke
  content: 'Very cool!


    I will have to try this model out.  I just signed up to the free trial of Github
    Copilot and I''m really impressed with it.  It''s really cool having it automatically
    integrated into VS Studio Code.  Not just the fact that it can do common functions,
    but that it intelligently guesses variable names and stuff like that.


    I''m definitely curious to see how StarChat compares.


    What are your impressions of StarChat versus other recent LLMs, like WizardLM
    30B, Guanaco 30B, etc?  Is it noticeably better at coding than them?'
  created_at: 2023-06-11 20:25:33+00:00
  edited: false
  hidden: false
  id: 64863bcdbac7613d07998bdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
      fullname: Tailor Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjohnson
      type: user
    createdAt: '2023-06-12T15:35:22.000Z'
    data:
      edited: true
      editors:
      - tjohnson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9376522302627563
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
          fullname: Tailor Johnson
          isHf: false
          isPro: false
          name: tjohnson
          type: user
        html: '<p>This GPTQ and the GGML''s just aren''t usable. Ive got a RTX3090
          and core i9 with 128GB ram and at its absolute fastest I can''t run an inference
          example under 15 seconds. If I ask it what a piece of code does depending
          on the token len its 30 seconds to a minute...</p>

          <p>Nonetheless, thank you so much for all that you contribute, it is absolutely
          amazing</p>

          '
        raw: 'This GPTQ and the GGML''s just aren''t usable. Ive got a RTX3090 and
          core i9 with 128GB ram and at its absolute fastest I can''t run an inference
          example under 15 seconds. If I ask it what a piece of code does depending
          on the token len its 30 seconds to a minute...


          Nonetheless, thank you so much for all that you contribute, it is absolutely
          amazing'
        updatedAt: '2023-06-12T15:36:42.183Z'
      numEdits: 1
      reactions: []
    id: 64873b3a89a7434ac981cd61
    type: comment
  author: tjohnson
  content: 'This GPTQ and the GGML''s just aren''t usable. Ive got a RTX3090 and core
    i9 with 128GB ram and at its absolute fastest I can''t run an inference example
    under 15 seconds. If I ask it what a piece of code does depending on the token
    len its 30 seconds to a minute...


    Nonetheless, thank you so much for all that you contribute, it is absolutely amazing'
  created_at: 2023-06-12 14:35:22+00:00
  edited: true
  hidden: false
  id: 64873b3a89a7434ac981cd61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T15:39:04.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.95494544506073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry to hear that!  Not surprised to hear the GGMLs are slow as
          there''s no GPU acceleration in this GGML format yet so it''s all down to
          the CPU, and unfortunately the non-Llama GGML formats have seen very few
          of the extensive performance optimisations being done on the llama.cpp project.</p>

          <p>I''m more surprised that you''re finding the GPTQ so slow.  What tokens/s
          are you getting?</p>

          '
        raw: 'Sorry to hear that!  Not surprised to hear the GGMLs are slow as there''s
          no GPU acceleration in this GGML format yet so it''s all down to the CPU,
          and unfortunately the non-Llama GGML formats have seen very few of the extensive
          performance optimisations being done on the llama.cpp project.


          I''m more surprised that you''re finding the GPTQ so slow.  What tokens/s
          are you getting?'
        updatedAt: '2023-06-12T15:39:04.902Z'
      numEdits: 0
      reactions: []
    id: 64873c1820fb2e386b6963ec
    type: comment
  author: TheBloke
  content: 'Sorry to hear that!  Not surprised to hear the GGMLs are slow as there''s
    no GPU acceleration in this GGML format yet so it''s all down to the CPU, and
    unfortunately the non-Llama GGML formats have seen very few of the extensive performance
    optimisations being done on the llama.cpp project.


    I''m more surprised that you''re finding the GPTQ so slow.  What tokens/s are
    you getting?'
  created_at: 2023-06-12 14:39:04+00:00
  edited: false
  hidden: false
  id: 64873c1820fb2e386b6963ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
      fullname: Tailor Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjohnson
      type: user
    createdAt: '2023-06-12T19:41:09.000Z'
    data:
      edited: false
      editors:
      - tjohnson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8767163753509521
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
          fullname: Tailor Johnson
          isHf: false
          isPro: false
          name: tjohnson
          type: user
        html: "<p>I'm sorry <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ I misspoke.  The issue seems to be less the GPTQ model and more to your\
          \ point with @PanQiWei on Github that the \"CUDA extension is not installed\"\
          . If I run nvcc --version I get what you told <span data-props=\"{&quot;user&quot;:&quot;TheFaheem&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheFaheem\"\
          >@<span class=\"underline\">TheFaheem</span></a></span>\n\n\t</span></span>\
          \ (Cuda compilation tools, release 11.8, V11.8.89. Build cuda_11.8.r11.8/compiler.31833905_0)\
          \ and nvidia-smi reads (CUDA Version: 12.0). I am using CUDA everywhere\
          \ else just fine with torch, other libs, etc. However, the kicker is if\
          \ I load the model using @PanQiWei's AutoGPTQForCausalLM even though it\
          \ says CUDA extension is not installed I get 10068MiB/ 24576MiB on my nvidia-smi\
          \ so its clearly loading the model in vram.</p>\n"
        raw: 'I''m sorry @TheBloke, I misspoke.  The issue seems to be less the GPTQ
          model and more to your point with @PanQiWei on Github that the "CUDA extension
          is not installed". If I run nvcc --version I get what you told @TheFaheem
          (Cuda compilation tools, release 11.8, V11.8.89. Build cuda_11.8.r11.8/compiler.31833905_0)
          and nvidia-smi reads (CUDA Version: 12.0). I am using CUDA everywhere else
          just fine with torch, other libs, etc. However, the kicker is if I load
          the model using @PanQiWei''s AutoGPTQForCausalLM even though it says CUDA
          extension is not installed I get 10068MiB/ 24576MiB on my nvidia-smi so
          its clearly loading the model in vram.'
        updatedAt: '2023-06-12T19:41:09.691Z'
      numEdits: 0
      reactions: []
    id: 648774d5e2bc0e1c3f17adb4
    type: comment
  author: tjohnson
  content: 'I''m sorry @TheBloke, I misspoke.  The issue seems to be less the GPTQ
    model and more to your point with @PanQiWei on Github that the "CUDA extension
    is not installed". If I run nvcc --version I get what you told @TheFaheem (Cuda
    compilation tools, release 11.8, V11.8.89. Build cuda_11.8.r11.8/compiler.31833905_0)
    and nvidia-smi reads (CUDA Version: 12.0). I am using CUDA everywhere else just
    fine with torch, other libs, etc. However, the kicker is if I load the model using
    @PanQiWei''s AutoGPTQForCausalLM even though it says CUDA extension is not installed
    I get 10068MiB/ 24576MiB on my nvidia-smi so its clearly loading the model in
    vram.'
  created_at: 2023-06-12 18:41:09+00:00
  edited: false
  hidden: false
  id: 648774d5e2bc0e1c3f17adb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T19:50:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957878053188324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK yeah I thought it might be that.  Can you try compiling from
          source:</p>

          <pre><code>pip uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          git checkout v0.2.1

          pip install .

          </code></pre>

          <p>Then testing again.</p>

          <p>The issue with no CUDA extension doesn''t affect the model loading. But
          it does affect all the mathematical calculations that have to be done to
          read the GPTQ.  Hence the awful performance - they''re being done on CPU
          instead.</p>

          <p>This is quite a common problem at the moment, as you saw from those issues
          on the AutoGPTQ repo.  Unfortuantely PanQiWei doesn''t seem very active
          at the moment, so they''re not really being investigated.</p>

          <p>Hopefully compiling from source will fix this.</p>

          '
        raw: 'OK yeah I thought it might be that.  Can you try compiling from source:

          ```

          pip uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          git checkout v0.2.1

          pip install .

          ```


          Then testing again.


          The issue with no CUDA extension doesn''t affect the model loading. But
          it does affect all the mathematical calculations that have to be done to
          read the GPTQ.  Hence the awful performance - they''re being done on CPU
          instead.


          This is quite a common problem at the moment, as you saw from those issues
          on the AutoGPTQ repo.  Unfortuantely PanQiWei doesn''t seem very active
          at the moment, so they''re not really being investigated.


          Hopefully compiling from source will fix this.'
        updatedAt: '2023-06-12T19:50:45.170Z'
      numEdits: 0
      reactions: []
    id: 6487771520fb2e386b790a10
    type: comment
  author: TheBloke
  content: 'OK yeah I thought it might be that.  Can you try compiling from source:

    ```

    pip uninstall -y auto-gptq

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    git checkout v0.2.1

    pip install .

    ```


    Then testing again.


    The issue with no CUDA extension doesn''t affect the model loading. But it does
    affect all the mathematical calculations that have to be done to read the GPTQ.  Hence
    the awful performance - they''re being done on CPU instead.


    This is quite a common problem at the moment, as you saw from those issues on
    the AutoGPTQ repo.  Unfortuantely PanQiWei doesn''t seem very active at the moment,
    so they''re not really being investigated.


    Hopefully compiling from source will fix this.'
  created_at: 2023-06-12 18:50:45+00:00
  edited: false
  hidden: false
  id: 6487771520fb2e386b790a10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
      fullname: Tailor Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjohnson
      type: user
    createdAt: '2023-06-12T20:09:56.000Z'
    data:
      edited: false
      editors:
      - tjohnson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8956071138381958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
          fullname: Tailor Johnson
          isHf: false
          isPro: false
          name: tjohnson
          type: user
        html: "<p>Where would any of us be in this world without <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span>.  That was it...</p>\n<p>Merci beaucoup mon ami!</p>\n"
        raw: 'Where would any of us be in this world without @TheBloke.  That was
          it...


          Merci beaucoup mon ami!'
        updatedAt: '2023-06-12T20:09:56.207Z'
      numEdits: 0
      reactions: []
    id: 64877b94d67acb8ab6eaac13
    type: comment
  author: tjohnson
  content: 'Where would any of us be in this world without @TheBloke.  That was it...


    Merci beaucoup mon ami!'
  created_at: 2023-06-12 19:09:56+00:00
  edited: false
  hidden: false
  id: 64877b94d67acb8ab6eaac13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-13T01:13:06.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9649127721786499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I always compile from source so not sure what''s available pre-made,
          but AutoGPTQ is up to 0.2.3 now. I don''t think we should be pulling down
          the old 0.2.1 version, no?</p>

          <p>Though, I think AutoGPTQ is slower than GPTQ-for-LLaMa, or maybe that''s
          the perception I''m getting...hmmm.</p>

          '
        raw: 'I always compile from source so not sure what''s available pre-made,
          but AutoGPTQ is up to 0.2.3 now. I don''t think we should be pulling down
          the old 0.2.1 version, no?


          Though, I think AutoGPTQ is slower than GPTQ-for-LLaMa, or maybe that''s
          the perception I''m getting...hmmm.'
        updatedAt: '2023-06-13T01:13:06.861Z'
      numEdits: 0
      reactions: []
    id: 6487c2a2fa23bdf0ada9adce
    type: comment
  author: mancub
  content: 'I always compile from source so not sure what''s available pre-made, but
    AutoGPTQ is up to 0.2.3 now. I don''t think we should be pulling down the old
    0.2.1 version, no?


    Though, I think AutoGPTQ is slower than GPTQ-for-LLaMa, or maybe that''s the perception
    I''m getting...hmmm.'
  created_at: 2023-06-13 00:13:06+00:00
  edited: false
  hidden: false
  id: 6487c2a2fa23bdf0ada9adce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-13T12:57:48.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32836857438087463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: "<p>Tried updating things.. now I get..</p>\n<p>Traceback (most recent\
          \ call last): File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\\
          server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 94, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 296, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          C:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens\
          \ = model.model.model.embed_tokens File \u201CC:\\Projects\\AI\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019</p>\n"
        raw: "Tried updating things.. now I get..\n\nTraceback (most recent call last):\
          \ File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\\
          server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 94, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 296, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          C:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens\
          \ = model.model.model.embed_tokens File \u201CC:\\Projects\\AI\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019"
        updatedAt: '2023-06-13T12:57:48.800Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kenM1
    id: 648867ccfafabd4c297ba50d
    type: comment
  author: Boffy
  content: "Tried updating things.. now I get..\n\nTraceback (most recent call last):\
    \ File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\server.py\u201D\
    , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\\
    models.py\u201D, line 94, in load_model output = load_func(model_name) File \u201C\
    C:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\models.py\u201D\
    , line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \ File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\\
    AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
    \ File \u201CC:\\Projects\\AI\\one-click-installers\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\u201D, line 1614, in getattr raise\
    \ AttributeError(\u201C\u2018{}\u2019 object has no attribute \u2018{}\u2019\u201D\
    .format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019 object has no attribute\
    \ \u2018model\u2019"
  created_at: 2023-06-13 11:57:48+00:00
  edited: false
  hidden: false
  id: 648867ccfafabd4c297ba50d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
      fullname: Ken M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kenM1
      type: user
    createdAt: '2023-06-14T02:32:34.000Z'
    data:
      edited: false
      editors:
      - kenM1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41553011536598206
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
          fullname: Ken M
          isHf: false
          isPro: false
          name: kenM1
          type: user
        html: "<blockquote>\n<p>Tried updating things.. now I get..</p>\n<p>Traceback\
          \ (most recent call last): File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\server.py\u201D, line 70, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Projects\\\
          AI\\one-click-installers\\text-generation-webui\\modules\\models.py\u201D\
          , line 94, in load_model output = load_func(model_name) File \u201CC:\\\
          Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\models.py\u201D\
          , line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\\
          modules\\AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens\
          \ = model.model.model.embed_tokens File \u201CC:\\Projects\\AI\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019</p>\n</blockquote>\n<p>I am\
          \ having the same issue.<br>anyone have a work around this issue?</p>\n"
        raw: "> Tried updating things.. now I get..\n> \n> Traceback (most recent\
          \ call last): File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\\
          server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 94, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\u201D, line 296, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          C:\\Projects\\AI\\one-click-installers\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens\
          \ = model.model.model.embed_tokens File \u201CC:\\Projects\\AI\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019\n\nI am having the same issue.\n\
          anyone have a work around this issue?"
        updatedAt: '2023-06-14T02:32:34.982Z'
      numEdits: 0
      reactions: []
    id: 648926c21cff184698f48d2d
    type: comment
  author: kenM1
  content: "> Tried updating things.. now I get..\n> \n> Traceback (most recent call\
    \ last): File \u201CC:\\Projects\\AI\\one-click-installers\\text-generation-webui\\\
    server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
    \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
    text-generation-webui\\modules\\models.py\u201D, line 94, in load_model output\
    \ = load_func(model_name) File \u201CC:\\Projects\\AI\\one-click-installers\\\
    text-generation-webui\\modules\\models.py\u201D, line 296, in AutoGPTQ_loader\
    \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201CC:\\Projects\\\
    AI\\one-click-installers\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
    , line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
    \ File \u201CC:\\Projects\\AI\\one-click-installers\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\u201D, line 1614, in getattr raise\
    \ AttributeError(\u201C\u2018{}\u2019 object has no attribute \u2018{}\u2019\u201D\
    .format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019 object has no attribute\
    \ \u2018model\u2019\n\nI am having the same issue.\nanyone have a work around\
    \ this issue?"
  created_at: 2023-06-14 01:32:34+00:00
  edited: false
  hidden: false
  id: 648926c21cff184698f48d2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-14T18:33:51.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4835266172885895
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>I have found a workaround below ... oobagooba text webui seems to
          get broken constantly.. </p>

          <p>AutoGPTQ_loader.py</p>

          <p><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961">https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961</a></p>

          <pre><code># # These lines fix the multimodal extension when used with AutoGPTQ

          # if not hasattr(model, ''dtype''):

          #     model.dtype = model.model.dtype


          # if not hasattr(model, ''embed_tokens''):

          #     model.embed_tokens = model.model.model.embed_tokens


          # if not hasattr(model.model, ''embed_tokens''):

          #     model.model.embed_tokens = model.model.model.embed_tokens

          </code></pre>

          '
        raw: "I have found a workaround below ... oobagooba text webui seems to get\
          \ broken constantly.. \n\nAutoGPTQ_loader.py\n\nhttps://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961\n\
          \n    # # These lines fix the multimodal extension when used with AutoGPTQ\n\
          \    # if not hasattr(model, 'dtype'):\n    #     model.dtype = model.model.dtype\n\
          \n    # if not hasattr(model, 'embed_tokens'):\n    #     model.embed_tokens\
          \ = model.model.model.embed_tokens\n\n    # if not hasattr(model.model,\
          \ 'embed_tokens'):\n    #     model.model.embed_tokens = model.model.model.embed_tokens"
        updatedAt: '2023-06-14T18:33:51.581Z'
      numEdits: 0
      reactions: []
    id: 648a080f3688b02f2a5cbe54
    type: comment
  author: Boffy
  content: "I have found a workaround below ... oobagooba text webui seems to get\
    \ broken constantly.. \n\nAutoGPTQ_loader.py\n\nhttps://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961\n\
    \n    # # These lines fix the multimodal extension when used with AutoGPTQ\n \
    \   # if not hasattr(model, 'dtype'):\n    #     model.dtype = model.model.dtype\n\
    \n    # if not hasattr(model, 'embed_tokens'):\n    #     model.embed_tokens =\
    \ model.model.model.embed_tokens\n\n    # if not hasattr(model.model, 'embed_tokens'):\n\
    \    #     model.model.embed_tokens = model.model.model.embed_tokens"
  created_at: 2023-06-14 17:33:51+00:00
  edited: false
  hidden: false
  id: 648a080f3688b02f2a5cbe54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-14T18:34:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8189498782157898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yup that''s the fix for the moment until ooba fixes text-gen</p>

          '
        raw: Yup that's the fix for the moment until ooba fixes text-gen
        updatedAt: '2023-06-14T18:34:28.977Z'
      numEdits: 0
      reactions: []
    id: 648a0834fe11ebd748966911
    type: comment
  author: TheBloke
  content: Yup that's the fix for the moment until ooba fixes text-gen
  created_at: 2023-06-14 17:34:28+00:00
  edited: false
  hidden: false
  id: 648a0834fe11ebd748966911
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-14T18:38:27.000Z'
    data:
      edited: true
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9819818139076233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>I hope the fixes will fix performance for gpu''s on windows.. that''s
          another area with cuda etc versions etc  getting it all working with various
          diffferent versions is a mess, does anyone know if WSL is better or Linux?
          I''d be temped to dual boot if it was to see just cus it''s  shame to see
          a 4090 only being used 30% guess I wouldn''t care but the text response
          rate is only like 5t/s :(  </p>

          '
        raw: 'I hope the fixes will fix performance for gpu''s on windows.. that''s
          another area with cuda etc versions etc  getting it all working with various
          diffferent versions is a mess, does anyone know if WSL is better or Linux?
          I''d be temped to dual boot if it was to see just cus it''s  shame to see
          a 4090 only being used 30% guess I wouldn''t care but the text response
          rate is only like 5t/s :(  '
        updatedAt: '2023-06-14T18:38:45.544Z'
      numEdits: 1
      reactions: []
    id: 648a0923304a0e50d2a25ecd
    type: comment
  author: Boffy
  content: 'I hope the fixes will fix performance for gpu''s on windows.. that''s
    another area with cuda etc versions etc  getting it all working with various diffferent
    versions is a mess, does anyone know if WSL is better or Linux? I''d be temped
    to dual boot if it was to see just cus it''s  shame to see a 4090 only being used
    30% guess I wouldn''t care but the text response rate is only like 5t/s :(  '
  created_at: 2023-06-14 17:38:27+00:00
  edited: true
  hidden: false
  id: 648a0923304a0e50d2a25ecd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-15T00:17:12.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9647210240364075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I did not find the native Linux being better than WSL.</p>

          <p>Matter a fact I couldn''t fully load the models in Linux I used to load
          in WSL just fine, because X/Wayland was taking more VRAM away than Windows
          GUI.</p>

          <p>Obviously I could init 3, but then without a browser there''s no access
          to any UI''s like text-gen-webui, and stuck in CLI.</p>

          '
        raw: 'I did not find the native Linux being better than WSL.


          Matter a fact I couldn''t fully load the models in Linux I used to load
          in WSL just fine, because X/Wayland was taking more VRAM away than Windows
          GUI.


          Obviously I could init 3, but then without a browser there''s no access
          to any UI''s like text-gen-webui, and stuck in CLI.'
        updatedAt: '2023-06-15T00:17:12.296Z'
      numEdits: 0
      reactions: []
    id: 648a5888fcda61143195322c
    type: comment
  author: mancub
  content: 'I did not find the native Linux being better than WSL.


    Matter a fact I couldn''t fully load the models in Linux I used to load in WSL
    just fine, because X/Wayland was taking more VRAM away than Windows GUI.


    Obviously I could init 3, but then without a browser there''s no access to any
    UI''s like text-gen-webui, and stuck in CLI.'
  created_at: 2023-06-14 23:17:12+00:00
  edited: false
  hidden: false
  id: 648a5888fcda61143195322c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-06-15T00:45:14.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9546166062355042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<blockquote>

          <p>I did not find the native Linux being better than WSL.</p>

          <p>Matter a fact I couldn''t fully load the models in Linux I used to load
          in WSL just fine, because X/Wayland was taking more VRAM away than Windows
          GUI.</p>

          <p>Obviously I could init 3, but then without a browser there''s no access
          to any UI''s like text-gen-webui, and stuck in CLI.</p>

          </blockquote>

          <p>Or ''share'' and use a 2nd machine with a browser..   That is what i
          do.</p>

          '
        raw: "> I did not find the native Linux being better than WSL.\n> \n> Matter\
          \ a fact I couldn't fully load the models in Linux I used to load in WSL\
          \ just fine, because X/Wayland was taking more VRAM away than Windows GUI.\n\
          > \n> Obviously I could init 3, but then without a browser there's no access\
          \ to any UI's like text-gen-webui, and stuck in CLI.\n\nOr 'share' and use\
          \ a 2nd machine with a browser..   That is what i do."
        updatedAt: '2023-06-15T00:45:14.075Z'
      numEdits: 0
      reactions: []
    id: 648a5f1a273ac036cb249ac6
    type: comment
  author: Nurb432
  content: "> I did not find the native Linux being better than WSL.\n> \n> Matter\
    \ a fact I couldn't fully load the models in Linux I used to load in WSL just\
    \ fine, because X/Wayland was taking more VRAM away than Windows GUI.\n> \n> Obviously\
    \ I could init 3, but then without a browser there's no access to any UI's like\
    \ text-gen-webui, and stuck in CLI.\n\nOr 'share' and use a 2nd machine with a\
    \ browser..   That is what i do."
  created_at: 2023-06-14 23:45:14+00:00
  edited: false
  hidden: false
  id: 648a5f1a273ac036cb249ac6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/starchat-beta-GPTQ
repo_type: model
status: open
target_branch: null
title: is this working properly?
