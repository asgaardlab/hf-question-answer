!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kenM1
conflicting_files: null
created_at: 2023-06-14 14:23:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
      fullname: Ken M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kenM1
      type: user
    createdAt: '2023-06-14T15:23:14.000Z'
    data:
      edited: false
      editors:
      - kenM1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47145116329193115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
          fullname: Ken M
          isHf: false
          isPro: false
          name: kenM1
          type: user
        html: "<p>Does anyone know how do i fix this issue?<br>Im getting:<br>2023-06-14\
          \ 12:21:02 WARNING:The safetensors archive passed at models\\TheBloke_starchat-beta-GPTQ\\\
          gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to\
          \ save your model with the <code>save_pretrained</code> method. Defaulting\
          \ to 'pt' metadata.<br>2023-06-14 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM\
          \ hasn't fused attention module yet, will skip inject fused attention.<br>2023-06-14\
          \ 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM hasn't fused mlp module yet,\
          \ will skip inject fused mlp.<br>When i try to load the model i get:</p>\n\
          <p>Traceback (most recent call last): File \u201CC:\\Users\\me\\text-generation-webui\\\
          server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\me\\text-generation-webui\\\
          modules\\models.py\u201D, line 94, in load_model output = load_func(model_name)\
          \ File \u201CC:\\Users\\me\\text-generation-webui\\modules\\models.py\u201D\
          , line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\me\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
          \ File \u201CC:\\Users\\me\\miniconda3\\envs\\textgen\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\u201D, line 1614, in getattr raise AttributeError(\u201C\
          \u2018{}\u2019 object has no attribute \u2018{}\u2019\u201D.format( AttributeError:\
          \ \u2018GPTBigCodeForCausalLM\u2019 object has no attribute \u2018model\u2019\
          </p>\n<p>Any help would be appreciated.</p>\n"
        raw: "Does anyone know how do i fix this issue?\r\nIm getting:\r\n2023-06-14\
          \ 12:21:02 WARNING:The safetensors archive passed at models\\TheBloke_starchat-beta-GPTQ\\\
          gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to\
          \ save your model with the `save_pretrained` method. Defaulting to 'pt'\
          \ metadata.\r\n2023-06-14 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM hasn't\
          \ fused attention module yet, will skip inject fused attention.\r\n2023-06-14\
          \ 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM hasn't fused mlp module yet,\
          \ will skip inject fused mlp.\r\nWhen i try to load the model i get:\r\n\
          \r\nTraceback (most recent call last): File \u201CC:\\Users\\me\\text-generation-webui\\\
          server.py\u201D, line 70, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\me\\text-generation-webui\\\
          modules\\models.py\u201D, line 94, in load_model output = load_func(model_name)\
          \ File \u201CC:\\Users\\me\\text-generation-webui\\modules\\models.py\u201D\
          , line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\me\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
          \ File \u201CC:\\Users\\me\\miniconda3\\envs\\textgen\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\u201D, line 1614, in getattr raise AttributeError(\u201C\
          \u2018{}\u2019 object has no attribute \u2018{}\u2019\u201D.format( AttributeError:\
          \ \u2018GPTBigCodeForCausalLM\u2019 object has no attribute \u2018model\u2019\
          \r\n\r\nAny help would be appreciated."
        updatedAt: '2023-06-14T15:23:14.785Z'
      numEdits: 0
      reactions: []
    id: 6489db624344b9f97db195b8
    type: comment
  author: kenM1
  content: "Does anyone know how do i fix this issue?\r\nIm getting:\r\n2023-06-14\
    \ 12:21:02 WARNING:The safetensors archive passed at models\\TheBloke_starchat-beta-GPTQ\\\
    gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to save your\
    \ model with the `save_pretrained` method. Defaulting to 'pt' metadata.\r\n2023-06-14\
    \ 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM hasn't fused attention module yet,\
    \ will skip inject fused attention.\r\n2023-06-14 12:21:07 WARNING:GPTBigCodeGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\r\nWhen i try to load\
    \ the model i get:\r\n\r\nTraceback (most recent call last): File \u201CC:\\Users\\\
    me\\text-generation-webui\\server.py\u201D, line 70, in load_model_wrapper shared.model,\
    \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Users\\me\\\
    text-generation-webui\\modules\\models.py\u201D, line 94, in load_model output\
    \ = load_func(model_name) File \u201CC:\\Users\\me\\text-generation-webui\\modules\\\
    models.py\u201D, line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \ File \u201CC:\\Users\\me\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
    , line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
    \ File \u201CC:\\Users\\me\\miniconda3\\envs\\textgen\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\u201D, line 1614, in getattr raise AttributeError(\u201C\
    \u2018{}\u2019 object has no attribute \u2018{}\u2019\u201D.format( AttributeError:\
    \ \u2018GPTBigCodeForCausalLM\u2019 object has no attribute \u2018model\u2019\r\
    \n\r\nAny help would be appreciated."
  created_at: 2023-06-14 14:23:14+00:00
  edited: false
  hidden: false
  id: 6489db624344b9f97db195b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-14T17:55:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6490000486373901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is a new bug in text-generation-webui</p>

          <p>Until a proper fix is introduced, please follow the instructions at the
          end of this issue thread: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961">https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961</a></p>

          '
        raw: 'This is a new bug in text-generation-webui


          Until a proper fix is introduced, please follow the instructions at the
          end of this issue thread: https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961'
        updatedAt: '2023-06-14T17:55:07.954Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kenM1
    id: 6489fefb29d847e8b5d6a4a0
    type: comment
  author: TheBloke
  content: 'This is a new bug in text-generation-webui


    Until a proper fix is introduced, please follow the instructions at the end of
    this issue thread: https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961'
  created_at: 2023-06-14 16:55:07+00:00
  edited: false
  hidden: false
  id: 6489fefb29d847e8b5d6a4a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
      fullname: Ken M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kenM1
      type: user
    createdAt: '2023-06-14T22:39:12.000Z'
    data:
      edited: false
      editors:
      - kenM1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7306249141693115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c39eff83e558c8d9e4053a8d1d0ad80b.svg
          fullname: Ken M
          isHf: false
          isPro: false
          name: kenM1
          type: user
        html: '<blockquote>

          <p>This is a new bug in text-generation-webui</p>

          <p>Until a proper fix is introduced, please follow the instructions at the
          end of this issue thread: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961">https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961</a></p>

          </blockquote>

          <p>Successfully loaded TheBloke_starchat-beta-GPTQ</p>

          <p>Here''s a easy guide for anyone looking for a way to find file name:
          AutoGPTQ_loader.py<br>My was located at: C:\Users\me\text-generation-webui\modules<br>this
          line is located at the bottom of the file:</p>

          <pre><code># These lines fix the multimodal extension when used with AutoGPTQ

          # if not hasattr(model, ''dtype''):

          #     model.dtype = model.model.dtype


          # if not hasattr(model, ''embed_tokens''):

          #     model.embed_tokens = model.model.model.embed_tokens


          # if not hasattr(model.model, ''embed_tokens''):

          #     model.model.embed_tokens = model.model.model.embed_tokens

          </code></pre>

          <p>after its been edited i simply loaded the model and:</p>

          <p>Successfully loaded TheBloke_starchat-beta-GPTQ</p>

          '
        raw: "> This is a new bug in text-generation-webui\n> \n> Until a proper fix\
          \ is introduced, please follow the instructions at the end of this issue\
          \ thread: https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961\n\
          \nSuccessfully loaded TheBloke_starchat-beta-GPTQ\n\nHere's a easy guide\
          \ for anyone looking for a way to find file name: AutoGPTQ_loader.py\nMy\
          \ was located at: C:\\Users\\me\\text-generation-webui\\modules\nthis line\
          \ is located at the bottom of the file:\n\n\n    # These lines fix the multimodal\
          \ extension when used with AutoGPTQ\n    # if not hasattr(model, 'dtype'):\n\
          \    #     model.dtype = model.model.dtype\n\n    # if not hasattr(model,\
          \ 'embed_tokens'):\n    #     model.embed_tokens = model.model.model.embed_tokens\n\
          \n    # if not hasattr(model.model, 'embed_tokens'):\n    #     model.model.embed_tokens\
          \ = model.model.model.embed_tokens\n\nafter its been edited i simply loaded\
          \ the model and:\n\nSuccessfully loaded TheBloke_starchat-beta-GPTQ\n"
        updatedAt: '2023-06-14T22:39:12.640Z'
      numEdits: 0
      reactions: []
    id: 648a41908920f2eef12f1a8d
    type: comment
  author: kenM1
  content: "> This is a new bug in text-generation-webui\n> \n> Until a proper fix\
    \ is introduced, please follow the instructions at the end of this issue thread:\
    \ https://github.com/oobabooga/text-generation-webui/issues/2655#issuecomment-1590895961\n\
    \nSuccessfully loaded TheBloke_starchat-beta-GPTQ\n\nHere's a easy guide for anyone\
    \ looking for a way to find file name: AutoGPTQ_loader.py\nMy was located at:\
    \ C:\\Users\\me\\text-generation-webui\\modules\nthis line is located at the bottom\
    \ of the file:\n\n\n    # These lines fix the multimodal extension when used with\
    \ AutoGPTQ\n    # if not hasattr(model, 'dtype'):\n    #     model.dtype = model.model.dtype\n\
    \n    # if not hasattr(model, 'embed_tokens'):\n    #     model.embed_tokens =\
    \ model.model.model.embed_tokens\n\n    # if not hasattr(model.model, 'embed_tokens'):\n\
    \    #     model.model.embed_tokens = model.model.model.embed_tokens\n\nafter\
    \ its been edited i simply loaded the model and:\n\nSuccessfully loaded TheBloke_starchat-beta-GPTQ\n"
  created_at: 2023-06-14 21:39:12+00:00
  edited: false
  hidden: false
  id: 648a41908920f2eef12f1a8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:37:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8760638236999512
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks for the guide, kenM1.</p>

          <p>Future users: note that the bug in text-generation-webui has now been
          fixed, so all that is required to get it to working now is to update to
          the latest version of text-generation-webui</p>

          '
        raw: 'Thanks for the guide, kenM1.


          Future users: note that the bug in text-generation-webui has now been fixed,
          so all that is required to get it to working now is to update to the latest
          version of text-generation-webui'
        updatedAt: '2023-06-16T11:37:35.121Z'
      numEdits: 0
      reactions: []
    id: 648c497fe549be47af19fdc6
    type: comment
  author: TheBloke
  content: 'Thanks for the guide, kenM1.


    Future users: note that the bug in text-generation-webui has now been fixed, so
    all that is required to get it to working now is to update to the latest version
    of text-generation-webui'
  created_at: 2023-06-16 10:37:35+00:00
  edited: false
  hidden: false
  id: 648c497fe549be47af19fdc6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/starchat-beta-GPTQ
repo_type: model
status: open
target_branch: null
title: gptq_model-4bit--1g.safetensors does not contain metadata.
