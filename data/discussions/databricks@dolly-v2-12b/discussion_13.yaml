!!python/object:huggingface_hub.community.DiscussionWithDetails
author: seadude
conflicting_files: null
created_at: 2023-04-13 04:13:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-04-13T05:13:44.000Z'
    data:
      edited: true
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>Hello, </p>

          <p>Being a new user of self-hosted LLM''s, I need a little more info on
          the Usage steps.</p>

          <p><strong>Assuming I have a VM or Databricks instance with GPU''s, do I</strong>:</p>

          <ol>

          <li>Clone the dolly repo to a directory on the VM/Databricks filesystem?</li>

          </ol>

          <ul>

          <li>I don''t see "the model" in <a rel="nofollow" href="https://github.com/databrickslabs/dolly">your
          repo here</a> (I''m probably missing something fundamental here!)</li>

          <li>Do I need to download it to the VM/Databricks filesystem from somewhere
          else?</li>

          </ul>

          <ol start="2">

          <li><p>Create a virtual environment (<code>python -m venv .venv</code>)?</p>

          </li>

          <li><p>Run <code>pip install accelerate&gt;=0.12.0 transformers[torch]==4.25.1</code></p>

          </li>

          <li><p>For theses instructions, does <code>model=</code> point to the path
          I downloaded the model to?</p>

          </li>

          </ol>

          <pre><code>import torch

          from transformers import pipeline


          generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")

          </code></pre>

          <p>I think answers to these fundamental questions would help quickstart
          experimentation even faster.</p>

          <p>Thank you!</p>

          '
        raw: "Hello, \n\nBeing a new user of self-hosted LLM's, I need a little more\
          \ info on the Usage steps.\n\n**Assuming I have a VM or Databricks instance\
          \ with GPU's, do I**:\n\n1. Clone the dolly repo to a directory on the VM/Databricks\
          \ filesystem?\n  - I don't see \"the model\" in [your repo here](https://github.com/databrickslabs/dolly)\
          \ (I'm probably missing something fundamental here!)\n  - Do I need to download\
          \ it to the VM/Databricks filesystem from somewhere else?\n\n2. Create a\
          \ virtual environment (`python -m venv .venv`)?\n\n3. Run `pip install accelerate>=0.12.0\
          \ transformers[torch]==4.25.1`\n\n4. For theses instructions, does `model=`\
          \ point to the path I downloaded the model to?\n```\nimport torch\nfrom\
          \ transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\n```\n\nI think answers to these fundamental questions would help quickstart\
          \ experimentation even faster.\n\nThank you!"
        updatedAt: '2023-04-13T06:01:59.214Z'
      numEdits: 3
      reactions: []
    id: 64378f88a701a7e744c284aa
    type: comment
  author: seadude
  content: "Hello, \n\nBeing a new user of self-hosted LLM's, I need a little more\
    \ info on the Usage steps.\n\n**Assuming I have a VM or Databricks instance with\
    \ GPU's, do I**:\n\n1. Clone the dolly repo to a directory on the VM/Databricks\
    \ filesystem?\n  - I don't see \"the model\" in [your repo here](https://github.com/databrickslabs/dolly)\
    \ (I'm probably missing something fundamental here!)\n  - Do I need to download\
    \ it to the VM/Databricks filesystem from somewhere else?\n\n2. Create a virtual\
    \ environment (`python -m venv .venv`)?\n\n3. Run `pip install accelerate>=0.12.0\
    \ transformers[torch]==4.25.1`\n\n4. For theses instructions, does `model=` point\
    \ to the path I downloaded the model to?\n```\nimport torch\nfrom transformers\
    \ import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n```\n\
    \nI think answers to these fundamental questions would help quickstart experimentation\
    \ even faster.\n\nThank you!"
  created_at: 2023-04-13 04:13:44+00:00
  edited: true
  hidden: false
  id: 64378f88a701a7e744c284aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-04-13T06:07:24.000Z'
    data:
      edited: true
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>(Silly rabbit...)</p>

          <p>RE:  <a href="/databricks/dolly-v2-12b/discussions/1">#1</a>:  No need
          to clone the <code>dolly</code> repo</p>

          <ul>

          <li>"The model" is handled by the (HuggingFaces) <code>transformers</code>
          library (see <a href="/databricks/dolly-v2-12b/discussions/4">#4</a> below)</li>

          </ul>

          <p>RE: <a href="/databricks/dolly-v2-12b/discussions/2">#2</a>: I still
          don''t know how DataBricks handles venv''s...</p>

          <p>RE: <a href="/databricks/dolly-v2-12b/discussions/3">#3</a>: Yes, put
          this into the DataBricks notebook</p>

          <p>RE: <a href="/databricks/dolly-v2-12b/discussions/4">#4</a>: The <code>model=</code>
          points to the model that is hosted in HuggingFace(I think)</p>

          <ul>

          <li>The model is pulled in as part of the <code>pipeline()</code> function</li>

          <li><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6434d3bc95b8ab04938cd4da/EI82XGJjPG--1TiLtcwph.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6434d3bc95b8ab04938cd4da/EI82XGJjPG--1TiLtcwph.png"></a></li>

          </ul>

          '
        raw: '(Silly rabbit...)


          RE:  #1:  No need to clone the `dolly` repo

          - "The model" is handled by the (HuggingFaces) `transformers` library (see
          #4 below)


          RE: #2: I still don''t know how DataBricks handles venv''s...


          RE: #3: Yes, put this into the DataBricks notebook


          RE: #4: The `model=` points to the model that is hosted in HuggingFace(I
          think)

          - The model is pulled in as part of the `pipeline()` function

          - ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6434d3bc95b8ab04938cd4da/EI82XGJjPG--1TiLtcwph.png)'
        updatedAt: '2023-04-13T06:07:39.520Z'
      numEdits: 1
      reactions: []
    id: 64379c1cf8a71f96bcdaf973
    type: comment
  author: seadude
  content: '(Silly rabbit...)


    RE:  #1:  No need to clone the `dolly` repo

    - "The model" is handled by the (HuggingFaces) `transformers` library (see #4
    below)


    RE: #2: I still don''t know how DataBricks handles venv''s...


    RE: #3: Yes, put this into the DataBricks notebook


    RE: #4: The `model=` points to the model that is hosted in HuggingFace(I think)

    - The model is pulled in as part of the `pipeline()` function

    - ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6434d3bc95b8ab04938cd4da/EI82XGJjPG--1TiLtcwph.png)'
  created_at: 2023-04-13 05:07:24+00:00
  edited: true
  hidden: false
  id: 64379c1cf8a71f96bcdaf973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-13T06:12:51.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>If you are using a runtime like <code>12.2 LTS ML (includes Apache
          Spark 3.3.2, GPU, Scala 2.12)</code> in Databricks then you just need to
          pip install those two dependencies.  Actually I think transformers is likely
          already installed by default for that runtime.  So you might just need to
          pip install accelerate.   You don''t need to clone the repo or set up a
          virtual environment.  The Databricks cluster already sets up a venv for
          you with most packages  you''d need already installed.   So steps 1 and
          2 you list are not necessary.  If you copy and paste the code from step
          4 into a cell and run it then it should just work.  The Hugging Face libraries
          will download everything you need, including the model.  Hope that helps.</p>

          '
        raw: If you are using a runtime like `12.2 LTS ML (includes Apache Spark 3.3.2,
          GPU, Scala 2.12)` in Databricks then you just need to pip install those
          two dependencies.  Actually I think transformers is likely already installed
          by default for that runtime.  So you might just need to pip install accelerate.   You
          don't need to clone the repo or set up a virtual environment.  The Databricks
          cluster already sets up a venv for you with most packages  you'd need already
          installed.   So steps 1 and 2 you list are not necessary.  If you copy and
          paste the code from step 4 into a cell and run it then it should just work.  The
          Hugging Face libraries will download everything you need, including the
          model.  Hope that helps.
        updatedAt: '2023-04-13T06:12:51.365Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - seadude
    id: 64379d6340bf2c4964f2f2c5
    type: comment
  author: matthayes
  content: If you are using a runtime like `12.2 LTS ML (includes Apache Spark 3.3.2,
    GPU, Scala 2.12)` in Databricks then you just need to pip install those two dependencies.  Actually
    I think transformers is likely already installed by default for that runtime.  So
    you might just need to pip install accelerate.   You don't need to clone the repo
    or set up a virtual environment.  The Databricks cluster already sets up a venv
    for you with most packages  you'd need already installed.   So steps 1 and 2 you
    list are not necessary.  If you copy and paste the code from step 4 into a cell
    and run it then it should just work.  The Hugging Face libraries will download
    everything you need, including the model.  Hope that helps.
  created_at: 2023-04-13 05:12:51+00:00
  edited: false
  hidden: false
  id: 64379d6340bf2c4964f2f2c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T13:18:38.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>(transformers is already in the runtime, yes)</p>

          '
        raw: (transformers is already in the runtime, yes)
        updatedAt: '2023-04-13T13:18:38.946Z'
      numEdits: 0
      reactions: []
    id: 6438012e94faafc1a2dfa9c8
    type: comment
  author: srowen
  content: (transformers is already in the runtime, yes)
  created_at: 2023-04-13 12:18:38+00:00
  edited: false
  hidden: false
  id: 6438012e94faafc1a2dfa9c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:07:18.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>At this point there are more elaborate usage instructions with HF,
          and langchain, in the repo</p>

          '
        raw: At this point there are more elaborate usage instructions with HF, and
          langchain, in the repo
        updatedAt: '2023-04-19T19:07:18.823Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64403be62113f7dfcb55213c
    id: 64403be62113f7dfcb55213b
    type: comment
  author: srowen
  content: At this point there are more elaborate usage instructions with HF, and
    langchain, in the repo
  created_at: 2023-04-19 18:07:18+00:00
  edited: false
  hidden: false
  id: 64403be62113f7dfcb55213b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:07:18.000Z'
    data:
      status: closed
    id: 64403be62113f7dfcb55213c
    type: status-change
  author: srowen
  created_at: 2023-04-19 18:07:18+00:00
  id: 64403be62113f7dfcb55213c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: More detailed "Usage" docs?
