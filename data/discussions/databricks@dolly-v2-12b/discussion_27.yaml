!!python/object:huggingface_hub.community.DiscussionWithDetails
author: egorulz
conflicting_files: null
created_at: 2023-04-14 06:30:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c925c8e42fb84eb87863f60dd47a0ecd.svg
      fullname: Shyam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: egorulz
      type: user
    createdAt: '2023-04-14T07:30:53.000Z'
    data:
      edited: false
      editors:
      - egorulz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c925c8e42fb84eb87863f60dd47a0ecd.svg
          fullname: Shyam
          isHf: false
          isPro: false
          name: egorulz
          type: user
        html: '<p>What sort of system requirements would I need to run this model
          locally - as opposed to something like say, Vicuna-13b.</p>

          '
        raw: What sort of system requirements would I need to run this model locally
          - as opposed to something like say, Vicuna-13b.
        updatedAt: '2023-04-14T07:30:53.956Z'
      numEdits: 0
      reactions: []
    id: 6439012de1acfc375c694a4a
    type: comment
  author: egorulz
  content: What sort of system requirements would I need to run this model locally
    - as opposed to something like say, Vicuna-13b.
  created_at: 2023-04-14 06:30:53+00:00
  edited: false
  hidden: false
  id: 6439012de1acfc375c694a4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T12:33:25.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Ideally a GPU with at least 32GB of RAM for the 12B model. It should
          work in 16GB if you load in 8-bit.<br>The smaller models should work in
          less GPU RAM too.</p>

          '
        raw: 'Ideally a GPU with at least 32GB of RAM for the 12B model. It should
          work in 16GB if you load in 8-bit.

          The smaller models should work in less GPU RAM too.'
        updatedAt: '2023-04-14T12:33:25.837Z'
      numEdits: 0
      reactions: []
    id: 6439481511e9481b75e161df
    type: comment
  author: srowen
  content: 'Ideally a GPU with at least 32GB of RAM for the 12B model. It should work
    in 16GB if you load in 8-bit.

    The smaller models should work in less GPU RAM too.'
  created_at: 2023-04-14 11:33:25+00:00
  edited: false
  hidden: false
  id: 6439481511e9481b75e161df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-04-14T13:39:01.000Z'
    data:
      edited: true
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p>I can confirm that the 12B version runs on 1x RTX 3090 (24GB of\
          \ VRAM) loaded in int8 precision:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\n\nbase_model = <span\
          \ class=\"hljs-string\">\"databricks/dolly-v2-12b\"</span>\nload_8bit =\
          \ <span class=\"hljs-literal\">True</span>\n\ntokenizer = AutoTokenizer.from_pretrained(base_model,\
          \ padding_side=<span class=\"hljs-string\">\"left\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    base_model, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>\n)\n\nmodel.<span class=\"hljs-built_in\"\
          >eval</span>()\n<span class=\"hljs-keyword\">if</span> torch.__version__\
          \ &gt;= <span class=\"hljs-string\">\"2\"</span>:\n    model = torch.<span\
          \ class=\"hljs-built_in\">compile</span>(model)\n\npipe = InstructionTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\npipe(<span class=\"hljs-string\">\"any prompt you\
          \ want to provide\"</span>)\n</code></pre>\n<p>Don't forget to import the\
          \ <code>InstructionTextGenerationPipeline</code> given by the team.</p>\n"
        raw: "I can confirm that the 12B version runs on 1x RTX 3090 (24GB of VRAM)\
          \ loaded in int8 precision:\n\n```python\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n\nbase_model = \"databricks/dolly-v2-12b\"\nload_8bit\
          \ = True\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, padding_side=\"\
          left\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\
          \ load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\"\
          \n)\n\nmodel.eval()\nif torch.__version__ >= \"2\":\n    model = torch.compile(model)\n\
          \npipe = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\
          pipe(\"any prompt you want to provide\")\n```\n\nDon't forget to import\
          \ the `InstructionTextGenerationPipeline` given by the team."
        updatedAt: '2023-04-14T14:08:33.799Z'
      numEdits: 2
      reactions: []
    id: 64395775de56ab98fabca1bd
    type: comment
  author: chainyo
  content: "I can confirm that the 12B version runs on 1x RTX 3090 (24GB of VRAM)\
    \ loaded in int8 precision:\n\n```python\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\n\nbase_model = \"databricks/dolly-v2-12b\"\nload_8bit\
    \ = True\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, padding_side=\"\
    left\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model, load_in_8bit=load_8bit,\
    \ torch_dtype=torch.float16, device_map=\"auto\"\n)\n\nmodel.eval()\nif torch.__version__\
    \ >= \"2\":\n    model = torch.compile(model)\n\npipe = InstructionTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer)\npipe(\"any prompt you want to provide\")\n```\n\nDon't\
    \ forget to import the `InstructionTextGenerationPipeline` given by the team."
  created_at: 2023-04-14 12:39:01+00:00
  edited: true
  hidden: false
  id: 64395775de56ab98fabca1bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T13:52:38.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You can also just use trust_remote_code=''true'' to auto-import
          it but this works fine for sure.<br>I think bitsandbytes will complain if
          you set bfloat16 as it will end up using fp16 for the fp parts anyway, but
          it just ignores that</p>

          '
        raw: 'You can also just use trust_remote_code=''true'' to auto-import it but
          this works fine for sure.

          I think bitsandbytes will complain if you set bfloat16 as it will end up
          using fp16 for the fp parts anyway, but it just ignores that'
        updatedAt: '2023-04-14T13:52:38.449Z'
      numEdits: 0
      reactions: []
    id: 64395aa60cb95b3dbc8ea4bd
    type: comment
  author: srowen
  content: 'You can also just use trust_remote_code=''true'' to auto-import it but
    this works fine for sure.

    I think bitsandbytes will complain if you set bfloat16 as it will end up using
    fp16 for the fp parts anyway, but it just ignores that'
  created_at: 2023-04-14 12:52:38+00:00
  edited: false
  hidden: false
  id: 64395aa60cb95b3dbc8ea4bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-04-14T14:08:53.000Z'
    data:
      edited: false
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p>You are right <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/srowen\"\
          >@<span class=\"underline\">srowen</span></a></span>\n\n\t</span></span>\
          \ I updated the code snippet.</p>\n"
        raw: You are right @srowen I updated the code snippet.
        updatedAt: '2023-04-14T14:08:53.368Z'
      numEdits: 0
      reactions: []
    id: 64395e75cc228b8099b21492
    type: comment
  author: chainyo
  content: You are right @srowen I updated the code snippet.
  created_at: 2023-04-14 13:08:53+00:00
  edited: false
  hidden: false
  id: 64395e75cc228b8099b21492
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48314f3f38c39fd67365ff623dc641b0.svg
      fullname: KARAMJIT DAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rickey94
      type: user
    createdAt: '2023-04-16T09:12:53.000Z'
    data:
      edited: false
      editors:
      - rickey94
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48314f3f38c39fd67365ff623dc641b0.svg
          fullname: KARAMJIT DAS
          isHf: false
          isPro: false
          name: rickey94
          type: user
        html: '<p>Hi,</p>

          <p>I want to run the dolly 12 b model in Azure cloud. Can you suggest which
          VM I should go for?</p>

          '
        raw: 'Hi,


          I want to run the dolly 12 b model in Azure cloud. Can you suggest which
          VM I should go for?'
        updatedAt: '2023-04-16T09:12:53.289Z'
      numEdits: 0
      reactions: []
    id: 643bbc15823f2cb271392065
    type: comment
  author: rickey94
  content: 'Hi,


    I want to run the dolly 12 b model in Azure cloud. Can you suggest which VM I
    should go for?'
  created_at: 2023-04-16 08:12:53+00:00
  edited: false
  hidden: false
  id: 643bbc15823f2cb271392065
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-04-16T09:24:39.000Z'
    data:
      edited: false
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;rickey94&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rickey94\"\
          >@<span class=\"underline\">rickey94</span></a></span>\n\n\t</span></span>,\
          \ I don't know or use Azure, but from my experiment you need this to successfully\
          \ run the model:</p>\n<ul>\n<li>fp16: 40GB of VRAM -&gt; RTX A6000 or NVIDIA\
          \ A100 40GB</li>\n<li>int8 (with Peft): 15-24GB of VRAM (depending on the\
          \ prompt size) -&gt; NVIDIA V100 (16GB) or RTX 3090 (or similar)</li>\n\
          </ul>\n"
        raw: 'Hi @rickey94, I don''t know or use Azure, but from my experiment you
          need this to successfully run the model:

          - fp16: 40GB of VRAM -> RTX A6000 or NVIDIA A100 40GB

          - int8 (with Peft): 15-24GB of VRAM (depending on the prompt size) -> NVIDIA
          V100 (16GB) or RTX 3090 (or similar)'
        updatedAt: '2023-04-16T09:24:39.921Z'
      numEdits: 0
      reactions: []
    id: 643bbed7f925266fb598d207
    type: comment
  author: chainyo
  content: 'Hi @rickey94, I don''t know or use Azure, but from my experiment you need
    this to successfully run the model:

    - fp16: 40GB of VRAM -> RTX A6000 or NVIDIA A100 40GB

    - int8 (with Peft): 15-24GB of VRAM (depending on the prompt size) -> NVIDIA V100
    (16GB) or RTX 3090 (or similar)'
  created_at: 2023-04-16 08:24:39+00:00
  edited: false
  hidden: false
  id: 643bbed7f925266fb598d207
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48314f3f38c39fd67365ff623dc641b0.svg
      fullname: KARAMJIT DAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rickey94
      type: user
    createdAt: '2023-04-16T09:56:38.000Z'
    data:
      edited: false
      editors:
      - rickey94
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48314f3f38c39fd67365ff623dc641b0.svg
          fullname: KARAMJIT DAS
          isHf: false
          isPro: false
          name: rickey94
          type: user
        html: '<p>What''s the time takes to generate a response for decent size prompt?</p>

          '
        raw: What's the time takes to generate a response for decent size prompt?
        updatedAt: '2023-04-16T09:56:38.944Z'
      numEdits: 0
      reactions: []
    id: 643bc656c16819aac131b4b6
    type: comment
  author: rickey94
  content: What's the time takes to generate a response for decent size prompt?
  created_at: 2023-04-16 08:56:38+00:00
  edited: false
  hidden: false
  id: 643bc656c16819aac131b4b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-04-16T12:05:26.000Z'
    data:
      edited: true
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rickey94&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rickey94\">@<span class=\"\
          underline\">rickey94</span></a></span>\n\n\t</span></span> </p>\n<ul>\n\
          <li>fp16: between 5 and 15 sec.</li>\n<li>int8 and Peft: between 1 and 5\
          \ sec.</li>\n</ul>\n<p>It also depends on the <code>num_beams</code> you\
          \ require and any other generation parameters you use. I used long inputs\
          \ as a reference, between 1536 and 2048 tokens. You may also have a faster\
          \ inference time if your inputs are smaller.</p>\n"
        raw: "@rickey94 \n* fp16: between 5 and 15 sec.\n* int8 and Peft: between\
          \ 1 and 5 sec.\n\nIt also depends on the `num_beams` you require and any\
          \ other generation parameters you use. I used long inputs as a reference,\
          \ between 1536 and 2048 tokens. You may also have a faster inference time\
          \ if your inputs are smaller."
        updatedAt: '2023-04-16T12:07:17.765Z'
      numEdits: 1
      reactions: []
    id: 643be486c16819aac1326408
    type: comment
  author: chainyo
  content: "@rickey94 \n* fp16: between 5 and 15 sec.\n* int8 and Peft: between 1\
    \ and 5 sec.\n\nIt also depends on the `num_beams` you require and any other generation\
    \ parameters you use. I used long inputs as a reference, between 1536 and 2048\
    \ tokens. You may also have a faster inference time if your inputs are smaller."
  created_at: 2023-04-16 11:05:26+00:00
  edited: true
  hidden: false
  id: 643be486c16819aac1326408
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672531901326-6345bd89fe134dfd7a0dba40.png?w=200&h=200&f=face
      fullname: "Furkan G\xF6z\xFCkara"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MonsterMMORPG
      type: user
    createdAt: '2023-04-16T15:21:11.000Z'
    data:
      edited: false
      editors:
      - MonsterMMORPG
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672531901326-6345bd89fe134dfd7a0dba40.png?w=200&h=200&f=face
          fullname: "Furkan G\xF6z\xFCkara"
          isHf: false
          isPro: false
          name: MonsterMMORPG
          type: user
        html: '<p>Here the tutorial video for how to install and use on Windows </p>

          <p>The video includes a Gradio user interface script and teaches you how
          to enable load 8bit speed up and lower VRAM quantization </p>

          <p>The results I had was not very good though for some reason :/</p>

          <p><a rel="nofollow" href="https://youtu.be/ku6UvK1bsp4"><strong>Dolly 2.0
          : Free ChatGPT-like Model for Commercial Use - How To Install And Use Locally
          On Your PC</strong></a><br><a rel="nofollow" href="https://youtu.be/ku6UvK1bsp4"><img
          alt="image" src="https://user-images.githubusercontent.com/19240467/232322564-5889d165-a472-41f4-8747-d691c51c2609.png"></a></p>

          '
        raw: "Here the tutorial video for how to install and use on Windows \n\nThe\
          \ video includes a Gradio user interface script and teaches you how to enable\
          \ load 8bit speed up and lower VRAM quantization \n\nThe results I had was\
          \ not very good though for some reason :/\n\n[**Dolly 2.0 : Free ChatGPT-like\
          \ Model for Commercial Use - How To Install And Use Locally On Your PC**](https://youtu.be/ku6UvK1bsp4)\n\
          [![image](https://user-images.githubusercontent.com/19240467/232322564-5889d165-a472-41f4-8747-d691c51c2609.png)](https://youtu.be/ku6UvK1bsp4)"
        updatedAt: '2023-04-16T15:21:11.665Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - WangTai
    id: 643c126721686867003f593f
    type: comment
  author: MonsterMMORPG
  content: "Here the tutorial video for how to install and use on Windows \n\nThe\
    \ video includes a Gradio user interface script and teaches you how to enable\
    \ load 8bit speed up and lower VRAM quantization \n\nThe results I had was not\
    \ very good though for some reason :/\n\n[**Dolly 2.0 : Free ChatGPT-like Model\
    \ for Commercial Use - How To Install And Use Locally On Your PC**](https://youtu.be/ku6UvK1bsp4)\n\
    [![image](https://user-images.githubusercontent.com/19240467/232322564-5889d165-a472-41f4-8747-d691c51c2609.png)](https://youtu.be/ku6UvK1bsp4)"
  created_at: 2023-04-16 14:21:11+00:00
  edited: false
  hidden: false
  id: 643c126721686867003f593f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efb54cb273880a0b22782f618107e6c5.svg
      fullname: David Wootton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: drwootton
      type: user
    createdAt: '2023-04-18T14:27:52.000Z'
    data:
      edited: true
      editors:
      - drwootton
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efb54cb273880a0b22782f618107e6c5.svg
          fullname: David Wootton
          isHf: false
          isPro: false
          name: drwootton
          type: user
        html: '<p>I have the 12B model running on my computer running Linux using
          a RTX 3060 graphics card, a I9-10900X cpu and 48GB memory. I''m using <a
          rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>
          as the front end. The settings I tried were GPU memory 7.5GB, CPU memory
          22GB, auto-devices and load-in-8-bit.<br>Looking at memory usage, it looks
          like it gets anywhere close to using the 22GB CPU memory, but GPU memory
          does go above the 7.5GB limit.<br>It generates about 1 token per second.</p>

          <p>I got to around 1200-1500 tokens current + context/history with the dolly
          12B model.<br>You might be able to get more by tweaking the model settings,
          but this works as a starting point.</p>

          '
        raw: "I have the 12B model running on my computer running Linux using a RTX\
          \ 3060 graphics card, a I9-10900X cpu and 48GB memory. I'm using https://github.com/oobabooga/text-generation-webui\
          \ as the front end. The settings I tried were GPU memory 7.5GB, CPU memory\
          \ 22GB, auto-devices and load-in-8-bit. \nLooking at memory usage, it looks\
          \ like it gets anywhere close to using the 22GB CPU memory, but GPU memory\
          \ does go above the 7.5GB limit. \nIt generates about 1 token per second.\n\
          \nI got to around 1200-1500 tokens current + context/history with the dolly\
          \ 12B model.\nYou might be able to get more by tweaking the model settings,\
          \ but this works as a starting point."
        updatedAt: '2023-04-18T15:22:36.115Z'
      numEdits: 1
      reactions: []
    id: 643ea8e8e868b8d928244372
    type: comment
  author: drwootton
  content: "I have the 12B model running on my computer running Linux using a RTX\
    \ 3060 graphics card, a I9-10900X cpu and 48GB memory. I'm using https://github.com/oobabooga/text-generation-webui\
    \ as the front end. The settings I tried were GPU memory 7.5GB, CPU memory 22GB,\
    \ auto-devices and load-in-8-bit. \nLooking at memory usage, it looks like it\
    \ gets anywhere close to using the 22GB CPU memory, but GPU memory does go above\
    \ the 7.5GB limit. \nIt generates about 1 token per second.\n\nI got to around\
    \ 1200-1500 tokens current + context/history with the dolly 12B model.\nYou might\
    \ be able to get more by tweaking the model settings, but this works as a starting\
    \ point."
  created_at: 2023-04-18 13:27:52+00:00
  edited: true
  hidden: false
  id: 643ea8e8e868b8d928244372
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4392377a6ebfb4d9a0384b97110d1600.svg
      fullname: FelixAsanger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FelixAsanger
      type: user
    createdAt: '2023-04-18T15:10:44.000Z'
    data:
      edited: false
      editors:
      - FelixAsanger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4392377a6ebfb4d9a0384b97110d1600.svg
          fullname: FelixAsanger
          isHf: false
          isPro: false
          name: FelixAsanger
          type: user
        html: '<p>I just ran a few prompts through the model and apparently it took
          6-7 mins. I run on databricks on a Standard_NC6s_v3 machine with 112GB of
          memory.<br>Any hint why inference takes so long is highly appreciated!</p>

          '
        raw: "I just ran a few prompts through the model and apparently it took 6-7\
          \ mins. I run on databricks on a Standard_NC6s_v3 machine with 112GB of\
          \ memory. \nAny hint why inference takes so long is highly appreciated!"
        updatedAt: '2023-04-18T15:10:44.179Z'
      numEdits: 0
      reactions: []
    id: 643eb2f4c5fb5f563de213d8
    type: comment
  author: FelixAsanger
  content: "I just ran a few prompts through the model and apparently it took 6-7\
    \ mins. I run on databricks on a Standard_NC6s_v3 machine with 112GB of memory.\
    \ \nAny hint why inference takes so long is highly appreciated!"
  created_at: 2023-04-18 14:10:44+00:00
  edited: false
  hidden: false
  id: 643eb2f4c5fb5f563de213d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-18T15:15:48.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That''s a V100 16GB. The 12B model does not fit onto that GPU. So
          you are mostly running on the CPU and it takes a long time.<br>Did you look
          at <a rel="nofollow" href="https://github.com/databrickslabs/dolly#generating-on-other-instances">https://github.com/databrickslabs/dolly#generating-on-other-instances</a>
          ?<br>You need to load in 8-bit, but a 16GB V100 will struggle with the 12B
          model a bit.<br>Use A10 or better, or use the 7B model.</p>

          '
        raw: 'That''s a V100 16GB. The 12B model does not fit onto that GPU. So you
          are mostly running on the CPU and it takes a long time.

          Did you look at https://github.com/databrickslabs/dolly#generating-on-other-instances
          ?

          You need to load in 8-bit, but a 16GB V100 will struggle with the 12B model
          a bit.

          Use A10 or better, or use the 7B model.'
        updatedAt: '2023-04-18T15:15:48.275Z'
      numEdits: 0
      reactions: []
    id: 643eb4247c6ac292f57a335f
    type: comment
  author: srowen
  content: 'That''s a V100 16GB. The 12B model does not fit onto that GPU. So you
    are mostly running on the CPU and it takes a long time.

    Did you look at https://github.com/databrickslabs/dolly#generating-on-other-instances
    ?

    You need to load in 8-bit, but a 16GB V100 will struggle with the 12B model a
    bit.

    Use A10 or better, or use the 7B model.'
  created_at: 2023-04-18 14:15:48+00:00
  edited: false
  hidden: false
  id: 643eb4247c6ac292f57a335f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4392377a6ebfb4d9a0384b97110d1600.svg
      fullname: FelixAsanger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FelixAsanger
      type: user
    createdAt: '2023-04-18T15:50:30.000Z'
    data:
      edited: false
      editors:
      - FelixAsanger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4392377a6ebfb4d9a0384b97110d1600.svg
          fullname: FelixAsanger
          isHf: false
          isPro: false
          name: FelixAsanger
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> Thanks a lot for\
          \ the hint - completely confused a few things!</p>\n"
        raw: '@srowen Thanks a lot for the hint - completely confused a few things!'
        updatedAt: '2023-04-18T15:50:30.323Z'
      numEdits: 0
      reactions: []
    id: 643ebc46c5fb5f563de30426
    type: comment
  author: FelixAsanger
  content: '@srowen Thanks a lot for the hint - completely confused a few things!'
  created_at: 2023-04-18 14:50:30+00:00
  edited: false
  hidden: false
  id: 643ebc46c5fb5f563de30426
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a969fb182387e71891f077768aad7ff9.svg
      fullname: Soham S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: locallyai
      type: user
    createdAt: '2023-04-18T21:13:48.000Z'
    data:
      edited: true
      editors:
      - locallyai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a969fb182387e71891f077768aad7ff9.svg
          fullname: Soham S
          isHf: false
          isPro: false
          name: locallyai
          type: user
        html: '<p>When I am trying it locally, it says the <code>pytorch_model.bin</code>
          is not in the correct JSON format. I am using the following code:</p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b",
          padding_side="left")

          model = AutoModelForCausalLM.from_pretrained("./pytorch_model.bin", device_map="auto",
          torch_dtype=torch.bfloat16)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)


          res = generate_text("What are the differences between dog and cat?")

          print (res)

          </code></pre>

          <p>It says:</p>

          <pre><code>OSError: It looks like the config file at ''./pytorch_model.bin''
          is not a valid JSON file.

          </code></pre>

          <p>But changing to <code>model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          device_map="auto", torch_dtype=torch.bfloat16)</code> works. I have also
          tried using the exact same file as in <code>~/.cache/huggingface/hub/models--databricks--dolly-v2-12b/blobs/</code>,
          and that also does not work.</p>

          '
        raw: 'When I am trying it locally, it says the `pytorch_model.bin` is not
          in the correct JSON format. I am using the following code:


          ```

          tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b", padding_side="left")

          model = AutoModelForCausalLM.from_pretrained("./pytorch_model.bin", device_map="auto",
          torch_dtype=torch.bfloat16)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)


          res = generate_text("What are the differences between dog and cat?")

          print (res)

          ```


          It says:

          ```

          OSError: It looks like the config file at ''./pytorch_model.bin'' is not
          a valid JSON file.

          ```


          But changing to `model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          device_map="auto", torch_dtype=torch.bfloat16)` works. I have also tried
          using the exact same file as in `~/.cache/huggingface/hub/models--databricks--dolly-v2-12b/blobs/`,
          and that also does not work.'
        updatedAt: '2023-04-18T21:14:03.385Z'
      numEdits: 1
      reactions: []
    id: 643f080c46c418c9c68326c5
    type: comment
  author: locallyai
  content: 'When I am trying it locally, it says the `pytorch_model.bin` is not in
    the correct JSON format. I am using the following code:


    ```

    tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b", padding_side="left")

    model = AutoModelForCausalLM.from_pretrained("./pytorch_model.bin", device_map="auto",
    torch_dtype=torch.bfloat16)


    generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)


    res = generate_text("What are the differences between dog and cat?")

    print (res)

    ```


    It says:

    ```

    OSError: It looks like the config file at ''./pytorch_model.bin'' is not a valid
    JSON file.

    ```


    But changing to `model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
    device_map="auto", torch_dtype=torch.bfloat16)` works. I have also tried using
    the exact same file as in `~/.cache/huggingface/hub/models--databricks--dolly-v2-12b/blobs/`,
    and that also does not work.'
  created_at: 2023-04-18 20:13:48+00:00
  edited: true
  hidden: false
  id: 643f080c46c418c9c68326c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-18T21:16:36.000Z'
    data:
      edited: true
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Pass the directory containing this file, not the file path. It''s
          looking for several artifacts in that dir, not just the model. You do not
          need to download the model like this.</p>

          '
        raw: Pass the directory containing this file, not the file path. It's looking
          for several artifacts in that dir, not just the model. You do not need to
          download the model like this.
        updatedAt: '2023-04-18T21:17:18.768Z'
      numEdits: 1
      reactions: []
    id: 643f08b446c418c9c6833812
    type: comment
  author: srowen
  content: Pass the directory containing this file, not the file path. It's looking
    for several artifacts in that dir, not just the model. You do not need to download
    the model like this.
  created_at: 2023-04-18 20:16:36+00:00
  edited: true
  hidden: false
  id: 643f08b446c418c9c6833812
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1498aa4f0c153b543792ea214e565adc.svg
      fullname: Nagesh Pindi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nagesh
      type: user
    createdAt: '2023-04-19T05:58:51.000Z'
    data:
      edited: false
      editors:
      - Nagesh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1498aa4f0c153b543792ea214e565adc.svg
          fullname: Nagesh Pindi
          isHf: false
          isPro: false
          name: Nagesh
          type: user
        html: '<p>Hi,</p>

          <p>I want to run the dolly 12 b model in Cloudera workbench. Can anyone
          suggest how much RAM and GPU''s I should go for?</p>

          '
        raw: 'Hi,


          I want to run the dolly 12 b model in Cloudera workbench. Can anyone suggest
          how much RAM and GPU''s I should go for?'
        updatedAt: '2023-04-19T05:58:51.069Z'
      numEdits: 0
      reactions: []
    id: 643f831bf2ed3bc5c06fd3e6
    type: comment
  author: Nagesh
  content: 'Hi,


    I want to run the dolly 12 b model in Cloudera workbench. Can anyone suggest how
    much RAM and GPU''s I should go for?'
  created_at: 2023-04-19 04:58:51+00:00
  edited: false
  hidden: false
  id: 643f831bf2ed3bc5c06fd3e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T10:31:06.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You want an A100 ideally. See <a rel="nofollow" href="https://github.com/databrickslabs/dolly#training-on-other-instances">https://github.com/databrickslabs/dolly#training-on-other-instances</a></p>

          '
        raw: You want an A100 ideally. See https://github.com/databrickslabs/dolly#training-on-other-instances
        updatedAt: '2023-04-19T10:31:06.364Z'
      numEdits: 0
      reactions: []
    id: 643fc2ea6fd05d8230653cb1
    type: comment
  author: srowen
  content: You want an A100 ideally. See https://github.com/databrickslabs/dolly#training-on-other-instances
  created_at: 2023-04-19 09:31:06+00:00
  edited: false
  hidden: false
  id: 643fc2ea6fd05d8230653cb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:09:31.000Z'
    data:
      status: closed
    id: 64403c6b4164a65ca12cc3f1
    type: status-change
  author: srowen
  created_at: 2023-04-19 18:09:31+00:00
  id: 64403c6b4164a65ca12cc3f1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
      fullname: Juan Uys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: opyate
      type: user
    createdAt: '2023-05-16T11:01:30.000Z'
    data:
      edited: false
      editors:
      - opyate
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
          fullname: Juan Uys
          isHf: false
          isPro: false
          name: opyate
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;chainyo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/chainyo\">@<span class=\"\
          underline\">chainyo</span></a></span>\n\n\t</span></span> if you used LoRA,\
          \ would you mind sharing your LoraConfig? (<a href=\"https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408\"\
          >reference</a>)</p>\n"
        raw: '@chainyo if you used LoRA, would you mind sharing your LoraConfig? ([reference](https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408))'
        updatedAt: '2023-05-16T11:01:30.124Z'
      numEdits: 0
      reactions: []
    id: 6463628a32317fa48067b533
    type: comment
  author: opyate
  content: '@chainyo if you used LoRA, would you mind sharing your LoraConfig? ([reference](https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408))'
  created_at: 2023-05-16 10:01:30+00:00
  edited: false
  hidden: false
  id: 6463628a32317fa48067b533
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-05-16T12:02:05.000Z'
    data:
      edited: false
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;chainyo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/chainyo\"\
          >@<span class=\"underline\">chainyo</span></a></span>\n\n\t</span></span>,\
          \ if you used LoRA, would you mind sharing your LoraConfig? (<a href=\"\
          https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408\"\
          >reference</a>)</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;opyate&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/opyate\"\
          >@<span class=\"underline\">opyate</span></a></span>\n\n\t</span></span>\
          \ Sorry for the confusion. I discussed another alpaca/llama model loaded\
          \ using the LoRa Peft loader. You can find some code snippets on <a rel=\"\
          nofollow\" href=\"https://github.com/chainyo/llama-natural-instructions-finetuning/blob/main/benchmark.ipynb\"\
          >this repo</a></p>\n<p>But you don't need LoRa for this dolly model until\
          \ you fine-tune it using the LoRa technique.</p>\n"
        raw: '> @chainyo, if you used LoRA, would you mind sharing your LoraConfig?
          ([reference](https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408))


          @opyate Sorry for the confusion. I discussed another alpaca/llama model
          loaded using the LoRa Peft loader. You can find some code snippets on [this
          repo](https://github.com/chainyo/llama-natural-instructions-finetuning/blob/main/benchmark.ipynb)


          But you don''t need LoRa for this dolly model until you fine-tune it using
          the LoRa technique.'
        updatedAt: '2023-05-16T12:02:05.621Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - opyate
    id: 646370bd12814d75417920b8
    type: comment
  author: chainyo
  content: '> @chainyo, if you used LoRA, would you mind sharing your LoraConfig?
    ([reference](https://huggingface.co/databricks/dolly-v2-12b/discussions/27#643be486c16819aac1326408))


    @opyate Sorry for the confusion. I discussed another alpaca/llama model loaded
    using the LoRa Peft loader. You can find some code snippets on [this repo](https://github.com/chainyo/llama-natural-instructions-finetuning/blob/main/benchmark.ipynb)


    But you don''t need LoRa for this dolly model until you fine-tune it using the
    LoRa technique.'
  created_at: 2023-05-16 11:02:05+00:00
  edited: false
  hidden: false
  id: 646370bd12814d75417920b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e573ef3d82edf394ef1546b70e6da18.svg
      fullname: Ananya Tripathi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ananyaaa
      type: user
    createdAt: '2023-06-06T11:54:46.000Z'
    data:
      edited: false
      editors:
      - ananyaaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609593749046326
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e573ef3d82edf394ef1546b70e6da18.svg
          fullname: Ananya Tripathi
          isHf: false
          isPro: false
          name: ananyaaa
          type: user
        html: '<p>Hi, like in openAi we have token limit of 4096, do we have token
          limit in dolly 2 as well when we deploy it locally? Thanks!</p>

          '
        raw: Hi, like in openAi we have token limit of 4096, do we have token limit
          in dolly 2 as well when we deploy it locally? Thanks!
        updatedAt: '2023-06-06T11:54:46.327Z'
      numEdits: 0
      reactions: []
    id: 647f1e869c310244579f7938
    type: comment
  author: ananyaaa
  content: Hi, like in openAi we have token limit of 4096, do we have token limit
    in dolly 2 as well when we deploy it locally? Thanks!
  created_at: 2023-06-06 10:54:46+00:00
  edited: false
  hidden: false
  id: 647f1e869c310244579f7938
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-06T11:55:19.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4745899438858032
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yes, 2048 tokens</p>

          '
        raw: Yes, 2048 tokens
        updatedAt: '2023-06-06T11:55:19.736Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ananyaaa
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ananyaaa
    id: 647f1ea71a446a624a3e08da
    type: comment
  author: srowen
  content: Yes, 2048 tokens
  created_at: 2023-06-06 10:55:19+00:00
  edited: false
  hidden: false
  id: 647f1ea71a446a624a3e08da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
      fullname: 'YvonC '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toutski
      type: user
    createdAt: '2023-06-28T22:22:46.000Z'
    data:
      edited: false
      editors:
      - toutski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9379993081092834
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
          fullname: 'YvonC '
          isHf: false
          isPro: false
          name: toutski
          type: user
        html: '<p>Do you have a notebook to run dolly 2.0 on azure databricks, I try
          but I have error :-(</p>

          '
        raw: 'Do you have a notebook to run dolly 2.0 on azure databricks, I try but
          I have error :-(

          '
        updatedAt: '2023-06-28T22:22:46.062Z'
      numEdits: 0
      reactions: []
    id: 649cb2b69813ca2ced07be34
    type: comment
  author: toutski
  content: 'Do you have a notebook to run dolly 2.0 on azure databricks, I try but
    I have error :-(

    '
  created_at: 2023-06-28 21:22:46+00:00
  edited: false
  hidden: false
  id: 649cb2b69813ca2ced07be34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-28T22:45:56.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624450206756592
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yes, the snippet on the model page works. You need a big enough
          GPU and instance. You didn''t say what the problem was. </p>

          '
        raw: 'Yes, the snippet on the model page works. You need a big enough GPU
          and instance. You didn''t say what the problem was. '
        updatedAt: '2023-06-28T22:45:56.222Z'
      numEdits: 0
      reactions: []
    id: 649cb8246209253a768515f6
    type: comment
  author: srowen
  content: 'Yes, the snippet on the model page works. You need a big enough GPU and
    instance. You didn''t say what the problem was. '
  created_at: 2023-06-28 21:45:56+00:00
  edited: false
  hidden: false
  id: 649cb8246209253a768515f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
      fullname: 'YvonC '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toutski
      type: user
    createdAt: '2023-06-29T00:28:43.000Z'
    data:
      edited: false
      editors:
      - toutski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557844161987305
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
          fullname: 'YvonC '
          isHf: false
          isPro: false
          name: toutski
          type: user
        html: '<p>can you give me the lin k I do not see the snippet</p>

          '
        raw: can you give me the lin k I do not see the snippet
        updatedAt: '2023-06-29T00:28:43.951Z'
      numEdits: 0
      reactions: []
    id: 649cd03b6209253a7687d681
    type: comment
  author: toutski
  content: can you give me the lin k I do not see the snippet
  created_at: 2023-06-28 23:28:43+00:00
  edited: false
  hidden: false
  id: 649cd03b6209253a7687d681
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-29T00:43:05.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6927684545516968
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Just this very site. <a href="https://huggingface.co/databricks/dolly-v2-12b#usage">https://huggingface.co/databricks/dolly-v2-12b#usage</a></p>

          '
        raw: Just this very site. https://huggingface.co/databricks/dolly-v2-12b#usage
        updatedAt: '2023-06-29T00:43:05.138Z'
      numEdits: 0
      reactions: []
    id: 649cd39956fd5be3a049cbf3
    type: comment
  author: srowen
  content: Just this very site. https://huggingface.co/databricks/dolly-v2-12b#usage
  created_at: 2023-06-28 23:43:05+00:00
  edited: false
  hidden: false
  id: 649cd39956fd5be3a049cbf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
      fullname: 'YvonC '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toutski
      type: user
    createdAt: '2023-06-29T00:59:26.000Z'
    data:
      edited: false
      editors:
      - toutski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4194057881832123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
          fullname: 'YvonC '
          isHf: false
          isPro: false
          name: toutski
          type: user
        html: '<p>Merci, Thanks, Namaste :-)</p>

          '
        raw: Merci, Thanks, Namaste :-)
        updatedAt: '2023-06-29T00:59:26.696Z'
      numEdits: 0
      reactions: []
    id: 649cd76e7bc1f09aee1c66af
    type: comment
  author: toutski
  content: Merci, Thanks, Namaste :-)
  created_at: 2023-06-28 23:59:26+00:00
  edited: false
  hidden: false
  id: 649cd76e7bc1f09aee1c66af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
      fullname: 'YvonC '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toutski
      type: user
    createdAt: '2023-06-29T14:50:28.000Z'
    data:
      edited: false
      editors:
      - toutski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9356177449226379
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee10cfe31ae232ed31582e4907eb8347.svg
          fullname: 'YvonC '
          isHf: false
          isPro: false
          name: toutski
          type: user
        html: '<p>I have this error when I try to run : We couldn''t connect to ''<a
          rel="nofollow" href="https://huggingface.co''">https://huggingface.co''</a>
          to load this file, </p>

          '
        raw: 'I have this error when I try to run : We couldn''t connect to ''https://huggingface.co''
          to load this file, '
        updatedAt: '2023-06-29T14:50:28.849Z'
      numEdits: 0
      reactions: []
    id: 649d9a347ee7fc6c74366dcb
    type: comment
  author: toutski
  content: 'I have this error when I try to run : We couldn''t connect to ''https://huggingface.co''
    to load this file, '
  created_at: 2023-06-29 13:50:28+00:00
  edited: false
  hidden: false
  id: 649d9a347ee7fc6c74366dcb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-29T14:56:50.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9917608499526978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You''ll have to solve that access problem yourself, it''s specific
          to your env</p>

          '
        raw: You'll have to solve that access problem yourself, it's specific to your
          env
        updatedAt: '2023-06-29T14:56:50.089Z'
      numEdits: 0
      reactions: []
    id: 649d9bb27ee7fc6c7436938d
    type: comment
  author: srowen
  content: You'll have to solve that access problem yourself, it's specific to your
    env
  created_at: 2023-06-29 13:56:50+00:00
  edited: false
  hidden: false
  id: 649d9bb27ee7fc6c7436938d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1498aa4f0c153b543792ea214e565adc.svg
      fullname: Nagesh Pindi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nagesh
      type: user
    createdAt: '2023-09-26T13:30:26.000Z'
    data:
      edited: false
      editors:
      - Nagesh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7846580743789673
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1498aa4f0c153b543792ea214e565adc.svg
          fullname: Nagesh Pindi
          isHf: false
          isPro: false
          name: Nagesh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> </p>\n<p>I'm trying\
          \ to finetune \"TinyPixel/Llama-2-7B-bf16-sharded\" on 8 GB ram and one\
          \ GPU, but facing some issues like</p>\n<p>model = AutoModelForCausalLM.from_pretrained(<br>model_name,<br>quantization_config=bnb_config,<br>trust_remote_code=True)</p>\n\
          <p>RuntimeError: Failed to import transformers.models.llama.modeling_llama\
          \ because of the following error (look up to see its traceback):</p>\n<p>Is\
          \ it because of RAM and GPU?</p>\n"
        raw: "Hi @srowen \n\nI'm trying to finetune \"TinyPixel/Llama-2-7B-bf16-sharded\"\
          \ on 8 GB ram and one GPU, but facing some issues like\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          model_name,\nquantization_config=bnb_config,\ntrust_remote_code=True)\n\n\
          RuntimeError: Failed to import transformers.models.llama.modeling_llama\
          \ because of the following error (look up to see its traceback):\n\nIs it\
          \ because of RAM and GPU?"
        updatedAt: '2023-09-26T13:30:26.874Z'
      numEdits: 0
      reactions: []
    id: 6512dcf2f60393414af27ff4
    type: comment
  author: Nagesh
  content: "Hi @srowen \n\nI'm trying to finetune \"TinyPixel/Llama-2-7B-bf16-sharded\"\
    \ on 8 GB ram and one GPU, but facing some issues like\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    model_name,\nquantization_config=bnb_config,\ntrust_remote_code=True)\n\nRuntimeError:\
    \ Failed to import transformers.models.llama.modeling_llama because of the following\
    \ error (look up to see its traceback):\n\nIs it because of RAM and GPU?"
  created_at: 2023-09-26 12:30:26+00:00
  edited: false
  hidden: false
  id: 6512dcf2f60393414af27ff4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-09-26T13:31:05.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8989701271057129
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Wrong forum - not a question about Dolly.</p>

          '
        raw: Wrong forum - not a question about Dolly.
        updatedAt: '2023-09-26T13:31:05.540Z'
      numEdits: 0
      reactions: []
    id: 6512dd192a41af0d03cfb92f
    type: comment
  author: srowen
  content: Wrong forum - not a question about Dolly.
  created_at: 2023-09-26 12:31:05+00:00
  edited: false
  hidden: false
  id: 6512dd192a41af0d03cfb92f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Running dolly2 locally
