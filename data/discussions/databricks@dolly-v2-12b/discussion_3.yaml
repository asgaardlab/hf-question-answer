!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gustavomr
conflicting_files: null
created_at: 2023-04-12 19:23:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64371287b27986919e27ae4b/gqjkB_Ma7hFm4VpCZGMj_.png?w=200&h=200&f=face
      fullname: Gustavo Monti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gustavomr
      type: user
    createdAt: '2023-04-12T20:23:16.000Z'
    data:
      edited: false
      editors:
      - gustavomr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64371287b27986919e27ae4b/gqjkB_Ma7hFm4VpCZGMj_.png?w=200&h=200&f=face
          fullname: Gustavo Monti
          isHf: false
          isPro: false
          name: gustavomr
          type: user
        html: '<p>I think a lot of users might have this questions in order to have
          it locally and running on laptop and simple computers.</p>

          <p>Can we have an alternative version (simplest), maybe trained using same
          corpus but with less params in order to run on CPUS?</p>

          <p>By the way, congrats on this amazing job! Keep rocking! </p>

          '
        raw: "I think a lot of users might have this questions in order to have it\
          \ locally and running on laptop and simple computers.\r\n\r\nCan we have\
          \ an alternative version (simplest), maybe trained using same corpus but\
          \ with less params in order to run on CPUS?\r\n\r\nBy the way, congrats\
          \ on this amazing job! Keep rocking! "
        updatedAt: '2023-04-12T20:23:16.350Z'
      numEdits: 0
      reactions: []
    id: 643713340628510e7239a6bd
    type: comment
  author: gustavomr
  content: "I think a lot of users might have this questions in order to have it locally\
    \ and running on laptop and simple computers.\r\n\r\nCan we have an alternative\
    \ version (simplest), maybe trained using same corpus but with less params in\
    \ order to run on CPUS?\r\n\r\nBy the way, congrats on this amazing job! Keep\
    \ rocking! "
  created_at: 2023-04-12 19:23:16+00:00
  edited: false
  hidden: false
  id: 643713340628510e7239a6bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-12T20:45:04.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You can, but it would be very very slow. You really want a GPU.<br>The
          training code for "v2" will be on the repo soon, and you could use that
          to train from a smaller Pythia model.<br>Maybe the team will just do that.
          But models small enough to work on CPUs are &lt;100M params and that may
          not perform that well for the kind of text-gen QA task people expect to
          use this for.</p>

          '
        raw: 'You can, but it would be very very slow. You really want a GPU.

          The training code for "v2" will be on the repo soon, and you could use that
          to train from a smaller Pythia model.

          Maybe the team will just do that. But models small enough to work on CPUs
          are <100M params and that may not perform that well for the kind of text-gen
          QA task people expect to use this for.'
        updatedAt: '2023-04-12T20:45:04.011Z'
      numEdits: 0
      reactions: []
    id: 643718501ed4fe4c0e95afa9
    type: comment
  author: srowen
  content: 'You can, but it would be very very slow. You really want a GPU.

    The training code for "v2" will be on the repo soon, and you could use that to
    train from a smaller Pythia model.

    Maybe the team will just do that. But models small enough to work on CPUs are
    <100M params and that may not perform that well for the kind of text-gen QA task
    people expect to use this for.'
  created_at: 2023-04-12 19:45:04+00:00
  edited: false
  hidden: false
  id: 643718501ed4fe4c0e95afa9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34cc5b6ed65dae4cd8fb12e114877066.svg
      fullname: Satya Suman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: satyasumans
      type: user
    createdAt: '2023-04-13T05:29:21.000Z'
    data:
      edited: false
      editors:
      - satyasumans
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34cc5b6ed65dae4cd8fb12e114877066.svg
          fullname: Satya Suman
          isHf: false
          isPro: false
          name: satyasumans
          type: user
        html: '<p>Can we quantize the model, like someone did in Llama.cpp? (Pardon
          my ignorance)</p>

          '
        raw: Can we quantize the model, like someone did in Llama.cpp? (Pardon my
          ignorance)
        updatedAt: '2023-04-13T05:29:21.938Z'
      numEdits: 0
      reactions: []
    id: 64379331a1010492a8a8ad70
    type: comment
  author: satyasumans
  content: Can we quantize the model, like someone did in Llama.cpp? (Pardon my ignorance)
  created_at: 2023-04-13 04:29:21+00:00
  edited: false
  hidden: false
  id: 64379331a1010492a8a8ad70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T12:41:46.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Sure,  you can try. See <a rel="nofollow" href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a>
          for source code (2.0 training code coming soon)</p>

          '
        raw: Sure,  you can try. See https://github.com/databrickslabs/dolly for source
          code (2.0 training code coming soon)
        updatedAt: '2023-04-13T12:41:46.395Z'
      numEdits: 0
      reactions: []
    id: 6437f88a067c51eb8b335dc6
    type: comment
  author: srowen
  content: Sure,  you can try. See https://github.com/databrickslabs/dolly for source
    code (2.0 training code coming soon)
  created_at: 2023-04-13 11:41:46+00:00
  edited: false
  hidden: false
  id: 6437f88a067c51eb8b335dc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/586667e31728384fc0850fa7a2ace575.svg
      fullname: Asd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: compiled
      type: user
    createdAt: '2023-04-13T19:31:25.000Z'
    data:
      edited: false
      editors:
      - compiled
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/586667e31728384fc0850fa7a2ace575.svg
          fullname: Asd
          isHf: false
          isPro: false
          name: compiled
          type: user
        html: '<p>People have already done it here: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/569">https://github.com/ggerganov/llama.cpp/discussions/569</a>.
          Looks like it runs on CPU just fine :)</p>

          '
        raw: 'People have already done it here: https://github.com/ggerganov/llama.cpp/discussions/569.
          Looks like it runs on CPU just fine :)'
        updatedAt: '2023-04-13T19:31:25.915Z'
      numEdits: 0
      reactions: []
    id: 6438588d6c8841ba74e6f7f2
    type: comment
  author: compiled
  content: 'People have already done it here: https://github.com/ggerganov/llama.cpp/discussions/569.
    Looks like it runs on CPU just fine :)'
  created_at: 2023-04-13 18:31:25+00:00
  edited: false
  hidden: false
  id: 6438588d6c8841ba74e6f7f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b08aef5b885e9803e5bc74bae3ce816.svg
      fullname: Zatsepin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zatsepin
      type: user
    createdAt: '2023-04-14T19:23:03.000Z'
    data:
      edited: false
      editors:
      - Zatsepin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b08aef5b885e9803e5bc74bae3ce816.svg
          fullname: Zatsepin
          isHf: false
          isPro: false
          name: Zatsepin
          type: user
        html: '<p>Direct link to cpu ready version is here - <a href="https://huggingface.co/geemili/dolly-v2-12b/tree/main">https://huggingface.co/geemili/dolly-v2-12b/tree/main</a></p>

          '
        raw: Direct link to cpu ready version is here - https://huggingface.co/geemili/dolly-v2-12b/tree/main
        updatedAt: '2023-04-14T19:23:03.199Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - jeffwadsworth
    id: 6439a817a6b2f278af736f15
    type: comment
  author: Zatsepin
  content: Direct link to cpu ready version is here - https://huggingface.co/geemili/dolly-v2-12b/tree/main
  created_at: 2023-04-14 18:23:03+00:00
  edited: false
  hidden: false
  id: 6439a817a6b2f278af736f15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
      fullname: Naz Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: n940767
      type: user
    createdAt: '2023-04-19T15:13:04.000Z'
    data:
      edited: false
      editors:
      - n940767
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
          fullname: Naz Khan
          isHf: false
          isPro: false
          name: n940767
          type: user
        html: '<p>How do you run <a href="https://huggingface.co/geemili/dolly-v2-12b/tree/main">https://huggingface.co/geemili/dolly-v2-12b/tree/main</a>
          (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?</p>

          '
        raw: How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main
          (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?
        updatedAt: '2023-04-19T15:13:04.786Z'
      numEdits: 0
      reactions: []
    id: 644005002113f7dfcb4f9222
    type: comment
  author: n940767
  content: How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main (12b-quantized
    model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?
  created_at: 2023-04-19 14:13:04+00:00
  edited: false
  hidden: false
  id: 644005002113f7dfcb4f9222
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-04-20T16:07:34.000Z'
    data:
      edited: false
      editors:
      - jeffwadsworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<blockquote>

          <p>How do you run <a href="https://huggingface.co/geemili/dolly-v2-12b/tree/main">https://huggingface.co/geemili/dolly-v2-12b/tree/main</a>
          (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?</p>

          </blockquote>

          <p>You can try the following in the attachment.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg"><img
          alt="ProcedureDolly.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg"></a></p>

          '
        raw: '> How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main
          (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?


          You can try the following in the attachment.

          ![ProcedureDolly.jpg](https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg)'
        updatedAt: '2023-04-20T16:07:34.168Z'
      numEdits: 0
      reactions: []
    id: 64416346006550f1ed7146f4
    type: comment
  author: jeffwadsworth
  content: '> How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main
    (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?


    You can try the following in the attachment.

    ![ProcedureDolly.jpg](https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg)'
  created_at: 2023-04-20 15:07:34+00:00
  edited: false
  hidden: false
  id: 64416346006550f1ed7146f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
      fullname: Naz Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: n940767
      type: user
    createdAt: '2023-04-20T16:20:11.000Z'
    data:
      edited: false
      editors:
      - n940767
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
          fullname: Naz Khan
          isHf: false
          isPro: false
          name: n940767
          type: user
        html: '<blockquote>

          <blockquote>

          <p>How do you run <a href="https://huggingface.co/geemili/dolly-v2-12b/tree/main">https://huggingface.co/geemili/dolly-v2-12b/tree/main</a>
          (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?</p>

          </blockquote>

          <p>You can try the following in the attachment.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg"><img
          alt="ProcedureDolly.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg"></a></p>

          </blockquote>

          <p>Having trouble installing bitsnadbytes on databricks E8as_v4 GPU cluster?</p>

          '
        raw: "> > How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main\
          \ (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?\n\
          > \n> You can try the following in the attachment.\n> ![ProcedureDolly.jpg](https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg)\n\
          \nHaving trouble installing bitsnadbytes on databricks E8as_v4 GPU cluster?"
        updatedAt: '2023-04-20T16:20:11.534Z'
      numEdits: 0
      reactions: []
    id: 6441663be46e14ed55919de5
    type: comment
  author: n940767
  content: "> > How do you run https://huggingface.co/geemili/dolly-v2-12b/tree/main\
    \ (12b-quantized model, .ggml file) with AutoTokenizer and AutoModelForCausalLM?\n\
    > \n> You can try the following in the attachment.\n> ![ProcedureDolly.jpg](https://cdn-uploads.huggingface.co/production/uploads/62e1d11faeaaca0fbd5cfa66/Tf2ESdtqA_zqg5PzySzjd.jpeg)\n\
    \nHaving trouble installing bitsnadbytes on databricks E8as_v4 GPU cluster?"
  created_at: 2023-04-20 15:20:11+00:00
  edited: false
  hidden: false
  id: 6441663be46e14ed55919de5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T16:26:53.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Hm, what issue? bitsandbytes has been working fine for me</p>

          '
        raw: Hm, what issue? bitsandbytes has been working fine for me
        updatedAt: '2023-04-20T16:26:53.179Z'
      numEdits: 0
      reactions: []
    id: 644167cd4c2acf3398a78bf0
    type: comment
  author: srowen
  content: Hm, what issue? bitsandbytes has been working fine for me
  created_at: 2023-04-20 15:26:53+00:00
  edited: false
  hidden: false
  id: 644167cd4c2acf3398a78bf0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
      fullname: Naz Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: n940767
      type: user
    createdAt: '2023-04-20T16:40:49.000Z'
    data:
      edited: false
      editors:
      - n940767
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
          fullname: Naz Khan
          isHf: false
          isPro: false
          name: n940767
          type: user
        html: '<p>Library installation attempted on the driver node of cluster 0413-233703-4jtufovq
          and failed. Please refer to the following error message to fix the library
          or contact Databricks support. Error Code: DRIVER_LIBRARY_INSTALLATION_FAILURE.
          Error Message: org.apache.spark.SparkException: Process List(bash, /local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh,
          /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip, install, bitsandbytes==0.38.1,
          --index-url, <a rel="nofollow" href="https://github.com/timdettmers/bitsandbytes">https://github.com/timdettmers/bitsandbytes</a>,
          --disable-pip-version-check) exited with code 1. ERROR: Could not find a
          version that satisfies the requirement bitsandbytes==0.38.1 (from versions:
          none) ERROR: No matching distribution found for bitsandbytes==0.38.1</p>

          '
        raw: 'Library installation attempted on the driver node of cluster 0413-233703-4jtufovq
          and failed. Please refer to the following error message to fix the library
          or contact Databricks support. Error Code: DRIVER_LIBRARY_INSTALLATION_FAILURE.
          Error Message: org.apache.spark.SparkException: Process List(bash, /local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh,
          /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip, install, bitsandbytes==0.38.1,
          --index-url, https://github.com/timdettmers/bitsandbytes, --disable-pip-version-check)
          exited with code 1. ERROR: Could not find a version that satisfies the requirement
          bitsandbytes==0.38.1 (from versions: none) ERROR: No matching distribution
          found for bitsandbytes==0.38.1'
        updatedAt: '2023-04-20T16:40:49.503Z'
      numEdits: 0
      reactions: []
    id: 64416b117f13a7b5a2671aee
    type: comment
  author: n940767
  content: 'Library installation attempted on the driver node of cluster 0413-233703-4jtufovq
    and failed. Please refer to the following error message to fix the library or
    contact Databricks support. Error Code: DRIVER_LIBRARY_INSTALLATION_FAILURE. Error
    Message: org.apache.spark.SparkException: Process List(bash, /local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh,
    /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip, install, bitsandbytes==0.38.1,
    --index-url, https://github.com/timdettmers/bitsandbytes, --disable-pip-version-check)
    exited with code 1. ERROR: Could not find a version that satisfies the requirement
    bitsandbytes==0.38.1 (from versions: none) ERROR: No matching distribution found
    for bitsandbytes==0.38.1'
  created_at: 2023-04-20 15:40:49+00:00
  edited: false
  hidden: false
  id: 64416b117f13a7b5a2671aee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T17:01:41.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Works fine for me: <code>%pip install bitsandbytes==0.38.1</code>
          using the 13.0 ML runtime. How are you installing, exactly?</p>

          '
        raw: 'Works fine for me: `%pip install bitsandbytes==0.38.1` using the 13.0
          ML runtime. How are you installing, exactly?'
        updatedAt: '2023-04-20T17:01:41.254Z'
      numEdits: 0
      reactions: []
    id: 64416ff5006550f1ed72c7fd
    type: comment
  author: srowen
  content: 'Works fine for me: `%pip install bitsandbytes==0.38.1` using the 13.0
    ML runtime. How are you installing, exactly?'
  created_at: 2023-04-20 16:01:41+00:00
  edited: false
  hidden: false
  id: 64416ff5006550f1ed72c7fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
      fullname: Naz Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: n940767
      type: user
    createdAt: '2023-04-20T18:26:59.000Z'
    data:
      edited: false
      editors:
      - n940767
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26e1028a9ca251f16028362d6f915e27.svg
          fullname: Naz Khan
          isHf: false
          isPro: false
          name: n940767
          type: user
        html: '<p>using <code>%pip install bitsandbytes==0.38.1</code></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/636bce984942b4f89ae24e8e/Se48OCUnZlr54tsEniOAB.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/636bce984942b4f89ae24e8e/Se48OCUnZlr54tsEniOAB.png"></a></p>

          <p>Welcome to bitsandbytes. For bug reports, please run</p>

          <p>python -m bitsandbytes</p>

          <h1 id="and-submit-this-information-together-with-your-error-trace-to-httpsgithubcomtimdettmersbitsandbytesissues">
          and submit this information together with your error trace to: <a rel="nofollow"
          href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a></h1>

          <p>bin /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so<br>CUDA_SETUP:
          WARNING! libcudart.so not found in any environmental path. Searching in
          backup paths...<br>CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 7.0<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/</p>

          <p>/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:
          UserWarning: WARNING: The following directories listed in your path were
          found to be non-existent: {PosixPath(''/databricks/jars/*'')}<br>  warn(msg)<br>/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:
          UserWarning: Found duplicate [''libcudart.so'', ''libcudart.so.11.0'', ''libcudart.so.12.0'']
          files: {PosixPath(''/usr/local/cuda/lib64/libcudart.so''), PosixPath(''/usr/local/cuda/lib64/libcudart.so.11.0'')}..
          We''ll flip a coin and try one of these, in order to fail forward.<br>Either
          way, this might cause trouble in the future:<br>If you get <code>CUDA error:
          invalid device function</code> errors, the above might be the cause and
          the solution is to make sure only one [''libcudart.so'', ''libcudart.so.11.0'',
          ''libcudart.so.12.0''] in the paths that we search based on your env.<br>  warn(msg)<br>/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:
          UserWarning: WARNING: Compute capability &lt; 7.5 detected! Only slow 8-bit
          matmul is supported for your GPU!<br>  warn(msg)</p>

          <p>Then when I run <code>print(pipe("Explain to me the difference between
          nuclear fission and fusion."))</code></p>

          <p>RuntimeError: probability tensor contains either <code>inf</code>, <code>nan</code>
          or element &lt; 0</p>

          '
        raw: "using `%pip install bitsandbytes==0.38.1`\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636bce984942b4f89ae24e8e/Se48OCUnZlr54tsEniOAB.png)\n\
          \nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\
          \n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          bin /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
          CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching\
          \ in backup paths...\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 7.0\nCUDA SETUP:\
          \ Detected CUDA version 117\nCUDA SETUP: Loading binary /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/\n\
          \n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/databricks/jars/*')}\n  warn(msg)\n\
          /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
          \ UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0']\
          \ files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}..\
          \ We'll flip a coin and try one of these, in order to fail forward.\nEither\
          \ way, this might cause trouble in the future:\nIf you get `CUDA error:\
          \ invalid device function` errors, the above might be the cause and the\
          \ solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0',\
          \ 'libcudart.so.12.0'] in the paths that we search based on your env.\n\
          \  warn(msg)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
          \ UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit\
          \ matmul is supported for your GPU!\n  warn(msg)\n\nThen when I run `print(pipe(\"\
          Explain to me the difference between nuclear fission and fusion.\"))`\n\n\
          RuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ < 0"
        updatedAt: '2023-04-20T18:26:59.987Z'
      numEdits: 0
      reactions: []
    id: 644183f34c2acf3398aa9834
    type: comment
  author: n940767
  content: "using `%pip install bitsandbytes==0.38.1`\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/636bce984942b4f89ae24e8e/Se48OCUnZlr54tsEniOAB.png)\n\
    \nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\
    \n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    bin /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
    CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching\
    \ in backup paths...\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n\
    CUDA SETUP: Highest compute capability among GPUs detected: 7.0\nCUDA SETUP: Detected\
    \ CUDA version 117\nCUDA SETUP: Loading binary /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/\n\
    \n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('/databricks/jars/*')}\n  warn(msg)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
    \ UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0']\
    \ files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}..\
    \ We'll flip a coin and try one of these, in order to fail forward.\nEither way,\
    \ this might cause trouble in the future:\nIf you get `CUDA error: invalid device\
    \ function` errors, the above might be the cause and the solution is to make sure\
    \ only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths\
    \ that we search based on your env.\n  warn(msg)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fa3d848-9028-4c25-be76-e27f73042d8f/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145:\
    \ UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul\
    \ is supported for your GPU!\n  warn(msg)\n\nThen when I run `print(pipe(\"Explain\
    \ to me the difference between nuclear fission and fusion.\"))`\n\nRuntimeError:\
    \ probability tensor contains either `inf`, `nan` or element < 0"
  created_at: 2023-04-20 17:26:59+00:00
  edited: false
  hidden: false
  id: 644183f34c2acf3398aa9834
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T18:39:14.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah, that''s all working - it''s just that the model hits overflow
          on that input in 8-bit. This can happen. IIRC this seemed to happen on the
          V100, not A10, but may be just coincidence. Try an A10, or a smaller model.</p>

          '
        raw: Yeah, that's all working - it's just that the model hits overflow on
          that input in 8-bit. This can happen. IIRC this seemed to happen on the
          V100, not A10, but may be just coincidence. Try an A10, or a smaller model.
        updatedAt: '2023-04-20T18:39:14.008Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - n940767
    id: 644186d2e46e14ed559531d9
    type: comment
  author: srowen
  content: Yeah, that's all working - it's just that the model hits overflow on that
    input in 8-bit. This can happen. IIRC this seemed to happen on the V100, not A10,
    but may be just coincidence. Try an A10, or a smaller model.
  created_at: 2023-04-20 17:39:14+00:00
  edited: false
  hidden: false
  id: 644186d2e46e14ed559531d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-21T15:21:15.000Z'
    data:
      status: closed
    id: 6442a9ebf8b647fa4f516504
    type: status-change
  author: srowen
  created_at: 2023-04-21 14:21:15+00:00
  id: 6442a9ebf8b647fa4f516504
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Can we run this model on CPU?
