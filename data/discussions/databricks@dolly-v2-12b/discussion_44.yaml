!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvaropp
conflicting_files: null
created_at: 2023-04-19 14:11:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
      fullname: Alvaro Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvaropp
      type: user
    createdAt: '2023-04-19T15:11:33.000Z'
    data:
      edited: false
      editors:
      - alvaropp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
          fullname: Alvaro Perez
          isHf: false
          isPro: false
          name: alvaropp
          type: user
        html: "<p>Hi there,</p>\n<p>I'm experimenting with Dolly and I'm trying to\
          \ deploy it in SageMaker. It all works fine but I'm struggling to run inference\u2014\
          there's something going on with the data format I'm passing, but cannot\
          \ figure out what!</p>\n<pre><code>import json\n\nimport boto3\nimport sagemaker\n\
          from sagemaker.huggingface import HuggingFaceModel\n\n\n# %% Deploy new\
          \ model\nrole = sagemaker.get_execution_role()\nhub = {\"HF_MODEL_ID\":\
          \ \"databricks/dolly-v2-12b\", \"HF_TASK\": \"text-generation\"}\n\n# Create\
          \ Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version=\"\
          4.17.0\",\n    pytorch_version=\"1.10.2\",\n    py_version=\"py38\",\n \
          \   env=hub,\n    role=role,\n)\n\n# Deploy model to SageMaker Inference\n\
          predictor = huggingface_model.deploy(\n    initial_instance_count=1,  #\
          \ number of instances\n    instance_type=\"ml.m5.xlarge\",  # ec2 instance\
          \ type\n)\n\npredictor.predict({\"inputs\": \"Once upon a time there \"\
          })\n</code></pre>\n<p>results in:</p>\n<pre><code>ModelError: An error occurred\
          \ (ModelError) when calling the InvokeEndpoint operation: Received client\
          \ error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\"\
          : \"InternalServerException\",\n  \"message\": \"\\u0027gpt_neox\\u0027\"\
          \n}\n</code></pre>\n<p>I've tried using json strings but no luck either.</p>\n\
          <p>Any help appreciated!<br>Cheers.</p>\n"
        raw: "Hi there,\r\n\r\nI'm experimenting with Dolly and I'm trying to deploy\
          \ it in SageMaker. It all works fine but I'm struggling to run inference\u2014\
          there's something going on with the data format I'm passing, but cannot\
          \ figure out what!\r\n\r\n```\r\nimport json\r\n\r\nimport boto3\r\nimport\
          \ sagemaker\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\n\r\n\
          \r\n# %% Deploy new model\r\nrole = sagemaker.get_execution_role()\r\nhub\
          \ = {\"HF_MODEL_ID\": \"databricks/dolly-v2-12b\", \"HF_TASK\": \"text-generation\"\
          }\r\n\r\n# Create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
          \n    transformers_version=\"4.17.0\",\r\n    pytorch_version=\"1.10.2\"\
          ,\r\n    py_version=\"py38\",\r\n    env=hub,\r\n    role=role,\r\n)\r\n\
          \r\n# Deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n    initial_instance_count=1,  # number of instances\r\n    instance_type=\"\
          ml.m5.xlarge\",  # ec2 instance type\r\n)\r\n\r\npredictor.predict({\"inputs\"\
          : \"Once upon a time there \"})\r\n```\r\n\r\nresults in:\r\n```\r\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\r\n  \"code\"\
          : 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\\\
          u0027gpt_neox\\u0027\"\r\n}\r\n```\r\n\r\nI've tried using json strings\
          \ but no luck either.\r\n\r\nAny help appreciated!\r\nCheers."
        updatedAt: '2023-04-19T15:11:33.954Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - clang-kodex
    id: 644004a5e44f30a72322765d
    type: comment
  author: alvaropp
  content: "Hi there,\r\n\r\nI'm experimenting with Dolly and I'm trying to deploy\
    \ it in SageMaker. It all works fine but I'm struggling to run inference\u2014\
    there's something going on with the data format I'm passing, but cannot figure\
    \ out what!\r\n\r\n```\r\nimport json\r\n\r\nimport boto3\r\nimport sagemaker\r\
    \nfrom sagemaker.huggingface import HuggingFaceModel\r\n\r\n\r\n# %% Deploy new\
    \ model\r\nrole = sagemaker.get_execution_role()\r\nhub = {\"HF_MODEL_ID\": \"\
    databricks/dolly-v2-12b\", \"HF_TASK\": \"text-generation\"}\r\n\r\n# Create Hugging\
    \ Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n    transformers_version=\"\
    4.17.0\",\r\n    pytorch_version=\"1.10.2\",\r\n    py_version=\"py38\",\r\n \
    \   env=hub,\r\n    role=role,\r\n)\r\n\r\n# Deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n    initial_instance_count=1,  # number\
    \ of instances\r\n    instance_type=\"ml.m5.xlarge\",  # ec2 instance type\r\n\
    )\r\n\r\npredictor.predict({\"inputs\": \"Once upon a time there \"})\r\n```\r\
    \n\r\nresults in:\r\n```\r\nModelError: An error occurred (ModelError) when calling\
    \ the InvokeEndpoint operation: Received client error (400) from primary with\
    \ message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\
    \n  \"message\": \"\\u0027gpt_neox\\u0027\"\r\n}\r\n```\r\n\r\nI've tried using\
    \ json strings but no luck either.\r\n\r\nAny help appreciated!\r\nCheers."
  created_at: 2023-04-19 14:11:33+00:00
  edited: false
  hidden: false
  id: 644004a5e44f30a72322765d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T15:53:52.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That''s really a question for HF / Sagemaker, doesn''t look related
          to this model per se</p>

          '
        raw: That's really a question for HF / Sagemaker, doesn't look related to
          this model per se
        updatedAt: '2023-04-19T15:53:52.787Z'
      numEdits: 0
      reactions: []
    id: 64400e902113f7dfcb50ae44
    type: comment
  author: srowen
  content: That's really a question for HF / Sagemaker, doesn't look related to this
    model per se
  created_at: 2023-04-19 14:53:52+00:00
  edited: false
  hidden: false
  id: 64400e902113f7dfcb50ae44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646868425893-61980eaa3619d373ad154f7c.png?w=200&h=200&f=face
      fullname: Emily Webber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edubz
      type: user
    createdAt: '2023-04-19T20:07:48.000Z'
    data:
      edited: true
      editors:
      - edubz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646868425893-61980eaa3619d373ad154f7c.png?w=200&h=200&f=face
          fullname: Emily Webber
          isHf: false
          isPro: false
          name: edubz
          type: user
        html: '<p>Hi there, you''ve got a few items to unpack here. First, you want
          to point to a more recent version of the transformers SDK, ideally one that
          has support for all of the model objects needed for dolly. </p>

          <p>Second, this is a 12B parameter model. That means you are likely going
          to need more than one accelerator to host it. I''m testing this out on my
          end now, and will report back soon what seems to be the smallest number
          of accelerators. If you''re compiling it, you need fewer.</p>

          <p>Third, I would point to a hosting instance that uses accelerators, either
          inferentia (inf1) or NVIDIA (g''s or p''s).</p>

          <p>I''ll respond in a bit with more concrete guidance. In the meantime,
          Phillip has some great examples of doing this end-to-end here!</p>

          <ul>

          <li><a rel="nofollow" href="https://github.com/huggingface/notebooks/tree/main/sagemaker">https://github.com/huggingface/notebooks/tree/main/sagemaker</a></li>

          </ul>

          '
        raw: "Hi there, you've got a few items to unpack here. First, you want to\
          \ point to a more recent version of the transformers SDK, ideally one that\
          \ has support for all of the model objects needed for dolly. \n\nSecond,\
          \ this is a 12B parameter model. That means you are likely going to need\
          \ more than one accelerator to host it. I'm testing this out on my end now,\
          \ and will report back soon what seems to be the smallest number of accelerators.\
          \ If you're compiling it, you need fewer.\n\nThird, I would point to a hosting\
          \ instance that uses accelerators, either inferentia (inf1) or NVIDIA (g's\
          \ or p's).\n\nI'll respond in a bit with more concrete guidance. In the\
          \ meantime, Phillip has some great examples of doing this end-to-end here!\n\
          - https://github.com/huggingface/notebooks/tree/main/sagemaker"
        updatedAt: '2023-04-19T20:50:10.886Z'
      numEdits: 1
      reactions: []
    id: 64404a144164a65ca12e0e6b
    type: comment
  author: edubz
  content: "Hi there, you've got a few items to unpack here. First, you want to point\
    \ to a more recent version of the transformers SDK, ideally one that has support\
    \ for all of the model objects needed for dolly. \n\nSecond, this is a 12B parameter\
    \ model. That means you are likely going to need more than one accelerator to\
    \ host it. I'm testing this out on my end now, and will report back soon what\
    \ seems to be the smallest number of accelerators. If you're compiling it, you\
    \ need fewer.\n\nThird, I would point to a hosting instance that uses accelerators,\
    \ either inferentia (inf1) or NVIDIA (g's or p's).\n\nI'll respond in a bit with\
    \ more concrete guidance. In the meantime, Phillip has some great examples of\
    \ doing this end-to-end here!\n- https://github.com/huggingface/notebooks/tree/main/sagemaker"
  created_at: 2023-04-19 19:07:48+00:00
  edited: true
  hidden: false
  id: 64404a144164a65ca12e0e6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71cc94ce6556fbc9d268768dde8d5b8.svg
      fullname: Landon Lamb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wllamb
      type: user
    createdAt: '2023-04-20T02:21:17.000Z'
    data:
      edited: true
      editors:
      - wllamb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71cc94ce6556fbc9d268768dde8d5b8.svg
          fullname: Landon Lamb
          isHf: false
          isPro: false
          name: wllamb
          type: user
        html: '<p><code>Error [ModelError]: Received client error (400) from primary
          with message "{   "code": 400,   "type": "InternalServerException",   "message":
          "\u0027gpt_neox\u0027" }</code></p>

          <p>I got the same error when trying to run inference after deploying it
          as a SageMaker endpoint. I was trying to find out what went wrong and stumbled
          across this. I''m using dolly-v2-3b rather than 12b.</p>

          '
        raw: "`Error [ModelError]: Received client error (400) from primary with message\
          \ \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"\
          message\": \"\\u0027gpt_neox\\u0027\"\n}`\n\nI got the same error when trying\
          \ to run inference after deploying it as a SageMaker endpoint. I was trying\
          \ to find out what went wrong and stumbled across this. I'm using dolly-v2-3b\
          \ rather than 12b."
        updatedAt: '2023-04-20T02:21:30.581Z'
      numEdits: 1
      reactions: []
    id: 6440a19d194b02fd309b3f1a
    type: comment
  author: wllamb
  content: "`Error [ModelError]: Received client error (400) from primary with message\
    \ \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\"\
    : \"\\u0027gpt_neox\\u0027\"\n}`\n\nI got the same error when trying to run inference\
    \ after deploying it as a SageMaker endpoint. I was trying to find out what went\
    \ wrong and stumbled across this. I'm using dolly-v2-3b rather than 12b."
  created_at: 2023-04-20 01:21:17+00:00
  edited: true
  hidden: false
  id: 6440a19d194b02fd309b3f1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T02:22:54.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Just Googling, looks like this maybe (you need to tell it to use
          a newer transformers or something) <a rel="nofollow" href="https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993">https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993</a></p>

          '
        raw: Just Googling, looks like this maybe (you need to tell it to use a newer
          transformers or something) https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993
        updatedAt: '2023-04-20T02:22:54.213Z'
      numEdits: 0
      reactions: []
    id: 6440a1fef830989e056f399d
    type: comment
  author: srowen
  content: Just Googling, looks like this maybe (you need to tell it to use a newer
    transformers or something) https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993
  created_at: 2023-04-20 01:22:54+00:00
  edited: false
  hidden: false
  id: 6440a1fef830989e056f399d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71cc94ce6556fbc9d268768dde8d5b8.svg
      fullname: Landon Lamb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wllamb
      type: user
    createdAt: '2023-04-20T02:50:04.000Z'
    data:
      edited: false
      editors:
      - wllamb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71cc94ce6556fbc9d268768dde8d5b8.svg
          fullname: Landon Lamb
          isHf: false
          isPro: false
          name: wllamb
          type: user
        html: "<p>If I change my deployment configuration to update to the proper\
          \ transformers version/pytorch/pyversion </p>\n<pre><code>huggingface_model\
          \ = HuggingFaceModel(\n    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n\
          \    py_version='py39',\n    env=hub,\n    role=role, \n)\n</code></pre>\n\
          <p>I get a new error <code>Load model failed: databricks__dolly-v2-3b, error:\
          \ Worker died.</code></p>\n"
        raw: "If I change my deployment configuration to update to the proper transformers\
          \ version/pytorch/pyversion \n```\nhuggingface_model = HuggingFaceModel(\n\
          \ttransformers_version='4.26.0',\n\tpytorch_version='1.13.1',\n\tpy_version='py39',\n\
          \tenv=hub,\n\trole=role, \n)\n```\nI get a new error `Load model failed:\
          \ databricks__dolly-v2-3b, error: Worker died.`"
        updatedAt: '2023-04-20T02:50:04.601Z'
      numEdits: 0
      reactions: []
    id: 6440a85cd4229e14aea644d4
    type: comment
  author: wllamb
  content: "If I change my deployment configuration to update to the proper transformers\
    \ version/pytorch/pyversion \n```\nhuggingface_model = HuggingFaceModel(\n\ttransformers_version='4.26.0',\n\
    \tpytorch_version='1.13.1',\n\tpy_version='py39',\n\tenv=hub,\n\trole=role, \n\
    )\n```\nI get a new error `Load model failed: databricks__dolly-v2-3b, error:\
    \ Worker died.`"
  created_at: 2023-04-20 01:50:04+00:00
  edited: false
  hidden: false
  id: 6440a85cd4229e14aea644d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T03:03:52.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>As above. Likely you aren''t provisioning something too small for
          the model</p>

          '
        raw: As above. Likely you aren't provisioning something too small for the
          model
        updatedAt: '2023-04-20T03:03:52.466Z'
      numEdits: 0
      reactions: []
    id: 6440ab982113f7dfcb5f3360
    type: comment
  author: srowen
  content: As above. Likely you aren't provisioning something too small for the model
  created_at: 2023-04-20 02:03:52+00:00
  edited: false
  hidden: false
  id: 6440ab982113f7dfcb5f3360
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
      fullname: Alvaro Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvaropp
      type: user
    createdAt: '2023-04-20T12:01:14.000Z'
    data:
      edited: false
      editors:
      - alvaropp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
          fullname: Alvaro Perez
          isHf: false
          isPro: false
          name: alvaropp
          type: user
        html: "<p>Thanks for the responses.</p>\n<p>I've been playing with EC2 directly\u2014\
          no SageMaker\u2014and <code>dolly-v2-12b</code> runs fine on a <code>p3.2xlarge</code>\
          \ instance (quick enough for my experiments, anyway!) running the following\
          \ script:</p>\n<pre><code>import torch\nfrom langchain import PromptTemplate,\
          \ LLMChain\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers\
          \ import pipeline\n\n\nprint(\"Loading Dolly...\")\n\ngenerate_text = pipeline(\n\
          \    model=\"databricks/dolly-v2-12b\",\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n    return_full_text=True,\n\
          )\n\n\nprint(\"Prompting Dolly...\")\n\n# template for an instruction with\
          \ input\nprompt_with_context = PromptTemplate(\n    input_variables=[\"\
          instruction\", \"context\"],\n    template=\"{instruction}\\n\\nInput:\\\
          n{context}\",\n)\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\
          \nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n\
          \ncontext = \"\"\"George Washington (February 22, 1732 - December 14, 1799)\
          \ was an American military officer, statesman,\nand Founding Father who\
          \ served as the first president of the United States from 1789 to 1797.\"\
          \"\"\n\nprint(\n    llm_context_chain.predict(\n        instruction=\"When\
          \ was George Washington president?\", context=context\n    ).lstrip()\n\
          )\n</code></pre>\n<p>Now, back to SageMaker: I've then updated dependency\
          \ versions as per the comments above, and I'm now getting a new error regarding\
          \ running out of disk space. I'm using the following code:</p>\n<pre><code>import\
          \ json\n\nimport boto3\nimport sagemaker\nfrom sagemaker.huggingface import\
          \ HuggingFaceModel\n\nrole = sagemaker.get_execution_role()\nhub = {\n \
          \   'HF_MODEL_ID':'databricks/dolly-v2-12b',\n    'HF_TASK':'text-generation'\n\
          }\n\n# Create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n   \
          \ py_version='py39',\n    env=hub,\n    role=role,\n)\n\n# Deploy model\
          \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type=\"ml.p3.2xlarge\",\n    volume_size=512,\n)\n</code></pre>\n\
          <p>This instance should have 512GB of storage, more than enough for <code>dolly-v2-12b</code>\
          \ so not sure what's going on.</p>\n<p>Cheers!</p>\n"
        raw: "Thanks for the responses.\n\nI've been playing with EC2 directly\u2014\
          no SageMaker\u2014and `dolly-v2-12b` runs fine on a `p3.2xlarge` instance\
          \ (quick enough for my experiments, anyway!) running the following script:\n\
          ```\nimport torch\nfrom langchain import PromptTemplate, LLMChain\nfrom\
          \ langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\n\
          \n\nprint(\"Loading Dolly...\")\n\ngenerate_text = pipeline(\n    model=\"\
          databricks/dolly-v2-12b\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n    return_full_text=True,\n)\n\n\nprint(\"Prompting\
          \ Dolly...\")\n\n# template for an instruction with input\nprompt_with_context\
          \ = PromptTemplate(\n    input_variables=[\"instruction\", \"context\"],\n\
          \    template=\"{instruction}\\n\\nInput:\\n{context}\",\n)\n\nhf_pipeline\
          \ = HuggingFacePipeline(pipeline=generate_text)\n\nllm_context_chain = LLMChain(llm=hf_pipeline,\
          \ prompt=prompt_with_context)\n\ncontext = \"\"\"George Washington (February\
          \ 22, 1732 - December 14, 1799) was an American military officer, statesman,\n\
          and Founding Father who served as the first president of the United States\
          \ from 1789 to 1797.\"\"\"\n\nprint(\n    llm_context_chain.predict(\n \
          \       instruction=\"When was George Washington president?\", context=context\n\
          \    ).lstrip()\n)\n```\n\nNow, back to SageMaker: I've then updated dependency\
          \ versions as per the comments above, and I'm now getting a new error regarding\
          \ running out of disk space. I'm using the following code:\n```\nimport\
          \ json\n\nimport boto3\nimport sagemaker\nfrom sagemaker.huggingface import\
          \ HuggingFaceModel\n\nrole = sagemaker.get_execution_role()\nhub = {\n \
          \   'HF_MODEL_ID':'databricks/dolly-v2-12b',\n    'HF_TASK':'text-generation'\n\
          }\n\n# Create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n   \
          \ py_version='py39',\n    env=hub,\n    role=role,\n)\n\n# Deploy model\
          \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type=\"ml.p3.2xlarge\",\n    volume_size=512,\n)\n```\n\n\
          This instance should have 512GB of storage, more than enough for `dolly-v2-12b`\
          \ so not sure what's going on.\n\nCheers!"
        updatedAt: '2023-04-20T12:01:14.668Z'
      numEdits: 0
      reactions: []
    id: 6441298ae46e14ed5589f99c
    type: comment
  author: alvaropp
  content: "Thanks for the responses.\n\nI've been playing with EC2 directly\u2014\
    no SageMaker\u2014and `dolly-v2-12b` runs fine on a `p3.2xlarge` instance (quick\
    \ enough for my experiments, anyway!) running the following script:\n```\nimport\
    \ torch\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.llms import\
    \ HuggingFacePipeline\nfrom transformers import pipeline\n\n\nprint(\"Loading\
    \ Dolly...\")\n\ngenerate_text = pipeline(\n    model=\"databricks/dolly-v2-12b\"\
    ,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
    auto\",\n    return_full_text=True,\n)\n\n\nprint(\"Prompting Dolly...\")\n\n\
    # template for an instruction with input\nprompt_with_context = PromptTemplate(\n\
    \    input_variables=[\"instruction\", \"context\"],\n    template=\"{instruction}\\\
    n\\nInput:\\n{context}\",\n)\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\
    \nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n\n\
    context = \"\"\"George Washington (February 22, 1732 - December 14, 1799) was\
    \ an American military officer, statesman,\nand Founding Father who served as\
    \ the first president of the United States from 1789 to 1797.\"\"\"\n\nprint(\n\
    \    llm_context_chain.predict(\n        instruction=\"When was George Washington\
    \ president?\", context=context\n    ).lstrip()\n)\n```\n\nNow, back to SageMaker:\
    \ I've then updated dependency versions as per the comments above, and I'm now\
    \ getting a new error regarding running out of disk space. I'm using the following\
    \ code:\n```\nimport json\n\nimport boto3\nimport sagemaker\nfrom sagemaker.huggingface\
    \ import HuggingFaceModel\n\nrole = sagemaker.get_execution_role()\nhub = {\n\
    \    'HF_MODEL_ID':'databricks/dolly-v2-12b',\n    'HF_TASK':'text-generation'\n\
    }\n\n# Create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
    \    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n    py_version='py39',\n\
    \    env=hub,\n    role=role,\n)\n\n# Deploy model to SageMaker Inference\npredictor\
    \ = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type=\"\
    ml.p3.2xlarge\",\n    volume_size=512,\n)\n```\n\nThis instance should have 512GB\
    \ of storage, more than enough for `dolly-v2-12b` so not sure what's going on.\n\
    \nCheers!"
  created_at: 2023-04-20 11:01:14+00:00
  edited: false
  hidden: false
  id: 6441298ae46e14ed5589f99c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d35259d23d343f5aeb269d026ee3b66.svg
      fullname: Varun Prakash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vprakash
      type: user
    createdAt: '2023-04-20T14:38:45.000Z'
    data:
      edited: true
      editors:
      - vprakash
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d35259d23d343f5aeb269d026ee3b66.svg
          fullname: Varun Prakash
          isHf: false
          isPro: false
          name: vprakash
          type: user
        html: '<p>I am trying to deploy the dolly-v2-12b in to sagemaker. when trying
          to run inference running in to below errors.</p>

          <p>from sagemaker.huggingface import HuggingFaceModel<br>import sagemaker</p>

          <p>role = sagemaker.get_execution_role()<br>hub = {<br>    ''HF_MODEL_ID'':
          ''databricks/dolly-v2-12b'',<br>    ''HF_TASK'': ''text-generation'',<br>}</p>

          <p>huggingface_model = HuggingFaceModel(<br>    transformers_version=''4.17.0'',<br>    pytorch_version=''1.10.2'',<br>    py_version=''py38'',<br>    env=hub,<br>    role=role<br>)</p>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type=''ml.m5.xlarge'',<br>)</p>

          <p>sample_input = {<br>    ''inputs'': ''Can you please let us know more
          details about your''<br>}</p>

          <p>output = predictor.predict(sample_input)<br>print(output)</p>

          <p>This is leading to,<br>ModelError: An error occurred (ModelError) when
          calling the InvokeEndpoint operation: Received client error (400) from primary
          with message "{<br>  "code": 400,<br>  "type": "InternalServerException",<br>  "message":
          "\u0027gpt_neox\u0027"<br>}</p>

          <p>from sagemaker.huggingface import HuggingFaceModel<br>import sagemaker</p>

          <p>role = sagemaker.get_execution_role()<br>hub = {<br>    ''HF_MODEL_ID'':
          ''databricks/dolly-v2-12b'',<br>    ''HF_TASK'': ''text-generation'',<br>}</p>

          <p>huggingface_model = HuggingFaceModel(<br>    transformers_version=''4.26.0'',<br>    pytorch_version=''1.13.1'',<br>    py_version=''py39'',<br>    env=hub,<br>    role=role,<br>)</p>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type=''ml.m5.xlarge'',<br>)</p>

          <p>sample_input = {<br>    ''inputs'': ''Can you please let us know more
          details about your''<br>}</p>

          <p>output = predictor.predict(sample_input)<br>print(output)</p>

          <p>This is leading to,<br>ModelError: An error occurred (ModelError) when
          calling the InvokeEndpoint operation: Received client error (400) from primary
          with message "{<br>  "code": 400,<br>  "type": "InternalServerException",<br>  "message":
          "Loading this pipeline requires you to execute the code in the pipeline
          file in that repo on your local machine. Make sure you have read the code
          there to avoid malicious use, then set the option <code>trust_remote_code\u003dTrue</code>
          to remove this error."<br>}</p>

          <p>I am not sure what is missing.</p>

          <p>Any help appreciated!</p>

          '
        raw: "I am trying to deploy the dolly-v2-12b in to sagemaker. when trying\
          \ to run inference running in to below errors.\n\nfrom sagemaker.huggingface\
          \ import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n\
          hub = {\n    'HF_MODEL_ID': 'databricks/dolly-v2-12b',\n    'HF_TASK': 'text-generation',\n\
          }\n\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.17.0',\n\
          \    pytorch_version='1.10.2',\n    py_version='py38',\n    env=hub,\n \
          \   role=role\n)\n\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type='ml.m5.xlarge',\n)\n\nsample_input = {\n    'inputs':\
          \ 'Can you please let us know more details about your'\n}\n\noutput = predictor.predict(sample_input)\n\
          print(output)\n\nThis is leading to,\nModelError: An error occurred (ModelError)\
          \ when calling the InvokeEndpoint operation: Received client error (400)\
          \ from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n\n\nfrom sagemaker.huggingface\
          \ import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n\
          hub = {\n    'HF_MODEL_ID': 'databricks/dolly-v2-12b',\n    'HF_TASK': 'text-generation',\n\
          }\n\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.26.0',\n\
          \    pytorch_version='1.13.1',\n    py_version='py39',\n    env=hub,\n \
          \   role=role,\n)\n\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
          \    instance_type='ml.m5.xlarge',\n)\n\nsample_input = {\n    'inputs':\
          \ 'Can you please let us know more details about your'\n}\n\noutput = predictor.predict(sample_input)\n\
          print(output)\n\nThis is leading to,\nModelError: An error occurred (ModelError)\
          \ when calling the InvokeEndpoint operation: Received client error (400)\
          \ from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"Loading this pipeline requires you to execute the code\
          \ in the pipeline file in that repo on your local machine. Make sure you\
          \ have read the code there to avoid malicious use, then set the option `trust_remote_code\\\
          u003dTrue` to remove this error.\"\n}\n\nI am not sure what is missing.\n\
          \nAny help appreciated!"
        updatedAt: '2023-04-20T14:42:43.212Z'
      numEdits: 1
      reactions: []
    id: 64414e754c2acf3398a475a4
    type: comment
  author: vprakash
  content: "I am trying to deploy the dolly-v2-12b in to sagemaker. when trying to\
    \ run inference running in to below errors.\n\nfrom sagemaker.huggingface import\
    \ HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n\
    hub = {\n    'HF_MODEL_ID': 'databricks/dolly-v2-12b',\n    'HF_TASK': 'text-generation',\n\
    }\n\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.17.0',\n\
    \    pytorch_version='1.10.2',\n    py_version='py38',\n    env=hub,\n    role=role\n\
    )\n\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n  \
    \  instance_type='ml.m5.xlarge',\n)\n\nsample_input = {\n    'inputs': 'Can you\
    \ please let us know more details about your'\n}\n\noutput = predictor.predict(sample_input)\n\
    print(output)\n\nThis is leading to,\nModelError: An error occurred (ModelError)\
    \ when calling the InvokeEndpoint operation: Received client error (400) from\
    \ primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
    ,\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n\n\nfrom sagemaker.huggingface\
    \ import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n\
    hub = {\n    'HF_MODEL_ID': 'databricks/dolly-v2-12b',\n    'HF_TASK': 'text-generation',\n\
    }\n\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.26.0',\n\
    \    pytorch_version='1.13.1',\n    py_version='py39',\n    env=hub,\n    role=role,\n\
    )\n\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n  \
    \  instance_type='ml.m5.xlarge',\n)\n\nsample_input = {\n    'inputs': 'Can you\
    \ please let us know more details about your'\n}\n\noutput = predictor.predict(sample_input)\n\
    print(output)\n\nThis is leading to,\nModelError: An error occurred (ModelError)\
    \ when calling the InvokeEndpoint operation: Received client error (400) from\
    \ primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
    ,\n  \"message\": \"Loading this pipeline requires you to execute the code in\
    \ the pipeline file in that repo on your local machine. Make sure you have read\
    \ the code there to avoid malicious use, then set the option `trust_remote_code\\\
    u003dTrue` to remove this error.\"\n}\n\nI am not sure what is missing.\n\nAny\
    \ help appreciated!"
  created_at: 2023-04-20 13:38:45+00:00
  edited: true
  hidden: false
  id: 64414e754c2acf3398a475a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T14:47:55.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Again , your hardware is far too small for this model. An m5.xlarge
          doesn''t even have a GPU. See above.<br>That isn''t the problem here. I''m
          not sure anyone has figured out here how to set trust_remote_code=True,
          which is needed to load the model''s pipeline, in the SM integration.</p>

          '
        raw: 'Again , your hardware is far too small for this model. An m5.xlarge
          doesn''t even have a GPU. See above.

          That isn''t the problem here. I''m not sure anyone has figured out here
          how to set trust_remote_code=True, which is needed to load the model''s
          pipeline, in the SM integration.'
        updatedAt: '2023-04-20T14:47:55.060Z'
      numEdits: 0
      reactions: []
    id: 6441509b4c2acf3398a4a6d9
    type: comment
  author: srowen
  content: 'Again , your hardware is far too small for this model. An m5.xlarge doesn''t
    even have a GPU. See above.

    That isn''t the problem here. I''m not sure anyone has figured out here how to
    set trust_remote_code=True, which is needed to load the model''s pipeline, in
    the SM integration.'
  created_at: 2023-04-20 13:47:55+00:00
  edited: false
  hidden: false
  id: 6441509b4c2acf3398a4a6d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-21T16:53:59.000Z'
    data:
      status: closed
    id: 6442bfa7f8b647fa4f52f21f
    type: status-change
  author: srowen
  created_at: 2023-04-21 15:53:59+00:00
  id: 6442bfa7f8b647fa4f52f21f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d918aa1579c4be3383e472f44db04abc.svg
      fullname: Janet H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: janeth8
      type: user
    createdAt: '2023-04-21T17:08:18.000Z'
    data:
      edited: true
      editors:
      - janeth8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d918aa1579c4be3383e472f44db04abc.svg
          fullname: Janet H
          isHf: false
          isPro: false
          name: janeth8
          type: user
        html: "<p>I was able to set trust_remote_code=True by overriding the default\
          \ method for loading a model following documentation here <a href=\"https://huggingface.co/docs/sagemaker/inference#user-defined-code-and-modules\"\
          >https://huggingface.co/docs/sagemaker/inference#user-defined-code-and-modules</a>.</p>\n\
          <p>I created an <code>inference.py</code> with the following code:</p>\n\
          <pre><code>from transformers import pipeline\nimport torch\n\ndef model_fn(model_dir):\n\
          \    \"\"\"\n    Overrides the default model load function in the HuggingFace\
          \ Deep Learning Container\n    \"\"\"\n    instruct_pipeline = pipeline(model=\"\
          databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True,\
          \ device_map=\"auto\")\n    return instruct_pipeline\n</code></pre>\n<p>and\
          \ <code>requirements.txt</code> with:</p>\n<pre><code>accelerate==0.18.0\n\
          </code></pre>\n<p>Then I followed instructions <a href=\"https://huggingface.co/docs/sagemaker/inference#create-a-model-artifact-for-deployment\"\
          >here</a> for creating  a model artifact and uploaded to s3. Then you can\
          \ deploy an endpoint with:</p>\n<pre><code>from sagemaker.huggingface.model\
          \ import HuggingFaceModel\n\n# create Hugging Face Model Class\nhuggingface_model\
          \ = HuggingFaceModel(\n   model_data=\"s3://your_bucket/your_dolly_path/model.tar.gz\"\
          ,  # path to your trained SageMaker model\n   role=role,               \
          \                             # IAM role with permissions to create an endpoint\n\
          \   transformers_version=\"4.26.0\",                           # Transformers\
          \ version used\n   pytorch_version=\"1.13.1\",                         \
          \       # PyTorch version used\n   py_version='py39',                  \
          \                  # Python version used\n)\n\n# deploy model to SageMaker\
          \ Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n\
          \   instance_type=\"ml.g5.4xlarge\"\n)\n</code></pre>\n<p>Note: I tested\
          \ this with the <code>databricks/dolly-v2-3b</code> model, so the <code>ml.g5.4xlarge</code>\
          \ may not be enough for the larger models</p>\n"
        raw: "I was able to set trust_remote_code=True by overriding the default method\
          \ for loading a model following documentation here https://huggingface.co/docs/sagemaker/inference#user-defined-code-and-modules.\n\
          \nI created an `inference.py` with the following code:\n```\nfrom transformers\
          \ import pipeline\nimport torch\n\ndef model_fn(model_dir):\n    \"\"\"\n\
          \    Overrides the default model load function in the HuggingFace Deep Learning\
          \ Container\n    \"\"\"\n    instruct_pipeline = pipeline(model=\"databricks/dolly-v2-3b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\n    return instruct_pipeline\n```\n\nand `requirements.txt` with:\n\n\
          ```\naccelerate==0.18.0\n```\n\nThen I followed instructions [here](https://huggingface.co/docs/sagemaker/inference#create-a-model-artifact-for-deployment)\
          \ for creating  a model artifact and uploaded to s3. Then you can deploy\
          \ an endpoint with:\n\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\
          \n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \   model_data=\"s3://your_bucket/your_dolly_path/model.tar.gz\",  # path\
          \ to your trained SageMaker model\n   role=role,                       \
          \                     # IAM role with permissions to create an endpoint\n\
          \   transformers_version=\"4.26.0\",                           # Transformers\
          \ version used\n   pytorch_version=\"1.13.1\",                         \
          \       # PyTorch version used\n   py_version='py39',                  \
          \                  # Python version used\n)\n\n# deploy model to SageMaker\
          \ Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n\
          \   instance_type=\"ml.g5.4xlarge\"\n)\n```\n\nNote: I tested this with\
          \ the `databricks/dolly-v2-3b` model, so the `ml.g5.4xlarge` may not be\
          \ enough for the larger models"
        updatedAt: '2023-04-21T17:09:03.967Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - srowen
        - alvaropp
        - mcapizzi
        - ybm11
    id: 6442c3028569978432faba7e
    type: comment
  author: janeth8
  content: "I was able to set trust_remote_code=True by overriding the default method\
    \ for loading a model following documentation here https://huggingface.co/docs/sagemaker/inference#user-defined-code-and-modules.\n\
    \nI created an `inference.py` with the following code:\n```\nfrom transformers\
    \ import pipeline\nimport torch\n\ndef model_fn(model_dir):\n    \"\"\"\n    Overrides\
    \ the default model load function in the HuggingFace Deep Learning Container\n\
    \    \"\"\"\n    instruct_pipeline = pipeline(model=\"databricks/dolly-v2-3b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n \
    \   return instruct_pipeline\n```\n\nand `requirements.txt` with:\n\n```\naccelerate==0.18.0\n\
    ```\n\nThen I followed instructions [here](https://huggingface.co/docs/sagemaker/inference#create-a-model-artifact-for-deployment)\
    \ for creating  a model artifact and uploaded to s3. Then you can deploy an endpoint\
    \ with:\n\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n#\
    \ create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=\"\
    s3://your_bucket/your_dolly_path/model.tar.gz\",  # path to your trained SageMaker\
    \ model\n   role=role,                                            # IAM role with\
    \ permissions to create an endpoint\n   transformers_version=\"4.26.0\",     \
    \                      # Transformers version used\n   pytorch_version=\"1.13.1\"\
    ,                                # PyTorch version used\n   py_version='py39',\
    \                                    # Python version used\n)\n\n# deploy model\
    \ to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n\
    \   instance_type=\"ml.g5.4xlarge\"\n)\n```\n\nNote: I tested this with the `databricks/dolly-v2-3b`\
    \ model, so the `ml.g5.4xlarge` may not be enough for the larger models"
  created_at: 2023-04-21 16:08:18+00:00
  edited: true
  hidden: false
  id: 6442c3028569978432faba7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84cf38f99c6808ba6b3a16c197491663.svg
      fullname: Tim Esler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timesler
      type: user
    createdAt: '2023-04-23T02:08:51.000Z'
    data:
      edited: false
      editors:
      - timesler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84cf38f99c6808ba6b3a16c197491663.svg
          fullname: Tim Esler
          isHf: false
          isPro: false
          name: timesler
          type: user
        html: "<p>Here's a gist showing a working method for deploying the <code>dolly-v2-12b</code>\
          \ model on a <code>g5.4xlarge</code> instance.</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec\"\
          >https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;alvaropp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alvaropp\">@<span class=\"\
          underline\">alvaropp</span></a></span>\n\n\t</span></span> I believe the\
          \ issue with running out of disk space was because the 512GB disk mount\
          \ on SageMaker is at <code>/home/ec2-user/SageMaker</code>, but HuggingFace\
          \ libraries default to storing files in a cache at <code>/home/ec2-user/.cache/...</code>.\
          \ The solution is to set the <code>HF_HOME</code> env var to a location\
          \ under  <code>/home/ec2-user/SageMaker</code>. Importantly, if you set\
          \ the env var in python, make sure you do it <em>before</em> importing HuggingFace\
          \ libraries to make sure it gets used. I've included that in the linked\
          \ gist.</p>\n<p>To get the 12b model running on a <code>g5.4xlarge</code>\
          \ instance, I think you'll also need to set <code>load_in_8bit</code> to\
          \ <code>True</code>.</p>\n"
        raw: 'Here''s a gist showing a working method for deploying the `dolly-v2-12b`
          model on a `g5.4xlarge` instance.


          https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec


          @alvaropp I believe the issue with running out of disk space was because
          the 512GB disk mount on SageMaker is at `/home/ec2-user/SageMaker`, but
          HuggingFace libraries default to storing files in a cache at `/home/ec2-user/.cache/...`.
          The solution is to set the `HF_HOME` env var to a location under  `/home/ec2-user/SageMaker`.
          Importantly, if you set the env var in python, make sure you do it _before_
          importing HuggingFace libraries to make sure it gets used. I''ve included
          that in the linked gist.


          To get the 12b model running on a `g5.4xlarge` instance, I think you''ll
          also need to set `load_in_8bit` to `True`.'
        updatedAt: '2023-04-23T02:08:51.746Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - srowen
        - alvaropp
        - vprakash
        - clang-kodex
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - amita-singh
    id: 644493335298d19c9c087c16
    type: comment
  author: timesler
  content: 'Here''s a gist showing a working method for deploying the `dolly-v2-12b`
    model on a `g5.4xlarge` instance.


    https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec


    @alvaropp I believe the issue with running out of disk space was because the 512GB
    disk mount on SageMaker is at `/home/ec2-user/SageMaker`, but HuggingFace libraries
    default to storing files in a cache at `/home/ec2-user/.cache/...`. The solution
    is to set the `HF_HOME` env var to a location under  `/home/ec2-user/SageMaker`.
    Importantly, if you set the env var in python, make sure you do it _before_ importing
    HuggingFace libraries to make sure it gets used. I''ve included that in the linked
    gist.


    To get the 12b model running on a `g5.4xlarge` instance, I think you''ll also
    need to set `load_in_8bit` to `True`.'
  created_at: 2023-04-23 01:08:51+00:00
  edited: false
  hidden: false
  id: 644493335298d19c9c087c16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
      fullname: Alvaro Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvaropp
      type: user
    createdAt: '2023-04-25T10:48:39.000Z'
    data:
      edited: false
      editors:
      - alvaropp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
          fullname: Alvaro Perez
          isHf: false
          isPro: false
          name: alvaropp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;timesler&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/timesler\">@<span class=\"\
          underline\">timesler</span></a></span>\n\n\t</span></span>, <span data-props=\"\
          {&quot;user&quot;:&quot;janeth8&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/janeth8\">@<span class=\"underline\">janeth8</span></a></span>\n\
          \n\t</span></span> many thanks for the response, that makes sense!</p>\n"
        raw: '@timesler, @janeth8 many thanks for the response, that makes sense!'
        updatedAt: '2023-04-25T10:48:39.510Z'
      numEdits: 0
      reactions: []
    id: 6447b007e54b488070a89948
    type: comment
  author: alvaropp
  content: '@timesler, @janeth8 many thanks for the response, that makes sense!'
  created_at: 2023-04-25 09:48:39+00:00
  edited: false
  hidden: false
  id: 6447b007e54b488070a89948
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
      fullname: Alvaro Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvaropp
      type: user
    createdAt: '2023-04-25T13:18:27.000Z'
    data:
      edited: true
      editors:
      - alvaropp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
          fullname: Alvaro Perez
          isHf: false
          isPro: false
          name: alvaropp
          type: user
        html: "<p>Right, so I've followed <span data-props=\"{&quot;user&quot;:&quot;timesler&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/timesler\"\
          >@<span class=\"underline\">timesler</span></a></span>\n\n\t</span></span>'s\
          \ instructions and I'm running into the following error, which seems to\
          \ be some sort of overflow:</p>\n<pre><code>ModelError: An error occurred\
          \ (ModelError) when calling the InvokeEndpoint operation: Received client\
          \ error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\"\
          : \"InternalServerException\",\n  \"message\": \"probability tensor contains\
          \ either `inf`, `nan` or element \\u003c 0\"\n}\n</code></pre>\n<p>I'm using\
          \ a ml.p3.8xlarge instance, which is perfectly capable of running dolly-v2-12b\
          \ in my experiments using EC2 directly, without SageMaker.</p>\n"
        raw: "Right, so I've followed @timesler's instructions and I'm running into\
          \ the following error, which seems to be some sort of overflow:\n```\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\n  \"code\"\
          : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"probability\
          \ tensor contains either `inf`, `nan` or element \\u003c 0\"\n}\n```\n\n\
          I'm using a ml.p3.8xlarge instance, which is perfectly capable of running\
          \ dolly-v2-12b in my experiments using EC2 directly, without SageMaker."
        updatedAt: '2023-04-25T13:20:52.275Z'
      numEdits: 1
      reactions: []
    id: 6447d3233e498d66918f95ee
    type: comment
  author: alvaropp
  content: "Right, so I've followed @timesler's instructions and I'm running into\
    \ the following error, which seems to be some sort of overflow:\n```\nModelError:\
    \ An error occurred (ModelError) when calling the InvokeEndpoint operation: Received\
    \ client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\"\
    : \"InternalServerException\",\n  \"message\": \"probability tensor contains either\
    \ `inf`, `nan` or element \\u003c 0\"\n}\n```\n\nI'm using a ml.p3.8xlarge instance,\
    \ which is perfectly capable of running dolly-v2-12b in my experiments using EC2\
    \ directly, without SageMaker."
  created_at: 2023-04-25 12:18:27+00:00
  edited: true
  hidden: false
  id: 6447d3233e498d66918f95ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646868425893-61980eaa3619d373ad154f7c.png?w=200&h=200&f=face
      fullname: Emily Webber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edubz
      type: user
    createdAt: '2023-05-05T19:15:03.000Z'
    data:
      edited: false
      editors:
      - edubz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646868425893-61980eaa3619d373ad154f7c.png?w=200&h=200&f=face
          fullname: Emily Webber
          isHf: false
          isPro: false
          name: edubz
          type: user
        html: '<p>Here''s a working version for ya!</p>

          <p><a rel="nofollow" href="https://github.com/dhawalkp/dolly-12b/blob/main/dolly-12b-deepspeed-sagemaker.ipynb">https://github.com/dhawalkp/dolly-12b/blob/main/dolly-12b-deepspeed-sagemaker.ipynb</a></p>

          '
        raw: 'Here''s a working version for ya!


          https://github.com/dhawalkp/dolly-12b/blob/main/dolly-12b-deepspeed-sagemaker.ipynb'
        updatedAt: '2023-05-05T19:15:03.650Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - srowen
        - timesler
    id: 645555b7d55525a4feecf68b
    type: comment
  author: edubz
  content: 'Here''s a working version for ya!


    https://github.com/dhawalkp/dolly-12b/blob/main/dolly-12b-deepspeed-sagemaker.ipynb'
  created_at: 2023-05-05 18:15:03+00:00
  edited: false
  hidden: false
  id: 645555b7d55525a4feecf68b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
      fullname: Alvaro Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvaropp
      type: user
    createdAt: '2023-05-06T07:29:36.000Z'
    data:
      edited: false
      editors:
      - alvaropp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbf2672c8422a81ada625d9532d3d706.svg
          fullname: Alvaro Perez
          isHf: false
          isPro: false
          name: alvaropp
          type: user
        html: "<p>That's great, thanks!</p>\n<p>After a bit of trial an error, noticed\
          \ that <span data-props=\"{&quot;user&quot;:&quot;timesler&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/timesler\">@<span class=\"\
          underline\">timesler</span></a></span>\n\n\t</span></span>'s code (<a rel=\"\
          nofollow\" href=\"https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec\"\
          >https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec</a>)\
          \ works perfectly fine as well.</p>\n<p>I'm not 100% sure of why it works\
          \ on <code>g5.4xlarge</code> and not on <code>ml.p3.8xlarge</code>\u2014\
          they seem to have similar specs!</p>\n"
        raw: "That's great, thanks!\n\nAfter a bit of trial an error, noticed that\
          \ @timesler's code (https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec)\
          \ works perfectly fine as well.\n\nI'm not 100% sure of why it works on\
          \ `g5.4xlarge` and not on `ml.p3.8xlarge`\u2014they seem to have similar\
          \ specs!"
        updatedAt: '2023-05-06T07:29:36.192Z'
      numEdits: 0
      reactions: []
    id: 645601e078c059b099abbc6a
    type: comment
  author: alvaropp
  content: "That's great, thanks!\n\nAfter a bit of trial an error, noticed that @timesler's\
    \ code (https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec) works\
    \ perfectly fine as well.\n\nI'm not 100% sure of why it works on `g5.4xlarge`\
    \ and not on `ml.p3.8xlarge`\u2014they seem to have similar specs!"
  created_at: 2023-05-06 06:29:36+00:00
  edited: false
  hidden: false
  id: 645601e078c059b099abbc6a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Deploying in SageMaker
