!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sagar3745
conflicting_files: null
created_at: 2023-04-13 11:58:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-04-13T12:58:29.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "/backup/Vidya/test1.py",
          line 6, in <br>    generator = pipeline(model=model, tokenizer=tokenizer)<br>  File
          "/backup/Vidya/llama/lib/python3.9/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 712, in pipeline<br>    raise RuntimeError(<br>RuntimeError: Inferring
          the task automatically requires to check the hub with a model_id defined
          as a <code>str</code>.GPTNeoXForCausalLM(<br>  (gpt_neox): GPTNeoXModel(<br>    (embed_in):
          Embedding(50280, 5120)<br>    (layers): ModuleList(<br>      (0-35): 36
          x GPTNeoXLayer(<br>        (input_layernorm): LayerNorm((5120,), eps=1e-05,
          elementwise_affine=True)<br>        (post_attention_layernorm): LayerNorm((5120,),
          eps=1e-05, elementwise_affine=True)<br>        (attention): GPTNeoXAttention(<br>          (rotary_emb):
          RotaryEmbedding()<br>          (query_key_value): Linear(in_features=5120,
          out_features=15360, bias=True)<br>          (dense): Linear(in_features=5120,
          out_features=5120, bias=True)<br>        )<br>        (mlp): GPTNeoXMLP(<br>          (dense_h_to_4h):
          Linear(in_features=5120, out_features=20480, bias=True)<br>          (dense_4h_to_h):
          Linear(in_features=20480, out_features=5120, bias=True)<br>          (act):
          GELUActivation()<br>        )<br>      )<br>    )<br>    (final_layer_norm):
          LayerNorm((5120,), eps=1e-05, elementwise_affine=True)<br>  )<br>  (embed_out):
          Linear(in_features=5120, out_features=50280, bias=False)<br>) is not a valid
          model_id.</p>

          <p>code:-<br>from transformers import AutoTokenizer, AutoModelForCausalLM,
          pipeline</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("/backup/Vidya/dolly-v2-12b")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("/backup/Vidya/dolly-v2-12b")<br>generator
          = pipeline(model=model, tokenizer=tokenizer)<br>print(generator("I can''t
          believe you did such a ")) </p>

          <p>RAM:- 120GB</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"/backup/Vidya/test1.py\"\
          , line 6, in <module>\r\n    generator = pipeline(model=model, tokenizer=tokenizer)\r\
          \n  File \"/backup/Vidya/llama/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 712, in pipeline\r\n    raise RuntimeError(\r\nRuntimeError: Inferring\
          \ the task automatically requires to check the hub with a model_id defined\
          \ as a `str`.GPTNeoXForCausalLM(\r\n  (gpt_neox): GPTNeoXModel(\r\n    (embed_in):\
          \ Embedding(50280, 5120)\r\n    (layers): ModuleList(\r\n      (0-35): 36\
          \ x GPTNeoXLayer(\r\n        (input_layernorm): LayerNorm((5120,), eps=1e-05,\
          \ elementwise_affine=True)\r\n        (post_attention_layernorm): LayerNorm((5120,),\
          \ eps=1e-05, elementwise_affine=True)\r\n        (attention): GPTNeoXAttention(\r\
          \n          (rotary_emb): RotaryEmbedding()\r\n          (query_key_value):\
          \ Linear(in_features=5120, out_features=15360, bias=True)\r\n          (dense):\
          \ Linear(in_features=5120, out_features=5120, bias=True)\r\n        )\r\n\
          \        (mlp): GPTNeoXMLP(\r\n          (dense_h_to_4h): Linear(in_features=5120,\
          \ out_features=20480, bias=True)\r\n          (dense_4h_to_h): Linear(in_features=20480,\
          \ out_features=5120, bias=True)\r\n          (act): GELUActivation()\r\n\
          \        )\r\n      )\r\n    )\r\n    (final_layer_norm): LayerNorm((5120,),\
          \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (embed_out): Linear(in_features=5120,\
          \ out_features=50280, bias=False)\r\n) is not a valid model_id.\r\n\r\n\r\
          \n\r\ncode:- \r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ pipeline\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"/backup/Vidya/dolly-v2-12b\"\
          )\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"/backup/Vidya/dolly-v2-12b\"\
          )\r\ngenerator = pipeline(model=model, tokenizer=tokenizer)\r\nprint(generator(\"\
          I can't believe you did such a \")) \r\n\r\n\r\n\r\nRAM:- 120GB"
        updatedAt: '2023-04-13T12:58:29.588Z'
      numEdits: 0
      reactions: []
    id: 6437fc7570cb1a21df886e25
    type: comment
  author: Sagar3745
  content: "Traceback (most recent call last):\r\n  File \"/backup/Vidya/test1.py\"\
    , line 6, in <module>\r\n    generator = pipeline(model=model, tokenizer=tokenizer)\r\
    \n  File \"/backup/Vidya/llama/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
    , line 712, in pipeline\r\n    raise RuntimeError(\r\nRuntimeError: Inferring\
    \ the task automatically requires to check the hub with a model_id defined as\
    \ a `str`.GPTNeoXForCausalLM(\r\n  (gpt_neox): GPTNeoXModel(\r\n    (embed_in):\
    \ Embedding(50280, 5120)\r\n    (layers): ModuleList(\r\n      (0-35): 36 x GPTNeoXLayer(\r\
    \n        (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\r\
    \n        (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\r\
    \n        (attention): GPTNeoXAttention(\r\n          (rotary_emb): RotaryEmbedding()\r\
    \n          (query_key_value): Linear(in_features=5120, out_features=15360, bias=True)\r\
    \n          (dense): Linear(in_features=5120, out_features=5120, bias=True)\r\n\
    \        )\r\n        (mlp): GPTNeoXMLP(\r\n          (dense_h_to_4h): Linear(in_features=5120,\
    \ out_features=20480, bias=True)\r\n          (dense_4h_to_h): Linear(in_features=20480,\
    \ out_features=5120, bias=True)\r\n          (act): GELUActivation()\r\n     \
    \   )\r\n      )\r\n    )\r\n    (final_layer_norm): LayerNorm((5120,), eps=1e-05,\
    \ elementwise_affine=True)\r\n  )\r\n  (embed_out): Linear(in_features=5120, out_features=50280,\
    \ bias=False)\r\n) is not a valid model_id.\r\n\r\n\r\n\r\ncode:- \r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM, pipeline\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    /backup/Vidya/dolly-v2-12b\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    /backup/Vidya/dolly-v2-12b\")\r\ngenerator = pipeline(model=model, tokenizer=tokenizer)\r\
    \nprint(generator(\"I can't believe you did such a \")) \r\n\r\n\r\n\r\nRAM:-\
    \ 120GB"
  created_at: 2023-04-13 11:58:29+00:00
  edited: false
  hidden: false
  id: 6437fc7570cb1a21df886e25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T13:26:15.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>What do you have in "/backup/Vidya/dolly-v2-12b" ? it is saying
          that is not a saved model folder. Just pass "databricks/dolly-v2-12b" instead</p>

          '
        raw: What do you have in "/backup/Vidya/dolly-v2-12b" ? it is saying that
          is not a saved model folder. Just pass "databricks/dolly-v2-12b" instead
        updatedAt: '2023-04-13T13:26:15.431Z'
      numEdits: 0
      reactions: []
    id: 643802f7f8a71f96bcde6ffb
    type: comment
  author: srowen
  content: What do you have in "/backup/Vidya/dolly-v2-12b" ? it is saying that is
    not a saved model folder. Just pass "databricks/dolly-v2-12b" instead
  created_at: 2023-04-13 12:26:15+00:00
  edited: false
  hidden: false
  id: 643802f7f8a71f96bcde6ffb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-04-14T03:47:22.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>I have some space issues, where the model is actually stored. so
          I downloaded the model and give the path to it.</p>

          '
        raw: I have some space issues, where the model is actually stored. so I downloaded
          the model and give the path to it.
        updatedAt: '2023-04-14T03:47:22.704Z'
      numEdits: 0
      reactions: []
    id: 6438cccaa71ecf7effc80608
    type: comment
  author: Sagar3745
  content: I have some space issues, where the model is actually stored. so I downloaded
    the model and give the path to it.
  created_at: 2023-04-14 02:47:22+00:00
  edited: false
  hidden: false
  id: 6438cccaa71ecf7effc80608
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T04:03:16.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I have a different idea. Try adding task=''text-generation'' to
          your pipeline() call?</p>

          '
        raw: I have a different idea. Try adding task='text-generation' to your pipeline()
          call?
        updatedAt: '2023-04-14T04:03:16.631Z'
      numEdits: 0
      reactions: []
    id: 6438d08407583375d7820de7
    type: comment
  author: srowen
  content: I have a different idea. Try adding task='text-generation' to your pipeline()
    call?
  created_at: 2023-04-14 03:03:16+00:00
  edited: false
  hidden: false
  id: 6438d08407583375d7820de7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
      fullname: Vidya Sagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sagar3745
      type: user
    createdAt: '2023-04-14T05:44:27.000Z'
    data:
      edited: false
      editors:
      - Sagar3745
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70498895c03501e327352542c9f440c2.svg
          fullname: Vidya Sagar
          isHf: false
          isPro: false
          name: Sagar3745
          type: user
        html: '<p>OMG, it worked. thanks for the help.<br>I did both things that you
          have mentioned </p>

          <ol>

          <li>direct model name.</li>

          <li>text="text-generation"</li>

          </ol>

          <p>The code I have used:-</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b")<br>generator
          = pipeline(task=''text-generation'', model=model, tokenizer=tokenizer)<br>print(generator("I
          can''t believe you did such a "))</p>

          <p>RAM:- 120GB.<br>while running it went up to like 55GB.</p>

          '
        raw: "OMG, it worked. thanks for the help.\nI did both things that you have\
          \ mentioned \n1. direct model name.\n2. text=\"text-generation\"\n\nThe\
          \ code I have used:-\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\"\
          )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\"\
          )\ngenerator = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n\
          print(generator(\"I can't believe you did such a \"))\n\n\nRAM:- 120GB.\n\
          while running it went up to like 55GB."
        updatedAt: '2023-04-14T05:44:27.587Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - OZ1150
    id: 6438e83bf79e7f4fd93f65d9
    type: comment
  author: Sagar3745
  content: "OMG, it worked. thanks for the help.\nI did both things that you have\
    \ mentioned \n1. direct model name.\n2. text=\"text-generation\"\n\nThe code I\
    \ have used:-\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
    \ pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\"\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\"\
    )\ngenerator = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n\
    print(generator(\"I can't believe you did such a \"))\n\n\nRAM:- 120GB.\nwhile\
    \ running it went up to like 55GB."
  created_at: 2023-04-14 04:44:27+00:00
  edited: false
  hidden: false
  id: 6438e83bf79e7f4fd93f65d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T12:12:50.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>OK that makes some sense. I think Matt is fixing the custom pipeline
          class for this model to set that task type and that should resolve too.</p>

          '
        raw: OK that makes some sense. I think Matt is fixing the custom pipeline
          class for this model to set that task type and that should resolve too.
        updatedAt: '2023-04-14T12:12:50.400Z'
      numEdits: 0
      reactions: []
    id: 64394342c5f84418d04d7932
    type: comment
  author: srowen
  content: OK that makes some sense. I think Matt is fixing the custom pipeline class
    for this model to set that task type and that should resolve too.
  created_at: 2023-04-14 11:12:50+00:00
  edited: false
  hidden: false
  id: 64394342c5f84418d04d7932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:08:04.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Should be all set now. See the latest examples in the HF model pages.</p>

          '
        raw: Should be all set now. See the latest examples in the HF model pages.
        updatedAt: '2023-04-19T19:08:04.718Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64403c142113f7dfcb55260f
    id: 64403c142113f7dfcb55260e
    type: comment
  author: srowen
  content: Should be all set now. See the latest examples in the HF model pages.
  created_at: 2023-04-19 18:08:04+00:00
  edited: false
  hidden: false
  id: 64403c142113f7dfcb55260e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:08:04.000Z'
    data:
      status: closed
    id: 64403c142113f7dfcb55260f
    type: status-change
  author: srowen
  created_at: 2023-04-19 18:08:04+00:00
  id: 64403c142113f7dfcb55260f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Error while running.
