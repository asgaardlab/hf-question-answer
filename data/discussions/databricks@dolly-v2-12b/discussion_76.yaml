!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SaiVikhyath
conflicting_files: null
created_at: 2023-06-11 04:53:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c75cead4b2a01939fbaa613b06ad3ca6.svg
      fullname: Sai Vikhyath Kudhroli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaiVikhyath
      type: user
    createdAt: '2023-06-11T05:53:42.000Z'
    data:
      edited: false
      editors:
      - SaiVikhyath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9804326891899109
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c75cead4b2a01939fbaa613b06ad3ca6.svg
          fullname: Sai Vikhyath Kudhroli
          isHf: false
          isPro: false
          name: SaiVikhyath
          type: user
        html: '<p>Is there a way that I can restrict the dolly model to answer a question
          only based on the information that I provide? I have tried different formats,
          but Dolly considers the general knowledge that it has to answer the question.
          </p>

          <p>Example prompt: Company A secures a deal from Company B for supplying
          resistors. Company C is setting up a new branch for manufacturing bulbs
          in Japan. Consider only this information and tell if company A supplies
          resistors to company C.</p>

          <p>Here, the answer should have been No, because there is no direct relationship
          between A and C for resistors, but the response from dolly is as follows:</p>

          <p>Yes, Company A is a global leader in supplying resistors and is located
          in Japan. Company C is also located in Japan. So, company A can supply company
          C with resistors.</p>

          <p>I have tried many such prompts but dolly doesn''t seem to restrict itself
          to the context that I provide it. Can I get some assistance here?</p>

          '
        raw: "Is there a way that I can restrict the dolly model to answer a question\
          \ only based on the information that I provide? I have tried different formats,\
          \ but Dolly considers the general knowledge that it has to answer the question.\
          \ \r\n\r\nExample prompt: Company A secures a deal from Company B for supplying\
          \ resistors. Company C is setting up a new branch for manufacturing bulbs\
          \ in Japan. Consider only this information and tell if company A supplies\
          \ resistors to company C.\r\n\r\nHere, the answer should have been No, because\
          \ there is no direct relationship between A and C for resistors, but the\
          \ response from dolly is as follows:\r\n\r\nYes, Company A is a global leader\
          \ in supplying resistors and is located in Japan. Company C is also located\
          \ in Japan. So, company A can supply company C with resistors.\r\n\r\n\r\
          \nI have tried many such prompts but dolly doesn't seem to restrict itself\
          \ to the context that I provide it. Can I get some assistance here?"
        updatedAt: '2023-06-11T05:53:42.395Z'
      numEdits: 0
      reactions: []
    id: 648561663e1dd2d1fe6c5b82
    type: comment
  author: SaiVikhyath
  content: "Is there a way that I can restrict the dolly model to answer a question\
    \ only based on the information that I provide? I have tried different formats,\
    \ but Dolly considers the general knowledge that it has to answer the question.\
    \ \r\n\r\nExample prompt: Company A secures a deal from Company B for supplying\
    \ resistors. Company C is setting up a new branch for manufacturing bulbs in Japan.\
    \ Consider only this information and tell if company A supplies resistors to company\
    \ C.\r\n\r\nHere, the answer should have been No, because there is no direct relationship\
    \ between A and C for resistors, but the response from dolly is as follows:\r\n\
    \r\nYes, Company A is a global leader in supplying resistors and is located in\
    \ Japan. Company C is also located in Japan. So, company A can supply company\
    \ C with resistors.\r\n\r\n\r\nI have tried many such prompts but dolly doesn't\
    \ seem to restrict itself to the context that I provide it. Can I get some assistance\
    \ here?"
  created_at: 2023-06-11 04:53:42+00:00
  edited: false
  hidden: false
  id: 648561663e1dd2d1fe6c5b82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-11T12:36:46.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780117869377136
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You can''t do that with LLMs in general, no. After all at some level
          it has to learn language from training data to answer your prompt. You can
          try further prompt engineering like, "you are an expert that will answer
          this question logically".</p>

          '
        raw: You can't do that with LLMs in general, no. After all at some level it
          has to learn language from training data to answer your prompt. You can
          try further prompt engineering like, "you are an expert that will answer
          this question logically".
        updatedAt: '2023-06-11T12:36:46.984Z'
      numEdits: 0
      reactions: []
    id: 6485bfdea3893fa104f97b6a
    type: comment
  author: srowen
  content: You can't do that with LLMs in general, no. After all at some level it
    has to learn language from training data to answer your prompt. You can try further
    prompt engineering like, "you are an expert that will answer this question logically".
  created_at: 2023-06-11 11:36:46+00:00
  edited: false
  hidden: false
  id: 6485bfdea3893fa104f97b6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-17T14:32:59.000Z'
    data:
      status: closed
    id: 648dc41b941869a01cfed563
    type: status-change
  author: srowen
  created_at: 2023-06-17 13:32:59+00:00
  id: 648dc41b941869a01cfed563
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 76
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Restricting dolly model to answer a closed question
