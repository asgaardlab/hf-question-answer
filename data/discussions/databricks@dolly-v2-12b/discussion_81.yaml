!!python/object:huggingface_hub.community.DiscussionWithDetails
author: apadros01
conflicting_files: null
created_at: 2023-06-28 12:46:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
      fullname: Alex Padros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apadros01
      type: user
    createdAt: '2023-06-28T13:46:19.000Z'
    data:
      edited: false
      editors:
      - apadros01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7616502046585083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
          fullname: Alex Padros
          isHf: false
          isPro: false
          name: apadros01
          type: user
        html: '<p>Hello, I''d like to use Dolly in a very similar manner as in <a
          href="/databricks/dolly-v2-12b/discussions/73">#73</a> , but in my case
          the 12B parameters model. I paste below the general idea of the code, which
          is the same than in <a href="/databricks/dolly-v2-12b/discussions/73">#73</a>
          :</p>

          <hr>

          <p>query = ("Question here")<br>docs = knowledge_base.similarity_search(query)</p>

          <p>from langchain.llms import HuggingFacePipeline<br>hf_pipeline = HuggingFacePipeline(pipeline=generate_text)</p>

          <p>import torch<br>from transformers import pipeline<br>generate_text =
          pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,<br>                         trust_remote_code=True,
          device_map="auto", return_full_text=True)</p>

          <p>#chain = load_qa_chain(llm=hf_pipeline, chain_type="stuff") # load question
          answering chain<br>#response = chain.run(input_documents = docs, question
          = query, return_full_text=True) # run the chain<br>#display(response)</p>

          <hr>

          <p>My question is, would it be possible to run this code in a production
          environment using as GPU an NVIDIA RTX A2000 ? </p>

          <p>Otherwise, would it be possible with two, three or four of these?</p>

          <p>If not, what kind of GPU would you recommend for my task that is economically
          viable?</p>

          <p>Thank you in advance.</p>

          '
        raw: "Hello, I'd like to use Dolly in a very similar manner as in #73 , but\
          \ in my case the 12B parameters model. I paste below the general idea of\
          \ the code, which is the same than in #73 :\r\n\r\n-----------------------------------\r\
          \nquery = (\"Question here\")\r\ndocs = knowledge_base.similarity_search(query)\r\
          \n\r\nfrom langchain.llms import HuggingFacePipeline\r\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\r\
          \n\r\nimport torch\r\nfrom transformers import pipeline\r\ngenerate_text\
          \ = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\r\
          \n                         trust_remote_code=True, device_map=\"auto\",\
          \ return_full_text=True)\r\n\r\n#chain = load_qa_chain(llm=hf_pipeline,\
          \ chain_type=\"stuff\") # load question answering chain \r\n#response =\
          \ chain.run(input_documents = docs, question = query, return_full_text=True)\
          \ # run the chain\r\n#display(response)\r\n-------------------------------------\r\
          \n\r\nMy question is, would it be possible to run this code in a production\
          \ environment using as GPU an NVIDIA RTX A2000 ? \r\n\r\nOtherwise, would\
          \ it be possible with two, three or four of these?\r\n\r\nIf not, what kind\
          \ of GPU would you recommend for my task that is economically viable?\r\n\
          \r\n\r\nThank you in advance.\r\n\r\n"
        updatedAt: '2023-06-28T13:46:19.147Z'
      numEdits: 0
      reactions: []
    id: 649c39ab1de774e30a06fe62
    type: comment
  author: apadros01
  content: "Hello, I'd like to use Dolly in a very similar manner as in #73 , but\
    \ in my case the 12B parameters model. I paste below the general idea of the code,\
    \ which is the same than in #73 :\r\n\r\n-----------------------------------\r\
    \nquery = (\"Question here\")\r\ndocs = knowledge_base.similarity_search(query)\r\
    \n\r\nfrom langchain.llms import HuggingFacePipeline\r\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\r\
    \n\r\nimport torch\r\nfrom transformers import pipeline\r\ngenerate_text = pipeline(model=\"\
    databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\r\n                   \
    \      trust_remote_code=True, device_map=\"auto\", return_full_text=True)\r\n\
    \r\n#chain = load_qa_chain(llm=hf_pipeline, chain_type=\"stuff\") # load question\
    \ answering chain \r\n#response = chain.run(input_documents = docs, question =\
    \ query, return_full_text=True) # run the chain\r\n#display(response)\r\n-------------------------------------\r\
    \n\r\nMy question is, would it be possible to run this code in a production environment\
    \ using as GPU an NVIDIA RTX A2000 ? \r\n\r\nOtherwise, would it be possible with\
    \ two, three or four of these?\r\n\r\nIf not, what kind of GPU would you recommend\
    \ for my task that is economically viable?\r\n\r\n\r\nThank you in advance.\r\n\
    \r\n"
  created_at: 2023-06-28 12:46:19+00:00
  edited: false
  hidden: false
  id: 649c39ab1de774e30a06fe62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-28T14:19:08.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9887462854385376
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Looks like the A2000 has only 6GB of RAM, which is pretty small
          for this. For 12B you would need at least 3 to even load in 8-bit, and more
          like 3 or 4 for 16-bit. You want a GPU with at least 16GB of RAM. </p>

          '
        raw: 'Looks like the A2000 has only 6GB of RAM, which is pretty small for
          this. For 12B you would need at least 3 to even load in 8-bit, and more
          like 3 or 4 for 16-bit. You want a GPU with at least 16GB of RAM. '
        updatedAt: '2023-06-28T14:19:08.755Z'
      numEdits: 0
      reactions: []
    id: 649c415c336584fa4d92ba2b
    type: comment
  author: srowen
  content: 'Looks like the A2000 has only 6GB of RAM, which is pretty small for this.
    For 12B you would need at least 3 to even load in 8-bit, and more like 3 or 4
    for 16-bit. You want a GPU with at least 16GB of RAM. '
  created_at: 2023-06-28 13:19:08+00:00
  edited: false
  hidden: false
  id: 649c415c336584fa4d92ba2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
      fullname: Alex Padros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apadros01
      type: user
    createdAt: '2023-06-28T14:34:45.000Z'
    data:
      edited: false
      editors:
      - apadros01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8668702244758606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
          fullname: Alex Padros
          isHf: false
          isPro: false
          name: apadros01
          type: user
        html: '<p>Okey, thanks a lot! So it is possible for my use case to run the
          code in production environment with, for instance, 3 or 4 GPUs of 6GB of
          RAM each? </p>

          '
        raw: 'Okey, thanks a lot! So it is possible for my use case to run the code
          in production environment with, for instance, 3 or 4 GPUs of 6GB of RAM
          each? '
        updatedAt: '2023-06-28T14:34:45.536Z'
      numEdits: 0
      reactions: []
    id: 649c45050431942ba8e30074
    type: comment
  author: apadros01
  content: 'Okey, thanks a lot! So it is possible for my use case to run the code
    in production environment with, for instance, 3 or 4 GPUs of 6GB of RAM each? '
  created_at: 2023-06-28 13:34:45+00:00
  edited: false
  hidden: false
  id: 649c45050431942ba8e30074
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-28T14:37:11.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9585649371147156
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah, but it won''t work well - spreading over multiple GPUs and
          running in 8-bit. Use a smaller model like 3B and you can get away with
          2 x GPUs and 16-bit</p>

          '
        raw: Yeah, but it won't work well - spreading over multiple GPUs and running
          in 8-bit. Use a smaller model like 3B and you can get away with 2 x GPUs
          and 16-bit
        updatedAt: '2023-06-28T14:37:11.454Z'
      numEdits: 0
      reactions: []
    id: 649c4597bbc74ae4d1a8c48f
    type: comment
  author: srowen
  content: Yeah, but it won't work well - spreading over multiple GPUs and running
    in 8-bit. Use a smaller model like 3B and you can get away with 2 x GPUs and 16-bit
  created_at: 2023-06-28 13:37:11+00:00
  edited: false
  hidden: false
  id: 649c4597bbc74ae4d1a8c48f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
      fullname: Alex Padros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apadros01
      type: user
    createdAt: '2023-06-28T15:07:16.000Z'
    data:
      edited: false
      editors:
      - apadros01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9867624640464783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
          fullname: Alex Padros
          isHf: false
          isPro: false
          name: apadros01
          type: user
        html: '<p>So the key factor is 16-bit, no? How many GPUs of those would I
          need for the 7B model?</p>

          '
        raw: So the key factor is 16-bit, no? How many GPUs of those would I need
          for the 7B model?
        updatedAt: '2023-06-28T15:07:16.116Z'
      numEdits: 0
      reactions: []
    id: 649c4ca47bc1f09aee0bc7da
    type: comment
  author: apadros01
  content: So the key factor is 16-bit, no? How many GPUs of those would I need for
    the 7B model?
  created_at: 2023-06-28 14:07:16+00:00
  edited: false
  hidden: false
  id: 649c4ca47bc1f09aee0bc7da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-28T15:31:19.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9243037104606628
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You can do the math. 7B x 2 bytes = 14GB at least. 3 x 6GB GPUs
          should give you room for the model and for inference</p>

          '
        raw: You can do the math. 7B x 2 bytes = 14GB at least. 3 x 6GB GPUs should
          give you room for the model and for inference
        updatedAt: '2023-06-28T15:31:19.669Z'
      numEdits: 0
      reactions: []
    id: 649c52479bf2dd5a3cd2d3e3
    type: comment
  author: srowen
  content: You can do the math. 7B x 2 bytes = 14GB at least. 3 x 6GB GPUs should
    give you room for the model and for inference
  created_at: 2023-06-28 14:31:19+00:00
  edited: false
  hidden: false
  id: 649c52479bf2dd5a3cd2d3e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/104c1f927f7c6bc2939cb741ce058fa0.svg
      fullname: Alex Padros
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apadros01
      type: user
    createdAt: '2023-06-29T06:46:42.000Z'
    data:
      status: closed
    id: 649d28d21bafbcc83acc3666
    type: status-change
  author: apadros01
  created_at: 2023-06-29 05:46:42+00:00
  id: 649d28d21bafbcc83acc3666
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 81
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Minimum GPU requirements for knowledge base task in production environment
