!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xy-covey
conflicting_files: null
created_at: 2023-04-12 21:08:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
      fullname: XY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xy-covey
      type: user
    createdAt: '2023-04-12T22:08:24.000Z'
    data:
      edited: false
      editors:
      - xy-covey
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
          fullname: XY
          isHf: false
          isPro: false
          name: xy-covey
          type: user
        html: '<p>How can I provide context to the instruct_pipeline func?<br>e.g.
          I want to provide a paragraph as context and ask questions like "how many
          years has Jon lived in Miami?"</p>

          '
        raw: "How can I provide context to the instruct_pipeline func?\r\ne.g. I want\
          \ to provide a paragraph as context and ask questions like \"how many years\
          \ has Jon lived in Miami?\""
        updatedAt: '2023-04-12T22:08:24.969Z'
      numEdits: 0
      reactions: []
    id: 64372bd8b8ea86e5032888e0
    type: comment
  author: xy-covey
  content: "How can I provide context to the instruct_pipeline func?\r\ne.g. I want\
    \ to provide a paragraph as context and ask questions like \"how many years has\
    \ Jon lived in Miami?\""
  created_at: 2023-04-12 21:08:24+00:00
  edited: false
  hidden: false
  id: 64372bd8b8ea86e5032888e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-12T22:10:28.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You just put it in the input however you like, same with any text-generation
          model. You can send a string like "Jon likes to fish and has red hair. He
          has lived in Miami since 1998. He moved from Tallahassee.  How many years
          has Jon lived in Miami?" </p>

          <p>langchain can help you build this on top of an LLM, helping you look
          up context relevant to the question and stuff it in a prompt for you automatically,
          before passing to the LLM.</p>

          '
        raw: "You just put it in the input however you like, same with any text-generation\
          \ model. You can send a string like \"Jon likes to fish and has red hair.\
          \ He has lived in Miami since 1998. He moved from Tallahassee.  How many\
          \ years has Jon lived in Miami?\" \n\nlangchain can help you build this\
          \ on top of an LLM, helping you look up context relevant to the question\
          \ and stuff it in a prompt for you automatically, before passing to the\
          \ LLM."
        updatedAt: '2023-04-12T22:10:28.590Z'
      numEdits: 0
      reactions: []
    id: 64372c549e8d019de8356da7
    type: comment
  author: srowen
  content: "You just put it in the input however you like, same with any text-generation\
    \ model. You can send a string like \"Jon likes to fish and has red hair. He has\
    \ lived in Miami since 1998. He moved from Tallahassee.  How many years has Jon\
    \ lived in Miami?\" \n\nlangchain can help you build this on top of an LLM, helping\
    \ you look up context relevant to the question and stuff it in a prompt for you\
    \ automatically, before passing to the LLM."
  created_at: 2023-04-12 21:10:28+00:00
  edited: false
  hidden: false
  id: 64372c549e8d019de8356da7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-12T22:24:48.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: "<p>You can also try using an instruction of the form below. Some of\
          \ the data the model was trained on had this form. It can either be in the\
          \ instruction itself as <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/srowen\"\
          >@<span class=\"underline\">srowen</span></a></span>\n\n\t</span></span>\
          \ mentioned or below as input. Longer context probably work better as an\
          \ input as below. </p>\n<pre><code>how many years has Jon lived in Miami?\n\
          \nInput:\nContext about John.\n</code></pre>\n"
        raw: "You can also try using an instruction of the form below. Some of the\
          \ data the model was trained on had this form. It can either be in the instruction\
          \ itself as @srowen mentioned or below as input. Longer context probably\
          \ work better as an input as below. \n\n```\nhow many years has Jon lived\
          \ in Miami?\n\nInput:\nContext about John.\n```"
        updatedAt: '2023-04-12T22:24:48.448Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - xy-covey
    id: 64372fb0a1010492a8a5b0b0
    type: comment
  author: matthayes
  content: "You can also try using an instruction of the form below. Some of the data\
    \ the model was trained on had this form. It can either be in the instruction\
    \ itself as @srowen mentioned or below as input. Longer context probably work\
    \ better as an input as below. \n\n```\nhow many years has Jon lived in Miami?\n\
    \nInput:\nContext about John.\n```"
  created_at: 2023-04-12 21:24:48+00:00
  edited: false
  hidden: false
  id: 64372fb0a1010492a8a5b0b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
      fullname: XY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xy-covey
      type: user
    createdAt: '2023-04-12T22:33:46.000Z'
    data:
      edited: false
      editors:
      - xy-covey
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
          fullname: XY
          isHf: false
          isPro: false
          name: xy-covey
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> Thanks. What if\
          \ the paragraph is semi structured data like resume and I want to ask questions\
          \ like \"how many years has this candidate worked at company ABC?\"</p>\n"
        raw: '@srowen Thanks. What if the paragraph is semi structured data like resume
          and I want to ask questions like "how many years has this candidate worked
          at company ABC?"'
        updatedAt: '2023-04-12T22:33:46.598Z'
      numEdits: 0
      reactions: []
    id: 643731ca9e9ac888d2ea2088
    type: comment
  author: xy-covey
  content: '@srowen Thanks. What if the paragraph is semi structured data like resume
    and I want to ask questions like "how many years has this candidate worked at
    company ABC?"'
  created_at: 2023-04-12 21:33:46+00:00
  edited: false
  hidden: false
  id: 643731ca9e9ac888d2ea2088
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
      fullname: XY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xy-covey
      type: user
    createdAt: '2023-04-12T22:56:01.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/1cc964f7d10c95f0337784b0423c0787.svg
          fullname: XY
          isHf: false
          isPro: false
          name: xy-covey
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-12T23:04:25.849Z'
      numEdits: 0
      reactions: []
    id: 643737013cb6c4b57079abb9
    type: comment
  author: xy-covey
  content: This comment has been hidden
  created_at: 2023-04-12 21:56:01+00:00
  edited: true
  hidden: true
  id: 643737013cb6c4b57079abb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-12T23:06:04.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>The input to these types of LLMs needs to be text. You would have
          to extract the text from a document first in order to feed it into a prompt.
          Langchain has some related tools here for extracting text chunks from PDFs,
          etc that might come in handy, alongside all the other things it does. But
          you could extract the text however you can.</p>

          <p>There are many reasons it could be slow, but should be more like seconds
          on a large GPU. It''s hard to say without knowing how you are using it on
          what hardware. Like, if you are running on CPU only, yes it will take forever.
          See the github repo for more information <a rel="nofollow" href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a></p>

          '
        raw: 'The input to these types of LLMs needs to be text. You would have to
          extract the text from a document first in order to feed it into a prompt.
          Langchain has some related tools here for extracting text chunks from PDFs,
          etc that might come in handy, alongside all the other things it does. But
          you could extract the text however you can.


          There are many reasons it could be slow, but should be more like seconds
          on a large GPU. It''s hard to say without knowing how you are using it on
          what hardware. Like, if you are running on CPU only, yes it will take forever.
          See the github repo for more information https://github.com/databrickslabs/dolly'
        updatedAt: '2023-04-12T23:06:04.677Z'
      numEdits: 0
      reactions: []
    id: 6437395c1c434dbfbeb1da35
    type: comment
  author: srowen
  content: 'The input to these types of LLMs needs to be text. You would have to extract
    the text from a document first in order to feed it into a prompt. Langchain has
    some related tools here for extracting text chunks from PDFs, etc that might come
    in handy, alongside all the other things it does. But you could extract the text
    however you can.


    There are many reasons it could be slow, but should be more like seconds on a
    large GPU. It''s hard to say without knowing how you are using it on what hardware.
    Like, if you are running on CPU only, yes it will take forever. See the github
    repo for more information https://github.com/databrickslabs/dolly'
  created_at: 2023-04-12 22:06:04+00:00
  edited: false
  hidden: false
  id: 6437395c1c434dbfbeb1da35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb472b70e66fe9e644bfba0d01b47cb9.svg
      fullname: Girijesh Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Girijesh1996
      type: user
    createdAt: '2023-04-13T14:42:20.000Z'
    data:
      edited: true
      editors:
      - Girijesh1996
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb472b70e66fe9e644bfba0d01b47cb9.svg
          fullname: Girijesh Singh
          isHf: false
          isPro: false
          name: Girijesh1996
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span>, I looked at the\
          \ link <a rel=\"nofollow\" href=\"https://github.com/databrickslabs/dolly\"\
          >https://github.com/databrickslabs/dolly</a> for any code snippet which\
          \ can be used to provide context for question answering, but I couldn't\
          \ find anything. The nearest example I was able to find is on the link <a\
          \ href=\"https://huggingface.co/databricks/dolly-v1-6b\">https://huggingface.co/databricks/dolly-v1-6b</a>,\
          \ which is being used for generation task. Could you please point out it\
          \ any code snippet which can be used for providing context and performing\
          \ closed QA. Thanks<br><span data-props=\"{&quot;user&quot;:&quot;xy-covey&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xy-covey\"\
          >@<span class=\"underline\">xy-covey</span></a></span>\n\n\t</span></span>\
          \ It would be amazing if you can provide your code snippet for your resume\
          \ question answering please. Thanks</p>\n"
        raw: 'Hi @srowen, I looked at the link https://github.com/databrickslabs/dolly
          for any code snippet which can be used to provide context for question answering,
          but I couldn''t find anything. The nearest example I was able to find is
          on the link https://huggingface.co/databricks/dolly-v1-6b, which is being
          used for generation task. Could you please point out it any code snippet
          which can be used for providing context and performing closed QA. Thanks

          @xy-covey It would be amazing if you can provide your code snippet for your
          resume question answering please. Thanks'
        updatedAt: '2023-04-13T14:46:09.670Z'
      numEdits: 1
      reactions: []
    id: 643814cc067c51eb8b345235
    type: comment
  author: Girijesh1996
  content: 'Hi @srowen, I looked at the link https://github.com/databrickslabs/dolly
    for any code snippet which can be used to provide context for question answering,
    but I couldn''t find anything. The nearest example I was able to find is on the
    link https://huggingface.co/databricks/dolly-v1-6b, which is being used for generation
    task. Could you please point out it any code snippet which can be used for providing
    context and performing closed QA. Thanks

    @xy-covey It would be amazing if you can provide your code snippet for your resume
    question answering please. Thanks'
  created_at: 2023-04-13 13:42:20+00:00
  edited: true
  hidden: false
  id: 643814cc067c51eb8b345235
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T14:48:08.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>The code isn''t any different. The input is. You put context in
          the string you supply, as I mentioned above. This is how all similar LLMs
          take context as input. Have you looked at langchain? May be more what you''re
          looking for to put on top of this model.</p>

          '
        raw: The code isn't any different. The input is. You put context in the string
          you supply, as I mentioned above. This is how all similar LLMs take context
          as input. Have you looked at langchain? May be more what you're looking
          for to put on top of this model.
        updatedAt: '2023-04-13T14:48:08.757Z'
      numEdits: 0
      reactions: []
    id: 6438162878e36ab7b68b9c83
    type: comment
  author: srowen
  content: The code isn't any different. The input is. You put context in the string
    you supply, as I mentioned above. This is how all similar LLMs take context as
    input. Have you looked at langchain? May be more what you're looking for to put
    on top of this model.
  created_at: 2023-04-13 13:48:08+00:00
  edited: false
  hidden: false
  id: 6438162878e36ab7b68b9c83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-17T20:04:39.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>Please see the updated model card for examples on how to provide
          context.  It should now be pretty easy to do this with LangChain given the
          updated pipeline code.</p>

          '
        raw: Please see the updated model card for examples on how to provide context.  It
          should now be pretty easy to do this with LangChain given the updated pipeline
          code.
        updatedAt: '2023-04-17T20:04:39.894Z'
      numEdits: 0
      reactions: []
      relatedEventId: 643da657d4dcedc3186a27f8
    id: 643da657d4dcedc3186a27f7
    type: comment
  author: matthayes
  content: Please see the updated model card for examples on how to provide context.  It
    should now be pretty easy to do this with LangChain given the updated pipeline
    code.
  created_at: 2023-04-17 19:04:39+00:00
  edited: false
  hidden: false
  id: 643da657d4dcedc3186a27f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-17T20:04:39.000Z'
    data:
      status: closed
    id: 643da657d4dcedc3186a27f8
    type: status-change
  author: matthayes
  created_at: 2023-04-17 19:04:39+00:00
  id: 643da657d4dcedc3186a27f8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: provide context to instruct_pipeline
