!!python/object:huggingface_hub.community.DiscussionWithDetails
author: murali4aji
conflicting_files: null
created_at: 2023-05-19 08:20:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c93332dd952b6e5bc7d8c1aeb58ab44.svg
      fullname: Murali Mohan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: murali4aji
      type: user
    createdAt: '2023-05-19T09:20:10.000Z'
    data:
      edited: false
      editors:
      - murali4aji
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c93332dd952b6e5bc7d8c1aeb58ab44.svg
          fullname: Murali Mohan
          isHf: false
          isPro: false
          name: murali4aji
          type: user
        html: '<p>I''m using Nvidia A10 which has 4 GPUs. I made multiple inferences
          in parallel. I got responses. When I check the GPU usage, it seems only
          one GPU has been used. How can I use all 4 GPUs to make the inferences concurrently?
          I tried many ways like DataParallel. It''s not working. Pls provide any
          suggestions </p>

          '
        raw: 'I''m using Nvidia A10 which has 4 GPUs. I made multiple inferences in
          parallel. I got responses. When I check the GPU usage, it seems only one
          GPU has been used. How can I use all 4 GPUs to make the inferences concurrently?
          I tried many ways like DataParallel. It''s not working. Pls provide any
          suggestions '
        updatedAt: '2023-05-19T09:20:10.672Z'
      numEdits: 0
      reactions: []
    id: 64673f4a374fe5728d3e3a81
    type: comment
  author: murali4aji
  content: 'I''m using Nvidia A10 which has 4 GPUs. I made multiple inferences in
    parallel. I got responses. When I check the GPU usage, it seems only one GPU has
    been used. How can I use all 4 GPUs to make the inferences concurrently? I tried
    many ways like DataParallel. It''s not working. Pls provide any suggestions '
  created_at: 2023-05-19 08:20:10+00:00
  edited: false
  hidden: false
  id: 64673f4a374fe5728d3e3a81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-19T11:56:15.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You would have to run different processes, each pointing to a different
          GPU - cuda:0, cuda:1, etc. Not sure how you are generating, so hard to say
          more</p>

          '
        raw: You would have to run different processes, each pointing to a different
          GPU - cuda:0, cuda:1, etc. Not sure how you are generating, so hard to say
          more
        updatedAt: '2023-05-19T11:56:15.881Z'
      numEdits: 0
      reactions: []
    id: 646763dfa48c2b6f0d5f1811
    type: comment
  author: srowen
  content: You would have to run different processes, each pointing to a different
    GPU - cuda:0, cuda:1, etc. Not sure how you are generating, so hard to say more
  created_at: 2023-05-19 10:56:15+00:00
  edited: false
  hidden: false
  id: 646763dfa48c2b6f0d5f1811
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c93332dd952b6e5bc7d8c1aeb58ab44.svg
      fullname: Murali Mohan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: murali4aji
      type: user
    createdAt: '2023-05-19T15:05:46.000Z'
    data:
      edited: false
      editors:
      - murali4aji
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c93332dd952b6e5bc7d8c1aeb58ab44.svg
          fullname: Murali Mohan
          isHf: false
          isPro: false
          name: murali4aji
          type: user
        html: '<p>Thanks for your reply. I created an API using fast API. When I make
          the inference call by API, I can see all the GPUs used.</p>

          '
        raw: Thanks for your reply. I created an API using fast API. When I make the
          inference call by API, I can see all the GPUs used.
        updatedAt: '2023-05-19T15:05:46.830Z'
      numEdits: 0
      reactions: []
    id: 6467904aa48c2b6f0d6254fb
    type: comment
  author: murali4aji
  content: Thanks for your reply. I created an API using fast API. When I make the
    inference call by API, I can see all the GPUs used.
  created_at: 2023-05-19 14:05:46+00:00
  edited: false
  hidden: false
  id: 6467904aa48c2b6f0d6254fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-19T15:07:14.000Z'
    data:
      status: closed
    id: 646790a2e92e2372d5d0408b
    type: status-change
  author: srowen
  created_at: 2023-05-19 14:07:14+00:00
  id: 646790a2e92e2372d5d0408b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Inference with dolly model using multiple GPUs
