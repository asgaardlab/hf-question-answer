!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jacobgoss
conflicting_files: null
created_at: 2023-04-13 13:00:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
      fullname: Jacob Goss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jacobgoss
      type: user
    createdAt: '2023-04-13T14:00:50.000Z'
    data:
      edited: false
      editors:
      - jacobgoss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
          fullname: Jacob Goss
          isHf: false
          isPro: false
          name: jacobgoss
          type: user
        html: "<p>I have been running dolly-v2-12b quantized and have tried to copy\
          \ the config from <a rel=\"nofollow\" href=\"https://github.com/databrickslabs/dolly\"\
          >https://github.com/databrickslabs/dolly</a>. I have found the model likes\
          \ to hallucinate in responses or have lots of repetition. It often returns\
          \ multiple responses to the same instruction. Is everyone experiencing this\
          \ and has anyone found config/prompt changes to resolve this?</p>\n<p>e.g.<br>Instruction:\
          \ </p>\n<p><code>give me 5 examples of colours</code></p>\n<p>Response:\
          \ </p>\n<pre><code>green, blue, orange, red, yellow\n\n\nGreen - plants\
          \ and trees contain chlorophyll, which is colourless.\nBlue - Paint contains\
          \ cobalt or ultramarine, which is a pale blue.\nOrange -Found in carrots,\
          \ and many other fruits and vegetables.\nRed - Colour found in most plants\
          \ and animals, as well as many flowers.\nYellow - Paint contains Ferric\
          \ Oxalate, which gives it its colour.\n\n\nHere are some more examples of\
          \ colours:\nWhite - Found in snow and frost, as well as pure desert sand.\n\
          Black - The absence of colour.\nMagenta - Found in Middle Eastern rugs,\
          \ as well as the rosseta, a traditional Spanish outfit.\nGreen blue - Often\
          \ refers to the spectrum of colour when blue and green are put together.\n\
          \n\nHere are 5 more examples of colours:\nWhite - Often found in clothing\
          \ and other items, as well as behind modern decor.\nBlack - Always the absence\
          \ of colour.\nMagenta - Deep, intense red colour makes it a good choice\
          \ for bloodstain pattern.\nGreen blue - Often refers to the spectrum of\
          \ colour when blue and green are put together.\n\n\nHere are\n</code></pre>\n\
          <p>Setup:</p>\n<p>I have copied to prompting technique from their github:\
          \ <a rel=\"nofollow\" href=\"https://github.com/databrickslabs/dolly/blob/master/training/consts.py\"\
          >https://github.com/databrickslabs/dolly/blob/master/training/consts.py</a></p>\n\
          <p>Which looks like:</p>\n<pre><code>Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\n###\
          \ Instruction:\ngive me 5 examples of colours\n\n### Response\n</code></pre>\n\
          <p>And generation config to match their github</p>\n<pre><code>generation_config\
          \ = GenerationConfig(\n    max_new_tokens=256,\n    top_p=0.92,\n    top_k=0,\n\
          \    do_sample=True\n)\n</code></pre>\n"
        raw: "I have been running dolly-v2-12b quantized and have tried to copy the\
          \ config from https://github.com/databrickslabs/dolly. I have found the\
          \ model likes to hallucinate in responses or have lots of repetition. It\
          \ often returns multiple responses to the same instruction. Is everyone\
          \ experiencing this and has anyone found config/prompt changes to resolve\
          \ this?\r\n\r\ne.g.\r\nInstruction: \r\n\r\n`give me 5 examples of colours`\r\
          \n\r\nResponse: \r\n\r\n```\r\ngreen, blue, orange, red, yellow\r\n\r\n\r\
          \nGreen - plants and trees contain chlorophyll, which is colourless.\r\n\
          Blue - Paint contains cobalt or ultramarine, which is a pale blue.\r\nOrange\
          \ -Found in carrots, and many other fruits and vegetables.\r\nRed - Colour\
          \ found in most plants and animals, as well as many flowers.\r\nYellow -\
          \ Paint contains Ferric Oxalate, which gives it its colour.\r\n\r\n\r\n\
          Here are some more examples of colours:\r\nWhite - Found in snow and frost,\
          \ as well as pure desert sand.\r\nBlack - The absence of colour.\r\nMagenta\
          \ - Found in Middle Eastern rugs, as well as the rosseta, a traditional\
          \ Spanish outfit.\r\nGreen blue - Often refers to the spectrum of colour\
          \ when blue and green are put together.\r\n\r\n\r\nHere are 5 more examples\
          \ of colours:\r\nWhite - Often found in clothing and other items, as well\
          \ as behind modern decor.\r\nBlack - Always the absence of colour.\r\nMagenta\
          \ - Deep, intense red colour makes it a good choice for bloodstain pattern.\r\
          \nGreen blue - Often refers to the spectrum of colour when blue and green\
          \ are put together.\r\n\r\n\r\nHere are\r\n```\r\n\r\nSetup:\r\n\r\nI have\
          \ copied to prompting technique from their github: https://github.com/databrickslabs/dolly/blob/master/training/consts.py\r\
          \n\r\nWhich looks like:\r\n\r\n```\r\nBelow is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\r\n\
          ### Instruction:\r\ngive me 5 examples of colours\r\n\r\n### Response\r\n\
          \r\n```\r\n\r\nAnd generation config to match their github\r\n```\r\ngeneration_config\
          \ = GenerationConfig(\r\n    max_new_tokens=256,\r\n    top_p=0.92,\r\n\
          \    top_k=0,\r\n    do_sample=True\r\n)\r\n```\r\n\r\n\r\n"
        updatedAt: '2023-04-13T14:00:50.856Z'
      numEdits: 0
      reactions: []
    id: 64380b12819f3ab20d174508
    type: comment
  author: jacobgoss
  content: "I have been running dolly-v2-12b quantized and have tried to copy the\
    \ config from https://github.com/databrickslabs/dolly. I have found the model\
    \ likes to hallucinate in responses or have lots of repetition. It often returns\
    \ multiple responses to the same instruction. Is everyone experiencing this and\
    \ has anyone found config/prompt changes to resolve this?\r\n\r\ne.g.\r\nInstruction:\
    \ \r\n\r\n`give me 5 examples of colours`\r\n\r\nResponse: \r\n\r\n```\r\ngreen,\
    \ blue, orange, red, yellow\r\n\r\n\r\nGreen - plants and trees contain chlorophyll,\
    \ which is colourless.\r\nBlue - Paint contains cobalt or ultramarine, which is\
    \ a pale blue.\r\nOrange -Found in carrots, and many other fruits and vegetables.\r\
    \nRed - Colour found in most plants and animals, as well as many flowers.\r\n\
    Yellow - Paint contains Ferric Oxalate, which gives it its colour.\r\n\r\n\r\n\
    Here are some more examples of colours:\r\nWhite - Found in snow and frost, as\
    \ well as pure desert sand.\r\nBlack - The absence of colour.\r\nMagenta - Found\
    \ in Middle Eastern rugs, as well as the rosseta, a traditional Spanish outfit.\r\
    \nGreen blue - Often refers to the spectrum of colour when blue and green are\
    \ put together.\r\n\r\n\r\nHere are 5 more examples of colours:\r\nWhite - Often\
    \ found in clothing and other items, as well as behind modern decor.\r\nBlack\
    \ - Always the absence of colour.\r\nMagenta - Deep, intense red colour makes\
    \ it a good choice for bloodstain pattern.\r\nGreen blue - Often refers to the\
    \ spectrum of colour when blue and green are put together.\r\n\r\n\r\nHere are\r\
    \n```\r\n\r\nSetup:\r\n\r\nI have copied to prompting technique from their github:\
    \ https://github.com/databrickslabs/dolly/blob/master/training/consts.py\r\n\r\
    \nWhich looks like:\r\n\r\n```\r\nBelow is an instruction that describes a task.\
    \ Write a response that appropriately completes the request.\r\n### Instruction:\r\
    \ngive me 5 examples of colours\r\n\r\n### Response\r\n\r\n```\r\n\r\nAnd generation\
    \ config to match their github\r\n```\r\ngeneration_config = GenerationConfig(\r\
    \n    max_new_tokens=256,\r\n    top_p=0.92,\r\n    top_k=0,\r\n    do_sample=True\r\
    \n)\r\n```\r\n\r\n\r\n"
  created_at: 2023-04-13 13:00:50+00:00
  edited: false
  hidden: false
  id: 64380b12819f3ab20d174508
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T18:53:34.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I have experienced this when the EOS token isn''t dealt with properly.
          Are you using the generation code snippet, or a pipeline()? Those should
          work in this respect, but if you''re doing this manually like i have in
          some other cases with HF pipelines, I find you need to explicitly tell the
          pipeline the EOS token. Barring that you can try setting repetition_penalty=0.3,
          that kind of thing, to discourage this.</p>

          '
        raw: I have experienced this when the EOS token isn't dealt with properly.
          Are you using the generation code snippet, or a pipeline()? Those should
          work in this respect, but if you're doing this manually like i have in some
          other cases with HF pipelines, I find you need to explicitly tell the pipeline
          the EOS token. Barring that you can try setting repetition_penalty=0.3,
          that kind of thing, to discourage this.
        updatedAt: '2023-04-13T18:53:34.942Z'
      numEdits: 0
      reactions: []
    id: 64384fae07583375d77ec7b5
    type: comment
  author: srowen
  content: I have experienced this when the EOS token isn't dealt with properly. Are
    you using the generation code snippet, or a pipeline()? Those should work in this
    respect, but if you're doing this manually like i have in some other cases with
    HF pipelines, I find you need to explicitly tell the pipeline the EOS token. Barring
    that you can try setting repetition_penalty=0.3, that kind of thing, to discourage
    this.
  created_at: 2023-04-13 17:53:34+00:00
  edited: false
  hidden: false
  id: 64384fae07583375d77ec7b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-17T20:00:40.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: "<p>Echoing <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/srowen\"\
          >@<span class=\"underline\">srowen</span></a></span>\n\n\t</span></span>,\
          \ It looks like you haven't configured the EOS token.  Make sure you are\
          \ using the <code>pipeline</code>, as this will use the pipeline code in\
          \ this repo for generation.   From your example it appears that maybe the\
          \ response ends after <code>green, blue, orange, red, yellow</code> but\
          \ that the EOS token is being ignored and then the generation continues.</p>\n\
          <p>Following the updated model card instructions, I used LangChain to create\
          \ some examples:</p>\n<pre><code>for _ in range(20):\n  print(llm_chain.predict(instruction=\"\
          give me 5 examples of colours\").lstrip())\n  print(\"======\")\n</code></pre>\n\
          <p>Output:</p>\n<pre><code>blue, red, green, yellow, black\n======\n- red\n\
          - blue\n- yellow\n- green\n- purple\n======\nRed\nBlue\nYellow\nGreen\n\
          Indigo\n======\nviolet, blue, green, yellow and orange\n======\nRed\nYellow\n\
          Blue\nGreen\nPurple\n======\nred\nblue\nyellow\ngreen\norange\n======\n\
          - red\n- blue\n- green\n- orange\n- purple\n======\nblue, yellow, green,\
          \ red, black\n======\nblack, blue, green, yellow, orange\n======\n- orange\n\
          - blue\n- green\n- red\n- purple\n======\nblack, white, yellow, green, blue\n\
          ======\nRed\nYellow\nBlue\nOrange\nGreen\n======\nRed\nYellow\nBlue\nGreen\n\
          Orange\n======\nblack, white, blue, red and green\n======\nblack, white,\
          \ blue, red, yellow\n======\nThe colours can be divided in to 3 primary\
          \ colours, and 2 secondary colours. The primary colours are, Red, Yellow,\
          \ and Blue. The secondary colours are, Orange, and Turquoise.\n======\n\
          Blue, green, orange, purple, red\n======\nred\nblue\nyellow\ngreen\npink\n\
          ======\nblue, red, green, yellow, black\n======\n- red\n- blue\n- green\n\
          - yellow\n- purple\n======\n</code></pre>\n"
        raw: "Echoing @srowen, It looks like you haven't configured the EOS token.\
          \  Make sure you are using the `pipeline`, as this will use the pipeline\
          \ code in this repo for generation.   From your example it appears that\
          \ maybe the response ends after `green, blue, orange, red, yellow` but that\
          \ the EOS token is being ignored and then the generation continues.\n\n\
          Following the updated model card instructions, I used LangChain to create\
          \ some examples:\n\n```\nfor _ in range(20):\n  print(llm_chain.predict(instruction=\"\
          give me 5 examples of colours\").lstrip())\n  print(\"======\")\n```\n\n\
          Output:\n\n```\nblue, red, green, yellow, black\n======\n- red\n- blue\n\
          - yellow\n- green\n- purple\n======\nRed\nBlue\nYellow\nGreen\nIndigo\n\
          ======\nviolet, blue, green, yellow and orange\n======\nRed\nYellow\nBlue\n\
          Green\nPurple\n======\nred\nblue\nyellow\ngreen\norange\n======\n- red\n\
          - blue\n- green\n- orange\n- purple\n======\nblue, yellow, green, red, black\n\
          ======\nblack, blue, green, yellow, orange\n======\n- orange\n- blue\n-\
          \ green\n- red\n- purple\n======\nblack, white, yellow, green, blue\n======\n\
          Red\nYellow\nBlue\nOrange\nGreen\n======\nRed\nYellow\nBlue\nGreen\nOrange\n\
          ======\nblack, white, blue, red and green\n======\nblack, white, blue, red,\
          \ yellow\n======\nThe colours can be divided in to 3 primary colours, and\
          \ 2 secondary colours. The primary colours are, Red, Yellow, and Blue. The\
          \ secondary colours are, Orange, and Turquoise.\n======\nBlue, green, orange,\
          \ purple, red\n======\nred\nblue\nyellow\ngreen\npink\n======\nblue, red,\
          \ green, yellow, black\n======\n- red\n- blue\n- green\n- yellow\n- purple\n\
          ======\n```"
        updatedAt: '2023-04-17T20:00:40.184Z'
      numEdits: 0
      reactions: []
    id: 643da56827c06bb2bf7c7adb
    type: comment
  author: matthayes
  content: "Echoing @srowen, It looks like you haven't configured the EOS token. \
    \ Make sure you are using the `pipeline`, as this will use the pipeline code in\
    \ this repo for generation.   From your example it appears that maybe the response\
    \ ends after `green, blue, orange, red, yellow` but that the EOS token is being\
    \ ignored and then the generation continues.\n\nFollowing the updated model card\
    \ instructions, I used LangChain to create some examples:\n\n```\nfor _ in range(20):\n\
    \  print(llm_chain.predict(instruction=\"give me 5 examples of colours\").lstrip())\n\
    \  print(\"======\")\n```\n\nOutput:\n\n```\nblue, red, green, yellow, black\n\
    ======\n- red\n- blue\n- yellow\n- green\n- purple\n======\nRed\nBlue\nYellow\n\
    Green\nIndigo\n======\nviolet, blue, green, yellow and orange\n======\nRed\nYellow\n\
    Blue\nGreen\nPurple\n======\nred\nblue\nyellow\ngreen\norange\n======\n- red\n\
    - blue\n- green\n- orange\n- purple\n======\nblue, yellow, green, red, black\n\
    ======\nblack, blue, green, yellow, orange\n======\n- orange\n- blue\n- green\n\
    - red\n- purple\n======\nblack, white, yellow, green, blue\n======\nRed\nYellow\n\
    Blue\nOrange\nGreen\n======\nRed\nYellow\nBlue\nGreen\nOrange\n======\nblack,\
    \ white, blue, red and green\n======\nblack, white, blue, red, yellow\n======\n\
    The colours can be divided in to 3 primary colours, and 2 secondary colours. The\
    \ primary colours are, Red, Yellow, and Blue. The secondary colours are, Orange,\
    \ and Turquoise.\n======\nBlue, green, orange, purple, red\n======\nred\nblue\n\
    yellow\ngreen\npink\n======\nblue, red, green, yellow, black\n======\n- red\n\
    - blue\n- green\n- yellow\n- purple\n======\n```"
  created_at: 2023-04-17 19:00:40+00:00
  edited: false
  hidden: false
  id: 643da56827c06bb2bf7c7adb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/749e811586314421a488954a34f39e36.svg
      fullname: Carlos Alas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 7th-Samurai
      type: user
    createdAt: '2023-04-17T20:35:45.000Z'
    data:
      edited: false
      editors:
      - 7th-Samurai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/749e811586314421a488954a34f39e36.svg
          fullname: Carlos Alas
          isHf: false
          isPro: false
          name: 7th-Samurai
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;jacobgoss&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jacobgoss\"\
          >@<span class=\"underline\">jacobgoss</span></a></span>\n\n\t</span></span>\
          \ Quick question: When you say quantized, did you quantize it? Or is it\
          \ available here in the hub?</p>\n"
        raw: 'Hi @jacobgoss Quick question: When you say quantized, did you quantize
          it? Or is it available here in the hub?'
        updatedAt: '2023-04-17T20:35:45.556Z'
      numEdits: 0
      reactions: []
    id: 643dada1317127fb1e2f9961
    type: comment
  author: 7th-Samurai
  content: 'Hi @jacobgoss Quick question: When you say quantized, did you quantize
    it? Or is it available here in the hub?'
  created_at: 2023-04-17 19:35:45+00:00
  edited: false
  hidden: false
  id: 643dada1317127fb1e2f9961
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
      fullname: Jacob Goss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jacobgoss
      type: user
    createdAt: '2023-04-18T08:53:57.000Z'
    data:
      edited: false
      editors:
      - jacobgoss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
          fullname: Jacob Goss
          isHf: false
          isPro: false
          name: jacobgoss
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;7th-Samurai&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/7th-Samurai\"\
          >@<span class=\"underline\">7th-Samurai</span></a></span>\n\n\t</span></span>\
          \ You can quantize the model when loading it from huggingface by using the\
          \ <code>load_in_8bit</code> kwarg like this</p>\n<pre><code>tokenizer =\
          \ AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          databricks/dolly-v2-12b\", device_map='auto', load_in_8bit=True)\n</code></pre>\n\
          <p>I was able to fit the 12b model into a g5d.2xlarge instance on AWS which\
          \ has 32GB RAM and an A10 GPU.</p>\n<p>This does require a couple of libraries\
          \ to be installed like <code>accelerate</code> and <code>bitsandbytes</code></p>\n"
        raw: '@7th-Samurai You can quantize the model when loading it from huggingface
          by using the `load_in_8bit` kwarg like this


          ```

          tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b")

          model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          device_map=''auto'', load_in_8bit=True)

          ```


          I was able to fit the 12b model into a g5d.2xlarge instance on AWS which
          has 32GB RAM and an A10 GPU.


          This does require a couple of libraries to be installed like `accelerate`
          and `bitsandbytes`'
        updatedAt: '2023-04-18T08:53:57.730Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - 7th-Samurai
    id: 643e5aa576082ad7c0eb4f36
    type: comment
  author: jacobgoss
  content: '@7th-Samurai You can quantize the model when loading it from huggingface
    by using the `load_in_8bit` kwarg like this


    ```

    tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b")

    model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b", device_map=''auto'',
    load_in_8bit=True)

    ```


    I was able to fit the 12b model into a g5d.2xlarge instance on AWS which has 32GB
    RAM and an A10 GPU.


    This does require a couple of libraries to be installed like `accelerate` and
    `bitsandbytes`'
  created_at: 2023-04-18 07:53:57+00:00
  edited: false
  hidden: false
  id: 643e5aa576082ad7c0eb4f36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/749e811586314421a488954a34f39e36.svg
      fullname: Carlos Alas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 7th-Samurai
      type: user
    createdAt: '2023-04-18T15:36:33.000Z'
    data:
      edited: false
      editors:
      - 7th-Samurai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/749e811586314421a488954a34f39e36.svg
          fullname: Carlos Alas
          isHf: false
          isPro: false
          name: 7th-Samurai
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jacobgoss&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jacobgoss\">@<span class=\"\
          underline\">jacobgoss</span></a></span>\n\n\t</span></span> Great! Thanks!</p>\n"
        raw: '@jacobgoss Great! Thanks!'
        updatedAt: '2023-04-18T15:36:33.267Z'
      numEdits: 0
      reactions: []
    id: 643eb901e868b8d92825d568
    type: comment
  author: 7th-Samurai
  content: '@jacobgoss Great! Thanks!'
  created_at: 2023-04-18 14:36:33+00:00
  edited: false
  hidden: false
  id: 643eb901e868b8d92825d568
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
      fullname: Jacob Goss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jacobgoss
      type: user
    createdAt: '2023-04-19T13:50:35.000Z'
    data:
      edited: false
      editors:
      - jacobgoss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a15c8e95110e8f3a7d9471a88a0c93b.svg
          fullname: Jacob Goss
          isHf: false
          isPro: false
          name: jacobgoss
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;matthayes&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/matthayes\">@<span class=\"underline\">matthayes</span></a></span>\n\
          \n\t</span></span> Thanks for the advice. </p>\n<p>What's confusing me still\
          \ is that if the EOS token was not configured correctly then the model would\
          \ be generating the EOS token and then continuing to generate after it.\
          \ However the model isn't outputting the EOS token (### End) or the token_id\
          \ for it. </p>\n<p>I have tried the smaller model without quantisation and\
          \ it seemed to work as expected (generating the eos token and stopping generation)\
          \ however the quantized 12b model just never generated the EOS token. </p>\n\
          <p>I wonder if its some issue with the quantization</p>\n"
        raw: "@srowen @matthayes Thanks for the advice. \n\nWhat's confusing me still\
          \ is that if the EOS token was not configured correctly then the model would\
          \ be generating the EOS token and then continuing to generate after it.\
          \ However the model isn't outputting the EOS token (### End) or the token_id\
          \ for it. \n\nI have tried the smaller model without quantisation and it\
          \ seemed to work as expected (generating the eos token and stopping generation)\
          \ however the quantized 12b model just never generated the EOS token. \n\
          \nI wonder if its some issue with the quantization"
        updatedAt: '2023-04-19T13:50:35.286Z'
      numEdits: 0
      reactions: []
    id: 643ff1abdbd88206a8356c9d
    type: comment
  author: jacobgoss
  content: "@srowen @matthayes Thanks for the advice. \n\nWhat's confusing me still\
    \ is that if the EOS token was not configured correctly then the model would be\
    \ generating the EOS token and then continuing to generate after it. However the\
    \ model isn't outputting the EOS token (### End) or the token_id for it. \n\n\
    I have tried the smaller model without quantisation and it seemed to work as expected\
    \ (generating the eos token and stopping generation) however the quantized 12b\
    \ model just never generated the EOS token. \n\nI wonder if its some issue with\
    \ the quantization"
  created_at: 2023-04-19 12:50:35+00:00
  edited: false
  hidden: false
  id: 643ff1abdbd88206a8356c9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e5bde3fc7a6692d35f157ace5f2c8ab.svg
      fullname: Somesh Kumar Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: someshsingh22
      type: user
    createdAt: '2023-05-01T01:26:03.000Z'
    data:
      edited: false
      editors:
      - someshsingh22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e5bde3fc7a6692d35f157ace5f2c8ab.svg
          fullname: Somesh Kumar Singh
          isHf: false
          isPro: false
          name: someshsingh22
          type: user
        html: '<p>Facing the same issue on using quantized model</p>

          '
        raw: Facing the same issue on using quantized model
        updatedAt: '2023-05-01T01:26:03.640Z'
      numEdits: 0
      reactions: []
    id: 644f152ba00f4b11d3a01412
    type: comment
  author: someshsingh22
  content: Facing the same issue on using quantized model
  created_at: 2023-05-01 00:26:03+00:00
  edited: false
  hidden: false
  id: 644f152ba00f4b11d3a01412
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-01T01:44:26.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Are you using the current InstructPIpeline that you get when you
          load with pipeline()? it will at least handle end of sequence as intended.
          It doesn''t mean you can''t get repetition, but if it''s very apparent,
          probably you aren''t doing what this model''s pipeline does</p>

          '
        raw: Are you using the current InstructPIpeline that you get when you load
          with pipeline()? it will at least handle end of sequence as intended. It
          doesn't mean you can't get repetition, but if it's very apparent, probably
          you aren't doing what this model's pipeline does
        updatedAt: '2023-05-01T01:44:26.047Z'
      numEdits: 0
      reactions: []
    id: 644f197acf72e60a5b87ca23
    type: comment
  author: srowen
  content: Are you using the current InstructPIpeline that you get when you load with
    pipeline()? it will at least handle end of sequence as intended. It doesn't mean
    you can't get repetition, but if it's very apparent, probably you aren't doing
    what this model's pipeline does
  created_at: 2023-05-01 00:44:26+00:00
  edited: false
  hidden: false
  id: 644f197acf72e60a5b87ca23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-09T13:39:30.000Z'
    data:
      status: closed
    id: 645a4d1299a55c573259f19a
    type: status-change
  author: srowen
  created_at: 2023-05-09 12:39:30+00:00
  id: 645a4d1299a55c573259f19a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: How to get better responses and prevent repetition?
