!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vinayrks
conflicting_files: null
created_at: 2023-04-12 23:48:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
      fullname: Vinay Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinayrks
      type: user
    createdAt: '2023-04-13T00:48:19.000Z'
    data:
      edited: false
      editors:
      - vinayrks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
          fullname: Vinay Yadav
          isHf: false
          isPro: false
          name: vinayrks
          type: user
        html: '<p>I am getting following error :<br>Could not load model databricks/dolly-v2-12b
          with any of the following classes: (&lt;class<br>''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''&gt;,<br>&lt;class
          ''transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM''&gt;)</p>

          '
        raw: "I am getting following error :\r\nCould not load model databricks/dolly-v2-12b\
          \ with any of the following classes: (<class \r\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>,\
          \ \r\n<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>)"
        updatedAt: '2023-04-13T00:48:19.769Z'
      numEdits: 0
      reactions: []
    id: 64375153cd93f4c9a34c2198
    type: comment
  author: vinayrks
  content: "I am getting following error :\r\nCould not load model databricks/dolly-v2-12b\
    \ with any of the following classes: (<class \r\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>,\
    \ \r\n<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>)"
  created_at: 2023-04-12 23:48:19+00:00
  edited: false
  hidden: false
  id: 64375153cd93f4c9a34c2198
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T01:52:42.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>If you''re on a Mac, many models like this won''t work, I''m pretty
          sure. See <a rel="nofollow" href="https://github.com/databrickslabs/dolly/issues/60">https://github.com/databrickslabs/dolly/issues/60</a></p>

          '
        raw: If you're on a Mac, many models like this won't work, I'm pretty sure.
          See https://github.com/databrickslabs/dolly/issues/60
        updatedAt: '2023-04-13T01:52:42.203Z'
      numEdits: 0
      reactions: []
    id: 6437606ab1071789ec6e3c4c
    type: comment
  author: srowen
  content: If you're on a Mac, many models like this won't work, I'm pretty sure.
    See https://github.com/databrickslabs/dolly/issues/60
  created_at: 2023-04-13 00:52:42+00:00
  edited: false
  hidden: false
  id: 6437606ab1071789ec6e3c4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
      fullname: Vinay Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinayrks
      type: user
    createdAt: '2023-04-13T04:00:17.000Z'
    data:
      edited: false
      editors:
      - vinayrks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
          fullname: Vinay Yadav
          isHf: false
          isPro: false
          name: vinayrks
          type: user
        html: '<p>I am on Ubuntu </p>

          <p>They just updated their documentation and sample code.<br>New sample
          code is working fine at my end now :</p>

          <p>Working code :<br>import torch<br>from transformers import pipeline</p>

          <p>generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")</p>

          '
        raw: "I am on Ubuntu \n\nThey just updated their documentation and sample\
          \ code.\nNew sample code is working fine at my end now :\n\nWorking code\
          \ :\nimport torch\nfrom transformers import pipeline\n\ngenerate_text =\
          \ pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\n"
        updatedAt: '2023-04-13T04:00:17.583Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64377e51369f6f907f5ceca9
    id: 64377e51369f6f907f5ceca8
    type: comment
  author: vinayrks
  content: "I am on Ubuntu \n\nThey just updated their documentation and sample code.\n\
    New sample code is working fine at my end now :\n\nWorking code :\nimport torch\n\
    from transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"
  created_at: 2023-04-13 03:00:17+00:00
  edited: false
  hidden: false
  id: 64377e51369f6f907f5ceca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
      fullname: Vinay Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinayrks
      type: user
    createdAt: '2023-04-13T04:00:17.000Z'
    data:
      status: closed
    id: 64377e51369f6f907f5ceca9
    type: status-change
  author: vinayrks
  created_at: 2023-04-13 03:00:17+00:00
  id: 64377e51369f6f907f5ceca9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-13T04:12:55.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>Great!  Happy the updated code fixed your issue.</p>

          '
        raw: Great!  Happy the updated code fixed your issue.
        updatedAt: '2023-04-13T04:12:55.831Z'
      numEdits: 0
      reactions: []
    id: 643781475824aa4f5cec4d09
    type: comment
  author: matthayes
  content: Great!  Happy the updated code fixed your issue.
  created_at: 2023-04-13 03:12:55+00:00
  edited: false
  hidden: false
  id: 643781475824aa4f5cec4d09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b8b80e6eeeccb1354d0f762786fc3ff.svg
      fullname: Ramesh Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rameshkumargupta
      type: user
    createdAt: '2023-04-14T15:44:31.000Z'
    data:
      edited: false
      editors:
      - rameshkumargupta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b8b80e6eeeccb1354d0f762786fc3ff.svg
          fullname: Ramesh Gupta
          isHf: false
          isPro: false
          name: rameshkumargupta
          type: user
        html: '<p>does not work for me. I am on ubuntu. how is it working for others?</p>

          '
        raw: does not work for me. I am on ubuntu. how is it working for others?
        updatedAt: '2023-04-14T15:44:31.707Z'
      numEdits: 0
      reactions: []
    id: 643974dfcc228b8099b2cd51
    type: comment
  author: rameshkumargupta
  content: does not work for me. I am on ubuntu. how is it working for others?
  created_at: 2023-04-14 14:44:31+00:00
  edited: false
  hidden: false
  id: 643974dfcc228b8099b2cd51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71850c4c30c63c59f273a4dca9afb311.svg
      fullname: saurabh semwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saurabh93
      type: user
    createdAt: '2023-04-15T05:02:22.000Z'
    data:
      edited: false
      editors:
      - saurabh93
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71850c4c30c63c59f273a4dca9afb311.svg
          fullname: saurabh semwal
          isHf: false
          isPro: false
          name: saurabh93
          type: user
        html: '<p>not working for me as well</p>

          '
        raw: not working for me as well
        updatedAt: '2023-04-15T05:02:22.295Z'
      numEdits: 0
      reactions: []
    id: 643a2fde5f7d67fb191b4c35
    type: comment
  author: saurabh93
  content: not working for me as well
  created_at: 2023-04-15 04:02:22+00:00
  edited: false
  hidden: false
  id: 643a2fde5f7d67fb191b4c35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
      fullname: Vinay Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinayrks
      type: user
    createdAt: '2023-04-17T13:39:52.000Z'
    data:
      edited: false
      editors:
      - vinayrks
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1760a32d86f6f643703c2ab361571c72.svg
          fullname: Vinay Yadav
          isHf: false
          isPro: false
          name: vinayrks
          type: user
        html: '<p>what exact problem are you facing?</p>

          '
        raw: what exact problem are you facing?
        updatedAt: '2023-04-17T13:39:52.223Z'
      numEdits: 0
      reactions: []
    id: 643d4c28455be6574c84030c
    type: comment
  author: vinayrks
  content: what exact problem are you facing?
  created_at: 2023-04-17 12:39:52+00:00
  edited: false
  hidden: false
  id: 643d4c28455be6574c84030c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67e871176f3f87832f2af58f7e0b1dc7.svg
      fullname: Usman khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yumyumkills
      type: user
    createdAt: '2023-05-01T12:11:30.000Z'
    data:
      edited: false
      editors:
      - yumyumkills
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67e871176f3f87832f2af58f7e0b1dc7.svg
          fullname: Usman khan
          isHf: false
          isPro: false
          name: yumyumkills
          type: user
        html: "<p>When i use device_map='auto' code works fine when i use cuda , it\
          \ gives error below, i am on windows 10 with rtx2080 and cuda tool kit is\
          \ already installed.</p>\n<p>import torch<br>from transformers import pipeline<br>import\
          \ time</p>\n<h1 id=\"use-dolly-v2-12b-if-youre-using-colab-pro-using-pythia-28b-for-free-colab\"\
          >use dolly-v2-12b if you're using Colab Pro+, using pythia-2.8b for Free\
          \ Colab</h1>\n<p>generate_text = pipeline(<br>    model=\"databricks/dolly-v2-2-8b\"\
          ,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>\
          \    device_map=\"cuda\"<br>)</p>\n<p>def prompt(s):<br>    start = time.time()<br>\
          \    response = generate_text(s)<br>    end = time.time()<br>    print(end\
          \ - start)<br>    return response</p>\n<p>print(prompt(\"First man on the\
          \ moon?\"))</p>\n<p>The argument <code>trust_remote_code</code> is to be\
          \ used with Auto classes. It has no effect here and is ignored.<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 c:\\Users\\\
          Ehsan\\Desktop\\opengpt4alltest1.py:77 in                              \
          \           \u2502<br>\u2502                                           \
          \                                                       \u2502<br>\u2502\
          \   74 from transformers import pipeline                               \
          \                            \u2502<br>\u2502   75 import time         \
          \                                                                      \
          \  \u2502<br>\u2502   76 # use dolly-v2-12b if you're using Colab Pro+,\
          \ using pythia-2.8b for Free Colab             \u2502<br>\u2502 \u2771 77\
          \ generate_text = pipeline(                                            \
          \                       \u2502<br>\u2502   78 \u2502   model=\"databricks/dolly-v2-2-8b\"\
          ,                                                       \u2502<br>\u2502\
          \   79 \u2502   torch_dtype=torch.bfloat16,                            \
          \                                 \u2502<br>\u2502   80 \u2502   trust_remote_code=True,\
          \                                                                 \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502 C:\\Users\\Ehsan\\\
          AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\\
          pipelines_ \u2502<br>\u2502 <em>init</em>_.py:779 in pipeline          \
          \                                                             \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502   776 \u2502   # Forced if\
          \ framework already defined, inferred if it's None                     \
          \      \u2502<br>\u2502   777 \u2502   # Will load the correct model if\
          \ possible                                              \u2502<br>\u2502\
          \   778 \u2502   model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\"\
          : targeted_task[\"pt\"]}                 \u2502<br>\u2502 \u2771 779 \u2502\
          \   framework, model = infer_framework_load_model(                     \
          \                    \u2502<br>\u2502   780 \u2502   \u2502   model,   \
          \                                                                      \
          \    \u2502<br>\u2502   781 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   782 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 C:\\Users\\Ehsan\\AppData\\Local\\Programs\\\
          Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\b \u2502\
          <br>\u2502 ase.py:271 in infer_framework_load_model                    \
          \                                     \u2502<br>\u2502                 \
          \                                                                      \
          \           \u2502<br>\u2502    268 \u2502   \u2502   \u2502   \u2502  \
          \ continue                                                             \
          \     \u2502<br>\u2502    269 \u2502   \u2502                          \
          \                                                           \u2502<br>\u2502\
          \    270 \u2502   \u2502   if isinstance(model, str):                  \
          \                                      \u2502<br>\u2502 \u2771  271 \u2502\
          \   \u2502   \u2502   raise ValueError(f\"Could not load model {model} with\
          \ any of the following cl  \u2502<br>\u2502    272 \u2502              \
          \                                                                      \
          \     \u2502<br>\u2502    273 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
          \ in str(inspect.getmro(model.__clas  \u2502<br>\u2502    274 \u2502   return\
          \ framework, model                                                     \
          \          \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>ValueError: Could not load\
          \ model databricks/dolly-v2-2-8b with any of the following classes: (&lt;class\
          \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,<br>&lt;class\
          \ 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'&gt;).</p>\n"
        raw: "When i use device_map='auto' code works fine when i use cuda , it gives\
          \ error below, i am on windows 10 with rtx2080 and cuda tool kit is already\
          \ installed.\n\n\n\n\nimport torch\nfrom transformers import pipeline\n\
          import time\n# use dolly-v2-12b if you're using Colab Pro+, using pythia-2.8b\
          \ for Free Colab\ngenerate_text = pipeline(\n    model=\"databricks/dolly-v2-2-8b\"\
          , \n    torch_dtype=torch.bfloat16, \n    trust_remote_code=True,\n    device_map=\"\
          cuda\"\n)\n\n\ndef prompt(s):\n    start = time.time()\n    response = generate_text(s)\
          \     \n    end = time.time()\n    print(end - start)\n    return response\n\
          \nprint(prompt(\"First man on the moon?\"))\n\n\nThe argument `trust_remote_code`\
          \ is to be used with Auto classes. It has no effect here and is ignored.\n\
          \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ c:\\Users\\Ehsan\\Desktop\\opengpt4alltest1.py:77 in <module>        \
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502   74 from transformers import pipeline             \
          \                                              \u2502\n\u2502   75 import\
          \ time                                                                 \
          \                \u2502\n\u2502   76 # use dolly-v2-12b if you're using\
          \ Colab Pro+, using pythia-2.8b for Free Colab             \u2502\n\u2502\
          \ \u2771 77 generate_text = pipeline(                                  \
          \                                 \u2502\n\u2502   78 \u2502   model=\"\
          databricks/dolly-v2-2-8b\",                                            \
          \           \u2502\n\u2502   79 \u2502   torch_dtype=torch.bfloat16,   \
          \                                                          \u2502\n\u2502\
          \   80 \u2502   trust_remote_code=True,                                \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 C:\\Users\\Ehsan\\AppData\\Local\\Programs\\Python\\\
          Python39\\lib\\site-packages\\transformers\\pipelines\\_ \u2502\n\u2502\
          \ _init__.py:779 in pipeline                                           \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   776 \u2502   # Forced if framework already defined, inferred\
          \ if it's None                           \u2502\n\u2502   777 \u2502   #\
          \ Will load the correct model if possible                              \
          \                \u2502\n\u2502   778 \u2502   model_classes = {\"tf\":\
          \ targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}                \
          \ \u2502\n\u2502 \u2771 779 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502\n\u2502   780 \u2502  \
          \ \u2502   model,                                                      \
          \                       \u2502\n\u2502   781 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n\u2502 \
          \  782 \u2502   \u2502   config=config,                                \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 C:\\Users\\Ehsan\\AppData\\Local\\Programs\\Python\\\
          Python39\\lib\\site-packages\\transformers\\pipelines\\b \u2502\n\u2502\
          \ ase.py:271 in infer_framework_load_model                             \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    268 \u2502   \u2502   \u2502   \u2502   continue           \
          \                                                       \u2502\n\u2502 \
          \   269 \u2502   \u2502                                                \
          \                                     \u2502\n\u2502    270 \u2502   \u2502\
          \   if isinstance(model, str):                                         \
          \               \u2502\n\u2502 \u2771  271 \u2502   \u2502   \u2502   raise\
          \ ValueError(f\"Could not load model {model} with any of the following cl\
          \  \u2502\n\u2502    272 \u2502                                        \
          \                                                 \u2502\n\u2502    273\
          \ \u2502   framework = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502\n\u2502    274 \u2502   return framework, model              \
          \                                                 \u2502\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F\nValueError: Could not load model databricks/dolly-v2-2-8b with any\
          \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \      \n<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>,\
          \ <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>)."
        updatedAt: '2023-05-01T12:11:30.185Z'
      numEdits: 0
      reactions: []
    id: 644fac7220ba3e3e4be6504e
    type: comment
  author: yumyumkills
  content: "When i use device_map='auto' code works fine when i use cuda , it gives\
    \ error below, i am on windows 10 with rtx2080 and cuda tool kit is already installed.\n\
    \n\n\n\nimport torch\nfrom transformers import pipeline\nimport time\n# use dolly-v2-12b\
    \ if you're using Colab Pro+, using pythia-2.8b for Free Colab\ngenerate_text\
    \ = pipeline(\n    model=\"databricks/dolly-v2-2-8b\", \n    torch_dtype=torch.bfloat16,\
    \ \n    trust_remote_code=True,\n    device_map=\"cuda\"\n)\n\n\ndef prompt(s):\n\
    \    start = time.time()\n    response = generate_text(s)     \n    end = time.time()\n\
    \    print(end - start)\n    return response\n\nprint(prompt(\"First man on the\
    \ moon?\"))\n\n\nThe argument `trust_remote_code` is to be used with Auto classes.\
    \ It has no effect here and is ignored.\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
    \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 c:\\\
    Users\\Ehsan\\Desktop\\opengpt4alltest1.py:77 in <module>                    \
    \                    \u2502\n\u2502                                          \
    \                                                        \u2502\n\u2502   74 from\
    \ transformers import pipeline                                               \
    \            \u2502\n\u2502   75 import time                                 \
    \                                                \u2502\n\u2502   76 # use dolly-v2-12b\
    \ if you're using Colab Pro+, using pythia-2.8b for Free Colab             \u2502\
    \n\u2502 \u2771 77 generate_text = pipeline(                                 \
    \                                  \u2502\n\u2502   78 \u2502   model=\"databricks/dolly-v2-2-8b\"\
    ,                                                       \u2502\n\u2502   79 \u2502\
    \   torch_dtype=torch.bfloat16,                                              \
    \               \u2502\n\u2502   80 \u2502   trust_remote_code=True,         \
    \                                                        \u2502\n\u2502      \
    \                                                                            \
    \                \u2502\n\u2502 C:\\Users\\Ehsan\\AppData\\Local\\Programs\\Python\\\
    Python39\\lib\\site-packages\\transformers\\pipelines\\_ \u2502\n\u2502 _init__.py:779\
    \ in pipeline                                                                \
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502   776 \u2502   # Forced\
    \ if framework already defined, inferred if it's None                        \
    \   \u2502\n\u2502   777 \u2502   # Will load the correct model if possible  \
    \                                            \u2502\n\u2502   778 \u2502   model_classes\
    \ = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}           \
    \      \u2502\n\u2502 \u2771 779 \u2502   framework, model = infer_framework_load_model(\
    \                                         \u2502\n\u2502   780 \u2502   \u2502\
    \   model,                                                                   \
    \          \u2502\n\u2502   781 \u2502   \u2502   model_classes=model_classes,\
    \                                                       \u2502\n\u2502   782 \u2502\
    \   \u2502   config=config,                                                  \
    \                   \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502 C:\\Users\\\
    Ehsan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\\
    pipelines\\b \u2502\n\u2502 ase.py:271 in infer_framework_load_model         \
    \                                                \u2502\n\u2502              \
    \                                                                            \
    \        \u2502\n\u2502    268 \u2502   \u2502   \u2502   \u2502   continue  \
    \                                                                \u2502\n\u2502\
    \    269 \u2502   \u2502                                                     \
    \                                \u2502\n\u2502    270 \u2502   \u2502   if isinstance(model,\
    \ str):                                                        \u2502\n\u2502\
    \ \u2771  271 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load model\
    \ {model} with any of the following cl  \u2502\n\u2502    272 \u2502         \
    \                                                                            \
    \    \u2502\n\u2502    273 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
    \ in str(inspect.getmro(model.__clas  \u2502\n\u2502    274 \u2502   return framework,\
    \ model                                                               \u2502\n\
    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: Could not\
    \ load model databricks/dolly-v2-2-8b with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,      \n<class\
    \ 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class\
    \ 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>)."
  created_at: 2023-05-01 11:11:30+00:00
  edited: false
  hidden: false
  id: 644fac7220ba3e3e4be6504e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-01T12:15:10.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>There are several things wrong here - old model version string and
          device_map="cuda" doesn''t quite make sense. Please start over from the
          given example in the model card. This is also a different discussion now.</p>

          '
        raw: There are several things wrong here - old model version string and device_map="cuda"
          doesn't quite make sense. Please start over from the given example in the
          model card. This is also a different discussion now.
        updatedAt: '2023-05-01T12:15:10.910Z'
      numEdits: 0
      reactions: []
    id: 644fad4e28774bd665d47f6e
    type: comment
  author: srowen
  content: There are several things wrong here - old model version string and device_map="cuda"
    doesn't quite make sense. Please start over from the given example in the model
    card. This is also a different discussion now.
  created_at: 2023-05-01 11:15:10+00:00
  edited: false
  hidden: false
  id: 644fad4e28774bd665d47f6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d60e29c312185e534b6cbfd84e0ce9f.svg
      fullname: Srajan Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgpt18
      type: user
    createdAt: '2023-05-21T23:43:03.000Z'
    data:
      edited: false
      editors:
      - sgpt18
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d60e29c312185e534b6cbfd84e0ce9f.svg
          fullname: Srajan Gupta
          isHf: false
          isPro: false
          name: sgpt18
          type: user
        html: '<p>not sure if there was a solution for it yet?</p>

          '
        raw: not sure if there was a solution for it yet?
        updatedAt: '2023-05-21T23:43:03.030Z'
      numEdits: 0
      reactions: []
    id: 646aac873721aab2edfaf536
    type: comment
  author: sgpt18
  content: not sure if there was a solution for it yet?
  created_at: 2023-05-21 22:43:03+00:00
  edited: false
  hidden: false
  id: 646aac873721aab2edfaf536
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Could not load model error
