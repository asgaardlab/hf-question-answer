!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AeroG
conflicting_files: null
created_at: 2023-05-23 12:16:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
      fullname: Gavita Regunath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroG
      type: user
    createdAt: '2023-05-23T13:16:18.000Z'
    data:
      edited: false
      editors:
      - AeroG
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
          fullname: Gavita Regunath
          isHf: false
          isPro: false
          name: AeroG
          type: user
        html: '<p>Hi there,</p>

          <p>I love using Dolly, so thanks for the great work so far!</p>

          <p>Just wondering if there are any options to use Dolly-v2-12b without GPUs?
          Although all Dolly models are available via huggingface pipelines, based
          on my understanding, you still need suitable hardware (GPUs) to run the
          big models, the -12b model. Would Databricks consider hosting their models
          on HuggingFace  so users are able to use the model via the Inference API?</p>

          <p>Or are there any other ways currently of using Dolly-12b if one does
          not access to GPUs ? </p>

          <p>Thanks.</p>

          '
        raw: "Hi there,\r\n\r\nI love using Dolly, so thanks for the great work so\
          \ far!\r\n\r\nJust wondering if there are any options to use Dolly-v2-12b\
          \ without GPUs? Although all Dolly models are available via huggingface\
          \ pipelines, based on my understanding, you still need suitable hardware\
          \ (GPUs) to run the big models, the -12b model. Would Databricks consider\
          \ hosting their models on HuggingFace  so users are able to use the model\
          \ via the Inference API?\r\n\r\nOr are there any other ways currently of\
          \ using Dolly-12b if one does not access to GPUs ? \r\n\r\nThanks."
        updatedAt: '2023-05-23T13:16:18.923Z'
      numEdits: 0
      reactions: []
    id: 646cbca2c6d119496dd956b4
    type: comment
  author: AeroG
  content: "Hi there,\r\n\r\nI love using Dolly, so thanks for the great work so far!\r\
    \n\r\nJust wondering if there are any options to use Dolly-v2-12b without GPUs?\
    \ Although all Dolly models are available via huggingface pipelines, based on\
    \ my understanding, you still need suitable hardware (GPUs) to run the big models,\
    \ the -12b model. Would Databricks consider hosting their models on HuggingFace\
    \  so users are able to use the model via the Inference API?\r\n\r\nOr are there\
    \ any other ways currently of using Dolly-12b if one does not access to GPUs ?\
    \ \r\n\r\nThanks."
  created_at: 2023-05-23 12:16:18+00:00
  edited: false
  hidden: false
  id: 646cbca2c6d119496dd956b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-23T13:21:33.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>They all already work out of the box on CPUs. They''re just slow.
          I don''t believe HF can serve the model. You can serve them on Databricks
          or other models serving services</p>

          '
        raw: They all already work out of the box on CPUs. They're just slow. I don't
          believe HF can serve the model. You can serve them on Databricks or other
          models serving services
        updatedAt: '2023-05-23T13:21:33.577Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AeroG
    id: 646cbddd87ed262149b19979
    type: comment
  author: srowen
  content: They all already work out of the box on CPUs. They're just slow. I don't
    believe HF can serve the model. You can serve them on Databricks or other models
    serving services
  created_at: 2023-05-23 12:21:33+00:00
  edited: false
  hidden: false
  id: 646cbddd87ed262149b19979
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
      fullname: Gavita Regunath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroG
      type: user
    createdAt: '2023-05-23T13:32:29.000Z'
    data:
      edited: false
      editors:
      - AeroG
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
          fullname: Gavita Regunath
          isHf: false
          isPro: false
          name: AeroG
          type: user
        html: '<p>Ok - so I can serve the models using MLFlow on Databricks (which
          is what I am using). Do you happen to have any guidelines as to how to go
          about doing this? </p>

          <p>Also, I''ve tried the -7b model and instead of being slow, I had errors
          for which upon analysing meant that I needed a better cluster than the standard
          Standard_DS4_v2.</p>

          '
        raw: "Ok - so I can serve the models using MLFlow on Databricks (which is\
          \ what I am using). Do you happen to have any guidelines as to how to go\
          \ about doing this? \n\nAlso, I've tried the -7b model and instead of being\
          \ slow, I had errors for which upon analysing meant that I needed a better\
          \ cluster than the standard Standard_DS4_v2."
        updatedAt: '2023-05-23T13:32:29.892Z'
      numEdits: 0
      reactions: []
    id: 646cc06dc51833760b52a00d
    type: comment
  author: AeroG
  content: "Ok - so I can serve the models using MLFlow on Databricks (which is what\
    \ I am using). Do you happen to have any guidelines as to how to go about doing\
    \ this? \n\nAlso, I've tried the -7b model and instead of being slow, I had errors\
    \ for which upon analysing meant that I needed a better cluster than the standard\
    \ Standard_DS4_v2."
  created_at: 2023-05-23 12:32:29+00:00
  edited: false
  hidden: false
  id: 646cc06dc51833760b52a00d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-23T13:45:49.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>See model serving docs for Databricks. You need the GPU private
          preview which your acct team can help with. </p>

          <p>Not sure about the second, you haven''t said what the problem is.</p>

          '
        raw: "See model serving docs for Databricks. You need the GPU private preview\
          \ which your acct team can help with. \n\nNot sure about the second, you\
          \ haven't said what the problem is."
        updatedAt: '2023-05-23T13:45:49.308Z'
      numEdits: 0
      reactions: []
    id: 646cc38dc23858f139320de4
    type: comment
  author: srowen
  content: "See model serving docs for Databricks. You need the GPU private preview\
    \ which your acct team can help with. \n\nNot sure about the second, you haven't\
    \ said what the problem is."
  created_at: 2023-05-23 12:45:49+00:00
  edited: false
  hidden: false
  id: 646cc38dc23858f139320de4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
      fullname: Gavita Regunath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroG
      type: user
    createdAt: '2023-05-23T18:41:26.000Z'
    data:
      edited: false
      editors:
      - AeroG
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643702c3715232c98000e2c3f2cefda2.svg
          fullname: Gavita Regunath
          isHf: false
          isPro: false
          name: AeroG
          type: user
        html: "<p>Thanks!<br>With regards to the second point. I was working on Q&amp;A\
          \ over PDF's. The following is what I have done after extracting the text\
          \ from PDFs, chunking the files and creating embeddings.<br>The code then\
          \ takes the user question, performs a similarity search and produces the\
          \ output:</p>\n<pre><code>query = (\"Question here\")\ndocs = knowledge_base.similarity_search(query)\n\
          \nfrom langchain.llms import HuggingFacePipeline\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\
          \nimport torch\nfrom transformers import pipeline\ngenerate_text = pipeline(model=\"\
          databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\n                \
          \         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n\
          \n#chain = load_qa_chain(llm=hf_pipeline, chain_type=\"stuff\") # load question\
          \ answering chain \n#response = chain.run(input_documents = docs, question\
          \ = query, return_full_text=True) # run the chain\n#display(response)\n\
          </code></pre>\n<p>Error message:</p>\n<pre><code>Fatal error: The Python\
          \ kernel is unresponsive.\nThe Python process exited with exit code 137\
          \ (SIGKILL: Killed). This may have been caused by an OOM error. Check your\
          \ command's memory usage.\n\nThe last 10 KB of the process's stderr and\
          \ stdout can be found below. See driver logs for full logs.\n</code></pre>\n"
        raw: "Thanks!\nWith regards to the second point. I was working on Q&A over\
          \ PDF's. The following is what I have done after extracting the text from\
          \ PDFs, chunking the files and creating embeddings.\nThe code then takes\
          \ the user question, performs a similarity search and produces the output:\n\
          ```\nquery = (\"Question here\")\ndocs = knowledge_base.similarity_search(query)\n\
          \nfrom langchain.llms import HuggingFacePipeline\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\
          \nimport torch\nfrom transformers import pipeline\ngenerate_text = pipeline(model=\"\
          databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\n                \
          \         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n\
          \n#chain = load_qa_chain(llm=hf_pipeline, chain_type=\"stuff\") # load question\
          \ answering chain \n#response = chain.run(input_documents = docs, question\
          \ = query, return_full_text=True) # run the chain\n#display(response)\n\n\
          ```\nError message:\n```\nFatal error: The Python kernel is unresponsive.\n\
          The Python process exited with exit code 137 (SIGKILL: Killed). This may\
          \ have been caused by an OOM error. Check your command's memory usage.\n\
          \nThe last 10 KB of the process's stderr and stdout can be found below.\
          \ See driver logs for full logs.\n```"
        updatedAt: '2023-05-23T18:41:26.689Z'
      numEdits: 0
      reactions: []
    id: 646d08d6e0c5e39573516c6d
    type: comment
  author: AeroG
  content: "Thanks!\nWith regards to the second point. I was working on Q&A over PDF's.\
    \ The following is what I have done after extracting the text from PDFs, chunking\
    \ the files and creating embeddings.\nThe code then takes the user question, performs\
    \ a similarity search and produces the output:\n```\nquery = (\"Question here\"\
    )\ndocs = knowledge_base.similarity_search(query)\n\nfrom langchain.llms import\
    \ HuggingFacePipeline\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\
    \nimport torch\nfrom transformers import pipeline\ngenerate_text = pipeline(model=\"\
    databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\n                      \
    \   trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n\n#chain\
    \ = load_qa_chain(llm=hf_pipeline, chain_type=\"stuff\") # load question answering\
    \ chain \n#response = chain.run(input_documents = docs, question = query, return_full_text=True)\
    \ # run the chain\n#display(response)\n\n```\nError message:\n```\nFatal error:\
    \ The Python kernel is unresponsive.\nThe Python process exited with exit code\
    \ 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your\
    \ command's memory usage.\n\nThe last 10 KB of the process's stderr and stdout\
    \ can be found below. See driver logs for full logs.\n```"
  created_at: 2023-05-23 17:41:26+00:00
  edited: false
  hidden: false
  id: 646d08d6e0c5e39573516c6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-23T18:46:06.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That''s indeed almost surely OOM. You do not need a cluster, you
          need a larger single instance.</p>

          '
        raw: That's indeed almost surely OOM. You do not need a cluster, you need
          a larger single instance.
        updatedAt: '2023-05-23T18:46:06.765Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AeroG
    id: 646d09eee0c5e39573519796
    type: comment
  author: srowen
  content: That's indeed almost surely OOM. You do not need a cluster, you need a
    larger single instance.
  created_at: 2023-05-23 17:46:06+00:00
  edited: false
  hidden: false
  id: 646d09eee0c5e39573519796
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-25T19:57:18.000Z'
    data:
      status: closed
    id: 646fbd9ebc42f4b00233a6df
    type: status-change
  author: srowen
  created_at: 2023-05-25 18:57:18+00:00
  id: 646fbd9ebc42f4b00233a6df
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 73
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Options for running dolly-v2-12b - no access to GPUs
