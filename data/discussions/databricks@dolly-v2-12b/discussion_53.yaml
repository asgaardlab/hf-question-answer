!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NewsSoup
conflicting_files: null
created_at: 2023-04-25 11:32:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
      fullname: Arno Duvenhage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NewsSoup
      type: user
    createdAt: '2023-04-25T12:32:56.000Z'
    data:
      edited: false
      editors:
      - NewsSoup
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
          fullname: Arno Duvenhage
          isHf: false
          isPro: false
          name: NewsSoup
          type: user
        html: '<p>I made a discovery, but I am busy trying to open a business and
          am the only one working on the website, so it will be months or years before
          I can do anything about it. However, I am writing it here because taking
          the concept further might help produce better language models (not my expertise).</p>

          <p>I have discovered a technique that allows me to reduce the computations
          of a semantic search by 70x. The brief is that you can use Principal Component
          Analysis (PCA) to rotate your embeddings so as to produce an ordered set
          of axes from the largest distribution of points to the smallest. For corpora
          smaller than 1 000 documents, you only need to perform a vector comparison
          of the first 10 principal components or less (the rest don''t even need
          to be calculated). </p>

          <p>A query vector only needs to be rotated using the same transformation
          to map to the same vector space that the PCA produced. Since word embeddings
          tend to have ~700 dimensions, the ability to compare them using only 10
          principal components (the widest shadows the document embeddings can cast
          on 10 dimensions) represents a 70x reduction in computation time for standard
          sentence transformers.</p>

          <p>This got me thinking, if a PCA produces vectors that are optimised for
          searching then can we not take a parent model, do PCA on the vector embeddings
          of a model''s training data, with or without reducing dimensions, to obtain
          the transformation matrix from one coordinate system to the other, and then
          train a student language model using the PCA-optimised embedding dimensions.</p>

          <p>This is far beyond my expertise, I am just clustering documents and summarising
          the topics. I needed to reduce the cost and speed of semantic search, I
          know very little about training models. </p>

          <p>If you find this idea interesting, please take it further. If you can
          use PCA to reduce the number of dimensions in a language model, you can
          have smaller models with high performance. I only ask that you thank me
          for the suggestion in the Acknowledgements. </p>

          '
        raw: "I made a discovery, but I am busy trying to open a business and am the\
          \ only one working on the website, so it will be months or years before\
          \ I can do anything about it. However, I am writing it here because taking\
          \ the concept further might help produce better language models (not my\
          \ expertise).\r\n\r\nI have discovered a technique that allows me to reduce\
          \ the computations of a semantic search by 70x. The brief is that you can\
          \ use Principal Component Analysis (PCA) to rotate your embeddings so as\
          \ to produce an ordered set of axes from the largest distribution of points\
          \ to the smallest. For corpora smaller than 1 000 documents, you only need\
          \ to perform a vector comparison of the first 10 principal components or\
          \ less (the rest don't even need to be calculated). \r\n\r\nA query vector\
          \ only needs to be rotated using the same transformation to map to the same\
          \ vector space that the PCA produced. Since word embeddings tend to have\
          \ ~700 dimensions, the ability to compare them using only 10 principal components\
          \ (the widest shadows the document embeddings can cast on 10 dimensions)\
          \ represents a 70x reduction in computation time for standard sentence transformers.\r\
          \n\r\nThis got me thinking, if a PCA produces vectors that are optimised\
          \ for searching then can we not take a parent model, do PCA on the vector\
          \ embeddings of a model's training data, with or without reducing dimensions,\
          \ to obtain the transformation matrix from one coordinate system to the\
          \ other, and then train a student language model using the PCA-optimised\
          \ embedding dimensions.\r\n\r\nThis is far beyond my expertise, I am just\
          \ clustering documents and summarising the topics. I needed to reduce the\
          \ cost and speed of semantic search, I know very little about training models.\
          \ \r\n\r\nIf you find this idea interesting, please take it further. If\
          \ you can use PCA to reduce the number of dimensions in a language model,\
          \ you can have smaller models with high performance. I only ask that you\
          \ thank me for the suggestion in the Acknowledgements. "
        updatedAt: '2023-04-25T12:32:56.038Z'
      numEdits: 0
      reactions: []
    id: 6447c8786ffed6ece1fdc82d
    type: comment
  author: NewsSoup
  content: "I made a discovery, but I am busy trying to open a business and am the\
    \ only one working on the website, so it will be months or years before I can\
    \ do anything about it. However, I am writing it here because taking the concept\
    \ further might help produce better language models (not my expertise).\r\n\r\n\
    I have discovered a technique that allows me to reduce the computations of a semantic\
    \ search by 70x. The brief is that you can use Principal Component Analysis (PCA)\
    \ to rotate your embeddings so as to produce an ordered set of axes from the largest\
    \ distribution of points to the smallest. For corpora smaller than 1 000 documents,\
    \ you only need to perform a vector comparison of the first 10 principal components\
    \ or less (the rest don't even need to be calculated). \r\n\r\nA query vector\
    \ only needs to be rotated using the same transformation to map to the same vector\
    \ space that the PCA produced. Since word embeddings tend to have ~700 dimensions,\
    \ the ability to compare them using only 10 principal components (the widest shadows\
    \ the document embeddings can cast on 10 dimensions) represents a 70x reduction\
    \ in computation time for standard sentence transformers.\r\n\r\nThis got me thinking,\
    \ if a PCA produces vectors that are optimised for searching then can we not take\
    \ a parent model, do PCA on the vector embeddings of a model's training data,\
    \ with or without reducing dimensions, to obtain the transformation matrix from\
    \ one coordinate system to the other, and then train a student language model\
    \ using the PCA-optimised embedding dimensions.\r\n\r\nThis is far beyond my expertise,\
    \ I am just clustering documents and summarising the topics. I needed to reduce\
    \ the cost and speed of semantic search, I know very little about training models.\
    \ \r\n\r\nIf you find this idea interesting, please take it further. If you can\
    \ use PCA to reduce the number of dimensions in a language model, you can have\
    \ smaller models with high performance. I only ask that you thank me for the suggestion\
    \ in the Acknowledgements. "
  created_at: 2023-04-25 11:32:56+00:00
  edited: false
  hidden: false
  id: 6447c8786ffed6ece1fdc82d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-25T12:44:07.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yes it''s a fine and standard idea. You could also simply use an
          embedding that produces smaller dimensions in the first place. The downside
          is that you must apply the whole PCA to new inputs too. It also may not
          gain as much speed as one might expect as vector operations are already
          highly parallel on GPUs.</p>

          <p>What I''m not super sure about is how much that affects the quality of
          the embedding. You are retaining most of the variance (it may not be 10
          dimensions in all cases, note, that have ''most'' of the variance) but losing
          some information. PCA preserves the inner product, IIRC, but this is also
          PCA on uncentered data. I don''t remember enough to evaluate how it affects
          cosine distance (doesn''t?) or Euclidean (does?). So I think it''s an open
          question how much computation it costs and what the downsides are, but yes
          fine idea.</p>

          '
        raw: 'Yes it''s a fine and standard idea. You could also simply use an embedding
          that produces smaller dimensions in the first place. The downside is that
          you must apply the whole PCA to new inputs too. It also may not gain as
          much speed as one might expect as vector operations are already highly parallel
          on GPUs.


          What I''m not super sure about is how much that affects the quality of the
          embedding. You are retaining most of the variance (it may not be 10 dimensions
          in all cases, note, that have ''most'' of the variance) but losing some
          information. PCA preserves the inner product, IIRC, but this is also PCA
          on uncentered data. I don''t remember enough to evaluate how it affects
          cosine distance (doesn''t?) or Euclidean (does?). So I think it''s an open
          question how much computation it costs and what the downsides are, but yes
          fine idea.'
        updatedAt: '2023-04-25T12:44:07.988Z'
      numEdits: 0
      reactions: []
    id: 6447cb173e498d66918ec299
    type: comment
  author: srowen
  content: 'Yes it''s a fine and standard idea. You could also simply use an embedding
    that produces smaller dimensions in the first place. The downside is that you
    must apply the whole PCA to new inputs too. It also may not gain as much speed
    as one might expect as vector operations are already highly parallel on GPUs.


    What I''m not super sure about is how much that affects the quality of the embedding.
    You are retaining most of the variance (it may not be 10 dimensions in all cases,
    note, that have ''most'' of the variance) but losing some information. PCA preserves
    the inner product, IIRC, but this is also PCA on uncentered data. I don''t remember
    enough to evaluate how it affects cosine distance (doesn''t?) or Euclidean (does?).
    So I think it''s an open question how much computation it costs and what the downsides
    are, but yes fine idea.'
  created_at: 2023-04-25 11:44:07+00:00
  edited: false
  hidden: false
  id: 6447cb173e498d66918ec299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T14:03:45.000Z'
    data:
      status: closed
    id: 64492f41d5d86def91d17059
    type: status-change
  author: srowen
  created_at: 2023-04-26 13:03:45+00:00
  id: 64492f41d5d86def91d17059
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
      fullname: Arno Duvenhage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NewsSoup
      type: user
    createdAt: '2023-04-26T14:35:09.000Z'
    data:
      edited: false
      editors:
      - NewsSoup
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
          fullname: Arno Duvenhage
          isHf: false
          isPro: false
          name: NewsSoup
          type: user
        html: '<p>Sorry for the late reply, new accounts can only comment once a day.</p>

          <p>For ordinary training the PCA would do nothing, because it amounts to
          rotating the embeddings around the origin until you finds an ordered list
          of directions with decreasing spread of information.</p>

          <p>However, the PCA can reveal how many dimensions carry meaningful information
          and how many can be cut off when making a distil model.</p>

          <p>For semantic search you do not need to train a transformer either (since
          PCA amounts to a rotation of the embedding vectors). </p>

          <p>It is only useful for taking a large model and using it to train a smaller
          parameter model. PCA could reveal the minimum amount of embedding dimensions
          you can get away with, given any specific parent model.</p>

          <p>In other words, language models draw shapes in random orientations, but
          if you align those shapes you can see where the flesh is and which parts
          of the shape contribute little.</p>

          '
        raw: "Sorry for the late reply, new accounts can only comment once a day.\n\
          \nFor ordinary training the PCA would do nothing, because it amounts to\
          \ rotating the embeddings around the origin until you finds an ordered list\
          \ of directions with decreasing spread of information.\n\nHowever, the PCA\
          \ can reveal how many dimensions carry meaningful information and how many\
          \ can be cut off when making a distil model.\n\nFor semantic search you\
          \ do not need to train a transformer either (since PCA amounts to a rotation\
          \ of the embedding vectors). \n\nIt is only useful for taking a large model\
          \ and using it to train a smaller parameter model. PCA could reveal the\
          \ minimum amount of embedding dimensions you can get away with, given any\
          \ specific parent model.\n\nIn other words, language models draw shapes\
          \ in random orientations, but if you align those shapes you can see where\
          \ the flesh is and which parts of the shape contribute little."
        updatedAt: '2023-04-26T14:35:09.834Z'
      numEdits: 0
      reactions: []
    id: 6449369dd16a70c015961185
    type: comment
  author: NewsSoup
  content: "Sorry for the late reply, new accounts can only comment once a day.\n\n\
    For ordinary training the PCA would do nothing, because it amounts to rotating\
    \ the embeddings around the origin until you finds an ordered list of directions\
    \ with decreasing spread of information.\n\nHowever, the PCA can reveal how many\
    \ dimensions carry meaningful information and how many can be cut off when making\
    \ a distil model.\n\nFor semantic search you do not need to train a transformer\
    \ either (since PCA amounts to a rotation of the embedding vectors). \n\nIt is\
    \ only useful for taking a large model and using it to train a smaller parameter\
    \ model. PCA could reveal the minimum amount of embedding dimensions you can get\
    \ away with, given any specific parent model.\n\nIn other words, language models\
    \ draw shapes in random orientations, but if you align those shapes you can see\
    \ where the flesh is and which parts of the shape contribute little."
  created_at: 2023-04-26 13:35:09+00:00
  edited: false
  hidden: false
  id: 6449369dd16a70c015961185
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T15:04:59.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I get it, but, PCA is minimizing distance to a hyperplane in the
          embedding space, which is a transformation that is not necessarily compatible
          with the semantics of how the embedding space is used. In particular you
          typically do not use Euclidean distance, but cosine similarity. It''s probably
          ''pretty fine'' in practice, but that is the reason it''s not obviously
          the right thing to do.</p>

          <p>Compare that to simply using a smaller embedding too, which does not
          need a transform applied at runtime.</p>

          '
        raw: 'I get it, but, PCA is minimizing distance to a hyperplane in the embedding
          space, which is a transformation that is not necessarily compatible with
          the semantics of how the embedding space is used. In particular you typically
          do not use Euclidean distance, but cosine similarity. It''s probably ''pretty
          fine'' in practice, but that is the reason it''s not obviously the right
          thing to do.


          Compare that to simply using a smaller embedding too, which does not need
          a transform applied at runtime.'
        updatedAt: '2023-04-26T15:04:59.634Z'
      numEdits: 0
      reactions: []
    id: 64493d9b1af713976c2c1128
    type: comment
  author: srowen
  content: 'I get it, but, PCA is minimizing distance to a hyperplane in the embedding
    space, which is a transformation that is not necessarily compatible with the semantics
    of how the embedding space is used. In particular you typically do not use Euclidean
    distance, but cosine similarity. It''s probably ''pretty fine'' in practice, but
    that is the reason it''s not obviously the right thing to do.


    Compare that to simply using a smaller embedding too, which does not need a transform
    applied at runtime.'
  created_at: 2023-04-26 14:04:59+00:00
  edited: false
  hidden: false
  id: 64493d9b1af713976c2c1128
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
      fullname: Arno Duvenhage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NewsSoup
      type: user
    createdAt: '2023-05-31T21:02:37.000Z'
    data:
      edited: true
      editors:
      - NewsSoup
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcf9615172394100a051039f59758105.svg
          fullname: Arno Duvenhage
          isHf: false
          isPro: false
          name: NewsSoup
          type: user
        html: '<p>So in hind sight, a different algorithm for PCA is the Singular
          Value Decomposition (SVD). In LLM language you should recognise that as
          the same algorithm used by LoRA (low rank adaptation). Turns out we were
          using it already, by a different name.</p>

          <p>The standard algorithm for PCA is the eigen-decomposition of the covariance,
          but multiplying the left two matrices of the SVD produces the same output
          as the eigen-decomposition - so they are equivalent, being related by either
          a factorisation or a multiplication.</p>

          '
        raw: 'So in hind sight, a different algorithm for PCA is the Singular Value
          Decomposition (SVD). In LLM language you should recognise that as the same
          algorithm used by LoRA (low rank adaptation). Turns out we were using it
          already, by a different name.


          The standard algorithm for PCA is the eigen-decomposition of the covariance,
          but multiplying the left two matrices of the SVD produces the same output
          as the eigen-decomposition - so they are equivalent, being related by either
          a factorisation or a multiplication.'
        updatedAt: '2023-05-31T21:06:11.332Z'
      numEdits: 2
      reactions: []
    id: 6477b5ed04aa03da2ac3e7e7
    type: comment
  author: NewsSoup
  content: 'So in hind sight, a different algorithm for PCA is the Singular Value
    Decomposition (SVD). In LLM language you should recognise that as the same algorithm
    used by LoRA (low rank adaptation). Turns out we were using it already, by a different
    name.


    The standard algorithm for PCA is the eigen-decomposition of the covariance, but
    multiplying the left two matrices of the SVD produces the same output as the eigen-decomposition
    - so they are equivalent, being related by either a factorisation or a multiplication.'
  created_at: 2023-05-31 20:02:37+00:00
  edited: true
  hidden: false
  id: 6477b5ed04aa03da2ac3e7e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 53
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Training dolly models using optimised embedding spaces (potentially)
