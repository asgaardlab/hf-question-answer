!!python/object:huggingface_hub.community.DiscussionWithDetails
author: seadude
conflicting_files: null
created_at: 2023-05-22 21:33:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-05-22T22:33:07.000Z'
    data:
      edited: false
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>Hello,</p>

          <p>I stood up a new Azure Databricks GPU cluster to experiment with DollyV2.
          I ran the first three commands in the HuggingFace model card:</p>

          <pre><code># 1 --------

          %pip install "accelerate&gt;=0.16.0,&lt;1" "transformers[torch]&gt;=4.28.1,&lt;5"
          "torch&gt;=1.13.1,&lt;2"


          # 2 ---------

          import torch

          from transformers import pipeline


          generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")


          # 3 ------------

          res = generate_text("Explain to me the difference between nuclear fission
          and fusion.")

          print(res[0]["generated_text"])

          </code></pre>

          <p>Command <a href="/databricks/dolly-v2-12b/discussions/3">#3</a> above
          results in:</p>

          <pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 492.00
          MiB (GPU 0; 15.78 GiB total capacity; 14.16 GiB already allocated; 283.50
          MiB free; 14.37 GiB reserved in total by PyTorch) If reserved memory is
          &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          </code></pre>

          <p>How do I address this?</p>

          '
        raw: "Hello,\r\n\r\nI stood up a new Azure Databricks GPU cluster to experiment\
          \ with DollyV2. I ran the first three commands in the HuggingFace model\
          \ card:\r\n\r\n```\r\n# 1 --------\r\n%pip install \"accelerate>=0.16.0,<1\"\
          \ \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\"\r\n\r\n# 2 ---------\r\
          \nimport torch\r\nfrom transformers import pipeline\r\n\r\ngenerate_text\
          \ = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\r\n\r\n# 3 ------------\r\
          \nres = generate_text(\"Explain to me the difference between nuclear fission\
          \ and fusion.\")\r\nprint(res[0][\"generated_text\"])\r\n```\r\n\r\nCommand\
          \ #3 above results in:\r\n\r\n```\r\nOutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 492.00 MiB (GPU 0; 15.78 GiB total capacity; 14.16 GiB\
          \ already allocated; 283.50 MiB free; 14.37 GiB reserved in total by PyTorch)\
          \ If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n```\r\n\r\nHow do I address this?\r\n"
        updatedAt: '2023-05-22T22:33:07.210Z'
      numEdits: 0
      reactions: []
    id: 646beda3ec9a61e8717e7649
    type: comment
  author: seadude
  content: "Hello,\r\n\r\nI stood up a new Azure Databricks GPU cluster to experiment\
    \ with DollyV2. I ran the first three commands in the HuggingFace model card:\r\
    \n\r\n```\r\n# 1 --------\r\n%pip install \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\"\
    \ \"torch>=1.13.1,<2\"\r\n\r\n# 2 ---------\r\nimport torch\r\nfrom transformers\
    \ import pipeline\r\n\r\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\r\n\
    \r\n# 3 ------------\r\nres = generate_text(\"Explain to me the difference between\
    \ nuclear fission and fusion.\")\r\nprint(res[0][\"generated_text\"])\r\n```\r\
    \n\r\nCommand #3 above results in:\r\n\r\n```\r\nOutOfMemoryError: CUDA out of\
    \ memory. Tried to allocate 492.00 MiB (GPU 0; 15.78 GiB total capacity; 14.16\
    \ GiB already allocated; 283.50 MiB free; 14.37 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n```\r\n\r\nHow do I address this?\r\n"
  created_at: 2023-05-22 21:33:07+00:00
  edited: false
  hidden: false
  id: 646beda3ec9a61e8717e7649
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-22T22:52:58.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>It''s really important to say what hardware you are using. Here
          I can see you have a 16GB GPU. That will not fit  a 12B param model in 16-bit
          (2 x 12 = 24 &gt; 16). You can try loading in 8-bit, maybe.</p>

          '
        raw: It's really important to say what hardware you are using. Here I can
          see you have a 16GB GPU. That will not fit  a 12B param model in 16-bit
          (2 x 12 = 24 > 16). You can try loading in 8-bit, maybe.
        updatedAt: '2023-05-22T22:52:58.323Z'
      numEdits: 0
      reactions: []
    id: 646bf24aed228272134028f4
    type: comment
  author: srowen
  content: It's really important to say what hardware you are using. Here I can see
    you have a 16GB GPU. That will not fit  a 12B param model in 16-bit (2 x 12 =
    24 > 16). You can try loading in 8-bit, maybe.
  created_at: 2023-05-22 21:52:58+00:00
  edited: false
  hidden: false
  id: 646bf24aed228272134028f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-05-22T23:11:20.000Z'
    data:
      edited: false
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>Thank you for the response. </p>

          <p>When you say "8-bit" are you referring to changing <code>torch_dtype=torch.bfloat16</code>
          to <code>.bfloat8</code>?</p>

          '
        raw: "Thank you for the response. \n\nWhen you say \"8-bit\" are you referring\
          \ to changing `torch_dtype=torch.bfloat16` to `.bfloat8`?"
        updatedAt: '2023-05-22T23:11:20.530Z'
      numEdits: 0
      reactions: []
    id: 646bf698db697c798a4f7fa7
    type: comment
  author: seadude
  content: "Thank you for the response. \n\nWhen you say \"8-bit\" are you referring\
    \ to changing `torch_dtype=torch.bfloat16` to `.bfloat8`?"
  created_at: 2023-05-22 22:11:20+00:00
  edited: false
  hidden: false
  id: 646bf698db697c798a4f7fa7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-22T23:12:23.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>No, load_in_8bit=True. See the github repo readme <a rel="nofollow"
          href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a></p>

          '
        raw: No, load_in_8bit=True. See the github repo readme https://github.com/databrickslabs/dolly
        updatedAt: '2023-05-22T23:12:23.883Z'
      numEdits: 0
      reactions: []
    id: 646bf6d7f85ebf65c541cea5
    type: comment
  author: srowen
  content: No, load_in_8bit=True. See the github repo readme https://github.com/databrickslabs/dolly
  created_at: 2023-05-22 22:12:23+00:00
  edited: false
  hidden: false
  id: 646bf6d7f85ebf65c541cea5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-05-22T23:33:54.000Z'
    data:
      edited: false
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>Ok, I will check that. </p>

          <p>I was able to load in the 7B model. I was able to get the model to respond
          to 1 question. Then it too exhibited this same error...</p>

          <p>Do I need to flush something between requests?</p>

          <pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09
          GiB (GPU 0; 15.78 GiB total capacity; 14.41 GiB already allocated; 225.50
          MiB free; 14.43 GiB reserved in total by PyTorch) If reserved memory is
          &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          </code></pre>

          '
        raw: "Ok, I will check that. \n\nI was able to load in the 7B model. I was\
          \ able to get the model to respond to 1 question. Then it too exhibited\
          \ this same error...\n\nDo I need to flush something between requests?\n\
          \n```\nOutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB\
          \ (GPU 0; 15.78 GiB total capacity; 14.41 GiB already allocated; 225.50\
          \ MiB free; 14.43 GiB reserved in total by PyTorch) If reserved memory is\
          \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          ```"
        updatedAt: '2023-05-22T23:33:54.480Z'
      numEdits: 0
      reactions: []
    id: 646bfbe2db697c798a501760
    type: comment
  author: seadude
  content: "Ok, I will check that. \n\nI was able to load in the 7B model. I was able\
    \ to get the model to respond to 1 question. Then it too exhibited this same error...\n\
    \nDo I need to flush something between requests?\n\n```\nOutOfMemoryError: CUDA\
    \ out of memory. Tried to allocate 1.09 GiB (GPU 0; 15.78 GiB total capacity;\
    \ 14.41 GiB already allocated; 225.50 MiB free; 14.43 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    ```"
  created_at: 2023-05-22 22:33:54+00:00
  edited: false
  hidden: false
  id: 646bfbe2db697c798a501760
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-22T23:42:46.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Are you sure you aren''t'' loading twice?</p>

          '
        raw: Are you sure you aren't' loading twice?
        updatedAt: '2023-05-22T23:42:46.918Z'
      numEdits: 0
      reactions: []
    id: 646bfdf6f85ebf65c5429b4b
    type: comment
  author: srowen
  content: Are you sure you aren't' loading twice?
  created_at: 2023-05-22 22:42:46+00:00
  edited: false
  hidden: false
  id: 646bfdf6f85ebf65c5429b4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-05-23T00:24:35.000Z'
    data:
      edited: true
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>I''m not sure. </p>

          <p>How do I "refresh the notebook" and start over? I''ve tried to <code>Detach</code>
          and <code>Detach and reattached</code>.</p>

          '
        raw: "I'm not sure. \n\nHow do I \"refresh the notebook\" and start over?\
          \ I've tried to `Detach` and `Detach and reattached`."
        updatedAt: '2023-05-23T00:25:51.919Z'
      numEdits: 1
      reactions: []
    id: 646c07c35d68f5c15a37b436
    type: comment
  author: seadude
  content: "I'm not sure. \n\nHow do I \"refresh the notebook\" and start over? I've\
    \ tried to `Detach` and `Detach and reattached`."
  created_at: 2023-05-22 23:24:35+00:00
  edited: true
  hidden: false
  id: 646c07c35d68f5c15a37b436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-23T00:25:21.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Detach/reattach. But, I''m just saying check that you didn''t run
          the code to load the model a second time to make the second generation</p>

          '
        raw: Detach/reattach. But, I'm just saying check that you didn't run the code
          to load the model a second time to make the second generation
        updatedAt: '2023-05-23T00:25:21.753Z'
      numEdits: 0
      reactions: []
    id: 646c07f1f85ebf65c543b4fc
    type: comment
  author: srowen
  content: Detach/reattach. But, I'm just saying check that you didn't run the code
    to load the model a second time to make the second generation
  created_at: 2023-05-22 23:25:21+00:00
  edited: false
  hidden: false
  id: 646c07f1f85ebf65c543b4fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
      fullname: Eric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seadude
      type: user
    createdAt: '2023-05-23T00:38:24.000Z'
    data:
      edited: false
      editors:
      - seadude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd154fdb4776e5478bfeee3e93b50d6e.svg
          fullname: Eric
          isHf: false
          isPro: false
          name: seadude
          type: user
        html: '<p>Yeah. When I Detach/Reattach, then run the entire notebook again,
          I get:</p>

          <pre><code>%pip install "accelerate&gt;=0.16.0,&lt;1" "transformers[torch]&gt;=4.28.1,&lt;5"
          "torch&gt;=1.13.1,&lt;2"


          import torch

          from transformers import pipeline


          generate_text = pipeline(model="databricks/dolly-v2-7b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")


          res = generate_text("""my prompt""")

          print(res[0]["generated_text"])


          #____ERROR___


          OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0;
          15.78 GiB total capacity; 14.20 GiB already allocated; 443.50 MiB free;
          14.21 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          </code></pre>

          <p>I''ll mess with it again tomorrow</p>

          '
        raw: 'Yeah. When I Detach/Reattach, then run the entire notebook again, I
          get:


          ```

          %pip install "accelerate>=0.16.0,<1" "transformers[torch]>=4.28.1,<5" "torch>=1.13.1,<2"


          import torch

          from transformers import pipeline


          generate_text = pipeline(model="databricks/dolly-v2-7b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")


          res = generate_text("""my prompt""")

          print(res[0]["generated_text"])


          #____ERROR___


          OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0;
          15.78 GiB total capacity; 14.20 GiB already allocated; 443.50 MiB free;
          14.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          ```


          I''ll mess with it again tomorrow'
        updatedAt: '2023-05-23T00:38:24.133Z'
      numEdits: 0
      reactions: []
    id: 646c0b00db697c798a51c52d
    type: comment
  author: seadude
  content: 'Yeah. When I Detach/Reattach, then run the entire notebook again, I get:


    ```

    %pip install "accelerate>=0.16.0,<1" "transformers[torch]>=4.28.1,<5" "torch>=1.13.1,<2"


    import torch

    from transformers import pipeline


    generate_text = pipeline(model="databricks/dolly-v2-7b", torch_dtype=torch.bfloat16,
    trust_remote_code=True, device_map="auto")


    res = generate_text("""my prompt""")

    print(res[0]["generated_text"])


    #____ERROR___


    OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0; 15.78
    GiB total capacity; 14.20 GiB already allocated; 443.50 MiB free; 14.21 GiB reserved
    in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb
    to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    ```


    I''ll mess with it again tomorrow'
  created_at: 2023-05-22 23:38:24+00:00
  edited: false
  hidden: false
  id: 646c0b00db697c798a51c52d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-23T00:50:46.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That looks ok except you''re pretty close to oom already loading
          14gb of params in 16gb. Generation that runs on a few hundred tokens might
          oom. Limit max_new_tokens to 64 maybe</p>

          '
        raw: That looks ok except you're pretty close to oom already loading 14gb
          of params in 16gb. Generation that runs on a few hundred tokens might oom.
          Limit max_new_tokens to 64 maybe
        updatedAt: '2023-05-23T00:50:46.937Z'
      numEdits: 0
      reactions: []
    id: 646c0de6db697c798a521962
    type: comment
  author: srowen
  content: That looks ok except you're pretty close to oom already loading 14gb of
    params in 16gb. Generation that runs on a few hundred tokens might oom. Limit
    max_new_tokens to 64 maybe
  created_at: 2023-05-22 23:50:46+00:00
  edited: false
  hidden: false
  id: 646c0de6db697c798a521962
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8ab5d1e74cf13d80e46bb108e24fbf4.svg
      fullname: TengFeiQu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qutf
      type: user
    createdAt: '2023-05-23T05:45:28.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/c8ab5d1e74cf13d80e46bb108e24fbf4.svg
          fullname: TengFeiQu
          isHf: false
          isPro: false
          name: qutf
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-23T06:55:54.410Z'
      numEdits: 0
      reactions: []
    id: 646c52f8db697c798a5ca748
    type: comment
  author: qutf
  content: This comment has been hidden
  created_at: 2023-05-23 04:45:28+00:00
  edited: true
  hidden: true
  id: 646c52f8db697c798a5ca748
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-25T19:57:02.000Z'
    data:
      status: closed
    id: 646fbd8e6098ee820fbf4c35
    type: status-change
  author: srowen
  created_at: 2023-05-25 18:57:02+00:00
  id: 646fbd8e6098ee820fbf4c35
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 72
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: ' OutOfMemoryError: CUDA out of memory'
