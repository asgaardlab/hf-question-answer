!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MJDLPRO
conflicting_files: null
created_at: 2023-05-05 00:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
      fullname: Mohit Juneja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MJDLPRO
      type: user
    createdAt: '2023-05-05T01:28:14.000Z'
    data:
      edited: true
      editors:
      - MJDLPRO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
          fullname: Mohit Juneja
          isHf: false
          isPro: false
          name: MJDLPRO
          type: user
        html: "<p>I get this Runtime error for Langchain summarization pipeline for\
          \ any chain_type : map-reduce, stuff etc. This works fine with OpenAI LLM.\
          \ Something is not right in Dolly 2. <span data-props=\"{&quot;user&quot;:&quot;hwchase17&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hwchase17\"\
          >@<span class=\"underline\">hwchase17</span></a></span>\n\n\t</span></span>\
          \ could you share some clue why this could be happening? Is this because\
          \ of modeling of gpt neox?  I am setting the max_length in model_kwargs\
          \ so it is <em>NOT</em> addressed in the other thread <a href=\"https://huggingface.co/databricks/dolly-v2-12b/discussions/10\"\
          >https://huggingface.co/databricks/dolly-v2-12b/discussions/10</a> and it\
          \ is regardless of the dolly model version - consistent error across dolly-v2-3b,\
          \ 7b, 12b.</p>\n<p>RuntimeError: The size of tensor a (2048) must match\
          \ the size of tensor b (7080) at non-singleton dimension 3</p>\n<p>env/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:219,\
          \ in GPTNeoXAttention._attn(self, query, key, value, attention_mask, head_mask)<br>\
          \    216 # Need to be a tensor, otherwise we get error: <code>RuntimeError:\
          \ expected scalar type float but found double</code>.<br>    217 # Need\
          \ to be on the same device, otherwise <code>RuntimeError: ..., x and y to\
          \ be on the same device</code><br>    218 mask_value = torch.tensor(mask_value,\
          \ dtype=attn_scores.dtype).to(attn_scores.device)<br>--&gt; 219 attn_scores\
          \ = torch.where(causal_mask, attn_scores, mask_value)<br>    221 if attention_mask\
          \ is not None:<br>    222     # Apply the attention mask<br>    223    \
          \ attn_scores = attn_scores + attention_mask</p>\n<p>from langchain import\
          \ HuggingFacePipeline<br>from langchain.chains.summarize import load_summarize_chain<br>from\
          \ langchain.document_loaders import TextLoader<br>from langchain.text_splitter\
          \ import RecursiveCharacterTextSplitter<br>from langchain import PromptTemplate,\
          \ LLMChain</p>\n<p>llm = HuggingFacePipeline.from_model_id(model_id=\"databricks/dolly-v2-3b\"\
          , device=0, task=\"text-generation\",<br>                              \
          \          model_kwargs={\"temperature\":0, \"max_length\": 1024})</p>\n\
          <p>loader = TextLoader('/d/document/Anydocument.txt')<br>documents = loader.load()</p>\n\
          <h1 id=\"get-your-splitter-ready\">Get your splitter ready</h1>\n<p>text_splitter\
          \ = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)</p>\n\
          <h1 id=\"split-your-docs-into-texts\">Split your docs into texts</h1>\n\
          <p>texts = text_splitter.split_documents(documents)</p>\n<h1 id=\"summarization-chain\"\
          >Summarization chain</h1>\n<p>chain = load_summarize_chain(llm, chain_type=\"\
          stuff\", verbose=True)<br>chain.run(texts)</p>\n"
        raw: "I get this Runtime error for Langchain summarization pipeline for any\
          \ chain_type : map-reduce, stuff etc. This works fine with OpenAI LLM. Something\
          \ is not right in Dolly 2. @hwchase17 could you share some clue why this\
          \ could be happening? Is this because of modeling of gpt neox?  I am setting\
          \ the max_length in model_kwargs so it is *NOT* addressed in the other thread\
          \ https://huggingface.co/databricks/dolly-v2-12b/discussions/10 and it is\
          \ regardless of the dolly model version - consistent error across dolly-v2-3b,\
          \ 7b, 12b.\n\nRuntimeError: The size of tensor a (2048) must match the size\
          \ of tensor b (7080) at non-singleton dimension 3\n\nenv/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:219,\
          \ in GPTNeoXAttention._attn(self, query, key, value, attention_mask, head_mask)\n\
          \    216 # Need to be a tensor, otherwise we get error: `RuntimeError: expected\
          \ scalar type float but found double`.\n    217 # Need to be on the same\
          \ device, otherwise `RuntimeError: ..., x and y to be on the same device`\n\
          \    218 mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n\
          --> 219 attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n\
          \    221 if attention_mask is not None:\n    222     # Apply the attention\
          \ mask\n    223     attn_scores = attn_scores + attention_mask\n\n\nfrom\
          \ langchain import HuggingFacePipeline\nfrom langchain.chains.summarize\
          \ import load_summarize_chain\nfrom langchain.document_loaders import TextLoader\n\
          from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom\
          \ langchain import PromptTemplate, LLMChain\n\nllm = HuggingFacePipeline.from_model_id(model_id=\"\
          databricks/dolly-v2-3b\", device=0, task=\"text-generation\", \n       \
          \                                 model_kwargs={\"temperature\":0, \"max_length\"\
          : 1024})\n\nloader = TextLoader('/d/document/Anydocument.txt')\ndocuments\
          \ = loader.load()\n\n# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=700,\
          \ chunk_overlap=50)\n\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)\n\
          \n# Summarization chain\nchain = load_summarize_chain(llm, chain_type=\"\
          stuff\", verbose=True)\nchain.run(texts)"
        updatedAt: '2023-05-05T01:29:39.818Z'
      numEdits: 1
      reactions: []
    id: 64545baeafea4a7004a0f558
    type: comment
  author: MJDLPRO
  content: "I get this Runtime error for Langchain summarization pipeline for any\
    \ chain_type : map-reduce, stuff etc. This works fine with OpenAI LLM. Something\
    \ is not right in Dolly 2. @hwchase17 could you share some clue why this could\
    \ be happening? Is this because of modeling of gpt neox?  I am setting the max_length\
    \ in model_kwargs so it is *NOT* addressed in the other thread https://huggingface.co/databricks/dolly-v2-12b/discussions/10\
    \ and it is regardless of the dolly model version - consistent error across dolly-v2-3b,\
    \ 7b, 12b.\n\nRuntimeError: The size of tensor a (2048) must match the size of\
    \ tensor b (7080) at non-singleton dimension 3\n\nenv/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:219,\
    \ in GPTNeoXAttention._attn(self, query, key, value, attention_mask, head_mask)\n\
    \    216 # Need to be a tensor, otherwise we get error: `RuntimeError: expected\
    \ scalar type float but found double`.\n    217 # Need to be on the same device,\
    \ otherwise `RuntimeError: ..., x and y to be on the same device`\n    218 mask_value\
    \ = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n\
    --> 219 attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n    221\
    \ if attention_mask is not None:\n    222     # Apply the attention mask\n   \
    \ 223     attn_scores = attn_scores + attention_mask\n\n\nfrom langchain import\
    \ HuggingFacePipeline\nfrom langchain.chains.summarize import load_summarize_chain\n\
    from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter\
    \ import RecursiveCharacterTextSplitter\nfrom langchain import PromptTemplate,\
    \ LLMChain\n\nllm = HuggingFacePipeline.from_model_id(model_id=\"databricks/dolly-v2-3b\"\
    , device=0, task=\"text-generation\", \n                                     \
    \   model_kwargs={\"temperature\":0, \"max_length\": 1024})\n\nloader = TextLoader('/d/document/Anydocument.txt')\n\
    documents = loader.load()\n\n# Get your splitter ready\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=700,\
    \ chunk_overlap=50)\n\n# Split your docs into texts\ntexts = text_splitter.split_documents(documents)\n\
    \n# Summarization chain\nchain = load_summarize_chain(llm, chain_type=\"stuff\"\
    , verbose=True)\nchain.run(texts)"
  created_at: 2023-05-05 00:28:14+00:00
  edited: true
  hidden: false
  id: 64545baeafea4a7004a0f558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-05T01:36:22.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>This is just ''you sent way too much input''. Context window is
          2048 and you (langchain) sent 7000+. Set verbose=True on chain to see why</p>

          '
        raw: This is just 'you sent way too much input'. Context window is 2048 and
          you (langchain) sent 7000+. Set verbose=True on chain to see why
        updatedAt: '2023-05-05T01:36:22.279Z'
      numEdits: 0
      reactions: []
    id: 64545d96ecdd69b8134f3a8a
    type: comment
  author: srowen
  content: This is just 'you sent way too much input'. Context window is 2048 and
    you (langchain) sent 7000+. Set verbose=True on chain to see why
  created_at: 2023-05-05 00:36:22+00:00
  edited: false
  hidden: false
  id: 64545d96ecdd69b8134f3a8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
      fullname: Mohit Juneja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MJDLPRO
      type: user
    createdAt: '2023-05-05T12:02:04.000Z'
    data:
      edited: true
      editors:
      - MJDLPRO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
          fullname: Mohit Juneja
          isHf: false
          isPro: false
          name: MJDLPRO
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> seems like you\
          \ missed the langchain textsplitter API and I should have mentioned it.\
          \ Text is sent in chunks of 700 tokens with 20 token overlap. I am not sending\
          \ 7080 and every time error has a different size for 2nd value. For example,\
          \ the size of tensor a stays at 2048 in this error regardless of max_length\
          \ in model_kwargs i.e., max_length is 512 or 1024 but the size of tensor\
          \ a stays (2048) and the size of tensor b is different each time. This case\
          \ it was 7080 but other times it is 2049 or something different. Seems to\
          \ be a bug in modeling_gpt_neox.py or some place you may be making use of\
          \ broadcasting incorrectly? </p>\n<p>RuntimeError: The size of tensor a\
          \ (2048) must match the size of tensor b (7080) at non-singleton dimension\
          \ 3</p>\n<p>Reference line for text splitter:<br> texts = text_splitter.split_documents(documents)</p>\n\
          <p>Additionally, my code snippet clearly has verbose=True and I see the\
          \ error about modeling_gpt_neox.py --&gt; 219 attn_scores = torch.where(causal_mask,\
          \ attn_scores, mask_value)</p>\n<p>Do I need to set other sizes in model_kwargs\
          \ for this to work?</p>\n<p>Reference code for verbose:<br>chain = load_summarize_chain(llm,\
          \ chain_type=\"stuff\", verbose=True)<br>chain.run(texts)</p>\n"
        raw: "@srowen seems like you missed the langchain textsplitter API and I should\
          \ have mentioned it. Text is sent in chunks of 700 tokens with 20 token\
          \ overlap. I am not sending 7080 and every time error has a different size\
          \ for 2nd value. For example, the size of tensor a stays at 2048 in this\
          \ error regardless of max_length in model_kwargs i.e., max_length is 512\
          \ or 1024 but the size of tensor a stays (2048) and the size of tensor b\
          \ is different each time. This case it was 7080 but other times it is 2049\
          \ or something different. Seems to be a bug in modeling_gpt_neox.py or some\
          \ place you may be making use of broadcasting incorrectly? \n\nRuntimeError:\
          \ The size of tensor a (2048) must match the size of tensor b (7080) at\
          \ non-singleton dimension 3\n\nReference line for text splitter:\n texts\
          \ = text_splitter.split_documents(documents)\n\nAdditionally, my code snippet\
          \ clearly has verbose=True and I see the error about modeling_gpt_neox.py\
          \ --> 219 attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n\
          \nDo I need to set other sizes in model_kwargs for this to work?\n\nReference\
          \ code for verbose: \nchain = load_summarize_chain(llm, chain_type=\"stuff\"\
          , verbose=True)\nchain.run(texts)"
        updatedAt: '2023-05-05T12:08:55.830Z'
      numEdits: 1
      reactions: []
    id: 6454f03ca473375be56cecc3
    type: comment
  author: MJDLPRO
  content: "@srowen seems like you missed the langchain textsplitter API and I should\
    \ have mentioned it. Text is sent in chunks of 700 tokens with 20 token overlap.\
    \ I am not sending 7080 and every time error has a different size for 2nd value.\
    \ For example, the size of tensor a stays at 2048 in this error regardless of\
    \ max_length in model_kwargs i.e., max_length is 512 or 1024 but the size of tensor\
    \ a stays (2048) and the size of tensor b is different each time. This case it\
    \ was 7080 but other times it is 2049 or something different. Seems to be a bug\
    \ in modeling_gpt_neox.py or some place you may be making use of broadcasting\
    \ incorrectly? \n\nRuntimeError: The size of tensor a (2048) must match the size\
    \ of tensor b (7080) at non-singleton dimension 3\n\nReference line for text splitter:\n\
    \ texts = text_splitter.split_documents(documents)\n\nAdditionally, my code snippet\
    \ clearly has verbose=True and I see the error about modeling_gpt_neox.py -->\
    \ 219 attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n\nDo I\
    \ need to set other sizes in model_kwargs for this to work?\n\nReference code\
    \ for verbose: \nchain = load_summarize_chain(llm, chain_type=\"stuff\", verbose=True)\n\
    chain.run(texts)"
  created_at: 2023-05-05 11:02:04+00:00
  edited: true
  hidden: false
  id: 6454f03ca473375be56cecc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-05T12:09:09.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I see it, but, something is sending much more text to the model.
          To debug, set verbose=True and see what''s going in first. Have you checked
          that?</p>

          '
        raw: I see it, but, something is sending much more text to the model. To debug,
          set verbose=True and see what's going in first. Have you checked that?
        updatedAt: '2023-05-05T12:09:09.847Z'
      numEdits: 0
      reactions: []
    id: 6454f1e5d55525a4fee350a0
    type: comment
  author: srowen
  content: I see it, but, something is sending much more text to the model. To debug,
    set verbose=True and see what's going in first. Have you checked that?
  created_at: 2023-05-05 11:09:09+00:00
  edited: false
  hidden: false
  id: 6454f1e5d55525a4fee350a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
      fullname: Mohit Juneja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MJDLPRO
      type: user
    createdAt: '2023-05-05T12:20:35.000Z'
    data:
      edited: true
      editors:
      - MJDLPRO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
          fullname: Mohit Juneja
          isHf: false
          isPro: false
          name: MJDLPRO
          type: user
        html: '<p>Yes I have checked that. I always had verbose=True set. The chunks
          are all less than 700 tokens as set by Textsplitter. </p>

          <p>I see the error about modeling_gpt_neox.py --&gt; 219 attn_scores = torch.where(causal_mask,
          attn_scores, mask_value)</p>

          '
        raw: "Yes I have checked that. I always had verbose=True set. The chunks are\
          \ all less than 700 tokens as set by Textsplitter. \n\nI see the error about\
          \ modeling_gpt_neox.py --> 219 attn_scores = torch.where(causal_mask, attn_scores,\
          \ mask_value)"
        updatedAt: '2023-05-05T12:21:03.444Z'
      numEdits: 1
      reactions: []
    id: 6454f493d55525a4fee3938c
    type: comment
  author: MJDLPRO
  content: "Yes I have checked that. I always had verbose=True set. The chunks are\
    \ all less than 700 tokens as set by Textsplitter. \n\nI see the error about modeling_gpt_neox.py\
    \ --> 219 attn_scores = torch.where(causal_mask, attn_scores, mask_value)"
  created_at: 2023-05-05 11:20:35+00:00
  edited: true
  hidden: false
  id: 6454f493d55525a4fee3938c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-05T12:28:46.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I can''t quite make out what you posted, but if that''s a stack
          trace, then it''s just showing where it the mismatch occurred.<br>Are you
          sure ''stuff'' isn''t just stuffing all the pieces into the model at once?
          I think you need to use something like the ''map_reduce'' type to summarize
          chunks then summarize the summaries.</p>

          '
        raw: 'I can''t quite make out what you posted, but if that''s a stack trace,
          then it''s just showing where it the mismatch occurred.

          Are you sure ''stuff'' isn''t just stuffing all the pieces into the model
          at once? I think you need to use something like the ''map_reduce'' type
          to summarize chunks then summarize the summaries.'
        updatedAt: '2023-05-05T12:28:46.185Z'
      numEdits: 0
      reactions: []
    id: 6454f67efe2f48cb4b63ed34
    type: comment
  author: srowen
  content: 'I can''t quite make out what you posted, but if that''s a stack trace,
    then it''s just showing where it the mismatch occurred.

    Are you sure ''stuff'' isn''t just stuffing all the pieces into the model at once?
    I think you need to use something like the ''map_reduce'' type to summarize chunks
    then summarize the summaries.'
  created_at: 2023-05-05 11:28:46+00:00
  edited: false
  hidden: false
  id: 6454f67efe2f48cb4b63ed34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
      fullname: Mohit Juneja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MJDLPRO
      type: user
    createdAt: '2023-05-05T12:53:07.000Z'
    data:
      edited: false
      editors:
      - MJDLPRO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667667808415-noauth.png?w=200&h=200&f=face
          fullname: Mohit Juneja
          isHf: false
          isPro: false
          name: MJDLPRO
          type: user
        html: '<p>I understand the code and comments got mixed to cause confusion.
          I tried map_reduce and it also failed in the last step. I think I understand
          the problem, while creating the summary of summaries (last summary), the
          size of text must be greater but then the same pipeline works like a charm
          with LLM = OpenAI(...) with map_reduce Langchain chain_type.<br>Something
          gets handled by Langchain and OpenAI combination but fails with Langchain
          and Dolly-LLM combination i.e., Langchain and Dolly 2 don''t work as well.
          I am not sure if it will be possible to do all root cause analysis and resolve
          the root cause on this thread. Nevertheless, thanks for your help.</p>

          '
        raw: "I understand the code and comments got mixed to cause confusion. I tried\
          \ map_reduce and it also failed in the last step. I think I understand the\
          \ problem, while creating the summary of summaries (last summary), the size\
          \ of text must be greater but then the same pipeline works like a charm\
          \ with LLM = OpenAI(...) with map_reduce Langchain chain_type. \nSomething\
          \ gets handled by Langchain and OpenAI combination but fails with Langchain\
          \ and Dolly-LLM combination i.e., Langchain and Dolly 2 don't work as well.\
          \ I am not sure if it will be possible to do all root cause analysis and\
          \ resolve the root cause on this thread. Nevertheless, thanks for your help."
        updatedAt: '2023-05-05T12:53:07.038Z'
      numEdits: 0
      reactions: []
    id: 6454fc33a473375be56e0575
    type: comment
  author: MJDLPRO
  content: "I understand the code and comments got mixed to cause confusion. I tried\
    \ map_reduce and it also failed in the last step. I think I understand the problem,\
    \ while creating the summary of summaries (last summary), the size of text must\
    \ be greater but then the same pipeline works like a charm with LLM = OpenAI(...)\
    \ with map_reduce Langchain chain_type. \nSomething gets handled by Langchain\
    \ and OpenAI combination but fails with Langchain and Dolly-LLM combination i.e.,\
    \ Langchain and Dolly 2 don't work as well. I am not sure if it will be possible\
    \ to do all root cause analysis and resolve the root cause on this thread. Nevertheless,\
    \ thanks for your help."
  created_at: 2023-05-05 11:53:07+00:00
  edited: false
  hidden: false
  id: 6454fc33a473375be56e0575
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-05T13:16:29.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>The difference is simply this model has a smaller 2k token window,
          not 8k. Thats all there is to it</p>

          '
        raw: The difference is simply this model has a smaller 2k token window, not
          8k. Thats all there is to it
        updatedAt: '2023-05-05T13:16:29.508Z'
      numEdits: 0
      reactions: []
    id: 645501adf61f10d69dc6713f
    type: comment
  author: srowen
  content: The difference is simply this model has a smaller 2k token window, not
    8k. Thats all there is to it
  created_at: 2023-05-05 12:16:29+00:00
  edited: false
  hidden: false
  id: 645501adf61f10d69dc6713f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-09T13:40:14.000Z'
    data:
      status: closed
    id: 645a4d3effcd6a698fb0cf00
    type: status-change
  author: srowen
  created_at: 2023-05-09 12:40:14+00:00
  id: 645a4d3effcd6a698fb0cf00
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be7369190ca2c5c370249cb60cd524fe.svg
      fullname: Anna Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maibananna
      type: user
    createdAt: '2023-05-11T18:20:42.000Z'
    data:
      edited: true
      editors:
      - maibananna
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be7369190ca2c5c370249cb60cd524fe.svg
          fullname: Anna Mai
          isHf: false
          isPro: false
          name: maibananna
          type: user
        html: '<p>I am having the same issue... but I can''t use OpenAI as my LLM
          so I''m wondering if anyone has any other ideas on how to fix this? I am
          tempted to I write my own summarizer function that will do the same as langchain''s
          load_summarize_chain...<br>Btw I am using map_reduce but like MJDLPRO mentioned
          still gives same error in the last step: chain = load_summarize_chain(llm
          = hf_dolly_pipeline, chain_type = "map_reduce")</p>

          '
        raw: 'I am having the same issue... but I can''t use OpenAI as my LLM so I''m
          wondering if anyone has any other ideas on how to fix this? I am tempted
          to I write my own summarizer function that will do the same as langchain''s
          load_summarize_chain...

          Btw I am using map_reduce but like MJDLPRO mentioned still gives same error
          in the last step: chain = load_summarize_chain(llm = hf_dolly_pipeline,
          chain_type = "map_reduce")'
        updatedAt: '2023-05-11T18:21:08.271Z'
      numEdits: 1
      reactions: []
    id: 645d31faf1e3b219cb0ccec6
    type: comment
  author: maibananna
  content: 'I am having the same issue... but I can''t use OpenAI as my LLM so I''m
    wondering if anyone has any other ideas on how to fix this? I am tempted to I
    write my own summarizer function that will do the same as langchain''s load_summarize_chain...

    Btw I am using map_reduce but like MJDLPRO mentioned still gives same error in
    the last step: chain = load_summarize_chain(llm = hf_dolly_pipeline, chain_type
    = "map_reduce")'
  created_at: 2023-05-11 17:20:42+00:00
  edited: true
  hidden: false
  id: 645d31faf1e3b219cb0ccec6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 63
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: The size of tensor a (2048) must match the size of tensor b
  (7080) at non-singleton dimension 3'
