!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KiranAli
conflicting_files: null
created_at: 2023-04-20 17:45:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-04-20T18:45:24.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>I''m trying to feed pdfs to Dolly for Q/As. Following is the snippet
          of code that I''m using.</p>

          <pre><code>loader = TextLoader("doc.txt")

          documents = loader.load()

          text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

          texts = text_splitter.split_documents(documents)


          embeddings = OpenAIEmbeddings()

          docsearch = Chroma.from_documents(texts, embeddings)

          </code></pre>

          <p>Is there any other option for generating embeddings to store in vector
          store or OpenAIEmbeddings is the best option?</p>

          '
        raw: "I'm trying to feed pdfs to Dolly for Q/As. Following is the snippet\
          \ of code that I'm using.\r\n\r\n    loader = TextLoader(\"doc.txt\")\r\n\
          \    documents = loader.load()\r\n    text_splitter = CharacterTextSplitter(chunk_size=1000,\
          \ chunk_overlap=0)\r\n    texts = text_splitter.split_documents(documents)\r\
          \n\r\n    embeddings = OpenAIEmbeddings()\r\n    docsearch = Chroma.from_documents(texts,\
          \ embeddings)\r\n\r\nIs there any other option for generating embeddings\
          \ to store in vector store or OpenAIEmbeddings is the best option?\r\n\r\
          \n"
        updatedAt: '2023-04-20T18:45:24.020Z'
      numEdits: 0
      reactions: []
    id: 64418844608bf7e3ad77e874
    type: comment
  author: KiranAli
  content: "I'm trying to feed pdfs to Dolly for Q/As. Following is the snippet of\
    \ code that I'm using.\r\n\r\n    loader = TextLoader(\"doc.txt\")\r\n    documents\
    \ = loader.load()\r\n    text_splitter = CharacterTextSplitter(chunk_size=1000,\
    \ chunk_overlap=0)\r\n    texts = text_splitter.split_documents(documents)\r\n\
    \r\n    embeddings = OpenAIEmbeddings()\r\n    docsearch = Chroma.from_documents(texts,\
    \ embeddings)\r\n\r\nIs there any other option for generating embeddings to store\
    \ in vector store or OpenAIEmbeddings is the best option?\r\n\r\n"
  created_at: 2023-04-20 17:45:24+00:00
  edited: false
  hidden: false
  id: 64418844608bf7e3ad77e874
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-20T18:58:07.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: "<p>It doesn't look like you are loading PDFs there? You want this:\
          \ <a rel=\"nofollow\" href=\"https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\"\
          >https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html</a><br>Language\
          \ models don't operate on PDFs, but, if you get text from PDFs, sure.</p>\n\
          <p>You can plug in any embedding model to langchain, not just OpenAI, though\
          \ it works well.<br>Dolly is not an encoder model though. It'd be overkill\
          \ anyway. Just use a sentence-transfomers model.</p>\n<pre><code>from langchain.embeddings\
          \ import HuggingFaceEmbeddings\n hf_embed = HuggingFaceEmbeddings(model_name=\"\
          sentence-transformers/all-mpnet-base-v2\")\n</code></pre>\n<p>Dolly could\
          \ be used as the text-generation LLM part though.<br>Databricks has a whole\
          \ demo at <a rel=\"nofollow\" href=\"https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot\"\
          >https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot</a></p>\n"
        raw: "It doesn't look like you are loading PDFs there? You want this: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\n\
          Language models don't operate on PDFs, but, if you get text from PDFs, sure.\n\
          \nYou can plug in any embedding model to langchain, not just OpenAI, though\
          \ it works well.\nDolly is not an encoder model though. It'd be overkill\
          \ anyway. Just use a sentence-transfomers model.\n```\nfrom langchain.embeddings\
          \ import HuggingFaceEmbeddings\n hf_embed = HuggingFaceEmbeddings(model_name=\"\
          sentence-transformers/all-mpnet-base-v2\")\n```\n\nDolly could be used as\
          \ the text-generation LLM part though.\nDatabricks has a whole demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot"
        updatedAt: '2023-04-20T18:58:07.774Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KiranAli
    id: 64418b3f006550f1ed75a507
    type: comment
  author: srowen
  content: "It doesn't look like you are loading PDFs there? You want this: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\n\
    Language models don't operate on PDFs, but, if you get text from PDFs, sure.\n\
    \nYou can plug in any embedding model to langchain, not just OpenAI, though it\
    \ works well.\nDolly is not an encoder model though. It'd be overkill anyway.\
    \ Just use a sentence-transfomers model.\n```\nfrom langchain.embeddings import\
    \ HuggingFaceEmbeddings\n hf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"\
    )\n```\n\nDolly could be used as the text-generation LLM part though.\nDatabricks\
    \ has a whole demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot"
  created_at: 2023-04-20 17:58:07+00:00
  edited: false
  hidden: false
  id: 64418b3f006550f1ed75a507
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-23T14:46:39.000Z'
    data:
      status: closed
    id: 644544cf0f2fc80feb2481a0
    type: status-change
  author: srowen
  created_at: 2023-04-23 13:46:39+00:00
  id: 644544cf0f2fc80feb2481a0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-04-26T19:37:12.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>I''m following this demo <a rel="nofollow" href="https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot">https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot</a>.
          But is it possible to fine-tune it on raw data instead of instruction-based
          dataset?</p>

          '
        raw: I'm following this demo https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot.
          But is it possible to fine-tune it on raw data instead of instruction-based
          dataset?
        updatedAt: '2023-04-26T19:37:12.004Z'
      numEdits: 0
      reactions: []
    id: 64497d681af713976c3190eb
    type: comment
  author: KiranAli
  content: I'm following this demo https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot.
    But is it possible to fine-tune it on raw data instead of instruction-based dataset?
  created_at: 2023-04-26 18:37:12+00:00
  edited: false
  hidden: false
  id: 64497d681af713976c3190eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T19:48:17.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I think that''s an unrelated question. Yes, but you would modify
          code at <a rel="nofollow" href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a>
          to accept different input, rather than form question-response into text
          strings. It''s not clear whether tuning on raw data makes its output do
          what you want though.</p>

          '
        raw: I think that's an unrelated question. Yes, but you would modify code
          at https://github.com/databrickslabs/dolly to accept different input, rather
          than form question-response into text strings. It's not clear whether tuning
          on raw data makes its output do what you want though.
        updatedAt: '2023-04-26T19:48:17.220Z'
      numEdits: 0
      reactions: []
    id: 64498001a281f51a62b07541
    type: comment
  author: srowen
  content: I think that's an unrelated question. Yes, but you would modify code at
    https://github.com/databrickslabs/dolly to accept different input, rather than
    form question-response into text strings. It's not clear whether tuning on raw
    data makes its output do what you want though.
  created_at: 2023-04-26 18:48:17+00:00
  edited: false
  hidden: false
  id: 64498001a281f51a62b07541
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-02T19:05:24.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: "<blockquote>\n<p>It doesn't look like you are loading PDFs there? You\
          \ want this: <a rel=\"nofollow\" href=\"https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\"\
          >https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html</a><br>Language\
          \ models don't operate on PDFs, but, if you get text from PDFs, sure.</p>\n\
          <p>You can plug in any embedding model to langchain, not just OpenAI, though\
          \ it works well.<br>Dolly is not an encoder model though. It'd be overkill\
          \ anyway. Just use a sentence-transfomers model.</p>\n<pre><code>from langchain.embeddings\
          \ import HuggingFaceEmbeddings\n hf_embed = HuggingFaceEmbeddings(model_name=\"\
          sentence-transformers/all-mpnet-base-v2\")\n</code></pre>\n<p>Dolly could\
          \ be used as the text-generation LLM part though.<br>Databricks has a whole\
          \ demo at <a rel=\"nofollow\" href=\"https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot\"\
          >https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot</a></p>\n\
          </blockquote>\n<p>OpenAIEmbeddings works very well but HuggingfaceEmbeddings\
          \ gives very poor result</p>\n"
        raw: "> It doesn't look like you are loading PDFs there? You want this: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\n\
          > Language models don't operate on PDFs, but, if you get text from PDFs,\
          \ sure.\n> \n> You can plug in any embedding model to langchain, not just\
          \ OpenAI, though it works well.\n> Dolly is not an encoder model though.\
          \ It'd be overkill anyway. Just use a sentence-transfomers model.\n> ```\n\
          > from langchain.embeddings import HuggingFaceEmbeddings\n>  hf_embed =\
          \ HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"\
          )\n> ```\n> \n> Dolly could be used as the text-generation LLM part though.\n\
          > Databricks has a whole demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot\n\
          \nOpenAIEmbeddings works very well but HuggingfaceEmbeddings gives very\
          \ poor result"
        updatedAt: '2023-05-02T19:05:24.404Z'
      numEdits: 0
      reactions: []
    id: 64515ef4b3f75261a7da83f6
    type: comment
  author: KiranAli
  content: "> It doesn't look like you are loading PDFs there? You want this: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html\n\
    > Language models don't operate on PDFs, but, if you get text from PDFs, sure.\n\
    > \n> You can plug in any embedding model to langchain, not just OpenAI, though\
    \ it works well.\n> Dolly is not an encoder model though. It'd be overkill anyway.\
    \ Just use a sentence-transfomers model.\n> ```\n> from langchain.embeddings import\
    \ HuggingFaceEmbeddings\n>  hf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"\
    )\n> ```\n> \n> Dolly could be used as the text-generation LLM part though.\n\
    > Databricks has a whole demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot\n\
    \nOpenAIEmbeddings works very well but HuggingfaceEmbeddings gives very poor result"
  created_at: 2023-05-02 18:05:24+00:00
  edited: false
  hidden: false
  id: 64515ef4b3f75261a7da83f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-02T19:10:16.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>HuggingFaceEmbeddings isn''t an embedding, but a way to apply other
          embeddings. Sure, use whatever embedding you like.</p>

          '
        raw: HuggingFaceEmbeddings isn't an embedding, but a way to apply other embeddings.
          Sure, use whatever embedding you like.
        updatedAt: '2023-05-02T19:10:16.083Z'
      numEdits: 0
      reactions: []
    id: 645160185fb40b9f50b0b93f
    type: comment
  author: srowen
  content: HuggingFaceEmbeddings isn't an embedding, but a way to apply other embeddings.
    Sure, use whatever embedding you like.
  created_at: 2023-05-02 18:10:16+00:00
  edited: false
  hidden: false
  id: 645160185fb40b9f50b0b93f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-05T06:14:56.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span>, I know this is\
          \ not the right place to ask this question but if you have any idea kindly\
          \ guide me. I have developed a complete pipeline that reads a text doc file\
          \ and fed vectors to dolly-v2-7b. I'm running it on VM having two 2 V100\
          \ 16GB GPUs. It takes 12s to generate answer to simple question like What\
          \ is \"product name\"? (it's mentioned at the start of doc.)</p>\n<p>I'm\
          \ following this  <a rel=\"nofollow\" href=\"https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot\"\
          >https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot</a>.\
          \ At the very last step its mentioned that optimum will improve inference\
          \ greatly so use that to enhance inference speed. I'm using it as </p>\n\
          <pre><code>      tokenizer = AutoTokenizer.from_pretrained(input_model,\
          \ padding_side=\"left\")\n      model = ORTModelForCausalLM.from_pretrained(\"\
          databricks/dolly-v2-3b\", export=True, provider=\"CUDAExecutionProvider\"\
          )\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>     2023-05-05\
          \ 04:39:44.132458586 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp]\
          \ Rerunning with verbose output on a non-minimal build will show node assignments.\n\
          \     2023-05-05 04:39:44.653800957 [E:onnxruntime:, inference_session.cc:1532\
          \ operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368\
          \ void* \n     onnxruntime::BFCArena::AllocateRawInternal(size_t, bool,\
          \ onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to\
          \ allocate memory for requested buffer of size 78643200\n</code></pre>\n\
          <p>I think optimum is not using both GPUs plus need guidance on if I'm on\
          \ the right track.</p>\n"
        raw: "Hi @srowen, I know this is not the right place to ask this question\
          \ but if you have any idea kindly guide me. I have developed a complete\
          \ pipeline that reads a text doc file and fed vectors to dolly-v2-7b. I'm\
          \ running it on VM having two 2 V100 16GB GPUs. It takes 12s to generate\
          \ answer to simple question like What is \"product name\"? (it's mentioned\
          \ at the start of doc.)\n\nI'm following this  https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot.\
          \ At the very last step its mentioned that optimum will improve inference\
          \ greatly so use that to enhance inference speed. I'm using it as \n\n \
          \         tokenizer = AutoTokenizer.from_pretrained(input_model, padding_side=\"\
          left\")\n          model = ORTModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\"\
          , export=True, provider=\"CUDAExecutionProvider\")\n\nI get the following\
          \ error:\n\n         2023-05-05 04:39:44.132458586 [W:onnxruntime:, session_state.cc:1138\
          \ VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal\
          \ build will show node assignments.\n         2023-05-05 04:39:44.653800957\
          \ [E:onnxruntime:, inference_session.cc:1532 operator()] Exception during\
          \ initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368\
          \ void* \n         onnxruntime::BFCArena::AllocateRawInternal(size_t, bool,\
          \ onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to\
          \ allocate memory for requested buffer of size 78643200\n\nI think optimum\
          \ is not using both GPUs plus need guidance on if I'm on the right track."
        updatedAt: '2023-05-05T06:14:56.195Z'
      numEdits: 0
      reactions: []
    id: 64549ee0fe2f48cb4b5ba2f2
    type: comment
  author: KiranAli
  content: "Hi @srowen, I know this is not the right place to ask this question but\
    \ if you have any idea kindly guide me. I have developed a complete pipeline that\
    \ reads a text doc file and fed vectors to dolly-v2-7b. I'm running it on VM having\
    \ two 2 V100 16GB GPUs. It takes 12s to generate answer to simple question like\
    \ What is \"product name\"? (it's mentioned at the start of doc.)\n\nI'm following\
    \ this  https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot.\
    \ At the very last step its mentioned that optimum will improve inference greatly\
    \ so use that to enhance inference speed. I'm using it as \n\n          tokenizer\
    \ = AutoTokenizer.from_pretrained(input_model, padding_side=\"left\")\n      \
    \    model = ORTModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", export=True,\
    \ provider=\"CUDAExecutionProvider\")\n\nI get the following error:\n\n      \
    \   2023-05-05 04:39:44.132458586 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp]\
    \ Rerunning with verbose output on a non-minimal build will show node assignments.\n\
    \         2023-05-05 04:39:44.653800957 [E:onnxruntime:, inference_session.cc:1532\
    \ operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368\
    \ void* \n         onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*,\
    \ bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested\
    \ buffer of size 78643200\n\nI think optimum is not using both GPUs plus need\
    \ guidance on if I'm on the right track."
  created_at: 2023-05-05 05:14:56+00:00
  edited: false
  hidden: false
  id: 64549ee0fe2f48cb4b5ba2f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-05T12:32:31.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>It won''t use 2 GPUs without device_map="auto" or something similar.
          You should also load in 16 bit with torch_dtype=torch.float16. This says
          you ran out of GPU mem and those might help.</p>

          '
        raw: It won't use 2 GPUs without device_map="auto" or something similar. You
          should also load in 16 bit with torch_dtype=torch.float16. This says you
          ran out of GPU mem and those might help.
        updatedAt: '2023-05-05T12:32:31.227Z'
      numEdits: 0
      reactions: []
    id: 6454f75ff61f10d69dc57591
    type: comment
  author: srowen
  content: It won't use 2 GPUs without device_map="auto" or something similar. You
    should also load in 16 bit with torch_dtype=torch.float16. This says you ran out
    of GPU mem and those might help.
  created_at: 2023-05-05 11:32:31+00:00
  edited: false
  hidden: false
  id: 6454f75ff61f10d69dc57591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-15T05:57:39.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span> , optimum doesn't\
          \ make use of multiple GPUs. So I successfully converted it onnx format\
          \ on an A100 40GB VM. It enhanced inference speed by 1-2s. Earlier it took\
          \ 3-4s to generate answers to simple questions like \"What is product?\"\
          \ But it takes 34GB memory to run dolly in onnx format. I think that's too\
          \ much to run a smaller model. Earlier it was taking 6-7GB. Any pointers\
          \ on this? I've been stuck for a long time on this step, which prevents\
          \ me from moving forward to the next step. I want to build a chatbot to\
          \ which I feed lots of books and then use it for my customers. Plus I think\
          \ inference speed would also be affected in case of multiple queries at\
          \ same.</p>\n"
        raw: Hi @srowen , optimum doesn't make use of multiple GPUs. So I successfully
          converted it onnx format on an A100 40GB VM. It enhanced inference speed
          by 1-2s. Earlier it took 3-4s to generate answers to simple questions like
          "What is product?" But it takes 34GB memory to run dolly in onnx format.
          I think that's too much to run a smaller model. Earlier it was taking 6-7GB.
          Any pointers on this? I've been stuck for a long time on this step, which
          prevents me from moving forward to the next step. I want to build a chatbot
          to which I feed lots of books and then use it for my customers. Plus I think
          inference speed would also be affected in case of multiple queries at same.
        updatedAt: '2023-05-15T05:57:39.245Z'
      numEdits: 0
      reactions: []
    id: 6461c9d3a3081fffa0610d95
    type: comment
  author: KiranAli
  content: Hi @srowen , optimum doesn't make use of multiple GPUs. So I successfully
    converted it onnx format on an A100 40GB VM. It enhanced inference speed by 1-2s.
    Earlier it took 3-4s to generate answers to simple questions like "What is product?"
    But it takes 34GB memory to run dolly in onnx format. I think that's too much
    to run a smaller model. Earlier it was taking 6-7GB. Any pointers on this? I've
    been stuck for a long time on this step, which prevents me from moving forward
    to the next step. I want to build a chatbot to which I feed lots of books and
    then use it for my customers. Plus I think inference speed would also be affected
    in case of multiple queries at same.
  created_at: 2023-05-15 04:57:39+00:00
  edited: false
  hidden: false
  id: 6461c9d3a3081fffa0610d95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-15T12:30:04.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Use a smaller model? it takes a few seconds for me on an A10, not
          sure what your current setup or issue is</p>

          '
        raw: Use a smaller model? it takes a few seconds for me on an A10, not sure
          what your current setup or issue is
        updatedAt: '2023-05-15T12:30:04.849Z'
      numEdits: 0
      reactions: []
    id: 646225cc5eec3c624b010941
    type: comment
  author: srowen
  content: Use a smaller model? it takes a few seconds for me on an A10, not sure
    what your current setup or issue is
  created_at: 2023-05-15 11:30:04+00:00
  edited: false
  hidden: false
  id: 646225cc5eec3c624b010941
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-15T12:31:33.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>i''m using dolly-v2-3b. It can take 3-5s on A100.</p>

          '
        raw: i'm using dolly-v2-3b. It can take 3-5s on A100.
        updatedAt: '2023-05-15T12:31:33.327Z'
      numEdits: 0
      reactions: []
    id: 64622625724c301fafac9139
    type: comment
  author: KiranAli
  content: i'm using dolly-v2-3b. It can take 3-5s on A100.
  created_at: 2023-05-15 11:31:33+00:00
  edited: false
  hidden: false
  id: 64622625724c301fafac9139
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-15T12:42:16.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I''m seeing 1s on a smaller GPU. What are your generation settings
          and input/output length? these make a big difference to speed.</p>

          '
        raw: I'm seeing 1s on a smaller GPU. What are your generation settings and
          input/output length? these make a big difference to speed.
        updatedAt: '2023-05-15T12:42:16.208Z'
      numEdits: 0
      reactions: []
    id: 646228a8a0b47a9ad4eba5dd
    type: comment
  author: srowen
  content: I'm seeing 1s on a smaller GPU. What are your generation settings and input/output
    length? these make a big difference to speed.
  created_at: 2023-05-15 11:42:16+00:00
  edited: false
  hidden: false
  id: 646228a8a0b47a9ad4eba5dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-15T13:09:39.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer,
          return_full_text=True, max_new_tokens=256, top_p=0.95, top_k=50, task=''text-generation'',
          torch_dtype=torch.bfloat16)</p>

          '
        raw: InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer, return_full_text=True,
          max_new_tokens=256, top_p=0.95, top_k=50, task='text-generation', torch_dtype=torch.bfloat16)
        updatedAt: '2023-05-15T13:09:39.185Z'
      numEdits: 0
      reactions: []
    id: 64622f13fe351c4ea2915bb7
    type: comment
  author: KiranAli
  content: InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer, return_full_text=True,
    max_new_tokens=256, top_p=0.95, top_k=50, task='text-generation', torch_dtype=torch.bfloat16)
  created_at: 2023-05-15 12:09:39+00:00
  edited: false
  hidden: false
  id: 64622f13fe351c4ea2915bb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-05-16T12:26:29.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>This simple script is taking 5.4s</p>

          <p>import time</p>

          <p>import torch<br>from instruct_pipeline import InstructionTextGenerationPipeline<br>from
          transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-7b", padding_side="left")<br>model
          = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-7b", device_map="auto",
          torch_dtype=torch.float16)</p>

          <p>generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)<br>start
          = time.time()<br>res = generate_text("Explain to me the difference between
          nuclear fission and fusion.")<br>end = time.time()<br>print(res[0]["generated_text"])<br>print(end
          - start)</p>

          <p>Running on A10 24GB on huggingface space.</p>

          '
        raw: 'This simple script is taking 5.4s


          import time


          import torch

          from instruct_pipeline import InstructionTextGenerationPipeline

          from transformers import AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-7b", padding_side="left")

          model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-7b", device_map="auto",
          torch_dtype=torch.float16)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)

          start = time.time()

          res = generate_text("Explain to me the difference between nuclear fission
          and fusion.")

          end = time.time()

          print(res[0]["generated_text"])

          print(end - start)



          Running on A10 24GB on huggingface space.'
        updatedAt: '2023-05-16T12:26:29.817Z'
      numEdits: 0
      reactions: []
    id: 646376751ac9eb9637cfaae6
    type: comment
  author: KiranAli
  content: 'This simple script is taking 5.4s


    import time


    import torch

    from instruct_pipeline import InstructionTextGenerationPipeline

    from transformers import AutoModelForCausalLM, AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-7b", padding_side="left")

    model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-7b", device_map="auto",
    torch_dtype=torch.float16)


    generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)

    start = time.time()

    res = generate_text("Explain to me the difference between nuclear fission and
    fusion.")

    end = time.time()

    print(res[0]["generated_text"])

    print(end - start)



    Running on A10 24GB on huggingface space.'
  created_at: 2023-05-16 11:26:29+00:00
  edited: false
  hidden: false
  id: 646376751ac9eb9637cfaae6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-16T13:12:09.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You should load in bfloat16 but that''s separate.<br>Please use
          pipeline() to load as shown in model card. Might work better. This depends
          a lot on generation settings</p>

          '
        raw: 'You should load in bfloat16 but that''s separate.

          Please use pipeline() to load as shown in model card. Might work better.
          This depends a lot on generation settings'
        updatedAt: '2023-05-16T13:12:09.353Z'
      numEdits: 0
      reactions: []
    id: 6463812912814d754179c97c
    type: comment
  author: srowen
  content: 'You should load in bfloat16 but that''s separate.

    Please use pipeline() to load as shown in model card. Might work better. This
    depends a lot on generation settings'
  created_at: 2023-05-16 12:12:09+00:00
  edited: false
  hidden: false
  id: 6463812912814d754179c97c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 50
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Feed PDFs to Dolly
