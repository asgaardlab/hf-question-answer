!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Iamexperimenting
conflicting_files: null
created_at: 2023-11-06 18:52:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
      fullname: IamexperimentingNow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Iamexperimenting
      type: user
    createdAt: '2023-11-06T18:52:51.000Z'
    data:
      edited: true
      editors:
      - Iamexperimenting
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9242377281188965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
          fullname: IamexperimentingNow
          isHf: false
          isPro: false
          name: Iamexperimenting
          type: user
        html: '<p>Hi Team,</p>

          <p>I''m using Dollyv2 LLM for Q&amp;A application, and using Langchain.
          </p>

          <p>Here, my input data are from pdf files, I have around 50 pdf files and
          each files are different from each other.</p>

          <p>Here, my question is, is it meaningful to add conversational buffer memory
          to this application (Q&amp;A)? because when I logically think, LLM expects
          Prompt + Question + relevant chunks. so every time if I ask question to
          the model it sends different question with different context and those context
          are different from each other.</p>

          <p>when I ask question to them model for first time, it takes/remember the
          question, context, answer and this question belongs to 1st pdf document.
          And all of a sudden if I ask a question about 25th pdf document which is
          completely different the 1st pdf document, here does the conversation buffer
          memory helps?</p>

          '
        raw: "Hi Team,\n\nI'm using Dollyv2 LLM for Q&A application, and using Langchain.\
          \ \n\nHere, my input data are from pdf files, I have around 50 pdf files\
          \ and each files are different from each other.\n\nHere, my question is,\
          \ is it meaningful to add conversational buffer memory to this application\
          \ (Q&A)? because when I logically think, LLM expects Prompt + Question +\
          \ relevant chunks. so every time if I ask question to the model it sends\
          \ different question with different context and those context are different\
          \ from each other.\n\nwhen I ask question to them model for first time,\
          \ it takes/remember the question, context, answer and this question belongs\
          \ to 1st pdf document. And all of a sudden if I ask a question about 25th\
          \ pdf document which is completely different the 1st pdf document, here\
          \ does the conversation buffer memory helps?\n"
        updatedAt: '2023-11-06T18:55:27.175Z'
      numEdits: 1
      reactions: []
    id: 654936037ab20d807dd7366c
    type: comment
  author: Iamexperimenting
  content: "Hi Team,\n\nI'm using Dollyv2 LLM for Q&A application, and using Langchain.\
    \ \n\nHere, my input data are from pdf files, I have around 50 pdf files and each\
    \ files are different from each other.\n\nHere, my question is, is it meaningful\
    \ to add conversational buffer memory to this application (Q&A)? because when\
    \ I logically think, LLM expects Prompt + Question + relevant chunks. so every\
    \ time if I ask question to the model it sends different question with different\
    \ context and those context are different from each other.\n\nwhen I ask question\
    \ to them model for first time, it takes/remember the question, context, answer\
    \ and this question belongs to 1st pdf document. And all of a sudden if I ask\
    \ a question about 25th pdf document which is completely different the 1st pdf\
    \ document, here does the conversation buffer memory helps?\n"
  created_at: 2023-11-06 18:52:51+00:00
  edited: true
  hidden: false
  id: 654936037ab20d807dd7366c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 92
repo_id: databricks/dolly-v2-12b
repo_type: model
status: open
target_branch: null
title: Adding Buffer memory to Q&A application
