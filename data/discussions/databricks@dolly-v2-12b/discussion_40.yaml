!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kareem22
conflicting_files: null
created_at: 2023-04-18 13:41:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dcf3b72ad5e2fdc8bc6f0224a48f170.svg
      fullname: kareem desouky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kareem22
      type: user
    createdAt: '2023-04-18T14:41:53.000Z'
    data:
      edited: false
      editors:
      - kareem22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dcf3b72ad5e2fdc8bc6f0224a48f170.svg
          fullname: kareem desouky
          isHf: false
          isPro: false
          name: kareem22
          type: user
        html: "<p>context = \"\"\"George Washington (February 22, 1732[b] \u2013 December\
          \ 14, 1799) was an American military officer, statesman,<br>and Founding\
          \ Father who served as the first president of the United States from 1789\
          \ to 1797.\"\"\"</p>\n<p>print(llm_context_chain.predict(instruction=\"\
          When was George Washington president?\", context=context).lstrip())</p>\n\
          <p>give me that </p>\n<pre><code>150 response = self.pipeline(prompt)\n\
          151 if self.pipeline.task == \"text-generation\":\n152     # Text generation\
          \ return includes the starter text.\n</code></pre>\n<p>--&gt; 153     text\
          \ = response[0][\"generated_text\"][len(prompt) :]<br>    154 elif self.pipeline.task\
          \ == \"text2text-generation\":<br>    155     text = response[0][\"generated_text\"\
          ]</p>\n<p>TypeError: string indices must be integers</p>\n"
        raw: "context = \"\"\"George Washington (February 22, 1732[b] \u2013 December\
          \ 14, 1799) was an American military officer, statesman,\r\nand Founding\
          \ Father who served as the first president of the United States from 1789\
          \ to 1797.\"\"\"\r\n\r\nprint(llm_context_chain.predict(instruction=\"When\
          \ was George Washington president?\", context=context).lstrip())\r\n\r\n\
          give me that \r\n\r\n    150 response = self.pipeline(prompt)\r\n    151\
          \ if self.pipeline.task == \"text-generation\":\r\n    152     # Text generation\
          \ return includes the starter text.\r\n--> 153     text = response[0][\"\
          generated_text\"][len(prompt) :]\r\n    154 elif self.pipeline.task == \"\
          text2text-generation\":\r\n    155     text = response[0][\"generated_text\"\
          ]\r\n\r\nTypeError: string indices must be integers\r\n"
        updatedAt: '2023-04-18T14:41:53.518Z'
      numEdits: 0
      reactions: []
    id: 643eac31e5500bc3b9a57969
    type: comment
  author: kareem22
  content: "context = \"\"\"George Washington (February 22, 1732[b] \u2013 December\
    \ 14, 1799) was an American military officer, statesman,\r\nand Founding Father\
    \ who served as the first president of the United States from 1789 to 1797.\"\"\
    \"\r\n\r\nprint(llm_context_chain.predict(instruction=\"When was George Washington\
    \ president?\", context=context).lstrip())\r\n\r\ngive me that \r\n\r\n    150\
    \ response = self.pipeline(prompt)\r\n    151 if self.pipeline.task == \"text-generation\"\
    :\r\n    152     # Text generation return includes the starter text.\r\n--> 153\
    \     text = response[0][\"generated_text\"][len(prompt) :]\r\n    154 elif self.pipeline.task\
    \ == \"text2text-generation\":\r\n    155     text = response[0][\"generated_text\"\
    ]\r\n\r\nTypeError: string indices must be integers\r\n"
  created_at: 2023-04-18 13:41:53+00:00
  edited: false
  hidden: false
  id: 643eac31e5500bc3b9a57969
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-18T14:47:07.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Looks like you didn''t set return_full_text=True as in the example<br><a
          rel="nofollow" href="https://github.com/databrickslabs/dolly/blob/master/examples/langchain.py#L60">https://github.com/databrickslabs/dolly/blob/master/examples/langchain.py#L60</a></p>

          '
        raw: 'Looks like you didn''t set return_full_text=True as in the example

          https://github.com/databrickslabs/dolly/blob/master/examples/langchain.py#L60'
        updatedAt: '2023-04-18T14:47:07.644Z'
      numEdits: 0
      reactions: []
    id: 643ead6b8208445855013598
    type: comment
  author: srowen
  content: 'Looks like you didn''t set return_full_text=True as in the example

    https://github.com/databrickslabs/dolly/blob/master/examples/langchain.py#L60'
  created_at: 2023-04-18 13:47:07+00:00
  edited: false
  hidden: false
  id: 643ead6b8208445855013598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
      fullname: Chen Yao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cysony
      type: user
    createdAt: '2023-04-18T16:09:54.000Z'
    data:
      edited: false
      editors:
      - cysony
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
          fullname: Chen Yao
          isHf: false
          isPro: false
          name: cysony
          type: user
        html: '<p>Is there an example of how it is used with RetrievalQA? used the
          exact same setup in the model card, wrapping pipe with HuggingFacePipeline,
          and pass it on to RetrievalQA.from_chain_type as the llm arg. However, getting
          the error " The following <code>model_kwargs</code> are not used by the
          model: [''return_full_text'']"</p>

          '
        raw: 'Is there an example of how it is used with RetrievalQA? used the exact
          same setup in the model card, wrapping pipe with HuggingFacePipeline, and
          pass it on to RetrievalQA.from_chain_type as the llm arg. However, getting
          the error " The following `model_kwargs` are not used by the model: [''return_full_text'']"'
        updatedAt: '2023-04-18T16:09:54.690Z'
      numEdits: 0
      reactions: []
    id: 643ec0d2f2ed3bc5c05feaec
    type: comment
  author: cysony
  content: 'Is there an example of how it is used with RetrievalQA? used the exact
    same setup in the model card, wrapping pipe with HuggingFacePipeline, and pass
    it on to RetrievalQA.from_chain_type as the llm arg. However, getting the error
    " The following `model_kwargs` are not used by the model: [''return_full_text'']"'
  created_at: 2023-04-18 15:09:54+00:00
  edited: false
  hidden: false
  id: 643ec0d2f2ed3bc5c05feaec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-18T16:35:15.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Show how you are loading the pipeline</p>

          '
        raw: Show how you are loading the pipeline
        updatedAt: '2023-04-18T16:35:15.110Z'
      numEdits: 0
      reactions: []
    id: 643ec6c38d9a6e3bba31fdba
    type: comment
  author: srowen
  content: Show how you are loading the pipeline
  created_at: 2023-04-18 15:35:15+00:00
  edited: false
  hidden: false
  id: 643ec6c38d9a6e3bba31fdba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
      fullname: Chen Yao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cysony
      type: user
    createdAt: '2023-04-18T22:17:05.000Z'
    data:
      edited: false
      editors:
      - cysony
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
          fullname: Chen Yao
          isHf: false
          isPro: false
          name: cysony
          type: user
        html: '<p>nothing really fancy, pretty standard as the examples from the model
          card ...<br>pipe = pipeline(model=model_path, torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto", return_full_text=True)<br>llm
          = HuggingFacePipeline(pipeline=pipe)</p>

          <p>and then using the llm in below:</p>

          <p>qa = RetrievalQA.from_chain_type(<br>    llm=llm, chain_type="stuff"<br>    ,retriever=retriever<br>    ,return_source_documents=True<br>    )</p>

          <p>and above error occured when using qa to do document QA as qa({''query'':
          "some random question"})</p>

          '
        raw: "nothing really fancy, pretty standard as the examples from the model\
          \ card ... \npipe = pipeline(model=model_path, torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n\
          llm = HuggingFacePipeline(pipeline=pipe)\n\nand then using the llm in below:\n\
          \nqa = RetrievalQA.from_chain_type(\n    llm=llm, chain_type=\"stuff\"\n\
          \    ,retriever=retriever\n    ,return_source_documents=True\n    )\n\n\
          and above error occured when using qa to do document QA as qa({'query':\
          \ \"some random question\"})"
        updatedAt: '2023-04-18T22:17:05.526Z'
      numEdits: 0
      reactions: []
    id: 643f16e146c418c9c68447a9
    type: comment
  author: cysony
  content: "nothing really fancy, pretty standard as the examples from the model card\
    \ ... \npipe = pipeline(model=model_path, torch_dtype=torch.bfloat16, trust_remote_code=True,\
    \ device_map=\"auto\", return_full_text=True)\nllm = HuggingFacePipeline(pipeline=pipe)\n\
    \nand then using the llm in below:\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\
    \ chain_type=\"stuff\"\n    ,retriever=retriever\n    ,return_source_documents=True\n\
    \    )\n\nand above error occured when using qa to do document QA as qa({'query':\
    \ \"some random question\"})"
  created_at: 2023-04-18 21:17:05+00:00
  edited: false
  hidden: false
  id: 643f16e146c418c9c68447a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-18T22:20:59.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: "<p>Hm, this works fine for me, as an example. Are you loading from\
          \ some local copy? </p>\n<pre><code>  instruct_pipeline = pipeline(model=\"\
          databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16, trust_remote_code=True,\
          \ device_map=\"auto\", \n    return_full_text=True, do_sample=False, max_new_tokens=128)\n\
          \  \n  prompt_with_context = PromptTemplate(input_variables=[\"question\"\
          , \"context\"], template=\"{context}\\n\\n{question}\")\n\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n\
          \  load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt_with_context)\n\
          </code></pre>\n"
        raw: "Hm, this works fine for me, as an example. Are you loading from some\
          \ local copy? \n\n```\n  instruct_pipeline = pipeline(model=\"databricks/dolly-v2-7b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          , \n    return_full_text=True, do_sample=False, max_new_tokens=128)\n  \n\
          \  prompt_with_context = PromptTemplate(input_variables=[\"question\", \"\
          context\"], template=\"{context}\\n\\n{question}\")\n\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n\
          \  load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt_with_context)\n\
          ```"
        updatedAt: '2023-04-18T22:20:59.498Z'
      numEdits: 0
      reactions: []
    id: 643f17cbf2ed3bc5c067198c
    type: comment
  author: srowen
  content: "Hm, this works fine for me, as an example. Are you loading from some local\
    \ copy? \n\n```\n  instruct_pipeline = pipeline(model=\"databricks/dolly-v2-7b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", \n\
    \    return_full_text=True, do_sample=False, max_new_tokens=128)\n  \n  prompt_with_context\
    \ = PromptTemplate(input_variables=[\"question\", \"context\"], template=\"{context}\\\
    n\\n{question}\")\n\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n\
    \  load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt_with_context)\n\
    ```"
  created_at: 2023-04-18 21:20:59+00:00
  edited: false
  hidden: false
  id: 643f17cbf2ed3bc5c067198c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
      fullname: Chen Yao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cysony
      type: user
    createdAt: '2023-04-19T14:19:21.000Z'
    data:
      edited: false
      editors:
      - cysony
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7957fc5fe5b6ce75b9294980603e571.svg
          fullname: Chen Yao
          isHf: false
          isPro: false
          name: cysony
          type: user
        html: '<p>yes, used a local copy. tried the same code but still got the same
          error  " The following model_kwargs are not used by the model: [''return_full_text'']".</p>

          '
        raw: 'yes, used a local copy. tried the same code but still got the same error  "
          The following model_kwargs are not used by the model: [''return_full_text'']".'
        updatedAt: '2023-04-19T14:19:21.411Z'
      numEdits: 0
      reactions: []
    id: 643ff8694164a65ca125e575
    type: comment
  author: cysony
  content: 'yes, used a local copy. tried the same code but still got the same error  "
    The following model_kwargs are not used by the model: [''return_full_text'']".'
  created_at: 2023-04-19 13:19:21+00:00
  edited: false
  hidden: false
  id: 643ff8694164a65ca125e575
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T14:22:19.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>II think you have and old copy without the instruct pipeline or
          something if you get that error. Use my example where you let it download
          from HF.</p>

          '
        raw: II think you have and old copy without the instruct pipeline or something
          if you get that error. Use my example where you let it download from HF.
        updatedAt: '2023-04-19T14:22:19.849Z'
      numEdits: 0
      reactions: []
    id: 643ff91bd4229e14ae9584a5
    type: comment
  author: srowen
  content: II think you have and old copy without the instruct pipeline or something
    if you get that error. Use my example where you let it download from HF.
  created_at: 2023-04-19 13:22:19+00:00
  edited: false
  hidden: false
  id: 643ff91bd4229e14ae9584a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-21T16:54:09.000Z'
    data:
      status: closed
    id: 6442bfb1ce9c6d85e5bc7ae1
    type: status-change
  author: srowen
  created_at: 2023-04-21 15:54:09+00:00
  id: 6442bfb1ce9c6d85e5bc7ae1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/584e7e1823c7c28404c1405d7d789885.svg
      fullname: Aditya Kashyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aditya92
      type: user
    createdAt: '2023-09-07T04:15:00.000Z'
    data:
      edited: true
      editors:
      - aditya92
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7299302816390991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/584e7e1823c7c28404c1405d7d789885.svg
          fullname: Aditya Kashyap
          isHf: false
          isPro: false
          name: aditya92
          type: user
        html: '<p>I get the same error: " The following model_kwargs are not used
          by the model: [''return_full_text'']".</p>

          <p>I am on the latest hugging face and langchain packages</p>

          <p>Edit: I am also using a local copy</p>

          '
        raw: 'I get the same error: " The following model_kwargs are not used by the
          model: [''return_full_text'']".


          I am on the latest hugging face and langchain packages


          Edit: I am also using a local copy'
        updatedAt: '2023-09-07T04:15:36.436Z'
      numEdits: 1
      reactions: []
    id: 64f94e4437d3c6eb54f19e42
    type: comment
  author: aditya92
  content: 'I get the same error: " The following model_kwargs are not used by the
    model: [''return_full_text'']".


    I am on the latest hugging face and langchain packages


    Edit: I am also using a local copy'
  created_at: 2023-09-07 03:15:00+00:00
  edited: true
  hidden: false
  id: 64f94e4437d3c6eb54f19e42
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: langchain error
