!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicolaschaillan
conflicting_files: null
created_at: 2023-04-13 17:06:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-04-13T18:06:18.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Hello,</p>

          <p>We are running the code:</p>

          <p>import torch<br>from transformers import pipeline,  AutoModelForCausalLM<br>print(''got
          here'')<br>generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map="auto")<br>print(''got here2'')<br>generate_text("Who
          is Nic Chaillan?")<br>print(''got here3'')</p>

          <p>On an Azure NV48s v3 (24 GPU vcpus, 224 GiB memory)</p>

          <p>We get the error:</p>

          <p>got here<br>got here2<br>Traceback (most recent call last):<br>  File
          "/datadrive/dolly-v2-12b/test.py", line 8, in <br>    generate_text("Who
          is Nic Chaillan?")<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py",
          line 1074, in <strong>call</strong><br>    return self.run_single(inputs,
          preprocess_params, forward_params, postprocess_params)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py",
          line 1081, in run_single<br>    model_outputs = self.forward(model_inputs,
          **forward_params)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py",
          line 990, in forward<br>    model_outputs = self._forward(model_inputs,
          **forward_params)<br>  File "/home/nicos/.cache/huggingface/modules/transformers_modules/databricks/dolly-v2-12b/f8adc425f3ce69a26d57c89c1b69429a74e2ec0e/instruct_pipeline.py",
          line 103, in _forward<br>    generated_sequence = self.model.generate(<br>  File
          "/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py", line
          115, in decorate_context<br>    return func(*args, **kwargs)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py",
          line 1571, in generate<br>    return self.sample(<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py",
          line 2534, in sample<br>    outputs = self(<br>  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py",
          line 654, in forward<br>    outputs = self.gpt_neox(<br>  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py",
          line 546, in forward<br>    outputs = layer(<br>  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py",
          line 319, in forward<br>    attention_layer_outputs = self.attention(<br>  File
          "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line
          1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py",
          line 153, in forward<br>    attn_output, attn_weights = self._attn(query,
          key, value, attention_mask, head_mask)<br>  File "/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py",
          line 233, in _attn<br>    attn_output = torch.matmul(attn_weights, value)<br>RuntimeError:
          CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling <code>cublasGemmStridedBatchedExFix(handle,
          opa, opb, (int)m, (int)n, (int)k, (void*)&amp;falpha, a, CUDA_R_16BF, (int)lda,
          stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&amp;fbeta, c, CUDA_R_16BF,
          (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)</code></p>

          <p>Any clue what to do to fix this?</p>

          '
        raw: "Hello,\r\n\r\nWe are running the code:\r\n\r\nimport torch\r\nfrom transformers\
          \ import pipeline,  AutoModelForCausalLM\r\nprint('got here')\r\ngenerate_text\
          \ = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\r\nprint('got here2')\r\n\
          generate_text(\"Who is Nic Chaillan?\")\r\nprint('got here3')\r\n\r\nOn\
          \ an Azure NV48s v3 (24 GPU vcpus, 224 GiB memory)\r\n\r\nWe get the error:\r\
          \n\r\ngot here\r\ngot here2\r\nTraceback (most recent call last):\r\n  File\
          \ \"/datadrive/dolly-v2-12b/test.py\", line 8, in <module>\r\n    generate_text(\"\
          Who is Nic Chaillan?\")\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
          , line 1074, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
          , line 1081, in run_single\r\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
          , line 990, in forward\r\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/nicos/.cache/huggingface/modules/transformers_modules/databricks/dolly-v2-12b/f8adc425f3ce69a26d57c89c1b69429a74e2ec0e/instruct_pipeline.py\"\
          , line 103, in _forward\r\n    generated_sequence = self.model.generate(\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\"\
          , line 1571, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\"\
          , line 2534, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
          , line 654, in forward\r\n    outputs = self.gpt_neox(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
          , line 546, in forward\r\n    outputs = layer(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
          , line 319, in forward\r\n    attention_layer_outputs = self.attention(\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
          , line 153, in forward\r\n    attn_output, attn_weights = self._attn(query,\
          \ key, value, attention_mask, head_mask)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
          , line 233, in _attn\r\n    attn_output = torch.matmul(attn_weights, value)\r\
          \nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
          \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
          \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF,\
          \ (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\
          \n\r\n\r\nAny clue what to do to fix this?"
        updatedAt: '2023-04-13T18:06:18.522Z'
      numEdits: 0
      reactions: []
    id: 6438449ae95aa634ce76df05
    type: comment
  author: nicolaschaillan
  content: "Hello,\r\n\r\nWe are running the code:\r\n\r\nimport torch\r\nfrom transformers\
    \ import pipeline,  AutoModelForCausalLM\r\nprint('got here')\r\ngenerate_text\
    \ = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True,\
    \ device_map=\"auto\")\r\nprint('got here2')\r\ngenerate_text(\"Who is Nic Chaillan?\"\
    )\r\nprint('got here3')\r\n\r\nOn an Azure NV48s v3 (24 GPU vcpus, 224 GiB memory)\r\
    \n\r\nWe get the error:\r\n\r\ngot here\r\ngot here2\r\nTraceback (most recent\
    \ call last):\r\n  File \"/datadrive/dolly-v2-12b/test.py\", line 8, in <module>\r\
    \n    generate_text(\"Who is Nic Chaillan?\")\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
    , line 1074, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
    , line 1081, in run_single\r\n    model_outputs = self.forward(model_inputs, **forward_params)\r\
    \n  File \"/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\"\
    , line 990, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\
    \n  File \"/home/nicos/.cache/huggingface/modules/transformers_modules/databricks/dolly-v2-12b/f8adc425f3ce69a26d57c89c1b69429a74e2ec0e/instruct_pipeline.py\"\
    , line 103, in _forward\r\n    generated_sequence = self.model.generate(\r\n \
    \ File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\",\
    \ line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\"\
    , line 1571, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\"\
    , line 2534, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
    , line 654, in forward\r\n    outputs = self.gpt_neox(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
    , line 546, in forward\r\n    outputs = layer(\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
    , line 319, in forward\r\n    attention_layer_outputs = self.attention(\r\n  File\
    \ \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    /usr/local/lib/python3.9/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\
    \n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
    , line 153, in forward\r\n    attn_output, attn_weights = self._attn(query, key,\
    \ value, attention_mask, head_mask)\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\"\
    , line 233, in _attn\r\n    attn_output = torch.matmul(attn_weights, value)\r\n\
    RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
    \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
    \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc,\
    \ stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\n\r\
    \n\r\nAny clue what to do to fix this?"
  created_at: 2023-04-13 17:06:18+00:00
  edited: false
  hidden: false
  id: 6438449ae95aa634ce76df05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T18:38:34.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>This means you don''t have all the NVIDIA libraries installed. Here
          it''s complaining about CUBLAS. You can see what you have to add to a standard
          runtime in Databricks for example, here: <a rel="nofollow" href="https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L27">https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L27</a>
          That might be a clue.</p>

          '
        raw: 'This means you don''t have all the NVIDIA libraries installed. Here
          it''s complaining about CUBLAS. You can see what you have to add to a standard
          runtime in Databricks for example, here: https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L27
          That might be a clue.'
        updatedAt: '2023-04-13T18:38:34.473Z'
      numEdits: 0
      reactions: []
    id: 64384c2af79e7f4fd93b70ec
    type: comment
  author: srowen
  content: 'This means you don''t have all the NVIDIA libraries installed. Here it''s
    complaining about CUBLAS. You can see what you have to add to a standard runtime
    in Databricks for example, here: https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L27
    That might be a clue.'
  created_at: 2023-04-13 17:38:34+00:00
  edited: false
  hidden: false
  id: 64384c2af79e7f4fd93b70ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75fd099f88772990b2dee2df42d30210.svg
      fullname: Hamza Farhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HamzaFarhan
      type: user
    createdAt: '2023-04-14T20:07:57.000Z'
    data:
      edited: false
      editors:
      - HamzaFarhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75fd099f88772990b2dee2df42d30210.svg
          fullname: Hamza Farhan
          isHf: false
          isPro: false
          name: HamzaFarhan
          type: user
        html: '<p>I have the same error. Any luck on solving this?</p>

          '
        raw: I have the same error. Any luck on solving this?
        updatedAt: '2023-04-14T20:07:57.919Z'
      numEdits: 0
      reactions: []
    id: 6439b29d68228e8b33451fac
    type: comment
  author: HamzaFarhan
  content: I have the same error. Any luck on solving this?
  created_at: 2023-04-14 19:07:57+00:00
  edited: false
  hidden: false
  id: 6439b29d68228e8b33451fac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T20:09:44.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I think this can also arise as an "out of memory" error. Please,
          it''s more helpful if people say how they are running this, and whether
          you''ve ruled out what is in previous comments!</p>

          '
        raw: I think this can also arise as an "out of memory" error. Please, it's
          more helpful if people say how they are running this, and whether you've
          ruled out what is in previous comments!
        updatedAt: '2023-04-14T20:09:44.792Z'
      numEdits: 0
      reactions: []
    id: 6439b308cc228b8099b488bb
    type: comment
  author: srowen
  content: I think this can also arise as an "out of memory" error. Please, it's more
    helpful if people say how they are running this, and whether you've ruled out
    what is in previous comments!
  created_at: 2023-04-14 19:09:44+00:00
  edited: false
  hidden: false
  id: 6439b308cc228b8099b488bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75fd099f88772990b2dee2df42d30210.svg
      fullname: Hamza Farhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HamzaFarhan
      type: user
    createdAt: '2023-04-14T20:20:30.000Z'
    data:
      edited: false
      editors:
      - HamzaFarhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75fd099f88772990b2dee2df42d30210.svg
          fullname: Hamza Farhan
          isHf: false
          isPro: false
          name: HamzaFarhan
          type: user
        html: '<p>My Code:</p>

          <p>from transformers import pipeline<br>generate_text = pipeline(model="databricks/dolly-v2-3b",
          torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=''auto'')<br>edu_prompt
          = "Extract the universities from the following text: My name is Hamza and
          I have a bachelor''s degree from the university of toronto and a master''s
          degree from the university of waterloo."<br>edu = generate_text(edu_prompt)</p>

          <p>12 GB GPU<br>torch 1.13.1 with cuda 11.7</p>

          <p>I don''t think a 6GB model should give me an "out of memory" error.</p>

          '
        raw: 'My Code:


          from transformers import pipeline

          generate_text = pipeline(model="databricks/dolly-v2-3b", torch_dtype=torch.bfloat16,
          trust_remote_code=True, device_map=''auto'')

          edu_prompt = "Extract the universities from the following text: My name
          is Hamza and I have a bachelor''s degree from the university of toronto
          and a master''s degree from the university of waterloo."

          edu = generate_text(edu_prompt)


          12 GB GPU

          torch 1.13.1 with cuda 11.7


          I don''t think a 6GB model should give me an "out of memory" error.'
        updatedAt: '2023-04-14T20:20:30.793Z'
      numEdits: 0
      reactions: []
    id: 6439b58e1eddadc9404ce35f
    type: comment
  author: HamzaFarhan
  content: 'My Code:


    from transformers import pipeline

    generate_text = pipeline(model="databricks/dolly-v2-3b", torch_dtype=torch.bfloat16,
    trust_remote_code=True, device_map=''auto'')

    edu_prompt = "Extract the universities from the following text: My name is Hamza
    and I have a bachelor''s degree from the university of toronto and a master''s
    degree from the university of waterloo."

    edu = generate_text(edu_prompt)


    12 GB GPU

    torch 1.13.1 with cuda 11.7


    I don''t think a 6GB model should give me an "out of memory" error.'
  created_at: 2023-04-14 19:20:30+00:00
  edited: false
  hidden: false
  id: 6439b58e1eddadc9404ce35f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T20:24:10.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah that''s not it, but do you have cublas installed? See above</p>

          '
        raw: Yeah that's not it, but do you have cublas installed? See above
        updatedAt: '2023-04-14T20:24:10.958Z'
      numEdits: 0
      reactions: []
    id: 6439b66acc228b8099b49f93
    type: comment
  author: srowen
  content: Yeah that's not it, but do you have cublas installed? See above
  created_at: 2023-04-14 19:24:10+00:00
  edited: false
  hidden: false
  id: 6439b66acc228b8099b49f93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
      fullname: Tomas Olsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tool10
      type: user
    createdAt: '2023-04-19T13:53:48.000Z'
    data:
      edited: false
      editors:
      - Tool10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
          fullname: Tomas Olsson
          isHf: false
          isPro: false
          name: Tool10
          type: user
        html: '<p>Hi.<br>I have the same problem on an Ubuntu 20.04 server with plenty
          of memory. Have you had any success fixing this error?<br>/Tomas</p>

          '
        raw: 'Hi.

          I have the same problem on an Ubuntu 20.04 server with plenty of memory.
          Have you had any success fixing this error?

          /Tomas'
        updatedAt: '2023-04-19T13:53:48.555Z'
      numEdits: 0
      reactions: []
    id: 643ff26cd4229e14ae94e21e
    type: comment
  author: Tool10
  content: 'Hi.

    I have the same problem on an Ubuntu 20.04 server with plenty of memory. Have
    you had any success fixing this error?

    /Tomas'
  created_at: 2023-04-19 12:53:48+00:00
  edited: false
  hidden: false
  id: 643ff26cd4229e14ae94e21e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T13:54:54.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Do you have the right cublas installed? What lib version vs what
          CUDA?</p>

          '
        raw: Do you have the right cublas installed? What lib version vs what CUDA?
        updatedAt: '2023-04-19T13:54:54.063Z'
      numEdits: 0
      reactions: []
    id: 643ff2ae4164a65ca1256007
    type: comment
  author: srowen
  content: Do you have the right cublas installed? What lib version vs what CUDA?
  created_at: 2023-04-19 12:54:54+00:00
  edited: false
  hidden: false
  id: 643ff2ae4164a65ca1256007
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
      fullname: Tomas Olsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tool10
      type: user
    createdAt: '2023-04-19T14:01:30.000Z'
    data:
      edited: false
      editors:
      - Tool10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
          fullname: Tomas Olsson
          isHf: false
          isPro: false
          name: Tool10
          type: user
        html: '<blockquote>

          <p>Do you have the right cublas installed? What lib version vs what CUDA?</p>

          </blockquote>

          <p>Which version should I have? I have cuda 11.7.</p>

          '
        raw: '> Do you have the right cublas installed? What lib version vs what CUDA?


          Which version should I have? I have cuda 11.7.'
        updatedAt: '2023-04-19T14:01:30.262Z'
      numEdits: 0
      reactions: []
    id: 643ff43a4164a65ca12582ef
    type: comment
  author: Tool10
  content: '> Do you have the right cublas installed? What lib version vs what CUDA?


    Which version should I have? I have cuda 11.7.'
  created_at: 2023-04-19 13:01:30+00:00
  edited: false
  hidden: false
  id: 643ff43a4164a65ca12582ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T14:12:54.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>This is all covered in the provided training scripts.<br><a rel="nofollow"
          href="https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L53">https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L53</a></p>

          '
        raw: 'This is all covered in the provided training scripts.

          https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L53'
        updatedAt: '2023-04-19T14:12:54.175Z'
      numEdits: 0
      reactions: []
    id: 643ff6e6dbd88206a835ea62
    type: comment
  author: srowen
  content: 'This is all covered in the provided training scripts.

    https://github.com/databrickslabs/dolly/blob/master/train_dolly.py#L53'
  created_at: 2023-04-19 13:12:54+00:00
  edited: false
  hidden: false
  id: 643ff6e6dbd88206a835ea62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-21T15:21:30.000Z'
    data:
      status: closed
    id: 6442a9faf8b647fa4f5165d1
    type: status-change
  author: srowen
  created_at: 2023-04-21 14:21:30+00:00
  id: 6442a9faf8b647fa4f5165d1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
      fullname: Tomas Olsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tool10
      type: user
    createdAt: '2023-04-24T10:44:04.000Z'
    data:
      edited: false
      editors:
      - Tool10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
          fullname: Tomas Olsson
          isHf: false
          isPro: false
          name: Tool10
          type: user
        html: '<p>Sorry, since I am new user I could not reply anymore last week.
          This problem is not solved. I have created a Dockerfile with the correct
          cublas version, but it does not work as follows (it ends with the same error):<br>------
          Dockerfile ------<br>FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel<br>WORKDIR
          /app/dolly</p>

          <p>RUN apt-get upgrade<br>ADD <a rel="nofollow" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb</a>
          /tmp<br>RUN dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb<br>ADD
          <a rel="nofollow" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-11-3_11.5.1.109-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-11-3_11.5.1.109-1_amd64.deb</a>
          /tmp<br>ADD <a rel="nofollow" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb</a>
          /tmp<br>RUN dpkg -i /tmp/libcublas-11-3_11.5.1.109-1_amd64.deb<br>RUN dpkg
          -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb<br>ADD  <a rel="nofollow"
          href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb</a>
          /tmp<br>RUN dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb<br>ADD
          <a rel="nofollow" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb</a>
          /tmp<br>RUN dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb<br>RUN
          pip install accelerate&gt;=0.12.0 transformers[torch]==4.25.1<br>RUN pip
          install ipython<br>ADD <a href="https://huggingface.co/databricks/dolly-v2-3b/raw/main/instruct_pipeline.py">https://huggingface.co/databricks/dolly-v2-3b/raw/main/instruct_pipeline.py</a>
          .<br>COPY ./init_dolly.py .</p>

          <p>CMD  DISABLE_ADDMM_CUDA_LT=1 ipython -i init_dolly.py</p>

          <p>------ init_dolly.py ------<br>import torch<br>from instruct_pipeline
          import InstructionTextGenerationPipeline<br>from transformers import AutoModelForCausalLM,
          AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b",
          padding_side="left")<br>model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          device_map="auto", torch_dtype=torch.bfloat16)</p>

          <p>generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)</p>

          '
        raw: 'Sorry, since I am new user I could not reply anymore last week. This
          problem is not solved. I have created a Dockerfile with the correct cublas
          version, but it does not work as follows (it ends with the same error):

          ------ Dockerfile ------

          FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel

          WORKDIR /app/dolly


          RUN apt-get upgrade

          ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb
          /tmp

          RUN dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb

          ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-11-3_11.5.1.109-1_amd64.deb
          /tmp

          ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb
          /tmp

          RUN dpkg -i /tmp/libcublas-11-3_11.5.1.109-1_amd64.deb

          RUN dpkg -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb

          ADD  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb
          /tmp

          RUN dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb

          ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb
          /tmp

          RUN dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb

          RUN pip install accelerate>=0.12.0 transformers[torch]==4.25.1

          RUN pip install ipython

          ADD https://huggingface.co/databricks/dolly-v2-3b/raw/main/instruct_pipeline.py
          .

          COPY ./init_dolly.py .


          CMD  DISABLE_ADDMM_CUDA_LT=1 ipython -i init_dolly.py


          ------ init_dolly.py ------

          import torch

          from instruct_pipeline import InstructionTextGenerationPipeline

          from transformers import AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b", padding_side="left")

          model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          device_map="auto", torch_dtype=torch.bfloat16)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)'
        updatedAt: '2023-04-24T10:44:04.607Z'
      numEdits: 0
      reactions: []
    id: 64465d742f3d84a7a886e4ea
    type: comment
  author: Tool10
  content: 'Sorry, since I am new user I could not reply anymore last week. This problem
    is not solved. I have created a Dockerfile with the correct cublas version, but
    it does not work as follows (it ends with the same error):

    ------ Dockerfile ------

    FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel

    WORKDIR /app/dolly


    RUN apt-get upgrade

    ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb
    /tmp

    RUN dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb

    ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-11-3_11.5.1.109-1_amd64.deb
    /tmp

    ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb
    /tmp

    RUN dpkg -i /tmp/libcublas-11-3_11.5.1.109-1_amd64.deb

    RUN dpkg -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb

    ADD  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb
    /tmp

    RUN dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb

    ADD https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb
    /tmp

    RUN dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb

    RUN pip install accelerate>=0.12.0 transformers[torch]==4.25.1

    RUN pip install ipython

    ADD https://huggingface.co/databricks/dolly-v2-3b/raw/main/instruct_pipeline.py
    .

    COPY ./init_dolly.py .


    CMD  DISABLE_ADDMM_CUDA_LT=1 ipython -i init_dolly.py


    ------ init_dolly.py ------

    import torch

    from instruct_pipeline import InstructionTextGenerationPipeline

    from transformers import AutoModelForCausalLM, AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b", padding_side="left")

    model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b", device_map="auto",
    torch_dtype=torch.bfloat16)


    generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)'
  created_at: 2023-04-24 09:44:04+00:00
  edited: false
  hidden: false
  id: 64465d742f3d84a7a886e4ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-24T12:35:00.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>What hardware? this would only run on an A100 as you''ve written
          it.</p>

          '
        raw: What hardware? this would only run on an A100 as you've written it.
        updatedAt: '2023-04-24T12:35:00.255Z'
      numEdits: 0
      reactions: []
    id: 644677741173e85ac7f0fdb1
    type: comment
  author: srowen
  content: What hardware? this would only run on an A100 as you've written it.
  created_at: 2023-04-24 11:35:00+00:00
  edited: false
  hidden: false
  id: 644677741173e85ac7f0fdb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
      fullname: Tomas Olsson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tool10
      type: user
    createdAt: '2023-04-24T12:45:35.000Z'
    data:
      edited: false
      editors:
      - Tool10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9edea99a574395d30d1c13406d023c.svg
          fullname: Tomas Olsson
          isHf: false
          isPro: false
          name: Tool10
          type: user
        html: '<blockquote>

          <p>What hardware? this would only run on an A100 as you''ve written it.</p>

          </blockquote>

          <p>OK, then that is why it doesn''t work. How do I change the used hardware?</p>

          '
        raw: '> What hardware? this would only run on an A100 as you''ve written it.


          OK, then that is why it doesn''t work. How do I change the used hardware?'
        updatedAt: '2023-04-24T12:45:35.443Z'
      numEdits: 0
      reactions: []
    id: 644679ef615bd6eeafa90fe5
    type: comment
  author: Tool10
  content: '> What hardware? this would only run on an A100 as you''ve written it.


    OK, then that is why it doesn''t work. How do I change the used hardware?'
  created_at: 2023-04-24 11:45:35+00:00
  edited: false
  hidden: false
  id: 644679ef615bd6eeafa90fe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-24T12:53:26.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>I suspect OOM or something, but what error are you getting? maybe
          this should be a separate thread with more info.<br>You control the hardware
          by, well, choosing where you run it?</p>

          '
        raw: 'I suspect OOM or something, but what error are you getting? maybe this
          should be a separate thread with more info.

          You control the hardware by, well, choosing where you run it?'
        updatedAt: '2023-04-24T12:53:26.784Z'
      numEdits: 0
      reactions: []
    id: 64467bc6a808163cefffb61f
    type: comment
  author: srowen
  content: 'I suspect OOM or something, but what error are you getting? maybe this
    should be a separate thread with more info.

    You control the hardware by, well, choosing where you run it?'
  created_at: 2023-04-24 11:53:26+00:00
  edited: false
  hidden: false
  id: 64467bc6a808163cefffb61f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: 'Error: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED '
