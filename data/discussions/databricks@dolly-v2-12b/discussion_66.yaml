!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Automated8756
conflicting_files: null
created_at: 2023-05-07 22:21:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e85a1a632b0af7186af1da66d20d01a.svg
      fullname: Bryan Tor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Automated8756
      type: user
    createdAt: '2023-05-07T23:21:42.000Z'
    data:
      edited: false
      editors:
      - Automated8756
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e85a1a632b0af7186af1da66d20d01a.svg
          fullname: Bryan Tor
          isHf: false
          isPro: false
          name: Automated8756
          type: user
        html: '<p>I have been able to run Dolly v2 on my computer using only CPU with
          16GB of RAM using the <a rel="nofollow" href="https://github.com/ggerganov/ggml">ggml
          model format</a>. Someone has put together <a rel="nofollow" href="https://github.com/ggerganov/ggml/tree/master/examples/dolly-v2">an
          example</a> of converting the original Dolly model file to the ggml file
          format.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/640a5fc39989bcb117282606/he4jxXdUvc_QqvFs7aNQn.png"><img
          alt="Screenshot from 2023-05-07 16-17-32.png" src="https://cdn-uploads.huggingface.co/production/uploads/640a5fc39989bcb117282606/he4jxXdUvc_QqvFs7aNQn.png"></a></p>

          <p>The original example is missing the Context portion of the prompt, but
          I don''t think that will be very difficult to add in.</p>

          <p>If anyone is looking to run Dolly v2 on their CPU, this has been a viable
          solution for me. You may still encounter OOM errors running the conversion
          script on anything with less than 32GB ram however, but after finishing
          the conversion it has been smooth sailing. Is it possible that Databricks
          could provide an "official" copy of the ggml Dolly model?</p>

          '
        raw: "I have been able to run Dolly v2 on my computer using only CPU with\
          \ 16GB of RAM using the [ggml model format](https://github.com/ggerganov/ggml).\
          \ Someone has put together [an example](https://github.com/ggerganov/ggml/tree/master/examples/dolly-v2)\
          \ of converting the original Dolly model file to the ggml file format.\r\
          \n\r\n![Screenshot from 2023-05-07 16-17-32.png](https://cdn-uploads.huggingface.co/production/uploads/640a5fc39989bcb117282606/he4jxXdUvc_QqvFs7aNQn.png)\r\
          \n\r\nThe original example is missing the Context portion of the prompt,\
          \ but I don't think that will be very difficult to add in.\r\n\r\nIf anyone\
          \ is looking to run Dolly v2 on their CPU, this has been a viable solution\
          \ for me. You may still encounter OOM errors running the conversion script\
          \ on anything with less than 32GB ram however, but after finishing the conversion\
          \ it has been smooth sailing. Is it possible that Databricks could provide\
          \ an \"official\" copy of the ggml Dolly model?"
        updatedAt: '2023-05-07T23:21:42.234Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - qutf
      - count: 1
        reaction: "\U0001F44D"
        users:
        - qutf
    id: 64583286fa1e90a6b88c99e2
    type: comment
  author: Automated8756
  content: "I have been able to run Dolly v2 on my computer using only CPU with 16GB\
    \ of RAM using the [ggml model format](https://github.com/ggerganov/ggml). Someone\
    \ has put together [an example](https://github.com/ggerganov/ggml/tree/master/examples/dolly-v2)\
    \ of converting the original Dolly model file to the ggml file format.\r\n\r\n\
    ![Screenshot from 2023-05-07 16-17-32.png](https://cdn-uploads.huggingface.co/production/uploads/640a5fc39989bcb117282606/he4jxXdUvc_QqvFs7aNQn.png)\r\
    \n\r\nThe original example is missing the Context portion of the prompt, but I\
    \ don't think that will be very difficult to add in.\r\n\r\nIf anyone is looking\
    \ to run Dolly v2 on their CPU, this has been a viable solution for me. You may\
    \ still encounter OOM errors running the conversion script on anything with less\
    \ than 32GB ram however, but after finishing the conversion it has been smooth\
    \ sailing. Is it possible that Databricks could provide an \"official\" copy of\
    \ the ggml Dolly model?"
  created_at: 2023-05-07 22:21:42+00:00
  edited: false
  hidden: false
  id: 64583286fa1e90a6b88c99e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-07T23:27:45.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Oh nice, how fast does it run? you can already run it as-is on CPUs,
          but is this faster or more memory efficient?<br>There are so many possible
          variations to maintain (more sizes, quantized, safetensors, ONNX?, this,
          etc) that I don''t think we''d do it, but others can.</p>

          '
        raw: 'Oh nice, how fast does it run? you can already run it as-is on CPUs,
          but is this faster or more memory efficient?

          There are so many possible variations to maintain (more sizes, quantized,
          safetensors, ONNX?, this, etc) that I don''t think we''d do it, but others
          can.'
        updatedAt: '2023-05-07T23:27:45.594Z'
      numEdits: 0
      reactions: []
    id: 645833f1c9af80de21774a55
    type: comment
  author: srowen
  content: 'Oh nice, how fast does it run? you can already run it as-is on CPUs, but
    is this faster or more memory efficient?

    There are so many possible variations to maintain (more sizes, quantized, safetensors,
    ONNX?, this, etc) that I don''t think we''d do it, but others can.'
  created_at: 2023-05-07 22:27:45+00:00
  edited: false
  hidden: false
  id: 645833f1c9af80de21774a55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e85a1a632b0af7186af1da66d20d01a.svg
      fullname: Bryan Tor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Automated8756
      type: user
    createdAt: '2023-05-09T18:36:57.000Z'
    data:
      edited: false
      editors:
      - Automated8756
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e85a1a632b0af7186af1da66d20d01a.svg
          fullname: Bryan Tor
          isHf: false
          isPro: false
          name: Automated8756
          type: user
        html: '<p>The GGML model binaries I''m using are a quantized version of Dolly
          V2 to FP16 and to FP5 and implemented in C. The quantized models of course
          are more memory efficient and my experience is that running these in C is
          faster. I don''t have access to my machine this week, but I promise to update
          my discussion post here with more metrics soon (TM).</p>

          <p>The reasoning not to support every single variation of Dolly makes sense,
          and I''m not sure why I didn''t reason that out first haha. I''ll ask the
          GGML maintainers if they can add the files to their own <a href="https://huggingface.co/ggerganov/ggml">HF
          repository</a>.</p>

          '
        raw: 'The GGML model binaries I''m using are a quantized version of Dolly
          V2 to FP16 and to FP5 and implemented in C. The quantized models of course
          are more memory efficient and my experience is that running these in C is
          faster. I don''t have access to my machine this week, but I promise to update
          my discussion post here with more metrics soon (TM).


          The reasoning not to support every single variation of Dolly makes sense,
          and I''m not sure why I didn''t reason that out first haha. I''ll ask the
          GGML maintainers if they can add the files to their own [HF repository](https://huggingface.co/ggerganov/ggml).'
        updatedAt: '2023-05-09T18:36:57.814Z'
      numEdits: 0
      reactions: []
    id: 645a92c9dbf60d3733606607
    type: comment
  author: Automated8756
  content: 'The GGML model binaries I''m using are a quantized version of Dolly V2
    to FP16 and to FP5 and implemented in C. The quantized models of course are more
    memory efficient and my experience is that running these in C is faster. I don''t
    have access to my machine this week, but I promise to update my discussion post
    here with more metrics soon (TM).


    The reasoning not to support every single variation of Dolly makes sense, and
    I''m not sure why I didn''t reason that out first haha. I''ll ask the GGML maintainers
    if they can add the files to their own [HF repository](https://huggingface.co/ggerganov/ggml).'
  created_at: 2023-05-09 17:36:57+00:00
  edited: false
  hidden: false
  id: 645a92c9dbf60d3733606607
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-09T18:53:28.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>This could be an important variation if it''s very plausibly performant
          on a CPU, esp if it works on Macs, where the GPU is hard to get working.<br>Not
          out of the question for sure, I''m not the Decider either.<br>It could start
          as a separate model here.</p>

          '
        raw: 'This could be an important variation if it''s very plausibly performant
          on a CPU, esp if it works on Macs, where the GPU is hard to get working.

          Not out of the question for sure, I''m not the Decider either.

          It could start as a separate model here.'
        updatedAt: '2023-05-09T18:53:28.109Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Automated8756
    id: 645a96a8e505443f81955b55
    type: comment
  author: srowen
  content: 'This could be an important variation if it''s very plausibly performant
    on a CPU, esp if it works on Macs, where the GPU is hard to get working.

    Not out of the question for sure, I''m not the Decider either.

    It could start as a separate model here.'
  created_at: 2023-05-09 17:53:28+00:00
  edited: false
  hidden: false
  id: 645a96a8e505443f81955b55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-29T12:16:14.000Z'
    data:
      status: closed
    id: 6474978ef9e3e0b312eb7a26
    type: status-change
  author: srowen
  created_at: 2023-05-29 11:16:14+00:00
  id: 6474978ef9e3e0b312eb7a26
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Inference with Dolly v2 on CPU using GGML
