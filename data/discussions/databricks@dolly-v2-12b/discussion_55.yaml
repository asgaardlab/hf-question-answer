!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dy123
conflicting_files: null
created_at: 2023-04-25 15:02:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98c58b04cae2570f91864ffe0e7d8a97.svg
      fullname: Da-Yi Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dy123
      type: user
    createdAt: '2023-04-25T16:02:04.000Z'
    data:
      edited: true
      editors:
      - dy123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98c58b04cae2570f91864ffe0e7d8a97.svg
          fullname: Da-Yi Wu
          isHf: false
          isPro: false
          name: dy123
          type: user
        html: '<p>As title, When I run the following code, </p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> instruct_pipeline <span class="hljs-keyword">import</span>
          InstructionTextGenerationPipeline

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"databricks/dolly-v2-12b"</span>,
          padding_side=<span class="hljs-string">"left"</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"databricks/dolly-v2-12b"</span>,
          offload_folder=<span class="hljs-string">"offload"</span>, device_map=<span
          class="hljs-string">"auto"</span>, torch_dtype=torch.float16)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)


          res = generate_text(<span class="hljs-string">"Explain to me the difference
          between nuclear fission and fusion."</span>)

          <span class="hljs-built_in">print</span>(res[<span class="hljs-number">0</span>][<span
          class="hljs-string">"generated_text"</span>])

          </code></pre>

          <p>it come out an error: "cumsum_out_cpu" not implemented for ''BFloat16''.<br>My
          platform is Windows 11 and my torch version is ''1.10.1+cpu''</p>

          '
        raw: "As title, When I run the following code, \n```python\nimport torch\n\
          from instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          databricks/dolly-v2-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          databricks/dolly-v2-12b\", offload_folder=\"offload\", device_map=\"auto\"\
          , torch_dtype=torch.float16)\n\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\n\nres = generate_text(\"Explain to me the difference\
          \ between nuclear fission and fusion.\")\nprint(res[0][\"generated_text\"\
          ])\n```\n\nit come out an error: \"cumsum_out_cpu\" not implemented for\
          \ 'BFloat16'.\nMy platform is Windows 11 and my torch version is '1.10.1+cpu'"
        updatedAt: '2023-04-25T16:03:56.027Z'
      numEdits: 2
      reactions: []
    id: 6447f97c1d52a633c8f1d143
    type: comment
  author: dy123
  content: "As title, When I run the following code, \n```python\nimport torch\nfrom\
    \ instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    databricks/dolly-v2-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    databricks/dolly-v2-12b\", offload_folder=\"offload\", device_map=\"auto\", torch_dtype=torch.float16)\n\
    \ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\
    \nres = generate_text(\"Explain to me the difference between nuclear fission and\
    \ fusion.\")\nprint(res[0][\"generated_text\"])\n```\n\nit come out an error:\
    \ \"cumsum_out_cpu\" not implemented for 'BFloat16'.\nMy platform is Windows 11\
    \ and my torch version is '1.10.1+cpu'"
  created_at: 2023-04-25 15:02:04+00:00
  edited: true
  hidden: false
  id: 6447f97c1d52a633c8f1d143
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-25T16:30:31.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Use torch 1.13, that''s a good guess at what''s wrong here. See
          the latest snippet and requirements.txt in the repo, which outlines the
          recommended versions of things.</p>

          '
        raw: Use torch 1.13, that's a good guess at what's wrong here. See the latest
          snippet and requirements.txt in the repo, which outlines the recommended
          versions of things.
        updatedAt: '2023-04-25T16:30:31.731Z'
      numEdits: 0
      reactions: []
    id: 644800273411a0902bbc7f49
    type: comment
  author: srowen
  content: Use torch 1.13, that's a good guess at what's wrong here. See the latest
    snippet and requirements.txt in the repo, which outlines the recommended versions
    of things.
  created_at: 2023-04-25 15:30:31+00:00
  edited: false
  hidden: false
  id: 644800273411a0902bbc7f49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-28T13:59:50.000Z'
    data:
      status: closed
    id: 644bd156194e124dacb1da1b
    type: status-change
  author: srowen
  created_at: 2023-04-28 12:59:50+00:00
  id: 644bd156194e124dacb1da1b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 55
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: '"cumsum_out_cpu" not implemented for ''BFloat16'''
