!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gkrishnan
conflicting_files: null
created_at: 2023-08-05 21:07:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6b7244132df8df07df0ec954e03b8f.svg
      fullname: Ganesh Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gkrishnan
      type: user
    createdAt: '2023-08-05T22:07:11.000Z'
    data:
      edited: false
      editors:
      - gkrishnan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5513067841529846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6b7244132df8df07df0ec954e03b8f.svg
          fullname: Ganesh Krishnan
          isHf: false
          isPro: false
          name: gkrishnan
          type: user
        html: '<p>So here is my other question. If you were to work from OpenAI to
          LangChain. We take a given document and put it in a vector database and
          then you persist your Chroma vector store like so:</p>

          <p>from langchain.embeddings.openai import OpenAIEmbeddings<br>embeddings
          = OpenAIEmbeddings()</p>

          <p>from langchain.vectorstores import Chroma<br>persist_directory = "vector_db"<br>vectordb
          = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)</p>

          <p>vectordb.persist()<br>vectordb = None<br>vectordb = Chroma(persist_directory=persist_directory,
          embedding_function=embeddings)</p>

          <p>from langchain.chat_models import ChatOpenAI<br>llm = ChatOpenAI(temperature=0)<br>doc_retriever
          = vectordb.as_retriever()</p>

          <p>from langchain.chains import RetrievalQA<br>resume_qa = RetrievalQA.from_chain_type(llm=llm,
          chain_type="stuff", retriever=doc_retriever)</p>

          <p>As you can see here the "vectordb" retriever is stored in "doc_retriever".
          This is very simple. At this point you can run any query based off the documentation
          you uploaded. Now I am trying to do this with Dolly. </p>

          <p>Now here is my code:<br>loader = PyPDFLoader("/content/drive/MyDrive/Data
          Science/Capstone/Resume_guide.pdf")<br>pages = loader.load_and_split()</p>

          <p>import torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("databricks/dolly-v2-7b")</p>

          <p>from langchain.text_splitter import RecursiveCharacterTextSplitter</p>

          <p>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size = 1000,<br>    chunk_overlap  =
          20,<br>    length_function = len,<br>)</p>

          <p>documents = text_splitter.split_documents(pages)<br>from langchain.embeddings
          import HuggingFaceEmbeddings<br>embeddings = HuggingFaceEmbeddings()</p>

          <p>from langchain.vectorstores import Chroma</p>

          <p>persist_directory = "vector_db"<br>vectordb = Chroma.from_documents(documents=documents,
          embedding=embeddings, persist_directory=persist_directory)</p>

          <p>vectordb.persist()<br>vectordb = None</p>

          <p>vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)</p>

          <p>doc_retriever = vectordb.as_retriever()</p>

          <p>#Dolly Stuff<br>import torch<br>import os<br>os.environ["CUDA_VISIBLE_DEVICES"]="0"<br>from
          transformers import pipeline</p>

          <p>generate_text = pipeline(model="databricks/dolly-v2-7b", torch_dtype=torch.bfloat16,<br>                         trust_remote_code=True,
          device_map="auto", return_full_text=True)</p>

          <p>#Dolly Stuff<br>from langchain import PromptTemplate, LLMChain<br>from
          langchain.llms import HuggingFacePipeline</p>

          <p>#template for an instruction with no input<br>prompt = PromptTemplate(<br>    input_variables=["instruction"],<br>    template="{instruction}")</p>

          <p>#template for an instruction with input<br>prompt_with_context = PromptTemplate(<br>    input_variables=["instruction",
          "context"],<br>    template="{instruction}\n\nInput:\n{context}")</p>

          <p>hf_pipeline = HuggingFacePipeline(pipeline=generate_text)</p>

          <p>llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)<br>llm_context_chain
          = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)</p>

          <p>What I don''t understand is how will "doc_retriever"  be used in the
          llm_context_chain or llm_chain variables? </p>

          <p>I noticed no where in my code, I am not calling "doc_retriever". How
          would I do that? Is there any documentation for uploading your own documentation
          in Dolly while using HuggingFace? Any help would be greatly appreciated!</p>

          '
        raw: "So here is my other question. If you were to work from OpenAI to LangChain.\
          \ We take a given document and put it in a vector database and then you\
          \ persist your Chroma vector store like so:\r\n\r\nfrom langchain.embeddings.openai\
          \ import OpenAIEmbeddings\r\nembeddings = OpenAIEmbeddings()\r\n\r\nfrom\
          \ langchain.vectorstores import Chroma\r\npersist_directory = \"vector_db\"\
          \r\nvectordb = Chroma.from_documents(documents=documents, embedding=embeddings,\
          \ persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\nvectordb\
          \ = None\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
          \n\r\nfrom langchain.chat_models import ChatOpenAI\r\nllm = ChatOpenAI(temperature=0)\r\
          \ndoc_retriever = vectordb.as_retriever()\r\n\r\nfrom langchain.chains import\
          \ RetrievalQA\r\nresume_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"\
          stuff\", retriever=doc_retriever)\r\n\r\nAs you can see here the \"vectordb\"\
          \ retriever is stored in \"doc_retriever\". This is very simple. At this\
          \ point you can run any query based off the documentation you uploaded.\
          \ Now I am trying to do this with Dolly. \r\n\r\n\r\n\r\nNow here is my\
          \ code:\r\nloader = PyPDFLoader(\"/content/drive/MyDrive/Data Science/Capstone/Resume_guide.pdf\"\
          )\r\npages = loader.load_and_split()\r\n\r\nimport torch\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          databricks/dolly-v2-7b\")\r\n\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\
          \n\r\ntext_splitter = RecursiveCharacterTextSplitter(\r\n    chunk_size\
          \ = 1000,\r\n    chunk_overlap  = 20,\r\n    length_function = len,\r\n\
          )\r\n\r\ndocuments = text_splitter.split_documents(pages)\r\nfrom langchain.embeddings\
          \ import HuggingFaceEmbeddings\r\nembeddings = HuggingFaceEmbeddings()\r\
          \n\r\nfrom langchain.vectorstores import Chroma\r\n\r\npersist_directory\
          \ = \"vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents,\
          \ embedding=embeddings, persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\
          \nvectordb = None\r\n\r\nvectordb = Chroma(persist_directory=persist_directory,\
          \ embedding_function=embeddings)\r\n\r\ndoc_retriever = vectordb.as_retriever()\r\
          \n\r\n#Dolly Stuff\r\nimport torch\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ]=\"0\"\r\nfrom transformers import pipeline\r\n\r\ngenerate_text = pipeline(model=\"\
          databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\r\n              \
          \           trust_remote_code=True, device_map=\"auto\", return_full_text=True)\r\
          \n\r\n#Dolly Stuff\r\nfrom langchain import PromptTemplate, LLMChain\r\n\
          from langchain.llms import HuggingFacePipeline\r\n\r\n#template for an instruction\
          \ with no input\r\nprompt = PromptTemplate(\r\n    input_variables=[\"instruction\"\
          ],\r\n    template=\"{instruction}\")\r\n\r\n#template for an instruction\
          \ with input\r\nprompt_with_context = PromptTemplate(\r\n    input_variables=[\"\
          instruction\", \"context\"],\r\n    template=\"{instruction}\\n\\nInput:\\\
          n{context}\")\r\n\r\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\r\
          \n\r\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\r\nllm_context_chain\
          \ = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\r\n\r\n\r\nWhat\
          \ I don't understand is how will \"doc_retriever\"  be used in the llm_context_chain\
          \ or llm_chain variables? \r\n\r\nI noticed no where in my code, I am not\
          \ calling \"doc_retriever\". How would I do that? Is there any documentation\
          \ for uploading your own documentation in Dolly while using HuggingFace?\
          \ Any help would be greatly appreciated!\r\n\r\n"
        updatedAt: '2023-08-05T22:07:11.487Z'
      numEdits: 0
      reactions: []
    id: 64cec80fd8d09273729ab966
    type: comment
  author: gkrishnan
  content: "So here is my other question. If you were to work from OpenAI to LangChain.\
    \ We take a given document and put it in a vector database and then you persist\
    \ your Chroma vector store like so:\r\n\r\nfrom langchain.embeddings.openai import\
    \ OpenAIEmbeddings\r\nembeddings = OpenAIEmbeddings()\r\n\r\nfrom langchain.vectorstores\
    \ import Chroma\r\npersist_directory = \"vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents,\
    \ embedding=embeddings, persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\
    \nvectordb = None\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
    \n\r\nfrom langchain.chat_models import ChatOpenAI\r\nllm = ChatOpenAI(temperature=0)\r\
    \ndoc_retriever = vectordb.as_retriever()\r\n\r\nfrom langchain.chains import\
    \ RetrievalQA\r\nresume_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"\
    stuff\", retriever=doc_retriever)\r\n\r\nAs you can see here the \"vectordb\"\
    \ retriever is stored in \"doc_retriever\". This is very simple. At this point\
    \ you can run any query based off the documentation you uploaded. Now I am trying\
    \ to do this with Dolly. \r\n\r\n\r\n\r\nNow here is my code:\r\nloader = PyPDFLoader(\"\
    /content/drive/MyDrive/Data Science/Capstone/Resume_guide.pdf\")\r\npages = loader.load_and_split()\r\
    \n\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\r\n\r\n\
    from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n\r\ntext_splitter\
    \ = RecursiveCharacterTextSplitter(\r\n    chunk_size = 1000,\r\n    chunk_overlap\
    \  = 20,\r\n    length_function = len,\r\n)\r\n\r\ndocuments = text_splitter.split_documents(pages)\r\
    \nfrom langchain.embeddings import HuggingFaceEmbeddings\r\nembeddings = HuggingFaceEmbeddings()\r\
    \n\r\nfrom langchain.vectorstores import Chroma\r\n\r\npersist_directory = \"\
    vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents, embedding=embeddings,\
    \ persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\nvectordb =\
    \ None\r\n\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
    \n\r\ndoc_retriever = vectordb.as_retriever()\r\n\r\n#Dolly Stuff\r\nimport torch\r\
    \nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\nfrom transformers\
    \ import pipeline\r\n\r\ngenerate_text = pipeline(model=\"databricks/dolly-v2-7b\"\
    , torch_dtype=torch.bfloat16,\r\n                         trust_remote_code=True,\
    \ device_map=\"auto\", return_full_text=True)\r\n\r\n#Dolly Stuff\r\nfrom langchain\
    \ import PromptTemplate, LLMChain\r\nfrom langchain.llms import HuggingFacePipeline\r\
    \n\r\n#template for an instruction with no input\r\nprompt = PromptTemplate(\r\
    \n    input_variables=[\"instruction\"],\r\n    template=\"{instruction}\")\r\n\
    \r\n#template for an instruction with input\r\nprompt_with_context = PromptTemplate(\r\
    \n    input_variables=[\"instruction\", \"context\"],\r\n    template=\"{instruction}\\\
    n\\nInput:\\n{context}\")\r\n\r\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\r\
    \n\r\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\r\nllm_context_chain\
    \ = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\r\n\r\n\r\nWhat I don't\
    \ understand is how will \"doc_retriever\"  be used in the llm_context_chain or\
    \ llm_chain variables? \r\n\r\nI noticed no where in my code, I am not calling\
    \ \"doc_retriever\". How would I do that? Is there any documentation for uploading\
    \ your own documentation in Dolly while using HuggingFace? Any help would be greatly\
    \ appreciated!\r\n\r\n"
  created_at: 2023-08-05 21:07:11+00:00
  edited: false
  hidden: false
  id: 64cec80fd8d09273729ab966
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 88
repo_id: databricks/dolly-v2-12b
repo_type: model
status: open
target_branch: null
title: How to use your own data with Dolly
