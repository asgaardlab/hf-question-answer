!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepthoughts
conflicting_files: null
created_at: 2023-06-19 02:47:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-19T03:47:17.000Z'
    data:
      edited: false
      editors:
      - deepthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.525650143623352
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
          fullname: Petar Angelov
          isHf: false
          isPro: false
          name: deepthoughts
          type: user
        html: "<p>Using GCP P100 with 16GB memory, I get the following error when\
          \ calling generate_text() when loading the model using <code>load_in_8bit</code>.</p>\n\
          <pre><code>/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230:\
          \ UserWarning: where received a uint8 condition tensor. This behavior is\
          \ deprecated and will be removed in a future version of PyTorch. Use a boolean\
          \ condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n\
          \  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\nTraceback\
          \ (most recent call last):\n  File \"/home/admin/dolly-v2-12b/main.py\"\
          , line 33, in &lt;module&gt;\n    result = generate_text(prompt)\n  File\
          \ \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1120, in __call__\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1127, in run_single\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1026, in forward\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n  File \"/home/admin/dolly-v2-12b/instruct_pipeline.py\"\
          , line 132, in _forward\n    generated_sequence = self.model.generate(\n\
          \  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\n    return self.sample(\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2655, in sample\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\
          RuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ &lt; 0\n</code></pre>\n<p>I use the code below:</p>\n<p>import torch</p>\n\
          <p>modelPath = \"/home/admin/dolly-v2-12b\"</p>\n<p>from instruct_pipeline\
          \ import InstructionTextGenerationPipeline</p>\n<p>from transformers import\
          \ AutoModelForCausalLM, AutoTokenizer</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(modelPath,\
          \ padding_side=\"left\")</p>\n<p>model = AutoModelForCausalLM.from_pretrained(modelPath,\
          \ device_map=\"auto\", load_in_8bit=True, torch_dtype=torch.float16)</p>\n\
          <p>generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer,\
          \ torch_dtype=torch.float16)</p>\n<p>result = generate_text(\"Test prompt\"\
          )</p>\n"
        raw: "Using GCP P100 with 16GB memory, I get the following error when calling\
          \ generate_text() when loading the model using `load_in_8bit`.\r\n```\r\n\
          /opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230:\
          \ UserWarning: where received a uint8 condition tensor. This behavior is\
          \ deprecated and will be removed in a future version of PyTorch. Use a boolean\
          \ condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\r\
          \n  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\r\n\
          Traceback (most recent call last):\r\n  File \"/home/admin/dolly-v2-12b/main.py\"\
          , line 33, in <module>\r\n    result = generate_text(prompt)\r\n  File \"\
          /opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1120, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1127, in run_single\r\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1026, in forward\r\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/admin/dolly-v2-12b/instruct_pipeline.py\"\
          , line 132, in _forward\r\n    generated_sequence = self.model.generate(\r\
          \n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n \
          \ File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2655, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
          \nRuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ < 0\r\n```\r\n\r\nI use the code below:\r\n\r\nimport torch\r\n\r\nmodelPath\
          \ = \"/home/admin/dolly-v2-12b\"\r\n\r\nfrom instruct_pipeline import InstructionTextGenerationPipeline\r\
          \n\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\
          \ntokenizer = AutoTokenizer.from_pretrained(modelPath, padding_side=\"left\"\
          )\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(modelPath, device_map=\"\
          auto\", load_in_8bit=True, torch_dtype=torch.float16)\r\n\r\ngenerate_text\
          \ = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer,\
          \ torch_dtype=torch.float16)\r\n\r\nresult = generate_text(\"Test prompt\"\
          )"
        updatedAt: '2023-06-19T03:47:17.474Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - christangttt
    id: 648fcfc519234cd09c4c62af
    type: comment
  author: deepthoughts
  content: "Using GCP P100 with 16GB memory, I get the following error when calling\
    \ generate_text() when loading the model using `load_in_8bit`.\r\n```\r\n/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230:\
    \ UserWarning: where received a uint8 condition tensor. This behavior is deprecated\
    \ and will be removed in a future version of PyTorch. Use a boolean condition\
    \ instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\r\
    \n  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/admin/dolly-v2-12b/main.py\", line\
    \ 33, in <module>\r\n    result = generate_text(prompt)\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1120, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1127, in run_single\r\n    model_outputs = self.forward(model_inputs, **forward_params)\r\
    \n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1026, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\
    \n  File \"/home/admin/dolly-v2-12b/instruct_pipeline.py\", line 132, in _forward\r\
    \n    generated_sequence = self.model.generate(\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"\
    /opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1572, in generate\r\n    return self.sample(\r\n  File \"/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2655, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
    \nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\
    \n```\r\n\r\nI use the code below:\r\n\r\nimport torch\r\n\r\nmodelPath = \"/home/admin/dolly-v2-12b\"\
    \r\n\r\nfrom instruct_pipeline import InstructionTextGenerationPipeline\r\n\r\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(modelPath, padding_side=\"left\")\r\n\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(modelPath, device_map=\"auto\", load_in_8bit=True,\
    \ torch_dtype=torch.float16)\r\n\r\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer, torch_dtype=torch.float16)\r\n\r\nresult = generate_text(\"\
    Test prompt\")"
  created_at: 2023-06-19 02:47:17+00:00
  edited: false
  hidden: false
  id: 648fcfc519234cd09c4c62af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-19T03:53:04.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9302754998207092
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That just means 8 bit quantization failed for this input. Probably
          will not work out here. Use the 7B model in 16bit.</p>

          '
        raw: That just means 8 bit quantization failed for this input. Probably will
          not work out here. Use the 7B model in 16bit.
        updatedAt: '2023-06-19T03:53:04.175Z'
      numEdits: 0
      reactions: []
    id: 648fd12019234cd09c4c7ca8
    type: comment
  author: srowen
  content: That just means 8 bit quantization failed for this input. Probably will
    not work out here. Use the 7B model in 16bit.
  created_at: 2023-06-19 02:53:04+00:00
  edited: false
  hidden: false
  id: 648fd12019234cd09c4c7ca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-24T03:02:59.000Z'
    data:
      status: closed
    id: 64965ce398dca8fbe9bcb6e1
    type: status-change
  author: srowen
  created_at: 2023-06-24 02:02:59+00:00
  id: 64965ce398dca8fbe9bcb6e1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
      fullname: AayushShah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AayushShah
      type: user
    createdAt: '2023-09-28T09:56:27.000Z'
    data:
      edited: false
      editors:
      - AayushShah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523674845695496
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
          fullname: AayushShah
          isHf: false
          isPro: false
          name: AayushShah
          type: user
        html: "<h2 id=\"\U0001F321-have-you-tried-increasing-the-temperature\">\U0001F321\
          \ Have you tried increasing the temperature?</h2>\n<p>Well try increasing\
          \ the <code>temperature</code> value. I had very low temperature value along\
          \ with other parameters such as <code>top_k</code> and <code>top_p</code>\
          \ which made the next token distribution too steep and as the beam search's\
          \ logic, you will need to have multiple tokens available, and in the <strong>low\
          \ temperature</strong> case I couldn't have (because we know how temperature\
          \ works, right?)</p>\n<blockquote>\n<p>So I increased the temperature and\
          \ it worked. </p>\n</blockquote>\n<p>Try increasing the temp value and it\
          \ should just work, if there are no other complexity involved.</p>\n"
        raw: "## \U0001F321 Have you tried increasing the temperature?\n\nWell try\
          \ increasing the `temperature` value. I had very low temperature value along\
          \ with other parameters such as `top_k` and `top_p` which made the next\
          \ token distribution too steep and as the beam search's logic, you will\
          \ need to have multiple tokens available, and in the **low temperature**\
          \ case I couldn't have (because we know how temperature works, right?)\n\
          \n> So I increased the temperature and it worked. \n\nTry increasing the\
          \ temp value and it should just work, if there are no other complexity involved."
        updatedAt: '2023-09-28T09:56:27.931Z'
      numEdits: 0
      reactions: []
    id: 65154dcb29f25f659b62ac39
    type: comment
  author: AayushShah
  content: "## \U0001F321 Have you tried increasing the temperature?\n\nWell try increasing\
    \ the `temperature` value. I had very low temperature value along with other parameters\
    \ such as `top_k` and `top_p` which made the next token distribution too steep\
    \ and as the beam search's logic, you will need to have multiple tokens available,\
    \ and in the **low temperature** case I couldn't have (because we know how temperature\
    \ works, right?)\n\n> So I increased the temperature and it worked. \n\nTry increasing\
    \ the temp value and it should just work, if there are no other complexity involved."
  created_at: 2023-09-28 08:56:27+00:00
  edited: false
  hidden: false
  id: 65154dcb29f25f659b62ac39
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 77
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: 'generate_text() leads to `RuntimeError: probability tensor contains either
  `inf`, `nan` or element < 0 - ` using  `load_in_8bit`'
