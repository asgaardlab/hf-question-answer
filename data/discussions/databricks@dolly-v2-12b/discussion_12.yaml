!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicolaschaillan
conflicting_files: null
created_at: 2023-04-13 03:08:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-04-13T04:08:41.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Hello,</p>

          <p>Thanks so much for the amazing work.</p>

          <p>I''m running the model on Azure on a NV12s_v3<br>GPU with 12 core,<br>112
          gb</p>

          <p>Here is my code:</p>

          <p>import torch<br>from transformers import pipeline</p>

          <p>print(''got here'')<br>generate_text = pipeline(model="databricks/dolly-v2-12b",
          torch_dtype=torch.bfloat16, trust_remote_code=True, device_map="auto")</p>

          <p>print(''got here2'')<br>generate_text("Explain to me the difference between
          nuclear fission and fusion.")<br>print(''got here3'')</p>

          <p>I get "got here" and "got here 2" but I never get "got here3", I waited
          1H... </p>

          <p>Is this normal? Is NV12s_v3 not enough for the model for a single query?</p>

          <p>Thanks.</p>

          '
        raw: "Hello,\r\n\r\nThanks so much for the amazing work.\r\n\r\nI'm running\
          \ the model on Azure on a NV12s_v3\r\nGPU with 12 core,\r\n112 gb\r\n\r\n\
          Here is my code:\r\n\r\nimport torch\r\nfrom transformers import pipeline\r\
          \n\r\nprint('got here')\r\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\r\n\r\nprint('got here2')\r\ngenerate_text(\"Explain to me the difference\
          \ between nuclear fission and fusion.\")\r\nprint('got here3')\r\n\r\nI\
          \ get \"got here\" and \"got here 2\" but I never get \"got here3\", I waited\
          \ 1H... \r\n\r\nIs this normal? Is NV12s_v3 not enough for the model for\
          \ a single query?\r\n\r\nThanks."
        updatedAt: '2023-04-13T04:08:41.715Z'
      numEdits: 0
      reactions: []
    id: 6437804901429ecf2556a752
    type: comment
  author: nicolaschaillan
  content: "Hello,\r\n\r\nThanks so much for the amazing work.\r\n\r\nI'm running\
    \ the model on Azure on a NV12s_v3\r\nGPU with 12 core,\r\n112 gb\r\n\r\nHere\
    \ is my code:\r\n\r\nimport torch\r\nfrom transformers import pipeline\r\n\r\n\
    print('got here')\r\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\r\n\
    \r\nprint('got here2')\r\ngenerate_text(\"Explain to me the difference between\
    \ nuclear fission and fusion.\")\r\nprint('got here3')\r\n\r\nI get \"got here\"\
    \ and \"got here 2\" but I never get \"got here3\", I waited 1H... \r\n\r\nIs\
    \ this normal? Is NV12s_v3 not enough for the model for a single query?\r\n\r\n\
    Thanks."
  created_at: 2023-04-13 03:08:41+00:00
  edited: false
  hidden: false
  id: 6437804901429ecf2556a752
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T13:17:40.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>An M60 is too small for generation (8GB RAM). You have loaded the
          model mostly onto the CPU and it will take forever. You at least need to
          try 8-bit here, but, need a GPU with at least 16GB of RAM for that to load
          on the GPU.</p>

          '
        raw: An M60 is too small for generation (8GB RAM). You have loaded the model
          mostly onto the CPU and it will take forever. You at least need to try 8-bit
          here, but, need a GPU with at least 16GB of RAM for that to load on the
          GPU.
        updatedAt: '2023-04-13T13:17:40.965Z'
      numEdits: 0
      reactions: []
    id: 643800f478e36ab7b68ae57d
    type: comment
  author: srowen
  content: An M60 is too small for generation (8GB RAM). You have loaded the model
    mostly onto the CPU and it will take forever. You at least need to try 8-bit here,
    but, need a GPU with at least 16GB of RAM for that to load on the GPU.
  created_at: 2023-04-13 12:17:40+00:00
  edited: false
  hidden: false
  id: 643800f478e36ab7b68ae57d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb472b70e66fe9e644bfba0d01b47cb9.svg
      fullname: Girijesh Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Girijesh1996
      type: user
    createdAt: '2023-04-13T13:40:30.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/eb472b70e66fe9e644bfba0d01b47cb9.svg
          fullname: Girijesh Singh
          isHf: false
          isPro: false
          name: Girijesh1996
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-13T14:30:06.127Z'
      numEdits: 0
      reactions: []
    id: 6438064e94faafc1a2dfd7b0
    type: comment
  author: Girijesh1996
  content: This comment has been hidden
  created_at: 2023-04-13 12:40:30+00:00
  edited: true
  hidden: true
  id: 6438064e94faafc1a2dfd7b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T13:52:14.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Please see the repo for generation snippets: <a rel="nofollow" href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a><br>It
          isn''t a matter of different code, you need different hardware here.<br>Alternatively,
          much smaller models were just released. You can try the 2.7B model on an
          M60 and that should work</p>

          '
        raw: 'Please see the repo for generation snippets: https://github.com/databrickslabs/dolly

          It isn''t a matter of different code, you need different hardware here.

          Alternatively, much smaller models were just released. You can try the 2.7B
          model on an M60 and that should work'
        updatedAt: '2023-04-13T13:52:14.397Z'
      numEdits: 0
      reactions: []
    id: 6438090e2d5f15c425c14fd2
    type: comment
  author: srowen
  content: 'Please see the repo for generation snippets: https://github.com/databrickslabs/dolly

    It isn''t a matter of different code, you need different hardware here.

    Alternatively, much smaller models were just released. You can try the 2.7B model
    on an M60 and that should work'
  created_at: 2023-04-13 12:52:14+00:00
  edited: false
  hidden: false
  id: 6438090e2d5f15c425c14fd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-04-13T16:57:11.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Thank you, any recommendation for Azure on VMs that could run the
          full model ?</p>

          '
        raw: Thank you, any recommendation for Azure on VMs that could run the full
          model ?
        updatedAt: '2023-04-13T16:57:11.542Z'
      numEdits: 0
      reactions: []
    id: 6438346740bf2c4964f7f57e
    type: comment
  author: nicolaschaillan
  content: Thank you, any recommendation for Azure on VMs that could run the full
    model ?
  created_at: 2023-04-13 15:57:11+00:00
  edited: false
  hidden: false
  id: 6438346740bf2c4964f7f57e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T17:11:02.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>The full 12B model works on A100s. It also works on A10 GPUs if
          you load in 8-bit, and sounds like it works on T4 as well in 8-bit. It should
          also work on a 32GB V100 if you load in float16, not bfloat16</p>

          '
        raw: The full 12B model works on A100s. It also works on A10 GPUs if you load
          in 8-bit, and sounds like it works on T4 as well in 8-bit. It should also
          work on a 32GB V100 if you load in float16, not bfloat16
        updatedAt: '2023-04-13T17:11:02.509Z'
      numEdits: 0
      reactions: []
    id: 643837a65309735a4077db51
    type: comment
  author: srowen
  content: The full 12B model works on A100s. It also works on A10 GPUs if you load
    in 8-bit, and sounds like it works on T4 as well in 8-bit. It should also work
    on a 32GB V100 if you load in float16, not bfloat16
  created_at: 2023-04-13 16:11:02+00:00
  edited: false
  hidden: false
  id: 643837a65309735a4077db51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-04-14T10:54:01.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Would be great to document this and put the python code required
          for each use case!</p>

          '
        raw: Would be great to document this and put the python code required for
          each use case!
        updatedAt: '2023-04-14T10:54:01.235Z'
      numEdits: 0
      reactions: []
    id: 643930c99f49f6e6ee21c2c3
    type: comment
  author: nicolaschaillan
  content: Would be great to document this and put the python code required for each
    use case!
  created_at: 2023-04-14 09:54:01+00:00
  edited: false
  hidden: false
  id: 643930c99f49f6e6ee21c2c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T12:13:30.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah we''ll update everything for v2 more fully soon, including
          training and generation. Right now it''s assumed you''re working on an A100</p>

          '
        raw: Yeah we'll update everything for v2 more fully soon, including training
          and generation. Right now it's assumed you're working on an A100
        updatedAt: '2023-04-14T12:13:30.713Z'
      numEdits: 0
      reactions: []
    id: 6439436a0cb95b3dbc8df585
    type: comment
  author: srowen
  content: Yeah we'll update everything for v2 more fully soon, including training
    and generation. Right now it's assumed you're working on an A100
  created_at: 2023-04-14 11:13:30+00:00
  edited: false
  hidden: false
  id: 6439436a0cb95b3dbc8df585
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:06:47.000Z'
    data:
      status: closed
    id: 64403bc72113f7dfcb551e0b
    type: status-change
  author: srowen
  created_at: 2023-04-19 18:06:47+00:00
  id: 64403bc72113f7dfcb551e0b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Model not giving answer
