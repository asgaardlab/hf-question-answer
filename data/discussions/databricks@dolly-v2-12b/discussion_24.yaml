!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hf2477565
conflicting_files: null
created_at: 2023-04-13 20:32:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a41a278ed705b67490c13761d17d57e5.svg
      fullname: James Walsh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hf2477565
      type: user
    createdAt: '2023-04-13T21:32:12.000Z'
    data:
      edited: false
      editors:
      - hf2477565
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a41a278ed705b67490c13761d17d57e5.svg
          fullname: James Walsh
          isHf: false
          isPro: false
          name: hf2477565
          type: user
        html: '<p>Hi there,</p>

          <p>I''m new to transformers, torch, and basically any ML development from
          the last decade and I''m trying to get back into it.</p>

          <p>I''ve setup a jupyter notebook with torch and cuda enabled, I have an
          RTX 2080 8GB. I''m not expecting blistering performance, but should that
          be sufficient to build a pipeline from a pretrained model and get it to
          give me answers in say less than 10 minutes?</p>

          <p>This code runs without error in about 8 minutes or so:</p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b",
          padding_side="left")

          model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b",
          offload_folder="offload", torch_dtype=torch.bfloat16, device_map="auto",
          load_in_8bit=True)


          generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)

          </code></pre>

          <p>but</p>

          <pre><code>generate_text("tell a short story")

          </code></pre>

          <p>just seems to hang.</p>

          <p>I thought the pipeline inference would be relatively quick compared to
          loading the model. Are my expectations wrong?</p>

          '
        raw: "Hi there,\r\n\r\nI'm new to transformers, torch, and basically any ML\
          \ development from the last decade and I'm trying to get back into it.\r\
          \n\r\nI've setup a jupyter notebook with torch and cuda enabled, I have\
          \ an RTX 2080 8GB. I'm not expecting blistering performance, but should\
          \ that be sufficient to build a pipeline from a pretrained model and get\
          \ it to give me answers in say less than 10 minutes?\r\n\r\nThis code runs\
          \ without error in about 8 minutes or so:\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          databricks/dolly-v2-12b\", padding_side=\"left\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          databricks/dolly-v2-12b\", offload_folder=\"offload\", torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", load_in_8bit=True)\r\n\r\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\r\n```\r\n\r\nbut\r\n\r\n```\r\ngenerate_text(\"\
          tell a short story\")\r\n```\r\n\r\njust seems to hang.\r\n\r\nI thought\
          \ the pipeline inference would be relatively quick compared to loading the\
          \ model. Are my expectations wrong?"
        updatedAt: '2023-04-13T21:32:12.613Z'
      numEdits: 0
      reactions: []
    id: 643874dc27a3c24cf4a3af06
    type: comment
  author: hf2477565
  content: "Hi there,\r\n\r\nI'm new to transformers, torch, and basically any ML\
    \ development from the last decade and I'm trying to get back into it.\r\n\r\n\
    I've setup a jupyter notebook with torch and cuda enabled, I have an RTX 2080\
    \ 8GB. I'm not expecting blistering performance, but should that be sufficient\
    \ to build a pipeline from a pretrained model and get it to give me answers in\
    \ say less than 10 minutes?\r\n\r\nThis code runs without error in about 8 minutes\
    \ or so:\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\"\
    , padding_side=\"left\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\"\
    , offload_folder=\"offload\", torch_dtype=torch.bfloat16, device_map=\"auto\"\
    , load_in_8bit=True)\r\n\r\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer)\r\n```\r\n\r\nbut\r\n\r\n```\r\ngenerate_text(\"tell a\
    \ short story\")\r\n```\r\n\r\njust seems to hang.\r\n\r\nI thought the pipeline\
    \ inference would be relatively quick compared to loading the model. Are my expectations\
    \ wrong?"
  created_at: 2023-04-13 20:32:12+00:00
  edited: false
  hidden: false
  id: 643874dc27a3c24cf4a3af06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T22:15:47.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>device_map=''auto'' is causing so much confusion. You don''t have
          nearly enough GPU RAM to load so it loads most on the CPU and works but
          very slowly. Maybe we should just set the example to force CUDA 0 so it
          fails explicitly if it doesn''t fit  </p>

          <p>For 16GB GPUs you can get it to load in 8 bit. For 8GB won''t work. Use
          the 2.7B model?</p>

          '
        raw: "device_map='auto' is causing so much confusion. You don't have nearly\
          \ enough GPU RAM to load so it loads most on the CPU and works but very\
          \ slowly. Maybe we should just set the example to force CUDA 0 so it fails\
          \ explicitly if it doesn't fit  \n\nFor 16GB GPUs you can get it to load\
          \ in 8 bit. For 8GB won't work. Use the 2.7B model?"
        updatedAt: '2023-04-13T22:15:47.709Z'
      numEdits: 0
      reactions: []
    id: 64387f139dc7c0d6bcfe70d4
    type: comment
  author: srowen
  content: "device_map='auto' is causing so much confusion. You don't have nearly\
    \ enough GPU RAM to load so it loads most on the CPU and works but very slowly.\
    \ Maybe we should just set the example to force CUDA 0 so it fails explicitly\
    \ if it doesn't fit  \n\nFor 16GB GPUs you can get it to load in 8 bit. For 8GB\
    \ won't work. Use the 2.7B model?"
  created_at: 2023-04-13 21:15:47+00:00
  edited: false
  hidden: false
  id: 64387f139dc7c0d6bcfe70d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-13T22:17:03.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>To answer your question should be like 10-20 seconds on an A10.</p>

          '
        raw: To answer your question should be like 10-20 seconds on an A10.
        updatedAt: '2023-04-13T22:17:03.809Z'
      numEdits: 0
      reactions: []
    id: 64387f5fdca82729227076ed
    type: comment
  author: srowen
  content: To answer your question should be like 10-20 seconds on an A10.
  created_at: 2023-04-13 21:17:03+00:00
  edited: false
  hidden: false
  id: 64387f5fdca82729227076ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T19:08:55.000Z'
    data:
      status: closed
    id: 64403c47dbd88206a83d2bc9
    type: status-change
  author: srowen
  created_at: 2023-04-19 18:08:55+00:00
  id: 64403c47dbd88206a83d2bc9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637133632149-60f0c813807badf815f74473.png?w=200&h=200&f=face
      fullname: Van Landeghem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sofie
      type: user
    createdAt: '2023-04-26T14:56:09.000Z'
    data:
      edited: true
      editors:
      - Sofie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637133632149-60f0c813807badf815f74473.png?w=200&h=200&f=face
          fullname: Van Landeghem
          isHf: false
          isPro: false
          name: Sofie
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/srowen\">@<span class=\"\
          underline\">srowen</span></a></span>\n\n\t</span></span>, sorry to follow\
          \ up on a closed discussion, but I'm wondering how to specify the <code>device_map</code>\
          \ argument to force CUDA 0 and fail explicitly, as you suggested?</p>\n"
        raw: Hi @srowen, sorry to follow up on a closed discussion, but I'm wondering
          how to specify the `device_map` argument to force CUDA 0 and fail explicitly,
          as you suggested?
        updatedAt: '2023-04-26T21:20:32.978Z'
      numEdits: 1
      reactions: []
    id: 64493b8956cf5fbfddbf4e00
    type: comment
  author: Sofie
  content: Hi @srowen, sorry to follow up on a closed discussion, but I'm wondering
    how to specify the `device_map` argument to force CUDA 0 and fail explicitly,
    as you suggested?
  created_at: 2023-04-26 13:56:09+00:00
  edited: true
  hidden: false
  id: 64493b8956cf5fbfddbf4e00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T15:02:32.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You just set <code>device="cuda:0"</code> then, and you don''t need
          <code>accelerate</code> to figure out a device mapping in that case.</p>

          '
        raw: You just set `device="cuda:0"` then, and you don't need `accelerate`
          to figure out a device mapping in that case.
        updatedAt: '2023-04-26T15:02:32.431Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Sofie
    id: 64493d08111b3bf687797e0e
    type: comment
  author: srowen
  content: You just set `device="cuda:0"` then, and you don't need `accelerate` to
    figure out a device mapping in that case.
  created_at: 2023-04-26 14:02:32+00:00
  edited: false
  hidden: false
  id: 64493d08111b3bf687797e0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637133632149-60f0c813807badf815f74473.png?w=200&h=200&f=face
      fullname: Van Landeghem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sofie
      type: user
    createdAt: '2023-04-27T08:22:22.000Z'
    data:
      edited: false
      editors:
      - Sofie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637133632149-60f0c813807badf815f74473.png?w=200&h=200&f=face
          fullname: Van Landeghem
          isHf: false
          isPro: false
          name: Sofie
          type: user
        html: '<p>Thank you! That''s clear and works like a charm.</p>

          '
        raw: Thank you! That's clear and works like a charm.
        updatedAt: '2023-04-27T08:22:22.359Z'
      numEdits: 0
      reactions: []
    id: 644a30bef3ff2cb84eb3ad39
    type: comment
  author: Sofie
  content: Thank you! That's clear and works like a charm.
  created_at: 2023-04-27 07:22:22+00:00
  edited: false
  hidden: false
  id: 644a30bef3ff2cb84eb3ad39
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Rough estimates for text generation?
