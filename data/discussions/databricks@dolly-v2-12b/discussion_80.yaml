!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepthoughts
conflicting_files: null
created_at: 2023-06-26 03:47:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-26T04:47:17.000Z'
    data:
      edited: false
      editors:
      - deepthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9760662317276001
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
          fullname: Petar Angelov
          isHf: false
          isPro: false
          name: deepthoughts
          type: user
        html: "<p>All of the training code that I looked at appears to expect an existing\
          \ model. How can I train a brand new model ( say a new language ) and I\
          \ want to start everytning from scratch as to not pollute the model if it's\
          \ going to be optimized for a certain language ( for example )?</p>\n<p>My\
          \ first thinking is that I should go and explore how Pythia was trained\
          \ and what code it used to train it. However, I thought I'd ask here first\
          \ if there is already a way to use Dolly 2.0's training code.</p>\n<p>Great\
          \ job \U0001F917!</p>\n"
        raw: "All of the training code that I looked at appears to expect an existing\
          \ model. How can I train a brand new model ( say a new language ) and I\
          \ want to start everytning from scratch as to not pollute the model if it's\
          \ going to be optimized for a certain language ( for example )?\r\n\r\n\
          My first thinking is that I should go and explore how Pythia was trained\
          \ and what code it used to train it. However, I thought I'd ask here first\
          \ if there is already a way to use Dolly 2.0's training code.\r\n\r\nGreat\
          \ job \U0001F917!"
        updatedAt: '2023-06-26T04:47:17.031Z'
      numEdits: 0
      reactions: []
    id: 649918556117099581f69f3a
    type: comment
  author: deepthoughts
  content: "All of the training code that I looked at appears to expect an existing\
    \ model. How can I train a brand new model ( say a new language ) and I want to\
    \ start everytning from scratch as to not pollute the model if it's going to be\
    \ optimized for a certain language ( for example )?\r\n\r\nMy first thinking is\
    \ that I should go and explore how Pythia was trained and what code it used to\
    \ train it. However, I thought I'd ask here first if there is already a way to\
    \ use Dolly 2.0's training code.\r\n\r\nGreat job \U0001F917!"
  created_at: 2023-06-26 03:47:17+00:00
  edited: false
  hidden: false
  id: 649918556117099581f69f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-26T04:47:44.000Z'
    data:
      from: How to train Dolly 2.0 with a brand new data set ( Not Pythia ) ?
      to: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia's
        raw data and use a new language ) ?
    id: 649918707ac1bf03e21d5864
    type: title-change
  author: deepthoughts
  created_at: 2023-06-26 03:47:44+00:00
  id: 649918707ac1bf03e21d5864
  new_title: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia's
    raw data and use a new language ) ?
  old_title: How to train Dolly 2.0 with a brand new data set ( Not Pythia ) ?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-26T04:48:00.000Z'
    data:
      from: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia's
        raw data and use a new language ) ?
      to: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia
        and use a new language ) ?
    id: 64991880cd4afd93710323a7
    type: title-change
  author: deepthoughts
  created_at: 2023-06-26 03:48:00+00:00
  id: 64991880cd4afd93710323a7
  new_title: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia
    and use a new language ) ?
  old_title: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia's
    raw data and use a new language ) ?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-26T05:00:20.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9668290615081787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>You should be able to easily find the Pythia code and paper: <a
          rel="nofollow" href="https://github.com/EleutherAI/pythia">https://github.com/EleutherAI/pythia</a><br>But
          you''ll see that the base Pythia 12B model''s training took 72,300 A100
          hours (<a rel="nofollow" href="https://arxiv.org/pdf/2304.01373.pdf">https://arxiv.org/pdf/2304.01373.pdf</a>),
          which would cost you hundreds of thousands of dollars to reproduce.<br>That
          work goes into developing a base understanding of language that you want
          to reuse, not throw away and reproduce.<br>It also saw 200B tokens, and
          you would need to have something of that order of magnitude on hand to make
          this make sense.</p>

          <p>It''s rather the point here that outside of big orgs, this just isn''t
          feasible; fine-tuning very much is.</p>

          <p>But yes its training set is mostly English. Not entirely, and to a modest
          degree some language learning is language-agnostic.<br>You''d be much better
          served looking for another base model that covers the language of interest
          (what language?)</p>

          '
        raw: "You should be able to easily find the Pythia code and paper: https://github.com/EleutherAI/pythia\
          \ \nBut you'll see that the base Pythia 12B model's training took 72,300\
          \ A100 hours (https://arxiv.org/pdf/2304.01373.pdf), which would cost you\
          \ hundreds of thousands of dollars to reproduce.\nThat work goes into developing\
          \ a base understanding of language that you want to reuse, not throw away\
          \ and reproduce.\nIt also saw 200B tokens, and you would need to have something\
          \ of that order of magnitude on hand to make this make sense.\n\nIt's rather\
          \ the point here that outside of big orgs, this just isn't feasible; fine-tuning\
          \ very much is.\n\nBut yes its training set is mostly English. Not entirely,\
          \ and to a modest degree some language learning is language-agnostic.\n\
          You'd be much better served looking for another base model that covers the\
          \ language of interest (what language?)\n"
        updatedAt: '2023-06-26T05:00:20.530Z'
      numEdits: 0
      reactions: []
    id: 64991b6476d49ee00f7c2b43
    type: comment
  author: srowen
  content: "You should be able to easily find the Pythia code and paper: https://github.com/EleutherAI/pythia\
    \ \nBut you'll see that the base Pythia 12B model's training took 72,300 A100\
    \ hours (https://arxiv.org/pdf/2304.01373.pdf), which would cost you hundreds\
    \ of thousands of dollars to reproduce.\nThat work goes into developing a base\
    \ understanding of language that you want to reuse, not throw away and reproduce.\n\
    It also saw 200B tokens, and you would need to have something of that order of\
    \ magnitude on hand to make this make sense.\n\nIt's rather the point here that\
    \ outside of big orgs, this just isn't feasible; fine-tuning very much is.\n\n\
    But yes its training set is mostly English. Not entirely, and to a modest degree\
    \ some language learning is language-agnostic.\nYou'd be much better served looking\
    \ for another base model that covers the language of interest (what language?)\n"
  created_at: 2023-06-26 04:00:20+00:00
  edited: false
  hidden: false
  id: 64991b6476d49ee00f7c2b43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-26T05:13:43.000Z'
    data:
      edited: false
      editors:
      - deepthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611797332763672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
          fullname: Petar Angelov
          isHf: false
          isPro: false
          name: deepthoughts
          type: user
        html: '<p>That''s very informative. Thank you.</p>

          <p>My thinking was that the model I create will be one with significantly
          less params ( millions) with the hope that once the language is learned,
          fine tuning to specific domains in that language will outperform large llms
          trained on an entirely different language.</p>

          <p>I''m looking to train a model on an eastern European language. For example
          Ukrainian, Bulgarian, etc...</p>

          '
        raw: 'That''s very informative. Thank you.


          My thinking was that the model I create will be one with significantly less
          params ( millions) with the hope that once the language is learned, fine
          tuning to specific domains in that language will outperform large llms trained
          on an entirely different language.


          I''m looking to train a model on an eastern European language. For example
          Ukrainian, Bulgarian, etc...'
        updatedAt: '2023-06-26T05:13:43.183Z'
      numEdits: 0
      reactions: []
    id: 64991e8746f17389a00f7371
    type: comment
  author: deepthoughts
  content: 'That''s very informative. Thank you.


    My thinking was that the model I create will be one with significantly less params
    ( millions) with the hope that once the language is learned, fine tuning to specific
    domains in that language will outperform large llms trained on an entirely different
    language.


    I''m looking to train a model on an eastern European language. For example Ukrainian,
    Bulgarian, etc...'
  created_at: 2023-06-26 04:13:43+00:00
  edited: false
  hidden: false
  id: 64991e8746f17389a00f7371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-26T05:18:15.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9713637828826904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Maybe; you may find that you just don''t get enough language capability
          with a much smaller model. Another big factor here is, do you have enough
          Bulgarian, etc., text to support the training?<br>You can also just see
          how well off-the-shelf Pythia (or other) models understand these languages;
          it could be surprising. </p>

          '
        raw: 'Maybe; you may find that you just don''t get enough language capability
          with a much smaller model. Another big factor here is, do you have enough
          Bulgarian, etc., text to support the training?

          You can also just see how well off-the-shelf Pythia (or other) models understand
          these languages; it could be surprising. '
        updatedAt: '2023-06-26T05:18:15.042Z'
      numEdits: 0
      reactions: []
    id: 64991f97070a4be034146267
    type: comment
  author: srowen
  content: 'Maybe; you may find that you just don''t get enough language capability
    with a much smaller model. Another big factor here is, do you have enough Bulgarian,
    etc., text to support the training?

    You can also just see how well off-the-shelf Pythia (or other) models understand
    these languages; it could be surprising. '
  created_at: 2023-06-26 04:18:15+00:00
  edited: false
  hidden: false
  id: 64991f97070a4be034146267
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
      fullname: Petar Angelov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepthoughts
      type: user
    createdAt: '2023-06-27T04:13:09.000Z'
    data:
      edited: true
      editors:
      - deepthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.94236820936203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6d371ce1aa070f1d7672e01cc5e1d9a.svg
          fullname: Petar Angelov
          isHf: false
          isPro: false
          name: deepthoughts
          type: user
        html: '<p>What do you think about the benefit of using the Pythia model but
          dumping a bunch of language-specific raw data at it to improve the quality
          of that particular language? I tried the model with Ukranian and Bulgarian
          and it''s subpar as it starts conflacting languages a bit.</p>

          '
        raw: What do you think about the benefit of using the Pythia model but dumping
          a bunch of language-specific raw data at it to improve the quality of that
          particular language? I tried the model with Ukranian and Bulgarian and it's
          subpar as it starts conflacting languages a bit.
        updatedAt: '2023-06-27T04:31:27.365Z'
      numEdits: 1
      reactions: []
    id: 649a61d5facc059c2dbf56d9
    type: comment
  author: deepthoughts
  content: What do you think about the benefit of using the Pythia model but dumping
    a bunch of language-specific raw data at it to improve the quality of that particular
    language? I tried the model with Ukranian and Bulgarian and it's subpar as it
    starts conflacting languages a bit.
  created_at: 2023-06-27 03:13:09+00:00
  edited: true
  hidden: false
  id: 649a61d5facc059c2dbf56d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-07-15T16:00:39.000Z'
    data:
      status: closed
    id: 64b2c2a7ffed52962aa98416
    type: status-change
  author: srowen
  created_at: 2023-07-15 15:00:39+00:00
  id: 64b2c2a7ffed52962aa98416
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 80
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: How to train Dolly 2.0 with a brand new raw data set ( i.e. replace Pythia
  and use a new language ) ?
