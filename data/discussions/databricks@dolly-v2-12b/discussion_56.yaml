!!python/object:huggingface_hub.community.DiscussionWithDetails
author: niktarkon
conflicting_files: null
created_at: 2023-04-26 15:40:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
      fullname: Nikita Tarasov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niktarkon
      type: user
    createdAt: '2023-04-26T16:40:27.000Z'
    data:
      edited: false
      editors:
      - niktarkon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
          fullname: Nikita Tarasov
          isHf: false
          isPro: false
          name: niktarkon
          type: user
        html: '<p>Hi all!<br>Has anyone trained one of the dolly models with deepspeed?
          Please share the following information:</p>

          <ol>

          <li>What kind of dolly did you train?</li>

          <li>In what environment and on what type of GPU?</li>

          <li>How much is the GPU consumed in general when deepspeed is running? (Obviously
          it depends a lot, but I would like to know at least a lower estimate of
          how much GPU you need)</li>

          </ol>

          '
        raw: "Hi all!\r\nHas anyone trained one of the dolly models with deepspeed?\
          \ Please share the following information:\r\n1) What kind of dolly did you\
          \ train?\r\n2) In what environment and on what type of GPU?\r\n3) How much\
          \ is the GPU consumed in general when deepspeed is running? (Obviously it\
          \ depends a lot, but I would like to know at least a lower estimate of how\
          \ much GPU you need)"
        updatedAt: '2023-04-26T16:40:27.981Z'
      numEdits: 0
      reactions: []
    id: 644953fb16425b79ffee7632
    type: comment
  author: niktarkon
  content: "Hi all!\r\nHas anyone trained one of the dolly models with deepspeed?\
    \ Please share the following information:\r\n1) What kind of dolly did you train?\r\
    \n2) In what environment and on what type of GPU?\r\n3) How much is the GPU consumed\
    \ in general when deepspeed is running? (Obviously it depends a lot, but I would\
    \ like to know at least a lower estimate of how much GPU you need)"
  created_at: 2023-04-26 15:40:27+00:00
  edited: false
  hidden: false
  id: 644953fb16425b79ffee7632
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T16:45:02.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yes - that is what the provided training code does! <a rel="nofollow"
          href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a><br>You
          can tune the 3B, 7B, and 12B models.<br>Use an A100 if you can; the repo
          has notes about training on other instances.<br>How many GPU hours? depends
          on the type, number of GPUs, model you tune, input size, etc. You can expect
          about 100+ hours of A100 time for fine-tuning several epochs of the 12B
          model, as a data point.</p>

          '
        raw: 'Yes - that is what the provided training code does! https://github.com/databrickslabs/dolly

          You can tune the 3B, 7B, and 12B models.

          Use an A100 if you can; the repo has notes about training on other instances.

          How many GPU hours? depends on the type, number of GPUs, model you tune,
          input size, etc. You can expect about 100+ hours of A100 time for fine-tuning
          several epochs of the 12B model, as a data point.'
        updatedAt: '2023-04-26T16:45:02.218Z'
      numEdits: 0
      reactions: []
    id: 6449550eb3cd701e0a03fc0b
    type: comment
  author: srowen
  content: 'Yes - that is what the provided training code does! https://github.com/databrickslabs/dolly

    You can tune the 3B, 7B, and 12B models.

    Use an A100 if you can; the repo has notes about training on other instances.

    How many GPU hours? depends on the type, number of GPUs, model you tune, input
    size, etc. You can expect about 100+ hours of A100 time for fine-tuning several
    epochs of the 12B model, as a data point.'
  created_at: 2023-04-26 15:45:02+00:00
  edited: false
  hidden: false
  id: 6449550eb3cd701e0a03fc0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
      fullname: Nikita Tarasov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niktarkon
      type: user
    createdAt: '2023-04-26T17:37:46.000Z'
    data:
      edited: false
      editors:
      - niktarkon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
          fullname: Nikita Tarasov
          isHf: false
          isPro: false
          name: niktarkon
          type: user
        html: '<blockquote>

          <p>Yes - that is what the provided training code does! <a rel="nofollow"
          href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a><br>You
          can tune the 3B, 7B, and 12B models.<br>Use an A100 if you can; the repo
          has notes about training on other instances.<br>How many GPU hours? depends
          on the type, number of GPUs, model you tune, input size, etc. You can expect
          about 100+ hours of A100 time for fine-tuning several epochs of the 12B
          model, as a data point.</p>

          </blockquote>

          <p>Thank you!<br>Could you tell me if there are requirements specifically
          for system RAM?</p>

          <p>I decided to experimentally run in a base colab (I know I need a more
          powerful processor, but I wanted to make sure everything runs for me) and
          found that the training could not continue due to the system''s RAM being
          full.</p>

          <p>P.s.I tried to train dolly 2.8b on the dataset that is used for training
          by default in <a rel="nofollow" href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a>.
          I took the config and other settings from there.</p>

          '
        raw: '> Yes - that is what the provided training code does! https://github.com/databrickslabs/dolly

          > You can tune the 3B, 7B, and 12B models.

          > Use an A100 if you can; the repo has notes about training on other instances.

          > How many GPU hours? depends on the type, number of GPUs, model you tune,
          input size, etc. You can expect about 100+ hours of A100 time for fine-tuning
          several epochs of the 12B model, as a data point.


          Thank you!

          Could you tell me if there are requirements specifically for system RAM?


          I decided to experimentally run in a base colab (I know I need a more powerful
          processor, but I wanted to make sure everything runs for me) and found that
          the training could not continue due to the system''s RAM being full.


          P.s.I tried to train dolly 2.8b on the dataset that is used for training
          by default in https://github.com/databrickslabs/dolly. I took the config
          and other settings from there.'
        updatedAt: '2023-04-26T17:37:46.946Z'
      numEdits: 0
      reactions: []
    id: 6449616ab3cd701e0a050608
    type: comment
  author: niktarkon
  content: '> Yes - that is what the provided training code does! https://github.com/databrickslabs/dolly

    > You can tune the 3B, 7B, and 12B models.

    > Use an A100 if you can; the repo has notes about training on other instances.

    > How many GPU hours? depends on the type, number of GPUs, model you tune, input
    size, etc. You can expect about 100+ hours of A100 time for fine-tuning several
    epochs of the 12B model, as a data point.


    Thank you!

    Could you tell me if there are requirements specifically for system RAM?


    I decided to experimentally run in a base colab (I know I need a more powerful
    processor, but I wanted to make sure everything runs for me) and found that the
    training could not continue due to the system''s RAM being full.


    P.s.I tried to train dolly 2.8b on the dataset that is used for training by default
    in https://github.com/databrickslabs/dolly. I took the config and other settings
    from there.'
  created_at: 2023-04-26 16:37:46+00:00
  edited: false
  hidden: false
  id: 6449616ab3cd701e0a050608
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-26T17:43:17.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>GPU mem is really the limiting factor, not system RAM, but you''ll
          need probably 64GB of VM RAM to work with the 12B model to be safe.<br>Colab
          instances will not be powerful enough.</p>

          '
        raw: 'GPU mem is really the limiting factor, not system RAM, but you''ll need
          probably 64GB of VM RAM to work with the 12B model to be safe.

          Colab instances will not be powerful enough.'
        updatedAt: '2023-04-26T17:43:17.621Z'
      numEdits: 0
      reactions: []
    id: 644962b5a281f51a62ae0d95
    type: comment
  author: srowen
  content: 'GPU mem is really the limiting factor, not system RAM, but you''ll need
    probably 64GB of VM RAM to work with the 12B model to be safe.

    Colab instances will not be powerful enough.'
  created_at: 2023-04-26 16:43:17+00:00
  edited: false
  hidden: false
  id: 644962b5a281f51a62ae0d95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-28T14:00:01.000Z'
    data:
      status: closed
    id: 644bd16145e79023c7d53e99
    type: status-change
  author: srowen
  created_at: 2023-04-28 13:00:01+00:00
  id: 644bd16145e79023c7d53e99
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
      fullname: Nikita Tarasov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niktarkon
      type: user
    createdAt: '2023-06-19T11:18:45.000Z'
    data:
      edited: false
      editors:
      - niktarkon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9687939882278442
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6647a3605fe243f46371a04496f2a93d.svg
          fullname: Nikita Tarasov
          isHf: false
          isPro: false
          name: niktarkon
          type: user
        html: '<p>Could you tell me how many A100 is best to train 12b?</p>

          '
        raw: Could you tell me how many A100 is best to train 12b?
        updatedAt: '2023-06-19T11:18:45.330Z'
      numEdits: 0
      reactions: []
    id: 64903995cdfe00636ec2c3d6
    type: comment
  author: niktarkon
  content: Could you tell me how many A100 is best to train 12b?
  created_at: 2023-06-19 10:18:45+00:00
  edited: false
  hidden: false
  id: 64903995cdfe00636ec2c3d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-19T12:37:36.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9423324465751648
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>8 A100 is probably sufficient to train relatively rapidly but depends
          on your data, settings, requirements </p>

          '
        raw: '8 A100 is probably sufficient to train relatively rapidly but depends
          on your data, settings, requirements '
        updatedAt: '2023-06-19T12:37:36.062Z'
      numEdits: 0
      reactions: []
    id: 64904c104442ceabfc6a707d
    type: comment
  author: srowen
  content: '8 A100 is probably sufficient to train relatively rapidly but depends
    on your data, settings, requirements '
  created_at: 2023-06-19 11:37:36+00:00
  edited: false
  hidden: false
  id: 64904c104442ceabfc6a707d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678027285101-noauth.png?w=200&h=200&f=face
      fullname: gaurav makhija
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gaurav-mac
      type: user
    createdAt: '2023-07-20T19:21:11.000Z'
    data:
      edited: true
      editors:
      - gaurav-mac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.905466616153717
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678027285101-noauth.png?w=200&h=200&f=face
          fullname: gaurav makhija
          isHf: false
          isPro: false
          name: gaurav-mac
          type: user
        html: '<p>hi,<br>dolly-v2-3b is not working well on my dataset for closed-domain
          open-book QA even after tuning ''temperature'', ''prompt'', ''instruction'',
          ''context length'', ''data quality'' etc.<br>I am not tied to any organisation,
          so before purchasing 8xA100 40GB GPUs, I''d like to make sure my understanding
          if my approach is apt about training size   </p>

          <p>Question: To train dolly-v2-3b, for a new domain (closed-domain open-book
          QA), how many samples are good enough? </p>

          '
        raw: "hi, \ndolly-v2-3b is not working well on my dataset for closed-domain\
          \ open-book QA even after tuning 'temperature', 'prompt', 'instruction',\
          \ 'context length', 'data quality' etc.\nI am not tied to any organisation,\
          \ so before purchasing 8xA100 40GB GPUs, I'd like to make sure my understanding\
          \ if my approach is apt about training size   \n\nQuestion: To train dolly-v2-3b,\
          \ for a new domain (closed-domain open-book QA), how many samples are good\
          \ enough? "
        updatedAt: '2023-07-26T23:24:20.318Z'
      numEdits: 10
      reactions: []
    id: 64b98927f602541ef760219a
    type: comment
  author: gaurav-mac
  content: "hi, \ndolly-v2-3b is not working well on my dataset for closed-domain\
    \ open-book QA even after tuning 'temperature', 'prompt', 'instruction', 'context\
    \ length', 'data quality' etc.\nI am not tied to any organisation, so before purchasing\
    \ 8xA100 40GB GPUs, I'd like to make sure my understanding if my approach is apt\
    \ about training size   \n\nQuestion: To train dolly-v2-3b, for a new domain (closed-domain\
    \ open-book QA), how many samples are good enough? "
  created_at: 2023-07-20 18:21:11+00:00
  edited: true
  hidden: false
  id: 64b98927f602541ef760219a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 56
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: Training dolly with deepspeed
