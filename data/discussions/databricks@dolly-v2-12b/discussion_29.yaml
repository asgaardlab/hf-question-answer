!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sulemank
conflicting_files: null
created_at: 2023-04-14 11:53:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d12c8c705615d0b4c01b410f8682da4b.svg
      fullname: Suleman Kazi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sulemank
      type: user
    createdAt: '2023-04-14T12:53:08.000Z'
    data:
      edited: false
      editors:
      - sulemank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d12c8c705615d0b4c01b410f8682da4b.svg
          fullname: Suleman Kazi
          isHf: false
          isPro: false
          name: sulemank
          type: user
        html: "<p>I'm seeing something weird when running the model in 8bit vs bfloat16.\
          \ On an A100 GPU the model does inference <em>3x faster</em> with bfloat16\
          \ than with uint8.</p>\n<p>Here's how I construct the pipeline for bfloat16:</p>\n\
          <pre><code>instruct_pipeline = pipeline(\n    model=\"databricks/dolly-v2-12b\"\
          ,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n    model_kwargs={\"load_in_8bit\": False},\n)\n</code></pre>\n\
          <p>and for uint8:</p>\n<pre><code>instruct_pipeline = pipeline(\n    model=\"\
          databricks/dolly-v2-12b\",\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n    model_kwargs={\"load_in_8bit\": True\n)\n</code></pre>\n<p>Followed\
          \ by inference with:</p>\n<pre><code>instruct_pipeline(\"Tell me a short\
          \ story about little red riding hood.\")\n</code></pre>\n<p>Anything I'm\
          \ doing wrong? As this is pretty unexpected.</p>\n"
        raw: "I'm seeing something weird when running the model in 8bit vs bfloat16.\
          \ On an A100 GPU the model does inference *3x faster* with bfloat16 than\
          \ with uint8.\r\n\r\nHere's how I construct the pipeline for bfloat16:\r\
          \n```\r\ninstruct_pipeline = pipeline(\r\n    model=\"databricks/dolly-v2-12b\"\
          ,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n\
          \    device_map=\"auto\",\r\n    model_kwargs={\"load_in_8bit\": False},\r\
          \n)\r\n```\r\n\r\nand for uint8:\r\n```\r\ninstruct_pipeline = pipeline(\r\
          \n    model=\"databricks/dolly-v2-12b\",\r\n    trust_remote_code=True,\r\
          \n    device_map=\"auto\",\r\n    model_kwargs={\"load_in_8bit\": True\r\
          \n)\r\n```\r\n\r\nFollowed by inference with:\r\n```\r\ninstruct_pipeline(\"\
          Tell me a short story about little red riding hood.\")\r\n```\r\nAnything\
          \ I'm doing wrong? As this is pretty unexpected."
        updatedAt: '2023-04-14T12:53:08.659Z'
      numEdits: 0
      reactions: []
    id: 64394cb421221ac741172b46
    type: comment
  author: sulemank
  content: "I'm seeing something weird when running the model in 8bit vs bfloat16.\
    \ On an A100 GPU the model does inference *3x faster* with bfloat16 than with\
    \ uint8.\r\n\r\nHere's how I construct the pipeline for bfloat16:\r\n```\r\ninstruct_pipeline\
    \ = pipeline(\r\n    model=\"databricks/dolly-v2-12b\",\r\n    torch_dtype=torch.bfloat16,\r\
    \n    trust_remote_code=True,\r\n    device_map=\"auto\",\r\n    model_kwargs={\"\
    load_in_8bit\": False},\r\n)\r\n```\r\n\r\nand for uint8:\r\n```\r\ninstruct_pipeline\
    \ = pipeline(\r\n    model=\"databricks/dolly-v2-12b\",\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n    model_kwargs={\"load_in_8bit\": True\r\n)\r\n\
    ```\r\n\r\nFollowed by inference with:\r\n```\r\ninstruct_pipeline(\"Tell me a\
    \ short story about little red riding hood.\")\r\n```\r\nAnything I'm doing wrong?\
    \ As this is pretty unexpected."
  created_at: 2023-04-14 11:53:08+00:00
  edited: false
  hidden: false
  id: 64394cb421221ac741172b46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-14T12:55:37.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah, it depends on the hardware, but entirely possible. The integer
          units on the GPUs are separate from the FP units, so they have different
          capabilities here. I think the Hopper GPUs are supposed to focus more on
          int8 math. int8 is about memory size, not speed, in general. Speed seemed
          similar on an A10 for me.</p>

          '
        raw: Yeah, it depends on the hardware, but entirely possible. The integer
          units on the GPUs are separate from the FP units, so they have different
          capabilities here. I think the Hopper GPUs are supposed to focus more on
          int8 math. int8 is about memory size, not speed, in general. Speed seemed
          similar on an A10 for me.
        updatedAt: '2023-04-14T12:55:37.431Z'
      numEdits: 0
      reactions: []
    id: 64394d4968228e8b33422ce0
    type: comment
  author: srowen
  content: Yeah, it depends on the hardware, but entirely possible. The integer units
    on the GPUs are separate from the FP units, so they have different capabilities
    here. I think the Hopper GPUs are supposed to focus more on int8 math. int8 is
    about memory size, not speed, in general. Speed seemed similar on an A10 for me.
  created_at: 2023-04-14 11:55:37+00:00
  edited: false
  hidden: false
  id: 64394d4968228e8b33422ce0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d12c8c705615d0b4c01b410f8682da4b.svg
      fullname: Suleman Kazi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sulemank
      type: user
    createdAt: '2023-04-14T15:31:56.000Z'
    data:
      edited: false
      editors:
      - sulemank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d12c8c705615d0b4c01b410f8682da4b.svg
          fullname: Suleman Kazi
          isHf: false
          isPro: false
          name: sulemank
          type: user
        html: '<p>Thanks, yes it seems hardware dependent as you said. I see the same
          difference when using FLAN UL2 in uint8 vs bfloat16 modes on an A100</p>

          '
        raw: Thanks, yes it seems hardware dependent as you said. I see the same difference
          when using FLAN UL2 in uint8 vs bfloat16 modes on an A100
        updatedAt: '2023-04-14T15:31:56.130Z'
      numEdits: 0
      reactions: []
      relatedEventId: 643971ec68228e8b33434c2a
    id: 643971ec68228e8b33434c29
    type: comment
  author: sulemank
  content: Thanks, yes it seems hardware dependent as you said. I see the same difference
    when using FLAN UL2 in uint8 vs bfloat16 modes on an A100
  created_at: 2023-04-14 14:31:56+00:00
  edited: false
  hidden: false
  id: 643971ec68228e8b33434c29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d12c8c705615d0b4c01b410f8682da4b.svg
      fullname: Suleman Kazi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sulemank
      type: user
    createdAt: '2023-04-14T15:31:56.000Z'
    data:
      status: closed
    id: 643971ec68228e8b33434c2a
    type: status-change
  author: sulemank
  created_at: 2023-04-14 14:31:56+00:00
  id: 643971ec68228e8b33434c2a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: databricks/dolly-v2-12b
repo_type: model
status: closed
target_branch: null
title: uint8 actually slower than bfloat16?
