!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dfurman
conflicting_files: null
created_at: 2023-04-13 02:18:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurman
      type: user
    createdAt: '2023-04-13T03:18:48.000Z'
    data:
      edited: false
      editors:
      - dfurman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurman
          type: user
        html: '<p>The model_max_length in the tokenizer.config should be "2048" not
          "1000000000000000019884624838656". </p>

          '
        raw: 'The model_max_length in the tokenizer.config should be "2048" not "1000000000000000019884624838656". '
        updatedAt: '2023-04-13T03:18:48.056Z'
      numEdits: 0
      reactions: []
    id: 6437749801429ecf2556452c
    type: comment
  author: dfurman
  content: 'The model_max_length in the tokenizer.config should be "2048" not "1000000000000000019884624838656". '
  created_at: 2023-04-13 02:18:48+00:00
  edited: false
  hidden: false
  id: 6437749801429ecf2556452c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-13T03:51:55.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>It seems that this must be set automatically during the checkpointing
          process.  We did not explicitly add this setting.  Did you notice an issue
          associated with this?  Googling I found this thread: <a rel="nofollow" href="https://discuss.huggingface.co/t/tokenizers-what-this-max-length-number/28484">https://discuss.huggingface.co/t/tokenizers-what-this-max-length-number/28484</a></p>

          '
        raw: 'It seems that this must be set automatically during the checkpointing
          process.  We did not explicitly add this setting.  Did you notice an issue
          associated with this?  Googling I found this thread: https://discuss.huggingface.co/t/tokenizers-what-this-max-length-number/28484'
        updatedAt: '2023-04-13T03:51:55.565Z'
      numEdits: 0
      reactions: []
    id: 64377c5ba1010492a8a80529
    type: comment
  author: matthayes
  content: 'It seems that this must be set automatically during the checkpointing
    process.  We did not explicitly add this setting.  Did you notice an issue associated
    with this?  Googling I found this thread: https://discuss.huggingface.co/t/tokenizers-what-this-max-length-number/28484'
  created_at: 2023-04-13 02:51:55+00:00
  edited: false
  hidden: false
  id: 64377c5ba1010492a8a80529
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurman
      type: user
    createdAt: '2023-04-13T11:56:22.000Z'
    data:
      edited: false
      editors:
      - dfurman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurman
          type: user
        html: '<p>Yes - the model should have thrown a warning when you feed in a
          chunk that is larger than the window size but instead it just errors out.
          You should explicitly add the max window size in that variable (seems the
          Dolly-v1 model did have this correct).</p>

          '
        raw: Yes - the model should have thrown a warning when you feed in a chunk
          that is larger than the window size but instead it just errors out. You
          should explicitly add the max window size in that variable (seems the Dolly-v1
          model did have this correct).
        updatedAt: '2023-04-13T11:56:22.963Z'
      numEdits: 0
      reactions: []
    id: 6437ede62d5f15c425c06a71
    type: comment
  author: dfurman
  content: Yes - the model should have thrown a warning when you feed in a chunk that
    is larger than the window size but instead it just errors out. You should explicitly
    add the max window size in that variable (seems the Dolly-v1 model did have this
    correct).
  created_at: 2023-04-13 10:56:22+00:00
  edited: false
  hidden: false
  id: 6437ede62d5f15c425c06a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T16:54:48.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;matthayes&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/matthayes\">@<span class=\"\
          underline\">matthayes</span></a></span>\n\n\t</span></span>. This is still\
          \ erred and should be hard-coded in to whatever the window size is for the\
          \ model (I think it is 2048 based on initial testing). It is important because\
          \ people can use the truncation functionality when tokenizing - so that\
          \ when you feed in a chunk larger than the window size, the model doesn't\
          \ just err out.<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63ed0267684767daeca869a1/dKCnvs9ltwpcvbbD4OBlc.jpeg\"\
          ><img alt=\"IMG_8774.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ed0267684767daeca869a1/dKCnvs9ltwpcvbbD4OBlc.jpeg\"\
          ></a></p>\n"
        raw: "@matthayes. This is still erred and should be hard-coded in to whatever\
          \ the window size is for the model (I think it is 2048 based on initial\
          \ testing). It is important because people can use the truncation functionality\
          \ when tokenizing - so that when you feed in a chunk larger than the window\
          \ size, the model doesn't just err out. \n![IMG_8774.jpg](https://cdn-uploads.huggingface.co/production/uploads/63ed0267684767daeca869a1/dKCnvs9ltwpcvbbD4OBlc.jpeg)"
        updatedAt: '2023-04-27T16:54:48.740Z'
      numEdits: 0
      reactions: []
    id: 644aa8d8cb45734dfd441d8f
    type: comment
  author: dfurmanWMP
  content: "@matthayes. This is still erred and should be hard-coded in to whatever\
    \ the window size is for the model (I think it is 2048 based on initial testing).\
    \ It is important because people can use the truncation functionality when tokenizing\
    \ - so that when you feed in a chunk larger than the window size, the model doesn't\
    \ just err out. \n![IMG_8774.jpg](https://cdn-uploads.huggingface.co/production/uploads/63ed0267684767daeca869a1/dKCnvs9ltwpcvbbD4OBlc.jpeg)"
  created_at: 2023-04-27 15:54:48+00:00
  edited: false
  hidden: false
  id: 644aa8d8cb45734dfd441d8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-27T16:57:26.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;matthayes&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/matthayes\">@<span class=\"\
          underline\">matthayes</span></a></span>\n\n\t</span></span> it seems safe\
          \ to just set the tokenizer max length to 2048, as that is certainly an\
          \ upper bound on the max possible input size. I can just try that.</p>\n"
        raw: '@matthayes it seems safe to just set the tokenizer max length to 2048,
          as that is certainly an upper bound on the max possible input size. I can
          just try that.'
        updatedAt: '2023-04-27T16:57:26.994Z'
      numEdits: 0
      reactions: []
    id: 644aa976af97dfd24c0e4073
    type: comment
  author: srowen
  content: '@matthayes it seems safe to just set the tokenizer max length to 2048,
    as that is certainly an upper bound on the max possible input size. I can just
    try that.'
  created_at: 2023-04-27 15:57:26+00:00
  edited: false
  hidden: false
  id: 644aa976af97dfd24c0e4073
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T17:08:16.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: '<p>I am trying it too haha, will report back</p>

          '
        raw: I am trying it too haha, will report back
        updatedAt: '2023-04-27T17:08:16.958Z'
      numEdits: 0
      reactions: []
    id: 644aac00af97dfd24c0e74af
    type: comment
  author: dfurmanWMP
  content: I am trying it too haha, will report back
  created_at: 2023-04-27 16:08:16+00:00
  edited: false
  hidden: false
  id: 644aac00af97dfd24c0e74af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-27T17:19:01.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>I did some investigation a couple days ago and commented here: <a
          rel="nofollow" href="https://github.com/databrickslabs/dolly/issues/102">https://github.com/databrickslabs/dolly/issues/102</a></p>

          '
        raw: 'I did some investigation a couple days ago and commented here: https://github.com/databrickslabs/dolly/issues/102'
        updatedAt: '2023-04-27T17:19:01.419Z'
      numEdits: 0
      reactions: []
    id: 644aae85af97dfd24c0ea790
    type: comment
  author: matthayes
  content: 'I did some investigation a couple days ago and commented here: https://github.com/databrickslabs/dolly/issues/102'
  created_at: 2023-04-27 16:19:01+00:00
  edited: false
  hidden: false
  id: 644aae85af97dfd24c0ea790
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-27T17:22:10.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>I believe that setting the max length to 2048 is still going to
          lead to errors.  The model won''t be able to generate any new tokens as
          after the first token is generated it will exceed the max length.  Also
          it will truncate the prompt, so I suspect that the missing <code>### Response:</code>
          could cause poor performance.</p>

          '
        raw: I believe that setting the max length to 2048 is still going to lead
          to errors.  The model won't be able to generate any new tokens as after
          the first token is generated it will exceed the max length.  Also it will
          truncate the prompt, so I suspect that the missing `### Response:` could
          cause poor performance.
        updatedAt: '2023-04-27T17:22:10.780Z'
      numEdits: 0
      reactions: []
    id: 644aaf42cb45734dfd44a15f
    type: comment
  author: matthayes
  content: I believe that setting the max length to 2048 is still going to lead to
    errors.  The model won't be able to generate any new tokens as after the first
    token is generated it will exceed the max length.  Also it will truncate the prompt,
    so I suspect that the missing `### Response:` could cause poor performance.
  created_at: 2023-04-27 16:22:10+00:00
  edited: false
  hidden: false
  id: 644aaf42cb45734dfd44a15f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T17:22:17.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: "<p>Awesome thanks <span data-props=\"{&quot;user&quot;:&quot;matthayes&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/matthayes\"\
          >@<span class=\"underline\">matthayes</span></a></span>\n\n\t</span></span>,\
          \ so we should basically set it to 1769? Yes, I see that issue with truncating\
          \ the necessary special tokens, hmmm, not an easy fix then.</p>\n"
        raw: Awesome thanks @matthayes, so we should basically set it to 1769? Yes,
          I see that issue with truncating the necessary special tokens, hmmm, not
          an easy fix then.
        updatedAt: '2023-04-27T17:22:17.432Z'
      numEdits: 0
      reactions: []
    id: 644aaf49af97dfd24c0eb6e1
    type: comment
  author: dfurmanWMP
  content: Awesome thanks @matthayes, so we should basically set it to 1769? Yes,
    I see that issue with truncating the necessary special tokens, hmmm, not an easy
    fix then.
  created_at: 2023-04-27 16:22:17+00:00
  edited: false
  hidden: false
  id: 644aaf49af97dfd24c0eb6e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-27T17:27:35.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: "<p>If you look at the source for <code>TextGenerationPipeline</code>\
          \ (which our pipeline is somewhat derived from), it has an option <code>handle_long_generation</code>:</p>\n\
          <pre><code>            handle_long_generation (`str`, *optional*):\n   \
          \             By default, this pipelines does not handle long generation\
          \ (ones that exceed in one form or the other\n                the model\
          \ maximum length). There is no perfect way to adress this (more info\n \
          \               :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227).\
          \ This provides common\n                strategies to work around that problem\
          \ depending on your use case.\n\n                - `None` : default strategy\
          \ where nothing in particular happens\n                - `\"hole\"`: Truncates\
          \ left of input, and leaves a gap wide enough to let generation happen (might\n\
          \                  truncate a lot of the prompt and not suitable when generation\
          \ exceed the model capacity)\n</code></pre>\n<p>We could do something similar,\
          \ where basically we truncate the instruction before it is formatted into\
          \ the full prompt.  This isn't a perfect solution but could be an okay workaround.</p>\n"
        raw: "If you look at the source for `TextGenerationPipeline` (which our pipeline\
          \ is somewhat derived from), it has an option `handle_long_generation`:\n\
          \n```\n            handle_long_generation (`str`, *optional*):\n       \
          \         By default, this pipelines does not handle long generation (ones\
          \ that exceed in one form or the other\n                the model maximum\
          \ length). There is no perfect way to adress this (more info\n         \
          \       :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227).\
          \ This provides common\n                strategies to work around that problem\
          \ depending on your use case.\n\n                - `None` : default strategy\
          \ where nothing in particular happens\n                - `\"hole\"`: Truncates\
          \ left of input, and leaves a gap wide enough to let generation happen (might\n\
          \                  truncate a lot of the prompt and not suitable when generation\
          \ exceed the model capacity)\n```\n\nWe could do something similar, where\
          \ basically we truncate the instruction before it is formatted into the\
          \ full prompt.  This isn't a perfect solution but could be an okay workaround."
        updatedAt: '2023-04-27T17:27:35.562Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - dfurmanWMP
    id: 644ab087af97dfd24c0eced4
    type: comment
  author: matthayes
  content: "If you look at the source for `TextGenerationPipeline` (which our pipeline\
    \ is somewhat derived from), it has an option `handle_long_generation`:\n\n```\n\
    \            handle_long_generation (`str`, *optional*):\n                By default,\
    \ this pipelines does not handle long generation (ones that exceed in one form\
    \ or the other\n                the model maximum length). There is no perfect\
    \ way to adress this (more info\n                :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227).\
    \ This provides common\n                strategies to work around that problem\
    \ depending on your use case.\n\n                - `None` : default strategy where\
    \ nothing in particular happens\n                - `\"hole\"`: Truncates left\
    \ of input, and leaves a gap wide enough to let generation happen (might\n   \
    \               truncate a lot of the prompt and not suitable when generation\
    \ exceed the model capacity)\n```\n\nWe could do something similar, where basically\
    \ we truncate the instruction before it is formatted into the full prompt.  This\
    \ isn't a perfect solution but could be an okay workaround."
  created_at: 2023-04-27 16:27:35+00:00
  edited: false
  hidden: false
  id: 644ab087af97dfd24c0eced4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-27T17:29:29.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Yeah 2048 is still "too large" but less "too large" than the Very
          Large Integer it is now. It would at least fail more cleanly I think for
          most of the cases that trigger it.</p>

          '
        raw: Yeah 2048 is still "too large" but less "too large" than the Very Large
          Integer it is now. It would at least fail more cleanly I think for most
          of the cases that trigger it.
        updatedAt: '2023-04-27T17:29:29.341Z'
      numEdits: 0
      reactions: []
    id: 644ab0f9cb45734dfd44c315
    type: comment
  author: srowen
  content: Yeah 2048 is still "too large" but less "too large" than the Very Large
    Integer it is now. It would at least fail more cleanly I think for most of the
    cases that trigger it.
  created_at: 2023-04-27 16:29:29+00:00
  edited: false
  hidden: false
  id: 644ab0f9cb45734dfd44c315
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-27T17:33:24.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<blockquote>

          <p>so we should basically set it to 1769?</p>

          </blockquote>

          <p>At the moment if you are using the <code>InstructionTextGenerationPipeline</code>
          as is I think you should truncate the text before passing it into the pipeline.  Unfortunately
          it is a bit tricky to know what you should truncate it to, as you don''t
          know the length until you tokenize it.  You could tokenize, truncate, decode,
          and pass that into the pipeline.  If you are customizing <code>InstructionTextGenerationPipeline</code>
          then I suggest truncating the instruction preprocess before it is formatted,
          and derive the max length from 2048 - length of prompt - max_new tokens.  You
          could get the prompt length by computing the length of <code>PROMPT_FOR_GENERATION_FORMAT.format(instruction="")</code>
          for example.</p>

          '
        raw: '>so we should basically set it to 1769?


          At the moment if you are using the `InstructionTextGenerationPipeline` as
          is I think you should truncate the text before passing it into the pipeline.  Unfortunately
          it is a bit tricky to know what you should truncate it to, as you don''t
          know the length until you tokenize it.  You could tokenize, truncate, decode,
          and pass that into the pipeline.  If you are customizing `InstructionTextGenerationPipeline`
          then I suggest truncating the instruction preprocess before it is formatted,
          and derive the max length from 2048 - length of prompt - max_new tokens.  You
          could get the prompt length by computing the length of `PROMPT_FOR_GENERATION_FORMAT.format(instruction="")`
          for example.'
        updatedAt: '2023-04-27T17:33:24.759Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dfurmanWMP
    id: 644ab1e4d4483bfaa06d51a2
    type: comment
  author: matthayes
  content: '>so we should basically set it to 1769?


    At the moment if you are using the `InstructionTextGenerationPipeline` as is I
    think you should truncate the text before passing it into the pipeline.  Unfortunately
    it is a bit tricky to know what you should truncate it to, as you don''t know
    the length until you tokenize it.  You could tokenize, truncate, decode, and pass
    that into the pipeline.  If you are customizing `InstructionTextGenerationPipeline`
    then I suggest truncating the instruction preprocess before it is formatted, and
    derive the max length from 2048 - length of prompt - max_new tokens.  You could
    get the prompt length by computing the length of `PROMPT_FOR_GENERATION_FORMAT.format(instruction="")`
    for example.'
  created_at: 2023-04-27 16:33:24+00:00
  edited: false
  hidden: false
  id: 644ab1e4d4483bfaa06d51a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T17:33:32.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: '<p>It is pretty odd that a lot of the new models don''t seem to add
          model_max_length (like the new stabilityai models don''t either, it is that
          same large int) but other models (like flan-t5/flan-ul2) do have that in
          there. I wonder if this is something the Hugging Face team should check
          out, seems odd to default to some really larger integer...</p>

          '
        raw: It is pretty odd that a lot of the new models don't seem to add model_max_length
          (like the new stabilityai models don't either, it is that same large int)
          but other models (like flan-t5/flan-ul2) do have that in there. I wonder
          if this is something the Hugging Face team should check out, seems odd to
          default to some really larger integer...
        updatedAt: '2023-04-27T17:33:32.246Z'
      numEdits: 0
      reactions: []
    id: 644ab1ecaf97dfd24c0eec77
    type: comment
  author: dfurmanWMP
  content: It is pretty odd that a lot of the new models don't seem to add model_max_length
    (like the new stabilityai models don't either, it is that same large int) but
    other models (like flan-t5/flan-ul2) do have that in there. I wonder if this is
    something the Hugging Face team should check out, seems odd to default to some
    really larger integer...
  created_at: 2023-04-27 16:33:32+00:00
  edited: false
  hidden: false
  id: 644ab1ecaf97dfd24c0eec77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T17:34:29.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;matthayes&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/matthayes\"\
          >@<span class=\"underline\">matthayes</span></a></span>\n\n\t</span></span>,\
          \ that makes sense!</p>\n"
        raw: Thanks @matthayes, that makes sense!
        updatedAt: '2023-04-27T17:34:29.370Z'
      numEdits: 0
      reactions: []
    id: 644ab225d4483bfaa06d55f3
    type: comment
  author: dfurmanWMP
  content: Thanks @matthayes, that makes sense!
  created_at: 2023-04-27 16:34:29+00:00
  edited: false
  hidden: false
  id: 644ab225d4483bfaa06d55f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-04-27T17:35:37.000Z'
    data:
      edited: false
      editors:
      - dfurmanWMP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: '<p>Maybe I will just for now hard code a raise Exception within the
          InstructionTextGenerationPipeline directly before the model call (to err
          if len token ids is less than 2048).</p>

          '
        raw: Maybe I will just for now hard code a raise Exception within the InstructionTextGenerationPipeline
          directly before the model call (to err if len token ids is less than 2048).
        updatedAt: '2023-04-27T17:35:37.082Z'
      numEdits: 0
      reactions: []
    id: 644ab269af97dfd24c0ef489
    type: comment
  author: dfurmanWMP
  content: Maybe I will just for now hard code a raise Exception within the InstructionTextGenerationPipeline
    directly before the model call (to err if len token ids is less than 2048).
  created_at: 2023-04-27 16:35:37+00:00
  edited: false
  hidden: false
  id: 644ab269af97dfd24c0ef489
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: databricks/dolly-v2-12b
repo_type: model
status: open
target_branch: null
title: model_max_length in tokenizer_config.json is incorrect
