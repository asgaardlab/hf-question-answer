!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JoggyMuffin
conflicting_files: null
created_at: 2024-01-13 01:22:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
      fullname: John Banks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoggyMuffin
      type: user
    createdAt: '2024-01-13T01:22:09.000Z'
    data:
      edited: false
      editors:
      - JoggyMuffin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9631186127662659
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
          fullname: John Banks
          isHf: false
          isPro: false
          name: JoggyMuffin
          type: user
        html: '<p>After trying it for a bit, it gives LZLV a run for its money. I
          would have given up on it quickly after a lot of weirdness with chat formatting
          and speaking out of turn but the provided preset works quite well for keeping
          it in check, thank you for providing it in a convenient format.</p>

          '
        raw: After trying it for a bit, it gives LZLV a run for its money. I would
          have given up on it quickly after a lot of weirdness with chat formatting
          and speaking out of turn but the provided preset works quite well for keeping
          it in check, thank you for providing it in a convenient format.
        updatedAt: '2024-01-13T01:22:09.617Z'
      numEdits: 0
      reactions: []
    id: 65a1e5c181a46e7dd9bc69d8
    type: comment
  author: JoggyMuffin
  content: After trying it for a bit, it gives LZLV a run for its money. I would have
    given up on it quickly after a lot of weirdness with chat formatting and speaking
    out of turn but the provided preset works quite well for keeping it in check,
    thank you for providing it in a convenient format.
  created_at: 2024-01-13 01:22:09+00:00
  edited: false
  hidden: false
  id: 65a1e5c181a46e7dd9bc69d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
      fullname: John Banks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoggyMuffin
      type: user
    createdAt: '2024-01-13T03:16:26.000Z'
    data:
      edited: true
      editors:
      - JoggyMuffin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361274838447571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
          fullname: John Banks
          isHf: false
          isPro: false
          name: JoggyMuffin
          type: user
        html: '<p>Some things I''ve noticed and tried to tweak the preset for, though:<br>-Failure
          to add asterisks for story descriptions, the preset only specifies asterisks
          for internal dialogue, but not stuff happening in the story. This may cause
          the model to leave them off, especially at the start of a reply.<br>-Generating
          multiple replies in a row. The model will produce a full reply without issues
          but proceed to generate a second reply from char, including the {{char}}:.
          I think this is fixed by telling the model to only produce a single reply.<br>-Copying
          character cards verbatim for replies or very nearly so</p>

          '
        raw: 'Some things I''ve noticed and tried to tweak the preset for, though:

          -Failure to add asterisks for story descriptions, the preset only specifies
          asterisks for internal dialogue, but not stuff happening in the story. This
          may cause the model to leave them off, especially at the start of a reply.

          -Generating multiple replies in a row. The model will produce a full reply
          without issues but proceed to generate a second reply from char, including
          the {{char}}:. I think this is fixed by telling the model to only produce
          a single reply.

          -Copying character cards verbatim for replies or very nearly so'
        updatedAt: '2024-01-13T03:16:58.220Z'
      numEdits: 1
      reactions: []
    id: 65a2008a680cb2eb9497c78e
    type: comment
  author: JoggyMuffin
  content: 'Some things I''ve noticed and tried to tweak the preset for, though:

    -Failure to add asterisks for story descriptions, the preset only specifies asterisks
    for internal dialogue, but not stuff happening in the story. This may cause the
    model to leave them off, especially at the start of a reply.

    -Generating multiple replies in a row. The model will produce a full reply without
    issues but proceed to generate a second reply from char, including the {{char}}:.
    I think this is fixed by telling the model to only produce a single reply.

    -Copying character cards verbatim for replies or very nearly so'
  created_at: 2024-01-13 03:16:26+00:00
  edited: true
  hidden: false
  id: 65a2008a680cb2eb9497c78e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-13T05:20:17.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9783174991607666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>I''m glad you''re enjoying the model. It''s funny you should mention
          LZLV because I''m close to releasing a new merge that uses that model in
          the blend instead of Xwin and it''s really good. Expect something in the
          next week.</p>

          <p>I haven''t spent much time using the 70B version of this model, but I''ve
          gotten better results from the 103B version by playing around with temp
          and min-p. I didn''t understand how SillyTavern measures min-p when I made
          my recommendation initially. It''s inverted from what I put, so try a min-p
          setting of 0.8 instead of 0.2 and see if you get better results. You may
          also want to lower temp a little bit, but the higher you go with min-p,
          the higher you can push the temperature and still get good results. I''ll
          update the model card to reflect some of these changes. Hopefully that resolves
          some of the problems you''ve had.</p>

          <p>All that being said, this model is fundamentally not perfect. You''ll
          probably always have some issues with it around the edges. Stay tuned for
          its successor.</p>

          '
        raw: 'I''m glad you''re enjoying the model. It''s funny you should mention
          LZLV because I''m close to releasing a new merge that uses that model in
          the blend instead of Xwin and it''s really good. Expect something in the
          next week.


          I haven''t spent much time using the 70B version of this model, but I''ve
          gotten better results from the 103B version by playing around with temp
          and min-p. I didn''t understand how SillyTavern measures min-p when I made
          my recommendation initially. It''s inverted from what I put, so try a min-p
          setting of 0.8 instead of 0.2 and see if you get better results. You may
          also want to lower temp a little bit, but the higher you go with min-p,
          the higher you can push the temperature and still get good results. I''ll
          update the model card to reflect some of these changes. Hopefully that resolves
          some of the problems you''ve had.


          All that being said, this model is fundamentally not perfect. You''ll probably
          always have some issues with it around the edges. Stay tuned for its successor.'
        updatedAt: '2024-01-13T05:20:17.338Z'
      numEdits: 0
      reactions: []
    id: 65a21d913e3a00a9c1c240d0
    type: comment
  author: sophosympatheia
  content: 'I''m glad you''re enjoying the model. It''s funny you should mention LZLV
    because I''m close to releasing a new merge that uses that model in the blend
    instead of Xwin and it''s really good. Expect something in the next week.


    I haven''t spent much time using the 70B version of this model, but I''ve gotten
    better results from the 103B version by playing around with temp and min-p. I
    didn''t understand how SillyTavern measures min-p when I made my recommendation
    initially. It''s inverted from what I put, so try a min-p setting of 0.8 instead
    of 0.2 and see if you get better results. You may also want to lower temp a little
    bit, but the higher you go with min-p, the higher you can push the temperature
    and still get good results. I''ll update the model card to reflect some of these
    changes. Hopefully that resolves some of the problems you''ve had.


    All that being said, this model is fundamentally not perfect. You''ll probably
    always have some issues with it around the edges. Stay tuned for its successor.'
  created_at: 2024-01-13 05:20:17+00:00
  edited: false
  hidden: false
  id: 65a21d913e3a00a9c1c240d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
      fullname: John Banks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoggyMuffin
      type: user
    createdAt: '2024-01-14T00:09:38.000Z'
    data:
      edited: false
      editors:
      - JoggyMuffin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9709516167640686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
          fullname: John Banks
          isHf: false
          isPro: false
          name: JoggyMuffin
          type: user
        html: '<p>I''ve tried the updated gen preset but have been getting very repetitive
          results. It will repeat almost verbatim the same phrase several times over
          several paragraphs.</p>

          '
        raw: I've tried the updated gen preset but have been getting very repetitive
          results. It will repeat almost verbatim the same phrase several times over
          several paragraphs.
        updatedAt: '2024-01-14T00:09:38.567Z'
      numEdits: 0
      reactions: []
    id: 65a3264281a46e7dd93dd0a3
    type: comment
  author: JoggyMuffin
  content: I've tried the updated gen preset but have been getting very repetitive
    results. It will repeat almost verbatim the same phrase several times over several
    paragraphs.
  created_at: 2024-01-14 00:09:38+00:00
  edited: false
  hidden: false
  id: 65a3264281a46e7dd93dd0a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-14T02:11:49.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580838084220886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>How are you loading the model and what context length are you using?
          I''ve only ever tested the 70B model using my own exllama2 quant at 4.85bpw.
          It''s possible that other quantizations make this problem worse. If you
          can tell me your exact model version and sampler settings, I''ll give it
          a try on my end and see what''s up with it.</p>

          '
        raw: How are you loading the model and what context length are you using?
          I've only ever tested the 70B model using my own exllama2 quant at 4.85bpw.
          It's possible that other quantizations make this problem worse. If you can
          tell me your exact model version and sampler settings, I'll give it a try
          on my end and see what's up with it.
        updatedAt: '2024-01-14T02:11:49.769Z'
      numEdits: 0
      reactions: []
    id: 65a342e5c0e637bd9c10bfe5
    type: comment
  author: sophosympatheia
  content: How are you loading the model and what context length are you using? I've
    only ever tested the 70B model using my own exllama2 quant at 4.85bpw. It's possible
    that other quantizations make this problem worse. If you can tell me your exact
    model version and sampler settings, I'll give it a try on my end and see what's
    up with it.
  created_at: 2024-01-14 02:11:49+00:00
  edited: false
  hidden: false
  id: 65a342e5c0e637bd9c10bfe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
      fullname: John Banks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoggyMuffin
      type: user
    createdAt: '2024-01-14T07:58:17.000Z'
    data:
      edited: false
      editors:
      - JoggyMuffin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7234281897544861
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
          fullname: John Banks
          isHf: false
          isPro: false
          name: JoggyMuffin
          type: user
        html: '<p>I''m using llamacpp_HF at q3_K_M GGUF</p>

          '
        raw: I'm using llamacpp_HF at q3_K_M GGUF
        updatedAt: '2024-01-14T07:58:17.168Z'
      numEdits: 0
      reactions: []
    id: 65a3941960cc6b04c964c77c
    type: comment
  author: JoggyMuffin
  content: I'm using llamacpp_HF at q3_K_M GGUF
  created_at: 2024-01-14 07:58:17+00:00
  edited: false
  hidden: false
  id: 65a3941960cc6b04c964c77c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-15T01:44:06.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411599040031433
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>I tried TheBloke''s q3_K_M GGUF quant using the llamacpp_HF loader
          and it worked well for me at 4096 context, at least in a quick test scenario
          where there was already context for the model to draw upon. (In other words,
          I ran it in the middle of a chat I already had going.) Did you encounter
          problems near the start of a chat where there isn''t much context to guide
          the model? I recommend experimenting with the sampler settings and the system
          prompt, and providing additional contextual guidance to the model, to see
          if that alleviates the issues you reported. As best I can tell, the issues
          you encountered should be solvable, partially or fully, by experimenting
          with those settings.<br>Also, what are you using for the tokenizer? I don''t
          mess around with llamacpp very often, and I noticed that the llamacpp_hf
          loader presents you with two options: 1) place copies of the model''s tokenizer
          files into a directory with the .gguf file or 2) use a generic tokenizer
          provided by oobabooga. Since I have the full model on my local drive, I
          chose option 1 and copied over the four tokenizer files. If you''re using
          option 2, that could be another factor.<br>Just throwing out ideas. If you
          make a breakthrough, I''d be curious to know what solved the problems for
          you.</p>

          '
        raw: 'I tried TheBloke''s q3_K_M GGUF quant using the llamacpp_HF loader and
          it worked well for me at 4096 context, at least in a quick test scenario
          where there was already context for the model to draw upon. (In other words,
          I ran it in the middle of a chat I already had going.) Did you encounter
          problems near the start of a chat where there isn''t much context to guide
          the model? I recommend experimenting with the sampler settings and the system
          prompt, and providing additional contextual guidance to the model, to see
          if that alleviates the issues you reported. As best I can tell, the issues
          you encountered should be solvable, partially or fully, by experimenting
          with those settings.

          Also, what are you using for the tokenizer? I don''t mess around with llamacpp
          very often, and I noticed that the llamacpp_hf loader presents you with
          two options: 1) place copies of the model''s tokenizer files into a directory
          with the .gguf file or 2) use a generic tokenizer provided by oobabooga.
          Since I have the full model on my local drive, I chose option 1 and copied
          over the four tokenizer files. If you''re using option 2, that could be
          another factor.

          Just throwing out ideas. If you make a breakthrough, I''d be curious to
          know what solved the problems for you.'
        updatedAt: '2024-01-15T01:44:06.696Z'
      numEdits: 0
      reactions: []
    id: 65a48de6d0e350dbc9668b90
    type: comment
  author: sophosympatheia
  content: 'I tried TheBloke''s q3_K_M GGUF quant using the llamacpp_HF loader and
    it worked well for me at 4096 context, at least in a quick test scenario where
    there was already context for the model to draw upon. (In other words, I ran it
    in the middle of a chat I already had going.) Did you encounter problems near
    the start of a chat where there isn''t much context to guide the model? I recommend
    experimenting with the sampler settings and the system prompt, and providing additional
    contextual guidance to the model, to see if that alleviates the issues you reported.
    As best I can tell, the issues you encountered should be solvable, partially or
    fully, by experimenting with those settings.

    Also, what are you using for the tokenizer? I don''t mess around with llamacpp
    very often, and I noticed that the llamacpp_hf loader presents you with two options:
    1) place copies of the model''s tokenizer files into a directory with the .gguf
    file or 2) use a generic tokenizer provided by oobabooga. Since I have the full
    model on my local drive, I chose option 1 and copied over the four tokenizer files.
    If you''re using option 2, that could be another factor.

    Just throwing out ideas. If you make a breakthrough, I''d be curious to know what
    solved the problems for you.'
  created_at: 2024-01-15 01:44:06+00:00
  edited: false
  hidden: false
  id: 65a48de6d0e350dbc9668b90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
      fullname: John Banks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoggyMuffin
      type: user
    createdAt: '2024-01-15T04:09:26.000Z'
    data:
      edited: true
      editors:
      - JoggyMuffin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9803940057754517
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff7b8008814bcfdf06b8f964c6bbf9c3.svg
          fullname: John Banks
          isHf: false
          isPro: false
          name: JoggyMuffin
          type: user
        html: '<p>I''m using the tokenizer included in your repo, and the loader picks
          those up. After more testing, it appears that the repetition issue I described
          was a fluke, but has led me to finding that certain scenarios cause the
          model to have a single almost certain output, with very minor variations
          between regens, and very resistant to changes due to generation settings.
          I think this is what I observed with the highly repetitive outputs before,
          that situation caused the model to produce an almost identical reply for
          that given case regardless of settings, which just happened to be a bad
          reply with high repetition. The model still has issues with generating user
          dialogue despite my best efforts to dissuade it in the system prompt and
          negative CFG, but neither of them seem to have great influence on "stuck"
          outputs. I''ll keep messing with it.</p>

          '
        raw: I'm using the tokenizer included in your repo, and the loader picks those
          up. After more testing, it appears that the repetition issue I described
          was a fluke, but has led me to finding that certain scenarios cause the
          model to have a single almost certain output, with very minor variations
          between regens, and very resistant to changes due to generation settings.
          I think this is what I observed with the highly repetitive outputs before,
          that situation caused the model to produce an almost identical reply for
          that given case regardless of settings, which just happened to be a bad
          reply with high repetition. The model still has issues with generating user
          dialogue despite my best efforts to dissuade it in the system prompt and
          negative CFG, but neither of them seem to have great influence on "stuck"
          outputs. I'll keep messing with it.
        updatedAt: '2024-01-15T04:10:47.748Z'
      numEdits: 1
      reactions: []
    id: 65a4aff68448f47df2d47473
    type: comment
  author: JoggyMuffin
  content: I'm using the tokenizer included in your repo, and the loader picks those
    up. After more testing, it appears that the repetition issue I described was a
    fluke, but has led me to finding that certain scenarios cause the model to have
    a single almost certain output, with very minor variations between regens, and
    very resistant to changes due to generation settings. I think this is what I observed
    with the highly repetitive outputs before, that situation caused the model to
    produce an almost identical reply for that given case regardless of settings,
    which just happened to be a bad reply with high repetition. The model still has
    issues with generating user dialogue despite my best efforts to dissuade it in
    the system prompt and negative CFG, but neither of them seem to have great influence
    on "stuck" outputs. I'll keep messing with it.
  created_at: 2024-01-15 04:09:26+00:00
  edited: true
  hidden: false
  id: 65a4aff68448f47df2d47473
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-15T15:54:25.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981708824634552
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>I''m glad the repetition issue seems to be limited. When you describe
          it like that, I remember seeing that behavior myself a few times. It seems
          to be the model''s knee jerk reaction when it doesn''t know what to do to
          continue the chat from that point. I seem to recall I was able to break
          it out of that state by providing it with a system message to clarify what
          I wanted it to do next.<br>As for the tendency to write text as you, the
          user, I feel you there. I would say that is a known weakness of the model.
          You can suppress it mostly with prompting and manipulating sampler settings,
          but you''ll probably still encounter it sometimes.<br>Thanks for sharing
          your experiences with the model. This feedback helps me identify which issues
          are affecting people, and I take that into consideration when tuning future
          merges.</p>

          '
        raw: 'I''m glad the repetition issue seems to be limited. When you describe
          it like that, I remember seeing that behavior myself a few times. It seems
          to be the model''s knee jerk reaction when it doesn''t know what to do to
          continue the chat from that point. I seem to recall I was able to break
          it out of that state by providing it with a system message to clarify what
          I wanted it to do next.

          As for the tendency to write text as you, the user, I feel you there. I
          would say that is a known weakness of the model. You can suppress it mostly
          with prompting and manipulating sampler settings, but you''ll probably still
          encounter it sometimes.

          Thanks for sharing your experiences with the model. This feedback helps
          me identify which issues are affecting people, and I take that into consideration
          when tuning future merges.'
        updatedAt: '2024-01-15T15:54:25.852Z'
      numEdits: 0
      reactions: []
    id: 65a55531895d1eca73239c3f
    type: comment
  author: sophosympatheia
  content: 'I''m glad the repetition issue seems to be limited. When you describe
    it like that, I remember seeing that behavior myself a few times. It seems to
    be the model''s knee jerk reaction when it doesn''t know what to do to continue
    the chat from that point. I seem to recall I was able to break it out of that
    state by providing it with a system message to clarify what I wanted it to do
    next.

    As for the tendency to write text as you, the user, I feel you there. I would
    say that is a known weakness of the model. You can suppress it mostly with prompting
    and manipulating sampler settings, but you''ll probably still encounter it sometimes.

    Thanks for sharing your experiences with the model. This feedback helps me identify
    which issues are affecting people, and I take that into consideration when tuning
    future merges.'
  created_at: 2024-01-15 15:54:25+00:00
  edited: false
  hidden: false
  id: 65a55531895d1eca73239c3f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: sophosympatheia/Aurora-Nights-70B-v1.0
repo_type: model
status: open
target_branch: null
title: Very good model
