!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-05-12 05:10:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-12T06:10:37.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: "<p>Hello, I have download the model and try to run on koboldcpp. But\
          \ it does not work.<br>I have checked the SHA256 and confirm the file is\
          \ completed.</p>\n<pre><code># in llama.cpp\nerror loading model: unrecognized\
          \ tensor type 7\n\n#in koboldcpp\nInput: {\"n\": 1, \"max_context_length\"\
          : 2048, \"max_length\": 256, \"rep_pen\": 1.15, \"temperature\": 1, \"top_p\"\
          : 0.1, \"top_k\": 0, \"top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\"\
          : 1024, \"rep_pen_slope\": 0.7, \"sampler_order\": [0, 1, 2, 3, 4, 5, 6],\
          \ \"prompt\": \"Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\\n\\n\\n\\n### Instruction:\\\
          n\\n\\u80fd\\u8aaa\\u4e2d\\u6587\\u55ce\\uff1f\\n\\n### Response:\\n\\n\"\
          , \"quiet\": true, \"stop_sequence\": [\"\\n### Instruction:\", \"\\n###\
          \ Response:\"]}\n\nProcessing Prompt [BLAS] (45 / 45 tokens)ggml_new_tensor_impl:\
          \ not enough space in the context's memory pool (needed 819479152, available\
          \ 805306368)\n----------------------------------------\nException occurred\
          \ during processing of request from ('127.0.0.1', 57955)\nTraceback (most\
          \ recent call last):\n  File \"socketserver.py\", line 316, in _handle_request_noblock\n\
          \  File \"socketserver.py\", line 347, in process_request\n  File \"socketserver.py\"\
          , line 360, in finish_request\n  File \"koboldcpp.py\", line 196, in __call__\n\
          \  File \"http\\server.py\", line 651, in __init__\n  File \"socketserver.py\"\
          , line 747, in __init__\n  File \"http\\server.py\", line 425, in handle\n\
          \  File \"http\\server.py\", line 413, in handle_one_request\n  File \"\
          koboldcpp.py\", line 297, in do_POST\n  File \"koboldcpp.py\", line 170,\
          \ in generate\nOSError: exception: access violation writing 0x0000000000000000\n\
          ----------------------------------------\n</code></pre>\n"
        raw: "Hello, I have download the model and try to run on koboldcpp. But it\
          \ does not work.\r\nI have checked the SHA256 and confirm the file is completed.\r\
          \n```\r\n# in llama.cpp\r\nerror loading model: unrecognized tensor type\
          \ 7\r\n\r\n#in koboldcpp\r\nInput: {\"n\": 1, \"max_context_length\": 2048,\
          \ \"max_length\": 256, \"rep_pen\": 1.15, \"temperature\": 1, \"top_p\"\
          : 0.1, \"top_k\": 0, \"top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\"\
          : 1024, \"rep_pen_slope\": 0.7, \"sampler_order\": [0, 1, 2, 3, 4, 5, 6],\
          \ \"prompt\": \"Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\\n\\n\\n\\n### Instruction:\\\
          n\\n\\u80fd\\u8aaa\\u4e2d\\u6587\\u55ce\\uff1f\\n\\n### Response:\\n\\n\"\
          , \"quiet\": true, \"stop_sequence\": [\"\\n### Instruction:\", \"\\n###\
          \ Response:\"]}\r\n\r\nProcessing Prompt [BLAS] (45 / 45 tokens)ggml_new_tensor_impl:\
          \ not enough space in the context's memory pool (needed 819479152, available\
          \ 805306368)\r\n----------------------------------------\r\nException occurred\
          \ during processing of request from ('127.0.0.1', 57955)\r\nTraceback (most\
          \ recent call last):\r\n  File \"socketserver.py\", line 316, in _handle_request_noblock\r\
          \n  File \"socketserver.py\", line 347, in process_request\r\n  File \"\
          socketserver.py\", line 360, in finish_request\r\n  File \"koboldcpp.py\"\
          , line 196, in __call__\r\n  File \"http\\server.py\", line 651, in __init__\r\
          \n  File \"socketserver.py\", line 747, in __init__\r\n  File \"http\\server.py\"\
          , line 425, in handle\r\n  File \"http\\server.py\", line 413, in handle_one_request\r\
          \n  File \"koboldcpp.py\", line 297, in do_POST\r\n  File \"koboldcpp.py\"\
          , line 170, in generate\r\nOSError: exception: access violation writing\
          \ 0x0000000000000000\r\n----------------------------------------\r\n```"
        updatedAt: '2023-05-12T06:10:37.590Z'
      numEdits: 0
      reactions: []
    id: 645dd85da30926be63945dd6
    type: comment
  author: FenixInDarkSolo
  content: "Hello, I have download the model and try to run on koboldcpp. But it does\
    \ not work.\r\nI have checked the SHA256 and confirm the file is completed.\r\n\
    ```\r\n# in llama.cpp\r\nerror loading model: unrecognized tensor type 7\r\n\r\
    \n#in koboldcpp\r\nInput: {\"n\": 1, \"max_context_length\": 2048, \"max_length\"\
    : 256, \"rep_pen\": 1.15, \"temperature\": 1, \"top_p\": 0.1, \"top_k\": 0, \"\
    top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\": 1024, \"rep_pen_slope\"\
    : 0.7, \"sampler_order\": [0, 1, 2, 3, 4, 5, 6], \"prompt\": \"Below is an instruction\
    \ that describes a task. Write a response that appropriately completes the request.\\\
    n\\n\\n\\n### Instruction:\\n\\n\\u80fd\\u8aaa\\u4e2d\\u6587\\u55ce\\uff1f\\n\\\
    n### Response:\\n\\n\", \"quiet\": true, \"stop_sequence\": [\"\\n### Instruction:\"\
    , \"\\n### Response:\"]}\r\n\r\nProcessing Prompt [BLAS] (45 / 45 tokens)ggml_new_tensor_impl:\
    \ not enough space in the context's memory pool (needed 819479152, available 805306368)\r\
    \n----------------------------------------\r\nException occurred during processing\
    \ of request from ('127.0.0.1', 57955)\r\nTraceback (most recent call last):\r\
    \n  File \"socketserver.py\", line 316, in _handle_request_noblock\r\n  File \"\
    socketserver.py\", line 347, in process_request\r\n  File \"socketserver.py\"\
    , line 360, in finish_request\r\n  File \"koboldcpp.py\", line 196, in __call__\r\
    \n  File \"http\\server.py\", line 651, in __init__\r\n  File \"socketserver.py\"\
    , line 747, in __init__\r\n  File \"http\\server.py\", line 425, in handle\r\n\
    \  File \"http\\server.py\", line 413, in handle_one_request\r\n  File \"koboldcpp.py\"\
    , line 297, in do_POST\r\n  File \"koboldcpp.py\", line 170, in generate\r\nOSError:\
    \ exception: access violation writing 0x0000000000000000\r\n----------------------------------------\r\
    \n```"
  created_at: 2023-05-12 05:10:37+00:00
  edited: false
  hidden: false
  id: 645dd85da30926be63945dd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643404d84b34368fdb012fad/7TjjzTC57Qp1k1gD1jEsh.png?w=200&h=200&f=face
      fullname: Bill Gao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Billsfriend
      type: user
    createdAt: '2023-05-12T09:24:07.000Z'
    data:
      edited: false
      editors:
      - Billsfriend
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643404d84b34368fdb012fad/7TjjzTC57Qp1k1gD1jEsh.png?w=200&h=200&f=face
          fullname: Bill Gao
          isHf: false
          isPro: false
          name: Billsfriend
          type: user
        html: '<blockquote>

          <p>Hello, I have download the model and try to run on koboldcpp. But it
          does not work.<br>I have checked the SHA256 and confirm the file is completed.</p>

          <pre><code># in llama.cpp

          error loading model: unrecognized tensor type 7

          </code></pre>

          </blockquote>

          <p>llama.cpp quantization methods have been updated in May, please try cloning
          the latest llama.cpp repo and re-compile before loading the model.<br>It
          works well in my device.</p>

          <p>As for koboldcpp, I have not tested it. I would test it when I have time.</p>

          '
        raw: "> Hello, I have download the model and try to run on koboldcpp. But\
          \ it does not work.\n> I have checked the SHA256 and confirm the file is\
          \ completed.\n> ```\n> # in llama.cpp\n> error loading model: unrecognized\
          \ tensor type 7\n>``` \n\nllama.cpp quantization methods have been updated\
          \ in May, please try cloning the latest llama.cpp repo and re-compile before\
          \ loading the model.\nIt works well in my device.\n\nAs for koboldcpp, I\
          \ have not tested it. I would test it when I have time."
        updatedAt: '2023-05-12T09:24:07.177Z'
      numEdits: 0
      reactions: []
    id: 645e05b79e8b99feea8617b1
    type: comment
  author: Billsfriend
  content: "> Hello, I have download the model and try to run on koboldcpp. But it\
    \ does not work.\n> I have checked the SHA256 and confirm the file is completed.\n\
    > ```\n> # in llama.cpp\n> error loading model: unrecognized tensor type 7\n>```\
    \ \n\nllama.cpp quantization methods have been updated in May, please try cloning\
    \ the latest llama.cpp repo and re-compile before loading the model.\nIt works\
    \ well in my device.\n\nAs for koboldcpp, I have not tested it. I would test it\
    \ when I have time."
  created_at: 2023-05-12 08:24:07+00:00
  edited: false
  hidden: false
  id: 645e05b79e8b99feea8617b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-13T06:44:29.000Z'
    data:
      edited: true
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>mmm... I believe I have checkout the latest version of llama.cpp?</p>

          <pre><code>D:\program\llama.cpp&gt;python runner_interactive.py

          main -m ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin -t 12 -n -1 -c 2048
          --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r
          "User:" --keep -1 --interactive-first

          main: build = 536 (cdd5350)

          main: seed  = 1683959650

          llama.cpp: loading model from ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin

          llama_model_load_internal: format     = ggjt v1 (pre #1405)

          llama_model_load_internal: n_vocab    = 49954

          llama_model_load_internal: n_ctx      = 2048

          llama_model_load_internal: n_embd     = 4096

          llama_model_load_internal: n_mult     = 256

          llama_model_load_internal: n_head     = 32

          llama_model_load_internal: n_layer    = 32

          llama_model_load_internal: n_rot      = 128

          llama_model_load_internal: ftype      = 9 (mostly Q5_1)

          llama_model_load_internal: n_ff       = 11008

          llama_model_load_internal: n_parts    = 1

          llama_model_load_internal: model size = 7B

          error loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)

          llama_init_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin''

          main: error: unable to load model


          D:\program\llama.cpp&gt;git log -n 1 --pretty=format:''%H''

          ''cdd5350892b1d4e521e930c77341f858fcfcd433''


          D:\program\llama.cpp&gt;git merge fb62f924336c9746da9976c6ab3c2e6460258d54

          Already up to date.

          </code></pre>

          <p>And I have tested in the newest koboldcpp (1.21) and it works with a
          warning.</p>

          <pre><code>System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI =
          0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |

          llama.cpp: loading model from D:\program\koboldcpp\chinese-Alpaca-7b-plus-ggml-q5_1.bin

          llama_model_load_internal: format     = ggjt v1 (pre #1405)

          llama_model_load_internal: n_vocab    = 49954

          llama_model_load_internal: n_ctx      = 2048

          llama_model_load_internal: n_embd     = 4096

          llama_model_load_internal: n_mult     = 256

          llama_model_load_internal: n_head     = 32

          llama_model_load_internal: n_layer    = 32

          llama_model_load_internal: n_rot      = 128

          llama_model_load_internal: ftype      = 9 (mostly Q5_1)

          llama_model_load_internal: n_ff       = 11008

          llama_model_load_internal: n_parts    = 1

          llama_model_load_internal: model size = 7B


          Legacy LLAMA GGJT compatability changes triggered.

          llama_model_load_internal: ggml ctx size =  68.20 KB

          llama_model_load_internal: mem required  = 6749.78 MB (+ 1026.00 MB per
          state)

          llama_init_from_file: kv self size  = 1024.00 MB


          ---

          Warning: Your model has an INVALID or OUTDATED format (ver 3). Please reconvert
          it for better results!

          ---

          Load Model OK: True

          Embedded Kobold Lite loaded.

          Starting Kobold HTTP Server on port 5001

          </code></pre>

          '
        raw: 'mmm... I believe I have checkout the latest version of llama.cpp?

          ```

          D:\program\llama.cpp>python runner_interactive.py

          main -m ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin -t 12 -n -1 -c 2048
          --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r
          "User:" --keep -1 --interactive-first

          main: build = 536 (cdd5350)

          main: seed  = 1683959650

          llama.cpp: loading model from ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin

          llama_model_load_internal: format     = ggjt v1 (pre #1405)

          llama_model_load_internal: n_vocab    = 49954

          llama_model_load_internal: n_ctx      = 2048

          llama_model_load_internal: n_embd     = 4096

          llama_model_load_internal: n_mult     = 256

          llama_model_load_internal: n_head     = 32

          llama_model_load_internal: n_layer    = 32

          llama_model_load_internal: n_rot      = 128

          llama_model_load_internal: ftype      = 9 (mostly Q5_1)

          llama_model_load_internal: n_ff       = 11008

          llama_model_load_internal: n_parts    = 1

          llama_model_load_internal: model size = 7B

          error loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)

          llama_init_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin''

          main: error: unable to load model


          D:\program\llama.cpp>git log -n 1 --pretty=format:''%H''

          ''cdd5350892b1d4e521e930c77341f858fcfcd433''


          D:\program\llama.cpp>git merge fb62f924336c9746da9976c6ab3c2e6460258d54

          Already up to date.

          ```


          And I have tested in the newest koboldcpp (1.21) and it works with a warning.

          ```

          System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |

          llama.cpp: loading model from D:\program\koboldcpp\chinese-Alpaca-7b-plus-ggml-q5_1.bin

          llama_model_load_internal: format     = ggjt v1 (pre #1405)

          llama_model_load_internal: n_vocab    = 49954

          llama_model_load_internal: n_ctx      = 2048

          llama_model_load_internal: n_embd     = 4096

          llama_model_load_internal: n_mult     = 256

          llama_model_load_internal: n_head     = 32

          llama_model_load_internal: n_layer    = 32

          llama_model_load_internal: n_rot      = 128

          llama_model_load_internal: ftype      = 9 (mostly Q5_1)

          llama_model_load_internal: n_ff       = 11008

          llama_model_load_internal: n_parts    = 1

          llama_model_load_internal: model size = 7B


          Legacy LLAMA GGJT compatability changes triggered.

          llama_model_load_internal: ggml ctx size =  68.20 KB

          llama_model_load_internal: mem required  = 6749.78 MB (+ 1026.00 MB per
          state)

          llama_init_from_file: kv self size  = 1024.00 MB


          ---

          Warning: Your model has an INVALID or OUTDATED format (ver 3). Please reconvert
          it for better results!

          ---

          Load Model OK: True

          Embedded Kobold Lite loaded.

          Starting Kobold HTTP Server on port 5001

          ```'
        updatedAt: '2023-05-13T08:52:20.587Z'
      numEdits: 1
      reactions: []
    id: 645f31cd4357049a57f1ca65
    type: comment
  author: FenixInDarkSolo
  content: 'mmm... I believe I have checkout the latest version of llama.cpp?

    ```

    D:\program\llama.cpp>python runner_interactive.py

    main -m ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin -t 12 -n -1 -c 2048 --keep
    -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r "User:" --keep
    -1 --interactive-first

    main: build = 536 (cdd5350)

    main: seed  = 1683959650

    llama.cpp: loading model from ./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin

    llama_model_load_internal: format     = ggjt v1 (pre #1405)

    llama_model_load_internal: n_vocab    = 49954

    llama_model_load_internal: n_ctx      = 2048

    llama_model_load_internal: n_embd     = 4096

    llama_model_load_internal: n_mult     = 256

    llama_model_load_internal: n_head     = 32

    llama_model_load_internal: n_layer    = 32

    llama_model_load_internal: n_rot      = 128

    llama_model_load_internal: ftype      = 9 (mostly Q5_1)

    llama_model_load_internal: n_ff       = 11008

    llama_model_load_internal: n_parts    = 1

    llama_model_load_internal: model size = 7B

    error loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)

    llama_init_from_file: failed to load model

    llama_init_from_gpt_params: error: failed to load model ''./models/chinese-Alpaca-7b-plus-ggml-q5_1.bin''

    main: error: unable to load model


    D:\program\llama.cpp>git log -n 1 --pretty=format:''%H''

    ''cdd5350892b1d4e521e930c77341f858fcfcd433''


    D:\program\llama.cpp>git merge fb62f924336c9746da9976c6ab3c2e6460258d54

    Already up to date.

    ```


    And I have tested in the newest koboldcpp (1.21) and it works with a warning.

    ```

    System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI =
    0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0
    | BLAS = 1 | SSE3 = 1 | VSX = 0 |

    llama.cpp: loading model from D:\program\koboldcpp\chinese-Alpaca-7b-plus-ggml-q5_1.bin

    llama_model_load_internal: format     = ggjt v1 (pre #1405)

    llama_model_load_internal: n_vocab    = 49954

    llama_model_load_internal: n_ctx      = 2048

    llama_model_load_internal: n_embd     = 4096

    llama_model_load_internal: n_mult     = 256

    llama_model_load_internal: n_head     = 32

    llama_model_load_internal: n_layer    = 32

    llama_model_load_internal: n_rot      = 128

    llama_model_load_internal: ftype      = 9 (mostly Q5_1)

    llama_model_load_internal: n_ff       = 11008

    llama_model_load_internal: n_parts    = 1

    llama_model_load_internal: model size = 7B


    Legacy LLAMA GGJT compatability changes triggered.

    llama_model_load_internal: ggml ctx size =  68.20 KB

    llama_model_load_internal: mem required  = 6749.78 MB (+ 1026.00 MB per state)

    llama_init_from_file: kv self size  = 1024.00 MB


    ---

    Warning: Your model has an INVALID or OUTDATED format (ver 3). Please reconvert
    it for better results!

    ---

    Load Model OK: True

    Embedded Kobold Lite loaded.

    Starting Kobold HTTP Server on port 5001

    ```'
  created_at: 2023-05-13 05:44:29+00:00
  edited: true
  hidden: false
  id: 645f31cd4357049a57f1ca65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643404d84b34368fdb012fad/7TjjzTC57Qp1k1gD1jEsh.png?w=200&h=200&f=face
      fullname: Bill Gao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Billsfriend
      type: user
    createdAt: '2023-05-13T16:00:38.000Z'
    data:
      edited: true
      editors:
      - Billsfriend
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643404d84b34368fdb012fad/7TjjzTC57Qp1k1gD1jEsh.png?w=200&h=200&f=face
          fullname: Bill Gao
          isHf: false
          isPro: false
          name: Billsfriend
          type: user
        html: '<p>check your own output:</p>

          <blockquote>

          <pre><code>error loading model: this format is no longer supported (see
          https://github.com/ggerganov/llama.cpp/pull/1305)

          </code></pre>

          </blockquote>

          <p>llama.cpp developers just updated their Quantization formats Q4 and Q5
          in May 11th, and old q5_1 no longer supported.<br>Maybe try one of them:</p>

          <ol>

          <li>Clone the latest repo, re-compile and re-quantize yourself.</li>

          <li>Load q8_0 format model.</li>

          <li>Clone a old repo before May 11th and re-compile to load q5_1 model I
          provided.</li>

          </ol>

          '
        raw: 'check your own output:

          >```

          > error loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)

          >```


          llama.cpp developers just updated their Quantization formats Q4 and Q5 in
          May 11th, and old q5_1 no longer supported.

          Maybe try one of them:

          1. Clone the latest repo, re-compile and re-quantize yourself.

          2. Load q8_0 format model.

          3. Clone a old repo before May 11th and re-compile to load q5_1 model I
          provided.'
        updatedAt: '2023-05-13T16:01:19.690Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FenixInDarkSolo
    id: 645fb4264357049a57f59558
    type: comment
  author: Billsfriend
  content: 'check your own output:

    >```

    > error loading model: this format is no longer supported (see https://github.com/ggerganov/llama.cpp/pull/1305)

    >```


    llama.cpp developers just updated their Quantization formats Q4 and Q5 in May
    11th, and old q5_1 no longer supported.

    Maybe try one of them:

    1. Clone the latest repo, re-compile and re-quantize yourself.

    2. Load q8_0 format model.

    3. Clone a old repo before May 11th and re-compile to load q5_1 model I provided.'
  created_at: 2023-05-13 15:00:38+00:00
  edited: true
  hidden: false
  id: 645fb4264357049a57f59558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-15T09:30:12.000Z'
    data:
      edited: true
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>5_1 success run in koboldcpp 1.21 (instruct mode). q8_0  success
          run in newest llama.cpp. Thanks a lot.  (^_^)b</p>

          '
        raw: 5_1 success run in koboldcpp 1.21 (instruct mode). q8_0  success run
          in newest llama.cpp. Thanks a lot.  (^_^)b
        updatedAt: '2023-05-15T09:57:30.757Z'
      numEdits: 3
      reactions: []
    id: 6461fba4ed3c7e6fa5e34163
    type: comment
  author: FenixInDarkSolo
  content: 5_1 success run in koboldcpp 1.21 (instruct mode). q8_0  success run in
    newest llama.cpp. Thanks a lot.  (^_^)b
  created_at: 2023-05-15 08:30:12+00:00
  edited: true
  hidden: false
  id: 6461fba4ed3c7e6fa5e34163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643404d84b34368fdb012fad/7TjjzTC57Qp1k1gD1jEsh.png?w=200&h=200&f=face
      fullname: Bill Gao
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Billsfriend
      type: user
    createdAt: '2023-05-16T11:11:00.000Z'
    data:
      pinned: true
    id: 646364c495e64061c97b1c8e
    type: pinning-change
  author: Billsfriend
  created_at: 2023-05-16 10:11:00+00:00
  id: 646364c495e64061c97b1c8e
  type: pinning-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Billsfriend/chinese-Alpaca-7b-plus-ggml-q8_0
repo_type: model
status: open
target_branch: null
title: Failed to run at koboldcpp and llama.cpp
