!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bayang
conflicting_files: []
created_at: 2023-12-09 13:09:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c864114f616cf6ee9d76aee633b01c4.svg
      fullname: BaYang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bayang
      type: user
    createdAt: '2023-12-09T13:09:33.000Z'
    data:
      edited: true
      editors:
      - bayang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24522915482521057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c864114f616cf6ee9d76aee633b01c4.svg
          fullname: BaYang
          isHf: false
          isPro: false
          name: bayang
          type: user
        html: '<p>contains:</p>

          <ul>

          <li>config-flan-t5-xl.json</li>

          <li>model-flan-t5-xl.gguf</li>

          </ul>

          <p>quantization: q6k</p>

          '
        raw: 'contains:

          - config-flan-t5-xl.json

          - model-flan-t5-xl.gguf


          quantization: q6k'
        updatedAt: '2023-12-09T13:14:34.117Z'
      numEdits: 1
      reactions: []
    id: 6574670d14bfb7d56fb21747
    type: comment
  author: bayang
  content: 'contains:

    - config-flan-t5-xl.json

    - model-flan-t5-xl.gguf


    quantization: q6k'
  created_at: 2023-12-09 13:09:33+00:00
  edited: true
  hidden: false
  id: 6574670d14bfb7d56fb21747
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/9c864114f616cf6ee9d76aee633b01c4.svg
      fullname: BaYang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bayang
      type: user
    createdAt: '2023-12-09T13:09:33.000Z'
    data:
      oid: b73c421abf66672ced3df28368216071e321b1e1
      parents:
      - 2b4accb9c2cd56ea6329f54b87721a3950aec6e7
      subject: Add xl variant
    id: 6574670d0000000000000000
    type: commit
  author: bayang
  created_at: 2023-12-09 13:09:33+00:00
  id: 6574670d0000000000000000
  oid: b73c421abf66672ced3df28368216071e321b1e1
  summary: Add xl variant
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
      fullname: Laurent Mazare
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lmz
      type: user
    createdAt: '2023-12-10T19:32:32.000Z'
    data:
      edited: false
      editors:
      - lmz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686625599861145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
          fullname: Laurent Mazare
          isHf: false
          isPro: false
          name: lmz
          type: user
        html: '<p>Looks good, could you mention the command line / code change that
          you needed to be able to test this and how I can run it to try it out?</p>

          '
        raw: Looks good, could you mention the command line / code change that you
          needed to be able to test this and how I can run it to try it out?
        updatedAt: '2023-12-10T19:32:32.947Z'
      numEdits: 0
      reactions: []
    id: 65761250b4379e65a8b6eab9
    type: comment
  author: lmz
  content: Looks good, could you mention the command line / code change that you needed
    to be able to test this and how I can run it to try it out?
  created_at: 2023-12-10 19:32:32+00:00
  edited: false
  hidden: false
  id: 65761250b4379e65a8b6eab9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c864114f616cf6ee9d76aee633b01c4.svg
      fullname: BaYang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bayang
      type: user
    createdAt: '2023-12-10T22:51:12.000Z'
    data:
      edited: false
      editors:
      - bayang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6974075436592102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c864114f616cf6ee9d76aee633b01c4.svg
          fullname: BaYang
          isHf: false
          isPro: false
          name: bayang
          type: user
        html: "<ol>\n<li>Quantization:</li>\n</ol>\n<pre><code class=\"language-bash\"\
          >cargo run --example tensor-tools --release -- quantize --quantization q6k\
          \ PATH/TO/T5/model.safetensors /tmp/model.gguf\n</code></pre>\n<ol start=\"\
          2\">\n<li>Testing:<br>From Candle, I called my repo <code>deepfile/flan-t5-xl-gguf</code>\
          \ instead of <code>lmz/candle-quantized-t5</code>, because it contains <code>model-flan-t5-xl.gguf</code>\
          \ file in the main branch.</li>\n</ol>\n<pre><code>cargo run --example quantized-t5\
          \ --release -- --prompt \"translate to German: I'm living in Paris.\" --model-id\
          \ \"deepfile/flan-t5-xl-gguf\" --which \"flan-t5-xl\"\n...\n Ich wohne in\
          \ Paris.\n8 tokens generated (7.76 token/s)\n</code></pre>\n<p>(<span data-props=\"\
          {&quot;user&quot;:&quot;lmz&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lmz\">@<span class=\"underline\">lmz</span></a></span>\n\
          \n\t</span></span>  But this xl quantized model is worse than the quantized\
          \ large one on open-domain questions. I haven't test it yet on context-based\
          \ QA )</p>\n"
        raw: "1. Quantization:\n```bash\ncargo run --example tensor-tools --release\
          \ -- quantize --quantization q6k PATH/TO/T5/model.safetensors /tmp/model.gguf\n\
          ```\n2. Testing:\nFrom Candle, I called my repo `deepfile/flan-t5-xl-gguf`\
          \ instead of `lmz/candle-quantized-t5`, because it contains `model-flan-t5-xl.gguf`\
          \ file in the main branch.\n\n```\ncargo run --example quantized-t5 --release\
          \ -- --prompt \"translate to German: I'm living in Paris.\" --model-id \"\
          deepfile/flan-t5-xl-gguf\" --which \"flan-t5-xl\"\n...\n Ich wohne in Paris.\n\
          8 tokens generated (7.76 token/s)\n```\n(@lmz  But this xl quantized model\
          \ is worse than the quantized large one on open-domain questions. I haven't\
          \ test it yet on context-based QA )"
        updatedAt: '2023-12-10T22:51:12.880Z'
      numEdits: 0
      reactions: []
    id: 657640e09091da7dc67ce8f6
    type: comment
  author: bayang
  content: "1. Quantization:\n```bash\ncargo run --example tensor-tools --release\
    \ -- quantize --quantization q6k PATH/TO/T5/model.safetensors /tmp/model.gguf\n\
    ```\n2. Testing:\nFrom Candle, I called my repo `deepfile/flan-t5-xl-gguf` instead\
    \ of `lmz/candle-quantized-t5`, because it contains `model-flan-t5-xl.gguf` file\
    \ in the main branch.\n\n```\ncargo run --example quantized-t5 --release -- --prompt\
    \ \"translate to German: I'm living in Paris.\" --model-id \"deepfile/flan-t5-xl-gguf\"\
    \ --which \"flan-t5-xl\"\n...\n Ich wohne in Paris.\n8 tokens generated (7.76\
    \ token/s)\n```\n(@lmz  But this xl quantized model is worse than the quantized\
    \ large one on open-domain questions. I haven't test it yet on context-based QA\
    \ )"
  created_at: 2023-12-10 22:51:12+00:00
  edited: false
  hidden: false
  id: 657640e09091da7dc67ce8f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
      fullname: Laurent Mazare
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lmz
      type: user
    createdAt: '2023-12-11T00:10:21.000Z'
    data:
      status: merged
    id: 6576536dd40e6ed3266551e4
    type: status-change
  author: lmz
  created_at: 2023-12-11 00:10:21+00:00
  id: 6576536dd40e6ed3266551e4
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 05ea393d0c229963549351e1387b4af7dee1ad82
num: 3
repo_id: lmz/candle-quantized-t5
repo_type: model
status: merged
target_branch: refs/heads/main
title: Add xl variant
