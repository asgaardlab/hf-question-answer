!!python/object:huggingface_hub.community.DiscussionWithDetails
author: samikr
conflicting_files: null
created_at: 2023-11-29 17:34:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/939f66020f89e1983ea4724cb622afeb.svg
      fullname: Samik R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samikr
      type: user
    createdAt: '2023-11-29T17:34:59.000Z'
    data:
      edited: true
      editors:
      - samikr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6066150665283203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/939f66020f89e1983ea4724cb622afeb.svg
          fullname: Samik R
          isHf: false
          isPro: false
          name: samikr
          type: user
        html: "<p>I tried using this model today with llama.cpp, however received\
          \ a version error.</p>\n<pre><code>./build/bin/main -m ./models/3B/orca-mini-3b.q4_0.gguf\
          \ -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\n\
          Log start\nmain: build = 1575 (64e64aa)\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2)\
          \ 9.4.0 for x86_64-linux-gnu\nmain: seed  = 1701267953\nggml_init_cublas:\
          \ GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n\
          ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX\
          \ 3060 Laptop GPU, compute capability 8.6\ngguf_init_from_file: GGUFv1 is\
          \ no longer supported. please use a more up-to-date version\nerror loading\
          \ model: llama_model_loader: failed to load model from ./models/3B/orca-mini-3b.q4_0.gguf\n\
          \nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params:\
          \ error: failed to load model './models/3B/orca-mini-3b.q4_0.gguf'\nmain:\
          \ error: unable to load model\n</code></pre>\n<p>Then I tried updating the\
          \ model format to GGUF v3, however still got the same error.</p>\n<pre><code>./build/bin/quantize\
          \ ./models/3B/orca-mini-3b.q4_0.gguf ./models/3B/orca-mini-3b.q4_0-v2.gguf\
          \ COPY\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES:\
          \ yes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce\
          \ RTX 3060 Laptop GPU, compute capability 8.6\nmain: build = 1575 (64e64aa)\n\
          main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n\
          main: quantizing './models/3B/orca-mini-3b.q4_0.gguf' to './models/3B/orca-mini-3b.q4_0-v2.gguf'\
          \ as COPY\ngguf_init_from_file: GGUFv1 is no longer supported. please use\
          \ a more up-to-date version\nllama_model_quantize: failed to quantize: llama_model_loader:\
          \ failed to load model from ./models/3B/orca-mini-3b.q4_0.gguf\n\nmain:\
          \ failed to quantize model from './models/3B/orca-mini-3b.q4_0.gguf'\n</code></pre>\n\
          <p>Do you rem what GGUF format version you are using? I am using the latest\
          \ master of llama.cpp (updated today).</p>\n<p>Thanks!</p>\n"
        raw: "I tried using this model today with llama.cpp, however received a version\
          \ error.\n```\n./build/bin/main -m ./models/3B/orca-mini-3b.q4_0.gguf -n\
          \ 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\n\
          Log start\nmain: build = 1575 (64e64aa)\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2)\
          \ 9.4.0 for x86_64-linux-gnu\nmain: seed  = 1701267953\nggml_init_cublas:\
          \ GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n\
          ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX\
          \ 3060 Laptop GPU, compute capability 8.6\ngguf_init_from_file: GGUFv1 is\
          \ no longer supported. please use a more up-to-date version\nerror loading\
          \ model: llama_model_loader: failed to load model from ./models/3B/orca-mini-3b.q4_0.gguf\n\
          \nllama_load_model_from_file: failed to load model\nllama_init_from_gpt_params:\
          \ error: failed to load model './models/3B/orca-mini-3b.q4_0.gguf'\nmain:\
          \ error: unable to load model\n```\n\nThen I tried updating the model format\
          \ to GGUF v3, however still got the same error.\n```\n./build/bin/quantize\
          \ ./models/3B/orca-mini-3b.q4_0.gguf ./models/3B/orca-mini-3b.q4_0-v2.gguf\
          \ COPY\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES:\
          \ yes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce\
          \ RTX 3060 Laptop GPU, compute capability 8.6\nmain: build = 1575 (64e64aa)\n\
          main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n\
          main: quantizing './models/3B/orca-mini-3b.q4_0.gguf' to './models/3B/orca-mini-3b.q4_0-v2.gguf'\
          \ as COPY\ngguf_init_from_file: GGUFv1 is no longer supported. please use\
          \ a more up-to-date version\nllama_model_quantize: failed to quantize: llama_model_loader:\
          \ failed to load model from ./models/3B/orca-mini-3b.q4_0.gguf\n\nmain:\
          \ failed to quantize model from './models/3B/orca-mini-3b.q4_0.gguf'\n```\n\
          \nDo you rem what GGUF format version you are using? I am using the latest\
          \ master of llama.cpp (updated today).\n\nThanks!"
        updatedAt: '2023-11-30T06:37:56.633Z'
      numEdits: 1
      reactions: []
    id: 65677643063938c9b40c7972
    type: comment
  author: samikr
  content: "I tried using this model today with llama.cpp, however received a version\
    \ error.\n```\n./build/bin/main -m ./models/3B/orca-mini-3b.q4_0.gguf -n 256 --repeat_penalty\
    \ 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\nLog start\nmain: build\
    \ = 1575 (64e64aa)\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\
    \ for x86_64-linux-gnu\nmain: seed  = 1701267953\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:\
    \   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found\
    \ 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability\
    \ 8.6\ngguf_init_from_file: GGUFv1 is no longer supported. please use a more up-to-date\
    \ version\nerror loading model: llama_model_loader: failed to load model from\
    \ ./models/3B/orca-mini-3b.q4_0.gguf\n\nllama_load_model_from_file: failed to\
    \ load model\nllama_init_from_gpt_params: error: failed to load model './models/3B/orca-mini-3b.q4_0.gguf'\n\
    main: error: unable to load model\n```\n\nThen I tried updating the model format\
    \ to GGUF v3, however still got the same error.\n```\n./build/bin/quantize ./models/3B/orca-mini-3b.q4_0.gguf\
    \ ./models/3B/orca-mini-3b.q4_0-v2.gguf COPY\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:\
    \   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found\
    \ 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability\
    \ 8.6\nmain: build = 1575 (64e64aa)\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2)\
    \ 9.4.0 for x86_64-linux-gnu\nmain: quantizing './models/3B/orca-mini-3b.q4_0.gguf'\
    \ to './models/3B/orca-mini-3b.q4_0-v2.gguf' as COPY\ngguf_init_from_file: GGUFv1\
    \ is no longer supported. please use a more up-to-date version\nllama_model_quantize:\
    \ failed to quantize: llama_model_loader: failed to load model from ./models/3B/orca-mini-3b.q4_0.gguf\n\
    \nmain: failed to quantize model from './models/3B/orca-mini-3b.q4_0.gguf'\n```\n\
    \nDo you rem what GGUF format version you are using? I am using the latest master\
    \ of llama.cpp (updated today).\n\nThanks!"
  created_at: 2023-11-29 17:34:59+00:00
  edited: true
  hidden: false
  id: 65677643063938c9b40c7972
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: juanjgit/orca_mini_3B-GGUF
repo_type: model
status: open
target_branch: null
title: GGUF version for model
