!!python/object:huggingface_hub.community.DiscussionWithDetails
author: appliedstuff
conflicting_files: null
created_at: 2023-08-02 18:56:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
      fullname: PK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appliedstuff
      type: user
    createdAt: '2023-08-02T19:56:45.000Z'
    data:
      edited: false
      editors:
      - appliedstuff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9358354210853577
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
          fullname: PK
          isHf: false
          isPro: false
          name: appliedstuff
          type: user
        html: '<p>Thank you for providing your model here! Compared to another LLama
          2 fine tunned on German, this one is quite better!</p>

          <p>I can not find any information regarding the data set you used for fine
          tuning or any information regarding your training / fine tuning approach
          with german language? If you could elaborate on that, it would be very interesting
          for me :-)</p>

          <p>Thanks in advance!</p>

          '
        raw: "Thank you for providing your model here! Compared to another LLama 2\
          \ fine tunned on German, this one is quite better!\r\n\r\nI can not find\
          \ any information regarding the data set you used for fine tuning or any\
          \ information regarding your training / fine tuning approach with german\
          \ language? If you could elaborate on that, it would be very interesting\
          \ for me :-)\r\n\r\nThanks in advance!"
        updatedAt: '2023-08-02T19:56:45.643Z'
      numEdits: 0
      reactions: []
    id: 64cab4fda34a1fab19fba240
    type: comment
  author: appliedstuff
  content: "Thank you for providing your model here! Compared to another LLama 2 fine\
    \ tunned on German, this one is quite better!\r\n\r\nI can not find any information\
    \ regarding the data set you used for fine tuning or any information regarding\
    \ your training / fine tuning approach with german language? If you could elaborate\
    \ on that, it would be very interesting for me :-)\r\n\r\nThanks in advance!"
  created_at: 2023-08-02 18:56:45+00:00
  edited: false
  hidden: false
  id: 64cab4fda34a1fab19fba240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-08-03T18:59:27.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955711841583252
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: '<p>Hi, I plan do do a writeup once the model performs better; this
          was just a first experiment.</p>

          <p>Basically I used some German conversation data to finetune this model
          (which I can''t share at this point unfortunately but working on it). I
          would be interested in your results/feedback for different kind of prompts
          to improve the model! </p>

          '
        raw: 'Hi, I plan do do a writeup once the model performs better; this was
          just a first experiment.


          Basically I used some German conversation data to finetune this model (which
          I can''t share at this point unfortunately but working on it). I would be
          interested in your results/feedback for different kind of prompts to improve
          the model! '
        updatedAt: '2023-08-03T18:59:27.496Z'
      numEdits: 0
      reactions: []
    id: 64cbf90f942890af93662e05
    type: comment
  author: jphme
  content: 'Hi, I plan do do a writeup once the model performs better; this was just
    a first experiment.


    Basically I used some German conversation data to finetune this model (which I
    can''t share at this point unfortunately but working on it). I would be interested
    in your results/feedback for different kind of prompts to improve the model! '
  created_at: 2023-08-03 17:59:27+00:00
  edited: false
  hidden: false
  id: 64cbf90f942890af93662e05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
      fullname: PK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appliedstuff
      type: user
    createdAt: '2023-08-03T20:02:38.000Z'
    data:
      edited: false
      editors:
      - appliedstuff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590221643447876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
          fullname: PK
          isHf: false
          isPro: false
          name: appliedstuff
          type: user
        html: '<p>Thanks for your reply! </p>

          <p>Your work is really interesting. Can you provide some rough information
          about the training in advance: such as some metrics about your data set
          (size)? Was the fine tunning on general German conversation data or just
          specific conversation data? How an example look like? How long your training
          take and what gpu you used? </p>

          <p>That would be really valuable to know!</p>

          '
        raw: "Thanks for your reply! \n\nYour work is really interesting. Can you\
          \ provide some rough information about the training in advance: such as\
          \ some metrics about your data set (size)? Was the fine tunning on general\
          \ German conversation data or just specific conversation data? How an example\
          \ look like? How long your training take and what gpu you used? \n\nThat\
          \ would be really valuable to know!"
        updatedAt: '2023-08-03T20:02:38.640Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Pawloo
        - Ollyi
    id: 64cc07de71b435a75dcf17c2
    type: comment
  author: appliedstuff
  content: "Thanks for your reply! \n\nYour work is really interesting. Can you provide\
    \ some rough information about the training in advance: such as some metrics about\
    \ your data set (size)? Was the fine tunning on general German conversation data\
    \ or just specific conversation data? How an example look like? How long your\
    \ training take and what gpu you used? \n\nThat would be really valuable to know!"
  created_at: 2023-08-03 19:02:38+00:00
  edited: false
  hidden: false
  id: 64cc07de71b435a75dcf17c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jphme/Llama-2-13b-chat-german-GGML
repo_type: model
status: open
target_branch: null
title: Used Data Set and Training / Fine Tuning Approach with German Language?
