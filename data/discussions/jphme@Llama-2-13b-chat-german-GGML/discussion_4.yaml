!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FrankDase
conflicting_files: null
created_at: 2023-11-21 17:13:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a75218d4aa4e9a23540390c0b3487ec4.svg
      fullname: Frank Dase
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FrankDase
      type: user
    createdAt: '2023-11-21T17:13:11.000Z'
    data:
      edited: true
      editors:
      - FrankDase
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960956335067749
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a75218d4aa4e9a23540390c0b3487ec4.svg
          fullname: Frank Dase
          isHf: false
          isPro: false
          name: FrankDase
          type: user
        html: "<p>Hi,</p>\n<p>I receive from my database some long text answers that\
          \ I like to send to the LLM together with my question.<br>I tell the model\
          \ that I will send multiple text blocks and that it should not answer until\
          \ all is sent and I ask my question.<br>But for some reason it answers after\
          \ each text input. </p>\n<p>Is that not possible with this model?</p>\n\
          <p>Example:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/654d368dd2db4280d92967d2/7MYCwDHLYtW98GuokY2-p.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/654d368dd2db4280d92967d2/7MYCwDHLYtW98GuokY2-p.png\"\
          ></a></p>\n<p>My script is sending it this way:</p>\n<pre><code>{\n  model:\
          \ 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n  messages: [\n    {\n    \
          \  role: 'user',\n      content: 'Das folgende Skript ist Teil eines gr\xF6\
          \xDFeren Textes, den ich analysieren m\xF6chte. Ich werde dir den Text in\
          \ Abschnitten (Chunks) senden, die jeweils weniger als 1000 Zeichen lang\
          \ sind, um die Token-Limits zu beachten. Bitte warte mit deiner Analyse\
          \ oder Antwort, bis ich alle Teile des Textes gesendet habe. Ich werde dir\
          \ signalisieren, wenn der gesamte Text \xFCbermittelt wurde und ich bereit\
          \ bin, deine umfassende Analyse zu erhalten. Chinas Staatschef Xi Jinping\
          \ hat eine \"sofortige Waffenruhe\" im Krieg zwischen Israel und der radikalislamischen\
          \ Hamas und eine Freilassung der \"zivilen Gefangenen\" gefordert. Xi sagte\
          \ laut der staatlichen Nachrichtenagentur Xinhua bei einem virtuellen Sondergipfel\
          \ der BRICS-Staaten Brasilien, Russland, Indien, China und S\xFCdafrika,\
          \ \"alle Konfliktparteien\" sollten den Beschuss und die Kampfhandlungen\
          \ \"sofort\" einstellen. Das war Abschnitt 1 von 2. Bitte warte auf die\
          \ folgenden Abschnitte, bevor du antwortest.'\n    }\n  ],\n  temperature:\
          \ 0.7\n}\n{\n  model: 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n  messages:\
          \ [\n    {\n      role: 'user',\n      content: 'Xi rief die Konfliktparteien\
          \ dem Bericht zufolge zudem dazu auf, \"jegliche Gewalt und Angriffe auf\
          \ Zivilisten\" zu beenden und \"zivile Gefangene\" freizulassen. In \xC4\
          u\xDFerungen Xis bei der Videokonferenz, die von einem Dolmetscher \xFC\
          bersetzt wurden, rief der chinesische Pr\xE4sident zudem zu einer \"internationalen\
          \ Friedenskonferenz\" zur Beendigung des Gaza-Kriegs auf. Dabei m\xFCsse\
          \ es auch um \"eine baldige L\xF6sung der Pal\xE4stina-Frage\" gehen, die\
          \ \"umfassend, gerecht und nachhaltig\" sei und ohne die es im Nahen Osten\
          \ \"keinen nachhaltigen Frieden\" geben werde. Das war der letzte Abschnitt.\
          \ Alle Teile des Textes wurden gesendet. Basierend auf dem gesamten \xFC\
          bermittelten Text, hier ist meine spezifische Frage: Was fordert Xi Jinping?'\n\
          \    }\n  ],\n  temperature: 0.7\n}\n</code></pre>\n<p>thanks in advance<br>Frank</p>\n"
        raw: "Hi,\n\nI receive from my database some long text answers that I like\
          \ to send to the LLM together with my question. \nI tell the model that\
          \ I will send multiple text blocks and that it should not answer until all\
          \ is sent and I ask my question. \nBut for some reason it answers after\
          \ each text input. \n\nIs that not possible with this model?\n\nExample:\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/654d368dd2db4280d92967d2/7MYCwDHLYtW98GuokY2-p.png)\n\
          \nMy script is sending it this way:\n\n```\n{\n  model: 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n\
          \  messages: [\n    {\n      role: 'user',\n      content: 'Das folgende\
          \ Skript ist Teil eines gr\xF6\xDFeren Textes, den ich analysieren m\xF6\
          chte. Ich werde dir den Text in Abschnitten (Chunks) senden, die jeweils\
          \ weniger als 1000 Zeichen lang sind, um die Token-Limits zu beachten. Bitte\
          \ warte mit deiner Analyse oder Antwort, bis ich alle Teile des Textes gesendet\
          \ habe. Ich werde dir signalisieren, wenn der gesamte Text \xFCbermittelt\
          \ wurde und ich bereit bin, deine umfassende Analyse zu erhalten. Chinas\
          \ Staatschef Xi Jinping hat eine \"sofortige Waffenruhe\" im Krieg zwischen\
          \ Israel und der radikalislamischen Hamas und eine Freilassung der \"zivilen\
          \ Gefangenen\" gefordert. Xi sagte laut der staatlichen Nachrichtenagentur\
          \ Xinhua bei einem virtuellen Sondergipfel der BRICS-Staaten Brasilien,\
          \ Russland, Indien, China und S\xFCdafrika, \"alle Konfliktparteien\" sollten\
          \ den Beschuss und die Kampfhandlungen \"sofort\" einstellen. Das war Abschnitt\
          \ 1 von 2. Bitte warte auf die folgenden Abschnitte, bevor du antwortest.'\n\
          \    }\n  ],\n  temperature: 0.7\n}\n{\n  model: 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n\
          \  messages: [\n    {\n      role: 'user',\n      content: 'Xi rief die\
          \ Konfliktparteien dem Bericht zufolge zudem dazu auf, \"jegliche Gewalt\
          \ und Angriffe auf Zivilisten\" zu beenden und \"zivile Gefangene\" freizulassen.\
          \ In \xC4u\xDFerungen Xis bei der Videokonferenz, die von einem Dolmetscher\
          \ \xFCbersetzt wurden, rief der chinesische Pr\xE4sident zudem zu einer\
          \ \"internationalen Friedenskonferenz\" zur Beendigung des Gaza-Kriegs auf.\
          \ Dabei m\xFCsse es auch um \"eine baldige L\xF6sung der Pal\xE4stina-Frage\"\
          \ gehen, die \"umfassend, gerecht und nachhaltig\" sei und ohne die es im\
          \ Nahen Osten \"keinen nachhaltigen Frieden\" geben werde. Das war der letzte\
          \ Abschnitt. Alle Teile des Textes wurden gesendet. Basierend auf dem gesamten\
          \ \xFCbermittelten Text, hier ist meine spezifische Frage: Was fordert Xi\
          \ Jinping?'\n    }\n  ],\n  temperature: 0.7\n}\n``` \n\nthanks in advance\n\
          Frank"
        updatedAt: '2023-11-21T17:29:02.698Z'
      numEdits: 3
      reactions: []
    id: 655ce5272735108d497bf1b3
    type: comment
  author: FrankDase
  content: "Hi,\n\nI receive from my database some long text answers that I like to\
    \ send to the LLM together with my question. \nI tell the model that I will send\
    \ multiple text blocks and that it should not answer until all is sent and I ask\
    \ my question. \nBut for some reason it answers after each text input. \n\nIs\
    \ that not possible with this model?\n\nExample:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/654d368dd2db4280d92967d2/7MYCwDHLYtW98GuokY2-p.png)\n\
    \nMy script is sending it this way:\n\n```\n{\n  model: 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n\
    \  messages: [\n    {\n      role: 'user',\n      content: 'Das folgende Skript\
    \ ist Teil eines gr\xF6\xDFeren Textes, den ich analysieren m\xF6chte. Ich werde\
    \ dir den Text in Abschnitten (Chunks) senden, die jeweils weniger als 1000 Zeichen\
    \ lang sind, um die Token-Limits zu beachten. Bitte warte mit deiner Analyse oder\
    \ Antwort, bis ich alle Teile des Textes gesendet habe. Ich werde dir signalisieren,\
    \ wenn der gesamte Text \xFCbermittelt wurde und ich bereit bin, deine umfassende\
    \ Analyse zu erhalten. Chinas Staatschef Xi Jinping hat eine \"sofortige Waffenruhe\"\
    \ im Krieg zwischen Israel und der radikalislamischen Hamas und eine Freilassung\
    \ der \"zivilen Gefangenen\" gefordert. Xi sagte laut der staatlichen Nachrichtenagentur\
    \ Xinhua bei einem virtuellen Sondergipfel der BRICS-Staaten Brasilien, Russland,\
    \ Indien, China und S\xFCdafrika, \"alle Konfliktparteien\" sollten den Beschuss\
    \ und die Kampfhandlungen \"sofort\" einstellen. Das war Abschnitt 1 von 2. Bitte\
    \ warte auf die folgenden Abschnitte, bevor du antwortest.'\n    }\n  ],\n  temperature:\
    \ 0.7\n}\n{\n  model: 'llama2-13b-chat-german.ggmlv3.q4_0.bin',\n  messages: [\n\
    \    {\n      role: 'user',\n      content: 'Xi rief die Konfliktparteien dem\
    \ Bericht zufolge zudem dazu auf, \"jegliche Gewalt und Angriffe auf Zivilisten\"\
    \ zu beenden und \"zivile Gefangene\" freizulassen. In \xC4u\xDFerungen Xis bei\
    \ der Videokonferenz, die von einem Dolmetscher \xFCbersetzt wurden, rief der\
    \ chinesische Pr\xE4sident zudem zu einer \"internationalen Friedenskonferenz\"\
    \ zur Beendigung des Gaza-Kriegs auf. Dabei m\xFCsse es auch um \"eine baldige\
    \ L\xF6sung der Pal\xE4stina-Frage\" gehen, die \"umfassend, gerecht und nachhaltig\"\
    \ sei und ohne die es im Nahen Osten \"keinen nachhaltigen Frieden\" geben werde.\
    \ Das war der letzte Abschnitt. Alle Teile des Textes wurden gesendet. Basierend\
    \ auf dem gesamten \xFCbermittelten Text, hier ist meine spezifische Frage: Was\
    \ fordert Xi Jinping?'\n    }\n  ],\n  temperature: 0.7\n}\n``` \n\nthanks in\
    \ advance\nFrank"
  created_at: 2023-11-21 17:13:11+00:00
  edited: true
  hidden: false
  id: 655ce5272735108d497bf1b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-23T23:57:26.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396538734436035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: '<p>Hi Frank,</p>

          <p>I don''t know which software you are using but some tips:</p>

          <ul>

          <li>Use the new <a href="https://huggingface.co/jphme/em_german_leo_mistral">EM
          German Leo Mistral</a>, it should be way better than this model (and as
          its 7b you can fit more into its context window and has 4096 context size,
          easily extendable to 8k+).</li>

          <li>It doesn''t make sense to split the text into different chunk to stay
          under the context window (except you prepare a summary or shorten the chunks
          somehow) as the text has to be in the context window to get "attention"
          from the model (and it doesn''t make a difference whether its in the same
          message or multiple messages).</li>

          </ul>

          <p>Hope that helps!</p>

          '
        raw: "Hi Frank,\n\nI don't know which software you are using but some tips:\n\
          * Use the new [EM German Leo Mistral](https://huggingface.co/jphme/em_german_leo_mistral),\
          \ it should be way better than this model (and as its 7b you can fit more\
          \ into its context window and has 4096 context size, easily extendable to\
          \ 8k+).\n* It doesn't make sense to split the text into different chunk\
          \ to stay under the context window (except you prepare a summary or shorten\
          \ the chunks somehow) as the text has to be in the context window to get\
          \ \"attention\" from the model (and it doesn't make a difference whether\
          \ its in the same message or multiple messages). \n\nHope that helps!"
        updatedAt: '2023-11-23T23:57:26.151Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655fe6e6a4c9a1dd9430e16a
    id: 655fe6e6a4c9a1dd9430e163
    type: comment
  author: jphme
  content: "Hi Frank,\n\nI don't know which software you are using but some tips:\n\
    * Use the new [EM German Leo Mistral](https://huggingface.co/jphme/em_german_leo_mistral),\
    \ it should be way better than this model (and as its 7b you can fit more into\
    \ its context window and has 4096 context size, easily extendable to 8k+).\n*\
    \ It doesn't make sense to split the text into different chunk to stay under the\
    \ context window (except you prepare a summary or shorten the chunks somehow)\
    \ as the text has to be in the context window to get \"attention\" from the model\
    \ (and it doesn't make a difference whether its in the same message or multiple\
    \ messages). \n\nHope that helps!"
  created_at: 2023-11-23 23:57:26+00:00
  edited: false
  hidden: false
  id: 655fe6e6a4c9a1dd9430e163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-23T23:57:26.000Z'
    data:
      status: closed
    id: 655fe6e6a4c9a1dd9430e16a
    type: status-change
  author: jphme
  created_at: 2023-11-23 23:57:26+00:00
  id: 655fe6e6a4c9a1dd9430e16a
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65c6787bbc1dceaf598ffb8d1f9f079c.svg
      fullname: Dirk Hofmeister
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dho
      type: user
    createdAt: '2023-11-24T10:31:04.000Z'
    data:
      edited: false
      editors:
      - dho
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9414185285568237
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65c6787bbc1dceaf598ffb8d1f9f079c.svg
          fullname: Dirk Hofmeister
          isHf: false
          isPro: false
          name: dho
          type: user
        html: '<p>i would disagree, sorry, i have tested various RAG tasks against
          both this LLM and the leo-mistral-hessianai-7b-chat. For more complex reasoning
          tasks, the LLama-2 gives a better answer. In addition, Mistral hallucinates
          more frequently, but this can be stopped by QLora fine-tuning.</p>

          '
        raw: i would disagree, sorry, i have tested various RAG tasks against both
          this LLM and the leo-mistral-hessianai-7b-chat. For more complex reasoning
          tasks, the LLama-2 gives a better answer. In addition, Mistral hallucinates
          more frequently, but this can be stopped by QLora fine-tuning.
        updatedAt: '2023-11-24T10:31:04.665Z'
      numEdits: 0
      reactions: []
    id: 65607b6827463ffc080e66e6
    type: comment
  author: dho
  content: i would disagree, sorry, i have tested various RAG tasks against both this
    LLM and the leo-mistral-hessianai-7b-chat. For more complex reasoning tasks, the
    LLama-2 gives a better answer. In addition, Mistral hallucinates more frequently,
    but this can be stopped by QLora fine-tuning.
  created_at: 2023-11-24 10:31:04+00:00
  edited: false
  hidden: false
  id: 65607b6827463ffc080e66e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a75218d4aa4e9a23540390c0b3487ec4.svg
      fullname: Frank Dase
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FrankDase
      type: user
    createdAt: '2023-11-24T12:33:21.000Z'
    data:
      edited: false
      editors:
      - FrankDase
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9294590950012207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a75218d4aa4e9a23540390c0b3487ec4.svg
          fullname: Frank Dase
          isHf: false
          isPro: false
          name: FrankDase
          type: user
        html: '<p>The EM German Leo Mistral is faster but the LLama2 gives more robust
          answers. With the Leo Mistral I have often answers with repeated text.</p>

          <p>To answer which software I use: I use <a rel="nofollow" href="https://localai.io">https://localai.io</a>
          as API server and for the frontend I wrote my own application with NodeJs
          and Express.<br>Example of my frontend: <a rel="nofollow" href="https://share.vidyard.com/watch/p8mWyKyHFnWXQ5Nynit9D4">https://share.vidyard.com/watch/p8mWyKyHFnWXQ5Nynit9D4</a>?
          </p>

          '
        raw: "The EM German Leo Mistral is faster but the LLama2 gives more robust\
          \ answers. With the Leo Mistral I have often answers with repeated text.\n\
          \nTo answer which software I use: I use https://localai.io as API server\
          \ and for the frontend I wrote my own application with NodeJs and Express.\
          \ \nExample of my frontend: https://share.vidyard.com/watch/p8mWyKyHFnWXQ5Nynit9D4? "
        updatedAt: '2023-11-24T12:33:21.940Z'
      numEdits: 0
      reactions: []
    id: 65609811ab484c4a3afe4d30
    type: comment
  author: FrankDase
  content: "The EM German Leo Mistral is faster but the LLama2 gives more robust answers.\
    \ With the Leo Mistral I have often answers with repeated text.\n\nTo answer which\
    \ software I use: I use https://localai.io as API server and for the frontend\
    \ I wrote my own application with NodeJs and Express. \nExample of my frontend:\
    \ https://share.vidyard.com/watch/p8mWyKyHFnWXQ5Nynit9D4? "
  created_at: 2023-11-24 12:33:21+00:00
  edited: false
  hidden: false
  id: 65609811ab484c4a3afe4d30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-24T14:39:04.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9421269297599792
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: "<blockquote>\n<p>i would disagree, sorry, i have tested various RAG\
          \ tasks against both this LLM and the leo-mistral-hessianai-7b-chat. For\
          \ more complex reasoning tasks, the LLama-2 gives a better answer. In addition,\
          \ Mistral hallucinates more frequently, but this can be stopped by QLora\
          \ fine-tuning.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;dho&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dho\"\
          >@<span class=\"underline\">dho</span></a></span>\n\n\t</span></span> Many\
          \ thanks for your feedback! Would you be able to provide an example for\
          \ one of these RAG-related reasoning tasks (preferably here: <a rel=\"nofollow\"\
          \ href=\"https://github.com/jphme/EM_German/issues\">https://github.com/jphme/EM_German/issues</a>\
          \ )?<br>For this model, RAG was more an afterthought/gimmick, but we are\
          \ currently preparing the data for the next model generation with more robust\
          \ RAG capabilities and I would love to add some training examples for cases\
          \ like you mentioned.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;FrankDase&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/FrankDase\"\
          >@<span class=\"underline\">FrankDase</span></a></span>\n\n\t</span></span>\
          \ same for you, can you give an example for one of these prompts? Did you\
          \ try to increase the temperature and presence/frequency penalty slightly?</p>\n\
          <p>Many thanks!</p>\n"
        raw: '> i would disagree, sorry, i have tested various RAG tasks against both
          this LLM and the leo-mistral-hessianai-7b-chat. For more complex reasoning
          tasks, the LLama-2 gives a better answer. In addition, Mistral hallucinates
          more frequently, but this can be stopped by QLora fine-tuning.


          @dho Many thanks for your feedback! Would you be able to provide an example
          for one of these RAG-related reasoning tasks (preferably here: https://github.com/jphme/EM_German/issues
          )?

          For this model, RAG was more an afterthought/gimmick, but we are currently
          preparing the data for the next model generation with more robust RAG capabilities
          and I would love to add some training examples for cases like you mentioned.


          @FrankDase same for you, can you give an example for one of these prompts?
          Did you try to increase the temperature and presence/frequency penalty slightly?


          Many thanks!


          '
        updatedAt: '2023-11-24T14:39:04.647Z'
      numEdits: 0
      reactions: []
    id: 6560b588ddb286a70b24f42f
    type: comment
  author: jphme
  content: '> i would disagree, sorry, i have tested various RAG tasks against both
    this LLM and the leo-mistral-hessianai-7b-chat. For more complex reasoning tasks,
    the LLama-2 gives a better answer. In addition, Mistral hallucinates more frequently,
    but this can be stopped by QLora fine-tuning.


    @dho Many thanks for your feedback! Would you be able to provide an example for
    one of these RAG-related reasoning tasks (preferably here: https://github.com/jphme/EM_German/issues
    )?

    For this model, RAG was more an afterthought/gimmick, but we are currently preparing
    the data for the next model generation with more robust RAG capabilities and I
    would love to add some training examples for cases like you mentioned.


    @FrankDase same for you, can you give an example for one of these prompts? Did
    you try to increase the temperature and presence/frequency penalty slightly?


    Many thanks!


    '
  created_at: 2023-11-24 14:39:04+00:00
  edited: false
  hidden: false
  id: 6560b588ddb286a70b24f42f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: jphme/Llama-2-13b-chat-german-GGML
repo_type: model
status: closed
target_branch: null
title: How can I send a reference text to the model which is splitted into multiple
  text blocks?
