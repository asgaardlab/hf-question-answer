!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Marissa
conflicting_files: null
created_at: 2022-05-26 18:30:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44f15ba9834e5f9e1a5e1a8da31c80f9.svg
      fullname: Marissa Gerchick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marissa
      type: user
    createdAt: '2022-05-26T19:30:32.000Z'
    data:
      edited: false
      editors:
      - Marissa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44f15ba9834e5f9e1a5e1a8da31c80f9.svg
          fullname: Marissa Gerchick
          isHf: false
          isPro: false
          name: Marissa
          type: user
        html: '<p>This model card is awesome, and it would be great to build it out
          even more! Could you share details to estimate the CO2 emissions, like the
          training time, cloud provider and compute region? </p>

          <p>Also, the model card for the roBERTa base model (<a href="https://huggingface.co/roberta-base">https://huggingface.co/roberta-base</a>)
          talks about how bias issues with roBERTa will affect fine tuned versions
          of the model, too. Are there resources we can use to better understand potential
          biases associated with this model? </p>

          '
        raw: "This model card is awesome, and it would be great to build it out even\
          \ more! Could you share details to estimate the CO2 emissions, like the\
          \ training time, cloud provider and compute region? \r\n\r\nAlso, the model\
          \ card for the roBERTa base model (https://huggingface.co/roberta-base)\
          \ talks about how bias issues with roBERTa will affect fine tuned versions\
          \ of the model, too. Are there resources we can use to better understand\
          \ potential biases associated with this model? "
        updatedAt: '2022-05-26T19:30:32.000Z'
      numEdits: 0
      reactions: []
    id: 628fd558267c3813eb5b205f
    type: comment
  author: Marissa
  content: "This model card is awesome, and it would be great to build it out even\
    \ more! Could you share details to estimate the CO2 emissions, like the training\
    \ time, cloud provider and compute region? \r\n\r\nAlso, the model card for the\
    \ roBERTa base model (https://huggingface.co/roberta-base) talks about how bias\
    \ issues with roBERTa will affect fine tuned versions of the model, too. Are there\
    \ resources we can use to better understand potential biases associated with this\
    \ model? "
  created_at: 2022-05-26 18:30:32+00:00
  edited: false
  hidden: false
  id: 628fd558267c3813eb5b205f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608628467492-5fd88a878d159e92b8d35feb.jpeg?w=200&h=200&f=face
      fullname: timo moeller
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: timomo
      type: user
    createdAt: '2022-05-27T11:13:42.000Z'
    data:
      edited: false
      editors:
      - timomo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608628467492-5fd88a878d159e92b8d35feb.jpeg?w=200&h=200&f=face
          fullname: timo moeller
          isHf: false
          isPro: false
          name: timomo
          type: user
        html: '<p>Hey Marissa,</p>

          <p>thanks a lot for the model card praise. Highly appreciated that you are
          pushing awereness of co2 efficiency and bias in LMs.</p>

          <h1 id="carbon-footprint">carbon footprint</h1>

          <p>since the model is only finetuned on the SQuAD QA dataset and not pretrained
          I suspect the footprint for this training run will be low. So it is trained
          on a 4x V100 GPU (p3.8xlarge) for 2 epochs, which take roughly 15 mins each,
          so 30 mins in total. I am reasonaly certain the AWS region was Ireland (eu-west-1).<br>Btw
          could you share how you then compute the carbon footprint. And also an additional
          suggestion (not sure if you already discussed this): how about including
          inference footprints for standard GPU hardware in the model card? You know,
          so RoBERTa pretraining from scratch and finetuning on SQuAD seems to me
          only a tiny fraction of the real footprint, considfering this model is downloaded
          and hopefully used more than 1 million times each month...</p>

          <h1 id="bias">bias</h1>

          <p>Of course the bias in vanilla RoBERTa is rather large. Since the training
          data was SQuAD, english wikipedia pages and factual questions and answers
          I suspect the QA bias is rather small. I found a rather recent publication
          on bias in QA datasets, including SQuAD: <a rel="nofollow" href="https://aclanthology.org/2021.mrqa-1.9.pdf">https://aclanthology.org/2021.mrqa-1.9.pdf</a>
          TLDR: the bias is rather small for this dataset and closed domain use case.
          They found some bias for open domain question answering in the retriever,
          when the questions are underspecified (not part of this model), which is
          understandable.<br>About annotator selection bias, I know that we carefully
          selected a diverse set of annotators for our GermanQuAD dataset. I think
          SQuAD used mechanical turk without annotator selection unfortunately...</p>

          <h1 id="model-card">model card</h1>

          <p>Hey we are also happy with details in our model card(s) and it seems
          obvious that our well structured model card helps adoption of the model.
          But it is of course not perfect by any means and are striving to make the
          model cards more standardized so people can quickly judge if the model is
          fit for their purpose. I know HF has initiated an internship on improving
          model cards (is this you? :D ). If you have any more info and pointers how
          we can help/improve please let us know.</p>

          <p>Kind regards, Timo</p>

          '
        raw: 'Hey Marissa,


          thanks a lot for the model card praise. Highly appreciated that you are
          pushing awereness of co2 efficiency and bias in LMs.


          # carbon footprint


          since the model is only finetuned on the SQuAD QA dataset and not pretrained
          I suspect the footprint for this training run will be low. So it is trained
          on a 4x V100 GPU (p3.8xlarge) for 2 epochs, which take roughly 15 mins each,
          so 30 mins in total. I am reasonaly certain the AWS region was Ireland (eu-west-1).

          Btw could you share how you then compute the carbon footprint. And also
          an additional suggestion (not sure if you already discussed this): how about
          including inference footprints for standard GPU hardware in the model card?
          You know, so RoBERTa pretraining from scratch and finetuning on SQuAD seems
          to me only a tiny fraction of the real footprint, considfering this model
          is downloaded and hopefully used more than 1 million times each month...


          # bias

          Of course the bias in vanilla RoBERTa is rather large. Since the training
          data was SQuAD, english wikipedia pages and factual questions and answers
          I suspect the QA bias is rather small. I found a rather recent publication
          on bias in QA datasets, including SQuAD: https://aclanthology.org/2021.mrqa-1.9.pdf
          TLDR: the bias is rather small for this dataset and closed domain use case.
          They found some bias for open domain question answering in the retriever,
          when the questions are underspecified (not part of this model), which is
          understandable.

          About annotator selection bias, I know that we carefully selected a diverse
          set of annotators for our GermanQuAD dataset. I think SQuAD used mechanical
          turk without annotator selection unfortunately...


          # model card

          Hey we are also happy with details in our model card(s) and it seems obvious
          that our well structured model card helps adoption of the model. But it
          is of course not perfect by any means and are striving to make the model
          cards more standardized so people can quickly judge if the model is fit
          for their purpose. I know HF has initiated an internship on improving model
          cards (is this you? :D ). If you have any more info and pointers how we
          can help/improve please let us know.


          Kind regards, Timo

          '
        updatedAt: '2022-05-27T11:13:42.000Z'
      numEdits: 0
      reactions: []
    id: 6290b266a29097b211b82700
    type: comment
  author: timomo
  content: 'Hey Marissa,


    thanks a lot for the model card praise. Highly appreciated that you are pushing
    awereness of co2 efficiency and bias in LMs.


    # carbon footprint


    since the model is only finetuned on the SQuAD QA dataset and not pretrained I
    suspect the footprint for this training run will be low. So it is trained on a
    4x V100 GPU (p3.8xlarge) for 2 epochs, which take roughly 15 mins each, so 30
    mins in total. I am reasonaly certain the AWS region was Ireland (eu-west-1).

    Btw could you share how you then compute the carbon footprint. And also an additional
    suggestion (not sure if you already discussed this): how about including inference
    footprints for standard GPU hardware in the model card? You know, so RoBERTa pretraining
    from scratch and finetuning on SQuAD seems to me only a tiny fraction of the real
    footprint, considfering this model is downloaded and hopefully used more than
    1 million times each month...


    # bias

    Of course the bias in vanilla RoBERTa is rather large. Since the training data
    was SQuAD, english wikipedia pages and factual questions and answers I suspect
    the QA bias is rather small. I found a rather recent publication on bias in QA
    datasets, including SQuAD: https://aclanthology.org/2021.mrqa-1.9.pdf TLDR: the
    bias is rather small for this dataset and closed domain use case. They found some
    bias for open domain question answering in the retriever, when the questions are
    underspecified (not part of this model), which is understandable.

    About annotator selection bias, I know that we carefully selected a diverse set
    of annotators for our GermanQuAD dataset. I think SQuAD used mechanical turk without
    annotator selection unfortunately...


    # model card

    Hey we are also happy with details in our model card(s) and it seems obvious that
    our well structured model card helps adoption of the model. But it is of course
    not perfect by any means and are striving to make the model cards more standardized
    so people can quickly judge if the model is fit for their purpose. I know HF has
    initiated an internship on improving model cards (is this you? :D ). If you have
    any more info and pointers how we can help/improve please let us know.


    Kind regards, Timo

    '
  created_at: 2022-05-27 10:13:42+00:00
  edited: false
  hidden: false
  id: 6290b266a29097b211b82700
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44f15ba9834e5f9e1a5e1a8da31c80f9.svg
      fullname: Marissa Gerchick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marissa
      type: user
    createdAt: '2022-05-30T18:17:43.000Z'
    data:
      edited: false
      editors:
      - Marissa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44f15ba9834e5f9e1a5e1a8da31c80f9.svg
          fullname: Marissa Gerchick
          isHf: false
          isPro: false
          name: Marissa
          type: user
        html: "<p>Thanks so much for this info, Timo! And yes, @Es-O and I are working\
          \ on model cards! If it\u2019s ok with you, we can suggest some optional\
          \ additions to your model card incorporating the information you shared\
          \ as a PR.  For example, we could add information on bias and limitations\
          \ of the model (perhaps in the \"Usage\" section)  and estimated carbon\
          \ emissions to the model card (here is an example of a model card with sections\
          \ covering those topics: <a href=\"https://huggingface.co/distilgpt2\">https://huggingface.co/distilgpt2</a>)\
          \ </p>\n<p>For the carbon footprint, we might use the MLCO2 calculator (<a\
          \ rel=\"nofollow\" href=\"https://mlco2.github.io/impact/#home\">https://mlco2.github.io/impact/#home</a>).\
          \ There's more info about including carbon emissions info in model cards\
          \ here: <a href=\"https://huggingface.co/docs/hub/model-repos#carbon-footprint-metadata\"\
          >https://huggingface.co/docs/hub/model-repos#carbon-footprint-metadata</a>.\
          \ Including inference footprints is a really interesting idea -- also looping\
          \ in <span data-props=\"{&quot;user&quot;:&quot;sasha&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sasha\">@<span class=\"\
          underline\">sasha</span></a></span>\n\n\t</span></span> who is the expert\
          \ on this!</p>\n<p>I'm so glad to hear that your model card has helped with\
          \ adoption of the model!!</p>\n"
        raw: "Thanks so much for this info, Timo! And yes, @Es-O and I are working\
          \ on model cards! If it\u2019s ok with you, we can suggest some optional\
          \ additions to your model card incorporating the information you shared\
          \ as a PR.  For example, we could add information on bias and limitations\
          \ of the model (perhaps in the \"Usage\" section)  and estimated carbon\
          \ emissions to the model card (here is an example of a model card with sections\
          \ covering those topics: https://huggingface.co/distilgpt2) \n\nFor the\
          \ carbon footprint, we might use the MLCO2 calculator (https://mlco2.github.io/impact/#home).\
          \ There's more info about including carbon emissions info in model cards\
          \ here: https://huggingface.co/docs/hub/model-repos#carbon-footprint-metadata.\
          \ Including inference footprints is a really interesting idea -- also looping\
          \ in @sasha who is the expert on this!\n\nI'm so glad to hear that your\
          \ model card has helped with adoption of the model!!"
        updatedAt: '2022-05-30T18:17:43.000Z'
      numEdits: 0
      reactions: []
    id: 62950a47a1e875b26e951314
    type: comment
  author: Marissa
  content: "Thanks so much for this info, Timo! And yes, @Es-O and I are working on\
    \ model cards! If it\u2019s ok with you, we can suggest some optional additions\
    \ to your model card incorporating the information you shared as a PR.  For example,\
    \ we could add information on bias and limitations of the model (perhaps in the\
    \ \"Usage\" section)  and estimated carbon emissions to the model card (here is\
    \ an example of a model card with sections covering those topics: https://huggingface.co/distilgpt2)\
    \ \n\nFor the carbon footprint, we might use the MLCO2 calculator (https://mlco2.github.io/impact/#home).\
    \ There's more info about including carbon emissions info in model cards here:\
    \ https://huggingface.co/docs/hub/model-repos#carbon-footprint-metadata. Including\
    \ inference footprints is a really interesting idea -- also looping in @sasha\
    \ who is the expert on this!\n\nI'm so glad to hear that your model card has helped\
    \ with adoption of the model!!"
  created_at: 2022-05-30 17:17:43+00:00
  edited: false
  hidden: false
  id: 62950a47a1e875b26e951314
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608628467492-5fd88a878d159e92b8d35feb.jpeg?w=200&h=200&f=face
      fullname: timo moeller
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: timomo
      type: user
    createdAt: '2022-05-30T20:44:24.000Z'
    data:
      edited: false
      editors:
      - timomo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608628467492-5fd88a878d159e92b8d35feb.jpeg?w=200&h=200&f=face
          fullname: timo moeller
          isHf: false
          isPro: false
          name: timomo
          type: user
        html: '<p>Sure, please go ahead and create a PR.</p>

          <p>Happy to discuss more on the inference carbon footprint.</p>

          '
        raw: 'Sure, please go ahead and create a PR.


          Happy to discuss more on the inference carbon footprint.'
        updatedAt: '2022-05-30T20:44:24.000Z'
      numEdits: 0
      reactions: []
    id: 62952ca81e87ffbe5c06eed3
    type: comment
  author: timomo
  content: 'Sure, please go ahead and create a PR.


    Happy to discuss more on the inference carbon footprint.'
  created_at: 2022-05-30 19:44:24+00:00
  edited: false
  hidden: false
  id: 62952ca81e87ffbe5c06eed3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620be6cec33ca69c4c821404/gakBUuCi5YXJWG2hRIAEo.jpeg?w=200&h=200&f=face
      fullname: Ezi Ozoani
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ezi
      type: user
    createdAt: '2022-06-01T20:53:24.000Z'
    data:
      edited: false
      editors:
      - Ezi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620be6cec33ca69c4c821404/gakBUuCi5YXJWG2hRIAEo.jpeg?w=200&h=200&f=face
          fullname: Ezi Ozoani
          isHf: true
          isPro: false
          name: Ezi
          type: user
        html: "<p>Hi Timo,</p>\n<p><strong>Additional Information</strong><br>If you\
          \ and the team have any additional information in regards to the \u201C\
          <em>Potential Users</em>\u201D, \u201C<em>Out-of-Scope Uses</em>\u201D in\
          \ the \u201C<em>Uses, Limitations and Risks</em>\u201D section , as well\
          \ as further \u201C<em>Limitations and Risks</em>\u201D in relation to the\
          \ model, that would aid when detailing the ethical considerations.</p>\n\
          <p><strong>Citations</strong><br>Does <em><strong>Deepset</strong></em>\
          \ have a preferred citation style?</p>\n"
        raw: "Hi Timo,\n\n**Additional Information**\nIf you and the team have any\
          \ additional information in regards to the \u201C*Potential Users*\u201D\
          , \u201C*Out-of-Scope Uses*\u201D in the \u201C*Uses, Limitations and Risks*\u201D\
          \ section , as well as further \u201C*Limitations and Risks*\u201D in relation\
          \ to the model, that would aid when detailing the ethical considerations.\n\
          \n**Citations**\nDoes ***Deepset*** have a preferred citation style?"
        updatedAt: '2022-06-01T20:53:24.000Z'
      numEdits: 0
      reactions: []
    id: 6297d1c4d74fe3213780c421
    type: comment
  author: Ezi
  content: "Hi Timo,\n\n**Additional Information**\nIf you and the team have any additional\
    \ information in regards to the \u201C*Potential Users*\u201D, \u201C*Out-of-Scope\
    \ Uses*\u201D in the \u201C*Uses, Limitations and Risks*\u201D section , as well\
    \ as further \u201C*Limitations and Risks*\u201D in relation to the model, that\
    \ would aid when detailing the ethical considerations.\n\n**Citations**\nDoes\
    \ ***Deepset*** have a preferred citation style?"
  created_at: 2022-06-01 19:53:24+00:00
  edited: false
  hidden: false
  id: 6297d1c4d74fe3213780c421
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: deepset/roberta-base-squad2
repo_type: model
status: open
target_branch: null
title: Building out the model card!
