!!python/object:huggingface_hub.community.DiscussionWithDetails
author: junhochoi
conflicting_files: null
created_at: 2023-10-06 05:43:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44964942aee8d0dd99862f8b5a38b6d1.svg
      fullname: Junho Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: junhochoi
      type: user
    createdAt: '2023-10-06T06:43:04.000Z'
    data:
      edited: true
      editors:
      - junhochoi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32286685705184937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44964942aee8d0dd99862f8b5a38b6d1.svg
          fullname: Junho Choi
          isHf: false
          isPro: false
          name: junhochoi
          type: user
        html: "<p>I tried to run this model on Macbook M1 Pro, using llama.cpp (48edda3)</p>\n\
          <pre><code>python3 convert.py ~/42dot_LLM-SFT-1.3B --vocabtype bpe\n</code></pre>\n\
          <p>It died with</p>\n<pre><code>Exception: Vocab size mismatch (model has\
          \ 50304, but &lt;snip&gt;/42dot_LLM-SFT-1.3B/vocab.json combined with &lt;snip&gt;/42dot_LLM-SFT-1.3B/added_tokens.json\
          \ has 50260).\n</code></pre>\n<p>To work around this issue, I added some\
          \ dummy tokens, as the following:</p>\n<pre><code>diff --git a/added_tokens.json\
          \ b/added_tokens.json\nindex c883403..a133144 100644\n--- a/added_tokens.json\n\
          +++ b/added_tokens.json\n@@ -2,5 +2,49 @@\n   \"&lt;|endoftext|&gt;\": 50256,\n\
          \   \"&lt;||bos||&gt;\": 50257,\n   \"&lt;||pad||&gt;\": 50258,\n-  \"&lt;||unk||&gt;\"\
          : 50259\n+  \"&lt;||unk||&gt;\": 50259,\n+  \"&lt;|tmp1|&gt;\": 50260,\n\
          +  \"&lt;|tmp2|&gt;\": 50261,\n+  \"&lt;|tmp3|&gt;\": 50262,\n+  \"&lt;|tmp4|&gt;\"\
          : 50263,\n+  \"&lt;|tmp5|&gt;\": 50264,\n+  \"&lt;|tmp6|&gt;\": 50265,\n\
          +  \"&lt;|tmp7|&gt;\": 50266,\n+  \"&lt;|tmp8|&gt;\": 50267,\n+  \"&lt;|tmp9|&gt;\"\
          : 50268,\n+  \"&lt;|tmp10|&gt;\": 50269,\n+  \"&lt;|tmp11|&gt;\": 50270,\n\
          +  \"&lt;|tmp12|&gt;\": 50271,\n+  \"&lt;|tmp13|&gt;\": 50272,\n+  \"&lt;|tmp14|&gt;\"\
          : 50273,\n+  \"&lt;|tmp15|&gt;\": 50274,\n+  \"&lt;|tmp16|&gt;\": 50275,\n\
          +  \"&lt;|tmp17|&gt;\": 50276,\n+  \"&lt;|tmp18|&gt;\": 50277,\n+  \"&lt;|tmp19|&gt;\"\
          : 50278,\n+  \"&lt;|tmp20|&gt;\": 50279,\n+  \"&lt;|tmp21|&gt;\": 50280,\n\
          +  \"&lt;|tmp22|&gt;\": 50281,\n+  \"&lt;|tmp23|&gt;\": 50282,\n+  \"&lt;|tmp24|&gt;\"\
          : 50283,\n+  \"&lt;|tmp25|&gt;\": 50284,\n+  \"&lt;|tmp26|&gt;\": 50285,\n\
          +  \"&lt;|tmp27|&gt;\": 50286,\n+  \"&lt;|tmp28|&gt;\": 50287,\n+  \"&lt;|tmp29|&gt;\"\
          : 50288,\n+  \"&lt;|tmp30|&gt;\": 50289,\n+  \"&lt;|tmp31|&gt;\": 50290,\n\
          +  \"&lt;|tmp32|&gt;\": 50291,\n+  \"&lt;|tmp33|&gt;\": 50292,\n+  \"&lt;|tmp34|&gt;\"\
          : 50293,\n+  \"&lt;|tmp35|&gt;\": 50294,\n+  \"&lt;|tmp36|&gt;\": 50295,\n\
          +  \"&lt;|tmp37|&gt;\": 50296,\n+  \"&lt;|tmp38|&gt;\": 50297,\n+  \"&lt;|tmp39|&gt;\"\
          : 50298,\n+  \"&lt;|tmp40|&gt;\": 50299,\n+  \"&lt;|tmp41|&gt;\": 50300,\n\
          +  \"&lt;|tmp42|&gt;\": 50301,\n+  \"&lt;|tmp43|&gt;\": 50302,\n+  \"&lt;|tmp44|&gt;\"\
          : 50303\n }\n</code></pre>\n<p>And now conversion works and I am able to\
          \ run llama.cpp with this model.</p>\n<p>How to convert to gguf/q4_0: (run\
          \ these after building llama.cpp)</p>\n<pre><code>python3 convert.py ~/42dot_LLM-SFT-1.3B\
          \ --vocabtype bpe\n./quantize ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0\
          \ q4_0\n</code></pre>\n<p>Run a simple chat:</p>\n<pre><code>$ ./main -m\
          \ ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0 -n -1 --color -ins -i\n\
          Log start\nmain: build = 1330 (48edda3)\nmain: built with Apple clang version\
          \ 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin22.6.0\n...\nsystem_info:\
          \ n_threads = 6 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0\
          \ | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA\
          \ = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\nmain:\
          \ interactive mode on.\nReverse prompt: '### Instruction:\n\n'\nsampling:\
          \ repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000,\
          \ frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000,\
          \ typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000,\
          \ mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict\
          \ = -1, n_keep = 1\n\n\n== Running in interactive mode. ==\n - Press Ctrl+C\
          \ to interject at any time.\n - Press Return to return control to LLaMa.\n\
          \ - To return control without starting a new line, end your input with '/'.\n\
          \ - If you want to submit another line, end your input with '\\'.\n\n\n\
          ### Instruction:\n\n\n&gt; \uD558\uB298\uC740 \uC65C \uD30C\uB780\uAC00\uC694\
          ?\n\uD558\uB298\uC774 \uD30C\uB797\uC2B5\uB2C8\uB2E4. \uC774\uC720\uB294\
          \ \uB300\uAE30 \uC911\uC5D0 \uC788\uB294 \uC218\uC99D\uAE30\uAC00 \uD587\
          \uBE5B\uC744 \uBC18\uC0AC\uD558\uC5EC \uC6B0\uB9AC \uB208\uC5D0 \uBCF4\uC774\
          \uB294 \uAC83\uC774\uAE30 \uB54C\uBB38\uC785\uB2C8\uB2E4. \uD0DC\uC591\uC5D0\
          \uC11C \uC628 \uBE5B\uC774 \uC9C0\uAD6C\uC5D0 \uB3C4\uB2EC\uD558\uBA74,\
          \ \uB300\uAE30\uB294 \uC801\uC678\uC120\uACFC \uC790\uC678\uC120\uC744 \uD761\
          \uC218\uD558\uACE0 \uC218\uC99D\uAE30\uAC00 \uBC18\uC0AC\uB97C \uD574\uC11C\
          \ \uC6B0\uB9AC\uAC00 \uBCFC \uC218 \uC788\uAC8C \uB429\uB2C8\uB2E4. \uC774\
          \ \uACFC\uC815\uC5D0\uC11C \uC0C9\uC0C1\uC758 \uBCC0\uD654\uAC00 \uC0DD\uAE30\
          \uAC8C \uB418\uB294\uB370, \uADF8 \uACB0\uACFC\uB85C \uC6B0\uB9AC\uB294\
          \ \uD558\uB298\uC774 \uD30C\uB797\uAC8C \uB290\uB07C\uAC8C \uB418\uB294\
          \ \uAC83\uC785\uB2C8\uB2E4.\n\n&gt;\n\nllama_print_timings:        load\
          \ time =    96.65 ms\nllama_print_timings:      sample time =   102.85 ms\
          \ /    84 runs   (    1.22 ms per token,   816.76 tokens per second)\nllama_print_timings:\
          \ prompt eval time =   166.23 ms /    17 tokens (    9.78 ms per token,\
          \   102.27 tokens per second)\nllama_print_timings:        eval time = \
          \  742.10 ms /    84 runs   (    8.83 ms per token,   113.19 tokens per\
          \ second)\nllama_print_timings:       total time =  8348.41 ms\n</code></pre>\n\
          <p>It is much faster than llama 7b/13b. Thanks for building this model!<br>I\
          \ won't make this a PR because adding such dummies is not a real fix. :)</p>\n"
        raw: "I tried to run this model on Macbook M1 Pro, using llama.cpp (48edda3)\n\
          \n```\npython3 convert.py ~/42dot_LLM-SFT-1.3B --vocabtype bpe\n```\n\n\
          It died with\n```\nException: Vocab size mismatch (model has 50304, but\
          \ <snip>/42dot_LLM-SFT-1.3B/vocab.json combined with <snip>/42dot_LLM-SFT-1.3B/added_tokens.json\
          \ has 50260).\n```\n\nTo work around this issue, I added some dummy tokens,\
          \ as the following:\n```\ndiff --git a/added_tokens.json b/added_tokens.json\n\
          index c883403..a133144 100644\n--- a/added_tokens.json\n+++ b/added_tokens.json\n\
          @@ -2,5 +2,49 @@\n   \"<|endoftext|>\": 50256,\n   \"<||bos||>\": 50257,\n\
          \   \"<||pad||>\": 50258,\n-  \"<||unk||>\": 50259\n+  \"<||unk||>\": 50259,\n\
          +  \"<|tmp1|>\": 50260,\n+  \"<|tmp2|>\": 50261,\n+  \"<|tmp3|>\": 50262,\n\
          +  \"<|tmp4|>\": 50263,\n+  \"<|tmp5|>\": 50264,\n+  \"<|tmp6|>\": 50265,\n\
          +  \"<|tmp7|>\": 50266,\n+  \"<|tmp8|>\": 50267,\n+  \"<|tmp9|>\": 50268,\n\
          +  \"<|tmp10|>\": 50269,\n+  \"<|tmp11|>\": 50270,\n+  \"<|tmp12|>\": 50271,\n\
          +  \"<|tmp13|>\": 50272,\n+  \"<|tmp14|>\": 50273,\n+  \"<|tmp15|>\": 50274,\n\
          +  \"<|tmp16|>\": 50275,\n+  \"<|tmp17|>\": 50276,\n+  \"<|tmp18|>\": 50277,\n\
          +  \"<|tmp19|>\": 50278,\n+  \"<|tmp20|>\": 50279,\n+  \"<|tmp21|>\": 50280,\n\
          +  \"<|tmp22|>\": 50281,\n+  \"<|tmp23|>\": 50282,\n+  \"<|tmp24|>\": 50283,\n\
          +  \"<|tmp25|>\": 50284,\n+  \"<|tmp26|>\": 50285,\n+  \"<|tmp27|>\": 50286,\n\
          +  \"<|tmp28|>\": 50287,\n+  \"<|tmp29|>\": 50288,\n+  \"<|tmp30|>\": 50289,\n\
          +  \"<|tmp31|>\": 50290,\n+  \"<|tmp32|>\": 50291,\n+  \"<|tmp33|>\": 50292,\n\
          +  \"<|tmp34|>\": 50293,\n+  \"<|tmp35|>\": 50294,\n+  \"<|tmp36|>\": 50295,\n\
          +  \"<|tmp37|>\": 50296,\n+  \"<|tmp38|>\": 50297,\n+  \"<|tmp39|>\": 50298,\n\
          +  \"<|tmp40|>\": 50299,\n+  \"<|tmp41|>\": 50300,\n+  \"<|tmp42|>\": 50301,\n\
          +  \"<|tmp43|>\": 50302,\n+  \"<|tmp44|>\": 50303\n }\n```\n\nAnd now conversion\
          \ works and I am able to run llama.cpp with this model.\n\nHow to convert\
          \ to gguf/q4_0: (run these after building llama.cpp)\n```\npython3 convert.py\
          \ ~/42dot_LLM-SFT-1.3B --vocabtype bpe\n./quantize ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
          \ ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0 q4_0\n```\n\nRun a simple\
          \ chat:\n```\n$ ./main -m ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0\
          \ -n -1 --color -ins -i\nLog start\nmain: build = 1330 (48edda3)\nmain:\
          \ built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin22.6.0\n\
          ...\nsystem_info: n_threads = 6 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 |\
          \ AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 |\
          \ F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 =\
          \ 0 | VSX = 0 |\nmain: interactive mode on.\nReverse prompt: '### Instruction:\n\
          \n'\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
          \ = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000,\
          \ top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0,\
          \ mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512,\
          \ n_batch = 512, n_predict = -1, n_keep = 1\n\n\n== Running in interactive\
          \ mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to\
          \ return control to LLaMa.\n - To return control without starting a new\
          \ line, end your input with '/'.\n - If you want to submit another line,\
          \ end your input with '\\'.\n\n\n### Instruction:\n\n\n> \uD558\uB298\uC740\
          \ \uC65C \uD30C\uB780\uAC00\uC694?\n\uD558\uB298\uC774 \uD30C\uB797\uC2B5\
          \uB2C8\uB2E4. \uC774\uC720\uB294 \uB300\uAE30 \uC911\uC5D0 \uC788\uB294\
          \ \uC218\uC99D\uAE30\uAC00 \uD587\uBE5B\uC744 \uBC18\uC0AC\uD558\uC5EC \uC6B0\
          \uB9AC \uB208\uC5D0 \uBCF4\uC774\uB294 \uAC83\uC774\uAE30 \uB54C\uBB38\uC785\
          \uB2C8\uB2E4. \uD0DC\uC591\uC5D0\uC11C \uC628 \uBE5B\uC774 \uC9C0\uAD6C\uC5D0\
          \ \uB3C4\uB2EC\uD558\uBA74, \uB300\uAE30\uB294 \uC801\uC678\uC120\uACFC\
          \ \uC790\uC678\uC120\uC744 \uD761\uC218\uD558\uACE0 \uC218\uC99D\uAE30\uAC00\
          \ \uBC18\uC0AC\uB97C \uD574\uC11C \uC6B0\uB9AC\uAC00 \uBCFC \uC218 \uC788\
          \uAC8C \uB429\uB2C8\uB2E4. \uC774 \uACFC\uC815\uC5D0\uC11C \uC0C9\uC0C1\uC758\
          \ \uBCC0\uD654\uAC00 \uC0DD\uAE30\uAC8C \uB418\uB294\uB370, \uADF8 \uACB0\
          \uACFC\uB85C \uC6B0\uB9AC\uB294 \uD558\uB298\uC774 \uD30C\uB797\uAC8C \uB290\
          \uB07C\uAC8C \uB418\uB294 \uAC83\uC785\uB2C8\uB2E4.\n\n>\n\nllama_print_timings:\
          \        load time =    96.65 ms\nllama_print_timings:      sample time\
          \ =   102.85 ms /    84 runs   (    1.22 ms per token,   816.76 tokens per\
          \ second)\nllama_print_timings: prompt eval time =   166.23 ms /    17 tokens\
          \ (    9.78 ms per token,   102.27 tokens per second)\nllama_print_timings:\
          \        eval time =   742.10 ms /    84 runs   (    8.83 ms per token,\
          \   113.19 tokens per second)\nllama_print_timings:       total time = \
          \ 8348.41 ms\n```\n\nIt is much faster than llama 7b/13b. Thanks for building\
          \ this model!\nI won't make this a PR because adding such dummies is not\
          \ a real fix. :)"
        updatedAt: '2023-10-06T07:03:55.782Z'
      numEdits: 4
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - likejazz
        - woody-cpp
        - seongmin42
        - Ru-k
        - cjlee42
        - de-jhj
        - ij5
    id: 651fac788746b91c7eb4f511
    type: comment
  author: junhochoi
  content: "I tried to run this model on Macbook M1 Pro, using llama.cpp (48edda3)\n\
    \n```\npython3 convert.py ~/42dot_LLM-SFT-1.3B --vocabtype bpe\n```\n\nIt died\
    \ with\n```\nException: Vocab size mismatch (model has 50304, but <snip>/42dot_LLM-SFT-1.3B/vocab.json\
    \ combined with <snip>/42dot_LLM-SFT-1.3B/added_tokens.json has 50260).\n```\n\
    \nTo work around this issue, I added some dummy tokens, as the following:\n```\n\
    diff --git a/added_tokens.json b/added_tokens.json\nindex c883403..a133144 100644\n\
    --- a/added_tokens.json\n+++ b/added_tokens.json\n@@ -2,5 +2,49 @@\n   \"<|endoftext|>\"\
    : 50256,\n   \"<||bos||>\": 50257,\n   \"<||pad||>\": 50258,\n-  \"<||unk||>\"\
    : 50259\n+  \"<||unk||>\": 50259,\n+  \"<|tmp1|>\": 50260,\n+  \"<|tmp2|>\": 50261,\n\
    +  \"<|tmp3|>\": 50262,\n+  \"<|tmp4|>\": 50263,\n+  \"<|tmp5|>\": 50264,\n+ \
    \ \"<|tmp6|>\": 50265,\n+  \"<|tmp7|>\": 50266,\n+  \"<|tmp8|>\": 50267,\n+  \"\
    <|tmp9|>\": 50268,\n+  \"<|tmp10|>\": 50269,\n+  \"<|tmp11|>\": 50270,\n+  \"\
    <|tmp12|>\": 50271,\n+  \"<|tmp13|>\": 50272,\n+  \"<|tmp14|>\": 50273,\n+  \"\
    <|tmp15|>\": 50274,\n+  \"<|tmp16|>\": 50275,\n+  \"<|tmp17|>\": 50276,\n+  \"\
    <|tmp18|>\": 50277,\n+  \"<|tmp19|>\": 50278,\n+  \"<|tmp20|>\": 50279,\n+  \"\
    <|tmp21|>\": 50280,\n+  \"<|tmp22|>\": 50281,\n+  \"<|tmp23|>\": 50282,\n+  \"\
    <|tmp24|>\": 50283,\n+  \"<|tmp25|>\": 50284,\n+  \"<|tmp26|>\": 50285,\n+  \"\
    <|tmp27|>\": 50286,\n+  \"<|tmp28|>\": 50287,\n+  \"<|tmp29|>\": 50288,\n+  \"\
    <|tmp30|>\": 50289,\n+  \"<|tmp31|>\": 50290,\n+  \"<|tmp32|>\": 50291,\n+  \"\
    <|tmp33|>\": 50292,\n+  \"<|tmp34|>\": 50293,\n+  \"<|tmp35|>\": 50294,\n+  \"\
    <|tmp36|>\": 50295,\n+  \"<|tmp37|>\": 50296,\n+  \"<|tmp38|>\": 50297,\n+  \"\
    <|tmp39|>\": 50298,\n+  \"<|tmp40|>\": 50299,\n+  \"<|tmp41|>\": 50300,\n+  \"\
    <|tmp42|>\": 50301,\n+  \"<|tmp43|>\": 50302,\n+  \"<|tmp44|>\": 50303\n }\n```\n\
    \nAnd now conversion works and I am able to run llama.cpp with this model.\n\n\
    How to convert to gguf/q4_0: (run these after building llama.cpp)\n```\npython3\
    \ convert.py ~/42dot_LLM-SFT-1.3B --vocabtype bpe\n./quantize ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
    \ ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0 q4_0\n```\n\nRun a simple chat:\n\
    ```\n$ ./main -m ~/42dot_LLM-SFT-1.3B/ggml-model-f32.gguf_q4.0 -n -1 --color -ins\
    \ -i\nLog start\nmain: build = 1330 (48edda3)\nmain: built with Apple clang version\
    \ 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin22.6.0\n...\nsystem_info: n_threads\
    \ = 6 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI =\
    \ 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD =\
    \ 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\nmain: interactive mode on.\n\
    Reverse prompt: '### Instruction:\n\n'\nsampling: repeat_last_n = 64, repeat_penalty\
    \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k\
    \ = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,\
    \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx\
    \ = 512, n_batch = 512, n_predict = -1, n_keep = 1\n\n\n== Running in interactive\
    \ mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return\
    \ control to LLaMa.\n - To return control without starting a new line, end your\
    \ input with '/'.\n - If you want to submit another line, end your input with\
    \ '\\'.\n\n\n### Instruction:\n\n\n> \uD558\uB298\uC740 \uC65C \uD30C\uB780\uAC00\
    \uC694?\n\uD558\uB298\uC774 \uD30C\uB797\uC2B5\uB2C8\uB2E4. \uC774\uC720\uB294\
    \ \uB300\uAE30 \uC911\uC5D0 \uC788\uB294 \uC218\uC99D\uAE30\uAC00 \uD587\uBE5B\
    \uC744 \uBC18\uC0AC\uD558\uC5EC \uC6B0\uB9AC \uB208\uC5D0 \uBCF4\uC774\uB294 \uAC83\
    \uC774\uAE30 \uB54C\uBB38\uC785\uB2C8\uB2E4. \uD0DC\uC591\uC5D0\uC11C \uC628 \uBE5B\
    \uC774 \uC9C0\uAD6C\uC5D0 \uB3C4\uB2EC\uD558\uBA74, \uB300\uAE30\uB294 \uC801\uC678\
    \uC120\uACFC \uC790\uC678\uC120\uC744 \uD761\uC218\uD558\uACE0 \uC218\uC99D\uAE30\
    \uAC00 \uBC18\uC0AC\uB97C \uD574\uC11C \uC6B0\uB9AC\uAC00 \uBCFC \uC218 \uC788\
    \uAC8C \uB429\uB2C8\uB2E4. \uC774 \uACFC\uC815\uC5D0\uC11C \uC0C9\uC0C1\uC758\
    \ \uBCC0\uD654\uAC00 \uC0DD\uAE30\uAC8C \uB418\uB294\uB370, \uADF8 \uACB0\uACFC\
    \uB85C \uC6B0\uB9AC\uB294 \uD558\uB298\uC774 \uD30C\uB797\uAC8C \uB290\uB07C\uAC8C\
    \ \uB418\uB294 \uAC83\uC785\uB2C8\uB2E4.\n\n>\n\nllama_print_timings:        load\
    \ time =    96.65 ms\nllama_print_timings:      sample time =   102.85 ms /  \
    \  84 runs   (    1.22 ms per token,   816.76 tokens per second)\nllama_print_timings:\
    \ prompt eval time =   166.23 ms /    17 tokens (    9.78 ms per token,   102.27\
    \ tokens per second)\nllama_print_timings:        eval time =   742.10 ms /  \
    \  84 runs   (    8.83 ms per token,   113.19 tokens per second)\nllama_print_timings:\
    \       total time =  8348.41 ms\n```\n\nIt is much faster than llama 7b/13b.\
    \ Thanks for building this model!\nI won't make this a PR because adding such\
    \ dummies is not a real fix. :)"
  created_at: 2023-10-06 05:43:04+00:00
  edited: true
  hidden: false
  id: 651fac788746b91c7eb4f511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668054183184-62af434f457691d789c36aeb.jpeg?w=200&h=200&f=face
      fullname: Sang Park
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: likejazz
      type: user
    createdAt: '2023-10-28T14:05:46.000Z'
    data:
      edited: true
      editors:
      - likejazz
      hidden: false
      identifiedLanguage:
        language: ko
        probability: 0.4179929792881012
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668054183184-62af434f457691d789c36aeb.jpeg?w=200&h=200&f=face
          fullname: Sang Park
          isHf: false
          isPro: false
          name: likejazz
          type: user
        html: "<p>Thank you for your feedback.<br>We've patched special tokens based\
          \ on your feedback.</p>\n<p>Now, you can run the 42dot LLM-SFT model in\
          \ llama.cpp using the guide below.</p>\n<ol>\n<li>Convert the 42dot LLM-SFT\
          \ model to ggml FP32 format.</li>\n</ol>\n<pre><code>$ python convert.py\
          \ ./42dot_LLM-SFT-1.3B/ --vocabtype bpe\n</code></pre>\n<ol start=\"2\"\
          >\n<li>Quantize the model to 4-bits(Optional).</li>\n</ol>\n<pre><code>$\
          \ ./quantize ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf ./42dot_LLM-SFT-1.3B/ggml-model-q4_0.gguf\
          \ q4_0\n</code></pre>\n<ol start=\"3\">\n<li>Run the inference. We recommend\
          \ our option.</li>\n</ol>\n<pre><code>$ ./main -m ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
          \ \\\n--temp 0.5 \\\n--top_p 0.95 \\\n--top_k 20 \\\n--n-predict 512 \\\n\
          --repeat-penalty 1.2 \\\n--color \\\n--prompt \"\uD638\uAE30\uC2EC \uB9CE\
          \uC740 \uC778\uAC04 (human)\uACFC \uC778\uACF5\uC9C0\uB2A5 \uBD07 (AI bot)\uC758\
          \ \uB300\uD654\uC785\uB2C8\uB2E4. \\n\uBD07\uC758 \uC774\uB984\uC740 42dot\
          \ LLM\uC774\uACE0 \uD3EC\uD2F0\uD22C\uB2F7 (42dot)\uC5D0\uC11C \uAC1C\uBC1C\
          \uD588\uC2B5\uB2C8\uB2E4. \\n\uBD07\uC740 \uC778\uAC04\uC758 \uC9C8\uBB38\
          \uC5D0 \uB300\uD574 \uCE5C\uC808\uD558\uAC8C \uC720\uC6A9\uD558\uACE0 \uC0C1\
          \uC138\uD55C \uB2F5\uBCC0\uC744 \uC81C\uACF5\uD569\uB2C8\uB2E4. \\n\" \\\
          \n--in-prefix \"&lt;human&gt;: \" \\\n--in-suffix \"&lt;bot&gt;:\" \\\n\
          --interactive-first\n</code></pre>\n<p>Thanks!</p>\n"
        raw: "Thank you for your feedback.\nWe've patched special tokens based on\
          \ your feedback.\n\nNow, you can run the 42dot LLM-SFT model in llama.cpp\
          \ using the guide below.\n\n1. Convert the 42dot LLM-SFT model to ggml FP32\
          \ format.\n```\n$ python convert.py ./42dot_LLM-SFT-1.3B/ --vocabtype bpe\n\
          ```\n\n2. Quantize the model to 4-bits(Optional).\n```\n$ ./quantize ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
          \ ./42dot_LLM-SFT-1.3B/ggml-model-q4_0.gguf q4_0\n```\n\n3. Run the inference.\
          \ We recommend our option.\n```\n$ ./main -m ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
          \ \\\n--temp 0.5 \\\n--top_p 0.95 \\\n--top_k 20 \\\n--n-predict 512 \\\n\
          --repeat-penalty 1.2 \\\n--color \\\n--prompt \"\uD638\uAE30\uC2EC \uB9CE\
          \uC740 \uC778\uAC04 (human)\uACFC \uC778\uACF5\uC9C0\uB2A5 \uBD07 (AI bot)\uC758\
          \ \uB300\uD654\uC785\uB2C8\uB2E4. \\n\uBD07\uC758 \uC774\uB984\uC740 42dot\
          \ LLM\uC774\uACE0 \uD3EC\uD2F0\uD22C\uB2F7 (42dot)\uC5D0\uC11C \uAC1C\uBC1C\
          \uD588\uC2B5\uB2C8\uB2E4. \\n\uBD07\uC740 \uC778\uAC04\uC758 \uC9C8\uBB38\
          \uC5D0 \uB300\uD574 \uCE5C\uC808\uD558\uAC8C \uC720\uC6A9\uD558\uACE0 \uC0C1\
          \uC138\uD55C \uB2F5\uBCC0\uC744 \uC81C\uACF5\uD569\uB2C8\uB2E4. \\n\" \\\
          \n--in-prefix \"<human>: \" \\\n--in-suffix \"<bot>:\" \\\n--interactive-first\n\
          ```\n\nThanks!"
        updatedAt: '2023-10-28T14:27:07.535Z'
      numEdits: 1
      reactions: []
      relatedEventId: 653d153ae971d8e61f4ec70c
    id: 653d153ae971d8e61f4ec70b
    type: comment
  author: likejazz
  content: "Thank you for your feedback.\nWe've patched special tokens based on your\
    \ feedback.\n\nNow, you can run the 42dot LLM-SFT model in llama.cpp using the\
    \ guide below.\n\n1. Convert the 42dot LLM-SFT model to ggml FP32 format.\n```\n\
    $ python convert.py ./42dot_LLM-SFT-1.3B/ --vocabtype bpe\n```\n\n2. Quantize\
    \ the model to 4-bits(Optional).\n```\n$ ./quantize ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
    \ ./42dot_LLM-SFT-1.3B/ggml-model-q4_0.gguf q4_0\n```\n\n3. Run the inference.\
    \ We recommend our option.\n```\n$ ./main -m ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf\
    \ \\\n--temp 0.5 \\\n--top_p 0.95 \\\n--top_k 20 \\\n--n-predict 512 \\\n--repeat-penalty\
    \ 1.2 \\\n--color \\\n--prompt \"\uD638\uAE30\uC2EC \uB9CE\uC740 \uC778\uAC04\
    \ (human)\uACFC \uC778\uACF5\uC9C0\uB2A5 \uBD07 (AI bot)\uC758 \uB300\uD654\uC785\
    \uB2C8\uB2E4. \\n\uBD07\uC758 \uC774\uB984\uC740 42dot LLM\uC774\uACE0 \uD3EC\uD2F0\
    \uD22C\uB2F7 (42dot)\uC5D0\uC11C \uAC1C\uBC1C\uD588\uC2B5\uB2C8\uB2E4. \\n\uBD07\
    \uC740 \uC778\uAC04\uC758 \uC9C8\uBB38\uC5D0 \uB300\uD574 \uCE5C\uC808\uD558\uAC8C\
    \ \uC720\uC6A9\uD558\uACE0 \uC0C1\uC138\uD55C \uB2F5\uBCC0\uC744 \uC81C\uACF5\uD569\
    \uB2C8\uB2E4. \\n\" \\\n--in-prefix \"<human>: \" \\\n--in-suffix \"<bot>:\" \\\
    \n--interactive-first\n```\n\nThanks!"
  created_at: 2023-10-28 13:05:46+00:00
  edited: true
  hidden: false
  id: 653d153ae971d8e61f4ec70b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668054183184-62af434f457691d789c36aeb.jpeg?w=200&h=200&f=face
      fullname: Sang Park
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: likejazz
      type: user
    createdAt: '2023-10-28T14:05:46.000Z'
    data:
      status: closed
    id: 653d153ae971d8e61f4ec70c
    type: status-change
  author: likejazz
  created_at: 2023-10-28 13:05:46+00:00
  id: 653d153ae971d8e61f4ec70c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44964942aee8d0dd99862f8b5a38b6d1.svg
      fullname: Junho Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: junhochoi
      type: user
    createdAt: '2023-10-28T16:16:45.000Z'
    data:
      edited: true
      editors:
      - junhochoi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5729414224624634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44964942aee8d0dd99862f8b5a38b6d1.svg
          fullname: Junho Choi
          isHf: false
          isPro: false
          name: junhochoi
          type: user
        html: '<p>LGTM. Thanks!</p>

          '
        raw: LGTM. Thanks!
        updatedAt: '2023-10-28T16:21:01.066Z'
      numEdits: 1
      reactions: []
    id: 653d33ed6bcb9f518a508120
    type: comment
  author: junhochoi
  content: LGTM. Thanks!
  created_at: 2023-10-28 15:16:45+00:00
  edited: true
  hidden: false
  id: 653d33ed6bcb9f518a508120
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: 42dot/42dot_LLM-SFT-1.3B
repo_type: model
status: closed
target_branch: null
title: Running on llama.cpp
