!!python/object:huggingface_hub.community.DiscussionWithDetails
author: azza696
conflicting_files: null
created_at: 2023-04-17 07:21:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
      fullname: azza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azza696
      type: user
    createdAt: '2023-04-17T08:21:09.000Z'
    data:
      edited: true
      editors:
      - azza696
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
          fullname: azza
          isHf: false
          isPro: false
          name: azza696
          type: user
        html: '<p>Hey, thanks a lot for sharing this model.<br>I''ve been trying to
          test it using colab since it should fit their GPU (15360 GB VRAM)<br>But
          when trying to lauch the server using this command : </p><pre>python server.py
          --model Thireus_Vicuna13B-v1.1-8bit-128g --model_type LLaMA --wbits 8 --groupsize
          128 --share </pre><br>The code returns without any message<p></p>

          <pre> bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so

          Loading Thireus_Vicuna13B-v1.1-8bit-128g...

          Found the following quantized model: models/Thireus_Vicuna13B-v1.1-8bit-128g/Vicuna13B-v1.1-8bit-128g.safetensors

          ^C </pre>

          <p>I''ve used both recommended and discouraged method but I have the same
          issue... Am i doing something wrong?</p>

          '
        raw: 'Hey, thanks a lot for sharing this model.

          I''ve been trying to test it using colab since it should fit their GPU (15360
          GB VRAM)

          But when trying to lauch the server using this command : <pre>python server.py
          --model Thireus_Vicuna13B-v1.1-8bit-128g --model_type LLaMA --wbits 8 --groupsize
          128 --share </pre>

          The code returns without any message

          <pre> bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so

          Loading Thireus_Vicuna13B-v1.1-8bit-128g...

          Found the following quantized model: models/Thireus_Vicuna13B-v1.1-8bit-128g/Vicuna13B-v1.1-8bit-128g.safetensors

          ^C </pre>

          I''ve used both recommended and discouraged method but I have the same issue...
          Am i doing something wrong?'
        updatedAt: '2023-04-18T09:44:18.870Z'
      numEdits: 1
      reactions: []
    id: 643d0175a16fa581f3f8f53b
    type: comment
  author: azza696
  content: 'Hey, thanks a lot for sharing this model.

    I''ve been trying to test it using colab since it should fit their GPU (15360
    GB VRAM)

    But when trying to lauch the server using this command : <pre>python server.py
    --model Thireus_Vicuna13B-v1.1-8bit-128g --model_type LLaMA --wbits 8 --groupsize
    128 --share </pre>

    The code returns without any message

    <pre> bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so

    Loading Thireus_Vicuna13B-v1.1-8bit-128g...

    Found the following quantized model: models/Thireus_Vicuna13B-v1.1-8bit-128g/Vicuna13B-v1.1-8bit-128g.safetensors

    ^C </pre>

    I''ve used both recommended and discouraged method but I have the same issue...
    Am i doing something wrong?'
  created_at: 2023-04-17 07:21:09+00:00
  edited: true
  hidden: false
  id: 643d0175a16fa581f3f8f53b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
      fullname: azza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azza696
      type: user
    createdAt: '2023-04-18T08:34:27.000Z'
    data:
      edited: true
      editors:
      - azza696
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
          fullname: azza
          isHf: false
          isPro: false
          name: azza696
          type: user
        html: '<p>Here is the notebook btw : <a rel="nofollow" href="https://colab.research.google.com/drive/1BDOJ3qGypmhrYV3hlhEGNA8T3BxrY5WE?usp=sharing">https://colab.research.google.com/drive/1BDOJ3qGypmhrYV3hlhEGNA8T3BxrY5WE?usp=sharing</a></p>

          '
        raw: 'Here is the notebook btw : https://colab.research.google.com/drive/1BDOJ3qGypmhrYV3hlhEGNA8T3BxrY5WE?usp=sharing'
        updatedAt: '2023-04-18T09:44:07.062Z'
      numEdits: 1
      reactions: []
    id: 643e561339aa9d1bd41a51fd
    type: comment
  author: azza696
  content: 'Here is the notebook btw : https://colab.research.google.com/drive/1BDOJ3qGypmhrYV3hlhEGNA8T3BxrY5WE?usp=sharing'
  created_at: 2023-04-18 07:34:27+00:00
  edited: true
  hidden: false
  id: 643e561339aa9d1bd41a51fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
      fullname: azza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azza696
      type: user
    createdAt: '2023-05-04T07:00:42.000Z'
    data:
      status: closed
    id: 6453581a61910b68fe2979d5
    type: status-change
  author: azza696
  created_at: 2023-05-04 06:00:42+00:00
  id: 6453581a61910b68fe2979d5
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9b5080bc1a49f37345f71c3098dd9c1d.svg
      fullname: azza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azza696
      type: user
    createdAt: '2023-05-04T07:00:47.000Z'
    data:
      status: open
    id: 6453581f0afb444bbee66fec
    type: status-change
  author: azza696
  created_at: 2023-05-04 06:00:47+00:00
  id: 6453581f0afb444bbee66fec
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Thireus/Vicuna13B-v1.1-8bit-128g
repo_type: model
status: open
target_branch: null
title: Can't lauch model using text-generation-webui
