!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andysalerno
conflicting_files: null
created_at: 2024-01-17 05:21:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
      fullname: andy s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andysalerno
      type: user
    createdAt: '2024-01-17T05:21:43.000Z'
    data:
      edited: false
      editors:
      - andysalerno
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.914814829826355
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
          fullname: andy s
          isHf: false
          isPro: false
          name: andysalerno
          type: user
        html: '<p>This model seems to use ChatML, but doesn''t have the &lt;|im_end|&gt;
          special token.</p>

          <p>Honestly this might not be a big problem. But it raises some interesting
          questions.</p>

          <ol>

          <li>Do models have a harder time learning the concept of "stopping" when
          they are required to track a sequence of multiple tokens, i.e. <code>[''&lt;'',
          ''|'', ''im'', ''_'', end'', ''|'', ''&gt;'']</code></li>

          <li>Does this introduce a problem where certain text sequences can make
          ambiguous representation of the stop string? I.e. if a model''s output ends
          with <code>&lt;</code>, then this tokenizes as <code>[''&lt;&lt;'', ''|'',
          ''im'', ''_'', end'', ''|'', ''&gt;'']</code>, and note how this no longer
          matches the tokenization of the expected stop sequence, because two <code>&lt;</code>
          in a row get tokenized as <code>&lt;&lt;</code> instead of <code>[''&lt;'',
          ''&lt;'']</code>. That''s just one example, I suppose there could be more.
          I guess this only has an impact during training, and in rare cases?</li>

          </ol>

          <p>I only bring this up because I am curious if it introduces subtle problems
          that are not easy to notice or can cause reduced model outputs.</p>

          '
        raw: "This model seems to use ChatML, but doesn't have the <|im_end|> special\
          \ token.\r\n\r\nHonestly this might not be a big problem. But it raises\
          \ some interesting questions.\r\n\r\n1. Do models have a harder time learning\
          \ the concept of \"stopping\" when they are required to track a sequence\
          \ of multiple tokens, i.e. `['<', '|', 'im', '_', end', '|', '>']`\r\n2.\
          \ Does this introduce a problem where certain text sequences can make ambiguous\
          \ representation of the stop string? I.e. if a model's output ends with\
          \ `<`, then this tokenizes as `['<<', '|', 'im', '_', end', '|', '>']`,\
          \ and note how this no longer matches the tokenization of the expected stop\
          \ sequence, because two `<` in a row get tokenized as `<<` instead of `['<',\
          \ '<']`. That's just one example, I suppose there could be more. I guess\
          \ this only has an impact during training, and in rare cases?\r\n\r\nI only\
          \ bring this up because I am curious if it introduces subtle problems that\
          \ are not easy to notice or can cause reduced model outputs."
        updatedAt: '2024-01-17T05:21:43.326Z'
      numEdits: 0
      reactions: []
    id: 65a763e73088b44cbb3131fa
    type: comment
  author: andysalerno
  content: "This model seems to use ChatML, but doesn't have the <|im_end|> special\
    \ token.\r\n\r\nHonestly this might not be a big problem. But it raises some interesting\
    \ questions.\r\n\r\n1. Do models have a harder time learning the concept of \"\
    stopping\" when they are required to track a sequence of multiple tokens, i.e.\
    \ `['<', '|', 'im', '_', end', '|', '>']`\r\n2. Does this introduce a problem\
    \ where certain text sequences can make ambiguous representation of the stop string?\
    \ I.e. if a model's output ends with `<`, then this tokenizes as `['<<', '|',\
    \ 'im', '_', end', '|', '>']`, and note how this no longer matches the tokenization\
    \ of the expected stop sequence, because two `<` in a row get tokenized as `<<`\
    \ instead of `['<', '<']`. That's just one example, I suppose there could be more.\
    \ I guess this only has an impact during training, and in rare cases?\r\n\r\n\
    I only bring this up because I am curious if it introduces subtle problems that\
    \ are not easy to notice or can cause reduced model outputs."
  created_at: 2024-01-17 05:21:43+00:00
  edited: false
  hidden: false
  id: 65a763e73088b44cbb3131fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a82cbe88ee4e6f3eed77b90551fbf06.svg
      fullname: Akhil Pratap Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akhil3417
      type: user
    createdAt: '2024-01-17T05:37:35.000Z'
    data:
      edited: false
      editors:
      - akhil3417
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9670819640159607
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a82cbe88ee4e6f3eed77b90551fbf06.svg
          fullname: Akhil Pratap Singh
          isHf: false
          isPro: false
          name: akhil3417
          type: user
        html: '<p>Ambiguities like this could potentially arise and confuse the model
          during training. However, these issues may not significantly degrade model
          performance unless encountered frequently. Nonetheless, using a single token
          as a stopping signal can help avoid such potential problems.</p>

          '
        raw: Ambiguities like this could potentially arise and confuse the model during
          training. However, these issues may not significantly degrade model performance
          unless encountered frequently. Nonetheless, using a single token as a stopping
          signal can help avoid such potential problems.
        updatedAt: '2024-01-17T05:37:35.201Z'
      numEdits: 0
      reactions: []
    id: 65a7679f3efe2c547c6ceca4
    type: comment
  author: akhil3417
  content: Ambiguities like this could potentially arise and confuse the model during
    training. However, these issues may not significantly degrade model performance
    unless encountered frequently. Nonetheless, using a single token as a stopping
    signal can help avoid such potential problems.
  created_at: 2024-01-17 05:37:35+00:00
  edited: false
  hidden: false
  id: 65a7679f3efe2c547c6ceca4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0352cb4998ff719f20a6e0fb01ab7d54.svg
      fullname: Dustin Ewan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DustinEwan
      type: user
    createdAt: '2024-01-17T21:05:26.000Z'
    data:
      edited: false
      editors:
      - DustinEwan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497973322868347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0352cb4998ff719f20a6e0fb01ab7d54.svg
          fullname: Dustin Ewan
          isHf: false
          isPro: false
          name: DustinEwan
          type: user
        html: '<p>My experience is that the mis-match between the mistral style  stop
          token and chatml style &lt;|im_end|&gt; stop token is causing the model
          to go on to generate synthetic user messages that it will then answer itself.</p>

          <p>Definitely seems like something that should be addressed.</p>

          '
        raw: 'My experience is that the mis-match between the mistral style </s> stop
          token and chatml style <|im_end|> stop token is causing the model to go
          on to generate synthetic user messages that it will then answer itself.


          Definitely seems like something that should be addressed.'
        updatedAt: '2024-01-17T21:05:26.456Z'
      numEdits: 0
      reactions: []
    id: 65a8411655b4aa2cf4f9b8c6
    type: comment
  author: DustinEwan
  content: 'My experience is that the mis-match between the mistral style </s> stop
    token and chatml style <|im_end|> stop token is causing the model to go on to
    generate synthetic user messages that it will then answer itself.


    Definitely seems like something that should be addressed.'
  created_at: 2024-01-17 21:05:26+00:00
  edited: false
  hidden: false
  id: 65a8411655b4aa2cf4f9b8c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a82cbe88ee4e6f3eed77b90551fbf06.svg
      fullname: Akhil Pratap Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akhil3417
      type: user
    createdAt: '2024-01-18T05:49:05.000Z'
    data:
      edited: true
      editors:
      - akhil3417
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9029939770698547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a82cbe88ee4e6f3eed77b90551fbf06.svg
          fullname: Akhil Pratap Singh
          isHf: false
          isPro: false
          name: akhil3417
          type: user
        html: '<blockquote>

          <p>model to go on to generate synthetic user messages that it will then
          answer itself.</p>

          </blockquote>

          <p>haha , facing this same issue .  </p>

          '
        raw: "> model to go on to generate synthetic user messages that it will then\
          \ answer itself.\n\nhaha , facing this same issue .  \n\n"
        updatedAt: '2024-01-22T13:09:40.151Z'
      numEdits: 1
      reactions: []
    id: 65a8bbd165e4f1a5eb5585a8
    type: comment
  author: akhil3417
  content: "> model to go on to generate synthetic user messages that it will then\
    \ answer itself.\n\nhaha , facing this same issue .  \n\n"
  created_at: 2024-01-18 05:49:05+00:00
  edited: true
  hidden: false
  id: 65a8bbd165e4f1a5eb5585a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-18T13:39:43.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575837254524231
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Hey thanks for the feedback. I had a few discussions about this
          issue, it''s a tricky question. I updated the tokenizer''s config and created
          a new GGUF version of the model here: <a href="https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2">https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2</a></p>

          <p>Do you mind testing it to tell me if it''s fixed or not? I''d also be
          interested in examples where the model doesn''t behave as expected.</p>

          '
        raw: 'Hey thanks for the feedback. I had a few discussions about this issue,
          it''s a tricky question. I updated the tokenizer''s config and created a
          new GGUF version of the model here: https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2


          Do you mind testing it to tell me if it''s fixed or not? I''d also be interested
          in examples where the model doesn''t behave as expected.'
        updatedAt: '2024-01-18T13:39:43.906Z'
      numEdits: 0
      reactions: []
    id: 65a92a1f50f85a33ed542e6a
    type: comment
  author: mlabonne
  content: 'Hey thanks for the feedback. I had a few discussions about this issue,
    it''s a tricky question. I updated the tokenizer''s config and created a new GGUF
    version of the model here: https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2


    Do you mind testing it to tell me if it''s fixed or not? I''d also be interested
    in examples where the model doesn''t behave as expected.'
  created_at: 2024-01-18 13:39:43+00:00
  edited: false
  hidden: false
  id: 65a92a1f50f85a33ed542e6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
      fullname: andy s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andysalerno
      type: user
    createdAt: '2024-01-20T22:21:04.000Z'
    data:
      edited: false
      editors:
      - andysalerno
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.922747790813446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
          fullname: andy s
          isHf: false
          isPro: false
          name: andysalerno
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mlabonne\">@<span class=\"\
          underline\">mlabonne</span></a></span>\n\n\t</span></span> I believe it's\
          \ not enough to simply update the tokenizer config - I think the model itself\
          \ needs to be updated to have a slot for the new token IDs in the input/output\
          \ layers. Check out this very recent PR in trl which adds a helper for doing\
          \ exactly this, seems pretty interesting:</p>\n<p><a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/trl/pull/1242/files\">https://github.com/huggingface/trl/pull/1242/files</a></p>\n"
        raw: '@mlabonne I believe it''s not enough to simply update the tokenizer
          config - I think the model itself needs to be updated to have a slot for
          the new token IDs in the input/output layers. Check out this very recent
          PR in trl which adds a helper for doing exactly this, seems pretty interesting:


          https://github.com/huggingface/trl/pull/1242/files'
        updatedAt: '2024-01-20T22:21:04.082Z'
      numEdits: 0
      reactions: []
    id: 65ac475014e6582c307c8c32
    type: comment
  author: andysalerno
  content: '@mlabonne I believe it''s not enough to simply update the tokenizer config
    - I think the model itself needs to be updated to have a slot for the new token
    IDs in the input/output layers. Check out this very recent PR in trl which adds
    a helper for doing exactly this, seems pretty interesting:


    https://github.com/huggingface/trl/pull/1242/files'
  created_at: 2024-01-20 22:21:04+00:00
  edited: false
  hidden: false
  id: 65ac475014e6582c307c8c32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-21T13:41:27.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956050455570221
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Yes, that doesn''t change the model but it helps depending on the
          inference tool you''re using (looking at you vllm). The model can use different
          chat templates by default, including chatml.</p>

          <p>I''ll read it, thanks!</p>

          '
        raw: 'Yes, that doesn''t change the model but it helps depending on the inference
          tool you''re using (looking at you vllm). The model can use different chat
          templates by default, including chatml.


          I''ll read it, thanks!'
        updatedAt: '2024-01-21T13:41:27.382Z'
      numEdits: 0
      reactions: []
    id: 65ad1f076a55aac02a7212b4
    type: comment
  author: mlabonne
  content: 'Yes, that doesn''t change the model but it helps depending on the inference
    tool you''re using (looking at you vllm). The model can use different chat templates
    by default, including chatml.


    I''ll read it, thanks!'
  created_at: 2024-01-21 13:41:27+00:00
  edited: false
  hidden: false
  id: 65ad1f076a55aac02a7212b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: J22
      type: user
    createdAt: '2024-01-22T02:22:27.000Z'
    data:
      edited: false
      editors:
      - J22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6144759058952332
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: J22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;andysalerno&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andysalerno\"\
          >@<span class=\"underline\">andysalerno</span></a></span>\n\n\t</span></span>\
          \ FYI:  Use token list <code>['&lt;', '|', 'im', '_', end', '|', '&gt;']</code>\
          \ as stopping works fine to me. See <a rel=\"nofollow\" href=\"https://github.com/foldl/chatllm.cpp\"\
          >ChatLLM.cpp</a>.</p>\n"
        raw: '@andysalerno FYI:  Use token list `[''<'', ''|'', ''im'', ''_'', end'',
          ''|'', ''>'']` as stopping works fine to me. See [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp).'
        updatedAt: '2024-01-22T02:22:27.185Z'
      numEdits: 0
      reactions: []
    id: 65add16350f85a33edfa89ad
    type: comment
  author: J22
  content: '@andysalerno FYI:  Use token list `[''<'', ''|'', ''im'', ''_'', end'',
    ''|'', ''>'']` as stopping works fine to me. See [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp).'
  created_at: 2024-01-22 02:22:27+00:00
  edited: false
  hidden: false
  id: 65add16350f85a33edfa89ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0352cb4998ff719f20a6e0fb01ab7d54.svg
      fullname: Dustin Ewan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DustinEwan
      type: user
    createdAt: '2024-01-22T11:14:41.000Z'
    data:
      edited: false
      editors:
      - DustinEwan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9174934029579163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0352cb4998ff719f20a6e0fb01ab7d54.svg
          fullname: Dustin Ewan
          isHf: false
          isPro: false
          name: DustinEwan
          type: user
        html: '<p>You can just use <code>[''&lt;|im_end|&gt;'']</code>, otherwise
          the model would stop prematurely on any of those tokens.  The braces, pipe,
          and underscore are commonly used in programming.</p>

          '
        raw: You can just use `['<|im_end|>']`, otherwise the model would stop prematurely
          on any of those tokens.  The braces, pipe, and underscore are commonly used
          in programming.
        updatedAt: '2024-01-22T11:14:41.639Z'
      numEdits: 0
      reactions: []
    id: 65ae4e2169cd2991eff43fe9
    type: comment
  author: DustinEwan
  content: You can just use `['<|im_end|>']`, otherwise the model would stop prematurely
    on any of those tokens.  The braces, pipe, and underscore are commonly used in
    programming.
  created_at: 2024-01-22 11:14:41+00:00
  edited: false
  hidden: false
  id: 65ae4e2169cd2991eff43fe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
      fullname: andy s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andysalerno
      type: user
    createdAt: '2024-01-22T20:42:52.000Z'
    data:
      edited: false
      editors:
      - andysalerno
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9558449387550354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
          fullname: andy s
          isHf: false
          isPro: false
          name: andysalerno
          type: user
        html: '<p>I think the above are both solutions if the problem is "I want this
          model to work properly in llama.cpp today" or the like.</p>

          <p>But from a more general standpoint, it seems intuitive to me that having
          the stop sequence as a special, dedicated eos_token is superior. For a few
          reasons. 1. standardization - it''s ideal if the ChatML (or whichever) format
          becomes the norm, not just the <code>chat_template</code> but also the <code>special_tokens</code>
          set. 2. generation quality - this is purely speculation on my part, but
          it makes sense that a single, special token for eos is superior to multiple,
          non-special tokens like [''&lt;'', ''|'', ...] for denoting end of turn
          in chat models. 3. from a text-streaming point of view, if you have a stateless
          API that''s streaming tokens, you would need to keep track of the last 7
          tokens to know if they were [''&lt;'', ''|'', ''im''...] in order to stop
          streaming. But with a dedicated special eos token, you don''t need to keep
          any state, the moment you see token_id 32002, you know to stop. (This is
          a problem I have been encountering lately)</p>

          '
        raw: 'I think the above are both solutions if the problem is "I want this
          model to work properly in llama.cpp today" or the like.


          But from a more general standpoint, it seems intuitive to me that having
          the stop sequence as a special, dedicated eos_token is superior. For a few
          reasons. 1. standardization - it''s ideal if the ChatML (or whichever) format
          becomes the norm, not just the `chat_template` but also the `special_tokens`
          set. 2. generation quality - this is purely speculation on my part, but
          it makes sense that a single, special token for eos is superior to multiple,
          non-special tokens like [''<'', ''|'', ...] for denoting end of turn in
          chat models. 3. from a text-streaming point of view, if you have a stateless
          API that''s streaming tokens, you would need to keep track of the last 7
          tokens to know if they were [''<'', ''|'', ''im''...] in order to stop streaming.
          But with a dedicated special eos token, you don''t need to keep any state,
          the moment you see token_id 32002, you know to stop. (This is a problem
          I have been encountering lately)'
        updatedAt: '2024-01-22T20:42:52.027Z'
      numEdits: 0
      reactions: []
    id: 65aed34c563e362f55393058
    type: comment
  author: andysalerno
  content: 'I think the above are both solutions if the problem is "I want this model
    to work properly in llama.cpp today" or the like.


    But from a more general standpoint, it seems intuitive to me that having the stop
    sequence as a special, dedicated eos_token is superior. For a few reasons. 1.
    standardization - it''s ideal if the ChatML (or whichever) format becomes the
    norm, not just the `chat_template` but also the `special_tokens` set. 2. generation
    quality - this is purely speculation on my part, but it makes sense that a single,
    special token for eos is superior to multiple, non-special tokens like [''<'',
    ''|'', ...] for denoting end of turn in chat models. 3. from a text-streaming
    point of view, if you have a stateless API that''s streaming tokens, you would
    need to keep track of the last 7 tokens to know if they were [''<'', ''|'', ''im''...]
    in order to stop streaming. But with a dedicated special eos token, you don''t
    need to keep any state, the moment you see token_id 32002, you know to stop. (This
    is a problem I have been encountering lately)'
  created_at: 2024-01-22 20:42:52+00:00
  edited: false
  hidden: false
  id: 65aed34c563e362f55393058
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-23T09:07:22.000Z'
    data:
      edited: false
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8915934562683105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: '<blockquote>

          <p>Hey thanks for the feedback. I had a few discussions about this issue,
          it''s a tricky question. I updated the tokenizer''s config and created a
          new GGUF version of the model here: <a href="https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2">https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2</a></p>

          <p>Do you mind testing it to tell me if it''s fixed or not? I''d also be
          interested in examples where the model doesn''t behave as expected.</p>

          </blockquote>

          <p>Maybe using this approach will help? <a href="https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser">https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser</a></p>

          <blockquote>

          <p>Prompt format: This model uses ChatML prompt format. NEW - &lt;|im_end|&gt;
          maps to token_id 2. This is the same token_id as  so applications that depend
          on EOS being token_id 2 (koboldAI) will work! (Thanks Henky for the feedback)</p>

          </blockquote>

          <p>As for example:</p>

          <p>I did a quantize on the latest version, for MLX. In my testing the model
          continues to ramble unfortunately. It does not output anymore &lt;|im_end|&gt;
          or &lt;|im_start|&gt; but now outputs &lt;/s&gt; and continues to answer
          itself.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/4GqYvEyatKQNzG051ankR.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/4GqYvEyatKQNzG051ankR.png"></a></p>

          '
        raw: "> Hey thanks for the feedback. I had a few discussions about this issue,\
          \ it's a tricky question. I updated the tokenizer's config and created a\
          \ new GGUF version of the model here: https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2\n\
          > \n> Do you mind testing it to tell me if it's fixed or not? I'd also be\
          \ interested in examples where the model doesn't behave as expected.\n\n\
          Maybe using this approach will help? https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\n\
          \n> Prompt format: This model uses ChatML prompt format. NEW - <|im_end|>\
          \ maps to token_id 2. This is the same token_id as </s> so applications\
          \ that depend on EOS being token_id 2 (koboldAI) will work! (Thanks Henky\
          \ for the feedback)\n\nAs for example:\n\nI did a quantize on the latest\
          \ version, for MLX. In my testing the model continues to ramble unfortunately.\
          \ It does not output anymore <|im_end|> or <|im_start|> but now outputs\
          \ <\\/s> and continues to answer itself.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/4GqYvEyatKQNzG051ankR.png)\n"
        updatedAt: '2024-01-23T09:07:22.303Z'
      numEdits: 0
      reactions: []
    id: 65af81ca101482afcc8b6fa6
    type: comment
  author: azinca
  content: "> Hey thanks for the feedback. I had a few discussions about this issue,\
    \ it's a tricky question. I updated the tokenizer's config and created a new GGUF\
    \ version of the model here: https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF-v2\n\
    > \n> Do you mind testing it to tell me if it's fixed or not? I'd also be interested\
    \ in examples where the model doesn't behave as expected.\n\nMaybe using this\
    \ approach will help? https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\n\
    \n> Prompt format: This model uses ChatML prompt format. NEW - <|im_end|> maps\
    \ to token_id 2. This is the same token_id as </s> so applications that depend\
    \ on EOS being token_id 2 (koboldAI) will work! (Thanks Henky for the feedback)\n\
    \nAs for example:\n\nI did a quantize on the latest version, for MLX. In my testing\
    \ the model continues to ramble unfortunately. It does not output anymore <|im_end|>\
    \ or <|im_start|> but now outputs <\\/s> and continues to answer itself.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/4GqYvEyatKQNzG051ankR.png)\n"
  created_at: 2024-01-23 09:07:22+00:00
  edited: false
  hidden: false
  id: 65af81ca101482afcc8b6fa6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-23T11:00:45.000Z'
    data:
      edited: true
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.936127245426178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>I tried a hacky solution that replaces the <code>&lt;s&gt;</code>
          and <code>&lt;/s&gt;</code> tokens. I cannot reproduce this behavior so
          I assume it''s a problem with the default configuration of the frontend
          you use. I''d like to get it right so people don''t have to care about setting
          the right tokens. Let me know if that new version works, thanks!</p>

          '
        raw: I tried a hacky solution that replaces the `<s>` and `</s>` tokens. I
          cannot reproduce this behavior so I assume it's a problem with the default
          configuration of the frontend you use. I'd like to get it right so people
          don't have to care about setting the right tokens. Let me know if that new
          version works, thanks!
        updatedAt: '2024-01-23T13:11:58.446Z'
      numEdits: 1
      reactions: []
    id: 65af9c5dfcbe71d6d5222cea
    type: comment
  author: mlabonne
  content: I tried a hacky solution that replaces the `<s>` and `</s>` tokens. I cannot
    reproduce this behavior so I assume it's a problem with the default configuration
    of the frontend you use. I'd like to get it right so people don't have to care
    about setting the right tokens. Let me know if that new version works, thanks!
  created_at: 2024-01-23 11:00:45+00:00
  edited: true
  hidden: false
  id: 65af9c5dfcbe71d6d5222cea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-23T12:27:26.000Z'
    data:
      edited: true
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034480452537537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: '<p>I just noticed that mlx_lm converter (to MLX) changes the config
          (special_tokens_map.json, tokenizer_config.json, etc). I don''t know exactly
          why. Files look similar, but not quite your version. I guess that is the
          problem in my case. I opened an issue here <a rel="nofollow" href="https://github.com/ml-explore/mlx-examples/issues/355">https://github.com/ml-explore/mlx-examples/issues/355</a></p>

          '
        raw: I just noticed that mlx_lm converter (to MLX) changes the config (special_tokens_map.json,
          tokenizer_config.json, etc). I don't know exactly why. Files look similar,
          but not quite your version. I guess that is the problem in my case. I opened
          an issue here https://github.com/ml-explore/mlx-examples/issues/355
        updatedAt: '2024-01-23T12:45:40.175Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mlabonne
    id: 65afb0ae06916708a94eef8d
    type: comment
  author: azinca
  content: I just noticed that mlx_lm converter (to MLX) changes the config (special_tokens_map.json,
    tokenizer_config.json, etc). I don't know exactly why. Files look similar, but
    not quite your version. I guess that is the problem in my case. I opened an issue
    here https://github.com/ml-explore/mlx-examples/issues/355
  created_at: 2024-01-23 12:27:26+00:00
  edited: true
  hidden: false
  id: 65afb0ae06916708a94eef8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-23T15:42:09.000Z'
    data:
      edited: true
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8757372498512268
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: "<p>Would it be possible (or help) to use this trick here?</p>\n<blockquote>\n\
          <p>&lt;|im_end|&gt; maps to token_id 2. This is the same token_id as  so\
          \ applications that depend on EOS being token_id 2 will work!</p>\n</blockquote>\n\
          <p><a href=\"https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser/raw/main/tokenizer_config.json\"\
          >https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser/raw/main/tokenizer_config.json</a></p>\n\
          <p>See</p>\n<pre><code class=\"language-json\"><span class=\"hljs-attr\"\
          >\"vocab\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-punctuation\">{</span>\n...\n    <span class=\"hljs-attr\">\"&lt;s&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >1</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\"\
          >\"&lt;unk&gt;\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">0</span><span class=\"hljs-punctuation\">,</span>\n    <span\
          \ class=\"hljs-attr\">\"&lt;|im_end|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">2</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"&lt;|im_start|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32000</span><span\
          \ class=\"hljs-punctuation\">,</span>\n...\n<span class=\"hljs-punctuation\"\
          >}</span>\n</code></pre>\n"
        raw: "Would it be possible (or help) to use this trick here?\n> <|im_end|>\
          \ maps to token_id 2. This is the same token_id as </s> so applications\
          \ that depend on EOS being token_id 2 will work!\n\nhttps://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser/raw/main/tokenizer_config.json\n\
          \nSee\n\n```json\n\"vocab\": {\n...\n    \"<s>\": 1,\n    \"<unk>\": 0,\n\
          \    \"<|im_end|>\": 2,\n    \"<|im_start|>\": 32000,\n...\n}\n```"
        updatedAt: '2024-01-23T15:59:38.445Z'
      numEdits: 4
      reactions: []
    id: 65afde5149a955de36dde8f2
    type: comment
  author: azinca
  content: "Would it be possible (or help) to use this trick here?\n> <|im_end|> maps\
    \ to token_id 2. This is the same token_id as </s> so applications that depend\
    \ on EOS being token_id 2 will work!\n\nhttps://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser/raw/main/tokenizer_config.json\n\
    \nSee\n\n```json\n\"vocab\": {\n...\n    \"<s>\": 1,\n    \"<unk>\": 0,\n    \"\
    <|im_end|>\": 2,\n    \"<|im_start|>\": 32000,\n...\n}\n```"
  created_at: 2024-01-23 15:42:09+00:00
  edited: true
  hidden: false
  id: 65afde5149a955de36dde8f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-23T16:17:21.000Z'
    data:
      edited: false
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8468473553657532
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: "<p>Just ignore my last request please. Now that I understand a bit\
          \ more, I found a fix/hack on my side. For now 2 and 32000 are pretty much\
          \ standard for ChatML models, so it should work fine:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">if</span> token <span class=\"\
          hljs-keyword\">in</span> [<span class=\"hljs-number\">2</span>, <span class=\"\
          hljs-number\">32000</span>, tokenizer.eos_token_id]:\n            <span\
          \ class=\"hljs-keyword\">break</span>\n</code></pre>\n"
        raw: "Just ignore my last request please. Now that I understand a bit more,\
          \ I found a fix/hack on my side. For now 2 and 32000 are pretty much standard\
          \ for ChatML models, so it should work fine:\n\n```python\nif token in [2,\
          \ 32000, tokenizer.eos_token_id]:\n            break\n```"
        updatedAt: '2024-01-23T16:17:21.272Z'
      numEdits: 0
      reactions: []
    id: 65afe691a0b4bf3b0e97d86f
    type: comment
  author: azinca
  content: "Just ignore my last request please. Now that I understand a bit more,\
    \ I found a fix/hack on my side. For now 2 and 32000 are pretty much standard\
    \ for ChatML models, so it should work fine:\n\n```python\nif token in [2, 32000,\
    \ tokenizer.eos_token_id]:\n            break\n```"
  created_at: 2024-01-23 16:17:21+00:00
  edited: false
  hidden: false
  id: 65afe691a0b4bf3b0e97d86f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-23T17:17:08.000Z'
    data:
      edited: true
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8838778138160706
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: '<p>Frankly, I do not know what is happening. It is not even an issue
          of my frontend not stopping on &lt;|im_end|&gt; or &lt;/s&gt; anymore. The
          model continues to write and there is no real stop word in there to be able
          to use it. I have 3 other chatml models, all work fine (same code).</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/cUjIrkDnh0yul6Kd2yvUs.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/cUjIrkDnh0yul6Kd2yvUs.png"></a></p>

          '
        raw: 'Frankly, I do not know what is happening. It is not even an issue of
          my frontend not stopping on <|im_end|> or <\/s> anymore. The model continues
          to write and there is no real stop word in there to be able to use it. I
          have 3 other chatml models, all work fine (same code).


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/cUjIrkDnh0yul6Kd2yvUs.png)

          '
        updatedAt: '2024-01-23T17:17:43.975Z'
      numEdits: 1
      reactions: []
    id: 65aff49483b7d2e7edb3027f
    type: comment
  author: azinca
  content: 'Frankly, I do not know what is happening. It is not even an issue of my
    frontend not stopping on <|im_end|> or <\/s> anymore. The model continues to write
    and there is no real stop word in there to be able to use it. I have 3 other chatml
    models, all work fine (same code).


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65622c5d2c671784e2d50688/cUjIrkDnh0yul6Kd2yvUs.png)

    '
  created_at: 2024-01-23 17:17:08+00:00
  edited: true
  hidden: false
  id: 65aff49483b7d2e7edb3027f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
      fullname: Florian.J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FlorianJc
      type: user
    createdAt: '2024-01-24T00:17:43.000Z'
    data:
      edited: true
      editors:
      - FlorianJc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9178071022033691
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
          fullname: Florian.J
          isHf: false
          isPro: false
          name: FlorianJc
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;azinca&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/azinca\">@<span class=\"\
          underline\">azinca</span></a></span>\n\n\t</span></span> : You have two\
          \ problems, the default chat template does not new line caracters.</p>\n\
          <p>And It seems that the last models from <span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mlabonne\"\
          >@<span class=\"underline\">mlabonne</span></a></span>\n\n\t</span></span>\
          \ does never generate the eos token on vllm.</p>\n<p>I don't know if this\
          \ is the origin of the problem (maybe a clue?), but I assume that the <code>config.json</code>\
          \ file is generated by <code>transformers.save_pretrained</code> and is\
          \ related to the training configuration.</p>\n<p>Now, the config.json defines\
          \ <code>\"vocab_size\": 32000</code>, which should be <code>32002</code>,\
          \ as we need to count <code>&lt;|im_start|&gt;</code> and <code>&lt;|im_end|&gt;</code>.</p>\n\
          <p>Setting this value manually causes an error.<br>So I think the problem\
          \ comes from the model and not from the configuration.</p>\n<p>After some\
          \ tests, it is indeed possible to encode <code>\"&lt;|im_end|&gt;\" =&gt;\
          \ 32000</code> with the tokenizer (if you print the tokens_ids after a request,\
          \ the token is encoded fine).<br>The problem is that the model never generates\
          \ this token.</p>\n<p>The fact that it seems to work with llama.cpp (I haven't\
          \ tried myself) could also be a clue, but I don't know enough about it.</p>\n"
        raw: '@azinca : You have two problems, the default chat template does not
          new line caracters.


          And It seems that the last models from @mlabonne does never generate the
          eos token on vllm.


          I don''t know if this is the origin of the problem (maybe a clue?), but
          I assume that the `config.json` file is generated by `transformers.save_pretrained`
          and is related to the training configuration.


          Now, the config.json defines `"vocab_size": 32000`, which should be `32002`,
          as we need to count `<|im_start|>` and `<|im_end|>`.


          Setting this value manually causes an error.

          So I think the problem comes from the model and not from the configuration.


          After some tests, it is indeed possible to encode `"<|im_end|>" => 32000`
          with the tokenizer (if you print the tokens_ids after a request, the token
          is encoded fine).

          The problem is that the model never generates this token.


          The fact that it seems to work with llama.cpp (I haven''t tried myself)
          could also be a clue, but I don''t know enough about it.'
        updatedAt: '2024-01-24T00:29:47.820Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - azinca
    id: 65b05727a46e8751c4028ca0
    type: comment
  author: FlorianJc
  content: '@azinca : You have two problems, the default chat template does not new
    line caracters.


    And It seems that the last models from @mlabonne does never generate the eos token
    on vllm.


    I don''t know if this is the origin of the problem (maybe a clue?), but I assume
    that the `config.json` file is generated by `transformers.save_pretrained` and
    is related to the training configuration.


    Now, the config.json defines `"vocab_size": 32000`, which should be `32002`, as
    we need to count `<|im_start|>` and `<|im_end|>`.


    Setting this value manually causes an error.

    So I think the problem comes from the model and not from the configuration.


    After some tests, it is indeed possible to encode `"<|im_end|>" => 32000` with
    the tokenizer (if you print the tokens_ids after a request, the token is encoded
    fine).

    The problem is that the model never generates this token.


    The fact that it seems to work with llama.cpp (I haven''t tried myself) could
    also be a clue, but I don''t know enough about it.'
  created_at: 2024-01-24 00:17:43+00:00
  edited: true
  hidden: false
  id: 65b05727a46e8751c4028ca0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-24T02:14:15.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960667073726654
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: '<p>Same problem, would like to get a copy of the effective solution.
          Tried several open source models with this problem, and feel that it is
          a problem with the mistral architecture using the chatml format for fine
          tuning.</p>

          '
        raw: Same problem, would like to get a copy of the effective solution. Tried
          several open source models with this problem, and feel that it is a problem
          with the mistral architecture using the chatml format for fine tuning.
        updatedAt: '2024-01-24T02:14:15.451Z'
      numEdits: 0
      reactions: []
    id: 65b07277fdf08903343f5caf
    type: comment
  author: Liangmingxin
  content: Same problem, would like to get a copy of the effective solution. Tried
    several open source models with this problem, and feel that it is a problem with
    the mistral architecture using the chatml format for fine tuning.
  created_at: 2024-01-24 02:14:15+00:00
  edited: false
  hidden: false
  id: 65b07277fdf08903343f5caf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-24T03:54:11.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9583956599235535
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: '<blockquote>

          <p>Same problem, would like to get a copy of the effective solution. Tried
          several open source models with this problem, and feel that it is a problem
          with the mistral architecture using the chatml format for fine tuning.</p>

          </blockquote>

          <p>It may also have something to do with the use of the bagel dataset, which
          mixes a variety of chat_templates.</p>

          '
        raw: '> Same problem, would like to get a copy of the effective solution.
          Tried several open source models with this problem, and feel that it is
          a problem with the mistral architecture using the chatml format for fine
          tuning.


          It may also have something to do with the use of the bagel dataset, which
          mixes a variety of chat_templates.'
        updatedAt: '2024-01-24T03:54:11.728Z'
      numEdits: 0
      reactions: []
    id: 65b089e37febbcc2afd0cb8d
    type: comment
  author: Liangmingxin
  content: '> Same problem, would like to get a copy of the effective solution. Tried
    several open source models with this problem, and feel that it is a problem with
    the mistral architecture using the chatml format for fine tuning.


    It may also have something to do with the use of the bagel dataset, which mixes
    a variety of chat_templates.'
  created_at: 2024-01-24 03:54:11+00:00
  edited: false
  hidden: false
  id: 65b089e37febbcc2afd0cb8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: J22
      type: user
    createdAt: '2024-01-24T04:49:59.000Z'
    data:
      edited: false
      editors:
      - J22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295787811279297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: J22
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Liangmingxin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Liangmingxin\"\
          >@<span class=\"underline\">Liangmingxin</span></a></span>\n\n\t</span></span>\
          \ Have you tried ChatLLM.cpp? It supports  token list <code>&lt;|im_end|&gt;</code>\
          \ as stopping well.</p>\n"
        raw: '@Liangmingxin Have you tried ChatLLM.cpp? It supports  token list `<|im_end|>`
          as stopping well.'
        updatedAt: '2024-01-24T04:49:59.051Z'
      numEdits: 0
      reactions: []
    id: 65b096f7b2c2baf2ff0f37ff
    type: comment
  author: J22
  content: '@Liangmingxin Have you tried ChatLLM.cpp? It supports  token list `<|im_end|>`
    as stopping well.'
  created_at: 2024-01-24 04:49:59+00:00
  edited: false
  hidden: false
  id: 65b096f7b2c2baf2ff0f37ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
      fullname: Andrei Zinca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azinca
      type: user
    createdAt: '2024-01-24T21:49:27.000Z'
    data:
      edited: true
      editors:
      - azinca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9607588648796082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98b7ca88f7ab981eda59a5cbfb0c233c.svg
          fullname: Andrei Zinca
          isHf: false
          isPro: false
          name: azinca
          type: user
        html: '<blockquote>

          <p>I tried a hacky solution that replaces the <code>&lt;s&gt;</code> and
          <code>&lt;/s&gt;</code> tokens. I cannot reproduce this behavior so I assume
          it''s a problem with the default configuration of the frontend you use.
          I''d like to get it right so people don''t have to care about setting the
          right tokens. Let me know if that new version works, thanks!</p>

          </blockquote>

          <p>Sorry for all this trouble but could you please undo that last change
          (or even better, the last 2 changes), because with it the model may not
          output any kind of stop word now.</p>

          <p>I found that the json configs at the time this model was made were quite
          OK: <a href="https://huggingface.co/mlx-community/NeuralBeagle14-7B-4bit-mlx/tree/main">https://huggingface.co/mlx-community/NeuralBeagle14-7B-4bit-mlx/tree/main</a></p>

          <p>Testing the model above I implemented a work-around on my front-end by
          watching explicitly for either "&lt;|im_end|&gt;" or "&lt;/s&gt;" text,
          since the tokenizer.eos_token_id is not very reliable, as is the case for
          this model (which I like a lot, and that is why I''ve been persistent in
          trying to make it work with my frontend to mlx-lm).</p>

          '
        raw: '> I tried a hacky solution that replaces the `<s>` and `</s>` tokens.
          I cannot reproduce this behavior so I assume it''s a problem with the default
          configuration of the frontend you use. I''d like to get it right so people
          don''t have to care about setting the right tokens. Let me know if that
          new version works, thanks!


          Sorry for all this trouble but could you please undo that last change (or
          even better, the last 2 changes), because with it the model may not output
          any kind of stop word now.


          I found that the json configs at the time this model was made were quite
          OK: https://huggingface.co/mlx-community/NeuralBeagle14-7B-4bit-mlx/tree/main


          Testing the model above I implemented a work-around on my front-end by watching
          explicitly for either "<|im_end|>" or "<\/s>" text, since the tokenizer.eos_token_id
          is not very reliable, as is the case for this model (which I like a lot,
          and that is why I''ve been persistent in trying to make it work with my
          frontend to mlx-lm).'
        updatedAt: '2024-01-24T21:53:45.826Z'
      numEdits: 3
      reactions: []
    id: 65b185e7b389ca2de11eb798
    type: comment
  author: azinca
  content: '> I tried a hacky solution that replaces the `<s>` and `</s>` tokens.
    I cannot reproduce this behavior so I assume it''s a problem with the default
    configuration of the frontend you use. I''d like to get it right so people don''t
    have to care about setting the right tokens. Let me know if that new version works,
    thanks!


    Sorry for all this trouble but could you please undo that last change (or even
    better, the last 2 changes), because with it the model may not output any kind
    of stop word now.


    I found that the json configs at the time this model was made were quite OK: https://huggingface.co/mlx-community/NeuralBeagle14-7B-4bit-mlx/tree/main


    Testing the model above I implemented a work-around on my front-end by watching
    explicitly for either "<|im_end|>" or "<\/s>" text, since the tokenizer.eos_token_id
    is not very reliable, as is the case for this model (which I like a lot, and that
    is why I''ve been persistent in trying to make it work with my frontend to mlx-lm).'
  created_at: 2024-01-24 21:49:27+00:00
  edited: true
  hidden: false
  id: 65b185e7b389ca2de11eb798
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mlabonne/NeuralBeagle14-7B
repo_type: model
status: open
target_branch: null
title: Model follows ChatML format, but does not have the special tokens for ChatML
