!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2024-01-21 06:59:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2024-01-21T06:59:30.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4132692813873291
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>Google Colab</p>\n<pre><code class=\"language-Python\">!pip install\
          \ -qU transformers accelerate\n\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          <span class=\"hljs-keyword\">import</span> transformers\n<span class=\"\
          hljs-keyword\">import</span> torch\n\nmodel = <span class=\"hljs-string\"\
          >\"mlabonne/NeuralBeagle14-7B\"</span>\nmessages = [{<span class=\"hljs-string\"\
          >\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"What is\
          \ a large language model?\"</span>}]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          prompt = tokenizer.apply_chat_template(messages, tokenize=<span class=\"\
          hljs-literal\">False</span>, add_generation_prompt=<span class=\"hljs-literal\"\
          >True</span>)\npipeline = transformers.pipeline(\n    <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n    model=model,\n    torch_dtype=torch.float16,\n\
          \    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n)\n\noutputs\
          \ = pipeline(prompt, max_new_tokens=<span class=\"hljs-number\">256</span>,\
          \ do_sample=<span class=\"hljs-literal\">True</span>, temperature=<span\
          \ class=\"hljs-number\">0.7</span>, top_k=<span class=\"hljs-number\">50</span>,\
          \ top_p=<span class=\"hljs-number\">0.95</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(outputs[<span class=\"hljs-number\">0</span>][<span class=\"\
          hljs-string\">\"generated_text\"</span>])\n</code></pre>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/_C9KrZzYnvcaFulOiGHKu.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/_C9KrZzYnvcaFulOiGHKu.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/7MjhOoBuhGttUhO7XH0sj.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/7MjhOoBuhGttUhO7XH0sj.png\"\
          ></a></p>\n"
        raw: "Google Colab\n```Python\n!pip install -qU transformers accelerate\n\n\
          from transformers import AutoTokenizer\nimport transformers\nimport torch\n\
          \nmodel = \"mlabonne/NeuralBeagle14-7B\"\nmessages = [{\"role\": \"user\"\
          , \"content\": \"What is a large language model?\"}]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs\
          \ = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7,\
          \ top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n```\n\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/_C9KrZzYnvcaFulOiGHKu.png)\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/7MjhOoBuhGttUhO7XH0sj.png)\n"
        updatedAt: '2024-01-21T07:01:27.652Z'
      numEdits: 1
      reactions: []
    id: 65acc0d25df673eec647b5d0
    type: comment
  author: NickyNicky
  content: "Google Colab\n```Python\n!pip install -qU transformers accelerate\n\n\
    from transformers import AutoTokenizer\nimport transformers\nimport torch\n\n\
    model = \"mlabonne/NeuralBeagle14-7B\"\nmessages = [{\"role\": \"user\", \"content\"\
    : \"What is a large language model?\"}]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipeline(prompt,\
    \ max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n\
    print(outputs[0][\"generated_text\"])\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/_C9KrZzYnvcaFulOiGHKu.png)\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/7MjhOoBuhGttUhO7XH0sj.png)\n"
  created_at: 2024-01-21 06:59:30+00:00
  edited: true
  hidden: false
  id: 65acc0d25df673eec647b5d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2024-01-21T07:02:25.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40306249260902405
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<pre><code class=\"language-Python\">!pip install -qU transformers\
          \ accelerate\n<span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer,\
          \ TrainingArguments, BitsAndBytesConfig\n<span class=\"hljs-keyword\">import</span>\
          \ transformers\n<span class=\"hljs-keyword\">import</span> torch,accelerate\n\
          \nmodel = <span class=\"hljs-string\">\"mlabonne/NeuralBeagle14-7B\"</span>\n\
          \nmodel_kwargs = {<span class=\"hljs-string\">\"device_map\"</span>: <span\
          \ class=\"hljs-string\">\"auto\"</span>, \n                <span class=\"\
          hljs-string\">\"load_in_4bit\"</span>: <span class=\"hljs-literal\">True</span>,\n\
          \                <span class=\"hljs-string\">\"torch_dtype\"</span>:torch.float16,\n\
          \                <span class=\"hljs-string\">\"device_map\"</span>:<span\
          \ class=\"hljs-string\">\"auto\"</span>,}\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model, **model_kwargs)\n\n\
          pipeline = transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    <span class=\"\
          hljs-comment\"># device=0,</span>\n)\n\nmessages = [{<span class=\"hljs-string\"\
          >\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"What is\
          \ a large language model?\"</span>}]\nprompt = tokenizer.apply_chat_template(messages,\
          \ tokenize=<span class=\"hljs-literal\">False</span>, add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>)\n\noutputs = pipeline(prompt, max_new_tokens=<span\
          \ class=\"hljs-number\">256</span>, do_sample=<span class=\"hljs-literal\"\
          >True</span>, temperature=<span class=\"hljs-number\">0.7</span>, top_k=<span\
          \ class=\"hljs-number\">50</span>, top_p=<span class=\"hljs-number\">0.95</span>)\n\
          <span class=\"hljs-built_in\">print</span>(outputs[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"generated_text\"</span>])\n</code></pre>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/jnOeEKuB9VfcPrvj4CIOO.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/jnOeEKuB9VfcPrvj4CIOO.png\"\
          ></a></p>\n"
        raw: "```Python\n!pip install -qU transformers accelerate\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n\
          import transformers\nimport torch,accelerate\n\nmodel = \"mlabonne/NeuralBeagle14-7B\"\
          \n\nmodel_kwargs = {\"device_map\": \"auto\", \n                \"load_in_4bit\"\
          : True,\n                \"torch_dtype\":torch.float16,\n              \
          \  \"device_map\":\"auto\",}\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model, **model_kwargs)\n\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    # device=0,\n)\n\nmessages = [{\"role\":\
          \ \"user\", \"content\": \"What is a large language model?\"}]\nprompt =\
          \ tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          \noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7,\
          \ top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n```\n\n\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/jnOeEKuB9VfcPrvj4CIOO.png)\n"
        updatedAt: '2024-01-21T07:02:25.209Z'
      numEdits: 0
      reactions: []
    id: 65acc18186f88a686b91c236
    type: comment
  author: NickyNicky
  content: "```Python\n!pip install -qU transformers accelerate\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n\
    import transformers\nimport torch,accelerate\n\nmodel = \"mlabonne/NeuralBeagle14-7B\"\
    \n\nmodel_kwargs = {\"device_map\": \"auto\", \n                \"load_in_4bit\"\
    : True,\n                \"torch_dtype\":torch.float16,\n                \"device_map\"\
    :\"auto\",}\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\nmodel = AutoModelForCausalLM.from_pretrained(model,\
    \ **model_kwargs)\n\npipeline = transformers.pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    # device=0,\n)\n\nmessages\
    \ = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n\
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
    \noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7,\
    \ top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/jnOeEKuB9VfcPrvj4CIOO.png)\n"
  created_at: 2024-01-21 07:02:25+00:00
  edited: false
  hidden: false
  id: 65acc18186f88a686b91c236
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ESQUWt-AmXMin1l_KxMTv.png?w=200&h=200&f=face
      fullname: Ola Magnusson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: reknine69
      type: user
    createdAt: '2024-01-21T09:41:10.000Z'
    data:
      edited: false
      editors:
      - reknine69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7238091230392456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ESQUWt-AmXMin1l_KxMTv.png?w=200&h=200&f=face
          fullname: Ola Magnusson
          isHf: false
          isPro: false
          name: reknine69
          type: user
        html: '<p>I''m still very new to this compared to a lot of you guys :)</p>

          <p>But I get something similar, but for a AMD ROCm setup.<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ySy3XS7W6X3J96j-4zOx5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ySy3XS7W6X3J96j-4zOx5.png"></a></p>

          <p>To get around it I''ll have resize token embeddings before running inference
          to get around out of bound warnings/errors:</p>

          <pre><code>model.resize_token_embeddings(len(tokenizer))

          </code></pre>

          <p>The models embed_tokens goes from 32000:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/Arh4Ueb-_uwsR9qXcCRm3.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/Arh4Ueb-_uwsR9qXcCRm3.png"></a></p>

          <p>to 32002</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/RHSYlsyynB60I26yDv6qO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/RHSYlsyynB60I26yDv6qO.png"></a></p>

          <p>After that I can run the pipeline without spamming device-side assertion.</p>

          '
        raw: 'I''m still very new to this compared to a lot of you guys :)


          But I get something similar, but for a AMD ROCm setup.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ySy3XS7W6X3J96j-4zOx5.png)


          To get around it I''ll have resize token embeddings before running inference
          to get around out of bound warnings/errors:

          ```

          model.resize_token_embeddings(len(tokenizer))

          ```

          The models embed_tokens goes from 32000:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/Arh4Ueb-_uwsR9qXcCRm3.png)


          to 32002


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/RHSYlsyynB60I26yDv6qO.png)


          After that I can run the pipeline without spamming device-side assertion.'
        updatedAt: '2024-01-21T09:41:10.488Z'
      numEdits: 0
      reactions: []
    id: 65ace6b6f8111f40c005f180
    type: comment
  author: reknine69
  content: 'I''m still very new to this compared to a lot of you guys :)


    But I get something similar, but for a AMD ROCm setup.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/ySy3XS7W6X3J96j-4zOx5.png)


    To get around it I''ll have resize token embeddings before running inference to
    get around out of bound warnings/errors:

    ```

    model.resize_token_embeddings(len(tokenizer))

    ```

    The models embed_tokens goes from 32000:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/Arh4Ueb-_uwsR9qXcCRm3.png)


    to 32002


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/650846769310ce8c405be0a7/RHSYlsyynB60I26yDv6qO.png)


    After that I can run the pipeline without spamming device-side assertion.'
  created_at: 2024-01-21 09:41:10+00:00
  edited: false
  hidden: false
  id: 65ace6b6f8111f40c005f180
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70f7c7681f873e823002d7c6ef3186d8.svg
      fullname: Fullname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Testator
      type: user
    createdAt: '2024-01-22T17:10:31.000Z'
    data:
      edited: true
      editors:
      - Testator
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959274411201477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70f7c7681f873e823002d7c6ef3186d8.svg
          fullname: Fullname
          isHf: false
          isPro: false
          name: Testator
          type: user
        html: '<p>I got the same error and could it only get working by using the  tokenizer_config.json
          from mistralai_Mistral-7B-Instruct-v0.2. I haven''t figured out yet which
          setting exactly causes this.<br>Anyway, after a quick test it seems Open
          Hermes 2.5 still wipes the floor with this model in terms of reasoning and
          it''s so censored it thinks stealing an egg from my chicken is unfair. I
          don''t expect fixing the config properly will change much.</p>

          '
        raw: "I got the same error and could it only get working by using the  tokenizer_config.json\
          \ from mistralai_Mistral-7B-Instruct-v0.2. I haven't figured out yet which\
          \ setting exactly causes this. \nAnyway, after a quick test it seems Open\
          \ Hermes 2.5 still wipes the floor with this model in terms of reasoning\
          \ and it's so censored it thinks stealing an egg from my chicken is unfair.\
          \ I don't expect fixing the config properly will change much."
        updatedAt: '2024-01-22T17:13:21.512Z'
      numEdits: 3
      reactions: []
    id: 65aea187fd71cbc318876fbc
    type: comment
  author: Testator
  content: "I got the same error and could it only get working by using the  tokenizer_config.json\
    \ from mistralai_Mistral-7B-Instruct-v0.2. I haven't figured out yet which setting\
    \ exactly causes this. \nAnyway, after a quick test it seems Open Hermes 2.5 still\
    \ wipes the floor with this model in terms of reasoning and it's so censored it\
    \ thinks stealing an egg from my chicken is unfair. I don't expect fixing the\
    \ config properly will change much."
  created_at: 2024-01-22 17:10:31+00:00
  edited: true
  hidden: false
  id: 65aea187fd71cbc318876fbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c2163a6040791552d54f53691463a13.svg
      fullname: kazoo bandito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kazoobandito
      type: user
    createdAt: '2024-01-24T04:38:20.000Z'
    data:
      edited: false
      editors:
      - kazoobandito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8735019564628601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c2163a6040791552d54f53691463a13.svg
          fullname: kazoo bandito
          isHf: false
          isPro: false
          name: kazoobandito
          type: user
        html: '<blockquote>

          <p>To get around it I''ll have resize token embeddings before running inference
          to get around out of bound warnings/errors:</p>

          <pre><code>model.resize_token_embeddings(len(tokenizer))

          </code></pre>

          </blockquote>

          <p>I have the same issue. Can you post exactly what files need to be changed?
          I am not familiar with the internals of OB. </p>

          '
        raw: '> To get around it I''ll have resize token embeddings before running
          inference to get around out of bound warnings/errors:

          > ```

          > model.resize_token_embeddings(len(tokenizer))

          > ```


          I have the same issue. Can you post exactly what files need to be changed?
          I am not familiar with the internals of OB. '
        updatedAt: '2024-01-24T04:38:20.172Z'
      numEdits: 0
      reactions: []
    id: 65b0943c165e69e0e6af8853
    type: comment
  author: kazoobandito
  content: '> To get around it I''ll have resize token embeddings before running inference
    to get around out of bound warnings/errors:

    > ```

    > model.resize_token_embeddings(len(tokenizer))

    > ```


    I have the same issue. Can you post exactly what files need to be changed? I am
    not familiar with the internals of OB. '
  created_at: 2024-01-24 04:38:20+00:00
  edited: false
  hidden: false
  id: 65b0943c165e69e0e6af8853
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a9b8872029ae329e7b92cef11a78049.svg
      fullname: SeaZeeHech
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SeaZeeHech
      type: user
    createdAt: '2024-01-24T13:46:35.000Z'
    data:
      edited: false
      editors:
      - SeaZeeHech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9622069001197815
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a9b8872029ae329e7b92cef11a78049.svg
          fullname: SeaZeeHech
          isHf: false
          isPro: false
          name: SeaZeeHech
          type: user
        html: '<p>Is it possible to just revert the config to the state in which the
          model was trained? Having to resize the embedding matrix (ie adding embeddings)
          seems very suboptimal? The added &lt;|im_start|&gt; embedding would basically
          be noise, requiring the model to learn it during fine-tuning. In my experiment,
          the fine-tuning goes incredibly poorly - the same as <a href="https://huggingface.co/leveldevai/MarcBeagle-7B/discussions/1">here</a>.
          </p>

          <p>Has anybody been able to successfully fine-tune this model? Performance
          seems strong, but my use case needs it to be fine-tuned, which goes very
          poorly here. </p>

          '
        raw: "Is it possible to just revert the config to the state in which the model\
          \ was trained? Having to resize the embedding matrix (ie adding embeddings)\
          \ seems very suboptimal? The added <|im_start|> embedding would basically\
          \ be noise, requiring the model to learn it during fine-tuning. In my experiment,\
          \ the fine-tuning goes incredibly poorly - the same as [here](https://huggingface.co/leveldevai/MarcBeagle-7B/discussions/1).\
          \ \n\nHas anybody been able to successfully fine-tune this model? Performance\
          \ seems strong, but my use case needs it to be fine-tuned, which goes very\
          \ poorly here. "
        updatedAt: '2024-01-24T13:46:35.140Z'
      numEdits: 0
      reactions: []
    id: 65b114bb1dbd85fd0c58e6e7
    type: comment
  author: SeaZeeHech
  content: "Is it possible to just revert the config to the state in which the model\
    \ was trained? Having to resize the embedding matrix (ie adding embeddings) seems\
    \ very suboptimal? The added <|im_start|> embedding would basically be noise,\
    \ requiring the model to learn it during fine-tuning. In my experiment, the fine-tuning\
    \ goes incredibly poorly - the same as [here](https://huggingface.co/leveldevai/MarcBeagle-7B/discussions/1).\
    \ \n\nHas anybody been able to successfully fine-tune this model? Performance\
    \ seems strong, but my use case needs it to be fine-tuned, which goes very poorly\
    \ here. "
  created_at: 2024-01-24 13:46:35+00:00
  edited: false
  hidden: false
  id: 65b114bb1dbd85fd0c58e6e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: mlabonne/NeuralBeagle14-7B
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: CUDA error: device-side assert triggered'
