!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chattyfish
conflicting_files: null
created_at: 2023-05-28 11:55:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62a328a6300258188379fa40943f887c.svg
      fullname: Chatty Fish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chattyfish
      type: user
    createdAt: '2023-05-28T12:55:31.000Z'
    data:
      edited: false
      editors:
      - chattyfish
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62a328a6300258188379fa40943f887c.svg
          fullname: Chatty Fish
          isHf: false
          isPro: false
          name: chattyfish
          type: user
        html: '<p>can I run it on a RTX4090?</p>

          '
        raw: can I run it on a RTX4090?
        updatedAt: '2023-05-28T12:55:31.124Z'
      numEdits: 0
      reactions: []
    id: 64734f4371f07ae738d121db
    type: comment
  author: chattyfish
  content: can I run it on a RTX4090?
  created_at: 2023-05-28 11:55:31+00:00
  edited: false
  hidden: false
  id: 64734f4371f07ae738d121db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/06477f0ffdaa2a58201f78874673a858.svg
      fullname: m33393
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: m33393
      type: user
    createdAt: '2023-05-28T16:19:17.000Z'
    data:
      edited: false
      editors:
      - m33393
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/06477f0ffdaa2a58201f78874673a858.svg
          fullname: m33393
          isHf: false
          isPro: false
          name: m33393
          type: user
        html: '<p>It takes ~42-43GB VRAM to run so no. Try two of them perhaps. But
          then there might be overhead when doing splits.</p>

          '
        raw: It takes ~42-43GB VRAM to run so no. Try two of them perhaps. But then
          there might be overhead when doing splits.
        updatedAt: '2023-05-28T16:19:17.800Z'
      numEdits: 0
      reactions: []
    id: 64737f052a74fb43ccdf9096
    type: comment
  author: m33393
  content: It takes ~42-43GB VRAM to run so no. Try two of them perhaps. But then
    there might be overhead when doing splits.
  created_at: 2023-05-28 15:19:17+00:00
  edited: false
  hidden: false
  id: 64737f052a74fb43ccdf9096
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62a328a6300258188379fa40943f887c.svg
      fullname: Chatty Fish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chattyfish
      type: user
    createdAt: '2023-05-28T16:29:34.000Z'
    data:
      edited: true
      editors:
      - chattyfish
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62a328a6300258188379fa40943f887c.svg
          fullname: Chatty Fish
          isHf: false
          isPro: false
          name: chattyfish
          type: user
        html: '<p>Thanks for the quick reply. My friend has a game computer with same
          24g 4090. I may borrow the entire PC from him, means I will use two PCs  not
          1 PC with two cards, then any way to run this model?</p>

          '
        raw: Thanks for the quick reply. My friend has a game computer with same 24g
          4090. I may borrow the entire PC from him, means I will use two PCs  not
          1 PC with two cards, then any way to run this model?
        updatedAt: '2023-05-28T16:33:26.917Z'
      numEdits: 1
      reactions: []
    id: 6473816e63001a0002ccd472
    type: comment
  author: chattyfish
  content: Thanks for the quick reply. My friend has a game computer with same 24g
    4090. I may borrow the entire PC from him, means I will use two PCs  not 1 PC
    with two cards, then any way to run this model?
  created_at: 2023-05-28 15:29:34+00:00
  edited: true
  hidden: false
  id: 6473816e63001a0002ccd472
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64438bf2af034cdfd699e46c/S3wo7qIdKWIh_CfYk3add.png?w=200&h=200&f=face
      fullname: PKFKNK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PKFKNK
      type: user
    createdAt: '2023-06-21T01:40:49.000Z'
    data:
      edited: false
      editors:
      - PKFKNK
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9109897613525391
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64438bf2af034cdfd699e46c/S3wo7qIdKWIh_CfYk3add.png?w=200&h=200&f=face
          fullname: PKFKNK
          isHf: false
          isPro: false
          name: PKFKNK
          type: user
        html: '<p>Just put the card in your computer... use a riser if you have to</p>

          '
        raw: Just put the card in your computer... use a riser if you have to
        updatedAt: '2023-06-21T01:40:49.337Z'
      numEdits: 0
      reactions: []
    id: 64925521dab6b57e1f5cb284
    type: comment
  author: PKFKNK
  content: Just put the card in your computer... use a riser if you have to
  created_at: 2023-06-21 00:40:49+00:00
  edited: false
  hidden: false
  id: 64925521dab6b57e1f5cb284
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: m33393/llama-65b-gptq-cuda-4bit-32g-safetensors
repo_type: model
status: open
target_branch: null
title: hardware requirement
