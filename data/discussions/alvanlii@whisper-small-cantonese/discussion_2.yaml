!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matthew36
conflicting_files: null
created_at: 2023-03-23 08:11:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55d34b9a5c6a7b73e93200519841abec.svg
      fullname: Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthew36
      type: user
    createdAt: '2023-03-23T09:11:37.000Z'
    data:
      edited: false
      editors:
      - matthew36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55d34b9a5c6a7b73e93200519841abec.svg
          fullname: Choi
          isHf: false
          isPro: false
          name: matthew36
          type: user
        html: '<p>How can I get the confidence score of the result?</p>

          '
        raw: How can I get the confidence score of the result?
        updatedAt: '2023-03-23T09:11:37.936Z'
      numEdits: 0
      reactions: []
    id: 641c17c9eec4fbbd3384acb5
    type: comment
  author: matthew36
  content: How can I get the confidence score of the result?
  created_at: 2023-03-23 08:11:37+00:00
  edited: false
  hidden: false
  id: 641c17c9eec4fbbd3384acb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676128434925-632bc1d36cb20ba0ae87bde3.jpeg?w=200&h=200&f=face
      fullname: Alvin Li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: alvanlii
      type: user
    createdAt: '2023-03-23T14:34:29.000Z'
    data:
      edited: false
      editors:
      - alvanlii
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676128434925-632bc1d36cb20ba0ae87bde3.jpeg?w=200&h=200&f=face
          fullname: Alvin Li
          isHf: false
          isPro: false
          name: alvanlii
          type: user
        html: "<p>Hey matthew, you can get the probabilities of each token like this:</p>\n\
          <p>Loading model</p>\n<pre><code>import librosa\n\nimport torch\nfrom transformers\
          \ import WhisperForConditionalGeneration, WhisperProcessor\n\ny, sr = librosa.load('audio.mp3',\
          \ sr=16000)\n\nMODEL_NAME = \"alvanlii/whisper-small-cantonese\"\n\nprocessor\
          \ = WhisperProcessor.from_pretrained(MODEL_NAME)\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).cuda()\n\
          \nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens =\
          \ []\nmodel.config.use_cache = False\n</code></pre>\n<p>Generate output,\
          \ note the <code>output_scores</code> flag</p>\n<pre><code>processed_in\
          \ = processor(y, sampling_rate=sr, return_tensors=\"pt\")\ngout = model.generate(\n\
          \    input_features=processed_in.input_features.cuda(), \n    output_scores=True,\
          \ return_dict_in_generate=True\n)\n</code></pre>\n<p>Compute softmax from\
          \ the scores</p>\n<pre><code>proba_scores = [torch.nn.functional.softmax(gout.scores[idx]).max()\
          \ for idx in range(len(gout.scores))]\n# the ids are now in .sequences\n\
          transcription = processor.batch_decode(gout.sequences, skip_special_tokens=True)[0]\n\
          print(transcription)\n</code></pre>\n"
        raw: "Hey matthew, you can get the probabilities of each token like this:\n\
          \nLoading model\n```\nimport librosa\n\nimport torch\nfrom transformers\
          \ import WhisperForConditionalGeneration, WhisperProcessor\n\ny, sr = librosa.load('audio.mp3',\
          \ sr=16000)\n\nMODEL_NAME = \"alvanlii/whisper-small-cantonese\"\n\nprocessor\
          \ = WhisperProcessor.from_pretrained(MODEL_NAME)\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).cuda()\n\
          \nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens =\
          \ []\nmodel.config.use_cache = False\n```\n\nGenerate output, note the `output_scores`\
          \ flag\n```\nprocessed_in = processor(y, sampling_rate=sr, return_tensors=\"\
          pt\")\ngout = model.generate(\n    input_features=processed_in.input_features.cuda(),\
          \ \n    output_scores=True, return_dict_in_generate=True\n)\n```\n\nCompute\
          \ softmax from the scores\n```\nproba_scores = [torch.nn.functional.softmax(gout.scores[idx]).max()\
          \ for idx in range(len(gout.scores))]\n# the ids are now in .sequences\n\
          transcription = processor.batch_decode(gout.sequences, skip_special_tokens=True)[0]\n\
          print(transcription)\n```"
        updatedAt: '2023-03-23T14:34:29.336Z'
      numEdits: 0
      reactions: []
    id: 641c6375a073e0d2931e6057
    type: comment
  author: alvanlii
  content: "Hey matthew, you can get the probabilities of each token like this:\n\n\
    Loading model\n```\nimport librosa\n\nimport torch\nfrom transformers import WhisperForConditionalGeneration,\
    \ WhisperProcessor\n\ny, sr = librosa.load('audio.mp3', sr=16000)\n\nMODEL_NAME\
    \ = \"alvanlii/whisper-small-cantonese\"\n\nprocessor = WhisperProcessor.from_pretrained(MODEL_NAME)\n\
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).cuda()\n\n\
    model.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\nmodel.config.use_cache\
    \ = False\n```\n\nGenerate output, note the `output_scores` flag\n```\nprocessed_in\
    \ = processor(y, sampling_rate=sr, return_tensors=\"pt\")\ngout = model.generate(\n\
    \    input_features=processed_in.input_features.cuda(), \n    output_scores=True,\
    \ return_dict_in_generate=True\n)\n```\n\nCompute softmax from the scores\n```\n\
    proba_scores = [torch.nn.functional.softmax(gout.scores[idx]).max() for idx in\
    \ range(len(gout.scores))]\n# the ids are now in .sequences\ntranscription = processor.batch_decode(gout.sequences,\
    \ skip_special_tokens=True)[0]\nprint(transcription)\n```"
  created_at: 2023-03-23 13:34:29+00:00
  edited: false
  hidden: false
  id: 641c6375a073e0d2931e6057
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55d34b9a5c6a7b73e93200519841abec.svg
      fullname: Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthew36
      type: user
    createdAt: '2023-03-23T15:50:04.000Z'
    data:
      edited: false
      editors:
      - matthew36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55d34b9a5c6a7b73e93200519841abec.svg
          fullname: Choi
          isHf: false
          isPro: false
          name: matthew36
          type: user
        html: "<p>Thank you so much alvanlii! I will try it later. \u611F\u8B1D\u5927\
          \u4F6C</p>\n"
        raw: "Thank you so much alvanlii! I will try it later. \u611F\u8B1D\u5927\u4F6C"
        updatedAt: '2023-03-23T15:50:04.850Z'
      numEdits: 0
      reactions: []
    id: 641c752caa941743c6c67f8a
    type: comment
  author: matthew36
  content: "Thank you so much alvanlii! I will try it later. \u611F\u8B1D\u5927\u4F6C"
  created_at: 2023-03-23 14:50:04+00:00
  edited: false
  hidden: false
  id: 641c752caa941743c6c67f8a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: alvanlii/whisper-small-cantonese
repo_type: model
status: open
target_branch: null
title: How to get the confidence of the transcribed result?
