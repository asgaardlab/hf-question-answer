!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wmr
conflicting_files: null
created_at: 2023-05-09 18:12:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7088106ac908d8a624aa882d4f7e40c3.svg
      fullname: wmr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wmr
      type: user
    createdAt: '2023-05-09T19:12:12.000Z'
    data:
      edited: false
      editors:
      - wmr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7088106ac908d8a624aa882d4f7e40c3.svg
          fullname: wmr
          isHf: false
          isPro: false
          name: wmr
          type: user
        html: "<p>I tried adding an alpaca style dataset with text-generation-webui\
          \ and it gave the following error in the training:</p>\n<pre><code>(...)\
          \ training.py\", line 247, in tokenize\n    result = shared.tokenizer(prompt,\
          \ truncation=True, max_length=cutoff_len + 1, padding=\"max_length\")\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          TypeError: 'LlamaCppModel' object is not callable\n</code></pre>\n<p>I'm\
          \ using q5_1.</p>\n"
        raw: "I tried adding an alpaca style dataset with text-generation-webui and\
          \ it gave the following error in the training:\r\n```\r\n(...) training.py\"\
          , line 247, in tokenize\r\n    result = shared.tokenizer(prompt, truncation=True,\
          \ max_length=cutoff_len + 1, padding=\"max_length\")\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nTypeError: 'LlamaCppModel' object is not callable\r\n``` \r\n\r\nI'm using\
          \ q5_1."
        updatedAt: '2023-05-09T19:12:12.556Z'
      numEdits: 0
      reactions: []
    id: 645a9b0c77d040cfd650197f
    type: comment
  author: wmr
  content: "I tried adding an alpaca style dataset with text-generation-webui and\
    \ it gave the following error in the training:\r\n```\r\n(...) training.py\",\
    \ line 247, in tokenize\r\n    result = shared.tokenizer(prompt, truncation=True,\
    \ max_length=cutoff_len + 1, padding=\"max_length\")\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \nTypeError: 'LlamaCppModel' object is not callable\r\n``` \r\n\r\nI'm using q5_1."
  created_at: 2023-05-09 18:12:12+00:00
  edited: false
  hidden: false
  id: 645a9b0c77d040cfd650197f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T10:05:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can''t train GGML models I''m afraid.</p>

          <p>You''ll need a model for GPU inference, either an unquantised HF model,
          or (I think) a GPTQ 4bit model can work.  I''ve never tried training in
          text-gen-ui myself so not sure of the specifics.  But it definitely can''t
          work on a GGML model with llama.cpp (not yet anyway - maybe llama.cpp will
          add that in the future!)</p>

          '
        raw: 'You can''t train GGML models I''m afraid.


          You''ll need a model for GPU inference, either an unquantised HF model,
          or (I think) a GPTQ 4bit model can work.  I''ve never tried training in
          text-gen-ui myself so not sure of the specifics.  But it definitely can''t
          work on a GGML model with llama.cpp (not yet anyway - maybe llama.cpp will
          add that in the future!)'
        updatedAt: '2023-05-10T10:05:37.768Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - wmr
        - mrmyh
        - hellopedro
        - Cris-AV
    id: 645b6c71b396a40a848dca21
    type: comment
  author: TheBloke
  content: 'You can''t train GGML models I''m afraid.


    You''ll need a model for GPU inference, either an unquantised HF model, or (I
    think) a GPTQ 4bit model can work.  I''ve never tried training in text-gen-ui
    myself so not sure of the specifics.  But it definitely can''t work on a GGML
    model with llama.cpp (not yet anyway - maybe llama.cpp will add that in the future!)'
  created_at: 2023-05-10 09:05:37+00:00
  edited: false
  hidden: false
  id: 645b6c71b396a40a848dca21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7088106ac908d8a624aa882d4f7e40c3.svg
      fullname: wmr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wmr
      type: user
    createdAt: '2023-05-10T11:38:55.000Z'
    data:
      edited: false
      editors:
      - wmr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7088106ac908d8a624aa882d4f7e40c3.svg
          fullname: wmr
          isHf: false
          isPro: false
          name: wmr
          type: user
        html: '<p>How to add new knowledge to this model then, without a full retraining?
          Thanks.</p>

          '
        raw: How to add new knowledge to this model then, without a full retraining?
          Thanks.
        updatedAt: '2023-05-10T11:38:55.530Z'
      numEdits: 0
      reactions: []
    id: 645b824f8bbb8592d915f8e3
    type: comment
  author: wmr
  content: How to add new knowledge to this model then, without a full retraining?
    Thanks.
  created_at: 2023-05-10 10:38:55+00:00
  edited: false
  hidden: false
  id: 645b824f8bbb8592d915f8e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T11:47:43.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You would do fine tuning on the HF model, available here: <a href="https://huggingface.co/TheBloke/wizardLM-7B-HF">https://huggingface.co/TheBloke/wizardLM-7B-HF</a></p>

          <p>You''ll need a GPU with enough VRAM, though. Which means you''ll need
          at least 16GB VRAM.  If you have less, you could investigate doing fine
          tuning in 4bit, eg check out these repos:<br><a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a><br><a
          rel="nofollow" href="https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning">https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning</a></p>

          '
        raw: 'You would do fine tuning on the HF model, available here: https://huggingface.co/TheBloke/wizardLM-7B-HF


          You''ll need a GPU with enough VRAM, though. Which means you''ll need at
          least 16GB VRAM.  If you have less, you could investigate doing fine tuning
          in 4bit, eg check out these repos:

          https://github.com/johnsmith0031/alpaca_lora_4bit

          https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning'
        updatedAt: '2023-05-10T11:48:09.666Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - wmr
    id: 645b845f0120c98d16a67029
    type: comment
  author: TheBloke
  content: 'You would do fine tuning on the HF model, available here: https://huggingface.co/TheBloke/wizardLM-7B-HF


    You''ll need a GPU with enough VRAM, though. Which means you''ll need at least
    16GB VRAM.  If you have less, you could investigate doing fine tuning in 4bit,
    eg check out these repos:

    https://github.com/johnsmith0031/alpaca_lora_4bit

    https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning'
  created_at: 2023-05-10 10:47:43+00:00
  edited: true
  hidden: false
  id: 645b845f0120c98d16a67029
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/wizardLM-7B-GGML
repo_type: model
status: open
target_branch: null
title: How to train this model with text-generation-webui?
