!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-04-28 17:43:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-28T18:43:23.000Z'
    data:
      edited: true
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<ul>

          <li><p>Obtain Q5_1 from: <a href="https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main">https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main</a></p>

          </li>

          <li><p>Obtain and install latest version of <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p>

          </li>

          </ul>

          <pre><code>git clone https://github.com/oobabooga/text-generation-webui

          cd text-generation-webui

          pip install -r requirements.txt

          </code></pre>

          <ul>

          <li><p>OPTIONAL (no longer needed as all models have been renamed to lowercase
          ''ggml'') - Rename WizardLM-7B.GGML.q5_1.bin to WizardLM-7B.ggml.q5_1.bin
          as per <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/ee68ec9079492a72a35c33d5000da432ce94af71/modules/models.py#LL46C1-L46C1">https://github.com/oobabooga/text-generation-webui/blob/ee68ec9079492a72a35c33d5000da432ce94af71/modules/models.py#LL46C1-L46C1</a>
          glob() is case sensitive :(</p>

          </li>

          <li><p>Place the model into models/TheBloke_wizardLM-7B-GGML of text-generation-webui</p>

          </li>

          <li><p>OPTION 1 (NO LONGER REQUIRED IF THE LATEST VERSION OF text-generation-webui
          WAS INSTALLED) - We need to upgrade llama-cpp-python because support was
          only added recently</p>

          </li>

          </ul>

          <pre><code>pip freeze | grep llama

          pip uninstall -y llama-cpp-python

          pip cache purge &amp;&amp; pip install llama-cpp-python==0.1.41 # or more
          recent, q5 support added to pypi in 0.1.39 - https://github.com/abetlen/llama-cpp-python/issues/124

          </code></pre>

          <pre><code>pip freeze | grep llama # output:

          llama-cpp-python==0.1.41

          </code></pre>

          <ul>

          <li>OPTION 2 (NO LONGER REQUIRED IF THE LATEST VERSION OF text-generation-webui
          WAS INSTALLED) - Alternatively, obtain and install the develop version</li>

          </ul>

          <pre><code>cd ~/

          rm -rf llama-cpp-python

          git clone https://github.com/abetlen/llama-cpp-python

          cd llama-cpp-python

          sed -i ''s/git@github.com:/https:\/\/github.com\//g'' .gitmodules

          git submodule update --init --recursive

          pip uninstall -y llama-cpp-python

          pip install scikit-build

          python3 setup.py develop

          </code></pre>

          <pre><code>pip freeze | grep llama # output:

          -e git+https://github.com/abetlen/llama-cpp-python@9339929f56ca71adb97930679c710a2458f877bd#egg=llama_cpp_python

          </code></pre>

          <ul>

          <li>Launch oobabooga''s text-generation-webui with llama.cpp</li>

          </ul>

          <pre><code>python server.py --model TheBloke_wizardLM-7B-GGML --threads
          4

          </code></pre>

          <p><a rel="nofollow" href="https://thireus.com/AI/WizardLM-7B.ggml.q5_1.png"><img
          alt="demo" src="https://thireus.com/AI/WizardLM-7B.ggml.q5_1.png"></a><br><code>Output
          generated in 11.22 seconds (4.10 tokens/s, 46 tokens, context 69, seed 1066937501)</code></p>

          '
        raw: "- Obtain Q5_1 from: https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main\n\
          \n- Obtain and install latest version of https://github.com/oobabooga/text-generation-webui\n\
          ```\ngit clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
          pip install -r requirements.txt\n```\n\n- OPTIONAL (no longer needed as\
          \ all models have been renamed to lowercase 'ggml') - Rename WizardLM-7B.GGML.q5_1.bin\
          \ to WizardLM-7B.ggml.q5_1.bin as per https://github.com/oobabooga/text-generation-webui/blob/ee68ec9079492a72a35c33d5000da432ce94af71/modules/models.py#LL46C1-L46C1\
          \ glob() is case sensitive :(\n\n- Place the model into models/TheBloke_wizardLM-7B-GGML\
          \ of text-generation-webui\n\n- OPTION 1 (NO LONGER REQUIRED IF THE LATEST\
          \ VERSION OF text-generation-webui WAS INSTALLED) - We need to upgrade llama-cpp-python\
          \ because support was only added recently\n```\npip freeze | grep llama\n\
          pip uninstall -y llama-cpp-python\npip cache purge && pip install llama-cpp-python==0.1.41\
          \ # or more recent, q5 support added to pypi in 0.1.39 - https://github.com/abetlen/llama-cpp-python/issues/124\n\
          ```\n\n```\npip freeze | grep llama # output:\nllama-cpp-python==0.1.41\n\
          ```\n\n- OPTION 2 (NO LONGER REQUIRED IF THE LATEST VERSION OF text-generation-webui\
          \ WAS INSTALLED) - Alternatively, obtain and install the develop version\n\
          ```\ncd ~/\nrm -rf llama-cpp-python\ngit clone https://github.com/abetlen/llama-cpp-python\n\
          cd llama-cpp-python\nsed -i 's/git@github.com:/https:\\/\\/github.com\\\
          //g' .gitmodules\ngit submodule update --init --recursive\npip uninstall\
          \ -y llama-cpp-python\npip install scikit-build\npython3 setup.py develop\n\
          ```\n\n```\npip freeze | grep llama # output:\n-e git+https://github.com/abetlen/llama-cpp-python@9339929f56ca71adb97930679c710a2458f877bd#egg=llama_cpp_python\n\
          ```\n\n- Launch oobabooga's text-generation-webui with llama.cpp \n```\n\
          python server.py --model TheBloke_wizardLM-7B-GGML --threads 4\n```\n\n\
          ![demo](https://thireus.com/AI/WizardLM-7B.ggml.q5_1.png)\n`Output generated\
          \ in 11.22 seconds (4.10 tokens/s, 46 tokens, context 69, seed 1066937501)`"
        updatedAt: '2023-05-02T21:59:15.018Z'
      numEdits: 9
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - efocht
        - wmr
        - Hanssep123
    id: 644c13cb22d211df64459ca2
    type: comment
  author: Thireus
  content: "- Obtain Q5_1 from: https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main\n\
    \n- Obtain and install latest version of https://github.com/oobabooga/text-generation-webui\n\
    ```\ngit clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
    pip install -r requirements.txt\n```\n\n- OPTIONAL (no longer needed as all models\
    \ have been renamed to lowercase 'ggml') - Rename WizardLM-7B.GGML.q5_1.bin to\
    \ WizardLM-7B.ggml.q5_1.bin as per https://github.com/oobabooga/text-generation-webui/blob/ee68ec9079492a72a35c33d5000da432ce94af71/modules/models.py#LL46C1-L46C1\
    \ glob() is case sensitive :(\n\n- Place the model into models/TheBloke_wizardLM-7B-GGML\
    \ of text-generation-webui\n\n- OPTION 1 (NO LONGER REQUIRED IF THE LATEST VERSION\
    \ OF text-generation-webui WAS INSTALLED) - We need to upgrade llama-cpp-python\
    \ because support was only added recently\n```\npip freeze | grep llama\npip uninstall\
    \ -y llama-cpp-python\npip cache purge && pip install llama-cpp-python==0.1.41\
    \ # or more recent, q5 support added to pypi in 0.1.39 - https://github.com/abetlen/llama-cpp-python/issues/124\n\
    ```\n\n```\npip freeze | grep llama # output:\nllama-cpp-python==0.1.41\n```\n\
    \n- OPTION 2 (NO LONGER REQUIRED IF THE LATEST VERSION OF text-generation-webui\
    \ WAS INSTALLED) - Alternatively, obtain and install the develop version\n```\n\
    cd ~/\nrm -rf llama-cpp-python\ngit clone https://github.com/abetlen/llama-cpp-python\n\
    cd llama-cpp-python\nsed -i 's/git@github.com:/https:\\/\\/github.com\\//g' .gitmodules\n\
    git submodule update --init --recursive\npip uninstall -y llama-cpp-python\npip\
    \ install scikit-build\npython3 setup.py develop\n```\n\n```\npip freeze | grep\
    \ llama # output:\n-e git+https://github.com/abetlen/llama-cpp-python@9339929f56ca71adb97930679c710a2458f877bd#egg=llama_cpp_python\n\
    ```\n\n- Launch oobabooga's text-generation-webui with llama.cpp \n```\npython\
    \ server.py --model TheBloke_wizardLM-7B-GGML --threads 4\n```\n\n![demo](https://thireus.com/AI/WizardLM-7B.ggml.q5_1.png)\n\
    `Output generated in 11.22 seconds (4.10 tokens/s, 46 tokens, context 69, seed\
    \ 1066937501)`"
  created_at: 2023-04-28 17:43:23+00:00
  edited: true
  hidden: false
  id: 644c13cb22d211df64459ca2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T19:08:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Awesome guide, thanks!  You can edit out point 3 as I''ve renamed
          all the files to <code>ggml.bin</code>. It''s dumb that textgen is case
          sensitive but for now it''s easier if I just change it here.</p>

          <p>I will link to your guide on the README. Thanks for posting it!</p>

          '
        raw: 'Awesome guide, thanks!  You can edit out point 3 as I''ve renamed all
          the files to `ggml.bin`. It''s dumb that textgen is case sensitive but for
          now it''s easier if I just change it here.


          I will link to your guide on the README. Thanks for posting it!'
        updatedAt: '2023-04-28T19:08:34.789Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Thireus
        - jgnr
    id: 644c19b20ce4f8fb516d75bf
    type: comment
  author: TheBloke
  content: 'Awesome guide, thanks!  You can edit out point 3 as I''ve renamed all
    the files to `ggml.bin`. It''s dumb that textgen is case sensitive but for now
    it''s easier if I just change it here.


    I will link to your guide on the README. Thanks for posting it!'
  created_at: 2023-04-28 18:08:34+00:00
  edited: false
  hidden: false
  id: 644c19b20ce4f8fb516d75bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T19:11:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Link added to README</p>

          '
        raw: Link added to README
        updatedAt: '2023-04-28T19:11:02.515Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Thireus
    id: 644c1a46ed08a4fdf4dff2cf
    type: comment
  author: TheBloke
  content: Link added to README
  created_at: 2023-04-28 18:11:02+00:00
  edited: false
  hidden: false
  id: 644c1a46ed08a4fdf4dff2cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-28T21:25:59.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>Thanks for your converted model!</p>

          <p>abetlen just released 0.1.39 to pypi. I''ve edited the guide.</p>

          '
        raw: 'Thanks for your converted model!


          abetlen just released 0.1.39 to pypi. I''ve edited the guide.'
        updatedAt: '2023-04-28T21:25:59.159Z'
      numEdits: 0
      reactions: []
    id: 644c39e7194e124dacc02a0f
    type: comment
  author: Thireus
  content: 'Thanks for your converted model!


    abetlen just released 0.1.39 to pypi. I''ve edited the guide.'
  created_at: 2023-04-28 20:25:59+00:00
  edited: false
  hidden: false
  id: 644c39e7194e124dacc02a0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2565af178fab90535aa3013b80eb190.svg
      fullname: aisis aisis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisis7
      type: user
    createdAt: '2023-06-20T04:19:58.000Z'
    data:
      edited: false
      editors:
      - aisis7
      hidden: false
      identifiedLanguage:
        language: ro
        probability: 0.5423654913902283
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2565af178fab90535aa3013b80eb190.svg
          fullname: aisis aisis
          isHf: false
          isPro: false
          name: aisis7
          type: user
        html: '<p>Will it utilize my GPU? I have 6gb vram 1060 6gb?</p>

          '
        raw: Will it utilize my GPU? I have 6gb vram 1060 6gb?
        updatedAt: '2023-06-20T04:19:58.251Z'
      numEdits: 0
      reactions: []
    id: 649128ee189ba64b842c80ae
    type: comment
  author: aisis7
  content: Will it utilize my GPU? I have 6gb vram 1060 6gb?
  created_at: 2023-06-20 03:19:58+00:00
  edited: false
  hidden: false
  id: 649128ee189ba64b842c80ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:00:42.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5716282725334167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Will it utilize my GPU? I have 6gb vram 1060 6gb?</p>

          </blockquote>

          <p>Yes, if you compile llama.cpp or llama-cpp-python with cuBLAS support.
          </p>

          <p>If you want to use a UI like text-generation-webui, you should use llama-cpp-python.  Details
          here for compiling that with GPU support: <a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal">https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal</a></p>

          '
        raw: "> Will it utilize my GPU? I have 6gb vram 1060 6gb?\n\nYes, if you compile\
          \ llama.cpp or llama-cpp-python with cuBLAS support. \n\nIf you want to\
          \ use a UI like text-generation-webui, you should use llama-cpp-python.\
          \  Details here for compiling that with GPU support: https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal"
        updatedAt: '2023-06-20T10:01:24.778Z'
      numEdits: 1
      reactions: []
    id: 649178ca3b5cf91ae5d89788
    type: comment
  author: TheBloke
  content: "> Will it utilize my GPU? I have 6gb vram 1060 6gb?\n\nYes, if you compile\
    \ llama.cpp or llama-cpp-python with cuBLAS support. \n\nIf you want to use a\
    \ UI like text-generation-webui, you should use llama-cpp-python.  Details here\
    \ for compiling that with GPU support: https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal"
  created_at: 2023-06-20 09:00:42+00:00
  edited: true
  hidden: false
  id: 649178ca3b5cf91ae5d89788
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/wizardLM-7B-GGML
repo_type: model
status: open
target_branch: null
title: '[GUIDE] Launch Q5_1 model with oobabooga''s text-generation-webui'
