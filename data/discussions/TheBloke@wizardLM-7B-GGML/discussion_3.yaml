!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Reggie
conflicting_files: null
created_at: 2023-04-27 13:54:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-27T14:54:41.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Hi,<br>Great work! Was just wondering if this GGML is expected to
          be better than say, the converted GGML of your GPTQ version?</p>

          '
        raw: "Hi,\r\nGreat work! Was just wondering if this GGML is expected to be\
          \ better than say, the converted GGML of your GPTQ version?"
        updatedAt: '2023-04-27T14:54:41.199Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - udaykumar97
        - nss-ysasaki
    id: 644a8cb1cb1654dcb6e769bf
    type: comment
  author: Reggie
  content: "Hi,\r\nGreat work! Was just wondering if this GGML is expected to be better\
    \ than say, the converted GGML of your GPTQ version?"
  created_at: 2023-04-27 13:54:41+00:00
  edited: false
  hidden: false
  id: 644a8cb1cb1654dcb6e769bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-27T20:23:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>So FYI my GGMLs are not converted from the GPTQs. They''re converted
          directly from the unquantised original repo.  I used to try making GGMLs
          from 4bit GPTQs, but the files scored very badly on artificial benchmarks
          and I could never understand why. And then the llama.cpp quantisation methods
          improved significantly, so there was no point even trying that.</p>

          <p>In terms of inference quality, I believe the quantised GGMLs have now
          overtaken GPTQ in benchmarks.  There''s an artificial LLM benchmark called
          perplexity.  GPTQ scores well and used to be better than q4_0 GGML, but
          recently the llama.cpp team have done a ton of work on 4bit quantisation
          and their new methods q4_2 and q4_3 now beat 4bit GPTQ in this benchmark.</p>

          <p>Then the new 5bit methods q5_0 and q5_1 are even better than that.</p>

          <p>So if you want the absolute maximum inference quality - but don''t have
          the resources to load the model in 16bit or 8bit - you would go for 4bit
          or 5bit GGML.</p>

          <p>However in practical usage most people aren''t going to be able to tell
          the difference between a very good quantisation and a slightly better quantisation.
          So the bigger question is whether you want CPU or GPU inference.</p>

          <p>If you have an NVidia GPU and can do GPU inference then generally it
          should be much faster than CPU inference, and most people will care about
          that a lot more than they will a 3% improvement on an artificial benchmark.</p>

          <p>So as a general rule I would say: if you have an NVidia GPU and can do
          GPU inference, you should.</p>

          <p>All that said: there are some GPTQ performance problems with WizardLM
          specifically, which I''ve not been able to resolve yet. They''re not a big
          issue if you use Linux or WSL2 and can use the Triton branch of GPTQ-for-LLaMa.
          But if you need to use the CUDA branch - eg because you''re on Windows -
          you may find you get only 1-4tokens/s in GPU inference. And llama.cpp GGML
          can beat that on many CPUs.</p>

          <p>So it''s a bit complicated for this model! But for any other 7B model,
          GPU inference will win on speed.</p>

          '
        raw: 'So FYI my GGMLs are not converted from the GPTQs. They''re converted
          directly from the unquantised original repo.  I used to try making GGMLs
          from 4bit GPTQs, but the files scored very badly on artificial benchmarks
          and I could never understand why. And then the llama.cpp quantisation methods
          improved significantly, so there was no point even trying that.


          In terms of inference quality, I believe the quantised GGMLs have now overtaken
          GPTQ in benchmarks.  There''s an artificial LLM benchmark called perplexity.  GPTQ
          scores well and used to be better than q4_0 GGML, but recently the llama.cpp
          team have done a ton of work on 4bit quantisation and their new methods
          q4_2 and q4_3 now beat 4bit GPTQ in this benchmark.


          Then the new 5bit methods q5_0 and q5_1 are even better than that.


          So if you want the absolute maximum inference quality - but don''t have
          the resources to load the model in 16bit or 8bit - you would go for 4bit
          or 5bit GGML.


          However in practical usage most people aren''t going to be able to tell
          the difference between a very good quantisation and a slightly better quantisation.
          So the bigger question is whether you want CPU or GPU inference.


          If you have an NVidia GPU and can do GPU inference then generally it should
          be much faster than CPU inference, and most people will care about that
          a lot more than they will a 3% improvement on an artificial benchmark.


          So as a general rule I would say: if you have an NVidia GPU and can do GPU
          inference, you should.


          All that said: there are some GPTQ performance problems with WizardLM specifically,
          which I''ve not been able to resolve yet. They''re not a big issue if you
          use Linux or WSL2 and can use the Triton branch of GPTQ-for-LLaMa. But if
          you need to use the CUDA branch - eg because you''re on Windows - you may
          find you get only 1-4tokens/s in GPU inference. And llama.cpp GGML can beat
          that on many CPUs.


          So it''s a bit complicated for this model! But for any other 7B model, GPU
          inference will win on speed.'
        updatedAt: '2023-04-27T20:23:28.770Z'
      numEdits: 0
      reactions:
      - count: 56
        reaction: "\U0001F44D"
        users:
        - Thireus
        - siddhesh22
        - CeeGee
        - Reggie
        - darxkies
        - lefnire
        - ekryski
        - boqsc
        - isevendays
        - artemka
        - zlzlzl
        - wrzaskun
        - LEMSM
        - marianbasti
        - sopack
        - Tom-Neverwinter
        - BlueAquilae
        - seosmooth
        - aminedjeghri
        - endolith
        - lucianosb
        - jialinliu
        - tjohnson
        - utensil
        - Goldenblood56
        - Filfil
        - georgiad
        - kujamara
        - AroOmega
        - TravelingMan
        - akkikiki
        - DrTako
        - Ilianos
        - drago1717
        - its-me
        - Cornmonster
        - BananaJoe
        - QtRoS
        - shafiqalibhai
        - tuxthepenguin84
        - thalamusprime
        - zhenpingfeng
        - streetyogi
        - Takuonline
        - bcvan83
        - udaykumar97
        - asniffin
        - t2050
        - mbuet2ner
        - Jasonnor
        - dotieuthien
        - jlzhou
        - Someman
        - nss-ysasaki
        - cayentine
        - thomastay
      - count: 14
        reaction: "\u2764\uFE0F"
        users:
        - seosmooth
        - Goldenblood56
        - DrTako
        - its-me
        - QtRoS
        - shafiqalibhai
        - vitakee
        - asniffin
        - mbuet2ner
        - Yangdf
        - dsmonk
        - Jasonnor
        - dotieuthien
        - jlzhou
    id: 644ad9c0cb45734dfd482b11
    type: comment
  author: TheBloke
  content: 'So FYI my GGMLs are not converted from the GPTQs. They''re converted directly
    from the unquantised original repo.  I used to try making GGMLs from 4bit GPTQs,
    but the files scored very badly on artificial benchmarks and I could never understand
    why. And then the llama.cpp quantisation methods improved significantly, so there
    was no point even trying that.


    In terms of inference quality, I believe the quantised GGMLs have now overtaken
    GPTQ in benchmarks.  There''s an artificial LLM benchmark called perplexity.  GPTQ
    scores well and used to be better than q4_0 GGML, but recently the llama.cpp team
    have done a ton of work on 4bit quantisation and their new methods q4_2 and q4_3
    now beat 4bit GPTQ in this benchmark.


    Then the new 5bit methods q5_0 and q5_1 are even better than that.


    So if you want the absolute maximum inference quality - but don''t have the resources
    to load the model in 16bit or 8bit - you would go for 4bit or 5bit GGML.


    However in practical usage most people aren''t going to be able to tell the difference
    between a very good quantisation and a slightly better quantisation. So the bigger
    question is whether you want CPU or GPU inference.


    If you have an NVidia GPU and can do GPU inference then generally it should be
    much faster than CPU inference, and most people will care about that a lot more
    than they will a 3% improvement on an artificial benchmark.


    So as a general rule I would say: if you have an NVidia GPU and can do GPU inference,
    you should.


    All that said: there are some GPTQ performance problems with WizardLM specifically,
    which I''ve not been able to resolve yet. They''re not a big issue if you use
    Linux or WSL2 and can use the Triton branch of GPTQ-for-LLaMa. But if you need
    to use the CUDA branch - eg because you''re on Windows - you may find you get
    only 1-4tokens/s in GPU inference. And llama.cpp GGML can beat that on many CPUs.


    So it''s a bit complicated for this model! But for any other 7B model, GPU inference
    will win on speed.'
  created_at: 2023-04-27 19:23:28+00:00
  edited: false
  hidden: false
  id: 644ad9c0cb45734dfd482b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-28T10:28:53.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Thank you for the detailed explanation. Didn''t realize Q4_3 &amp;
          4_2 were doing the rounds in llama.cpp. These things move so fast. 2 weeks
          is all you need to go out of touch!<br>Just wondering if adding a BLAS flag
          would improve speed? I read in some issue that BLAS speedup is kind of baked
          into the new 4_3 &amp; 4_2 methods. Or am I understanding it wrong?</p>

          '
        raw: 'Thank you for the detailed explanation. Didn''t realize Q4_3 & 4_2 were
          doing the rounds in llama.cpp. These things move so fast. 2 weeks is all
          you need to go out of touch!

          Just wondering if adding a BLAS flag would improve speed? I read in some
          issue that BLAS speedup is kind of baked into the new 4_3 & 4_2 methods.
          Or am I understanding it wrong?'
        updatedAt: '2023-04-28T10:28:53.267Z'
      numEdits: 0
      reactions: []
    id: 644b9fe56586065501e3ec15
    type: comment
  author: Reggie
  content: 'Thank you for the detailed explanation. Didn''t realize Q4_3 & 4_2 were
    doing the rounds in llama.cpp. These things move so fast. 2 weeks is all you need
    to go out of touch!

    Just wondering if adding a BLAS flag would improve speed? I read in some issue
    that BLAS speedup is kind of baked into the new 4_3 & 4_2 methods. Or am I understanding
    it wrong?'
  created_at: 2023-04-28 09:28:53+00:00
  edited: false
  hidden: false
  id: 644b9fe56586065501e3ec15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T13:32:22.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If you can add BLAS then I would. There''s certainly no harm.</p>

          <p>I know that BLAS is used for perplexity calculations and I believe it''s
          also used during inference for prompt evaluation. So it does help a bit
          with performance, especially on long prompts.</p>

          <p>On Ubuntu, I do the following on a new system:</p>

          <pre><code>sudo apt update -y &amp;&amp; sudo apt install -y libopenblas-dev
          libopenblas-base

          git clone https://github.com/ggerganov/llama.cpp

          cd llama.cpp

          LLAMA_OPENBLAS=1 make

          </code></pre>

          <p>Or if you have an NVidia GPU with CUDA installed, you can use CUBLAS
          instead with <code>LLAMA_CUBLAS=1 make</code>.  That is significantly faster
          than using OpenBLAS - for example when calculating perplexity on a 65B model,
          it was going to take 55 hours with OpenBLAS, but 3.5 hours with CUBLAS!</p>

          '
        raw: 'If you can add BLAS then I would. There''s certainly no harm.


          I know that BLAS is used for perplexity calculations and I believe it''s
          also used during inference for prompt evaluation. So it does help a bit
          with performance, especially on long prompts.


          On Ubuntu, I do the following on a new system:

          ```

          sudo apt update -y && sudo apt install -y libopenblas-dev libopenblas-base

          git clone https://github.com/ggerganov/llama.cpp

          cd llama.cpp

          LLAMA_OPENBLAS=1 make

          ```


          Or if you have an NVidia GPU with CUDA installed, you can use CUBLAS instead
          with `LLAMA_CUBLAS=1 make`.  That is significantly faster than using OpenBLAS
          - for example when calculating perplexity on a 65B model, it was going to
          take 55 hours with OpenBLAS, but 3.5 hours with CUBLAS!'
        updatedAt: '2023-04-28T13:33:07.347Z'
      numEdits: 2
      reactions:
      - count: 10
        reaction: "\U0001F44D"
        users:
        - Reggie
        - darxkies
        - ekryski
        - artemka
        - scootyoo
        - LEMSM
        - mlam14
        - TravelingMan
        - jasonvan
        - streetyogi
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
      - count: 1
        reaction: "\U0001F917"
        users:
        - Gregory-L
    id: 644bcae6cb0886c51c243b99
    type: comment
  author: TheBloke
  content: 'If you can add BLAS then I would. There''s certainly no harm.


    I know that BLAS is used for perplexity calculations and I believe it''s also
    used during inference for prompt evaluation. So it does help a bit with performance,
    especially on long prompts.


    On Ubuntu, I do the following on a new system:

    ```

    sudo apt update -y && sudo apt install -y libopenblas-dev libopenblas-base

    git clone https://github.com/ggerganov/llama.cpp

    cd llama.cpp

    LLAMA_OPENBLAS=1 make

    ```


    Or if you have an NVidia GPU with CUDA installed, you can use CUBLAS instead with
    `LLAMA_CUBLAS=1 make`.  That is significantly faster than using OpenBLAS - for
    example when calculating perplexity on a 65B model, it was going to take 55 hours
    with OpenBLAS, but 3.5 hours with CUBLAS!'
  created_at: 2023-04-28 12:32:22+00:00
  edited: true
  hidden: false
  id: 644bcae6cb0886c51c243b99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-29T07:15:29.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Thank you for all the great models and for being so helpful to the
          "less enlightened" like me!</p>

          '
        raw: Thank you for all the great models and for being so helpful to the "less
          enlightened" like me!
        updatedAt: '2023-04-29T07:15:29.232Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - darxkies
        - artemka
        - LEMSM
        - kujamara
        - TravelingMan
    id: 644cc4110dc952d245930dd1
    type: comment
  author: Reggie
  content: Thank you for all the great models and for being so helpful to the "less
    enlightened" like me!
  created_at: 2023-04-29 06:15:29+00:00
  edited: false
  hidden: false
  id: 644cc4110dc952d245930dd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
      fullname: Darx Kies
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darxkies
      type: user
    createdAt: '2023-04-30T18:52:20.000Z'
    data:
      edited: false
      editors:
      - darxkies
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e584cd353914b5320561199ddc383df5.svg
          fullname: Darx Kies
          isHf: false
          isPro: false
          name: darxkies
          type: user
        html: '<p>First of all, thank you for your work. Would it pay off to use 8bit/16bit
          GGML versions considering that it is a 7B?</p>

          '
        raw: First of all, thank you for your work. Would it pay off to use 8bit/16bit
          GGML versions considering that it is a 7B?
        updatedAt: '2023-04-30T18:52:20.699Z'
      numEdits: 0
      reactions: []
    id: 644eb8e4a00f4b11d39879d5
    type: comment
  author: darxkies
  content: First of all, thank you for your work. Would it pay off to use 8bit/16bit
    GGML versions considering that it is a 7B?
  created_at: 2023-04-30 17:52:20+00:00
  edited: false
  hidden: false
  id: 644eb8e4a00f4b11d39879d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T20:00:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>First of all, thank you for your work. Would it pay off to use 8bit/16bit
          GGML versions considering that it is a 7B?</p>

          </blockquote>

          <p>Definitely not. The inference will be much slower and the difference
          in theoretical accuracy between q5_1 and fp16 is so low that I can''t see
          how it''d be worth it being so much slower.</p>

          <p>Here''s the benchmark table from the llama.cpp README:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ShEtOy-Nu_-iyxf2SiOGU.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ShEtOy-Nu_-iyxf2SiOGU.png"></a></p>

          <p>For 7B, the difference in accuracy between q5_1 and fp16 is 0.006%! But
          the difference in speed is very significant. With 8 threads (<code>-t 8</code>),
          fp16 is 128ms/token (8 tokens/s) compared to 59m/s for q5_1 (17 tokens/s).</p>

          '
        raw: '> First of all, thank you for your work. Would it pay off to use 8bit/16bit
          GGML versions considering that it is a 7B?


          Definitely not. The inference will be much slower and the difference in
          theoretical accuracy between q5_1 and fp16 is so low that I can''t see how
          it''d be worth it being so much slower.


          Here''s the benchmark table from the llama.cpp README:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ShEtOy-Nu_-iyxf2SiOGU.png)


          For 7B, the difference in accuracy between q5_1 and fp16 is 0.006%! But
          the difference in speed is very significant. With 8 threads (`-t 8`), fp16
          is 128ms/token (8 tokens/s) compared to 59m/s for q5_1 (17 tokens/s).'
        updatedAt: '2023-04-30T20:00:41.124Z'
      numEdits: 0
      reactions:
      - count: 14
        reaction: "\U0001F44D"
        users:
        - darxkies
        - ekryski
        - artemka
        - zlzlzl
        - wrzaskun
        - sopack
        - jialinliu
        - Filfil
        - Ilianos
        - shafiqalibhai
        - cariaga
        - streetyogi
        - bcvan83
        - javenda
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
        - shafiqalibhai
        - khuam
        - Avaruuskettu
    id: 644ec8e9bf9683cba467d121
    type: comment
  author: TheBloke
  content: '> First of all, thank you for your work. Would it pay off to use 8bit/16bit
    GGML versions considering that it is a 7B?


    Definitely not. The inference will be much slower and the difference in theoretical
    accuracy between q5_1 and fp16 is so low that I can''t see how it''d be worth
    it being so much slower.


    Here''s the benchmark table from the llama.cpp README:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ShEtOy-Nu_-iyxf2SiOGU.png)


    For 7B, the difference in accuracy between q5_1 and fp16 is 0.006%! But the difference
    in speed is very significant. With 8 threads (`-t 8`), fp16 is 128ms/token (8
    tokens/s) compared to 59m/s for q5_1 (17 tokens/s).'
  created_at: 2023-04-30 19:00:41+00:00
  edited: false
  hidden: false
  id: 644ec8e9bf9683cba467d121
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-05-01T15:09:01.000Z'
    data:
      edited: true
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Do you have any inputs on the optimal number of threads? For 13B
          models I noticed inference speed seems to peak at 12 threads, and falls
          of after that. Havent really tested with 30B or 65B models.<br>Also any
          idea if it''s possible to do initial prompt ingestion with the GPU (CuBLAS)
          and then switch back to CPU for inference. CPU with 12 threads is actually
          faster at inference than the GPU for me, but GPU knocks it out of the park
          in terms of ingestion of long prompts.</p>

          '
        raw: 'Do you have any inputs on the optimal number of threads? For 13B models
          I noticed inference speed seems to peak at 12 threads, and falls of after
          that. Havent really tested with 30B or 65B models.

          Also any idea if it''s possible to do initial prompt ingestion with the
          GPU (CuBLAS) and then switch back to CPU for inference. CPU with 12 threads
          is actually faster at inference than the GPU for me, but GPU knocks it out
          of the park in terms of ingestion of long prompts.'
        updatedAt: '2023-05-01T15:10:11.793Z'
      numEdits: 1
      reactions: []
    id: 644fd60dd5f7dafcfa61832b
    type: comment
  author: Reggie
  content: 'Do you have any inputs on the optimal number of threads? For 13B models
    I noticed inference speed seems to peak at 12 threads, and falls of after that.
    Havent really tested with 30B or 65B models.

    Also any idea if it''s possible to do initial prompt ingestion with the GPU (CuBLAS)
    and then switch back to CPU for inference. CPU with 12 threads is actually faster
    at inference than the GPU for me, but GPU knocks it out of the park in terms of
    ingestion of long prompts.'
  created_at: 2023-05-01 14:09:01+00:00
  edited: true
  hidden: false
  id: 644fd60dd5f7dafcfa61832b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-01T15:32:36.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>The maximum you should try is your number of physical CPU cores.\
          \ Hyperthreading doesn't seem to help much or at all.</p>\n<p>I have an\
          \ 18 core, 36 thread system and so I have been using <code>-t 18</code>.</p>\n\
          <p>However yes there do appear to be bottlenecks with higher thread counts.\
          \ I've seen some graphs that show a drop off in performance above a certain\
          \ number of threads, and an indication that extra threads are basically\
          \ just idling.</p>\n<p>I just did a few quick tests, on WizardLM q5_1.</p>\n\
          <div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th>Threads</th>\n\
          <th>Run 1 ms/token</th>\n<th>Run 2 ms/token</th>\n<th>Run 3 ms/token</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td>18</td>\n<td>101.8</td>\n<td>102.1</td>\n\
          <td>102.1</td>\n</tr>\n<tr>\n<td>16</td>\n<td>85.4</td>\n<td>85</td>\n<td>86.8</td>\n\
          </tr>\n<tr>\n<td>12</td>\n<td>92.8</td>\n<td>93.5</td>\n<td>93.5</td>\n\
          </tr>\n<tr>\n<td>10</td>\n<td>107.5</td>\n<td>106</td>\n<td>107.3</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n<p>(Lower figures are better)</p>\n\
          <p>So the maximum performance was with 16 threads. But I'd say 12 threads\
          \ makes more sense if efficiency is any consideration, as the increase from\
          \ 12 to 16 was tiny.</p>\n<p>And If we were to work out the performance\
          \ per core we'd see that it's far from scaling linearly. So if maximum efficiency\
          \ was key, maybe even fewer threads would be best. Maybe that's why they\
          \ only went up to 8 threads in the llama.cpp benchmark.</p>\n<p>I don't\
          \ know how this varies with model size and quantisation method. That might\
          \ be worth some additional tests.</p>\n"
        raw: 'The maximum you should try is your number of physical CPU cores. Hyperthreading
          doesn''t seem to help much or at all.


          I have an 18 core, 36 thread system and so I have been using `-t 18`.


          However yes there do appear to be bottlenecks with higher thread counts.
          I''ve seen some graphs that show a drop off in performance above a certain
          number of threads, and an indication that extra threads are basically just
          idling.


          I just did a few quick tests, on WizardLM q5_1.


          | Threads | Run 1 ms/token | Run 2 ms/token | Run 3 ms/token |

          | -- | -- | -- | -- |

          | 18 | 101.8 | 102.1 | 102.1 |

          | 16 | 85.4 | 85 | 86.8 |

          | 12 | 92.8 | 93.5 | 93.5 |

          | 10 | 107.5 | 106 | 107.3 |


          (Lower figures are better)


          So the maximum performance was with 16 threads. But I''d say 12 threads
          makes more sense if efficiency is any consideration, as the increase from
          12 to 16 was tiny.


          And If we were to work out the performance per core we''d see that it''s
          far from scaling linearly. So if maximum efficiency was key, maybe even
          fewer threads would be best. Maybe that''s why they only went up to 8 threads
          in the llama.cpp benchmark.


          I don''t know how this varies with model size and quantisation method. That
          might be worth some additional tests.'
        updatedAt: '2023-05-01T15:33:18.625Z'
      numEdits: 1
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - darxkies
        - ekryski
        - boqsc
        - artemka
        - wrzaskun
        - Filfil
        - TravelingMan
        - BananaJoe
        - shafiqalibhai
        - Purinda
        - bcvan83
        - food-launcher
    id: 644fdb9420ba3e3e4beae5c7
    type: comment
  author: TheBloke
  content: 'The maximum you should try is your number of physical CPU cores. Hyperthreading
    doesn''t seem to help much or at all.


    I have an 18 core, 36 thread system and so I have been using `-t 18`.


    However yes there do appear to be bottlenecks with higher thread counts. I''ve
    seen some graphs that show a drop off in performance above a certain number of
    threads, and an indication that extra threads are basically just idling.


    I just did a few quick tests, on WizardLM q5_1.


    | Threads | Run 1 ms/token | Run 2 ms/token | Run 3 ms/token |

    | -- | -- | -- | -- |

    | 18 | 101.8 | 102.1 | 102.1 |

    | 16 | 85.4 | 85 | 86.8 |

    | 12 | 92.8 | 93.5 | 93.5 |

    | 10 | 107.5 | 106 | 107.3 |


    (Lower figures are better)


    So the maximum performance was with 16 threads. But I''d say 12 threads makes
    more sense if efficiency is any consideration, as the increase from 12 to 16 was
    tiny.


    And If we were to work out the performance per core we''d see that it''s far from
    scaling linearly. So if maximum efficiency was key, maybe even fewer threads would
    be best. Maybe that''s why they only went up to 8 threads in the llama.cpp benchmark.


    I don''t know how this varies with model size and quantisation method. That might
    be worth some additional tests.'
  created_at: 2023-05-01 14:32:36+00:00
  edited: true
  hidden: false
  id: 644fdb9420ba3e3e4beae5c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23aba778a7daae99348aeb0728cf4aec.svg
      fullname: Eric Kryski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ekryski
      type: user
    createdAt: '2023-05-07T02:36:59.000Z'
    data:
      edited: false
      editors:
      - ekryski
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23aba778a7daae99348aeb0728cf4aec.svg
          fullname: Eric Kryski
          isHf: false
          isPro: false
          name: ekryski
          type: user
        html: "<p>Just wanted to say thank you for your very helpful responses. \U0001F60A\
          </p>\n"
        raw: "Just wanted to say thank you for your very helpful responses. \U0001F60A"
        updatedAt: '2023-05-07T02:36:59.586Z'
      numEdits: 0
      reactions:
      - count: 14
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - isevendays
        - artemka
        - scepter
        - Neo-Cortex-42
        - Goldenblood56
        - TravelingMan
        - realsammyt
        - Ilianos
        - Purinda
        - bcvan83
        - mbuet2ner
        - TeamDman
        - jlzhou
    id: 64570ecb03625871eb7ed7fa
    type: comment
  author: ekryski
  content: "Just wanted to say thank you for your very helpful responses. \U0001F60A"
  created_at: 2023-05-07 01:36:59+00:00
  edited: false
  hidden: false
  id: 64570ecb03625871eb7ed7fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T06:47:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Glad to help!</p>

          '
        raw: Glad to help!
        updatedAt: '2023-05-07T06:47:20.074Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Renegadesoffun
    id: 64574978711ee86f6ee97ea6
    type: comment
  author: TheBloke
  content: Glad to help!
  created_at: 2023-05-07 05:47:20+00:00
  edited: false
  hidden: false
  id: 64574978711ee86f6ee97ea6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/wizardLM-7B-GGML
repo_type: model
status: open
target_branch: null
title: Which is better? GGML of GPTQ version or of the merged deltas
