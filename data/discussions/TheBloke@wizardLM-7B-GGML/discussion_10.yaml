!!python/object:huggingface_hub.community.DiscussionWithDetails
author: theman23290
conflicting_files: null
created_at: 2023-06-25 21:29:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otk2uo8o_Fygud_22UezU.jpeg?w=200&h=200&f=face
      fullname: Jack Hoffman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: theman23290
      type: user
    createdAt: '2023-06-25T22:29:11.000Z'
    data:
      edited: false
      editors:
      - theman23290
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8972585797309875
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otk2uo8o_Fygud_22UezU.jpeg?w=200&h=200&f=face
          fullname: Jack Hoffman
          isHf: false
          isPro: false
          name: theman23290
          type: user
        html: '<p>I am using Oobabooga to load this model along with the landmark
          Qlora and I am running into a "llamacppmodel object has no attribute dtype"
          error. I am wondering if anyone else is crazy enough to pair the ggml and
          qlora together like myself and make it work. I think it is because it wants
          the float16 version because the qlora was trained on float16 or if it is
          just impossible. Anyone get it to work or am I glossing over something?
          I have trust remote code on, context and truncate to 8192, and disable add
          the bos_token. If I force the generation I run into context overflow errors
          and crash.</p>

          <p>System Configuration:<br>OS: Debian 11<br>CPU: Xeon E5-2670v3 (25C Virtualized)<br>RAM:
          80GB<br>GPU: None</p>

          '
        raw: "I am using Oobabooga to load this model along with the landmark Qlora\
          \ and I am running into a \"llamacppmodel object has no attribute dtype\"\
          \ error. I am wondering if anyone else is crazy enough to pair the ggml\
          \ and qlora together like myself and make it work. I think it is because\
          \ it wants the float16 version because the qlora was trained on float16\
          \ or if it is just impossible. Anyone get it to work or am I glossing over\
          \ something? I have trust remote code on, context and truncate to 8192,\
          \ and disable add the bos_token. If I force the generation I run into context\
          \ overflow errors and crash.\r\n\r\nSystem Configuration:\r\nOS: Debian\
          \ 11\r\nCPU: Xeon E5-2670v3 (25C Virtualized)\r\nRAM: 80GB\r\nGPU: None"
        updatedAt: '2023-06-25T22:29:11.396Z'
      numEdits: 0
      reactions: []
    id: 6498bfb74ecdc6fbd699aef7
    type: comment
  author: theman23290
  content: "I am using Oobabooga to load this model along with the landmark Qlora\
    \ and I am running into a \"llamacppmodel object has no attribute dtype\" error.\
    \ I am wondering if anyone else is crazy enough to pair the ggml and qlora together\
    \ like myself and make it work. I think it is because it wants the float16 version\
    \ because the qlora was trained on float16 or if it is just impossible. Anyone\
    \ get it to work or am I glossing over something? I have trust remote code on,\
    \ context and truncate to 8192, and disable add the bos_token. If I force the\
    \ generation I run into context overflow errors and crash.\r\n\r\nSystem Configuration:\r\
    \nOS: Debian 11\r\nCPU: Xeon E5-2670v3 (25C Virtualized)\r\nRAM: 80GB\r\nGPU:\
    \ None"
  created_at: 2023-06-25 21:29:11+00:00
  edited: false
  hidden: false
  id: 6498bfb74ecdc6fbd699aef7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-25T22:56:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461722373962402
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m afraid you''re glossing over a very big something.  You cannot
          apply Landmark Attention to a GGML.   For there to be landmark attention
          support in GGML, it would have to be implemented directly in llama.cpp /
          llama-cpp-python, in C++.</p>

          <p>The reason we''re able to apply Landmark Attention to GPTQ and fp16 models
          is because they provide custom Python code which gets executed by the transformers
          library, triggered by the <code>trust_remote_code=True</code> argument.  This
          custom code tells it how to implement landmark attention.</p>

          <p>None of this is possible with GGML. There is no ability to execute custom
          code, and there is no code written for landmark attention with GGML anyway.</p>

          <p>There is work being done on extended context in llama, though not Landmark
          Attention.  Rather they are looking into the ROPE method.  If and when they
          implement that in Llama.cpp, you''d be able to load a ROPE model with increased
          context and it would work.</p>

          <p>So keep an eye on llama.cpp for developments in this area.  But in the
          meantime you won''t be able to apply them yourself.</p>

          '
        raw: 'I''m afraid you''re glossing over a very big something.  You cannot
          apply Landmark Attention to a GGML.   For there to be landmark attention
          support in GGML, it would have to be implemented directly in llama.cpp /
          llama-cpp-python, in C++.


          The reason we''re able to apply Landmark Attention to GPTQ and fp16 models
          is because they provide custom Python code which gets executed by the transformers
          library, triggered by the `trust_remote_code=True` argument.  This custom
          code tells it how to implement landmark attention.


          None of this is possible with GGML. There is no ability to execute custom
          code, and there is no code written for landmark attention with GGML anyway.


          There is work being done on extended context in llama, though not Landmark
          Attention.  Rather they are looking into the ROPE method.  If and when they
          implement that in Llama.cpp, you''d be able to load a ROPE model with increased
          context and it would work.


          So keep an eye on llama.cpp for developments in this area.  But in the meantime
          you won''t be able to apply them yourself.'
        updatedAt: '2023-06-25T22:56:59.420Z'
      numEdits: 0
      reactions: []
    id: 6498c63b9ae5acae9e02bc36
    type: comment
  author: TheBloke
  content: 'I''m afraid you''re glossing over a very big something.  You cannot apply
    Landmark Attention to a GGML.   For there to be landmark attention support in
    GGML, it would have to be implemented directly in llama.cpp / llama-cpp-python,
    in C++.


    The reason we''re able to apply Landmark Attention to GPTQ and fp16 models is
    because they provide custom Python code which gets executed by the transformers
    library, triggered by the `trust_remote_code=True` argument.  This custom code
    tells it how to implement landmark attention.


    None of this is possible with GGML. There is no ability to execute custom code,
    and there is no code written for landmark attention with GGML anyway.


    There is work being done on extended context in llama, though not Landmark Attention.  Rather
    they are looking into the ROPE method.  If and when they implement that in Llama.cpp,
    you''d be able to load a ROPE model with increased context and it would work.


    So keep an eye on llama.cpp for developments in this area.  But in the meantime
    you won''t be able to apply them yourself.'
  created_at: 2023-06-25 21:56:59+00:00
  edited: false
  hidden: false
  id: 6498c63b9ae5acae9e02bc36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otk2uo8o_Fygud_22UezU.jpeg?w=200&h=200&f=face
      fullname: Jack Hoffman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: theman23290
      type: user
    createdAt: '2023-06-25T23:19:17.000Z'
    data:
      edited: false
      editors:
      - theman23290
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9909094572067261
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/otk2uo8o_Fygud_22UezU.jpeg?w=200&h=200&f=face
          fullname: Jack Hoffman
          isHf: false
          isPro: false
          name: theman23290
          type: user
        html: '<p>Rip, I was hopeful. CPU only deployed to a rack has its strengths
          and weaknesses. I am hopeful for Oobabooga to adapt MPT 30b ggml into the
          service. I would just like to be broken free from 2048 jail lol.</p>

          '
        raw: Rip, I was hopeful. CPU only deployed to a rack has its strengths and
          weaknesses. I am hopeful for Oobabooga to adapt MPT 30b ggml into the service.
          I would just like to be broken free from 2048 jail lol.
        updatedAt: '2023-06-25T23:19:17.255Z'
      numEdits: 0
      reactions: []
    id: 6498cb75bdbb3c3333f6aaa3
    type: comment
  author: theman23290
  content: Rip, I was hopeful. CPU only deployed to a rack has its strengths and weaknesses.
    I am hopeful for Oobabooga to adapt MPT 30b ggml into the service. I would just
    like to be broken free from 2048 jail lol.
  created_at: 2023-06-25 22:19:17+00:00
  edited: false
  hidden: false
  id: 6498cb75bdbb3c3333f6aaa3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/wizardLM-7B-GGML
repo_type: model
status: open
target_branch: null
title: 'Landmark Qlora Compatibility '
