!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheStamp
conflicting_files: null
created_at: 2023-04-28 15:02:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7ac4351c4efb691f9eef3ecc7f0467b.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheStamp
      type: user
    createdAt: '2023-04-28T16:02:43.000Z'
    data:
      edited: false
      editors:
      - TheStamp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7ac4351c4efb691f9eef3ecc7f0467b.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: TheStamp
          type: user
        html: '<p>Much appreciated for these models! I''m curious about the different
          versions, can you point me to an article or discussion that goes into what
          the differences are between the files? Like between q4 and q5, and _0, _1,
          _2, etc?</p>

          <p>I''m pretty quick to learn, but its my google-fu has been failing me
          on the naming convention of these files! </p>

          <p>Thanks again!</p>

          '
        raw: "Much appreciated for these models! I'm curious about the different versions,\
          \ can you point me to an article or discussion that goes into what the differences\
          \ are between the files? Like between q4 and q5, and _0, _1, _2, etc?\r\n\
          \r\nI'm pretty quick to learn, but its my google-fu has been failing me\
          \ on the naming convention of these files! \r\n\r\nThanks again!"
        updatedAt: '2023-04-28T16:02:43.214Z'
      numEdits: 0
      reactions: []
    id: 644bee23778ecbfb977e4105
    type: comment
  author: TheStamp
  content: "Much appreciated for these models! I'm curious about the different versions,\
    \ can you point me to an article or discussion that goes into what the differences\
    \ are between the files? Like between q4 and q5, and _0, _1, _2, etc?\r\n\r\n\
    I'm pretty quick to learn, but its my google-fu has been failing me on the naming\
    \ convention of these files! \r\n\r\nThanks again!"
  created_at: 2023-04-28 15:02:43+00:00
  edited: false
  hidden: false
  id: 644bee23778ecbfb977e4105
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T16:56:57.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah some of these methods are very new and there's not much documentation\
          \ on them.  </p>\n<p>Here's a table that's part of the README for <a rel=\"\
          nofollow\" href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/W359v1upRw-pF5sRlXAPQ.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/W359v1upRw-pF5sRlXAPQ.png\"\
          ></a></p>\n<p>In this table, <code>ppl</code> means perplexity. It's an\
          \ artificial benchmark that gives a ballpark estimate for the quality of\
          \ a model's inference. Lower scores are better.</p>\n<p>So looking at 7B,\
          \ the full unquantised model (F16) scores 5.9565. Q5_1 scores 5.9934 which\
          \ is 0.07% worse.  Whereas q4_0 scores 6.2103 which is around 4% worse than\
          \ unquantised. The difference between q4_0 and q5_1 is about 3.5%.</p>\n\
          <p>Against the quality difference one also needs to consider other factors.\
          \ RAM usage might decide if you can load the model at all (you never want\
          \ to be swapping else performance goes through the floor), and then there's\
          \ inference speed.  That's represented in the table by ms/tok which is milliseconds\
          \ per token. <code><span data-props=\"{&quot;user&quot;:&quot;4th&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/4th\"\
          >@<span class=\"underline\">4th</span></a></span>\n\n\t</span></span></code>\
          \ means with using four threads for inference (<code>-t 4</code>) and <code>@8th</code>\
          \ is 8 threads (<code>-t 8</code>)</p>\n<p>Eg for 7B, a q4_0 is 47ms/tok\
          \ @8th compared to 59 for q5_1.  So the q5_1 has better theoretical quality,\
          \ but will take 25% longer to return a given response. </p>\n<p>A user needs\
          \ to compare all these factors and decide what's most important to them.</p>\n\
          <p>For 7B I think most people would be recommended to use the best quantisation\
          \ available, because 7B is already a small model and the RAM differences\
          \ are unlikely to affect many people, and 25% slower of \"very fast\" is\
          \ still pretty quick.</p>\n<p>When we start looking at 30B or 65B models,\
          \ the RAM differences and speed differences between q4 and q5 would be more\
          \ significant. It might be you have enough RAM for q4_2 but not for q5_1,\
          \ perhaps, and if you're only getting two tokens/s, then you might be loathe\
          \ to slow that down by a further 25%. </p>\n<p>Also, the delta between perplexity\
          \ scores (and thus accuracy) reduces with higher base models, meaning the\
          \ quality difference from lower bitrate quantisation may be less noticeable\
          \ because there's more data to begin with.  Looking at 13B in the table,\
          \ the perplexity difference between q5_1 and q4_0 is only 3.2% - a slightly\
          \ lower delta than the 3.5% we saw at 7B. And the difference will narrow\
          \ further for the larger models I believe.</p>\n<p>So for a 30B or 65B model\
          \ one may well prefer q4_0 or q4_2 to q5_0 or q5_1, where the opposite may\
          \ be true at 7B.</p>\n"
        raw: "Yeah some of these methods are very new and there's not much documentation\
          \ on them.  \n\nHere's a table that's part of the README for [llama.cpp](https://github.com/ggerganov/llama.cpp):\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/W359v1upRw-pF5sRlXAPQ.png)\n\
          \nIn this table, `ppl` means perplexity. It's an artificial benchmark that\
          \ gives a ballpark estimate for the quality of a model's inference. Lower\
          \ scores are better.\n\nSo looking at 7B, the full unquantised model (F16)\
          \ scores 5.9565. Q5_1 scores 5.9934 which is 0.07% worse.  Whereas q4_0\
          \ scores 6.2103 which is around 4% worse than unquantised. The difference\
          \ between q4_0 and q5_1 is about 3.5%.\n\nAgainst the quality difference\
          \ one also needs to consider other factors. RAM usage might decide if you\
          \ can load the model at all (you never want to be swapping else performance\
          \ goes through the floor), and then there's inference speed.  That's represented\
          \ in the table by ms/tok which is milliseconds per token. `@4th` means with\
          \ using four threads for inference (`-t 4`) and `@8th` is 8 threads (`-t\
          \ 8`)\n\nEg for 7B, a q4_0 is 47ms/tok @8th compared to 59 for q5_1.  So\
          \ the q5_1 has better theoretical quality, but will take 25% longer to return\
          \ a given response. \n\nA user needs to compare all these factors and decide\
          \ what's most important to them.\n\nFor 7B I think most people would be\
          \ recommended to use the best quantisation available, because 7B is already\
          \ a small model and the RAM differences are unlikely to affect many people,\
          \ and 25% slower of \"very fast\" is still pretty quick.\n\nWhen we start\
          \ looking at 30B or 65B models, the RAM differences and speed differences\
          \ between q4 and q5 would be more significant. It might be you have enough\
          \ RAM for q4_2 but not for q5_1, perhaps, and if you're only getting two\
          \ tokens/s, then you might be loathe to slow that down by a further 25%.\
          \ \n\nAlso, the delta between perplexity scores (and thus accuracy) reduces\
          \ with higher base models, meaning the quality difference from lower bitrate\
          \ quantisation may be less noticeable because there's more data to begin\
          \ with.  Looking at 13B in the table, the perplexity difference between\
          \ q5_1 and q4_0 is only 3.2% - a slightly lower delta than the 3.5% we saw\
          \ at 7B. And the difference will narrow further for the larger models I\
          \ believe.\n\nSo for a 30B or 65B model one may well prefer q4_0 or q4_2\
          \ to q5_0 or q5_1, where the opposite may be true at 7B."
        updatedAt: '2023-04-28T17:06:24.216Z'
      numEdits: 5
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - wmr
        - McSwitchy
    id: 644bfad945e79023c7dabbb2
    type: comment
  author: TheBloke
  content: "Yeah some of these methods are very new and there's not much documentation\
    \ on them.  \n\nHere's a table that's part of the README for [llama.cpp](https://github.com/ggerganov/llama.cpp):\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/W359v1upRw-pF5sRlXAPQ.png)\n\
    \nIn this table, `ppl` means perplexity. It's an artificial benchmark that gives\
    \ a ballpark estimate for the quality of a model's inference. Lower scores are\
    \ better.\n\nSo looking at 7B, the full unquantised model (F16) scores 5.9565.\
    \ Q5_1 scores 5.9934 which is 0.07% worse.  Whereas q4_0 scores 6.2103 which is\
    \ around 4% worse than unquantised. The difference between q4_0 and q5_1 is about\
    \ 3.5%.\n\nAgainst the quality difference one also needs to consider other factors.\
    \ RAM usage might decide if you can load the model at all (you never want to be\
    \ swapping else performance goes through the floor), and then there's inference\
    \ speed.  That's represented in the table by ms/tok which is milliseconds per\
    \ token. `@4th` means with using four threads for inference (`-t 4`) and `@8th`\
    \ is 8 threads (`-t 8`)\n\nEg for 7B, a q4_0 is 47ms/tok @8th compared to 59 for\
    \ q5_1.  So the q5_1 has better theoretical quality, but will take 25% longer\
    \ to return a given response. \n\nA user needs to compare all these factors and\
    \ decide what's most important to them.\n\nFor 7B I think most people would be\
    \ recommended to use the best quantisation available, because 7B is already a\
    \ small model and the RAM differences are unlikely to affect many people, and\
    \ 25% slower of \"very fast\" is still pretty quick.\n\nWhen we start looking\
    \ at 30B or 65B models, the RAM differences and speed differences between q4 and\
    \ q5 would be more significant. It might be you have enough RAM for q4_2 but not\
    \ for q5_1, perhaps, and if you're only getting two tokens/s, then you might be\
    \ loathe to slow that down by a further 25%. \n\nAlso, the delta between perplexity\
    \ scores (and thus accuracy) reduces with higher base models, meaning the quality\
    \ difference from lower bitrate quantisation may be less noticeable because there's\
    \ more data to begin with.  Looking at 13B in the table, the perplexity difference\
    \ between q5_1 and q4_0 is only 3.2% - a slightly lower delta than the 3.5% we\
    \ saw at 7B. And the difference will narrow further for the larger models I believe.\n\
    \nSo for a 30B or 65B model one may well prefer q4_0 or q4_2 to q5_0 or q5_1,\
    \ where the opposite may be true at 7B."
  created_at: 2023-04-28 15:56:57+00:00
  edited: true
  hidden: false
  id: 644bfad945e79023c7dabbb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7ac4351c4efb691f9eef3ecc7f0467b.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheStamp
      type: user
    createdAt: '2023-04-28T17:14:17.000Z'
    data:
      edited: false
      editors:
      - TheStamp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7ac4351c4efb691f9eef3ecc7f0467b.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: TheStamp
          type: user
        html: '<p>This is amazing! Thank you so much, I appreciate your added insight
          to help in the decision making!</p>

          '
        raw: This is amazing! Thank you so much, I appreciate your added insight to
          help in the decision making!
        updatedAt: '2023-04-28T17:14:17.104Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 644bfee9194e124dacb7fc51
    type: comment
  author: TheStamp
  content: This is amazing! Thank you so much, I appreciate your added insight to
    help in the decision making!
  created_at: 2023-04-28 16:14:17+00:00
  edited: false
  hidden: false
  id: 644bfee9194e124dacb7fc51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a7ac4351c4efb691f9eef3ecc7f0467b.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheStamp
      type: user
    createdAt: '2023-04-28T17:18:48.000Z'
    data:
      status: closed
    id: 644bfff8194e124dacb822b0
    type: status-change
  author: TheStamp
  created_at: 2023-04-28 16:18:48+00:00
  id: 644bfff8194e124dacb822b0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/wizardLM-7B-GGML
repo_type: model
status: closed
target_branch: null
title: Different Versions
