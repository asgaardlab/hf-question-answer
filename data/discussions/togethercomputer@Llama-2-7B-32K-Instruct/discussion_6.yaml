!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sadahila
conflicting_files: null
created_at: 2023-08-30 19:27:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
      fullname: Jiaming Zeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sadahila
      type: user
    createdAt: '2023-08-30T20:27:45.000Z'
    data:
      edited: true
      editors:
      - sadahila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405388236045837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
          fullname: Jiaming Zeng
          isHf: false
          isPro: false
          name: sadahila
          type: user
        html: '<p>Hi, </p>

          <p>Can I continue pretraining this model for text generation for domain
          adaptation purposes? Do you know if this would loose it''s instruct tuned
          abilities? </p>

          <p>Thanks in advance</p>

          '
        raw: "Hi, \n\nCan I continue pretraining this model for text generation for\
          \ domain adaptation purposes? Do you know if this would loose it's instruct\
          \ tuned abilities? \n\nThanks in advance"
        updatedAt: '2023-08-30T20:28:09.475Z'
      numEdits: 1
      reactions: []
    id: 64efa641911a74696e9d1245
    type: comment
  author: sadahila
  content: "Hi, \n\nCan I continue pretraining this model for text generation for\
    \ domain adaptation purposes? Do you know if this would loose it's instruct tuned\
    \ abilities? \n\nThanks in advance"
  created_at: 2023-08-30 19:27:45+00:00
  edited: true
  hidden: false
  id: 64efa641911a74696e9d1245
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-31T15:49:16.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9358690977096558
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;sadahila&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sadahila\"\
          >@<span class=\"underline\">sadahila</span></a></span>\n\n\t</span></span>\
          \ , that's an interesting question!</p>\n<p>I think it mostly depends on\
          \ for how long / how many tokens you continue training the model. Best is\
          \ of course if you have instruction data for your domain -- what would also\
          \ be interesting is to see whether it is possible to generate such a dataset\
          \ using instruction backtranslation (<a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2308.06259.pdf\"\
          >https://arxiv.org/pdf/2308.06259.pdf</a>). Let me know if you would try\
          \ that!</p>\n"
        raw: 'Hi @sadahila , that''s an interesting question!


          I think it mostly depends on for how long / how many tokens you continue
          training the model. Best is of course if you have instruction data for your
          domain -- what would also be interesting is to see whether it is possible
          to generate such a dataset using instruction backtranslation (https://arxiv.org/pdf/2308.06259.pdf).
          Let me know if you would try that!'
        updatedAt: '2023-08-31T15:49:16.122Z'
      numEdits: 0
      reactions: []
    id: 64f0b67ceeea81d0f8f612be
    type: comment
  author: mauriceweber
  content: 'Hi @sadahila , that''s an interesting question!


    I think it mostly depends on for how long / how many tokens you continue training
    the model. Best is of course if you have instruction data for your domain -- what
    would also be interesting is to see whether it is possible to generate such a
    dataset using instruction backtranslation (https://arxiv.org/pdf/2308.06259.pdf).
    Let me know if you would try that!'
  created_at: 2023-08-31 14:49:16+00:00
  edited: false
  hidden: false
  id: 64f0b67ceeea81d0f8f612be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
      fullname: Jiaming Zeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sadahila
      type: user
    createdAt: '2023-08-31T18:26:06.000Z'
    data:
      edited: false
      editors:
      - sadahila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9373029470443726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
          fullname: Jiaming Zeng
          isHf: false
          isPro: false
          name: sadahila
          type: user
        html: '<p>Thanks for the paper suggestion! I can also generate instruction
          data from our domain. However, I''m wondering if there is some benefit to
          doing Causal Language Modeling training first with some unsupervised data
          for domain adaptation and then doing instruction finetuning with our domain
          specific data. Or would it be advised that I start with the Llama-2-7B-32K
          instead of the Instruct version? </p>

          <p>Thanks. </p>

          '
        raw: "Thanks for the paper suggestion! I can also generate instruction data\
          \ from our domain. However, I'm wondering if there is some benefit to doing\
          \ Causal Language Modeling training first with some unsupervised data for\
          \ domain adaptation and then doing instruction finetuning with our domain\
          \ specific data. Or would it be advised that I start with the Llama-2-7B-32K\
          \ instead of the Instruct version? \n\nThanks. "
        updatedAt: '2023-08-31T18:26:06.919Z'
      numEdits: 0
      reactions: []
    id: 64f0db3e66552583a10d26b8
    type: comment
  author: sadahila
  content: "Thanks for the paper suggestion! I can also generate instruction data\
    \ from our domain. However, I'm wondering if there is some benefit to doing Causal\
    \ Language Modeling training first with some unsupervised data for domain adaptation\
    \ and then doing instruction finetuning with our domain specific data. Or would\
    \ it be advised that I start with the Llama-2-7B-32K instead of the Instruct version?\
    \ \n\nThanks. "
  created_at: 2023-08-31 17:26:06+00:00
  edited: false
  hidden: false
  id: 64f0db3e66552583a10d26b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-09-04T15:29:54.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9326995611190796
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: '<p>I see, I think it''s definitely worth a try to build on Llama-2-7B-32K
          if you have enough instruction tuning data for your domain (for comparison,
          the <a href="https://huggingface.co/datasets/togethercomputer/llama-instruct">llama-instruct</a>
          dataset has 19k samples). This way you would end up with a base model finetuned
          to your domain and then also instruction tuned. Out of curiosity, what is
          your technique to generate instructions?</p>

          '
        raw: I see, I think it's definitely worth a try to build on Llama-2-7B-32K
          if you have enough instruction tuning data for your domain (for comparison,
          the [llama-instruct](https://huggingface.co/datasets/togethercomputer/llama-instruct)
          dataset has 19k samples). This way you would end up with a base model finetuned
          to your domain and then also instruction tuned. Out of curiosity, what is
          your technique to generate instructions?
        updatedAt: '2023-09-04T15:29:54.448Z'
      numEdits: 0
      reactions: []
    id: 64f5f7f266c21a0926d263f0
    type: comment
  author: mauriceweber
  content: I see, I think it's definitely worth a try to build on Llama-2-7B-32K if
    you have enough instruction tuning data for your domain (for comparison, the [llama-instruct](https://huggingface.co/datasets/togethercomputer/llama-instruct)
    dataset has 19k samples). This way you would end up with a base model finetuned
    to your domain and then also instruction tuned. Out of curiosity, what is your
    technique to generate instructions?
  created_at: 2023-09-04 14:29:54+00:00
  edited: false
  hidden: false
  id: 64f5f7f266c21a0926d263f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
      fullname: Jiaming Zeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sadahila
      type: user
    createdAt: '2023-09-15T17:58:42.000Z'
    data:
      edited: false
      editors:
      - sadahila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9074851870536804
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
          fullname: Jiaming Zeng
          isHf: false
          isPro: false
          name: sadahila
          type: user
        html: '<p>Thanks for advice. I actually just add a header to our data similar
          to the dolly-instruct dataset: "Below is an instruction that describes a
          task. Write a response that appropriately completes the request. ### Instruction:"</p>

          '
        raw: 'Thanks for advice. I actually just add a header to our data similar
          to the dolly-instruct dataset: "Below is an instruction that describes a
          task. Write a response that appropriately completes the request. ### Instruction:"'
        updatedAt: '2023-09-15T17:58:42.675Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mauriceweber
    id: 65049b52ed19d8114b95cbe5
    type: comment
  author: sadahila
  content: 'Thanks for advice. I actually just add a header to our data similar to
    the dolly-instruct dataset: "Below is an instruction that describes a task. Write
    a response that appropriately completes the request. ### Instruction:"'
  created_at: 2023-09-15 16:58:42+00:00
  edited: false
  hidden: false
  id: 65049b52ed19d8114b95cbe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a3445613550fe88a1c90716871a472bf.svg
      fullname: Jiaming Zeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sadahila
      type: user
    createdAt: '2023-09-15T17:58:50.000Z'
    data:
      status: closed
    id: 65049b5ad3219dc63c4fae21
    type: status-change
  author: sadahila
  created_at: 2023-09-15 16:58:50+00:00
  id: 65049b5ad3219dc63c4fae21
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: togethercomputer/Llama-2-7B-32K-Instruct
repo_type: model
status: closed
target_branch: null
title: 'Can I continue pretraining this model for domain adaptation? '
