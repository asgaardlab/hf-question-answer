!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rozek
conflicting_files: null
created_at: 2023-08-31 02:31:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-08-31T03:31:17.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8129196763038635
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Thank you very much (again) for this marvellous work! Being able
          to use long contexts for analyzing texts with LLMs is really important!</p>

          <p>In order to use your model with <a rel="nofollow" href="https://github.com/rozek/llama.cpp">llama.cpp</a>,
          I''ve (again) generated some <a href="https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF">quantizations
          in GGUF format</a>.</p>

          <p>Assuming, that the prompt has the format described in the model card,
          the <a href="https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q8_0.gguf">Q8_0
          quantization</a> performs pretty well - on the other side, the <a href="https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q4_0.gguf">Q4_0
          quantization</a> hallucinates far too much.</p>

          <p>But, with 24GB of RAM, llama.cpp can now handle contexts up to the recommended
          limit of 32k when using the Q8_0 quantization - that''s really cool!</p>

          '
        raw: "Thank you very much (again) for this marvellous work! Being able to\
          \ use long contexts for analyzing texts with LLMs is really important!\r\
          \n\r\nIn order to use your model with [llama.cpp](https://github.com/rozek/llama.cpp),\
          \ I've (again) generated some [quantizations in GGUF format](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF).\r\
          \n\r\nAssuming, that the prompt has the format described in the model card,\
          \ the [Q8_0 quantization](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q8_0.gguf)\
          \ performs pretty well - on the other side, the [Q4_0 quantization](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q4_0.gguf)\
          \ hallucinates far too much.\r\n\r\nBut, with 24GB of RAM, llama.cpp can\
          \ now handle contexts up to the recommended limit of 32k when using the\
          \ Q8_0 quantization - that's really cool!"
        updatedAt: '2023-08-31T03:31:17.815Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - amaliak
    id: 64f00985c26b0228ec53baa9
    type: comment
  author: rozek
  content: "Thank you very much (again) for this marvellous work! Being able to use\
    \ long contexts for analyzing texts with LLMs is really important!\r\n\r\nIn order\
    \ to use your model with [llama.cpp](https://github.com/rozek/llama.cpp), I've\
    \ (again) generated some [quantizations in GGUF format](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF).\r\
    \n\r\nAssuming, that the prompt has the format described in the model card, the\
    \ [Q8_0 quantization](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q8_0.gguf)\
    \ performs pretty well - on the other side, the [Q4_0 quantization](https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF/blob/main/LLaMA-2-7B-32K-Instruct-Q4_0.gguf)\
    \ hallucinates far too much.\r\n\r\nBut, with 24GB of RAM, llama.cpp can now handle\
    \ contexts up to the recommended limit of 32k when using the Q8_0 quantization\
    \ - that's really cool!"
  created_at: 2023-08-31 02:31:17+00:00
  edited: false
  hidden: false
  id: 64f00985c26b0228ec53baa9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: togethercomputer/Llama-2-7B-32K-Instruct
repo_type: model
status: open
target_branch: null
title: Quantizations for llama.cpp
