!!python/object:huggingface_hub.community.DiscussionWithDetails
author: reubenlee3
conflicting_files: null
created_at: 2023-09-26 02:43:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5cc0c4e6874ceb01abae608638a0bc4a.svg
      fullname: Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: reubenlee3
      type: user
    createdAt: '2023-09-26T03:43:03.000Z'
    data:
      edited: true
      editors:
      - reubenlee3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7896214723587036
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5cc0c4e6874ceb01abae608638a0bc4a.svg
          fullname: Lee
          isHf: false
          isPro: false
          name: reubenlee3
          type: user
        html: '<p>I''m using the inference endpoint to quickly get one model up and
          running to test it. Below is my container configuration, but I can''t seem
          to make heads or tails of the error: Input validation error: <code>max_new_tokens</code>
          must be &lt;= 1. Given: 20. I''m using the GPU medium (Nvidia A10G) as my
          instance type.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6476c9e183d4fdaedddfe2ca/BKZlfGgo0eb0ISwks7Ryg.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6476c9e183d4fdaedddfe2ca/BKZlfGgo0eb0ISwks7Ryg.png"></a></p>

          <p>Any thoughts?</p>

          '
        raw: 'I''m using the inference endpoint to quickly get one model up and running
          to test it. Below is my container configuration, but I can''t seem to make
          heads or tails of the error: Input validation error: `max_new_tokens` must
          be <= 1. Given: 20. I''m using the GPU medium (Nvidia A10G) as my instance
          type.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476c9e183d4fdaedddfe2ca/BKZlfGgo0eb0ISwks7Ryg.png)


          Any thoughts?

          '
        updatedAt: '2023-09-26T03:45:18.609Z'
      numEdits: 1
      reactions: []
    id: 651253472f54c75d69f5d2b0
    type: comment
  author: reubenlee3
  content: 'I''m using the inference endpoint to quickly get one model up and running
    to test it. Below is my container configuration, but I can''t seem to make heads
    or tails of the error: Input validation error: `max_new_tokens` must be <= 1.
    Given: 20. I''m using the GPU medium (Nvidia A10G) as my instance type.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6476c9e183d4fdaedddfe2ca/BKZlfGgo0eb0ISwks7Ryg.png)


    Any thoughts?

    '
  created_at: 2023-09-26 02:43:03+00:00
  edited: true
  hidden: false
  id: 651253472f54c75d69f5d2b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-10-02T16:39:34.000Z'
    data:
      edited: true
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8577393889427185
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;reubenlee3&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/reubenlee3\"\
          >@<span class=\"underline\">reubenlee3</span></a></span>\n\n\t</span></span>\
          \ , I think you have to set the \"Max Number of Tokens (per Query)\" to\
          \ <code>Max Input Length (per Query)</code> + <code>max_new_tokens</code>\
          \ -- let me know if that solves the issue!</p>\n"
        raw: Hi @reubenlee3 , I think you have to set the "Max Number of Tokens (per
          Query)" to `Max Input Length (per Query)` + `max_new_tokens` -- let me know
          if that solves the issue!
        updatedAt: '2023-10-02T16:40:05.864Z'
      numEdits: 1
      reactions: []
    id: 651af2467a7ad76a365e1c8b
    type: comment
  author: mauriceweber
  content: Hi @reubenlee3 , I think you have to set the "Max Number of Tokens (per
    Query)" to `Max Input Length (per Query)` + `max_new_tokens` -- let me know if
    that solves the issue!
  created_at: 2023-10-02 15:39:34+00:00
  edited: true
  hidden: false
  id: 651af2467a7ad76a365e1c8b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: togethercomputer/Llama-2-7B-32K-Instruct
repo_type: model
status: open
target_branch: null
title: 'Input validation error: `max_new_tokens` must be <= 1. Given: 20'
