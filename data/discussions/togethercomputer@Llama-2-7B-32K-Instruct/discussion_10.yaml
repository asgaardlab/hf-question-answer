!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TZ20
conflicting_files: null
created_at: 2023-09-12 02:32:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/702a3ecb99ecdb9cc9e5cbec7169ae54.svg
      fullname: T Zeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TZ20
      type: user
    createdAt: '2023-09-12T03:32:15.000Z'
    data:
      edited: true
      editors:
      - TZ20
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8190434575080872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/702a3ecb99ecdb9cc9e5cbec7169ae54.svg
          fullname: T Zeng
          isHf: false
          isPro: false
          name: TZ20
          type: user
        html: '<p>Hi, if I set <code>trust_remote_code = False</code>when loading
          the model, will it just be the normal LlamaForCausalLM? If so, then running
          with 32K length would require too much computational power</p>

          '
        raw: Hi, if I set `trust_remote_code = False`when loading the model, will
          it just be the normal LlamaForCausalLM? If so, then running with 32K length
          would require too much computational power
        updatedAt: '2023-09-12T04:15:15.644Z'
      numEdits: 1
      reactions: []
    id: 64ffdbbf69219ce3e48ad3aa
    type: comment
  author: TZ20
  content: Hi, if I set `trust_remote_code = False`when loading the model, will it
    just be the normal LlamaForCausalLM? If so, then running with 32K length would
    require too much computational power
  created_at: 2023-09-12 02:32:15+00:00
  edited: true
  hidden: false
  id: 64ffdbbf69219ce3e48ad3aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-09-12T17:38:25.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8391939997673035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TZ20&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TZ20\">@<span class=\"\
          underline\">TZ20</span></a></span>\n\n\t</span></span> , thanks for your\
          \ question! Yes, setting <code>trust_remote_code=False</code>  will result\
          \ in using the LlamaForCausalLM hardcoded in the huggingface library. Since\
          \ this does not make use of flash attention, the speed will be lower and\
          \ memory footprint higher.</p>\n"
        raw: Hi @TZ20 , thanks for your question! Yes, setting `trust_remote_code=False`  will
          result in using the LlamaForCausalLM hardcoded in the huggingface library.
          Since this does not make use of flash attention, the speed will be lower
          and memory footprint higher.
        updatedAt: '2023-09-12T17:38:25.201Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TZ20
    id: 6500a2117804f04a1641d628
    type: comment
  author: mauriceweber
  content: Hi @TZ20 , thanks for your question! Yes, setting `trust_remote_code=False`  will
    result in using the LlamaForCausalLM hardcoded in the huggingface library. Since
    this does not make use of flash attention, the speed will be lower and memory
    footprint higher.
  created_at: 2023-09-12 16:38:25+00:00
  edited: false
  hidden: false
  id: 6500a2117804f04a1641d628
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: togethercomputer/Llama-2-7B-32K-Instruct
repo_type: model
status: open
target_branch: null
title: 'Loading model without fast-attn '
