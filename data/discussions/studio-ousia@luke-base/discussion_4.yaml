!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Saptarshi7
conflicting_files: null
created_at: 2023-04-18 02:08:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-04-18T03:08:35.000Z'
    data:
      edited: true
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Hi, when I try to run luke-base for masked language modelling, I''m
          continuously getting this error:</p>

          <p>RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmEx</p>

          <p>I''ve tried updating transformers, torch, accelerate, reducing batch
          size. However, nothing seems to work. Could you tell me what could be the
          issue?</p>

          <p>I tried changing from AutoModelForMaskedLM to LukeForMaskedLM, but I''m
          guessing we can''t pretrain without entity tokens correct?</p>

          '
        raw: 'Hi, when I try to run luke-base for masked language modelling, I''m
          continuously getting this error:


          RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmEx


          I''ve tried updating transformers, torch, accelerate, reducing batch size.
          However, nothing seems to work. Could you tell me what could be the issue?


          I tried changing from AutoModelForMaskedLM to LukeForMaskedLM, but I''m
          guessing we can''t pretrain without entity tokens correct?'
        updatedAt: '2023-04-18T04:39:57.284Z'
      numEdits: 1
      reactions: []
    id: 643e09b3975b365d472bcaab
    type: comment
  author: Saptarshi7
  content: 'Hi, when I try to run luke-base for masked language modelling, I''m continuously
    getting this error:


    RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmEx


    I''ve tried updating transformers, torch, accelerate, reducing batch size. However,
    nothing seems to work. Could you tell me what could be the issue?


    I tried changing from AutoModelForMaskedLM to LukeForMaskedLM, but I''m guessing
    we can''t pretrain without entity tokens correct?'
  created_at: 2023-04-18 02:08:35+00:00
  edited: true
  hidden: false
  id: 643e09b3975b365d472bcaab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-05-01T11:45:15.000Z'
    data:
      edited: false
      editors:
      - ryo0634
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: '<p>The LUKE model should work without entity inputs.<br>The error above
          seems to be related to CUDA issues.<br>Is it possible that there is a software
          and hardware mismatch causing this error?</p>

          '
        raw: 'The LUKE model should work without entity inputs.

          The error above seems to be related to CUDA issues.

          Is it possible that there is a software and hardware mismatch causing this
          error?'
        updatedAt: '2023-05-01T11:45:15.620Z'
      numEdits: 0
      reactions: []
    id: 644fa64b577838187ef6bde3
    type: comment
  author: ryo0634
  content: 'The LUKE model should work without entity inputs.

    The error above seems to be related to CUDA issues.

    Is it possible that there is a software and hardware mismatch causing this error?'
  created_at: 2023-05-01 10:45:15+00:00
  edited: false
  hidden: false
  id: 644fa64b577838187ef6bde3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-05-01T15:35:59.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Hi, Thank you for responding. Unfortunately, I have tried changing
          everything i.e updating all related packages - transformers, accelerate,
          torch, etc. but still get the same error...</p>

          '
        raw: Hi, Thank you for responding. Unfortunately, I have tried changing everything
          i.e updating all related packages - transformers, accelerate, torch, etc.
          but still get the same error...
        updatedAt: '2023-05-01T15:35:59.929Z'
      numEdits: 0
      reactions: []
    id: 644fdc5fd5f7dafcfa6217a2
    type: comment
  author: Saptarshi7
  content: Hi, Thank you for responding. Unfortunately, I have tried changing everything
    i.e updating all related packages - transformers, accelerate, torch, etc. but
    still get the same error...
  created_at: 2023-05-01 14:35:59+00:00
  edited: false
  hidden: false
  id: 644fdc5fd5f7dafcfa6217a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-05-02T01:16:08.000Z'
    data:
      edited: false
      editors:
      - ryo0634
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: '<p>Okay, I understand.<br>Does this issue persist with other models
          such as <code>BertForMaskedLM</code>?<br>If not, I''ll investigate whether
          a particular operation in LUKE is causing the error.</p>

          <p>It appears that similar errors have been resolved by downgrading torch.
          It might be worth trying that approach.<br><a rel="nofollow" href="https://discuss.pytorch.org/t/cuda-error-cublas-status-internal-error-when-calling-cublascreate-handle/114341">https://discuss.pytorch.org/t/cuda-error-cublas-status-internal-error-when-calling-cublascreate-handle/114341</a></p>

          '
        raw: 'Okay, I understand.

          Does this issue persist with other models such as `BertForMaskedLM`?

          If not, I''ll investigate whether a particular operation in LUKE is causing
          the error.


          It appears that similar errors have been resolved by downgrading torch.
          It might be worth trying that approach.

          https://discuss.pytorch.org/t/cuda-error-cublas-status-internal-error-when-calling-cublascreate-handle/114341'
        updatedAt: '2023-05-02T01:16:08.087Z'
      numEdits: 0
      reactions: []
    id: 6450645820ba3e3e4bf62a8f
    type: comment
  author: ryo0634
  content: 'Okay, I understand.

    Does this issue persist with other models such as `BertForMaskedLM`?

    If not, I''ll investigate whether a particular operation in LUKE is causing the
    error.


    It appears that similar errors have been resolved by downgrading torch. It might
    be worth trying that approach.

    https://discuss.pytorch.org/t/cuda-error-cublas-status-internal-error-when-calling-cublascreate-handle/114341'
  created_at: 2023-05-02 00:16:08+00:00
  edited: false
  hidden: false
  id: 6450645820ba3e3e4bf62a8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-05-02T15:44:35.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Thank you for the response. I believe I tried that as well i.e.
          trying to run on torch 1.8. However, the error persisted. I''ll look at
          1.7 though (which was mentioned in their link). And no, the problem did
          not occur for BERT, RoBERTa or distilbert</p>

          '
        raw: Thank you for the response. I believe I tried that as well i.e. trying
          to run on torch 1.8. However, the error persisted. I'll look at 1.7 though
          (which was mentioned in their link). And no, the problem did not occur for
          BERT, RoBERTa or distilbert
        updatedAt: '2023-05-02T15:44:35.976Z'
      numEdits: 0
      reactions: []
    id: 64512fe39d916c596e2b323c
    type: comment
  author: Saptarshi7
  content: Thank you for the response. I believe I tried that as well i.e. trying
    to run on torch 1.8. However, the error persisted. I'll look at 1.7 though (which
    was mentioned in their link). And no, the problem did not occur for BERT, RoBERTa
    or distilbert
  created_at: 2023-05-02 14:44:35+00:00
  edited: false
  hidden: false
  id: 64512fe39d916c596e2b323c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-05-05T07:33:56.000Z'
    data:
      edited: false
      editors:
      - ryo0634
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: '<p>I''m wondering where the error is occurring inside the model.<br>Do
          you observe any more specific errors when you set the environmental variable
          CUDA_LAUNCH_BLOCKING=1?<br>(This sometimes gives you more detailed error
          messages.)</p>

          '
        raw: 'I''m wondering where the error is occurring inside the model.

          Do you observe any more specific errors when you set the environmental variable
          CUDA_LAUNCH_BLOCKING=1?

          (This sometimes gives you more detailed error messages.)'
        updatedAt: '2023-05-05T07:33:56.284Z'
      numEdits: 0
      reactions: []
    id: 6454b164fe2f48cb4b5d7c74
    type: comment
  author: ryo0634
  content: 'I''m wondering where the error is occurring inside the model.

    Do you observe any more specific errors when you set the environmental variable
    CUDA_LAUNCH_BLOCKING=1?

    (This sometimes gives you more detailed error messages.)'
  created_at: 2023-05-05 06:33:56+00:00
  edited: false
  hidden: false
  id: 6454b164fe2f48cb4b5d7c74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-05-12T03:38:43.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Hello, sorry for the late reply. However, I''ll try my best to get
          back to you with a more detailed error message.</p>

          '
        raw: Hello, sorry for the late reply. However, I'll try my best to get back
          to you with a more detailed error message.
        updatedAt: '2023-05-12T03:38:43.564Z'
      numEdits: 0
      reactions: []
    id: 645db4c3f36ed281fac2be7d
    type: comment
  author: Saptarshi7
  content: Hello, sorry for the late reply. However, I'll try my best to get back
    to you with a more detailed error message.
  created_at: 2023-05-12 02:38:43+00:00
  edited: false
  hidden: false
  id: 645db4c3f36ed281fac2be7d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: studio-ousia/luke-base
repo_type: model
status: open
target_branch: null
title: CUDA error when trying to Pre-Train
