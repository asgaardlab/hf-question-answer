!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Saptarshi7
conflicting_files: null
created_at: 2023-02-22 23:18:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-02-22T23:18:22.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Hi,</p>

          <p>I''m trying to use LUKE on SQuAD. So, when I try to process the training
          examples with the tokenizer, I get this error:</p>

          <p>NotImplementedError: return_offset_mapping is not available when using
          Python tokenizers. To use this feature, change your tokenizer to one deriving
          from transformers.PreTrainedTokenizerFast.</p>

          <p>Could you tell me what the issue might be? I am running the latest HuggingFace
          version.</p>

          '
        raw: "Hi,\r\n\r\nI'm trying to use LUKE on SQuAD. So, when I try to process\
          \ the training examples with the tokenizer, I get this error:\r\n\r\nNotImplementedError:\
          \ return_offset_mapping is not available when using Python tokenizers. To\
          \ use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.\r\
          \n\r\nCould you tell me what the issue might be? I am running the latest\
          \ HuggingFace version."
        updatedAt: '2023-02-22T23:18:22.817Z'
      numEdits: 0
      reactions: []
    id: 63f6a2bea67b8acfa50331dc
    type: comment
  author: Saptarshi7
  content: "Hi,\r\n\r\nI'm trying to use LUKE on SQuAD. So, when I try to process\
    \ the training examples with the tokenizer, I get this error:\r\n\r\nNotImplementedError:\
    \ return_offset_mapping is not available when using Python tokenizers. To use\
    \ this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.\r\
    \n\r\nCould you tell me what the issue might be? I am running the latest HuggingFace\
    \ version."
  created_at: 2023-02-22 23:18:22+00:00
  edited: false
  hidden: false
  id: 63f6a2bea67b8acfa50331dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-02-24T01:49:04.000Z'
    data:
      edited: true
      editors:
      - ryo0634
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: '<p>The problem stems from the fact that the fast tokenizer is not implemented
          for <code>LukeTokenizer</code>, so it cannot be used with <code>return_offsets_mapping=True</code>.<br>As
          a workaround, you can use <code>roberta-base</code> for tokenizing text,
          which has its fast tokenizer implementation and the same wordpiece vocabulary
          as <code>luke-base</code>.</p>

          <p>Example:</p>

          <pre><code>&gt;&gt;  model_name = "roberta-base"

          &gt;&gt;  tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

          &gt;&gt;  tokenizer.encode_plus("this is test", return_offsets_mapping=True)

          {''input_ids'': [0, 9226, 16, 1296, 2], ''attention_mask'': [1, 1, 1, 1,
          1], ''offset_mapping'': [(0, 0), (0, 4), (5, 7), (8, 12), (0, 0)]}

          </code></pre>

          '
        raw: 'The problem stems from the fact that the fast tokenizer is not implemented
          for `LukeTokenizer`, so it cannot be used with `return_offsets_mapping=True`.

          As a workaround, you can use `roberta-base` for tokenizing text, which has
          its fast tokenizer implementation and the same wordpiece vocabulary as `luke-base`.


          Example:

          ```

          >>  model_name = "roberta-base"

          >>  tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

          >>  tokenizer.encode_plus("this is test", return_offsets_mapping=True)

          {''input_ids'': [0, 9226, 16, 1296, 2], ''attention_mask'': [1, 1, 1, 1,
          1], ''offset_mapping'': [(0, 0), (0, 4), (5, 7), (8, 12), (0, 0)]}

          ```'
        updatedAt: '2023-02-24T01:50:08.501Z'
      numEdits: 1
      reactions: []
    id: 63f817902b79f7695a358598
    type: comment
  author: ryo0634
  content: 'The problem stems from the fact that the fast tokenizer is not implemented
    for `LukeTokenizer`, so it cannot be used with `return_offsets_mapping=True`.

    As a workaround, you can use `roberta-base` for tokenizing text, which has its
    fast tokenizer implementation and the same wordpiece vocabulary as `luke-base`.


    Example:

    ```

    >>  model_name = "roberta-base"

    >>  tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

    >>  tokenizer.encode_plus("this is test", return_offsets_mapping=True)

    {''input_ids'': [0, 9226, 16, 1296, 2], ''attention_mask'': [1, 1, 1, 1, 1], ''offset_mapping'':
    [(0, 0), (0, 4), (5, 7), (8, 12), (0, 0)]}

    ```'
  created_at: 2023-02-24 01:49:04+00:00
  edited: true
  hidden: false
  id: 63f817902b79f7695a358598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-02-27T15:16:11.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Yes, thank you. This worked!</p>

          '
        raw: Yes, thank you. This worked!
        updatedAt: '2023-02-27T15:16:11.553Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ryo0634
      relatedEventId: 63fcc93bb907d43a91daa1e6
    id: 63fcc93bb907d43a91daa1e5
    type: comment
  author: Saptarshi7
  content: Yes, thank you. This worked!
  created_at: 2023-02-27 15:16:11+00:00
  edited: false
  hidden: false
  id: 63fcc93bb907d43a91daa1e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-02-27T15:16:11.000Z'
    data:
      status: closed
    id: 63fcc93bb907d43a91daa1e6
    type: status-change
  author: Saptarshi7
  created_at: 2023-02-27 15:16:11+00:00
  id: 63fcc93bb907d43a91daa1e6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: studio-ousia/luke-base
repo_type: model
status: closed
target_branch: null
title: Tokenizer Issue
