!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xun
conflicting_files: null
created_at: 2023-12-03 02:06:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a5bc85f3023e76fe8fc72cb2c06981a.svg
      fullname: feng
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: xun
      type: user
    createdAt: '2023-12-03T02:06:06.000Z'
    data:
      edited: false
      editors:
      - xun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5489779710769653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a5bc85f3023e76fe8fc72cb2c06981a.svg
          fullname: feng
          isHf: false
          isPro: false
          name: xun
          type: user
        html: "<pre><code>from transformers import AutoTokenizer, TextGenerationPipeline\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport logging\n\
          \nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)s [%(name)s]\
          \ %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\n\n\
          pretrained_model_dir = \"Qwen-Audio-Chat\"\nquantized_model_dir = \"Qwen-Audio-Chat-Int4\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True,trust_remote_code=True)\n\
          examples = [\n    tokenizer(\n        \"auto-gptq is an easy-to-use model\
          \ quantization library with user-friendly apis, based on GPTQ algorithm.\"\
          \n    )\n]\n\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize\
          \ model to 4-bit\n    group_size=128,  # it is recommended to set the value\
          \ to 128\n    desc_act=False,  # set to False can significantly speed up\
          \ inference but the perplexity may slightly bad\n    damp_percent=0.01,\n\
          \    static_groups=False,\n    sym=True,\n    true_sequential=True\n)\n\n\
          # load un-quantized model, by default, the model will always be loaded into\
          \ CPU memory\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config,trust_remote_code=True)\n\n# quantize model, the examples\
          \ should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\
          \nmodel.quantize(examples)\n\n# save quantized model\nmodel.save_quantized(quantized_model_dir)\n\
          \n# save quantized model using safetensors\n#model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n\n# push quantized model to Hugging Face Hub.\n\
          # to use use_auth_token=True, Login first via huggingface-cli login.\n#\
          \ or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n# (uncomment\
          \ the following three lines to enable this feature)\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
          \n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
          \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n\
          # model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n\
          \n# alternatively you can save and push at the same time\n# (uncomment the\
          \ following three lines to enable this feature)\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
          \n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
          \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n\
          # model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True,\
          \ commit_message=commit_message, use_auth_token=True)\n\n# load quantized\
          \ model to the first GPU\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\",trust_remote_code=True)\n\n# download quantized model\
          \ from Hugging Face Hub and load to the first GPU\n# model = AutoGPTQForCausalLM.from_quantized(repo_id,\
          \ device=\"cuda:0\", use_safetensors=True, use_triton=False)\n\n# inference\
          \ with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"\
          auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n\n# or you\
          \ can also use pipeline\npipeline = TextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"\
          ])\n</code></pre>\n"
        raw: "```\r\nfrom transformers import AutoTokenizer, TextGenerationPipeline\r\
          \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nimport\
          \ logging\r\n\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s %(levelname)s\
          \ [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\
          \r\n)\r\n\r\npretrained_model_dir = \"Qwen-Audio-Chat\"\r\nquantized_model_dir\
          \ = \"Qwen-Audio-Chat-Int4\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir,\
          \ use_fast=True,trust_remote_code=True)\r\nexamples = [\r\n    tokenizer(\r\
          \n        \"auto-gptq is an easy-to-use model quantization library with\
          \ user-friendly apis, based on GPTQ algorithm.\"\r\n    )\r\n]\r\n\r\nquantize_config\
          \ = BaseQuantizeConfig(\r\n    bits=4,  # quantize model to 4-bit\r\n  \
          \  group_size=128,  # it is recommended to set the value to 128\r\n    desc_act=False,\
          \  # set to False can significantly speed up inference but the perplexity\
          \ may slightly bad\r\n    damp_percent=0.01,\r\n    static_groups=False,\r\
          \n    sym=True,\r\n    true_sequential=True\r\n)\r\n\r\n# load un-quantized\
          \ model, by default, the model will always be loaded into CPU memory\r\n\
          model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config,trust_remote_code=True)\r\
          \n\r\n# quantize model, the examples should be list of dict whose keys can\
          \ only be \"input_ids\" and \"attention_mask\"\r\nmodel.quantize(examples)\r\
          \n\r\n# save quantized model\r\nmodel.save_quantized(quantized_model_dir)\r\
          \n\r\n# save quantized model using safetensors\r\n#model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\r\n\r\n# push quantized model to Hugging Face Hub.\r\
          \n# to use use_auth_token=True, Login first via huggingface-cli login.\r\
          \n# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\r\n# (uncomment\
          \ the following three lines to enable this feature)\r\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
          \r\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
          \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\r\
          \n# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\r\
          \n\r\n# alternatively you can save and push at the same time\r\n# (uncomment\
          \ the following three lines to enable this feature)\r\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
          \r\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
          \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\r\
          \n# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True,\
          \ commit_message=commit_message, use_auth_token=True)\r\n\r\n# load quantized\
          \ model to the first GPU\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\",trust_remote_code=True)\r\n\r\n# download quantized\
          \ model from Hugging Face Hub and load to the first GPU\r\n# model = AutoGPTQForCausalLM.from_quantized(repo_id,\
          \ device=\"cuda:0\", use_safetensors=True, use_triton=False)\r\n\r\n# inference\
          \ with model.generate\r\nprint(tokenizer.decode(model.generate(**tokenizer(\"\
          auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\r\n\r\n# or\
          \ you can also use pipeline\r\npipeline = TextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\r\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"\
          ])\r\n```"
        updatedAt: '2023-12-03T02:06:06.940Z'
      numEdits: 0
      reactions: []
    id: 656be28e27cb1927ca18579e
    type: comment
  author: xun
  content: "```\r\nfrom transformers import AutoTokenizer, TextGenerationPipeline\r\
    \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nimport logging\r\
    \n\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s %(levelname)s [%(name)s]\
    \ %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\r\n)\r\n\r\n\
    pretrained_model_dir = \"Qwen-Audio-Chat\"\r\nquantized_model_dir = \"Qwen-Audio-Chat-Int4\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True,trust_remote_code=True)\r\
    \nexamples = [\r\n    tokenizer(\r\n        \"auto-gptq is an easy-to-use model\
    \ quantization library with user-friendly apis, based on GPTQ algorithm.\"\r\n\
    \    )\r\n]\r\n\r\nquantize_config = BaseQuantizeConfig(\r\n    bits=4,  # quantize\
    \ model to 4-bit\r\n    group_size=128,  # it is recommended to set the value\
    \ to 128\r\n    desc_act=False,  # set to False can significantly speed up inference\
    \ but the perplexity may slightly bad\r\n    damp_percent=0.01,\r\n    static_groups=False,\r\
    \n    sym=True,\r\n    true_sequential=True\r\n)\r\n\r\n# load un-quantized model,\
    \ by default, the model will always be loaded into CPU memory\r\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
    \ quantize_config,trust_remote_code=True)\r\n\r\n# quantize model, the examples\
    \ should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\
    \r\nmodel.quantize(examples)\r\n\r\n# save quantized model\r\nmodel.save_quantized(quantized_model_dir)\r\
    \n\r\n# save quantized model using safetensors\r\n#model.save_quantized(quantized_model_dir,\
    \ use_safetensors=True)\r\n\r\n# push quantized model to Hugging Face Hub.\r\n\
    # to use use_auth_token=True, Login first via huggingface-cli login.\r\n# or pass\
    \ explcit token with: use_auth_token=\"hf_xxxxxxx\"\r\n# (uncomment the following\
    \ three lines to enable this feature)\r\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
    \r\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
    \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\r\n# model.push_to_hub(repo_id,\
    \ commit_message=commit_message, use_auth_token=True)\r\n\r\n# alternatively you\
    \ can save and push at the same time\r\n# (uncomment the following three lines\
    \ to enable this feature)\r\n# repo_id = f\"YourUserName/{quantized_model_dir}\"\
    \r\n# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits,\
    \ gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\r\n# model.push_to_hub(repo_id,\
    \ save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message,\
    \ use_auth_token=True)\r\n\r\n# load quantized model to the first GPU\r\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\"\
    ,trust_remote_code=True)\r\n\r\n# download quantized model from Hugging Face Hub\
    \ and load to the first GPU\r\n# model = AutoGPTQForCausalLM.from_quantized(repo_id,\
    \ device=\"cuda:0\", use_safetensors=True, use_triton=False)\r\n\r\n# inference\
    \ with model.generate\r\nprint(tokenizer.decode(model.generate(**tokenizer(\"\
    auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\r\n\r\n# or you\
    \ can also use pipeline\r\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\r\
    \nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\r\n```"
  created_at: 2023-12-03 02:06:06+00:00
  edited: false
  hidden: false
  id: 656be28e27cb1927ca18579e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: xun/Qwen-Audio-Chat-Int4
repo_type: model
status: open
target_branch: null
title: quantize code
