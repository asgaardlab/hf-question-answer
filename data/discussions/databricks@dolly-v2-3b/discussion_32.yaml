!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Santa-Sheep
conflicting_files: null
created_at: 2023-06-28 22:28:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e368c0a549e4e05c437f5efb584e0819.svg
      fullname: Peter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Santa-Sheep
      type: user
    createdAt: '2023-06-28T23:28:54.000Z'
    data:
      edited: false
      editors:
      - Santa-Sheep
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8672343492507935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e368c0a549e4e05c437f5efb584e0819.svg
          fullname: Peter
          isHf: false
          isPro: false
          name: Santa-Sheep
          type: user
        html: '<p>I only want the model to generate a few words however it seems to
          want to generate more, I can''t figure out a way to set max_tokens in the
          model<br>Here is my prompt: f"Is {x} associated with an event, concept,
          or topic that occurred or was relevant after September 2021? Answer with
          ''yes'', ''no'', or ''unsure''".<br>Here is my code:<br>from transformers
          import pipeline<br>generate_text = pipeline(model="databricks/dolly-v2-3b",
          torch_dtype=torch.bfloat16, trust_remote_code=True, device_map="auto")<br>res
          = generate_text(prompt[0]["generated_text"]</p>

          '
        raw: "I only want the model to generate a few words however it seems to want\
          \ to generate more, I can't figure out a way to set max_tokens in the model\r\
          \nHere is my prompt: f\"Is {x} associated with an event, concept, or topic\
          \ that occurred or was relevant after September 2021? Answer with 'yes',\
          \ 'no', or 'unsure'\".\r\nHere is my code:\r\nfrom transformers import pipeline\r\
          \ngenerate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\r\nres = generate_text(prompt[0][\"\
          generated_text\"]"
        updatedAt: '2023-06-28T23:28:54.815Z'
      numEdits: 0
      reactions: []
    id: 649cc2360c65a5d9697c6d12
    type: comment
  author: Santa-Sheep
  content: "I only want the model to generate a few words however it seems to want\
    \ to generate more, I can't figure out a way to set max_tokens in the model\r\n\
    Here is my prompt: f\"Is {x} associated with an event, concept, or topic that\
    \ occurred or was relevant after September 2021? Answer with 'yes', 'no', or 'unsure'\"\
    .\r\nHere is my code:\r\nfrom transformers import pipeline\r\ngenerate_text =\
    \ pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True,\
    \ device_map=\"auto\")\r\nres = generate_text(prompt[0][\"generated_text\"]"
  created_at: 2023-06-28 22:28:54+00:00
  edited: false
  hidden: false
  id: 649cc2360c65a5d9697c6d12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-29T02:54:02.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8486312627792358
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Just set max_new_tokens to something small. You can set this when
          you load the pipeline or when you call it. </p>

          '
        raw: 'Just set max_new_tokens to something small. You can set this when you
          load the pipeline or when you call it. '
        updatedAt: '2023-06-29T02:54:02.767Z'
      numEdits: 0
      reactions: []
    id: 649cf24a1b14867c42f6f578
    type: comment
  author: srowen
  content: 'Just set max_new_tokens to something small. You can set this when you
    load the pipeline or when you call it. '
  created_at: 2023-06-29 01:54:02+00:00
  edited: false
  hidden: false
  id: 649cf24a1b14867c42f6f578
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-07-15T16:03:10.000Z'
    data:
      status: closed
    id: 64b2c33ea248169796eb383c
    type: status-change
  author: srowen
  created_at: 2023-07-15 15:03:10+00:00
  id: 64b2c33ea248169796eb383c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: databricks/dolly-v2-3b
repo_type: model
status: closed
target_branch: null
title: How do I set the max tokens
