!!python/object:huggingface_hub.community.DiscussionWithDetails
author: opyate
conflicting_files: null
created_at: 2023-05-16 10:08:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
      fullname: Juan Uys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: opyate
      type: user
    createdAt: '2023-05-16T11:08:55.000Z'
    data:
      edited: true
      editors:
      - opyate
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
          fullname: Juan Uys
          isHf: false
          isPro: false
          name: opyate
          type: user
        html: "<p>I tried <a rel=\"nofollow\" href=\"https://github.com/kw2828/Dolly-2.0-Series/blob/main/fine_tuning_dolly_v2_lora_alpaca.ipynb\"\
          >this example</a>:</p>\n<pre><code class=\"language-python\">    model =\
          \ prepare_model_for_int8_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\
          \n    <span class=\"hljs-comment\"># The dimension used by the LoRA update\
          \ matrices</span>\n    LORA_R = <span class=\"hljs-number\">4</span>\n \
          \   <span class=\"hljs-comment\"># Scaling factor</span>\n    LORA_ALPHA\
          \ = <span class=\"hljs-number\">16</span>\n    LORA_DROPOUT = <span class=\"\
          hljs-number\">0.05</span>\n\n    <span class=\"hljs-comment\"># r and alpha\
          \ together control the total number of final trainable parameters when using\
          \ LoRA, giving you the flexibility to balance a trade-off between end performance\
          \ and compute efficiency.</span>\n    config = LoraConfig(\n        r=LORA_R,\n\
          \        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n  \
          \      bias=<span class=\"hljs-string\">\"none\"</span>,  <span class=\"\
          hljs-comment\"># Specifies if the bias parameters should be trained</span>\n\
          \        task_type=<span class=\"hljs-string\">\"CAUSAL_LM\"</span>,\n \
          \   )\n    model = get_peft_model(model, config)\n</code></pre>\n<p>I train,\
          \ and push to hub successfully.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/aHvzwFPW1x0SGkPQJ9GST.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/aHvzwFPW1x0SGkPQJ9GST.png\"\
          ></a></p>\n<p>But, when I try to use the adapter with the base model, I\
          \ get an error:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> peft <span class=\"hljs-keyword\">import</span>\
          \ PeftConfig\nconfig = PeftConfig.from_pretrained(repo_name)\n</code></pre>\n\
          <blockquote>\n<p>Out[19]: PeftConfig(peft_type='LORA', base_model_name_or_path='databricks/dolly-v2-3b',\
          \ task_type='CAUSAL_LM', inference_mode=True)</p>\n</blockquote>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n<span\
          \ class=\"hljs-keyword\">from</span> peft <span class=\"hljs-keyword\">import</span>\
          \ PeftModel\n<span class=\"hljs-keyword\">import</span> torch\n\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n\
          \    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=<span class=\"hljs-literal\">True</span>,\n)\n<span\
          \ class=\"hljs-comment\"># Load the LoRA model</span>\ninference_model =\
          \ PeftModel.from_pretrained(model, repo_name)  <span class=\"hljs-comment\"\
          ># &lt;-- error here</span>\ninference_model.<span class=\"hljs-built_in\"\
          >eval</span>()\ninference_model\n</code></pre>\n<p>Error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          File &lt;command-3660940350576262&gt;:12\n      5 model = AutoModelForCausalLM.from_pretrained(\n\
          \      6     config.base_model_name_or_path,\n      7     device_map=\"\
          auto\",\n      8     torch_dtype=torch.bfloat16,\n      9     trust_remote_code=True,\n\
          \     10 )\n     11 # Load the LoRA model\n---&gt; 12 inference_model =\
          \ PeftModel.from_pretrained(model, repo_name)\n     13 inference_model.eval()\n\
          \     14 inference_model\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:181,\
          \ in PeftModel.from_pretrained(cls, model, model_id, adapter_name, is_trainable,\
          \ **kwargs)\n    179 else:\n    180     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model,\
          \ config, adapter_name)\n--&gt; 181 model.load_adapter(model_id, adapter_name,\
          \ **kwargs)\n    182 return model\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:384,\
          \ in PeftModel.load_adapter(self, model_id, adapter_name, is_trainable,\
          \ **kwargs)\n    380 adapters_weights = torch.load(\n    381     filename,\
          \ map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"\
          cpu\")\n    382 )\n    383 # load the weights into the model\n--&gt; 384\
          \ set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)\n\
          \    385 if (\n    386     (getattr(self, \"hf_device_map\", None) is not\
          \ None)\n    387     and (len(set(self.hf_device_map.values()).intersection({\"\
          cpu\", \"disk\"})) &gt; 0)\n    388     and len(self.peft_config) == 1\n\
          \    389 ):\n    390     device_map = kwargs.get(\"device_map\", \"auto\"\
          )\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/utils/save_and_load.py:123,\
          \ in set_peft_model_state_dict(model, peft_model_state_dict, adapter_name)\n\
          \    120 else:\n    121     raise NotImplementedError\n--&gt; 123 model.load_state_dict(peft_model_state_dict,\
          \ strict=False)\n    124 if isinstance(config, PromptLearningConfig):\n\
          \    125     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n\
          \    126         {\"weight\": peft_model_state_dict[\"prompt_embeddings\"\
          ]}, strict=True\n    127     )\n\nFile /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1671,\
          \ in Module.load_state_dict(self, state_dict, strict)\n   1666         error_msgs.insert(\n\
          \   1667             0, 'Missing key(s) in state_dict: {}. '.format(\n \
          \  1668                 ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n\
          \   1670 if len(error_msgs) &gt; 0:\n-&gt; 1671     raise RuntimeError('Error(s)\
          \ in loading state_dict for {}:\\n\\t{}'.format(\n   1672              \
          \          self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n   1673\
          \ return _IncompatibleKeys(missing_keys, unexpected_keys)\n</code></pre>\n\
          <p>And then this is also printed out for layers 0 to 31.</p>\n<blockquote>\n\
          <p>RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\
          \ size mismatch for base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight:\
          \ copying a param with shape torch.Size([0]) from checkpoint, the shape\
          \ in current model is torch.Size([7680, 4]).</p>\n</blockquote>\n<p>Not\
          \ sure if this is due to a misconfigured <code>LoraConfig</code>,so any\
          \ pointers will be appreciated.</p>\n"
        raw: "I tried [this example](https://github.com/kw2828/Dolly-2.0-Series/blob/main/fine_tuning_dolly_v2_lora_alpaca.ipynb):\n\
          \n```python\n    model = prepare_model_for_int8_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\
          \n    # The dimension used by the LoRA update matrices\n    LORA_R = 4\n\
          \    # Scaling factor\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.05\n\n\
          \    # r and alpha together control the total number of final trainable\
          \ parameters when using LoRA, giving you the flexibility to balance a trade-off\
          \ between end performance and compute efficiency.\n    config = LoraConfig(\n\
          \        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n\
          \        bias=\"none\",  # Specifies if the bias parameters should be trained\n\
          \        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model,\
          \ config)\n```\n\nI train, and push to hub successfully.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/aHvzwFPW1x0SGkPQJ9GST.png)\n\
          \nBut, when I try to use the adapter with the base model, I get an error:\n\
          \n```python\nfrom peft import PeftConfig\nconfig = PeftConfig.from_pretrained(repo_name)\n\
          ```\n> Out[19]: PeftConfig(peft_type='LORA', base_model_name_or_path='databricks/dolly-v2-3b',\
          \ task_type='CAUSAL_LM', inference_mode=True)\n\n```python\nfrom transformers\
          \ import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n\
          \    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          )\n# Load the LoRA model\ninference_model = PeftModel.from_pretrained(model,\
          \ repo_name)  # <-- error here\ninference_model.eval()\ninference_model\n\
          ```\n\nError:\n\n```\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          File <command-3660940350576262>:12\n      5 model = AutoModelForCausalLM.from_pretrained(\n\
          \      6     config.base_model_name_or_path,\n      7     device_map=\"\
          auto\",\n      8     torch_dtype=torch.bfloat16,\n      9     trust_remote_code=True,\n\
          \     10 )\n     11 # Load the LoRA model\n---> 12 inference_model = PeftModel.from_pretrained(model,\
          \ repo_name)\n     13 inference_model.eval()\n     14 inference_model\n\n\
          File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:181,\
          \ in PeftModel.from_pretrained(cls, model, model_id, adapter_name, is_trainable,\
          \ **kwargs)\n    179 else:\n    180     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model,\
          \ config, adapter_name)\n--> 181 model.load_adapter(model_id, adapter_name,\
          \ **kwargs)\n    182 return model\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:384,\
          \ in PeftModel.load_adapter(self, model_id, adapter_name, is_trainable,\
          \ **kwargs)\n    380 adapters_weights = torch.load(\n    381     filename,\
          \ map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"\
          cpu\")\n    382 )\n    383 # load the weights into the model\n--> 384 set_peft_model_state_dict(self,\
          \ adapters_weights, adapter_name=adapter_name)\n    385 if (\n    386  \
          \   (getattr(self, \"hf_device_map\", None) is not None)\n    387     and\
          \ (len(set(self.hf_device_map.values()).intersection({\"cpu\", \"disk\"\
          })) > 0)\n    388     and len(self.peft_config) == 1\n    389 ):\n    390\
          \     device_map = kwargs.get(\"device_map\", \"auto\")\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/utils/save_and_load.py:123,\
          \ in set_peft_model_state_dict(model, peft_model_state_dict, adapter_name)\n\
          \    120 else:\n    121     raise NotImplementedError\n--> 123 model.load_state_dict(peft_model_state_dict,\
          \ strict=False)\n    124 if isinstance(config, PromptLearningConfig):\n\
          \    125     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n\
          \    126         {\"weight\": peft_model_state_dict[\"prompt_embeddings\"\
          ]}, strict=True\n    127     )\n\nFile /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1671,\
          \ in Module.load_state_dict(self, state_dict, strict)\n   1666         error_msgs.insert(\n\
          \   1667             0, 'Missing key(s) in state_dict: {}. '.format(\n \
          \  1668                 ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n\
          \   1670 if len(error_msgs) > 0:\n-> 1671     raise RuntimeError('Error(s)\
          \ in loading state_dict for {}:\\n\\t{}'.format(\n   1672              \
          \          self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n   1673\
          \ return _IncompatibleKeys(missing_keys, unexpected_keys)\n```\n\nAnd then\
          \ this is also printed out for layers 0 to 31.\n\n> RuntimeError: Error(s)\
          \ in loading state_dict for PeftModelForCausalLM: size mismatch for base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight:\
          \ copying a param with shape torch.Size([0]) from checkpoint, the shape\
          \ in current model is torch.Size([7680, 4]).\n\nNot sure if this is due\
          \ to a misconfigured `LoraConfig`,so any pointers will be appreciated."
        updatedAt: '2023-05-16T14:18:10.365Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - christangttt
    id: 646364471bb15fa38b0bd296
    type: comment
  author: opyate
  content: "I tried [this example](https://github.com/kw2828/Dolly-2.0-Series/blob/main/fine_tuning_dolly_v2_lora_alpaca.ipynb):\n\
    \n```python\n    model = prepare_model_for_int8_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\
    \n    # The dimension used by the LoRA update matrices\n    LORA_R = 4\n    #\
    \ Scaling factor\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.05\n\n    # r and\
    \ alpha together control the total number of final trainable parameters when using\
    \ LoRA, giving you the flexibility to balance a trade-off between end performance\
    \ and compute efficiency.\n    config = LoraConfig(\n        r=LORA_R,\n     \
    \   lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        bias=\"\
    none\",  # Specifies if the bias parameters should be trained\n        task_type=\"\
    CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n```\n\nI train,\
    \ and push to hub successfully.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/aHvzwFPW1x0SGkPQJ9GST.png)\n\
    \nBut, when I try to use the adapter with the base model, I get an error:\n\n\
    ```python\nfrom peft import PeftConfig\nconfig = PeftConfig.from_pretrained(repo_name)\n\
    ```\n> Out[19]: PeftConfig(peft_type='LORA', base_model_name_or_path='databricks/dolly-v2-3b',\
    \ task_type='CAUSAL_LM', inference_mode=True)\n\n```python\nfrom transformers\
    \ import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n\
    \    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    )\n# Load the LoRA model\ninference_model = PeftModel.from_pretrained(model, repo_name)\
    \  # <-- error here\ninference_model.eval()\ninference_model\n```\n\nError:\n\n\
    ```\n---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    File <command-3660940350576262>:12\n      5 model = AutoModelForCausalLM.from_pretrained(\n\
    \      6     config.base_model_name_or_path,\n      7     device_map=\"auto\"\
    ,\n      8     torch_dtype=torch.bfloat16,\n      9     trust_remote_code=True,\n\
    \     10 )\n     11 # Load the LoRA model\n---> 12 inference_model = PeftModel.from_pretrained(model,\
    \ repo_name)\n     13 inference_model.eval()\n     14 inference_model\n\nFile\
    \ /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:181,\
    \ in PeftModel.from_pretrained(cls, model, model_id, adapter_name, is_trainable,\
    \ **kwargs)\n    179 else:\n    180     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model,\
    \ config, adapter_name)\n--> 181 model.load_adapter(model_id, adapter_name, **kwargs)\n\
    \    182 return model\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/peft_model.py:384,\
    \ in PeftModel.load_adapter(self, model_id, adapter_name, is_trainable, **kwargs)\n\
    \    380 adapters_weights = torch.load(\n    381     filename, map_location=torch.device(\"\
    cuda\" if torch.cuda.is_available() else \"cpu\")\n    382 )\n    383 # load the\
    \ weights into the model\n--> 384 set_peft_model_state_dict(self, adapters_weights,\
    \ adapter_name=adapter_name)\n    385 if (\n    386     (getattr(self, \"hf_device_map\"\
    , None) is not None)\n    387     and (len(set(self.hf_device_map.values()).intersection({\"\
    cpu\", \"disk\"})) > 0)\n    388     and len(self.peft_config) == 1\n    389 ):\n\
    \    390     device_map = kwargs.get(\"device_map\", \"auto\")\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/peft/utils/save_and_load.py:123,\
    \ in set_peft_model_state_dict(model, peft_model_state_dict, adapter_name)\n \
    \   120 else:\n    121     raise NotImplementedError\n--> 123 model.load_state_dict(peft_model_state_dict,\
    \ strict=False)\n    124 if isinstance(config, PromptLearningConfig):\n    125\
    \     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n    126 \
    \        {\"weight\": peft_model_state_dict[\"prompt_embeddings\"]}, strict=True\n\
    \    127     )\n\nFile /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1671,\
    \ in Module.load_state_dict(self, state_dict, strict)\n   1666         error_msgs.insert(\n\
    \   1667             0, 'Missing key(s) in state_dict: {}. '.format(\n   1668\
    \                 ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n   1670\
    \ if len(error_msgs) > 0:\n-> 1671     raise RuntimeError('Error(s) in loading\
    \ state_dict for {}:\\n\\t{}'.format(\n   1672                        self.__class__.__name__,\
    \ \"\\n\\t\".join(error_msgs)))\n   1673 return _IncompatibleKeys(missing_keys,\
    \ unexpected_keys)\n```\n\nAnd then this is also printed out for layers 0 to 31.\n\
    \n> RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM: size\
    \ mismatch for base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight:\
    \ copying a param with shape torch.Size([0]) from checkpoint, the shape in current\
    \ model is torch.Size([7680, 4]).\n\nNot sure if this is due to a misconfigured\
    \ `LoraConfig`,so any pointers will be appreciated."
  created_at: 2023-05-16 10:08:55+00:00
  edited: true
  hidden: false
  id: 646364471bb15fa38b0bd296
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
      fullname: Juan Uys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: opyate
      type: user
    createdAt: '2023-05-16T14:34:16.000Z'
    data:
      edited: true
      editors:
      - opyate
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f1f7502ce39336c3faf40/GNFO3mU9Sn3OTAxVHWrsh.jpeg?w=200&h=200&f=face
          fullname: Juan Uys
          isHf: false
          isPro: false
          name: opyate
          type: user
        html: "<p>There's no <code>modules_to_save</code> or <code>target_modules</code>\
          \ in the referenced example, so I'm wondering if this has since become a\
          \ requirement to get Dolly to be PEFTuned with LoRA. (presuming the referenced\
          \ author got it working)</p>\n<p>Here's the architecture:</p>\n<pre><code>GPTNeoXForCausalLM(\n\
          \  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50280, 2560)\n \
          \   (layers): ModuleList(\n      (0): GPTNeoXLayer(\n        (input_layernorm):\
          \ LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm):\
          \ LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (attention):\
          \ GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n       \
          \   (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n\
          \          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n\
          \        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=2560,\
          \ out_features=10240, bias=True)\n          (dense_4h_to_h): Linear(in_features=10240,\
          \ out_features=2560, bias=True)\n          (act): GELUActivation()\n   \
          \     )\n      )\n      // GPTNeoXLayer repeated another 31 times\n    )\n\
          \    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n\
          \  )\n  (embed_out): Linear(in_features=2560, out_features=50280, bias=False)\n\
          )\n</code></pre>\n<p>So, I'm wondering if (some/all of) those variables\
          \ need to be <code>[\"layers\"]</code> or <code>[\"embed_out\"]</code> or\
          \ both, even.</p>\n<p>(I'll try these, and report back.)</p>\n"
        raw: "There's no `modules_to_save` or `target_modules` in the referenced example,\
          \ so I'm wondering if this has since become a requirement to get Dolly to\
          \ be PEFTuned with LoRA. (presuming the referenced author got it working)\n\
          \nHere's the architecture:\n\n```\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n\
          \    (embed_in): Embedding(50280, 2560)\n    (layers): ModuleList(\n   \
          \   (0): GPTNeoXLayer(\n        (input_layernorm): LayerNorm((2560,), eps=1e-05,\
          \ elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((2560,),\
          \ eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n\
          \          (rotary_emb): RotaryEmbedding()\n          (query_key_value):\
          \ Linear(in_features=2560, out_features=7680, bias=True)\n          (dense):\
          \ Linear(in_features=2560, out_features=2560, bias=True)\n        )\n  \
          \      (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=2560,\
          \ out_features=10240, bias=True)\n          (dense_4h_to_h): Linear(in_features=10240,\
          \ out_features=2560, bias=True)\n          (act): GELUActivation()\n   \
          \     )\n      )\n      // GPTNeoXLayer repeated another 31 times\n    )\n\
          \    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n\
          \  )\n  (embed_out): Linear(in_features=2560, out_features=50280, bias=False)\n\
          )\n```\n\nSo, I'm wondering if (some/all of) those variables need to be\
          \ `[\"layers\"]` or `[\"embed_out\"]` or both, even.\n\n(I'll try these,\
          \ and report back.)"
        updatedAt: '2023-05-16T14:40:17.434Z'
      numEdits: 1
      reactions: []
    id: 6463946895e64061c97d5259
    type: comment
  author: opyate
  content: "There's no `modules_to_save` or `target_modules` in the referenced example,\
    \ so I'm wondering if this has since become a requirement to get Dolly to be PEFTuned\
    \ with LoRA. (presuming the referenced author got it working)\n\nHere's the architecture:\n\
    \n```\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50280,\
    \ 2560)\n    (layers): ModuleList(\n      (0): GPTNeoXLayer(\n        (input_layernorm):\
    \ LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm):\
    \ LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (attention):\
    \ GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value):\
    \ Linear(in_features=2560, out_features=7680, bias=True)\n          (dense): Linear(in_features=2560,\
    \ out_features=2560, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n     \
    \     (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n\
    \          (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n\
    \          (act): GELUActivation()\n        )\n      )\n      // GPTNeoXLayer\
    \ repeated another 31 times\n    )\n    (final_layer_norm): LayerNorm((2560,),\
    \ eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=2560,\
    \ out_features=50280, bias=False)\n)\n```\n\nSo, I'm wondering if (some/all of)\
    \ those variables need to be `[\"layers\"]` or `[\"embed_out\"]` or both, even.\n\
    \n(I'll try these, and report back.)"
  created_at: 2023-05-16 13:34:16+00:00
  edited: true
  hidden: false
  id: 6463946895e64061c97d5259
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/UEeLkeMbtcoMfm72geVGF.jpeg?w=200&h=200&f=face
      fullname: Juan Uys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juan-cytora
      type: user
    createdAt: '2023-05-17T15:48:50.000Z'
    data:
      edited: false
      editors:
      - juan-cytora
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/UEeLkeMbtcoMfm72geVGF.jpeg?w=200&h=200&f=face
          fullname: Juan Uys
          isHf: false
          isPro: false
          name: juan-cytora
          type: user
        html: '<p>I cross-posted to Github: <a rel="nofollow" href="https://github.com/huggingface/peft/issues/460">https://github.com/huggingface/peft/issues/460</a></p>

          '
        raw: 'I cross-posted to Github: https://github.com/huggingface/peft/issues/460'
        updatedAt: '2023-05-17T15:48:50.530Z'
      numEdits: 0
      reactions: []
    id: 6464f762a0748f9aa4c496cd
    type: comment
  author: juan-cytora
  content: 'I cross-posted to Github: https://github.com/huggingface/peft/issues/460'
  created_at: 2023-05-17 14:48:50+00:00
  edited: false
  hidden: false
  id: 6464f762a0748f9aa4c496cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-31T12:05:49.000Z'
    data:
      status: closed
    id: 6477381d906bb0203e52bf43
    type: status-change
  author: srowen
  created_at: 2023-05-31 11:05:49+00:00
  id: 6477381d906bb0203e52bf43
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-06-15T12:06:37.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7321787476539612
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>For future readers, there are two things to know about using LoRA:</p>

          <p>You can''t use DeepSpeed ZeRO 3; set ZeRO level 2 or else the adapter.bin
          file gets truncated.<br>target_modules for a GPT-NeoX architecture should
          be <code>["query_key_value"]</code></p>

          '
        raw: 'For future readers, there are two things to know about using LoRA:


          You can''t use DeepSpeed ZeRO 3; set ZeRO level 2 or else the adapter.bin
          file gets truncated.

          target_modules for a GPT-NeoX architecture should be `["query_key_value"]`'
        updatedAt: '2023-06-15T12:06:37.708Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - juan-cytora
        - FuFuFU31231
        - chrisagblake
        - christangttt
    id: 648afecd1810b9e1cddcf677
    type: comment
  author: srowen
  content: 'For future readers, there are two things to know about using LoRA:


    You can''t use DeepSpeed ZeRO 3; set ZeRO level 2 or else the adapter.bin file
    gets truncated.

    target_modules for a GPT-NeoX architecture should be `["query_key_value"]`'
  created_at: 2023-06-15 11:06:37+00:00
  edited: false
  hidden: false
  id: 648afecd1810b9e1cddcf677
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: databricks/dolly-v2-3b
repo_type: model
status: closed
target_branch: null
title: PEFT/LoRA error when merging base and adapter
