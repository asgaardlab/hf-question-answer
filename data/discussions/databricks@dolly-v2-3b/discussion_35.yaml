!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ivgome
conflicting_files: null
created_at: 2023-07-07 12:58:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34b6b3e53639b812c4768df0a35c54db.svg
      fullname: ivan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivgome
      type: user
    createdAt: '2023-07-07T13:58:16.000Z'
    data:
      edited: false
      editors:
      - ivgome
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9167909026145935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34b6b3e53639b812c4768df0a35c54db.svg
          fullname: ivan
          isHf: false
          isPro: false
          name: ivgome
          type: user
        html: '<p>We have managed to launch the training script by providing our own
          dataset, following this guide.<br>However, we can launch the model in chatbot
          format before the training, but we are unable to launch it once it has been
          trained, as the ram consumption skyrockets, can we modify any parameter
          at configuration level to solve this problem?<br>We are currently following
          these steps, in colab free.<br><a rel="nofollow" href="https://colab.research.google.com/drive/1n5U13L0Bzhs32QO_bls5jwuZR62GPSwE?usp=sharing#scrollTo=zlw7IxfUED0a">https://colab.research.google.com/drive/1n5U13L0Bzhs32QO_bls5jwuZR62GPSwE?usp=sharing#scrollTo=zlw7IxfUED0a</a></p>

          '
        raw: "We have managed to launch the training script by providing our own dataset,\
          \ following this guide.\r\nHowever, we can launch the model in chatbot format\
          \ before the training, but we are unable to launch it once it has been trained,\
          \ as the ram consumption skyrockets, can we modify any parameter at configuration\
          \ level to solve this problem?\r\nWe are currently following these steps,\
          \ in colab free.\r\nhttps://colab.research.google.com/drive/1n5U13L0Bzhs32QO_bls5jwuZR62GPSwE?usp=sharing#scrollTo=zlw7IxfUED0a"
        updatedAt: '2023-07-07T13:58:16.328Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ivgome
    id: 64a819f8aeacff2ec4ac7a17
    type: comment
  author: ivgome
  content: "We have managed to launch the training script by providing our own dataset,\
    \ following this guide.\r\nHowever, we can launch the model in chatbot format\
    \ before the training, but we are unable to launch it once it has been trained,\
    \ as the ram consumption skyrockets, can we modify any parameter at configuration\
    \ level to solve this problem?\r\nWe are currently following these steps, in colab\
    \ free.\r\nhttps://colab.research.google.com/drive/1n5U13L0Bzhs32QO_bls5jwuZR62GPSwE?usp=sharing#scrollTo=zlw7IxfUED0a"
  created_at: 2023-07-07 12:58:16+00:00
  edited: false
  hidden: false
  id: 64a819f8aeacff2ec4ac7a17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-07-07T14:46:00.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9800543189048767
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>It''s not clear what you''re doing or on what hardware, but, just
          sounds like you don''t have enough mem to load it? you need to load on a
          GPU</p>

          '
        raw: It's not clear what you're doing or on what hardware, but, just sounds
          like you don't have enough mem to load it? you need to load on a GPU
        updatedAt: '2023-07-07T14:46:00.777Z'
      numEdits: 0
      reactions: []
    id: 64a82528b4e6769d1e9d2b2f
    type: comment
  author: srowen
  content: It's not clear what you're doing or on what hardware, but, just sounds
    like you don't have enough mem to load it? you need to load on a GPU
  created_at: 2023-07-07 13:46:00+00:00
  edited: false
  hidden: false
  id: 64a82528b4e6769d1e9d2b2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-07-15T16:03:27.000Z'
    data:
      status: closed
    id: 64b2c34ff44fd95749fc33f6
    type: status-change
  author: srowen
  created_at: 2023-07-15 15:03:27+00:00
  id: 64b2c34ff44fd95749fc33f6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca73a6dcffbee28380172b59894bcb8e.svg
      fullname: Stefan Goldener
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: renedlog
      type: user
    createdAt: '2023-07-17T10:50:38.000Z'
    data:
      edited: true
      editors:
      - renedlog
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5592586398124695
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca73a6dcffbee28380172b59894bcb8e.svg
          fullname: Stefan Goldener
          isHf: false
          isPro: false
          name: renedlog
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ivgome&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ivgome\">@<span class=\"\
          underline\">ivgome</span></a></span>\n\n\t</span></span> your code is running\
          \ well in colab free with few modifications - mostly due to memory limitations\
          \ (see copy and paste of your file with adjustments below which is running):</p>\n\
          <h1 id=\"---coding-utf-8---\">-<em>- coding: utf-8 -</em>-</h1>\n<p>\"\"\
          \"Fine-tuning Dolly 2.0 with LoRA and Alpaca.ipynb</p>\n<p>Automatically\
          \ generated by Colaboratory.</p>\n<p>Original file is located at<br>   \
          \ <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1-nyF2tdV7jOvxqR3OCw7Bv2IyZQpp_5D\"\
          >https://colab.research.google.com/drive/1-nyF2tdV7jOvxqR3OCw7Bv2IyZQpp_5D</a></p>\n\
          <h1 id=\"fine-tuning-dolly-20-with-lora\">Fine-tuning Dolly 2.0 with LoRA</h1>\n\
          <ul>\n<li>Dolly-v2-3b - <a href=\"https://huggingface.co/databricks/dolly-v2-3b\"\
          >https://huggingface.co/databricks/dolly-v2-3b</a></li>\n<li>LoRA paper\
          \ - <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2106.09685\">https://arxiv.org/abs/2106.09685</a></li>\n\
          <li>Alpaca Cleaned Dataset - <a rel=\"nofollow\" href=\"https://github.com/gururise/AlpacaDataCleaned\"\
          >https://github.com/gururise/AlpacaDataCleaned</a><br>\"\"\"</li>\n</ul>\n\
          <p>!git clone <a rel=\"nofollow\" href=\"https://github.com/gururise/AlpacaDataCleaned.git\"\
          >https://github.com/gururise/AlpacaDataCleaned.git</a></p>\n<p>ls AlpacaDataCleaned/</p>\n\
          <p>!pip install accelerate&gt;=0.21.0 transformers[torch]==4.30.2<br>!pip\
          \ install -q datasets loralib sentencepiece<br>!pip -q install git+<a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/peft.git\">https://github.com/huggingface/peft.git</a><br>!pip\
          \ -q install bitsandbytes</p>\n<h1 id=\"create-instruct-pipeline\">Create\
          \ Instruct Pipeline</h1>\n<p>import logging<br>import re</p>\n<p>import\
          \ numpy as np<br>from transformers import Pipeline, PreTrainedTokenizer</p>\n\
          <p>logger = logging.getLogger(<strong>name</strong>)</p>\n<p>INSTRUCTION_KEY\
          \ = \"### Instruction:\"<br>RESPONSE_KEY = \"### Response:\"<br>END_KEY\
          \ = \"### End\"<br>INTRO_BLURB = (<br>    \"Below is an instruction that\
          \ describes a task. Write a response that appropriately completes the request.\"\
          <br>)</p>\n<h1 id=\"this-is-the-prompt-that-is-used-for-generating-responses-using-an-already-trained-model--it-ends-with-the-response\"\
          >This is the prompt that is used for generating responses using an already\
          \ trained model.  It ends with the response</h1>\n<h1 id=\"key-where-the-job-of-the-model-is-to-provide-the-completion-that-follows-it-ie-the-response-itself\"\
          >key, where the job of the model is to provide the completion that follows\
          \ it (i.e. the response itself).</h1>\n<p>PROMPT_FOR_GENERATION_FORMAT =\
          \ \"\"\"{intro}<br>{instruction_key}<br>{instruction}<br>{response_key}<br>\"\
          \"\".format(<br>    intro=INTRO_BLURB,<br>    instruction_key=INSTRUCTION_KEY,<br>\
          \    instruction=\"{instruction}\",<br>    response_key=RESPONSE_KEY,<br>)</p>\n\
          <p>def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -&gt;\
          \ int:<br>    \"\"\"Gets the token ID for a given string that has been added\
          \ to the tokenizer as a special token.<br>    When training, we configure\
          \ the tokenizer so that the sequences like \"### Instruction:\" and \"###\
          \ End\" are<br>    treated specially and converted to a single, new token.\
          \  This retrieves the token ID each of these keys map to.<br>    Args:<br>\
          \        tokenizer (PreTrainedTokenizer): the tokenizer<br>        key (str):\
          \ the key to convert to a single token<br>    Raises:<br>        RuntimeError:\
          \ if more than one ID was generated<br>    Returns:<br>        int: the\
          \ token ID for the given key<br>    \"\"\"<br>    token_ids = tokenizer.encode(key)<br>\
          \    if len(token_ids) &gt; 1:<br>        raise ValueError(f\"Expected only\
          \ a single token for '{key}' but found {token_ids}\")<br>    return token_ids[0]</p>\n\
          <p>class InstructionTextGenerationPipeline(Pipeline):<br>    def <strong>init</strong>(<br>\
          \        self, *args, do_sample: bool = True, max_new_tokens: int = 256,\
          \ top_p: float = 0.92, top_k: int = 0, **kwargs<br>    ):<br>        super().<strong>init</strong>(*args,\
          \ do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k,\
          \ **kwargs)</p>\n<pre><code>def _sanitize_parameters(self, return_instruction_text=False,\
          \ **generate_kwargs):\n    preprocess_params = {}\n\n    # newer versions\
          \ of the tokenizer configure the response key as a special token.  newer\
          \ versions still may\n    # append a newline to yield a single token.  find\
          \ whatever token is configured for the response key.\n    tokenizer_response_key\
          \ = next(\n        (token for token in self.tokenizer.additional_special_tokens\
          \ if token.startswith(RESPONSE_KEY)), None\n    )\n\n    response_key_token_id\
          \ = None\n    end_key_token_id = None\n    if tokenizer_response_key:\n\
          \        try:\n            response_key_token_id = get_special_token_id(self.tokenizer,\
          \ tokenizer_response_key)\n            end_key_token_id = get_special_token_id(self.tokenizer,\
          \ END_KEY)\n\n            # Ensure generation stops once it generates \"\
          ### End\"\n            generate_kwargs[\"eos_token_id\"] = end_key_token_id\n\
          \        except ValueError:\n            pass\n\n    forward_params = generate_kwargs\n\
          \    postprocess_params = {\n        \"response_key_token_id\": response_key_token_id,\n\
          \        \"end_key_token_id\": end_key_token_id,\n        \"return_instruction_text\"\
          : return_instruction_text,\n    }\n\n    return preprocess_params, forward_params,\
          \ postprocess_params\n\ndef preprocess(self, instruction_text, **generate_kwargs):\n\
          \    prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n\
          \    inputs = self.tokenizer(\n        prompt_text,\n        return_tensors=\"\
          pt\",\n    )\n    inputs[\"prompt_text\"] = prompt_text\n    inputs[\"instruction_text\"\
          ] = instruction_text\n    return inputs\n\ndef _forward(self, model_inputs,\
          \ **generate_kwargs):\n    input_ids = model_inputs[\"input_ids\"]\n   \
          \ attention_mask = model_inputs.get(\"attention_mask\", None)\n    generated_sequence\
          \ = self.model.generate(\n        input_ids=input_ids.to(self.model.device),\n\
          \        attention_mask=attention_mask,\n        pad_token_id=self.tokenizer.pad_token_id,\n\
          \        **generate_kwargs,\n    )[0].cpu()\n    instruction_text = model_inputs.pop(\"\
          instruction_text\")\n    return {\"generated_sequence\": generated_sequence,\
          \ \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n\n\
          def postprocess(self, model_outputs, response_key_token_id, end_key_token_id,\
          \ return_instruction_text):\n    sequence = model_outputs[\"generated_sequence\"\
          ]\n    instruction_text = model_outputs[\"instruction_text\"]\n\n    # The\
          \ response will be set to this variable if we can identify it.\n    decoded\
          \ = None\n\n    # If we have token IDs for the response and end, then we\
          \ can find the tokens and only decode between them.\n    if response_key_token_id\
          \ and end_key_token_id:\n        # Find where \"### Response:\" is first\
          \ found in the generated tokens.  Considering this is part of the\n    \
          \    # prompt, we should definitely find it.  We will return the tokens\
          \ found after this token.\n        response_pos = None\n        response_positions\
          \ = np.where(sequence == response_key_token_id)[0]\n        if len(response_positions)\
          \ == 0:\n            logger.warn(f\"Could not find response key {response_key_token_id}\
          \ in: {sequence}\")\n        else:\n            response_pos = response_positions[0]\n\
          \n        if response_pos:\n            # Next find where \"### End\" is\
          \ located.  The model has been trained to end its responses with this\n\
          \            # sequence (or actually, the token ID it maps to, since it\
          \ is a special token).  We may not find\n            # this token, as the\
          \ response could be truncated.  If we don't find it then just return everything\n\
          \            # to the end.  Note that even though we set eos_token_id, we\
          \ still see the this token at the end.\n            end_pos = None\n   \
          \         end_positions = np.where(sequence == end_key_token_id)[0]\n  \
          \          if len(end_positions) &gt; 0:\n                end_pos = end_positions[0]\n\
          \n            decoded = self.tokenizer.decode(sequence[response_pos + 1\
          \ : end_pos]).strip()\n    else:\n        # Otherwise we'll decode everything\
          \ and use a regex to find the response and end.\n\n        fully_decoded\
          \ = self.tokenizer.decode(sequence)\n\n        # The response appears after\
          \ \"### Response:\".  The model has been trained to append \"### End\" at\
          \ the\n        # end.\n        m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\\
          s*End\", fully_decoded, flags=re.DOTALL)\n\n        if m:\n            decoded\
          \ = m.group(1).strip()\n        else:\n            # The model might not\
          \ generate the \"### End\" sequence before reaching the max tokens.  In\
          \ this case,\n            # return everything after \"### Response:\".\n\
          \            m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded,\
          \ flags=re.DOTALL)\n            if m:\n                decoded = m.group(1).strip()\n\
          \            else:\n                logger.warn(f\"Failed to find response\
          \ in:\\n{fully_decoded}\")\n\n    if return_instruction_text:\n        return\
          \ {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n\
          \n    return decoded\n</code></pre>\n<p>import torch<br>from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"\
          left\")</p>\n<p>quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)</p>\n\
          <p>model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\"\
          ,<br>                                             device_map=\"auto\",<br>\
          \                                             torch_dtype=torch.bfloat16,<br>\
          \                                             #torch_dtype=torch.int8,<br>\
          \                                             quantization_config=quantization_config,<br>\
          \                                             load_in_4bit=True,<br>   \
          \                                          #load_in_8bit=True,<br>     \
          \                                        )</p>\n<h1 id=\"generate_text--instructiontextgenerationpipelinemodelmodel-tokenizertokenizer\"\
          >generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)</h1>\n\
          <p>from datasets import load_dataset</p>\n<p>data = load_dataset(\"json\"\
          ,<br>                    data_files=\"./AlpacaDataCleaned/alpaca_data.json\"\
          )</p>\n<p>def generate_prompt(data_point):<br>    # taken from <a rel=\"\
          nofollow\" href=\"https://github.com/tloen/alpaca-lora\">https://github.com/tloen/alpaca-lora</a><br>\
          \    if data_point[\"instruction\"]:<br>        return f\"\"\"Below is an\
          \ instruction that describes a task, paired with an input that provides\
          \ further context. Write a response that appropriately completes the request.</p>\n\
          <h3 id=\"instruction\">Instruction:</h3>\n<p>{data_point[\"instruction\"\
          ]}</p>\n<h3 id=\"input\">Input:</h3>\n<p>{data_point[\"input\"]}</p>\n<h3\
          \ id=\"response\">Response:</h3>\n<p>{data_point[\"output\"]}\"\"\"<br>\
          \    else:<br>        return f\"\"\"Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.</p>\n\
          <h3 id=\"instruction-1\">Instruction:</h3>\n<p>{data_point[\"instruction\"\
          ]}</p>\n<h3 id=\"response-1\">Response:</h3>\n<p>{data_point[\"output\"\
          ]}\"\"\"</p>\n<p>data = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})</p>\n\
          <p>data</p>\n<p>\"\"\"## Finetuning Dolly\"\"\"</p>\n<p>import os</p>\n\
          <h1 id=\"osenvironcuda_visible_devices--0\">os.environ[\"CUDA_VISIBLE_DEVICES\"\
          ] = \"0\"</h1>\n<p>import torch<br>import torch.nn as nn<br>import bitsandbytes\
          \ as bnb<br>from datasets import load_dataset<br>import transformers<br>from\
          \ transformers import AutoTokenizer, AutoModel, AutoConfig, GPTJForCausalLM</p>\n\
          <p>from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model</p>\n\
          <h1 id=\"settings-for-a100---for-3090\">Settings for A100 - For 3090</h1>\n\
          <p>MICRO_BATCH_SIZE = 4  # change to 4 for 3090<br>BATCH_SIZE = 128<br>GRADIENT_ACCUMULATION_STEPS\
          \ = BATCH_SIZE // MICRO_BATCH_SIZE<br>EPOCHS = 2  # paper uses 3<br>LEARNING_RATE\
          \ = 2e-5<br>CUTOFF_LEN = 256<br>LORA_R = 4<br>LORA_ALPHA = 16<br>LORA_DROPOUT\
          \ = 0.05</p>\n<h1 id=\"settings-for-a100---for-3090-1\">Settings for A100\
          \ - For 3090</h1>\n<p>MICRO_BATCH_SIZE = 4  # change to 4 for 3090<br>BATCH_SIZE\
          \ = 32<br>GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE<br>EPOCHS\
          \ = 2  # paper uses 3<br>LEARNING_RATE = 2e-5<br>CUTOFF_LEN = 32<br>LORA_R\
          \ = 4<br>LORA_ALPHA = 16<br>LORA_DROPOUT = 0.05</p>\n<p>#model = prepare_model_for_int8_training(model,\
          \ use_gradient_checkpointing=True)</p>\n<p>config = LoraConfig(<br>    r=LORA_R,<br>\
          \    lora_alpha=LORA_ALPHA,<br>    lora_dropout=LORA_DROPOUT,<br>    bias=\"\
          none\",<br>    task_type=\"CAUSAL_LM\",<br>)<br>model = get_peft_model(model,\
          \ config)<br>tokenizer.pad_token_id = 0  # unk. we want this to be different\
          \ from the eos token</p>\n<p>data = load_dataset(\"json\", data_files=\"\
          ./AlpacaDataCleaned/alpaca_data_cleaned.json\")</p>\n<p>data = data.shuffle().map(<br>\
          \    lambda data_point: tokenizer(<br>        generate_prompt(data_point),<br>\
          \        truncation=True,<br>        max_length=CUTOFF_LEN,<br>        padding=\"\
          max_length\",<br>    )<br>)</p>\n<p>data</p>\n<p>from transformers.training_args\
          \ import ParallelMode</p>\n<p>trainer = transformers.Trainer(<br>    model=model,<br>\
          \    train_dataset=data[\"train\"],<br>    args=transformers.TrainingArguments(<br>\
          \        per_device_train_batch_size=MICRO_BATCH_SIZE,<br>        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,<br>\
          \        warmup_steps=100,<br>        num_train_epochs=EPOCHS,<br>     \
          \   learning_rate=LEARNING_RATE,<br>        fp16=True,<br>        #sharded_ddp=\"\
          zero_dp_3 auto_wrap\",<br>       # fsdp=\"full_shard auto_wrap\",<br>  \
          \      # model_parallel=True,<br>        #parallel_mode=ParallelMode.DISTRIBUTED,<br>\
          \        #is_model_parallel=True,<br>        logging_steps=1,<br>      \
          \  output_dir=\"lora-dolly\",<br>        save_total_limit=3,<br>    ) ,<br>\
          \    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),<br>)<br>model.config.use_cache = False<br>trainer.train(resume_from_checkpoint=False)</p>\n\
          <p>model.save_pretrained(\"alpaca-lora-dolly-2.0\")</p>\n<p>generate_text\
          \ = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)</p>\n\
          <p>generate_text(\"Look up the boiling point of water.\")</p>\n<p>generate_text(\"\
          Find the capital of Spain.\")</p>\n<p>generate_text(\"Translate the following\
          \ phrase into French: I love my dog\")</p>\n<p>generate_text(\"Given a set\
          \ of numbers, find the maximum value: Set: {10, 3, 25, 62, 16}\")</p>\n\
          <p>generate_text(\"Translate the following phrase into French: I love my\
          \ dog\")</p>\n"
        raw: "Hi @ivgome your code is running well in colab free with few modifications\
          \ - mostly due to memory limitations (see copy and paste of your file with\
          \ adjustments below which is running):\n\n\n# -*- coding: utf-8 -*-\n\"\"\
          \"Fine-tuning Dolly 2.0 with LoRA and Alpaca.ipynb\n\nAutomatically generated\
          \ by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1-nyF2tdV7jOvxqR3OCw7Bv2IyZQpp_5D\n\
          \n# Fine-tuning Dolly 2.0 with LoRA\n\n*   Dolly-v2-3b - https://huggingface.co/databricks/dolly-v2-3b\n\
          *   LoRA paper - https://arxiv.org/abs/2106.09685\n*   Alpaca Cleaned Dataset\
          \ - https://github.com/gururise/AlpacaDataCleaned\n\"\"\"\n\n!git clone\
          \ https://github.com/gururise/AlpacaDataCleaned.git\n\nls AlpacaDataCleaned/\n\
          \n!pip install accelerate>=0.21.0 transformers[torch]==4.30.2\n!pip install\
          \ -q datasets loralib sentencepiece\n!pip -q install git+https://github.com/huggingface/peft.git\n\
          !pip -q install bitsandbytes\n\n# Create Instruct Pipeline\nimport logging\n\
          import re\n\nimport numpy as np\nfrom transformers import Pipeline, PreTrainedTokenizer\n\
          \nlogger = logging.getLogger(__name__)\n\nINSTRUCTION_KEY = \"### Instruction:\"\
          \nRESPONSE_KEY = \"### Response:\"\nEND_KEY = \"### End\"\nINTRO_BLURB =\
          \ (\n    \"Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\"\n)\n\n# This is the prompt\
          \ that is used for generating responses using an already trained model.\
          \  It ends with the response\n# key, where the job of the model is to provide\
          \ the completion that follows it (i.e. the response itself).\nPROMPT_FOR_GENERATION_FORMAT\
          \ = \"\"\"{intro}\n{instruction_key}\n{instruction}\n{response_key}\n\"\"\
          \".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n\
          \    instruction=\"{instruction}\",\n    response_key=RESPONSE_KEY,\n)\n\
          \n\ndef get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) ->\
          \ int:\n    \"\"\"Gets the token ID for a given string that has been added\
          \ to the tokenizer as a special token.\n    When training, we configure\
          \ the tokenizer so that the sequences like \"### Instruction:\" and \"###\
          \ End\" are\n    treated specially and converted to a single, new token.\
          \  This retrieves the token ID each of these keys map to.\n    Args:\n \
          \       tokenizer (PreTrainedTokenizer): the tokenizer\n        key (str):\
          \ the key to convert to a single token\n    Raises:\n        RuntimeError:\
          \ if more than one ID was generated\n    Returns:\n        int: the token\
          \ ID for the given key\n    \"\"\"\n    token_ids = tokenizer.encode(key)\n\
          \    if len(token_ids) > 1:\n        raise ValueError(f\"Expected only a\
          \ single token for '{key}' but found {token_ids}\")\n    return token_ids[0]\n\
          \n\nclass InstructionTextGenerationPipeline(Pipeline):\n    def __init__(\n\
          \        self, *args, do_sample: bool = True, max_new_tokens: int = 256,\
          \ top_p: float = 0.92, top_k: int = 0, **kwargs\n    ):\n        super().__init__(*args,\
          \ do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k,\
          \ **kwargs)\n\n    def _sanitize_parameters(self, return_instruction_text=False,\
          \ **generate_kwargs):\n        preprocess_params = {}\n\n        # newer\
          \ versions of the tokenizer configure the response key as a special token.\
          \  newer versions still may\n        # append a newline to yield a single\
          \ token.  find whatever token is configured for the response key.\n    \
          \    tokenizer_response_key = next(\n            (token for token in self.tokenizer.additional_special_tokens\
          \ if token.startswith(RESPONSE_KEY)), None\n        )\n\n        response_key_token_id\
          \ = None\n        end_key_token_id = None\n        if tokenizer_response_key:\n\
          \            try:\n                response_key_token_id = get_special_token_id(self.tokenizer,\
          \ tokenizer_response_key)\n                end_key_token_id = get_special_token_id(self.tokenizer,\
          \ END_KEY)\n\n                # Ensure generation stops once it generates\
          \ \"### End\"\n                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n\
          \            except ValueError:\n                pass\n\n        forward_params\
          \ = generate_kwargs\n        postprocess_params = {\n            \"response_key_token_id\"\
          : response_key_token_id,\n            \"end_key_token_id\": end_key_token_id,\n\
          \            \"return_instruction_text\": return_instruction_text,\n   \
          \     }\n\n        return preprocess_params, forward_params, postprocess_params\n\
          \n    def preprocess(self, instruction_text, **generate_kwargs):\n     \
          \   prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n\
          \        inputs = self.tokenizer(\n            prompt_text,\n          \
          \  return_tensors=\"pt\",\n        )\n        inputs[\"prompt_text\"] =\
          \ prompt_text\n        inputs[\"instruction_text\"] = instruction_text\n\
          \        return inputs\n\n    def _forward(self, model_inputs, **generate_kwargs):\n\
          \        input_ids = model_inputs[\"input_ids\"]\n        attention_mask\
          \ = model_inputs.get(\"attention_mask\", None)\n        generated_sequence\
          \ = self.model.generate(\n            input_ids=input_ids.to(self.model.device),\n\
          \            attention_mask=attention_mask,\n            pad_token_id=self.tokenizer.pad_token_id,\n\
          \            **generate_kwargs,\n        )[0].cpu()\n        instruction_text\
          \ = model_inputs.pop(\"instruction_text\")\n        return {\"generated_sequence\"\
          : generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n\
          \n    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id,\
          \ return_instruction_text):\n        sequence = model_outputs[\"generated_sequence\"\
          ]\n        instruction_text = model_outputs[\"instruction_text\"]\n\n  \
          \      # The response will be set to this variable if we can identify it.\n\
          \        decoded = None\n\n        # If we have token IDs for the response\
          \ and end, then we can find the tokens and only decode between them.\n \
          \       if response_key_token_id and end_key_token_id:\n            # Find\
          \ where \"### Response:\" is first found in the generated tokens.  Considering\
          \ this is part of the\n            # prompt, we should definitely find it.\
          \  We will return the tokens found after this token.\n            response_pos\
          \ = None\n            response_positions = np.where(sequence == response_key_token_id)[0]\n\
          \            if len(response_positions) == 0:\n                logger.warn(f\"\
          Could not find response key {response_key_token_id} in: {sequence}\")\n\
          \            else:\n                response_pos = response_positions[0]\n\
          \n            if response_pos:\n                # Next find where \"###\
          \ End\" is located.  The model has been trained to end its responses with\
          \ this\n                # sequence (or actually, the token ID it maps to,\
          \ since it is a special token).  We may not find\n                # this\
          \ token, as the response could be truncated.  If we don't find it then just\
          \ return everything\n                # to the end.  Note that even though\
          \ we set eos_token_id, we still see the this token at the end.\n       \
          \         end_pos = None\n                end_positions = np.where(sequence\
          \ == end_key_token_id)[0]\n                if len(end_positions) > 0:\n\
          \                    end_pos = end_positions[0]\n\n                decoded\
          \ = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n\
          \        else:\n            # Otherwise we'll decode everything and use\
          \ a regex to find the response and end.\n\n            fully_decoded = self.tokenizer.decode(sequence)\n\
          \n            # The response appears after \"### Response:\".  The model\
          \ has been trained to append \"### End\" at the\n            # end.\n  \
          \          m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded,\
          \ flags=re.DOTALL)\n\n            if m:\n                decoded = m.group(1).strip()\n\
          \            else:\n                # The model might not generate the \"\
          ### End\" sequence before reaching the max tokens.  In this case,\n    \
          \            # return everything after \"### Response:\".\n            \
          \    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n\
          \                if m:\n                    decoded = m.group(1).strip()\n\
          \                else:\n                    logger.warn(f\"Failed to find\
          \ response in:\\n{fully_decoded}\")\n\n        if return_instruction_text:\n\
          \            return {\"instruction_text\": instruction_text, \"generated_text\"\
          : decoded}\n\n        return decoded\n\nimport torch\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"\
          left\")\n\nquantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\"\
          ,\n                                             device_map=\"auto\",\n \
          \                                            torch_dtype=torch.bfloat16,\n\
          \                                             #torch_dtype=torch.int8,\n\
          \                                             quantization_config=quantization_config,\n\
          \                                             load_in_4bit=True,\n     \
          \                                        #load_in_8bit=True,\n         \
          \                                    )\n\n# generate_text = InstructionTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\n\nfrom datasets import load_dataset\n\ndata = load_dataset(\"\
          json\",\n                    data_files=\"./AlpacaDataCleaned/alpaca_data.json\"\
          )\n\ndef generate_prompt(data_point):\n    # taken from https://github.com/tloen/alpaca-lora\n\
          \    if data_point[\"instruction\"]:\n        return f\"\"\"Below is an\
          \ instruction that describes a task, paired with an input that provides\
          \ further context. Write a response that appropriately completes the request.\n\
          \n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"\
          input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n\
          \        return f\"\"\"Below is an instruction that describes a task. Write\
          \ a response that appropriately completes the request.\n\n### Instruction:\n\
          {data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\
          \"\"\n\n\ndata = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})\n\
          \ndata\n\n\"\"\"## Finetuning Dolly\"\"\"\n\nimport os\n\n# os.environ[\"\
          CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport torch\nimport torch.nn as nn\nimport\
          \ bitsandbytes as bnb\nfrom datasets import load_dataset\nimport transformers\n\
          from transformers import AutoTokenizer, AutoModel, AutoConfig, GPTJForCausalLM\n\
          \nfrom peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n\
          \n# Settings for A100 - For 3090\nMICRO_BATCH_SIZE = 4  # change to 4 for\
          \ 3090\nBATCH_SIZE = 128\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\
          EPOCHS = 2  # paper uses 3\nLEARNING_RATE = 2e-5\nCUTOFF_LEN = 256\nLORA_R\
          \ = 4\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\n\n\n# Settings for A100 - For\
          \ 3090\nMICRO_BATCH_SIZE = 4  # change to 4 for 3090\nBATCH_SIZE = 32\n\
          GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 2\
          \  # paper uses 3\nLEARNING_RATE = 2e-5\nCUTOFF_LEN = 32\nLORA_R = 4\nLORA_ALPHA\
          \ = 16\nLORA_DROPOUT = 0.05\n\n#model = prepare_model_for_int8_training(model,\
          \ use_gradient_checkpointing=True)\n\nconfig = LoraConfig(\n    r=LORA_R,\n\
          \    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"\
          none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model,\
          \ config)\ntokenizer.pad_token_id = 0  # unk. we want this to be different\
          \ from the eos token\n\ndata = load_dataset(\"json\", data_files=\"./AlpacaDataCleaned/alpaca_data_cleaned.json\"\
          )\n\ndata = data.shuffle().map(\n    lambda data_point: tokenizer(\n   \
          \     generate_prompt(data_point),\n        truncation=True,\n        max_length=CUTOFF_LEN,\n\
          \        padding=\"max_length\",\n    )\n)\n\ndata\n\nfrom transformers.training_args\
          \ import ParallelMode\n\ntrainer = transformers.Trainer(\n    model=model,\n\
          \    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n\
          \        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n\
          \        warmup_steps=100,\n        num_train_epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n\
          \        fp16=True,\n        #sharded_ddp=\"zero_dp_3 auto_wrap\",\n   \
          \    # fsdp=\"full_shard auto_wrap\",\n        # model_parallel=True,\n\
          \        #parallel_mode=ParallelMode.DISTRIBUTED,\n        #is_model_parallel=True,\n\
          \        logging_steps=1,\n        output_dir=\"lora-dolly\",\n        save_total_limit=3,\n\
          \    ) ,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),\n)\nmodel.config.use_cache = False\ntrainer.train(resume_from_checkpoint=False)\n\
          \nmodel.save_pretrained(\"alpaca-lora-dolly-2.0\")\n\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\n\ngenerate_text(\"Look up the boiling point of water.\"\
          )\n\ngenerate_text(\"Find the capital of Spain.\")\n\ngenerate_text(\"Translate\
          \ the following phrase into French: I love my dog\")\n\ngenerate_text(\"\
          Given a set of numbers, find the maximum value: Set: {10, 3, 25, 62, 16}\"\
          )\n\ngenerate_text(\"Translate the following phrase into French: I love\
          \ my dog\")"
        updatedAt: '2023-07-17T10:55:01.897Z'
      numEdits: 1
      reactions: []
    id: 64b51cfeeec33e27dce399c4
    type: comment
  author: renedlog
  content: "Hi @ivgome your code is running well in colab free with few modifications\
    \ - mostly due to memory limitations (see copy and paste of your file with adjustments\
    \ below which is running):\n\n\n# -*- coding: utf-8 -*-\n\"\"\"Fine-tuning Dolly\
    \ 2.0 with LoRA and Alpaca.ipynb\n\nAutomatically generated by Colaboratory.\n\
    \nOriginal file is located at\n    https://colab.research.google.com/drive/1-nyF2tdV7jOvxqR3OCw7Bv2IyZQpp_5D\n\
    \n# Fine-tuning Dolly 2.0 with LoRA\n\n*   Dolly-v2-3b - https://huggingface.co/databricks/dolly-v2-3b\n\
    *   LoRA paper - https://arxiv.org/abs/2106.09685\n*   Alpaca Cleaned Dataset\
    \ - https://github.com/gururise/AlpacaDataCleaned\n\"\"\"\n\n!git clone https://github.com/gururise/AlpacaDataCleaned.git\n\
    \nls AlpacaDataCleaned/\n\n!pip install accelerate>=0.21.0 transformers[torch]==4.30.2\n\
    !pip install -q datasets loralib sentencepiece\n!pip -q install git+https://github.com/huggingface/peft.git\n\
    !pip -q install bitsandbytes\n\n# Create Instruct Pipeline\nimport logging\nimport\
    \ re\n\nimport numpy as np\nfrom transformers import Pipeline, PreTrainedTokenizer\n\
    \nlogger = logging.getLogger(__name__)\n\nINSTRUCTION_KEY = \"### Instruction:\"\
    \nRESPONSE_KEY = \"### Response:\"\nEND_KEY = \"### End\"\nINTRO_BLURB = (\n \
    \   \"Below is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\"\n)\n\n# This is the prompt that is used for generating\
    \ responses using an already trained model.  It ends with the response\n# key,\
    \ where the job of the model is to provide the completion that follows it (i.e.\
    \ the response itself).\nPROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n{instruction_key}\n\
    {instruction}\n{response_key}\n\"\"\".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n\
    \    instruction=\"{instruction}\",\n    response_key=RESPONSE_KEY,\n)\n\n\ndef\
    \ get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n   \
    \ \"\"\"Gets the token ID for a given string that has been added to the tokenizer\
    \ as a special token.\n    When training, we configure the tokenizer so that the\
    \ sequences like \"### Instruction:\" and \"### End\" are\n    treated specially\
    \ and converted to a single, new token.  This retrieves the token ID each of these\
    \ keys map to.\n    Args:\n        tokenizer (PreTrainedTokenizer): the tokenizer\n\
    \        key (str): the key to convert to a single token\n    Raises:\n      \
    \  RuntimeError: if more than one ID was generated\n    Returns:\n        int:\
    \ the token ID for the given key\n    \"\"\"\n    token_ids = tokenizer.encode(key)\n\
    \    if len(token_ids) > 1:\n        raise ValueError(f\"Expected only a single\
    \ token for '{key}' but found {token_ids}\")\n    return token_ids[0]\n\n\nclass\
    \ InstructionTextGenerationPipeline(Pipeline):\n    def __init__(\n        self,\
    \ *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92,\
    \ top_k: int = 0, **kwargs\n    ):\n        super().__init__(*args, do_sample=do_sample,\
    \ max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)\n\n    def\
    \ _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n\
    \        preprocess_params = {}\n\n        # newer versions of the tokenizer configure\
    \ the response key as a special token.  newer versions still may\n        # append\
    \ a newline to yield a single token.  find whatever token is configured for the\
    \ response key.\n        tokenizer_response_key = next(\n            (token for\
    \ token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)),\
    \ None\n        )\n\n        response_key_token_id = None\n        end_key_token_id\
    \ = None\n        if tokenizer_response_key:\n            try:\n             \
    \   response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n\
    \                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n\
    \n                # Ensure generation stops once it generates \"### End\"\n  \
    \              generate_kwargs[\"eos_token_id\"] = end_key_token_id\n        \
    \    except ValueError:\n                pass\n\n        forward_params = generate_kwargs\n\
    \        postprocess_params = {\n            \"response_key_token_id\": response_key_token_id,\n\
    \            \"end_key_token_id\": end_key_token_id,\n            \"return_instruction_text\"\
    : return_instruction_text,\n        }\n\n        return preprocess_params, forward_params,\
    \ postprocess_params\n\n    def preprocess(self, instruction_text, **generate_kwargs):\n\
    \        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n\
    \        inputs = self.tokenizer(\n            prompt_text,\n            return_tensors=\"\
    pt\",\n        )\n        inputs[\"prompt_text\"] = prompt_text\n        inputs[\"\
    instruction_text\"] = instruction_text\n        return inputs\n\n    def _forward(self,\
    \ model_inputs, **generate_kwargs):\n        input_ids = model_inputs[\"input_ids\"\
    ]\n        attention_mask = model_inputs.get(\"attention_mask\", None)\n     \
    \   generated_sequence = self.model.generate(\n            input_ids=input_ids.to(self.model.device),\n\
    \            attention_mask=attention_mask,\n            pad_token_id=self.tokenizer.pad_token_id,\n\
    \            **generate_kwargs,\n        )[0].cpu()\n        instruction_text\
    \ = model_inputs.pop(\"instruction_text\")\n        return {\"generated_sequence\"\
    : generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n\
    \n    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id,\
    \ return_instruction_text):\n        sequence = model_outputs[\"generated_sequence\"\
    ]\n        instruction_text = model_outputs[\"instruction_text\"]\n\n        #\
    \ The response will be set to this variable if we can identify it.\n        decoded\
    \ = None\n\n        # If we have token IDs for the response and end, then we can\
    \ find the tokens and only decode between them.\n        if response_key_token_id\
    \ and end_key_token_id:\n            # Find where \"### Response:\" is first found\
    \ in the generated tokens.  Considering this is part of the\n            # prompt,\
    \ we should definitely find it.  We will return the tokens found after this token.\n\
    \            response_pos = None\n            response_positions = np.where(sequence\
    \ == response_key_token_id)[0]\n            if len(response_positions) == 0:\n\
    \                logger.warn(f\"Could not find response key {response_key_token_id}\
    \ in: {sequence}\")\n            else:\n                response_pos = response_positions[0]\n\
    \n            if response_pos:\n                # Next find where \"### End\"\
    \ is located.  The model has been trained to end its responses with this\n   \
    \             # sequence (or actually, the token ID it maps to, since it is a\
    \ special token).  We may not find\n                # this token, as the response\
    \ could be truncated.  If we don't find it then just return everything\n     \
    \           # to the end.  Note that even though we set eos_token_id, we still\
    \ see the this token at the end.\n                end_pos = None\n           \
    \     end_positions = np.where(sequence == end_key_token_id)[0]\n            \
    \    if len(end_positions) > 0:\n                    end_pos = end_positions[0]\n\
    \n                decoded = self.tokenizer.decode(sequence[response_pos + 1 :\
    \ end_pos]).strip()\n        else:\n            # Otherwise we'll decode everything\
    \ and use a regex to find the response and end.\n\n            fully_decoded =\
    \ self.tokenizer.decode(sequence)\n\n            # The response appears after\
    \ \"### Response:\".  The model has been trained to append \"### End\" at the\n\
    \            # end.\n            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\\
    s*End\", fully_decoded, flags=re.DOTALL)\n\n            if m:\n              \
    \  decoded = m.group(1).strip()\n            else:\n                # The model\
    \ might not generate the \"### End\" sequence before reaching the max tokens.\
    \  In this case,\n                # return everything after \"### Response:\"\
    .\n                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded,\
    \ flags=re.DOTALL)\n                if m:\n                    decoded = m.group(1).strip()\n\
    \                else:\n                    logger.warn(f\"Failed to find response\
    \ in:\\n{fully_decoded}\")\n\n        if return_instruction_text:\n          \
    \  return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n\
    \n        return decoded\n\nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, BitsAndBytesConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    databricks/dolly-v2-3b\", padding_side=\"left\")\n\nquantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\",\n \
    \                                            device_map=\"auto\",\n          \
    \                                   torch_dtype=torch.bfloat16,\n            \
    \                                 #torch_dtype=torch.int8,\n                 \
    \                            quantization_config=quantization_config,\n      \
    \                                       load_in_4bit=True,\n                 \
    \                            #load_in_8bit=True,\n                           \
    \                  )\n\n# generate_text = InstructionTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer)\n\nfrom datasets import load_dataset\n\ndata = load_dataset(\"\
    json\",\n                    data_files=\"./AlpacaDataCleaned/alpaca_data.json\"\
    )\n\ndef generate_prompt(data_point):\n    # taken from https://github.com/tloen/alpaca-lora\n\
    \    if data_point[\"instruction\"]:\n        return f\"\"\"Below is an instruction\
    \ that describes a task, paired with an input that provides further context. Write\
    \ a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"\
    instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"\
    output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that\
    \ describes a task. Write a response that appropriately completes the request.\n\
    \n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"\
    output\"]}\"\"\"\n\n\ndata = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})\n\
    \ndata\n\n\"\"\"## Finetuning Dolly\"\"\"\n\nimport os\n\n# os.environ[\"CUDA_VISIBLE_DEVICES\"\
    ] = \"0\"\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom\
    \ datasets import load_dataset\nimport transformers\nfrom transformers import\
    \ AutoTokenizer, AutoModel, AutoConfig, GPTJForCausalLM\n\nfrom peft import prepare_model_for_int8_training,\
    \ LoraConfig, get_peft_model\n\n# Settings for A100 - For 3090\nMICRO_BATCH_SIZE\
    \ = 4  # change to 4 for 3090\nBATCH_SIZE = 128\nGRADIENT_ACCUMULATION_STEPS =\
    \ BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 2  # paper uses 3\nLEARNING_RATE =\
    \ 2e-5\nCUTOFF_LEN = 256\nLORA_R = 4\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\n\n\
    \n# Settings for A100 - For 3090\nMICRO_BATCH_SIZE = 4  # change to 4 for 3090\n\
    BATCH_SIZE = 32\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\
    EPOCHS = 2  # paper uses 3\nLEARNING_RATE = 2e-5\nCUTOFF_LEN = 32\nLORA_R = 4\n\
    LORA_ALPHA = 16\nLORA_DROPOUT = 0.05\n\n#model = prepare_model_for_int8_training(model,\
    \ use_gradient_checkpointing=True)\n\nconfig = LoraConfig(\n    r=LORA_R,\n  \
    \  lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n\
    \    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)\ntokenizer.pad_token_id\
    \ = 0  # unk. we want this to be different from the eos token\n\ndata = load_dataset(\"\
    json\", data_files=\"./AlpacaDataCleaned/alpaca_data_cleaned.json\")\n\ndata =\
    \ data.shuffle().map(\n    lambda data_point: tokenizer(\n        generate_prompt(data_point),\n\
    \        truncation=True,\n        max_length=CUTOFF_LEN,\n        padding=\"\
    max_length\",\n    )\n)\n\ndata\n\nfrom transformers.training_args import ParallelMode\n\
    \ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data[\"\
    train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n\
    \        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=100,\n\
    \        num_train_epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n    \
    \    fp16=True,\n        #sharded_ddp=\"zero_dp_3 auto_wrap\",\n       # fsdp=\"\
    full_shard auto_wrap\",\n        # model_parallel=True,\n        #parallel_mode=ParallelMode.DISTRIBUTED,\n\
    \        #is_model_parallel=True,\n        logging_steps=1,\n        output_dir=\"\
    lora-dolly\",\n        save_total_limit=3,\n    ) ,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
    \ mlm=False),\n)\nmodel.config.use_cache = False\ntrainer.train(resume_from_checkpoint=False)\n\
    \nmodel.save_pretrained(\"alpaca-lora-dolly-2.0\")\n\ngenerate_text = InstructionTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer)\n\ngenerate_text(\"Look up the boiling point of water.\"\
    )\n\ngenerate_text(\"Find the capital of Spain.\")\n\ngenerate_text(\"Translate\
    \ the following phrase into French: I love my dog\")\n\ngenerate_text(\"Given\
    \ a set of numbers, find the maximum value: Set: {10, 3, 25, 62, 16}\")\n\ngenerate_text(\"\
    Translate the following phrase into French: I love my dog\")"
  created_at: 2023-07-17 09:50:38+00:00
  edited: true
  hidden: false
  id: 64b51cfeeec33e27dce399c4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: databricks/dolly-v2-3b
repo_type: model
status: closed
target_branch: null
title: problem after training the model
