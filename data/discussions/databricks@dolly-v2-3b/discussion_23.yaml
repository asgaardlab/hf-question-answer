!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adityakad
conflicting_files: null
created_at: 2023-05-31 01:49:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
      fullname: Aditya Kad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityakad
      type: user
    createdAt: '2023-05-31T02:49:41.000Z'
    data:
      edited: true
      editors:
      - adityakad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
          fullname: Aditya Kad
          isHf: false
          isPro: false
          name: adityakad
          type: user
        html: '<p>#Hi,  I ran the below steps.<br>tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-3b",
          padding_side="left")<br>base_model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-3b",
          device_map="auto")</p>

          <p>#I saved the model after this.<br>base_model.save_pretrained("/home/ec2-user/SageMaker/models/dolly-v2-3b",
          from_pt=True)</p>

          <p>When I see the saved files, it is different from the one you see in the
          repo.<br>For instance, I see one 5.68GB bin file in the repo but the saved
          model file downloaded 2 bin files. One file is 10.1GB and other is 1.15GB.
          This does not match the files in this repo. </p>

          <p>Any idea why this is happening? What are the implications of this large
          model size?<br>Here is what I get after saving the pretrained model.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/vPhsEzE2KUhHWBYz7jX-6.png"><img
          alt="Screenshot 2023-05-30 at 7.48.33 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/vPhsEzE2KUhHWBYz7jX-6.png"></a></p>

          '
        raw: "#Hi,  I ran the below steps.\ntokenizer = AutoTokenizer.from_pretrained(\"\
          databricks/dolly-v2-3b\", padding_side=\"left\")\nbase_model = AutoModelForCausalLM.from_pretrained(\"\
          databricks/dolly-v2-3b\", device_map=\"auto\")\n\n#I saved the model after\
          \ this.\nbase_model.save_pretrained(\"/home/ec2-user/SageMaker/models/dolly-v2-3b\"\
          , from_pt=True)\n\nWhen I see the saved files, it is different from the\
          \ one you see in the repo.\nFor instance, I see one 5.68GB bin file in the\
          \ repo but the saved model file downloaded 2 bin files. One file is 10.1GB\
          \ and other is 1.15GB. This does not match the files in this repo. \n\n\
          Any idea why this is happening? What are the implications of this large\
          \ model size? \nHere is what I get after saving the pretrained model.\n\n\
          ![Screenshot 2023-05-30 at 7.48.33 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/vPhsEzE2KUhHWBYz7jX-6.png)"
        updatedAt: '2023-05-31T03:37:34.160Z'
      numEdits: 2
      reactions: []
    id: 6476b5c5bb7fdd7f425df29e
    type: comment
  author: adityakad
  content: "#Hi,  I ran the below steps.\ntokenizer = AutoTokenizer.from_pretrained(\"\
    databricks/dolly-v2-3b\", padding_side=\"left\")\nbase_model = AutoModelForCausalLM.from_pretrained(\"\
    databricks/dolly-v2-3b\", device_map=\"auto\")\n\n#I saved the model after this.\n\
    base_model.save_pretrained(\"/home/ec2-user/SageMaker/models/dolly-v2-3b\", from_pt=True)\n\
    \nWhen I see the saved files, it is different from the one you see in the repo.\n\
    For instance, I see one 5.68GB bin file in the repo but the saved model file downloaded\
    \ 2 bin files. One file is 10.1GB and other is 1.15GB. This does not match the\
    \ files in this repo. \n\nAny idea why this is happening? What are the implications\
    \ of this large model size? \nHere is what I get after saving the pretrained model.\n\
    \n![Screenshot 2023-05-30 at 7.48.33 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/vPhsEzE2KUhHWBYz7jX-6.png)"
  created_at: 2023-05-31 01:49:41+00:00
  edited: true
  hidden: false
  id: 6476b5c5bb7fdd7f425df29e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-31T02:58:06.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>It''s because you did not load in 16-bit, I''d imagine. You''re
          saving weights in 2x the precision and storage space.</p>

          '
        raw: It's because you did not load in 16-bit, I'd imagine. You're saving weights
          in 2x the precision and storage space.
        updatedAt: '2023-05-31T02:58:06.919Z'
      numEdits: 0
      reactions: []
    id: 6476b7be0214fec3e76cc46b
    type: comment
  author: srowen
  content: It's because you did not load in 16-bit, I'd imagine. You're saving weights
    in 2x the precision and storage space.
  created_at: 2023-05-31 01:58:06+00:00
  edited: false
  hidden: false
  id: 6476b7be0214fec3e76cc46b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
      fullname: Aditya Kad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityakad
      type: user
    createdAt: '2023-05-31T03:10:45.000Z'
    data:
      edited: false
      editors:
      - adityakad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
          fullname: Aditya Kad
          isHf: false
          isPro: false
          name: adityakad
          type: user
        html: '<p>I see, so when I tried running the results from the saved model,
          the latency was 3-4 times higher than the one from_pretrained. Shouldn''t
          the latency be the same in both the cases?</p>

          '
        raw: I see, so when I tried running the results from the saved model, the
          latency was 3-4 times higher than the one from_pretrained. Shouldn't the
          latency be the same in both the cases?
        updatedAt: '2023-05-31T03:10:45.092Z'
      numEdits: 0
      reactions: []
    id: 6476bab526ec66674c6edec6
    type: comment
  author: adityakad
  content: I see, so when I tried running the results from the saved model, the latency
    was 3-4 times higher than the one from_pretrained. Shouldn't the latency be the
    same in both the cases?
  created_at: 2023-05-31 02:10:45+00:00
  edited: false
  hidden: false
  id: 6476bab526ec66674c6edec6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-31T03:16:10.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>No, because you are doing more than twice the work in 32-bit math.
          I don''t see why you are doing it this way?</p>

          '
        raw: No, because you are doing more than twice the work in 32-bit math. I
          don't see why you are doing it this way?
        updatedAt: '2023-05-31T03:16:10.278Z'
      numEdits: 0
      reactions: []
    id: 6476bbfa26ec66674c6ef758
    type: comment
  author: srowen
  content: No, because you are doing more than twice the work in 32-bit math. I don't
    see why you are doing it this way?
  created_at: 2023-05-31 02:16:10+00:00
  edited: false
  hidden: false
  id: 6476bbfa26ec66674c6ef758
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
      fullname: Aditya Kad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityakad
      type: user
    createdAt: '2023-05-31T03:20:08.000Z'
    data:
      edited: true
      editors:
      - adityakad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
          fullname: Aditya Kad
          isHf: false
          isPro: false
          name: adityakad
          type: user
        html: '<p>Based on what you say, I am loading it originally from HuggingFace
          in 32 bit as well. Is that right? But the latency is really low on that
          one. How is that happening?</p>

          '
        raw: Based on what you say, I am loading it originally from HuggingFace in
          32 bit as well. Is that right? But the latency is really low on that one.
          How is that happening?
        updatedAt: '2023-05-31T03:29:48.856Z'
      numEdits: 1
      reactions: []
    id: 6476bce826ec66674c6f0804
    type: comment
  author: adityakad
  content: Based on what you say, I am loading it originally from HuggingFace in 32
    bit as well. Is that right? But the latency is really low on that one. How is
    that happening?
  created_at: 2023-05-31 02:20:08+00:00
  edited: true
  hidden: false
  id: 6476bce826ec66674c6f0804
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-05-31T03:32:06.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Ah ok I mistook the setup, you''re benchmarking loading this way
          too without saving. Yeah should be the same thing. Check the torch_dtype
          in both cases to confirm. Otherwise not sure why or maybe I''m wrong about
          the precision being the issue. </p>

          <p>Are you sure you are unloading the first model before loading the second
          ? Otherwise you might load the second only partly on the GPU</p>

          '
        raw: "Ah ok I mistook the setup, you're benchmarking loading this way too\
          \ without saving. Yeah should be the same thing. Check the torch_dtype in\
          \ both cases to confirm. Otherwise not sure why or maybe I'm wrong about\
          \ the precision being the issue. \n\nAre you sure you are unloading the\
          \ first model before loading the second ? Otherwise you might load the second\
          \ only partly on the GPU"
        updatedAt: '2023-05-31T03:32:06.542Z'
      numEdits: 0
      reactions: []
    id: 6476bfb609660ee8c7969ee4
    type: comment
  author: srowen
  content: "Ah ok I mistook the setup, you're benchmarking loading this way too without\
    \ saving. Yeah should be the same thing. Check the torch_dtype in both cases to\
    \ confirm. Otherwise not sure why or maybe I'm wrong about the precision being\
    \ the issue. \n\nAre you sure you are unloading the first model before loading\
    \ the second ? Otherwise you might load the second only partly on the GPU"
  created_at: 2023-05-31 02:32:06+00:00
  edited: false
  hidden: false
  id: 6476bfb609660ee8c7969ee4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
      fullname: Aditya Kad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityakad
      type: user
    createdAt: '2023-05-31T03:38:32.000Z'
    data:
      edited: true
      editors:
      - adityakad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3ba5f2b787ad4b5744dbf01f09adc16.svg
          fullname: Aditya Kad
          isHf: false
          isPro: false
          name: adityakad
          type: user
        html: '<p>When I load the model first from HuggingFace, it does show downloading
          5.68Gb like in the repo.</p>

          <p>I am saving this very same model.</p>

          <p>What do you mean by unloading the first model? How do I do that?</p>

          <p>These are the exact steps:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/QPcPxLXt4w229OcGKXNHl.png"><img
          alt="Screenshot 2023-05-30 at 8.50.36 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/QPcPxLXt4w229OcGKXNHl.png"></a></p>

          '
        raw: 'When I load the model first from HuggingFace, it does show downloading
          5.68Gb like in the repo.


          I am saving this very same model.


          What do you mean by unloading the first model? How do I do that?


          These are the exact steps:


          ![Screenshot 2023-05-30 at 8.50.36 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/QPcPxLXt4w229OcGKXNHl.png)'
        updatedAt: '2023-05-31T03:51:34.748Z'
      numEdits: 2
      reactions: []
    id: 6476c1380214fec3e76d80e8
    type: comment
  author: adityakad
  content: 'When I load the model first from HuggingFace, it does show downloading
    5.68Gb like in the repo.


    I am saving this very same model.


    What do you mean by unloading the first model? How do I do that?


    These are the exact steps:


    ![Screenshot 2023-05-30 at 8.50.36 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6452b5b10a19adf0445f9929/QPcPxLXt4w229OcGKXNHl.png)'
  created_at: 2023-05-31 02:38:32+00:00
  edited: true
  hidden: false
  id: 6476c1380214fec3e76d80e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-07-15T16:01:27.000Z'
    data:
      status: closed
    id: 64b2c2d7727711e4eaba9bc7
    type: status-change
  author: srowen
  created_at: 2023-07-15 15:01:27+00:00
  id: 64b2c2d7727711e4eaba9bc7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: databricks/dolly-v2-3b
repo_type: model
status: closed
target_branch: null
title: Save_pretained showing larger files than the one in the repo
