!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wangbc
conflicting_files: null
created_at: 2023-11-13 09:50:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7750ce61d3cc97f245c4a3f5c90db61f.svg
      fullname: bingchen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangbc
      type: user
    createdAt: '2023-11-13T09:50:27.000Z'
    data:
      edited: false
      editors:
      - wangbc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5299832224845886
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7750ce61d3cc97f245c4a3f5c90db61f.svg
          fullname: bingchen
          isHf: false
          isPro: false
          name: wangbc
          type: user
        html: "<p>\u60A8\u597D\uFF0C\u6211\u60F3\u6D4B\u8BD570B\u7684agentLM\u6A21\
          \u578B\uFF0C\u4F7F\u7528\u7684\u4EE3\u7801\u5982\u4E0B</p>\n<h1 id=\"load-model-directly\"\
          >Load model directly</h1>\n<p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(\"THUDM/agentlm-70b\")<br>model\
          \ = AutoModelForCausalLM.from_pretrained(\"THUDM/agentlm-70b\")</p>\n<p>\u7ED3\
          \u679C\u62A5\u9519\uFF1A<br>Traceback (most recent call last):<br>  File\
          \ \"load.py\", line 5, in <br>    model = AutoModelForCausalLM.from_pretrained(\"\
          THUDM/agentlm-70b\")<br>  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 485, in from_pretrained<br>    pretrained_model_name_or_path, *model_args,\
          \ config=config, **hub_kwargs, **kwargs<br>  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 2896, in from_pretrained<br>    keep_in_fp32_modules=keep_in_fp32_modules,<br>\
          \  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 3278, in _load_pretrained_model<br>    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.<strong>class</strong>.<strong>name</strong>}:\\\
          n\\t{error_msg}\")<br>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>\
          \        size mismatch for model.layers.0.self_attn.k_proj.weight: copying\
          \ a param with shape torch.Size([1024, 8192]) from checkpoint, the shape\
          \ in current model is torch.Size([8192, 8192]).<br>        size mismatch\
          \ for model.layers.0.self_attn.v_proj.weight: copying a param with shape\
          \ torch.Size([1024, 8192]) from checkpoint, the shape in current model is\
          \ torch.Size([8192, 8192]).<br>        size mismatch for model.layers.1.self_attn.k_proj.weight:\
          \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the\
          \ shape in current model is torch.Size([8192, 8192]).<br>        size mismatch\
          \ for model.layers.1.self_attn.v_proj.weight: copying a param with shape\
          \ torch.Size([1024, 8192]) from checkpoint, the shape in current model is\
          \ torch.Size([8192, 8192]).<br>        size mismatch for model.layers.2.self_attn.k_proj.weight:\
          \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the\
          \ shape in current model is torch.Size([8192, 8192]).</p>\n<p>\u8BF7\u95EE\
          \u4E4B\u524D\u6709\u9047\u5230\u8FC7\u4E48\uFF1F\u5982\u4F55\u89E3\u51B3\
          \u5462\uFF1F</p>\n"
        raw: "\u60A8\u597D\uFF0C\u6211\u60F3\u6D4B\u8BD570B\u7684agentLM\u6A21\u578B\
          \uFF0C\u4F7F\u7528\u7684\u4EE3\u7801\u5982\u4E0B\r\n\r\n# Load model directly\r\
          \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"THUDM/agentlm-70b\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          THUDM/agentlm-70b\")\r\n\r\n\u7ED3\u679C\u62A5\u9519\uFF1A\r\nTraceback\
          \ (most recent call last):\r\n  File \"load.py\", line 5, in <module>\r\n\
          \    model = AutoModelForCausalLM.from_pretrained(\"THUDM/agentlm-70b\"\
          )\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 485, in from_pretrained\r\n    pretrained_model_name_or_path, *model_args,\
          \ config=config, **hub_kwargs, **kwargs\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 2896, in from_pretrained\r\n    keep_in_fp32_modules=keep_in_fp32_modules,\r\
          \n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 3278, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\
          )\r\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\
          \n        size mismatch for model.layers.0.self_attn.k_proj.weight: copying\
          \ a param with shape torch.Size([1024, 8192]) from checkpoint, the shape\
          \ in current model is torch.Size([8192, 8192]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.v_proj.weight: copying a param with shape\
          \ torch.Size([1024, 8192]) from checkpoint, the shape in current model is\
          \ torch.Size([8192, 8192]).\r\n        size mismatch for model.layers.1.self_attn.k_proj.weight:\
          \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the\
          \ shape in current model is torch.Size([8192, 8192]).\r\n        size mismatch\
          \ for model.layers.1.self_attn.v_proj.weight: copying a param with shape\
          \ torch.Size([1024, 8192]) from checkpoint, the shape in current model is\
          \ torch.Size([8192, 8192]).\r\n        size mismatch for model.layers.2.self_attn.k_proj.weight:\
          \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the\
          \ shape in current model is torch.Size([8192, 8192]).\r\n\r\n\u8BF7\u95EE\
          \u4E4B\u524D\u6709\u9047\u5230\u8FC7\u4E48\uFF1F\u5982\u4F55\u89E3\u51B3\
          \u5462\uFF1F"
        updatedAt: '2023-11-13T09:50:27.273Z'
      numEdits: 0
      reactions: []
    id: 6551f1630aa8eba4c276dccc
    type: comment
  author: wangbc
  content: "\u60A8\u597D\uFF0C\u6211\u60F3\u6D4B\u8BD570B\u7684agentLM\u6A21\u578B\
    \uFF0C\u4F7F\u7528\u7684\u4EE3\u7801\u5982\u4E0B\r\n\r\n# Load model directly\r\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"THUDM/agentlm-70b\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    THUDM/agentlm-70b\")\r\n\r\n\u7ED3\u679C\u62A5\u9519\uFF1A\r\nTraceback (most\
    \ recent call last):\r\n  File \"load.py\", line 5, in <module>\r\n    model =\
    \ AutoModelForCausalLM.from_pretrained(\"THUDM/agentlm-70b\")\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 485, in from_pretrained\r\n    pretrained_model_name_or_path, *model_args,\
    \ config=config, **hub_kwargs, **kwargs\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
    , line 2896, in from_pretrained\r\n    keep_in_fp32_modules=keep_in_fp32_modules,\r\
    \n  File \"/home/tiger/.local/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
    , line 3278, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s) in\
    \ loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\r\nRuntimeError:\
    \ Error(s) in loading state_dict for LlamaForCausalLM:\r\n        size mismatch\
    \ for model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1024,\
    \ 8192]) from checkpoint, the shape in current model is torch.Size([8192, 8192]).\r\
    \n        size mismatch for model.layers.0.self_attn.v_proj.weight: copying a\
    \ param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current\
    \ model is torch.Size([8192, 8192]).\r\n        size mismatch for model.layers.1.self_attn.k_proj.weight:\
    \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([8192, 8192]).\r\n        size mismatch for model.layers.1.self_attn.v_proj.weight:\
    \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([8192, 8192]).\r\n        size mismatch for model.layers.2.self_attn.k_proj.weight:\
    \ copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([8192, 8192]).\r\n\r\n\u8BF7\u95EE\u4E4B\u524D\
    \u6709\u9047\u5230\u8FC7\u4E48\uFF1F\u5982\u4F55\u89E3\u51B3\u5462\uFF1F"
  created_at: 2023-11-13 09:50:27+00:00
  edited: false
  hidden: false
  id: 6551f1630aa8eba4c276dccc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/435507115bfc502c494989677cfca5fb.svg
      fullname: Rui Lu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: learning-rate
      type: user
    createdAt: '2023-11-14T15:48:23.000Z'
    data:
      edited: false
      editors:
      - learning-rate
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33113694190979004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/435507115bfc502c494989677cfca5fb.svg
          fullname: Rui Lu
          isHf: false
          isPro: false
          name: learning-rate
          type: user
        html: "<p>llama2 \u548C transformers version \u517C\u5BB9\u95EE\u9898\uFF0C\
          \u53EF\u53C2\u8003 <a rel=\"nofollow\" href=\"https://github.com/facebookresearch/llama/issues/378\"\
          >https://github.com/facebookresearch/llama/issues/378</a></p>\n"
        raw: "llama2 \u548C transformers version \u517C\u5BB9\u95EE\u9898\uFF0C\u53EF\
          \u53C2\u8003 [https://github.com/facebookresearch/llama/issues/378](https://github.com/facebookresearch/llama/issues/378)"
        updatedAt: '2023-11-14T15:48:23.209Z'
      numEdits: 0
      reactions: []
    id: 655396c791914629437e5e4d
    type: comment
  author: learning-rate
  content: "llama2 \u548C transformers version \u517C\u5BB9\u95EE\u9898\uFF0C\u53EF\
    \u53C2\u8003 [https://github.com/facebookresearch/llama/issues/378](https://github.com/facebookresearch/llama/issues/378)"
  created_at: 2023-11-14 15:48:23+00:00
  edited: false
  hidden: false
  id: 655396c791914629437e5e4d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: THUDM/agentlm-70b
repo_type: model
status: open
target_branch: null
title: "70B model\u542F\u52A8\u65F6\u52A0\u8F7D\u5931\u8D25"
