!!python/object:huggingface_hub.community.DiscussionWithDetails
author: danielpark
conflicting_files: null
created_at: 2023-08-01 01:37:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c6712883ce71db8ed9f78d/TJq6tY_DMiwAK2WfF04LR.jpeg?w=200&h=200&f=face
      fullname: Minwoo Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielpark
      type: user
    createdAt: '2023-08-01T02:37:04.000Z'
    data:
      edited: true
      editors:
      - danielpark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9479011297225952
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c6712883ce71db8ed9f78d/TJq6tY_DMiwAK2WfF04LR.jpeg?w=200&h=200&f=face
          fullname: Minwoo Park
          isHf: false
          isPro: false
          name: danielpark
          type: user
        html: '<p>Hello, </p>

          <p>I have a question about the sequence lengths for the following two models:</p>

          <ul>

          <li>upstage/Llama-2-70b-instruct-1024</li>

          <li>upstage/llama-30b-instruct-2048</li>

          </ul>

          <p>As far as I know, for LLaMA1, the default sequence length is 1024. However,
          to better capture context and improve training, you fine-tuned the model
          with a sequence length of 2048.<br>Now, for LLaMA2, despite having a default
          sequence length of 2048, I noticed that you fine-tuned it with a sequence
          length of 1024. I''m curious if there''s a specific reason for this choice.</p>

          <p>I''m interested in understanding the perspective from which you analyze
          the results of both models. </p>

          <p>If there''s anything I''m misunderstanding, I would appreciate your guidance.<br>Thank
          you.</p>

          '
        raw: "Hello, \n\nI have a question about the sequence lengths for the following\
          \ two models:\n- upstage/Llama-2-70b-instruct-1024\n- upstage/llama-30b-instruct-2048\n\
          \nAs far as I know, for LLaMA1, the default sequence length is 1024. However,\
          \ to better capture context and improve training, you fine-tuned the model\
          \ with a sequence length of 2048.\nNow, for LLaMA2, despite having a default\
          \ sequence length of 2048, I noticed that you fine-tuned it with a sequence\
          \ length of 1024. I'm curious if there's a specific reason for this choice.\n\
          \nI'm interested in understanding the perspective from which you analyze\
          \ the results of both models. \n\nIf there's anything I'm misunderstanding,\
          \ I would appreciate your guidance. \nThank you."
        updatedAt: '2023-08-03T01:26:52.922Z'
      numEdits: 3
      reactions: []
    id: 64c86fd0dc59e43a61ee8460
    type: comment
  author: danielpark
  content: "Hello, \n\nI have a question about the sequence lengths for the following\
    \ two models:\n- upstage/Llama-2-70b-instruct-1024\n- upstage/llama-30b-instruct-2048\n\
    \nAs far as I know, for LLaMA1, the default sequence length is 1024. However,\
    \ to better capture context and improve training, you fine-tuned the model with\
    \ a sequence length of 2048.\nNow, for LLaMA2, despite having a default sequence\
    \ length of 2048, I noticed that you fine-tuned it with a sequence length of 1024.\
    \ I'm curious if there's a specific reason for this choice.\n\nI'm interested\
    \ in understanding the perspective from which you analyze the results of both\
    \ models. \n\nIf there's anything I'm misunderstanding, I would appreciate your\
    \ guidance. \nThank you."
  created_at: 2023-08-01 01:37:04+00:00
  edited: true
  hidden: false
  id: 64c86fd0dc59e43a61ee8460
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c6712883ce71db8ed9f78d/TJq6tY_DMiwAK2WfF04LR.jpeg?w=200&h=200&f=face
      fullname: Minwoo Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielpark
      type: user
    createdAt: '2023-08-01T02:45:44.000Z'
    data:
      from: Sequence Length 1024 vs 2048 in LLaMA1 and 2, Which is better? Why?
      to: Which one is better, finetuning with a sequence length of 1024 or 2048 in
        LLaMA1 and LLaMA2? And what are the reasons behind the choice?
    id: 64c871d8d11ed5b1e2e0a4c4
    type: title-change
  author: danielpark
  created_at: 2023-08-01 01:45:44+00:00
  id: 64c871d8d11ed5b1e2e0a4c4
  new_title: Which one is better, finetuning with a sequence length of 1024 or 2048
    in LLaMA1 and LLaMA2? And what are the reasons behind the choice?
  old_title: Sequence Length 1024 vs 2048 in LLaMA1 and 2, Which is better? Why?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-08-02T01:00:19.000Z'
    data:
      edited: false
      editors:
      - Limerobot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9229064583778381
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
          fullname: Sanghoon Kim
          isHf: false
          isPro: false
          name: Limerobot
          type: user
        html: '<p>Hello.</p>

          <p>As you may have experienced, I believe Instruction Tuning of LLM is a
          field of empirical experimentation.</p>

          <p>In the case of llama-30b, a higher score was achieved with more Orca
          style dataset and max_seq_len:2048.</p>

          <p>However, for llama-2-70b, in our setting, a smaller size dataset and
          max_seq_len:1024 scored better.<br>In fact, it recorded the highest score
          on our internal leaderboard when only about 50k of a dataset other than
          Orca dataset was used.</p>

          <p>Llama-2-70b tended to overfit faster at max_seq_len:2048, so it performed
          worse than llama-2-70b-hf. However, we do not plan to do additional experiments
          to solve this. (Because there is not much benefit in terms of cost)<br>In
          conclusion, in our setting, the performance of llama-2-70b was better at
          max_seq_len:1024, so we chose 1024.</p>

          <p>I hope my answer was sufficient for you.</p>

          <p>(For reference, according to each model''s config.json, max_position_embeddings
          is 2048 for llma1 and 4096 for llama2.)</p>

          '
        raw: "Hello.\n\nAs you may have experienced, I believe Instruction Tuning\
          \ of LLM is a field of empirical experimentation.\n\nIn the case of llama-30b,\
          \ a higher score was achieved with more Orca style dataset and max_seq_len:2048.\n\
          \nHowever, for llama-2-70b, in our setting, a smaller size dataset and max_seq_len:1024\
          \ scored better. \nIn fact, it recorded the highest score on our internal\
          \ leaderboard when only about 50k of a dataset other than Orca dataset was\
          \ used.\n\nLlama-2-70b tended to overfit faster at max_seq_len:2048, so\
          \ it performed worse than llama-2-70b-hf. However, we do not plan to do\
          \ additional experiments to solve this. (Because there is not much benefit\
          \ in terms of cost)\nIn conclusion, in our setting, the performance of llama-2-70b\
          \ was better at max_seq_len:1024, so we chose 1024.\n\nI hope my answer\
          \ was sufficient for you.\n\n(For reference, according to each model's config.json,\
          \ max_position_embeddings is 2048 for llma1 and 4096 for llama2.)"
        updatedAt: '2023-08-02T01:00:19.171Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - danielpark
      - count: 1
        reaction: "\U0001F44D"
        users:
        - danielpark
    id: 64c9aaa3f7f4ccb5ea6385d4
    type: comment
  author: Limerobot
  content: "Hello.\n\nAs you may have experienced, I believe Instruction Tuning of\
    \ LLM is a field of empirical experimentation.\n\nIn the case of llama-30b, a\
    \ higher score was achieved with more Orca style dataset and max_seq_len:2048.\n\
    \nHowever, for llama-2-70b, in our setting, a smaller size dataset and max_seq_len:1024\
    \ scored better. \nIn fact, it recorded the highest score on our internal leaderboard\
    \ when only about 50k of a dataset other than Orca dataset was used.\n\nLlama-2-70b\
    \ tended to overfit faster at max_seq_len:2048, so it performed worse than llama-2-70b-hf.\
    \ However, we do not plan to do additional experiments to solve this. (Because\
    \ there is not much benefit in terms of cost)\nIn conclusion, in our setting,\
    \ the performance of llama-2-70b was better at max_seq_len:1024, so we chose 1024.\n\
    \nI hope my answer was sufficient for you.\n\n(For reference, according to each\
    \ model's config.json, max_position_embeddings is 2048 for llma1 and 4096 for\
    \ llama2.)"
  created_at: 2023-08-02 00:00:19+00:00
  edited: false
  hidden: false
  id: 64c9aaa3f7f4ccb5ea6385d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c6712883ce71db8ed9f78d/TJq6tY_DMiwAK2WfF04LR.jpeg?w=200&h=200&f=face
      fullname: Minwoo Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielpark
      type: user
    createdAt: '2023-08-03T01:22:49.000Z'
    data:
      edited: true
      editors:
      - danielpark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8849751353263855
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c6712883ce71db8ed9f78d/TJq6tY_DMiwAK2WfF04LR.jpeg?w=200&h=200&f=face
          fullname: Minwoo Park
          isHf: false
          isPro: false
          name: danielpark
          type: user
        html: '<p>Thank you so much for your thoughtful response.</p>

          '
        raw: Thank you so much for your thoughtful response.
        updatedAt: '2023-08-03T01:48:44.582Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Limerobot
    id: 64cb0169547d7092d7baab6c
    type: comment
  author: danielpark
  content: Thank you so much for your thoughtful response.
  created_at: 2023-08-03 00:22:49+00:00
  edited: true
  hidden: false
  id: 64cb0169547d7092d7baab6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-08-03T02:09:51.000Z'
    data:
      status: closed
    id: 64cb0c6f547d7092d7bc043c
    type: status-change
  author: Limerobot
  created_at: 2023-08-03 01:09:51+00:00
  id: 64cb0c6f547d7092d7bc043c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: upstage/llama-30b-instruct-2048
repo_type: model
status: closed
target_branch: null
title: Which one is better, finetuning with a sequence length of 1024 or 2048 in LLaMA1
  and LLaMA2? And what are the reasons behind the choice?
