!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aibarito-ua
conflicting_files: null
created_at: 2023-07-26 00:24:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efa6a18f67fd0967deb82fc878ade724.svg
      fullname: Aibar Oshakbayev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aibarito-ua
      type: user
    createdAt: '2023-07-26T01:24:40.000Z'
    data:
      edited: false
      editors:
      - aibarito-ua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752373695373535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efa6a18f67fd0967deb82fc878ade724.svg
          fullname: Aibar Oshakbayev
          isHf: false
          isPro: false
          name: aibarito-ua
          type: user
        html: '<p>Hello!<br>I am trying to run this model on one A100, but the speed
          is quite slow - 2 tokens/sec. Does anybody know how to make it faster?<br>I
          have tried 8-bit-mode and it is allocating twice less gpu memory, but the
          speed is not increasing.</p>

          '
        raw: "Hello!\r\nI am trying to run this model on one A100, but the speed is\
          \ quite slow - 2 tokens/sec. Does anybody know how to make it faster? \r\
          \nI have tried 8-bit-mode and it is allocating twice less gpu memory, but\
          \ the speed is not increasing."
        updatedAt: '2023-07-26T01:24:40.635Z'
      numEdits: 0
      reactions: []
    id: 64c075d8a2ec8cb2f5777be6
    type: comment
  author: aibarito-ua
  content: "Hello!\r\nI am trying to run this model on one A100, but the speed is\
    \ quite slow - 2 tokens/sec. Does anybody know how to make it faster? \r\nI have\
    \ tried 8-bit-mode and it is allocating twice less gpu memory, but the speed is\
    \ not increasing."
  created_at: 2023-07-26 00:24:40+00:00
  edited: false
  hidden: false
  id: 64c075d8a2ec8cb2f5777be6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-08-02T01:09:59.000Z'
    data:
      edited: false
      editors:
      - Limerobot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4370351731777191
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
          fullname: Sanghoon Kim
          isHf: false
          isPro: false
          name: Limerobot
          type: user
        html: '<p><a rel="nofollow" href="https://www.google.com/search?q=llama+inference+speed+optimization&amp;source=lnt&amp;tbs=qdr:m&amp;sa=X&amp;ved=2ahUKEwjTw-3X5LyAAxULG4gKHa23BrcQpwV6BAgDEA8&amp;biw=2168&amp;bih=1065&amp;dpr=2.36">https://www.google.com/search?q=llama+inference+speed+optimization&amp;source=lnt&amp;tbs=qdr:m&amp;sa=X&amp;ved=2ahUKEwjTw-3X5LyAAxULG4gKHa23BrcQpwV6BAgDEA8&amp;biw=2168&amp;bih=1065&amp;dpr=2.36</a><br>I
          hope this helps you.</p>

          '
        raw: 'https://www.google.com/search?q=llama+inference+speed+optimization&source=lnt&tbs=qdr:m&sa=X&ved=2ahUKEwjTw-3X5LyAAxULG4gKHa23BrcQpwV6BAgDEA8&biw=2168&bih=1065&dpr=2.36

          I hope this helps you.'
        updatedAt: '2023-08-02T01:09:59.745Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - aibarito-ua
        - userzyzz
    id: 64c9ace79471c54a94497929
    type: comment
  author: Limerobot
  content: 'https://www.google.com/search?q=llama+inference+speed+optimization&source=lnt&tbs=qdr:m&sa=X&ved=2ahUKEwjTw-3X5LyAAxULG4gKHa23BrcQpwV6BAgDEA8&biw=2168&bih=1065&dpr=2.36

    I hope this helps you.'
  created_at: 2023-08-02 00:09:59+00:00
  edited: false
  hidden: false
  id: 64c9ace79471c54a94497929
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: upstage/llama-30b-instruct-2048
repo_type: model
status: open
target_branch: null
title: Can it run faster than 2 tokens/second on one A100?
