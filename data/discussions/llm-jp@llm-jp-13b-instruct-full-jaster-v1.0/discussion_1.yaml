!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-10-20 06:01:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-20T07:01:58.000Z'
    data:
      edited: true
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5528604984283447
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>I cannot run this model with torch.float16. And, the load speed\
          \ is slower.</p>\n<p>I ran the following code:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\nmodel_name=<span class=\"\
          hljs-string\">\"llm-jp/llm-jp-13b-instruct-full-jaster-v1.0\"</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.float16) \ntext = <span class=\"hljs-string\">'\u81EA\
          \u7136\u8A00\u8A9E\u51E6\u7406\u3068\u306F\u4F55\u304B'</span> \ntext =\
          \ text + <span class=\"hljs-string\">\"### \u56DE\u7B54\uFF1A\"</span>\n\
          tokenized_input = tokenizer.encode(text, add_special_tokens=<span class=\"\
          hljs-literal\">False</span>, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).to(model.device)\n<span class=\"hljs-keyword\">with</span>\
          \ torch.no_grad():\n    output = model.generate(\n        tokenized_input,\n\
          \        max_new_tokens=<span class=\"hljs-number\">256</span>,\n      \
          \  do_sample=<span class=\"hljs-literal\">True</span>,\n        top_p=<span\
          \ class=\"hljs-number\">0.95</span>,\n        temperature=<span class=\"\
          hljs-number\">0.7</span>,\n    )[<span class=\"hljs-number\">0</span>]\n\
          <span class=\"hljs-built_in\">print</span>(tokenizer.decode(output))\n</code></pre>\n\
          <p>Then, I got the following error:</p>\n<pre><code class=\"language-bash\"\
          >/home/username/anaconda3/envs/pdf-agent/bin/python /mnt/my_raid/github/pdf-agent/llm_jp_13b.py\
          \ \nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 3/3 [00:08&lt;00:00,  2.79s/it]\nThe attention\
          \ mask and the pad token <span class=\"hljs-built_in\">id</span> were not\
          \ <span class=\"hljs-built_in\">set</span>. As a consequence, you may observe\
          \ unexpected behavior. Please pass your input<span class=\"hljs-string\"\
          >'s `attention_mask` to obtain reliable results.</span>\n<span class=\"\
          hljs-string\">Setting `pad_token_id` to `eos_token_id`:7 for open-end generation.</span>\n\
          <span class=\"hljs-string\">Traceback (most recent call last):</span>\n\
          <span class=\"hljs-string\">  File \"/mnt/my_raid/github/pdf-agent/llm_jp_13b.py\"\
          , line 10, in &lt;module&gt;</span>\n<span class=\"hljs-string\">    output\
          \ = model.generate(</span>\n<span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context</span>\n<span class=\"hljs-string\">   \
          \ return func(*args, **kwargs)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1652, in generate</span>\n<span class=\"hljs-string\">    return\
          \ self.sample(</span>\n<span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2734, in sample</span>\n<span class=\"hljs-string\">    outputs =\
          \ self(</span>\n<span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl</span>\n<span class=\"hljs-string\">    return\
          \ forward_call(*args, **kwargs)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 1076, in forward</span>\n<span class=\"hljs-string\">    transformer_outputs\
          \ = self.transformer(</span>\n<span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl</span>\n<span class=\"hljs-string\">    return\
          \ forward_call(*args, **kwargs)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 900, in forward</span>\n<span class=\"hljs-string\">    outputs =\
          \ block(</span>\n<span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl</span>\n<span class=\"hljs-string\">    return\
          \ forward_call(*args, **kwargs)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 389, in forward</span>\n<span class=\"hljs-string\">    hidden_states\
          \ = self.ln_1(hidden_states)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl</span>\n<span class=\"hljs-string\">    return\
          \ forward_call(*args, **kwargs)</span>\n<span class=\"hljs-string\">  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
          , line 190, in forward</span>\n<span class=\"hljs-string\">    return F.layer_norm(</span>\n\
          <span class=\"hljs-string\">  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 2515, in layer_norm</span>\n<span class=\"hljs-string\">    return\
          \ torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)</span>\n\
          <span class=\"hljs-string\">RuntimeError: \"LayerNormKernelImpl\" not implemented\
          \ for '</span>Half<span class=\"hljs-string\">'</span>\n</code></pre>\n\
          <p>But, I can run this model with torch.float32.</p>\n<pre><code class=\"\
          language-python\">- model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.float16)\n+ model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.float32)\n</code></pre>\n<p>And, I can run this model\
          \ with load_in_8bit.</p>\n<pre><code class=\"language-python\">- model =\
          \ AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          + model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>My environment\
          \ is as follows:</p>\n<ul>\n<li>OS: Ubuntu 20.04</li>\n<li>GPU: Core  i5-12400F</li>\n\
          <li>GPU: RTX A6000 48GBx2</li>\n<li>Memory: 128GB</li>\n<li>Pip freeze:<ul>\n\
          <li>accelerate==0.23.0</li>\n<li>bitsandbytes==0.41.1</li>\n<li>tokenizers==0.14.1</li>\n\
          <li>transformers==4.34.1</li>\n<li>torch==2.0.1</li>\n</ul>\n</li>\n</ul>\n\
          <p>What should I do?</p>\n"
        raw: "I cannot run this model with torch.float16. And, the load speed is slower.\n\
          \nI ran the following code:\n```python\nimport torch\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\nmodel_name=\"llm-jp/llm-jp-13b-instruct-full-jaster-v1.0\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.float16) \ntext = '\u81EA\u7136\u8A00\u8A9E\u51E6\u7406\
          \u3068\u306F\u4F55\u304B' \ntext = text + \"### \u56DE\u7B54\uFF1A\"\ntokenized_input\
          \ = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\"\
          ).to(model.device)\nwith torch.no_grad():\n    output = model.generate(\n\
          \        tokenized_input,\n        max_new_tokens=256,\n        do_sample=True,\n\
          \        top_p=0.95,\n        temperature=0.7,\n    )[0]\nprint(tokenizer.decode(output))\n\
          ```\n\nThen, I got the following error:\n```bash\n/home/username/anaconda3/envs/pdf-agent/bin/python\
          \ /mnt/my_raid/github/pdf-agent/llm_jp_13b.py \nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
          \ [00:08<00:00,  2.79s/it]\nThe attention mask and the pad token id were\
          \ not set. As a consequence, you may observe unexpected behavior. Please\
          \ pass your input's `attention_mask` to obtain reliable results.\nSetting\
          \ `pad_token_id` to `eos_token_id`:7 for open-end generation.\nTraceback\
          \ (most recent call last):\n  File \"/mnt/my_raid/github/pdf-agent/llm_jp_13b.py\"\
          , line 10, in <module>\n    output = model.generate(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1652, in generate\n    return self.sample(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2734, in sample\n    outputs = self(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 1076, in forward\n    transformer_outputs = self.transformer(\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 900, in forward\n    outputs = block(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
          , line 389, in forward\n    hidden_states = self.ln_1(hidden_states)\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
          , line 190, in forward\n    return F.layer_norm(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 2515, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n```\n\nBut, I can run this model with torch.float32.\n\
          ```python\n- model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          + model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n\
          ```\nAnd, I can run this model with load_in_8bit.\n```python\n- model =\
          \ AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          + model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)\n\
          ```\n\nMy environment is as follows:\n- OS: Ubuntu 20.04\n- GPU: Core  i5-12400F\n\
          - GPU: RTX A6000 48GBx2\n- Memory: 128GB\n- Pip freeze:\n    - accelerate==0.23.0\n\
          \    - bitsandbytes==0.41.1\n    - tokenizers==0.14.1\n    - transformers==4.34.1\n\
          \    - torch==2.0.1\n\nWhat should I do?"
        updatedAt: '2023-10-20T07:04:25.221Z'
      numEdits: 1
      reactions: []
    id: 653225e6791d5a26115989bb
    type: comment
  author: alfredplpl
  content: "I cannot run this model with torch.float16. And, the load speed is slower.\n\
    \nI ran the following code:\n```python\nimport torch\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\nmodel_name=\"llm-jp/llm-jp-13b-instruct-full-jaster-v1.0\"\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
    \ torch_dtype=torch.float16) \ntext = '\u81EA\u7136\u8A00\u8A9E\u51E6\u7406\u3068\
    \u306F\u4F55\u304B' \ntext = text + \"### \u56DE\u7B54\uFF1A\"\ntokenized_input\
    \ = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\
    with torch.no_grad():\n    output = model.generate(\n        tokenized_input,\n\
    \        max_new_tokens=256,\n        do_sample=True,\n        top_p=0.95,\n \
    \       temperature=0.7,\n    )[0]\nprint(tokenizer.decode(output))\n```\n\nThen,\
    \ I got the following error:\n```bash\n/home/username/anaconda3/envs/pdf-agent/bin/python\
    \ /mnt/my_raid/github/pdf-agent/llm_jp_13b.py \nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:08<00:00,  2.79s/it]\n\
    The attention mask and the pad token id were not set. As a consequence, you may\
    \ observe unexpected behavior. Please pass your input's `attention_mask` to obtain\
    \ reliable results.\nSetting `pad_token_id` to `eos_token_id`:7 for open-end generation.\n\
    Traceback (most recent call last):\n  File \"/mnt/my_raid/github/pdf-agent/llm_jp_13b.py\"\
    , line 10, in <module>\n    output = model.generate(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1652, in generate\n    return self.sample(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2734, in sample\n    outputs = self(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
    , line 1076, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
    , line 900, in forward\n    outputs = block(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\"\
    , line 389, in forward\n    hidden_states = self.ln_1(hidden_states)\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
    , line 190, in forward\n    return F.layer_norm(\n  File \"/home/username/anaconda3/envs/pdf-agent/lib/python3.10/site-packages/torch/nn/functional.py\"\
    , line 2515, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\n```\n\nBut, I can run this model with torch.float32.\n\
    ```python\n- model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
    + model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n\
    ```\nAnd, I can run this model with load_in_8bit.\n```python\n- model = AutoModelForCausalLM.from_pretrained(model_name,\
    \ torch_dtype=torch.float16)\n+ model = AutoModelForCausalLM.from_pretrained(model_name,\
    \ load_in_8bit=True)\n```\n\nMy environment is as follows:\n- OS: Ubuntu 20.04\n\
    - GPU: Core  i5-12400F\n- GPU: RTX A6000 48GBx2\n- Memory: 128GB\n- Pip freeze:\n\
    \    - accelerate==0.23.0\n    - bitsandbytes==0.41.1\n    - tokenizers==0.14.1\n\
    \    - transformers==4.34.1\n    - torch==2.0.1\n\nWhat should I do?"
  created_at: 2023-10-20 06:01:58+00:00
  edited: true
  hidden: false
  id: 653225e6791d5a26115989bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-20T07:18:11.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8054924011230469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: '<p>OK. I got it.<br>I run the code on CPU with torch.float16.<br>For
          example, according to Mr. Sasaki, I should change the code into the following
          code:</p>

          <pre><code class="language-python">- model = AutoModelForCausalLM.from_pretrained(model_name,
          torch_dtype=torch.float16)

          + model = AutoModelForCausalLM.from_pretrained(model_name,  torch_dtype=torch.float16,device_map=<span
          class="hljs-string">"auto"</span>)

          </code></pre>

          <p>The code run on GPU.<br>The problem is solved.</p>

          '
        raw: "OK. I got it.\nI run the code on CPU with torch.float16.\nFor example,\
          \ according to Mr. Sasaki, I should change the code into the following code:\n\
          ```python \n- model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          + model = AutoModelForCausalLM.from_pretrained(model_name,  torch_dtype=torch.float16,device_map=\"\
          auto\")\n```\nThe code run on GPU.\nThe problem is solved."
        updatedAt: '2023-10-20T07:18:11.555Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hkiyomaru
    id: 653229b3453e0c5fb6c57002
    type: comment
  author: alfredplpl
  content: "OK. I got it.\nI run the code on CPU with torch.float16.\nFor example,\
    \ according to Mr. Sasaki, I should change the code into the following code:\n\
    ```python \n- model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
    + model = AutoModelForCausalLM.from_pretrained(model_name,  torch_dtype=torch.float16,device_map=\"\
    auto\")\n```\nThe code run on GPU.\nThe problem is solved."
  created_at: 2023-10-20 06:18:11+00:00
  edited: false
  hidden: false
  id: 653229b3453e0c5fb6c57002
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1629713034466-60e6d3e9dbb8eafe2394cf5b.jpeg?w=200&h=200&f=face
      fullname: Hiroshi Matsuda
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hiroshi-matsuda-rit
      type: user
    createdAt: '2023-10-20T07:20:27.000Z'
    data:
      edited: false
      editors:
      - hiroshi-matsuda-rit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7923769950866699
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1629713034466-60e6d3e9dbb8eafe2394cf5b.jpeg?w=200&h=200&f=face
          fullname: Hiroshi Matsuda
          isHf: false
          isPro: false
          name: hiroshi-matsuda-rit
          type: user
        html: '<p>In your environment, you need to add an argument <code>device_map="auto"</code>
          for <code>AutoModelForCausalLM.from_pretrained()</code> and set the os environmental  variable
          <code>CUDA_VISIBLE_DEVICES=0</code>.</p>

          <p>By the way, which version of python are you using for this environment?</p>

          '
        raw: 'In your environment, you need to add an argument `device_map="auto"`
          for `AutoModelForCausalLM.from_pretrained()` and set the os environmental  variable
          `CUDA_VISIBLE_DEVICES=0`.


          By the way, which version of python are you using for this environment?'
        updatedAt: '2023-10-20T07:20:27.612Z'
      numEdits: 0
      reactions: []
    id: 65322a3b27561a21c8e4bea9
    type: comment
  author: hiroshi-matsuda-rit
  content: 'In your environment, you need to add an argument `device_map="auto"` for
    `AutoModelForCausalLM.from_pretrained()` and set the os environmental  variable
    `CUDA_VISIBLE_DEVICES=0`.


    By the way, which version of python are you using for this environment?'
  created_at: 2023-10-20 06:20:27+00:00
  edited: false
  hidden: false
  id: 65322a3b27561a21c8e4bea9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-20T07:28:26.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8232291340827942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: '<p>Thank you for your reply.</p>

          <p>My python is Python 3.10.13 with anaconda3.</p>

          '
        raw: 'Thank you for your reply.


          My python is Python 3.10.13 with anaconda3.'
        updatedAt: '2023-10-20T07:28:26.052Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hiroshi-matsuda-rit
    id: 65322c1ae78972593349585b
    type: comment
  author: alfredplpl
  content: 'Thank you for your reply.


    My python is Python 3.10.13 with anaconda3.'
  created_at: 2023-10-20 06:28:26+00:00
  edited: false
  hidden: false
  id: 65322c1ae78972593349585b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1629713034466-60e6d3e9dbb8eafe2394cf5b.jpeg?w=200&h=200&f=face
      fullname: Hiroshi Matsuda
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hiroshi-matsuda-rit
      type: user
    createdAt: '2023-10-21T00:23:28.000Z'
    data:
      edited: false
      editors:
      - hiroshi-matsuda-rit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.848878800868988
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1629713034466-60e6d3e9dbb8eafe2394cf5b.jpeg?w=200&h=200&f=face
          fullname: Hiroshi Matsuda
          isHf: false
          isPro: false
          name: hiroshi-matsuda-rit
          type: user
        html: "<p>Thank you for reporting the problem. <span data-props=\"{&quot;user&quot;:&quot;alfredplpl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alfredplpl\"\
          >@<span class=\"underline\">alfredplpl</span></a></span>\n\n\t</span></span><br>We\
          \ have revised the sample codes in hf hub.</p>\n"
        raw: "Thank you for reporting the problem. @alfredplpl \nWe have revised the\
          \ sample codes in hf hub."
        updatedAt: '2023-10-21T00:23:28.565Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alfredplpl
      relatedEventId: 65331a0002d1ecd54541fffe
    id: 65331a0002d1ecd54541fff7
    type: comment
  author: hiroshi-matsuda-rit
  content: "Thank you for reporting the problem. @alfredplpl \nWe have revised the\
    \ sample codes in hf hub."
  created_at: 2023-10-20 23:23:28+00:00
  edited: false
  hidden: false
  id: 65331a0002d1ecd54541fff7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1629713034466-60e6d3e9dbb8eafe2394cf5b.jpeg?w=200&h=200&f=face
      fullname: Hiroshi Matsuda
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hiroshi-matsuda-rit
      type: user
    createdAt: '2023-10-21T00:23:28.000Z'
    data:
      status: closed
    id: 65331a0002d1ecd54541fffe
    type: status-change
  author: hiroshi-matsuda-rit
  created_at: 2023-10-20 23:23:28+00:00
  id: 65331a0002d1ecd54541fffe
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: llm-jp/llm-jp-13b-instruct-full-jaster-v1.0
repo_type: model
status: closed
target_branch: null
title: Cannot run model with torch.float16
