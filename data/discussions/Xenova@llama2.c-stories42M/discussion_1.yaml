!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Noahloghman
conflicting_files: null
created_at: 2023-12-05 23:46:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05e659e1164c3dd916550d4caea09804.svg
      fullname: Brahim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Noahloghman
      type: user
    createdAt: '2023-12-05T23:46:05.000Z'
    data:
      edited: false
      editors:
      - Noahloghman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8417209386825562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05e659e1164c3dd916550d4caea09804.svg
          fullname: Brahim
          isHf: false
          isPro: false
          name: Noahloghman
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Xenova&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Xenova\">@<span class=\"\
          underline\">Xenova</span></a></span>\n\n\t</span></span>,<br>I'm converting\
          \ llama2 7B into onnx, but I have this error : (access token done, all setup\
          \ are ok)<br>[E:onnxruntime:, inference_session.cc:1533 onnxruntime::InferenceSession::Initialize::::operator\
          \ ()] Exception during initialization: D:\\a_work\\1\\s\\onnxruntime\\core\\\
          optimizer\\initializer.cc:31 onnxruntime::Initializer::Initializer !model_path.IsEmpty()\
          \ was false. model_path must not be empty. Ensure that a path is provided\
          \ when the model<br>is created or loaded.</p>\n<p>I did not see in your\
          \ folder the onnx_data files, did you loaded them inside the onnx file?\
          \  in my case I have .onnx files and .onnx_data files inside onnx folder.<br>why\
          \ do you think  I have this error ? I'm using transformers.js .<br>with\
          \ same files everything works good with python but not transformers.js</p>\n\
          <p>I'll appreciate your feedback, Thank you</p>\n"
        raw: "Hi @Xenova,\r\nI'm converting llama2 7B into onnx, but I have this error\
          \ : (access token done, all setup are ok)\r\n[E:onnxruntime:, inference_session.cc:1533\
          \ onnxruntime::InferenceSession::Initialize::<lambda_9a5be43270b854edd3ef320b0y5r7>::operator\
          \ ()] Exception during initialization: D:\\a\\_work\\1\\s\\onnxruntime\\\
          core\\optimizer\\initializer.cc:31 onnxruntime::Initializer::Initializer\
          \ !model_path.IsEmpty() was false. model_path must not be empty. Ensure\
          \ that a path is provided when the model \r\nis created or loaded.\r\n\r\
          \nI did not see in your folder the onnx_data files, did you loaded them\
          \ inside the onnx file?  in my case I have .onnx files and .onnx_data files\
          \ inside onnx folder.\r\nwhy do you think  I have this error ? I'm using\
          \ transformers.js .\r\nwith same files everything works good with python\
          \ but not transformers.js\r\n\r\nI'll appreciate your feedback, Thank you"
        updatedAt: '2023-12-05T23:46:05.316Z'
      numEdits: 0
      reactions: []
    id: 656fb63dd81a20af9c1772a2
    type: comment
  author: Noahloghman
  content: "Hi @Xenova,\r\nI'm converting llama2 7B into onnx, but I have this error\
    \ : (access token done, all setup are ok)\r\n[E:onnxruntime:, inference_session.cc:1533\
    \ onnxruntime::InferenceSession::Initialize::<lambda_9a5be43270b854edd3ef320b0y5r7>::operator\
    \ ()] Exception during initialization: D:\\a\\_work\\1\\s\\onnxruntime\\core\\\
    optimizer\\initializer.cc:31 onnxruntime::Initializer::Initializer !model_path.IsEmpty()\
    \ was false. model_path must not be empty. Ensure that a path is provided when\
    \ the model \r\nis created or loaded.\r\n\r\nI did not see in your folder the\
    \ onnx_data files, did you loaded them inside the onnx file?  in my case I have\
    \ .onnx files and .onnx_data files inside onnx folder.\r\nwhy do you think  I\
    \ have this error ? I'm using transformers.js .\r\nwith same files everything\
    \ works good with python but not transformers.js\r\n\r\nI'll appreciate your feedback,\
    \ Thank you"
  created_at: 2023-12-05 23:46:05+00:00
  edited: false
  hidden: false
  id: 656fb63dd81a20af9c1772a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
      fullname: Joshua
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Xenova
      type: user
    createdAt: '2023-12-05T23:49:27.000Z'
    data:
      edited: false
      editors:
      - Xenova
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409233093261719
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png?w=200&h=200&f=face
          fullname: Joshua
          isHf: true
          isPro: false
          name: Xenova
          type: user
        html: '<p>Hi there. This is most likely due to the limitation in version 1.14.0
          of <code>onnxruntime-web</code> / <code>onnxruntime-node </code>, which
          doesn''t support the external data format (for models &gt; 2GB, weights
          are split into a separate <code>.onnx_data</code> file).</p>

          <p>This will be fixed in a future update which does support the external
          data format. See <a rel="nofollow" href="https://github.com/microsoft/onnxruntime/issues/18586">here</a>
          for a recent issue on the matter.</p>

          '
        raw: 'Hi there. This is most likely due to the limitation in version 1.14.0
          of `onnxruntime-web` / `onnxruntime-node `, which doesn''t support the external
          data format (for models > 2GB, weights are split into a separate `.onnx_data`
          file).


          This will be fixed in a future update which does support the external data
          format. See [here](https://github.com/microsoft/onnxruntime/issues/18586)
          for a recent issue on the matter.'
        updatedAt: '2023-12-05T23:49:27.059Z'
      numEdits: 0
      reactions: []
    id: 656fb70750c011dc11196b47
    type: comment
  author: Xenova
  content: 'Hi there. This is most likely due to the limitation in version 1.14.0
    of `onnxruntime-web` / `onnxruntime-node `, which doesn''t support the external
    data format (for models > 2GB, weights are split into a separate `.onnx_data`
    file).


    This will be fixed in a future update which does support the external data format.
    See [here](https://github.com/microsoft/onnxruntime/issues/18586) for a recent
    issue on the matter.'
  created_at: 2023-12-05 23:49:27+00:00
  edited: false
  hidden: false
  id: 656fb70750c011dc11196b47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05e659e1164c3dd916550d4caea09804.svg
      fullname: Brahim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Noahloghman
      type: user
    createdAt: '2023-12-06T03:11:11.000Z'
    data:
      edited: false
      editors:
      - Noahloghman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560031294822693
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05e659e1164c3dd916550d4caea09804.svg
          fullname: Brahim
          isHf: false
          isPro: false
          name: Noahloghman
          type: user
        html: "<p>Ooh Thank you so much for this information, I spent days trying\
          \ to fix it \U0001F600</p>\n"
        raw: "Ooh Thank you so much for this information, I spent days trying to fix\
          \ it \U0001F600"
        updatedAt: '2023-12-06T03:11:11.774Z'
      numEdits: 0
      reactions: []
    id: 656fe64f8c2bfffc892eee5b
    type: comment
  author: Noahloghman
  content: "Ooh Thank you so much for this information, I spent days trying to fix\
    \ it \U0001F600"
  created_at: 2023-12-06 03:11:11+00:00
  edited: false
  hidden: false
  id: 656fe64f8c2bfffc892eee5b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Xenova/llama2.c-stories42M
repo_type: model
status: open
target_branch: null
title: 'Transformers.js and onnx with llama 2 7B error: [E:onnxruntime:, inference_session.cc:1533
  onnxruntime::InferenceSession'
