!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fergusq
conflicting_files: null
created_at: 2023-02-20 20:20:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633032699204-noauth.png?w=200&h=200&f=face
      fullname: Iikka Hauhio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fergusq
      type: user
    createdAt: '2023-02-20T20:20:41.000Z'
    data:
      edited: false
      editors:
      - fergusq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633032699204-noauth.png?w=200&h=200&f=face
          fullname: Iikka Hauhio
          isHf: false
          isPro: false
          name: fergusq
          type: user
        html: "<p>When trying to use this model, the program complains that it cannot\
          \ open the file. I get similar error with the 8B model, but 3B and smaller\
          \ models seem to work. Is the file corrupt or do I need to open it using\
          \ something else than the standard <code>AutoModel.from_pretrained(\"TurkuNLP/gpt3-finnish-13B\"\
          )</code> code?</p>\n<pre><code>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/transformers/modeling\
          \ \u2502\n\u2502 _utils.py:417 in load_state_dict                      \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502    414 \u2502   \u2502   \u2502   )     \
          \                                                                      \
          \  \u2502\n\u2502    415 \u2502   \u2502   return safe_load_file(checkpoint_file)\
          \                                            \u2502\n\u2502    416 \u2502\
          \   try:                                                               \
          \                   \u2502\n\u2502 \u2771  417 \u2502   \u2502   return\
          \ torch.load(checkpoint_file, map_location=\"cpu\")                    \
          \        \u2502\n\u2502    418 \u2502   except Exception as e:         \
          \                                                       \u2502\n\u2502 \
          \   419 \u2502   \u2502   try:                                         \
          \                                     \u2502\n\u2502    420 \u2502   \u2502\
          \   \u2502   with open(checkpoint_file) as f:                          \
          \                    \u2502\n\u2502                                    \
          \                                                              \u2502\n\u2502\
          \ /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
          \ \u2502\n\u2502 y:777 in load                                         \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502    774 \u2502   \u2502   \u2502   # If we\
          \ want to actually tail call to torch.jit.load, we need to             \
          \ \u2502\n\u2502    775 \u2502   \u2502   \u2502   # reset back to the original\
          \ position.                                        \u2502\n\u2502    776\
          \ \u2502   \u2502   \u2502   orig_position = opened_file.tell()        \
          \                                    \u2502\n\u2502 \u2771  777 \u2502 \
          \  \u2502   \u2502   with _open_zipfile_reader(opened_file) as opened_zipfile:\
          \                     \u2502\n\u2502    778 \u2502   \u2502   \u2502   \u2502\
          \   if _is_torchscript_zip(opened_zipfile):                            \
          \       \u2502\n\u2502    779 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   warnings.warn(\"'torch.load' received a zip file that looks like a To\
          \  \u2502\n\u2502    780 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502     \" dispatching to 'torch.jit.load' (call 'torch.jit.loa\
          \  \u2502\n\u2502                                                      \
          \                                            \u2502\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
          \ \u2502\n\u2502 y:282 in __init__                                     \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502    279                                  \
          \                                                         \u2502\n\u2502\
          \    280 class _open_zipfile_reader(_opener):                          \
          \                            \u2502\n\u2502    281 \u2502   def __init__(self,\
          \ name_or_buffer) -&gt; None:                                          \
          \ \u2502\n\u2502 \u2771  282 \u2502   \u2502   super(_open_zipfile_reader,\
          \ self).__init__(torch._C.PyTorchFileReader(name_or_bu  \u2502\n\u2502 \
          \   283                                                                \
          \                           \u2502\n\u2502    284                      \
          \                                                                     \u2502\
          \n\u2502    285 class _open_zipfile_writer_file(_opener):              \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nRuntimeError:\
          \ PytorchStreamReader failed reading zip archive: failed finding central\
          \ directory\n</code></pre>\n"
        raw: "When trying to use this model, the program complains that it cannot\
          \ open the file. I get similar error with the 8B model, but 3B and smaller\
          \ models seem to work. Is the file corrupt or do I need to open it using\
          \ something else than the standard `AutoModel.from_pretrained(\"TurkuNLP/gpt3-finnish-13B\"\
          )` code?\r\n\r\n```\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
          \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E\r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/transformers/modeling\
          \ \u2502\r\n\u2502 _utils.py:417 in load_state_dict                    \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502    414 \u2502   \u2502   \u2502   )\
          \                                                                      \
          \       \u2502\r\n\u2502    415 \u2502   \u2502   return safe_load_file(checkpoint_file)\
          \                                            \u2502\r\n\u2502    416 \u2502\
          \   try:                                                               \
          \                   \u2502\r\n\u2502 \u2771  417 \u2502   \u2502   return\
          \ torch.load(checkpoint_file, map_location=\"cpu\")                    \
          \        \u2502\r\n\u2502    418 \u2502   except Exception as e:       \
          \                                                         \u2502\r\n\u2502\
          \    419 \u2502   \u2502   try:                                        \
          \                                      \u2502\r\n\u2502    420 \u2502  \
          \ \u2502   \u2502   with open(checkpoint_file) as f:                   \
          \                           \u2502\r\n\u2502                           \
          \                                                                      \
          \ \u2502\r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
          \ \u2502\r\n\u2502 y:777 in load                                       \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502    774 \u2502   \u2502   \u2502   #\
          \ If we want to actually tail call to torch.jit.load, we need to       \
          \       \u2502\r\n\u2502    775 \u2502   \u2502   \u2502   # reset back\
          \ to the original position.                                        \u2502\
          \r\n\u2502    776 \u2502   \u2502   \u2502   orig_position = opened_file.tell()\
          \                                            \u2502\r\n\u2502 \u2771  777\
          \ \u2502   \u2502   \u2502   with _open_zipfile_reader(opened_file) as opened_zipfile:\
          \                     \u2502\r\n\u2502    778 \u2502   \u2502   \u2502 \
          \  \u2502   if _is_torchscript_zip(opened_zipfile):                    \
          \               \u2502\r\n\u2502    779 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   warnings.warn(\"'torch.load' received a zip file that looks\
          \ like a To  \u2502\r\n\u2502    780 \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   \u2502   \u2502     \" dispatching to 'torch.jit.load'\
          \ (call 'torch.jit.loa  \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
          \ \u2502\r\n\u2502 y:282 in __init__                                   \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502    279                            \
          \                                                               \u2502\r\
          \n\u2502    280 class _open_zipfile_reader(_opener):                   \
          \                                   \u2502\r\n\u2502    281 \u2502   def\
          \ __init__(self, name_or_buffer) -> None:                              \
          \             \u2502\r\n\u2502 \u2771  282 \u2502   \u2502   super(_open_zipfile_reader,\
          \ self).__init__(torch._C.PyTorchFileReader(name_or_bu  \u2502\r\n\u2502\
          \    283                                                               \
          \                            \u2502\r\n\u2502    284                   \
          \                                                                      \
          \  \u2502\r\n\u2502    285 class _open_zipfile_writer_file(_opener):   \
          \                                              \u2502\r\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F\r\nRuntimeError: PytorchStreamReader failed reading zip archive:\
          \ failed finding central directory\r\n```"
        updatedAt: '2023-02-20T20:20:41.410Z'
      numEdits: 0
      reactions: []
    id: 63f3d6190be81bdc5d97228e
    type: comment
  author: fergusq
  content: "When trying to use this model, the program complains that it cannot open\
    \ the file. I get similar error with the 8B model, but 3B and smaller models seem\
    \ to work. Is the file corrupt or do I need to open it using something else than\
    \ the standard `AutoModel.from_pretrained(\"TurkuNLP/gpt3-finnish-13B\")` code?\r\
    \n\r\n```\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
    \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/transformers/modeling\
    \ \u2502\r\n\u2502 _utils.py:417 in load_state_dict                          \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502    414 \u2502   \u2502   \u2502   )                       \
    \                                                      \u2502\r\n\u2502    415\
    \ \u2502   \u2502   return safe_load_file(checkpoint_file)                   \
    \                         \u2502\r\n\u2502    416 \u2502   try:              \
    \                                                                    \u2502\r\n\
    \u2502 \u2771  417 \u2502   \u2502   return torch.load(checkpoint_file, map_location=\"\
    cpu\")                            \u2502\r\n\u2502    418 \u2502   except Exception\
    \ as e:                                                                \u2502\r\
    \n\u2502    419 \u2502   \u2502   try:                                       \
    \                                       \u2502\r\n\u2502    420 \u2502   \u2502\
    \   \u2502   with open(checkpoint_file) as f:                                \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
    \ \u2502\r\n\u2502 y:777 in load                                             \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502    774 \u2502   \u2502   \u2502   # If we want to actually\
    \ tail call to torch.jit.load, we need to              \u2502\r\n\u2502    775\
    \ \u2502   \u2502   \u2502   # reset back to the original position.          \
    \                              \u2502\r\n\u2502    776 \u2502   \u2502   \u2502\
    \   orig_position = opened_file.tell()                                       \
    \     \u2502\r\n\u2502 \u2771  777 \u2502   \u2502   \u2502   with _open_zipfile_reader(opened_file)\
    \ as opened_zipfile:                     \u2502\r\n\u2502    778 \u2502   \u2502\
    \   \u2502   \u2502   if _is_torchscript_zip(opened_zipfile):                \
    \                   \u2502\r\n\u2502    779 \u2502   \u2502   \u2502   \u2502\
    \   \u2502   warnings.warn(\"'torch.load' received a zip file that looks like\
    \ a To  \u2502\r\n\u2502    780 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502     \" dispatching to 'torch.jit.load' (call 'torch.jit.loa\
    \  \u2502\r\n\u2502                                                          \
    \                                        \u2502\r\n\u2502 /home/iikkahau/Projektit/miniconda3/envs/fie3/lib/python3.10/site-packages/torch/serialization.p\
    \ \u2502\r\n\u2502 y:282 in __init__                                         \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502    279                                                    \
    \                                       \u2502\r\n\u2502    280 class _open_zipfile_reader(_opener):\
    \                                                      \u2502\r\n\u2502    281\
    \ \u2502   def __init__(self, name_or_buffer) -> None:                       \
    \                    \u2502\r\n\u2502 \u2771  282 \u2502   \u2502   super(_open_zipfile_reader,\
    \ self).__init__(torch._C.PyTorchFileReader(name_or_bu  \u2502\r\n\u2502    283\
    \                                                                            \
    \               \u2502\r\n\u2502    284                                      \
    \                                                     \u2502\r\n\u2502    285\
    \ class _open_zipfile_writer_file(_opener):                                  \
    \               \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
    \r\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding\
    \ central directory\r\n```"
  created_at: 2023-02-20 20:20:41+00:00
  edited: false
  hidden: false
  id: 63f3d6190be81bdc5d97228e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a78ce59506a416e3df1d283a13e5aefa.svg
      fullname: Sampo Pyysalo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: spyysalo
      type: user
    createdAt: '2023-02-21T14:01:03.000Z'
    data:
      edited: false
      editors:
      - spyysalo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a78ce59506a416e3df1d283a13e5aefa.svg
          fullname: Sampo Pyysalo
          isHf: false
          isPro: false
          name: spyysalo
          type: user
        html: '<p>Thank you for noting this, there appears to have been some issue
          with the upload! We''ll reupload ASAP</p>

          '
        raw: Thank you for noting this, there appears to have been some issue with
          the upload! We'll reupload ASAP
        updatedAt: '2023-02-21T14:01:03.571Z'
      numEdits: 0
      reactions: []
    id: 63f4ce9f9801da0debf0cf99
    type: comment
  author: spyysalo
  content: Thank you for noting this, there appears to have been some issue with the
    upload! We'll reupload ASAP
  created_at: 2023-02-21 14:01:03+00:00
  edited: false
  hidden: false
  id: 63f4ce9f9801da0debf0cf99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a78ce59506a416e3df1d283a13e5aefa.svg
      fullname: Sampo Pyysalo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: spyysalo
      type: user
    createdAt: '2023-02-22T12:01:26.000Z'
    data:
      edited: false
      editors:
      - spyysalo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a78ce59506a416e3df1d283a13e5aefa.svg
          fullname: Sampo Pyysalo
          isHf: false
          isPro: false
          name: spyysalo
          type: user
        html: '<p>This appears to have been an issue with git lfs file size limits.
          We''ve now re-uploaded a sharded version of the model, and it appears to
          be working OK via <code>from_pretrained</code>. Please let us know if you
          encounter any further issues!</p>

          '
        raw: This appears to have been an issue with git lfs file size limits. We've
          now re-uploaded a sharded version of the model, and it appears to be working
          OK via `from_pretrained`. Please let us know if you encounter any further
          issues!
        updatedAt: '2023-02-22T12:01:26.581Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fergusq
      relatedEventId: 63f60416bd4f5ad9bc5f7339
    id: 63f60416bd4f5ad9bc5f7338
    type: comment
  author: spyysalo
  content: This appears to have been an issue with git lfs file size limits. We've
    now re-uploaded a sharded version of the model, and it appears to be working OK
    via `from_pretrained`. Please let us know if you encounter any further issues!
  created_at: 2023-02-22 12:01:26+00:00
  edited: false
  hidden: false
  id: 63f60416bd4f5ad9bc5f7338
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a78ce59506a416e3df1d283a13e5aefa.svg
      fullname: Sampo Pyysalo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: spyysalo
      type: user
    createdAt: '2023-02-22T12:01:26.000Z'
    data:
      status: closed
    id: 63f60416bd4f5ad9bc5f7339
    type: status-change
  author: spyysalo
  created_at: 2023-02-22 12:01:26+00:00
  id: 63f60416bd4f5ad9bc5f7339
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TurkuNLP/gpt3-finnish-13B
repo_type: model
status: closed
target_branch: null
title: Model file appears to be corrupt
