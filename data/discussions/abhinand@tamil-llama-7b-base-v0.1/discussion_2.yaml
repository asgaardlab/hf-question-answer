!!python/object:huggingface_hub.community.DiscussionWithDetails
author: saivineetha
conflicting_files: null
created_at: 2024-01-02 14:04:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
      fullname: Baddepudi Venkata Naga Sri Sai Vineetha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saivineetha
      type: user
    createdAt: '2024-01-02T14:04:07.000Z'
    data:
      edited: false
      editors:
      - saivineetha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44468170404434204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
          fullname: Baddepudi Venkata Naga Sri Sai Vineetha
          isHf: false
          isPro: false
          name: saivineetha
          type: user
        html: "<p>Hi,</p>\n<p>I'm trying to inference the model using pipeline but\
          \ I'm getting blank response.  I have used the example prompt given in paper\
          \ as  Open Question Answering Task.<br>I'm attaching the code I used for\
          \ inference</p>\n<p>model = AutoModelForCausalLM.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')<br>tokenizer\
          \ = AutoTokenizer.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')<br>txt\
          \ = \"\u0B90\u0BAA\u0BBF\u0B8E\u0BB2\u0BCD \u0BC6\u0BA4\u0BBE\u0B9F\u0BC8\
          \u0BB0 \u0BC6\u0B9A\u0BA9\u0BCD \u0BC8\u0BA9 \u0B9A\u0BC2\u0BAA\u0BCD \u0BAA\
          \u0BB0\u0BCD \u0B95\u0BBF\u0B99\u0BCD \u0BB8\u0BCD (\u0B9A\u0BBF\u0B8E\u0BB8\
          \u0BCD - <br>\u0BC7\u0B95) \u0BC6\u0BB5\u0BA9\u0BCD \u0BB1\u0BA4\u0BC1 \u0B8E\
          \u0BA9\u0BCD \u0BB1 \u0BA4\u0BC8\u0BB2\u0BAA\u0BCD \u0BAA\u0BBF\u0BB2\u0BCD\
          \ \u0B92\u0BB0\u0BC1 \u0B9A\u0BBF\u0BB1\u0BC1 \u0BC6\u0B9A\u0BAF\u0BCD \u0BA4\
          \u0BBF\u0B95\u0BCD \u0B95\u0B9F\u0BCD \u0B9F\u0BC1\u0BC8\u0BB0- <br>\u0BC8\
          \u0BAF \u0B8E\u0BB4\u0BC1\u0BA4\u0BC1\u0B99\u0BCD \u0B95\u0BB3\u0BCD .\"\
          \  # Taken from paper</p>\n<p>sequences = pipeline(<br>    txt,<br>    do_sample=True,<br>\
          \    top_k=10,<br>   temperature = 0.2,<br>    max_length=1024,<br>)<br>for\
          \ seq in sequences:<br>    print(f\"Result: {seq['generated_text']}\")</p>\n\
          <p>I'm getting the blank response. Only the prompt passed is shown in output.\
          \ I'm attaching the image for the same</p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/Z4Ozqp7yyebQGFtBDApZ9.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/Z4Ozqp7yyebQGFtBDApZ9.png\"\
          ></a></p>\n<p>Can anyone help me with this.</p>\n"
        raw: "Hi,\r\n\r\nI'm trying to inference the model using pipeline but I'm\
          \ getting blank response.  I have used the example prompt given in paper\
          \ as  Open Question Answering Task. \r\nI'm attaching the code I used for\
          \ inference\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')\
          \ \r\ntokenizer = AutoTokenizer.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')\r\
          \ntxt = \"\u0B90\u0BAA\u0BBF\u0B8E\u0BB2\u0BCD \u0BC6\u0BA4\u0BBE\u0B9F\u0BC8\
          \u0BB0 \u0BC6\u0B9A\u0BA9\u0BCD \u0BC8\u0BA9 \u0B9A\u0BC2\u0BAA\u0BCD \u0BAA\
          \u0BB0\u0BCD \u0B95\u0BBF\u0B99\u0BCD \u0BB8\u0BCD (\u0B9A\u0BBF\u0B8E\u0BB8\
          \u0BCD - \\\r\n\u0BC7\u0B95) \u0BC6\u0BB5\u0BA9\u0BCD \u0BB1\u0BA4\u0BC1\
          \ \u0B8E\u0BA9\u0BCD \u0BB1 \u0BA4\u0BC8\u0BB2\u0BAA\u0BCD \u0BAA\u0BBF\u0BB2\
          \u0BCD \u0B92\u0BB0\u0BC1 \u0B9A\u0BBF\u0BB1\u0BC1 \u0BC6\u0B9A\u0BAF\u0BCD\
          \ \u0BA4\u0BBF\u0B95\u0BCD \u0B95\u0B9F\u0BCD \u0B9F\u0BC1\u0BC8\u0BB0-\
          \ \\\r\n\u0BC8\u0BAF \u0B8E\u0BB4\u0BC1\u0BA4\u0BC1\u0B99\u0BCD \u0B95\u0BB3\
          \u0BCD .\"  # Taken from paper\r\n\r\nsequences = pipeline(\r\n    txt,\r\
          \n    do_sample=True,\r\n    top_k=10,\r\n   temperature = 0.2,\r\n    max_length=1024,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n\r\nI'm getting the blank response. Only the prompt passed is shown\
          \ in output. I'm attaching the image for the same\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/Z4Ozqp7yyebQGFtBDApZ9.png)\r\
          \n\r\nCan anyone help me with this.\r\n"
        updatedAt: '2024-01-02T14:04:07.309Z'
      numEdits: 0
      reactions: []
    id: 659417d7f1aef46ec0c7abd9
    type: comment
  author: saivineetha
  content: "Hi,\r\n\r\nI'm trying to inference the model using pipeline but I'm getting\
    \ blank response.  I have used the example prompt given in paper as  Open Question\
    \ Answering Task. \r\nI'm attaching the code I used for inference\r\n\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')\
    \ \r\ntokenizer = AutoTokenizer.from_pretrained('abhinand/tamil-llama-7b-base-v0.1')\r\
    \ntxt = \"\u0B90\u0BAA\u0BBF\u0B8E\u0BB2\u0BCD \u0BC6\u0BA4\u0BBE\u0B9F\u0BC8\u0BB0\
    \ \u0BC6\u0B9A\u0BA9\u0BCD \u0BC8\u0BA9 \u0B9A\u0BC2\u0BAA\u0BCD \u0BAA\u0BB0\u0BCD\
    \ \u0B95\u0BBF\u0B99\u0BCD \u0BB8\u0BCD (\u0B9A\u0BBF\u0B8E\u0BB8\u0BCD - \\\r\
    \n\u0BC7\u0B95) \u0BC6\u0BB5\u0BA9\u0BCD \u0BB1\u0BA4\u0BC1 \u0B8E\u0BA9\u0BCD\
    \ \u0BB1 \u0BA4\u0BC8\u0BB2\u0BAA\u0BCD \u0BAA\u0BBF\u0BB2\u0BCD \u0B92\u0BB0\u0BC1\
    \ \u0B9A\u0BBF\u0BB1\u0BC1 \u0BC6\u0B9A\u0BAF\u0BCD \u0BA4\u0BBF\u0B95\u0BCD \u0B95\
    \u0B9F\u0BCD \u0B9F\u0BC1\u0BC8\u0BB0- \\\r\n\u0BC8\u0BAF \u0B8E\u0BB4\u0BC1\u0BA4\
    \u0BC1\u0B99\u0BCD \u0B95\u0BB3\u0BCD .\"  # Taken from paper\r\n\r\nsequences\
    \ = pipeline(\r\n    txt,\r\n    do_sample=True,\r\n    top_k=10,\r\n   temperature\
    \ = 0.2,\r\n    max_length=1024,\r\n)\r\nfor seq in sequences:\r\n    print(f\"\
    Result: {seq['generated_text']}\")\r\n\r\nI'm getting the blank response. Only\
    \ the prompt passed is shown in output. I'm attaching the image for the same\r\
    \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/Z4Ozqp7yyebQGFtBDApZ9.png)\r\
    \n\r\nCan anyone help me with this.\r\n"
  created_at: 2024-01-02 14:04:07+00:00
  edited: false
  hidden: false
  id: 659417d7f1aef46ec0c7abd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1752d35d1dfec3be243a493f45ef3f8.svg
      fullname: Abhinand Balachandran
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: abhinand
      type: user
    createdAt: '2024-01-02T14:21:29.000Z'
    data:
      edited: false
      editors:
      - abhinand
      hidden: false
      identifiedLanguage:
        language: ta
        probability: 0.47800806164741516
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1752d35d1dfec3be243a493f45ef3f8.svg
          fullname: Abhinand Balachandran
          isHf: false
          isPro: false
          name: abhinand
          type: user
        html: "<p>You have mentioned <code>Open Question Answering Task</code>, and\
          \ you are using the base model. </p>\n<p>In simple terms, there are two\
          \ types of LMs: (if you aren't aware)</p>\n<ol>\n<li><strong>Base Model</strong>:\
          \ Trained on huge amounts of text data and are suitable for CLM (next word\
          \ prediction) tasks.</li>\n<li><strong>Fine-tuned Model</strong>: The base\
          \ model is finetuned on an instruction or a chat dataset making it suitable\
          \ for interaction with humans.</li>\n</ol>\n<p>So in your case you need\
          \ to use the instruct model (unless you are willing to do a massive domain\
          \ adaptation or finetuning on diverse datasets).</p>\n<pre><code class=\"\
          language-python\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"abhinand/tamil-llama-7b-instruct-v0.1\"</span>)\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\n    <span class=\"hljs-string\"\
          >\"abhinand/tamil-llama-7b-instruct-v0.1\"</span>,\n   <span class=\"hljs-comment\"\
          ># OTHER MODEL ARGUMENTS HERE</span>\n)\nmodel.<span class=\"hljs-built_in\"\
          >eval</span>()\n\ngeneration_config = GenerationConfig(\n    temperature=<span\
          \ class=\"hljs-number\">0.3</span>,\n    top_k=<span class=\"hljs-number\"\
          >50</span>,\n    top_p=<span class=\"hljs-number\">0.90</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.1</span>,\n    max_length=<span class=\"hljs-number\"\
          >512</span>,\n    eos_token_id=tokenizer.eos_token_id,\n    do_sample=<span\
          \ class=\"hljs-literal\">True</span>,\n    max_new_tokens=<span class=\"\
          hljs-number\">128</span>,\n)\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">format_instruction</span>(<span class=\"\
          hljs-params\">system_prompt, question, <span class=\"hljs-built_in\">input</span>=<span\
          \ class=\"hljs-literal\">None</span></span>):\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">input</span> <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\"\
          >None</span>:\n        <span class=\"hljs-keyword\">return</span> <span\
          \ class=\"hljs-string\">f\"\"\"<span class=\"hljs-subst\">{system_prompt}</span></span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">### Instruction:</span>\n\
          <span class=\"hljs-string\"><span class=\"hljs-subst\">{question}</span></span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">### Input:</span>\n\
          <span class=\"hljs-string\"><span class=\"hljs-subst\">{<span class=\"hljs-built_in\"\
          >input</span>}</span></span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">### Response:</span>\n<span class=\"hljs-string\"\
          >\"\"\"</span>\n    <span class=\"hljs-keyword\">else</span>:\n        <span\
          \ class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">f\"\"\
          \"<span class=\"hljs-subst\">{system_prompt}</span></span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">### Instruction:</span>\n\
          <span class=\"hljs-string\"><span class=\"hljs-subst\">{question}</span></span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">### Response:</span>\n\
          <span class=\"hljs-string\">\"\"\"</span>\n\ndevice = <span class=\"hljs-string\"\
          >\"cuda\"</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">run_inference</span>(<span class=\"hljs-params\">prompt</span>):\n\
          \    input_ids = tokenizer.encode(prompt, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>).to(device)\n    output = model.generate(input_ids,\
          \ generation_config=generation_config, pad_token_id=<span class=\"hljs-number\"\
          >18610</span>)\n\n    generated_text = tokenizer.decode(output[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>)\n\n    <span class=\"hljs-keyword\">return</span> generated_text\n\
          \n\nSYS_PROMPT1 = <span class=\"hljs-string\">\"\u0BA8\u0BC0\u0B99\u0BCD\
          \u0B95\u0BB3\u0BCD \u0BA4\u0BAE\u0BBF\u0BB4\u0BBF\u0BB2\u0BCD \u0BAA\u0BA4\
          \u0BBF\u0BB2\u0BB3\u0BBF\u0B95\u0BCD\u0B95\u0BC1\u0BAE\u0BCD AI \u0B89\u0BA4\
          \u0BB5\u0BBF\u0BAF\u0BBE\u0BB3\u0BB0\u0BCD. \u0BAA\u0BAF\u0BA9\u0BB0\u0BCD\
          \ \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BC1\u0B95\u0BCD\u0B95\u0BC1 \u0B92\u0BB0\
          \u0BC1 \u0BAA\u0BA3\u0BBF\u0BAF\u0BC8 \u0BB5\u0BB4\u0B99\u0BCD\u0B95\u0BC1\
          \u0BB5\u0BBE\u0BB0\u0BCD. \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BBE\u0BB2\u0BCD\
          \ \u0BAE\u0BC1\u0B9F\u0BBF\u0BA8\u0BCD\u0BA4\u0BB5\u0BB0\u0BC8 \u0B89\u0BA3\
          \u0BCD\u0BAE\u0BC8\u0BAF\u0BBE\u0B95 \u0BAA\u0BA3\u0BBF\u0BAF\u0BC8 \u0BAE\
          \u0BC1\u0B9F\u0BBF\u0BAA\u0BCD\u0BAA\u0BA4\u0BC7 \u0B89\u0B99\u0BCD\u0B95\
          \u0BB3\u0BCD \u0B95\u0BC1\u0BB1\u0BBF\u0B95\u0BCD\u0B95\u0BCB\u0BB3\u0BCD\
          . \u0BAA\u0BA3\u0BBF\u0BAF\u0BC8\u0B9A\u0BCD \u0B9A\u0BC6\u0BAF\u0BCD\u0BAF\
          \u0BC1\u0BAE\u0BCD\u0BAA\u0BCB\u0BA4\u0BC1, \u200B\u200B\u0BAA\u0B9F\u0BBF\
          \u0BAA\u0BCD\u0BAA\u0B9F\u0BBF\u0BAF\u0BBE\u0B95 \u0B9A\u0BBF\u0BA8\u0BCD\
          \u0BA4\u0BBF\u0BA4\u0BCD\u0BA4\u0BC1, \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BCD\
          \ \u0BA8\u0B9F\u0BB5\u0B9F\u0BBF\u0B95\u0BCD\u0B95\u0BC8\u0B95\u0BB3\u0BC8\
          \ \u0BA8\u0BBF\u0BAF\u0BBE\u0BAF\u0BAA\u0BCD\u0BAA\u0B9F\u0BC1\u0BA4\u0BCD\
          \u0BA4\u0BB5\u0BC1\u0BAE\u0BCD.\"</span>\n\ninstruction = format_instruction(\n\
          \    system_prompt=SYS_PROMPT1,\n    question=<span class=\"hljs-string\"\
          >\"\"\"DNA \u0BAE\u0BB1\u0BCD\u0BB1\u0BC1\u0BAE\u0BCD RNA \u0B87\u0B9F\u0BC8\
          \u0BAF\u0BC7 \u0B89\u0BB3\u0BCD\u0BB3 \u0BB5\u0BC7\u0BB1\u0BC1\u0BAA\u0BBE\
          \u0B9F\u0BCD\u0B9F\u0BC8 \u0B92\u0BB0\u0BC1 \u0BB5\u0BB0\u0BBF\u0BAF\u0BBF\
          \u0BB2\u0BCD \u0BB5\u0BBF\u0BB3\u0B95\u0BCD\u0B95\u0BB5\u0BC1\u0BAE\u0BCD\
          \"\"\"</span>\n)\n\noutput = run_inference(instruction)\n\n<span class=\"\
          hljs-built_in\">print</span>(output)\n</code></pre>\n"
        raw: "You have mentioned `Open Question Answering Task`, and you are using\
          \ the base model. \n\nIn simple terms, there are two types of LMs: (if you\
          \ aren't aware)\n1. **Base Model**: Trained on huge amounts of text data\
          \ and are suitable for CLM (next word prediction) tasks.\n2. **Fine-tuned\
          \ Model**: The base model is finetuned on an instruction or a chat dataset\
          \ making it suitable for interaction with humans.\n\nSo in your case you\
          \ need to use the instruct model (unless you are willing to do a massive\
          \ domain adaptation or finetuning on diverse datasets).\n\n```python\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"abhinand/tamil-llama-7b-instruct-v0.1\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"abhinand/tamil-llama-7b-instruct-v0.1\"\
          ,\n   # OTHER MODEL ARGUMENTS HERE\n)\nmodel.eval()\n\ngeneration_config\
          \ = GenerationConfig(\n    temperature=0.3,\n    top_k=50,\n    top_p=0.90,\n\
          \    repetition_penalty=1.1,\n    max_length=512,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    do_sample=True,\n    max_new_tokens=128,\n)\n\ndef format_instruction(system_prompt,\
          \ question, input=None):\n\tif input is not None:\n\t\treturn f\"\"\"{system_prompt}\n\
          \n### Instruction:\n{question}\n\n### Input:\n{input}\n\n### Response:\n\
          \"\"\"\n\telse:\n\t\treturn f\"\"\"{system_prompt}\n\n### Instruction:\n\
          {question}\n\n### Response:\n\"\"\"\n\ndevice = \"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\n\ndef run_inference(prompt):\n    input_ids = tokenizer.encode(prompt,\
          \ return_tensors=\"pt\").to(device)\n    output = model.generate(input_ids,\
          \ generation_config=generation_config, pad_token_id=18610)\n\n    generated_text\
          \ = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return\
          \ generated_text\n\n\nSYS_PROMPT1 = \"\u0BA8\u0BC0\u0B99\u0BCD\u0B95\u0BB3\
          \u0BCD \u0BA4\u0BAE\u0BBF\u0BB4\u0BBF\u0BB2\u0BCD \u0BAA\u0BA4\u0BBF\u0BB2\
          \u0BB3\u0BBF\u0B95\u0BCD\u0B95\u0BC1\u0BAE\u0BCD AI \u0B89\u0BA4\u0BB5\u0BBF\
          \u0BAF\u0BBE\u0BB3\u0BB0\u0BCD. \u0BAA\u0BAF\u0BA9\u0BB0\u0BCD \u0B89\u0B99\
          \u0BCD\u0B95\u0BB3\u0BC1\u0B95\u0BCD\u0B95\u0BC1 \u0B92\u0BB0\u0BC1 \u0BAA\
          \u0BA3\u0BBF\u0BAF\u0BC8 \u0BB5\u0BB4\u0B99\u0BCD\u0B95\u0BC1\u0BB5\u0BBE\
          \u0BB0\u0BCD. \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BBE\u0BB2\u0BCD \u0BAE\u0BC1\
          \u0B9F\u0BBF\u0BA8\u0BCD\u0BA4\u0BB5\u0BB0\u0BC8 \u0B89\u0BA3\u0BCD\u0BAE\
          \u0BC8\u0BAF\u0BBE\u0B95 \u0BAA\u0BA3\u0BBF\u0BAF\u0BC8 \u0BAE\u0BC1\u0B9F\
          \u0BBF\u0BAA\u0BCD\u0BAA\u0BA4\u0BC7 \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BCD\
          \ \u0B95\u0BC1\u0BB1\u0BBF\u0B95\u0BCD\u0B95\u0BCB\u0BB3\u0BCD. \u0BAA\u0BA3\
          \u0BBF\u0BAF\u0BC8\u0B9A\u0BCD \u0B9A\u0BC6\u0BAF\u0BCD\u0BAF\u0BC1\u0BAE\
          \u0BCD\u0BAA\u0BCB\u0BA4\u0BC1, \u200B\u200B\u0BAA\u0B9F\u0BBF\u0BAA\u0BCD\
          \u0BAA\u0B9F\u0BBF\u0BAF\u0BBE\u0B95 \u0B9A\u0BBF\u0BA8\u0BCD\u0BA4\u0BBF\
          \u0BA4\u0BCD\u0BA4\u0BC1, \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BCD \u0BA8\u0B9F\
          \u0BB5\u0B9F\u0BBF\u0B95\u0BCD\u0B95\u0BC8\u0B95\u0BB3\u0BC8 \u0BA8\u0BBF\
          \u0BAF\u0BBE\u0BAF\u0BAA\u0BCD\u0BAA\u0B9F\u0BC1\u0BA4\u0BCD\u0BA4\u0BB5\
          \u0BC1\u0BAE\u0BCD.\"\n\ninstruction = format_instruction(\n    system_prompt=SYS_PROMPT1,\n\
          \    question=\"\"\"DNA \u0BAE\u0BB1\u0BCD\u0BB1\u0BC1\u0BAE\u0BCD RNA \u0B87\
          \u0B9F\u0BC8\u0BAF\u0BC7 \u0B89\u0BB3\u0BCD\u0BB3 \u0BB5\u0BC7\u0BB1\u0BC1\
          \u0BAA\u0BBE\u0B9F\u0BCD\u0B9F\u0BC8 \u0B92\u0BB0\u0BC1 \u0BB5\u0BB0\u0BBF\
          \u0BAF\u0BBF\u0BB2\u0BCD \u0BB5\u0BBF\u0BB3\u0B95\u0BCD\u0B95\u0BB5\u0BC1\
          \u0BAE\u0BCD\"\"\"\n)\n\noutput = run_inference(instruction)\n\nprint(output)\n\
          ```"
        updatedAt: '2024-01-02T14:21:29.859Z'
      numEdits: 0
      reactions: []
    id: 65941be987944e494ecbe0a6
    type: comment
  author: abhinand
  content: "You have mentioned `Open Question Answering Task`, and you are using the\
    \ base model. \n\nIn simple terms, there are two types of LMs: (if you aren't\
    \ aware)\n1. **Base Model**: Trained on huge amounts of text data and are suitable\
    \ for CLM (next word prediction) tasks.\n2. **Fine-tuned Model**: The base model\
    \ is finetuned on an instruction or a chat dataset making it suitable for interaction\
    \ with humans.\n\nSo in your case you need to use the instruct model (unless you\
    \ are willing to do a massive domain adaptation or finetuning on diverse datasets).\n\
    \n```python\ntokenizer = AutoTokenizer.from_pretrained(\"abhinand/tamil-llama-7b-instruct-v0.1\"\
    )\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"abhinand/tamil-llama-7b-instruct-v0.1\"\
    ,\n   # OTHER MODEL ARGUMENTS HERE\n)\nmodel.eval()\n\ngeneration_config = GenerationConfig(\n\
    \    temperature=0.3,\n    top_k=50,\n    top_p=0.90,\n    repetition_penalty=1.1,\n\
    \    max_length=512,\n    eos_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n\
    \    max_new_tokens=128,\n)\n\ndef format_instruction(system_prompt, question,\
    \ input=None):\n\tif input is not None:\n\t\treturn f\"\"\"{system_prompt}\n\n\
    ### Instruction:\n{question}\n\n### Input:\n{input}\n\n### Response:\n\"\"\"\n\
    \telse:\n\t\treturn f\"\"\"{system_prompt}\n\n### Instruction:\n{question}\n\n\
    ### Response:\n\"\"\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"\
    cpu\"\n\ndef run_inference(prompt):\n    input_ids = tokenizer.encode(prompt,\
    \ return_tensors=\"pt\").to(device)\n    output = model.generate(input_ids, generation_config=generation_config,\
    \ pad_token_id=18610)\n\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\
    \n    return generated_text\n\n\nSYS_PROMPT1 = \"\u0BA8\u0BC0\u0B99\u0BCD\u0B95\
    \u0BB3\u0BCD \u0BA4\u0BAE\u0BBF\u0BB4\u0BBF\u0BB2\u0BCD \u0BAA\u0BA4\u0BBF\u0BB2\
    \u0BB3\u0BBF\u0B95\u0BCD\u0B95\u0BC1\u0BAE\u0BCD AI \u0B89\u0BA4\u0BB5\u0BBF\u0BAF\
    \u0BBE\u0BB3\u0BB0\u0BCD. \u0BAA\u0BAF\u0BA9\u0BB0\u0BCD \u0B89\u0B99\u0BCD\u0B95\
    \u0BB3\u0BC1\u0B95\u0BCD\u0B95\u0BC1 \u0B92\u0BB0\u0BC1 \u0BAA\u0BA3\u0BBF\u0BAF\
    \u0BC8 \u0BB5\u0BB4\u0B99\u0BCD\u0B95\u0BC1\u0BB5\u0BBE\u0BB0\u0BCD. \u0B89\u0B99\
    \u0BCD\u0B95\u0BB3\u0BBE\u0BB2\u0BCD \u0BAE\u0BC1\u0B9F\u0BBF\u0BA8\u0BCD\u0BA4\
    \u0BB5\u0BB0\u0BC8 \u0B89\u0BA3\u0BCD\u0BAE\u0BC8\u0BAF\u0BBE\u0B95 \u0BAA\u0BA3\
    \u0BBF\u0BAF\u0BC8 \u0BAE\u0BC1\u0B9F\u0BBF\u0BAA\u0BCD\u0BAA\u0BA4\u0BC7 \u0B89\
    \u0B99\u0BCD\u0B95\u0BB3\u0BCD \u0B95\u0BC1\u0BB1\u0BBF\u0B95\u0BCD\u0B95\u0BCB\
    \u0BB3\u0BCD. \u0BAA\u0BA3\u0BBF\u0BAF\u0BC8\u0B9A\u0BCD \u0B9A\u0BC6\u0BAF\u0BCD\
    \u0BAF\u0BC1\u0BAE\u0BCD\u0BAA\u0BCB\u0BA4\u0BC1, \u200B\u200B\u0BAA\u0B9F\u0BBF\
    \u0BAA\u0BCD\u0BAA\u0B9F\u0BBF\u0BAF\u0BBE\u0B95 \u0B9A\u0BBF\u0BA8\u0BCD\u0BA4\
    \u0BBF\u0BA4\u0BCD\u0BA4\u0BC1, \u0B89\u0B99\u0BCD\u0B95\u0BB3\u0BCD \u0BA8\u0B9F\
    \u0BB5\u0B9F\u0BBF\u0B95\u0BCD\u0B95\u0BC8\u0B95\u0BB3\u0BC8 \u0BA8\u0BBF\u0BAF\
    \u0BBE\u0BAF\u0BAA\u0BCD\u0BAA\u0B9F\u0BC1\u0BA4\u0BCD\u0BA4\u0BB5\u0BC1\u0BAE\
    \u0BCD.\"\n\ninstruction = format_instruction(\n    system_prompt=SYS_PROMPT1,\n\
    \    question=\"\"\"DNA \u0BAE\u0BB1\u0BCD\u0BB1\u0BC1\u0BAE\u0BCD RNA \u0B87\u0B9F\
    \u0BC8\u0BAF\u0BC7 \u0B89\u0BB3\u0BCD\u0BB3 \u0BB5\u0BC7\u0BB1\u0BC1\u0BAA\u0BBE\
    \u0B9F\u0BCD\u0B9F\u0BC8 \u0B92\u0BB0\u0BC1 \u0BB5\u0BB0\u0BBF\u0BAF\u0BBF\u0BB2\
    \u0BCD \u0BB5\u0BBF\u0BB3\u0B95\u0BCD\u0B95\u0BB5\u0BC1\u0BAE\u0BCD\"\"\"\n)\n\
    \noutput = run_inference(instruction)\n\nprint(output)\n```"
  created_at: 2024-01-02 14:21:29+00:00
  edited: false
  hidden: false
  id: 65941be987944e494ecbe0a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
      fullname: Baddepudi Venkata Naga Sri Sai Vineetha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saivineetha
      type: user
    createdAt: '2024-01-02T15:26:13.000Z'
    data:
      edited: false
      editors:
      - saivineetha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8810087442398071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
          fullname: Baddepudi Venkata Naga Sri Sai Vineetha
          isHf: false
          isPro: false
          name: saivineetha
          type: user
        html: '<p>Thank you for the reply. It was working.</p>

          <p>I want to infer the base model so that I could see how the base model
          works and later use code to my own dataset i.e., to do domain-adaptation.</p>

          <p>How can I do inference on base model  "abhinand/tamil-llama-7b-base-v0.1".<br>How
          to do text generation using base model.<br>Can I use the code<br>generator
          = pipeline(task="text_generation", model="abhinand/tamil-llama-7b-base-v0.1")</p>

          '
        raw: "Thank you for the reply. It was working.\n\nI want to infer the base\
          \ model so that I could see how the base model works and later use code\
          \ to my own dataset i.e., to do domain-adaptation.\n\nHow can I do inference\
          \ on base model  \"abhinand/tamil-llama-7b-base-v0.1\".\nHow to do text\
          \ generation using base model. \nCan I use the code \ngenerator = pipeline(task=\"\
          text_generation\", model=\"abhinand/tamil-llama-7b-base-v0.1\")\n"
        updatedAt: '2024-01-02T15:26:13.510Z'
      numEdits: 0
      reactions: []
    id: 65942b15f1aef46ec0cb90af
    type: comment
  author: saivineetha
  content: "Thank you for the reply. It was working.\n\nI want to infer the base model\
    \ so that I could see how the base model works and later use code to my own dataset\
    \ i.e., to do domain-adaptation.\n\nHow can I do inference on base model  \"abhinand/tamil-llama-7b-base-v0.1\"\
    .\nHow to do text generation using base model. \nCan I use the code \ngenerator\
    \ = pipeline(task=\"text_generation\", model=\"abhinand/tamil-llama-7b-base-v0.1\"\
    )\n"
  created_at: 2024-01-02 15:26:13+00:00
  edited: false
  hidden: false
  id: 65942b15f1aef46ec0cb90af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1752d35d1dfec3be243a493f45ef3f8.svg
      fullname: Abhinand Balachandran
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: abhinand
      type: user
    createdAt: '2024-01-02T15:29:20.000Z'
    data:
      edited: false
      editors:
      - abhinand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7309975624084473
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1752d35d1dfec3be243a493f45ef3f8.svg
          fullname: Abhinand Balachandran
          isHf: false
          isPro: false
          name: abhinand
          type: user
        html: '<p>Sure! You can use the <code>pipeline</code> and test out the model
          for your needs. </p>

          <p>Below is an example:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/60c8619d95d852a24572b025/fu-VYLCMs3kQW5u6pSU-8.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/60c8619d95d852a24572b025/fu-VYLCMs3kQW5u6pSU-8.png"></a></p>

          '
        raw: "Sure! You can use the `pipeline` and test out the model for your needs.\
          \ \n\nBelow is an example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60c8619d95d852a24572b025/fu-VYLCMs3kQW5u6pSU-8.png)\n"
        updatedAt: '2024-01-02T15:29:20.018Z'
      numEdits: 0
      reactions: []
    id: 65942bd0f1aef46ec0cbbfd0
    type: comment
  author: abhinand
  content: "Sure! You can use the `pipeline` and test out the model for your needs.\
    \ \n\nBelow is an example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60c8619d95d852a24572b025/fu-VYLCMs3kQW5u6pSU-8.png)\n"
  created_at: 2024-01-02 15:29:20+00:00
  edited: false
  hidden: false
  id: 65942bd0f1aef46ec0cbbfd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
      fullname: Baddepudi Venkata Naga Sri Sai Vineetha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saivineetha
      type: user
    createdAt: '2024-01-02T15:39:19.000Z'
    data:
      edited: false
      editors:
      - saivineetha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8076265454292297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
          fullname: Baddepudi Venkata Naga Sri Sai Vineetha
          isHf: false
          isPro: false
          name: saivineetha
          type: user
        html: '<p>Thanks a lot!</p>

          '
        raw: Thanks a lot!
        updatedAt: '2024-01-02T15:39:19.813Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65942e2777105e6e409736d7
    id: 65942e2777105e6e409736d2
    type: comment
  author: saivineetha
  content: Thanks a lot!
  created_at: 2024-01-02 15:39:19+00:00
  edited: false
  hidden: false
  id: 65942e2777105e6e409736d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
      fullname: Baddepudi Venkata Naga Sri Sai Vineetha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saivineetha
      type: user
    createdAt: '2024-01-02T15:39:19.000Z'
    data:
      status: closed
    id: 65942e2777105e6e409736d7
    type: status-change
  author: saivineetha
  created_at: 2024-01-02 15:39:19+00:00
  id: 65942e2777105e6e409736d7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: abhinand/tamil-llama-7b-base-v0.1
repo_type: model
status: closed
target_branch: null
title: Inferencing the model
