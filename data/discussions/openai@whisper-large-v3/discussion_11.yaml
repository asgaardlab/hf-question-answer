!!python/object:huggingface_hub.community.DiscussionWithDetails
author: souvik0306
conflicting_files: null
created_at: 2023-11-10 11:29:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/5LilSOacmZeRRIIfimi7s.png?w=200&h=200&f=face
      fullname: Souvik Datta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: souvik0306
      type: user
    createdAt: '2023-11-10T11:29:51.000Z'
    data:
      edited: false
      editors:
      - souvik0306
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.882496178150177
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/5LilSOacmZeRRIIfimi7s.png?w=200&h=200&f=face
          fullname: Souvik Datta
          isHf: false
          isPro: false
          name: souvik0306
          type: user
        html: '<p>Large-v3 is very fast with batching as shown here --- <a href="https://huggingface.co/openai/whisper-large-v3">https://huggingface.co/openai/whisper-large-v3</a></p>

          <p>Batching speeds up the transcription process by a lot. The only reason
          I wish to use faster_whisper is cause it provides things like verbose, word
          level transcription</p>

          <p>Additionally support for various input params like best_of, beam_size
          etc all of which are supported by whisper - <a rel="nofollow" href="https://github.com/openai/whisper/blob/main/whisper/transcribe.py">https://github.com/openai/whisper/blob/main/whisper/transcribe.py</a></p>

          '
        raw: "Large-v3 is very fast with batching as shown here --- https://huggingface.co/openai/whisper-large-v3\r\
          \n\r\nBatching speeds up the transcription process by a lot. The only reason\
          \ I wish to use faster_whisper is cause it provides things like verbose,\
          \ word level transcription\r\n\r\nAdditionally support for various input\
          \ params like best_of, beam_size etc all of which are supported by whisper\
          \ - https://github.com/openai/whisper/blob/main/whisper/transcribe.py"
        updatedAt: '2023-11-10T11:29:51.128Z'
      numEdits: 0
      reactions: []
    id: 654e142f0c5bd4a9e8c9f252
    type: comment
  author: souvik0306
  content: "Large-v3 is very fast with batching as shown here --- https://huggingface.co/openai/whisper-large-v3\r\
    \n\r\nBatching speeds up the transcription process by a lot. The only reason I\
    \ wish to use faster_whisper is cause it provides things like verbose, word level\
    \ transcription\r\n\r\nAdditionally support for various input params like best_of,\
    \ beam_size etc all of which are supported by whisper - https://github.com/openai/whisper/blob/main/whisper/transcribe.py"
  created_at: 2023-11-10 11:29:51+00:00
  edited: false
  hidden: false
  id: 654e142f0c5bd4a9e8c9f252
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4ca1ce24e024f0503ea20c3ca3cbaea.svg
      fullname: Dheshan Mohandass
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dheshan
      type: user
    createdAt: '2023-11-10T14:57:39.000Z'
    data:
      edited: false
      editors:
      - dheshan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6820754408836365
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4ca1ce24e024f0503ea20c3ca3cbaea.svg
          fullname: Dheshan Mohandass
          isHf: false
          isPro: false
          name: dheshan
          type: user
        html: "<p>Using word level transcriptions, as specified in the Model Card:</p>\n\
          <pre><code class=\"language-python\">result = pipe(sample, return_timestamps=<span\
          \ class=\"hljs-string\">\"word\"</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(result[<span class=\"hljs-string\">\"chunks\"</span>])\n</code></pre>\n\
          <p>Should give you word level timestamps, something like:</p>\n<pre><code\
          \ class=\"language-json\"><span class=\"hljs-punctuation\">{</span>\n  \
          \  <span class=\"hljs-attr\">\"text\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\" the\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"timestamp\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \        <span class=\"hljs-number\">187.6</span><span class=\"hljs-punctuation\"\
          >,</span>\n        <span class=\"hljs-number\">188.64</span>\n    <span\
          \ class=\"hljs-punctuation\">]</span>\n<span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n<span class=\"hljs-punctuation\"\
          >{</span>\n    <span class=\"hljs-attr\">\"text\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\" fact\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"timestamp\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \        <span class=\"hljs-number\">188.64</span><span class=\"hljs-punctuation\"\
          >,</span>\n        <span class=\"hljs-number\">188.88</span>\n    <span\
          \ class=\"hljs-punctuation\">]</span>\n<span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n</code></pre>\n<p>This\
          \ is very similar to what you get with the whisper model, which looks something\
          \ like:</p>\n<pre><code class=\"language-json\"><span class=\"hljs-punctuation\"\
          >{</span>\n            <span class=\"hljs-attr\">\"id\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">0</span><span class=\"\
          hljs-punctuation\">,</span>\n            <span class=\"hljs-attr\">\"seek\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >0</span><span class=\"hljs-punctuation\">,</span>\n            <span class=\"\
          hljs-attr\">\"start\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">0.0</span><span class=\"hljs-punctuation\">,</span>\n\
          \            <span class=\"hljs-attr\">\"end\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">3.0</span><span class=\"hljs-punctuation\"\
          >,</span>\n            <span class=\"hljs-attr\">\"text\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-string\">\" Okay, so I've\
          \ started recording.\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \            <span class=\"hljs-attr\">\"tokens\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">[</span>\n                <span\
          \ class=\"hljs-number\">50364</span><span class=\"hljs-punctuation\">,</span>\n\
          \                <span class=\"hljs-number\">1033</span><span class=\"hljs-punctuation\"\
          >,</span>\n                <span class=\"hljs-number\">11</span><span class=\"\
          hljs-punctuation\">,</span>\n                ...\n                <span\
          \ class=\"hljs-number\">13</span><span class=\"hljs-punctuation\">,</span>\n\
          \                <span class=\"hljs-number\">50524</span>\n            <span\
          \ class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n\
          \            <span class=\"hljs-attr\">\"temperature\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">0.0</span><span\
          \ class=\"hljs-punctuation\">,</span>\n            <span class=\"hljs-attr\"\
          >\"avg_logprob\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">-0.43806132332223363</span><span class=\"hljs-punctuation\"\
          >,</span>\n            <span class=\"hljs-attr\">\"compression_ratio\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">1.2953020134228188</span><span\
          \ class=\"hljs-punctuation\">,</span>\n            <span class=\"hljs-attr\"\
          >\"no_speech_prob\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">0.1916283816099167</span><span class=\"hljs-punctuation\"\
          >,</span>\n            <span class=\"hljs-attr\">\"words\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \                <span class=\"hljs-punctuation\">{</span>\n           \
          \         <span class=\"hljs-attr\">\"word\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\" Okay,\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n                    <span class=\"hljs-attr\">\"start\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">0.0</span><span\
          \ class=\"hljs-punctuation\">,</span>\n                    <span class=\"\
          hljs-attr\">\"end\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">0.56</span><span class=\"hljs-punctuation\">,</span>\n\
          \                    <span class=\"hljs-attr\">\"probability\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">0.12234115600585938</span>\n\
          \                <span class=\"hljs-punctuation\">}</span><span class=\"\
          hljs-punctuation\">,</span>\n               ...\n                <span class=\"\
          hljs-punctuation\">{</span>\n                    <span class=\"hljs-attr\"\
          >\"word\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\" recording.\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \                    <span class=\"hljs-attr\">\"start\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">2.44</span><span\
          \ class=\"hljs-punctuation\">,</span>\n                    <span class=\"\
          hljs-attr\">\"end\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">3.0</span><span class=\"hljs-punctuation\">,</span>\n\
          \                    <span class=\"hljs-attr\">\"probability\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">0.8062686920166016</span>\n\
          \                <span class=\"hljs-punctuation\">}</span>\n           \
          \ <span class=\"hljs-punctuation\">]</span>\n        <span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n</code></pre>\n<p>While\
          \ the whisper model does provide more information, and other input params\
          \ might still not be available, word timestamps are currently possible</p>\n"
        raw: "Using word level transcriptions, as specified in the Model Card:\n\n\
          ```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"\
          chunks\"])\n```\n\nShould give you word level timestamps, something like:\n\
          ```json\n{\n    \"text\": \" the\",\n    \"timestamp\": [\n        187.6,\n\
          \        188.64\n    ]\n},\n{\n    \"text\": \" fact\",\n    \"timestamp\"\
          : [\n        188.64,\n        188.88\n    ]\n},\n```\n\nThis is very similar\
          \ to what you get with the whisper model, which looks something like:\n\
          ```json\n{\n            \"id\": 0,\n            \"seek\": 0,\n         \
          \   \"start\": 0.0,\n            \"end\": 3.0,\n            \"text\": \"\
          \ Okay, so I've started recording.\",\n            \"tokens\": [\n     \
          \           50364,\n                1033,\n                11,\n       \
          \         ...\n                13,\n                50524\n            ],\n\
          \            \"temperature\": 0.0,\n            \"avg_logprob\": -0.43806132332223363,\n\
          \            \"compression_ratio\": 1.2953020134228188,\n            \"\
          no_speech_prob\": 0.1916283816099167,\n            \"words\": [\n      \
          \          {\n                    \"word\": \" Okay,\",\n              \
          \      \"start\": 0.0,\n                    \"end\": 0.56,\n           \
          \         \"probability\": 0.12234115600585938\n                },\n   \
          \            ...\n                {\n                    \"word\": \" recording.\"\
          ,\n                    \"start\": 2.44,\n                    \"end\": 3.0,\n\
          \                    \"probability\": 0.8062686920166016\n             \
          \   }\n            ]\n        },\n```\n\nWhile the whisper model does provide\
          \ more information, and other input params might still not be available,\
          \ word timestamps are currently possible"
        updatedAt: '2023-11-10T14:57:39.668Z'
      numEdits: 0
      reactions: []
    id: 654e44e3073bd982c0c78781
    type: comment
  author: dheshan
  content: "Using word level transcriptions, as specified in the Model Card:\n\n```python\n\
    result = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n\
    ```\n\nShould give you word level timestamps, something like:\n```json\n{\n  \
    \  \"text\": \" the\",\n    \"timestamp\": [\n        187.6,\n        188.64\n\
    \    ]\n},\n{\n    \"text\": \" fact\",\n    \"timestamp\": [\n        188.64,\n\
    \        188.88\n    ]\n},\n```\n\nThis is very similar to what you get with the\
    \ whisper model, which looks something like:\n```json\n{\n            \"id\":\
    \ 0,\n            \"seek\": 0,\n            \"start\": 0.0,\n            \"end\"\
    : 3.0,\n            \"text\": \" Okay, so I've started recording.\",\n       \
    \     \"tokens\": [\n                50364,\n                1033,\n         \
    \       11,\n                ...\n                13,\n                50524\n\
    \            ],\n            \"temperature\": 0.0,\n            \"avg_logprob\"\
    : -0.43806132332223363,\n            \"compression_ratio\": 1.2953020134228188,\n\
    \            \"no_speech_prob\": 0.1916283816099167,\n            \"words\": [\n\
    \                {\n                    \"word\": \" Okay,\",\n              \
    \      \"start\": 0.0,\n                    \"end\": 0.56,\n                 \
    \   \"probability\": 0.12234115600585938\n                },\n               ...\n\
    \                {\n                    \"word\": \" recording.\",\n         \
    \           \"start\": 2.44,\n                    \"end\": 3.0,\n            \
    \        \"probability\": 0.8062686920166016\n                }\n            ]\n\
    \        },\n```\n\nWhile the whisper model does provide more information, and\
    \ other input params might still not be available, word timestamps are currently\
    \ possible"
  created_at: 2023-11-10 14:57:39+00:00
  edited: false
  hidden: false
  id: 654e44e3073bd982c0c78781
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/5LilSOacmZeRRIIfimi7s.png?w=200&h=200&f=face
      fullname: Souvik Datta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: souvik0306
      type: user
    createdAt: '2023-11-10T16:20:42.000Z'
    data:
      edited: true
      editors:
      - souvik0306
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4505921006202698
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/5LilSOacmZeRRIIfimi7s.png?w=200&h=200&f=face
          fullname: Souvik Datta
          isHf: false
          isPro: false
          name: souvik0306
          type: user
        html: '<p>I actually got an error --<br>result = pipe(sample, return_timestamps="word")<br>print(result["chunks"])</p>

          <p>I got the error with GPU, the pipeline code remains the same as model
          card</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/YFqu71ETrmTHJvlMy2fXC.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/YFqu71ETrmTHJvlMy2fXC.png"></a></p>

          '
        raw: 'I actually got an error --

          result = pipe(sample, return_timestamps="word")

          print(result["chunks"])


          I got the error with GPU, the pipeline code remains the same as model card


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/YFqu71ETrmTHJvlMy2fXC.png)

          '
        updatedAt: '2023-11-10T16:21:13.747Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - adilJ
      - count: 1
        reaction: "\U0001F44D"
        users:
        - paniedziejek
    id: 654e585a6ee27944d5354c14
    type: comment
  author: souvik0306
  content: 'I actually got an error --

    result = pipe(sample, return_timestamps="word")

    print(result["chunks"])


    I got the error with GPU, the pipeline code remains the same as model card


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ba46aa0a9866b28cb19a14/YFqu71ETrmTHJvlMy2fXC.png)

    '
  created_at: 2023-11-10 16:20:42+00:00
  edited: true
  hidden: false
  id: 654e585a6ee27944d5354c14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/020a1f243af516a992428d4687757a9e.svg
      fullname: Piotr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: piotrpolska
      type: user
    createdAt: '2023-11-14T12:11:29.000Z'
    data:
      edited: false
      editors:
      - piotrpolska
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5554469227790833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/020a1f243af516a992428d4687757a9e.svg
          fullname: Piotr
          isHf: false
          isPro: false
          name: piotrpolska
          type: user
        html: '<p>Use batch_size=1<br>results=pipe(batch_size=1,</p>

          '
        raw: 'Use batch_size=1

          results=pipe(batch_size=1,'
        updatedAt: '2023-11-14T12:11:29.573Z'
      numEdits: 0
      reactions: []
    id: 655363f170fb926b75cf188a
    type: comment
  author: piotrpolska
  content: 'Use batch_size=1

    results=pipe(batch_size=1,'
  created_at: 2023-11-14 12:11:29+00:00
  edited: false
  hidden: false
  id: 655363f170fb926b75cf188a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: How to get Word and Verbose level transcription?
