!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Oscaarjs
conflicting_files: null
created_at: 2023-11-17 14:21:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674633436381-62fa62118cd542e895b833f9.png?w=200&h=200&f=face
      fullname: Oscar Johansson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Oscaarjs
      type: user
    createdAt: '2023-11-17T14:21:01.000Z'
    data:
      edited: false
      editors:
      - Oscaarjs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7022249102592468
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674633436381-62fa62118cd542e895b833f9.png?w=200&h=200&f=face
          fullname: Oscar Johansson
          isHf: false
          isPro: false
          name: Oscaarjs
          type: user
        html: "<p>Is it possible to set different languages for different files in\
          \ when processing files in a batch?</p>\n<p>I.e; I can do </p>\n<pre><code>pipe\
          \ = pipeline(\n            \"automatic-speech-recognition\",\n         \
          \   model=self.model,\n            tokenizer=self.processor.tokenizer,\n\
          \            feature_extractor=self.processor.feature_extractor,\n     \
          \       torch_dtype=self.torch_dtype,\n            device=self.device,\n\
          \        )\n</code></pre>\n<p>and then</p>\n<pre><code>pipe(\n         \
          \   files,\n            chunk_length_s=self.config.get(\"chunk_length_s\"\
          , 30),\n            batch_size=self.config.get(\"batch_size\", 24),\n  \
          \          return_timestamps=True,\n            return_language=True,\n\
          \            generate_kwargs={\"language\": \"en\"},\n        )\n</code></pre>\n\
          <p>Where files is a list of paths to files.</p>\n<p>But this applies for\
          \ all files. Is it possible to somehow set it individually for each file?\
          \ I can ofc use a batch_size of 1 and just process it iteratively with different\
          \ kwargs for each file, but I'd like to get the speed-up that the batching\
          \ might entail.</p>\n"
        raw: "Is it possible to set different languages for different files in when\
          \ processing files in a batch?\r\n\r\nI.e; I can do \r\n\r\n```\r\npipe\
          \ = pipeline(\r\n            \"automatic-speech-recognition\",\r\n     \
          \       model=self.model,\r\n            tokenizer=self.processor.tokenizer,\r\
          \n            feature_extractor=self.processor.feature_extractor,\r\n  \
          \          torch_dtype=self.torch_dtype,\r\n            device=self.device,\r\
          \n        )\r\n```\r\nand then\r\n\r\n```\r\npipe(\r\n            files,\r\
          \n            chunk_length_s=self.config.get(\"chunk_length_s\", 30),\r\n\
          \            batch_size=self.config.get(\"batch_size\", 24),\r\n       \
          \     return_timestamps=True,\r\n            return_language=True,\r\n \
          \           generate_kwargs={\"language\": \"en\"},\r\n        )\r\n```\r\
          \nWhere files is a list of paths to files.\r\n\r\nBut this applies for all\
          \ files. Is it possible to somehow set it individually for each file? I\
          \ can ofc use a batch_size of 1 and just process it iteratively with different\
          \ kwargs for each file, but I'd like to get the speed-up that the batching\
          \ might entail."
        updatedAt: '2023-11-17T14:21:01.227Z'
      numEdits: 0
      reactions: []
    id: 655776cd60cb377db05c2dd3
    type: comment
  author: Oscaarjs
  content: "Is it possible to set different languages for different files in when\
    \ processing files in a batch?\r\n\r\nI.e; I can do \r\n\r\n```\r\npipe = pipeline(\r\
    \n            \"automatic-speech-recognition\",\r\n            model=self.model,\r\
    \n            tokenizer=self.processor.tokenizer,\r\n            feature_extractor=self.processor.feature_extractor,\r\
    \n            torch_dtype=self.torch_dtype,\r\n            device=self.device,\r\
    \n        )\r\n```\r\nand then\r\n\r\n```\r\npipe(\r\n            files,\r\n \
    \           chunk_length_s=self.config.get(\"chunk_length_s\", 30),\r\n      \
    \      batch_size=self.config.get(\"batch_size\", 24),\r\n            return_timestamps=True,\r\
    \n            return_language=True,\r\n            generate_kwargs={\"language\"\
    : \"en\"},\r\n        )\r\n```\r\nWhere files is a list of paths to files.\r\n\
    \r\nBut this applies for all files. Is it possible to somehow set it individually\
    \ for each file? I can ofc use a batch_size of 1 and just process it iteratively\
    \ with different kwargs for each file, but I'd like to get the speed-up that the\
    \ batching might entail."
  created_at: 2023-11-17 14:21:01+00:00
  edited: false
  hidden: false
  id: 655776cd60cb377db05c2dd3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Batched files with different languages
