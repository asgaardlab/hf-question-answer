!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dkincaid
conflicting_files: null
created_at: 2023-12-21 03:47:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634322030054-noauth.jpeg?w=200&h=200&f=face
      fullname: David Kincaid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dkincaid
      type: user
    createdAt: '2023-12-21T03:47:00.000Z'
    data:
      edited: false
      editors:
      - dkincaid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7667550444602966
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634322030054-noauth.jpeg?w=200&h=200&f=face
          fullname: David Kincaid
          isHf: false
          isPro: true
          name: dkincaid
          type: user
        html: "<p>I've been trying to deploy on Sagemaker and can't seem to get it\
          \ to work once deployed.</p>\n<p>I keep getting this error:</p>\n<pre><code>ModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\n  \"code\"\
          : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Wrong\
          \ index found for \\u003c|0.02|\\u003e: should be None but found 50366.\"\
          \n</code></pre>\n<p>I can't find much about this error anywhere, but was\
          \ wondering if it had to do with a transformers version problem. </p>\n\
          <p>Here's the code I'm using:</p>\n<pre><code class=\"language-python\"\
          >hub = {\n    <span class=\"hljs-string\">'HF_MODEL_ID'</span>:<span class=\"\
          hljs-string\">'openai/whisper-large-v3'</span>,\n    <span class=\"hljs-string\"\
          >'HF_TASK'</span>:<span class=\"hljs-string\">'automatic-speech-recognition'</span>\n\
          }\n\n<span class=\"hljs-comment\"># create Hugging Face Model Class</span>\n\
          huggingface_model = HuggingFaceModel(\n    transformers_version=<span class=\"\
          hljs-string\">'4.26.0'</span>,\n    pytorch_version=<span class=\"hljs-string\"\
          >'1.13.1'</span>,\n    py_version=<span class=\"hljs-string\">'py39'</span>,\n\
          \    env=hub,\n    role=role, \n)\n\n<span class=\"hljs-comment\"># deploy\
          \ model to SageMaker Inference</span>\naudio_serializer = DataSerializer(content_type=<span\
          \ class=\"hljs-string\">'audio/x-audio'</span>)\npredictor = huggingface_model.deploy(\n\
          \    initial_instance_count=<span class=\"hljs-number\">1</span>, <span\
          \ class=\"hljs-comment\"># number of instances</span>\n    instance_type=<span\
          \ class=\"hljs-string\">'ml.g4dn.xlarge'</span>, <span class=\"hljs-comment\"\
          ># ec2 instance type</span>\n    serializer=audio_serializer\n)\n</code></pre>\n"
        raw: "I've been trying to deploy on Sagemaker and can't seem to get it to\
          \ work once deployed.\r\n\r\nI keep getting this error:\r\n\r\n```\r\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\r\n  \"code\"\
          : 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\
          Wrong index found for \\u003c|0.02|\\u003e: should be None but found 50366.\"\
          \r\n```\r\n\r\nI can't find much about this error anywhere, but was wondering\
          \ if it had to do with a transformers version problem. \r\n\r\nHere's the\
          \ code I'm using:\r\n```python\r\nhub = {\r\n\t'HF_MODEL_ID':'openai/whisper-large-v3',\r\
          \n\t'HF_TASK':'automatic-speech-recognition'\r\n}\r\n\r\n# create Hugging\
          \ Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.26.0',\r\
          \n\tpytorch_version='1.13.1',\r\n\tpy_version='py39',\r\n\tenv=hub,\r\n\t\
          role=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\naudio_serializer\
          \ = DataSerializer(content_type='audio/x-audio')\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1, # number of instances\r\n\tinstance_type='ml.g4dn.xlarge',\
          \ # ec2 instance type\r\n    serializer=audio_serializer\r\n)\r\n```"
        updatedAt: '2023-12-21T03:47:00.202Z'
      numEdits: 0
      reactions: []
    id: 6583b53492a21e7694638362
    type: comment
  author: dkincaid
  content: "I've been trying to deploy on Sagemaker and can't seem to get it to work\
    \ once deployed.\r\n\r\nI keep getting this error:\r\n\r\n```\r\nModelError: An\
    \ error occurred (ModelError) when calling the InvokeEndpoint operation: Received\
    \ client error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"\
    type\": \"InternalServerException\",\r\n  \"message\": \"Wrong index found for\
    \ \\u003c|0.02|\\u003e: should be None but found 50366.\"\r\n```\r\n\r\nI can't\
    \ find much about this error anywhere, but was wondering if it had to do with\
    \ a transformers version problem. \r\n\r\nHere's the code I'm using:\r\n```python\r\
    \nhub = {\r\n\t'HF_MODEL_ID':'openai/whisper-large-v3',\r\n\t'HF_TASK':'automatic-speech-recognition'\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\ttransformers_version='4.26.0',\r\n\tpytorch_version='1.13.1',\r\n\tpy_version='py39',\r\
    \n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \naudio_serializer = DataSerializer(content_type='audio/x-audio')\r\npredictor\
    \ = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number of instances\r\
    \n\tinstance_type='ml.g4dn.xlarge', # ec2 instance type\r\n    serializer=audio_serializer\r\
    \n)\r\n```"
  created_at: 2023-12-21 03:47:00+00:00
  edited: false
  hidden: false
  id: 6583b53492a21e7694638362
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da679db22adc601fd561959d42d337d3.svg
      fullname: Gerald Horn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghorn
      type: user
    createdAt: '2023-12-22T01:31:30.000Z'
    data:
      edited: true
      editors:
      - ghorn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7778913974761963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da679db22adc601fd561959d42d337d3.svg
          fullname: Gerald Horn
          isHf: false
          isPro: false
          name: ghorn
          type: user
        html: "<p>I've been banging my head against this same problem all week. As\
          \ best I can tell, the \"Deploy this model using SageMaker SDK\" instructions\
          \ are incorrect.</p>\n<p>In particular, it seems the AWS Deep Learning Containers\
          \ only support up to <code>transformers</code> version <code>4.26.0</code>,\
          \ which is too low.</p>\n<p>I've been following these guides and deploying\
          \ a <code>model.tar.gz</code> that essentially consists of only an <code>code/inference.py</code>\
          \ and <code>code/requirements.txt</code> file so that I can force using\
          \ <code>transformers==4.36.2</code>, which does seem to work<br><a rel=\"\
          nofollow\" href=\"https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\"\
          >https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules</a><br><a\
          \ rel=\"nofollow\" href=\"https://aws.amazon.com/blogs/machine-learning/hugging-face-on-amazon-sagemaker-bring-your-own-scripts-and-data/\"\
          >https://aws.amazon.com/blogs/machine-learning/hugging-face-on-amazon-sagemaker-bring-your-own-scripts-and-data/</a></p>\n\
          <p>The <code>inference.py</code> pretty much just follows the getting started\
          \ instructions, looks more or less like</p>\n<pre><code class=\"language-python\"\
          >device = <span class=\"hljs-string\">\"cuda:0\"</span> <span class=\"hljs-keyword\"\
          >if</span> torch.cuda.is_available() <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-string\">\"cpu\"</span>\ntorch_dtype = torch.float16\
          \ <span class=\"hljs-keyword\">if</span> torch.cuda.is_available() <span\
          \ class=\"hljs-keyword\">else</span> torch.float32\n\nmodel_id = <span class=\"\
          hljs-string\">\"openai/whisper-large-v3\"</span>\ntask = <span class=\"\
          hljs-string\">\"automatic-speech-recognition\"</span>\n\n<span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">model_fn</span>(<span\
          \ class=\"hljs-params\">model_dir</span>):\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>, use_safetensors=<span class=\"hljs-literal\"\
          >True</span>\n    )\n    model.to(device)\n\n    processor = AutoProcessor.from_pretrained(model_id)\n\
          \n    <span class=\"hljs-keyword\">return</span> pipeline(\n        task,\n\
          \        model=model,\n        tokenizer=processor.tokenizer,\n        feature_extractor=processor.feature_extractor,\n\
          \        return_timestamps=<span class=\"hljs-literal\">True</span>,\n \
          \       torch_dtype=torch_dtype,\n        device=device,\n    )\n</code></pre>\n\
          <p>And actually now that I'm thinking about it, you may be able to just\
          \ get away with having the <code>requirements.txt</code> file and setting\
          \ the <code>HF_MODEL_ID</code> and <code>HF_TASK</code> environment variables...</p>\n"
        raw: "I've been banging my head against this same problem all week. As best\
          \ I can tell, the \"Deploy this model using SageMaker SDK\" instructions\
          \ are incorrect.\n\nIn particular, it seems the AWS Deep Learning Containers\
          \ only support up to `transformers` version `4.26.0`, which is too low.\n\
          \nI've been following these guides and deploying a `model.tar.gz` that essentially\
          \ consists of only an `code/inference.py` and `code/requirements.txt` file\
          \ so that I can force using `transformers==4.36.2`, which does seem to work\n\
          https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n\
          https://aws.amazon.com/blogs/machine-learning/hugging-face-on-amazon-sagemaker-bring-your-own-scripts-and-data/\n\
          \nThe `inference.py` pretty much just follows the getting started instructions,\
          \ looks more or less like\n\n```python\ndevice = \"cuda:0\" if torch.cuda.is_available()\
          \ else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available()\
          \ else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\ntask = \"\
          automatic-speech-recognition\"\n\ndef model_fn(model_dir):\n    model =\
          \ AutoModelForSpeechSeq2Seq.from_pretrained(\n        model_id, torch_dtype=torch_dtype,\
          \ low_cpu_mem_usage=True, use_safetensors=True\n    )\n    model.to(device)\n\
          \n    processor = AutoProcessor.from_pretrained(model_id)\n\n    return\
          \ pipeline(\n        task,\n        model=model,\n        tokenizer=processor.tokenizer,\n\
          \        feature_extractor=processor.feature_extractor,\n        return_timestamps=True,\n\
          \        torch_dtype=torch_dtype,\n        device=device,\n    )\n```\n\n\
          And actually now that I'm thinking about it, you may be able to just get\
          \ away with having the `requirements.txt` file and setting the `HF_MODEL_ID`\
          \ and `HF_TASK` environment variables..."
        updatedAt: '2023-12-22T01:34:32.586Z'
      numEdits: 1
      reactions: []
    id: 6584e6f20292cbbde24d89a1
    type: comment
  author: ghorn
  content: "I've been banging my head against this same problem all week. As best\
    \ I can tell, the \"Deploy this model using SageMaker SDK\" instructions are incorrect.\n\
    \nIn particular, it seems the AWS Deep Learning Containers only support up to\
    \ `transformers` version `4.26.0`, which is too low.\n\nI've been following these\
    \ guides and deploying a `model.tar.gz` that essentially consists of only an `code/inference.py`\
    \ and `code/requirements.txt` file so that I can force using `transformers==4.36.2`,\
    \ which does seem to work\nhttps://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n\
    https://aws.amazon.com/blogs/machine-learning/hugging-face-on-amazon-sagemaker-bring-your-own-scripts-and-data/\n\
    \nThe `inference.py` pretty much just follows the getting started instructions,\
    \ looks more or less like\n\n```python\ndevice = \"cuda:0\" if torch.cuda.is_available()\
    \ else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else\
    \ torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\ntask = \"automatic-speech-recognition\"\
    \n\ndef model_fn(model_dir):\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
    \        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
    \    )\n    model.to(device)\n\n    processor = AutoProcessor.from_pretrained(model_id)\n\
    \n    return pipeline(\n        task,\n        model=model,\n        tokenizer=processor.tokenizer,\n\
    \        feature_extractor=processor.feature_extractor,\n        return_timestamps=True,\n\
    \        torch_dtype=torch_dtype,\n        device=device,\n    )\n```\n\nAnd actually\
    \ now that I'm thinking about it, you may be able to just get away with having\
    \ the `requirements.txt` file and setting the `HF_MODEL_ID` and `HF_TASK` environment\
    \ variables..."
  created_at: 2023-12-22 01:31:30+00:00
  edited: true
  hidden: false
  id: 6584e6f20292cbbde24d89a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634322030054-noauth.jpeg?w=200&h=200&f=face
      fullname: David Kincaid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dkincaid
      type: user
    createdAt: '2023-12-22T02:39:27.000Z'
    data:
      edited: false
      editors:
      - dkincaid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.996343731880188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634322030054-noauth.jpeg?w=200&h=200&f=face
          fullname: David Kincaid
          isHf: false
          isPro: true
          name: dkincaid
          type: user
        html: '<p>Wow, this is great! Thank you so much for posting this. I didn''t
          know you could do this. </p>

          '
        raw: 'Wow, this is great! Thank you so much for posting this. I didn''t know
          you could do this. '
        updatedAt: '2023-12-22T02:39:27.474Z'
      numEdits: 0
      reactions: []
    id: 6584f6dffde12510fc7d3c2a
    type: comment
  author: dkincaid
  content: 'Wow, this is great! Thank you so much for posting this. I didn''t know
    you could do this. '
  created_at: 2023-12-22 02:39:27+00:00
  edited: false
  hidden: false
  id: 6584f6dffde12510fc7d3c2a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 58
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Does whisper-large-v3 work on Sagemaker?
