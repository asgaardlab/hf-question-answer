!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andyweiqiu
conflicting_files: null
created_at: 2023-11-20 03:14:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
      fullname: qiuwei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andyweiqiu
      type: user
    createdAt: '2023-11-20T03:14:59.000Z'
    data:
      edited: false
      editors:
      - andyweiqiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8843415975570679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
          fullname: qiuwei
          isHf: false
          isPro: false
          name: andyweiqiu
          type: user
        html: '<p>Now I have finetune large-v3 and the resulting model is model-00001-of-00002.safetensors
          and model-00002-of-00002.safetensors. How can I use these two models for
          evaluation?</p>

          '
        raw: Now I have finetune large-v3 and the resulting model is model-00001-of-00002.safetensors
          and model-00002-of-00002.safetensors. How can I use these two models for
          evaluation?
        updatedAt: '2023-11-20T03:14:59.028Z'
      numEdits: 0
      reactions: []
    id: 655acf33b4ab8556cf15e665
    type: comment
  author: andyweiqiu
  content: Now I have finetune large-v3 and the resulting model is model-00001-of-00002.safetensors
    and model-00002-of-00002.safetensors. How can I use these two models for evaluation?
  created_at: 2023-11-20 03:14:59+00:00
  edited: false
  hidden: false
  id: 655acf33b4ab8556cf15e665
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-20T17:50:11.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7587645649909973
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;andyweiqiu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andyweiqiu\"\
          >@<span class=\"underline\">andyweiqiu</span></a></span>\n\n\t</span></span>\
          \ - these are the <a href=\"https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\"\
          >sharded weights</a> split by the <code>.save_pretrained</code> method.\
          \ To use them for inference, simply follow the code snippets in the README,\
          \ replacing <code>model_id</code> with the repo id or path to your save\
          \ checkpoint: <a href=\"https://huggingface.co/openai/whisper-large-v3#usage\"\
          >https://huggingface.co/openai/whisper-large-v3#usage</a></p>\n<p>E.g. if\
          \ I trained a model and saved it under <code>sanchit-gandhi/whisper-large-v3-hi</code>,\
          \ I would set:</p>\n<pre><code class=\"language-python\">model_id = <span\
          \ class=\"hljs-string\">\"sanchit-gandhi/whisper-large-v3-hi\"</span>\n\
          </code></pre>\n<p>And keep the rest of the code example un-changed.</p>\n"
        raw: 'Hey @andyweiqiu - these are the [sharded weights](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size)
          split by the `.save_pretrained` method. To use them for inference, simply
          follow the code snippets in the README, replacing `model_id` with the repo
          id or path to your save checkpoint: https://huggingface.co/openai/whisper-large-v3#usage


          E.g. if I trained a model and saved it under `sanchit-gandhi/whisper-large-v3-hi`,
          I would set:

          ```python

          model_id = "sanchit-gandhi/whisper-large-v3-hi"

          ```

          And keep the rest of the code example un-changed.'
        updatedAt: '2023-11-20T17:50:11.867Z'
      numEdits: 0
      reactions: []
    id: 655b9c53f6b4c0428c872a49
    type: comment
  author: sanchit-gandhi
  content: 'Hey @andyweiqiu - these are the [sharded weights](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size)
    split by the `.save_pretrained` method. To use them for inference, simply follow
    the code snippets in the README, replacing `model_id` with the repo id or path
    to your save checkpoint: https://huggingface.co/openai/whisper-large-v3#usage


    E.g. if I trained a model and saved it under `sanchit-gandhi/whisper-large-v3-hi`,
    I would set:

    ```python

    model_id = "sanchit-gandhi/whisper-large-v3-hi"

    ```

    And keep the rest of the code example un-changed.'
  created_at: 2023-11-20 17:50:11+00:00
  edited: false
  hidden: false
  id: 655b9c53f6b4c0428c872a49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e67aba15dab9399c06be912719e1ec0b.svg
      fullname: Amgad Hasan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amgadhasan
      type: user
    createdAt: '2023-11-20T18:02:27.000Z'
    data:
      edited: false
      editors:
      - amgadhasan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737274050712585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e67aba15dab9399c06be912719e1ec0b.svg
          fullname: Amgad Hasan
          isHf: false
          isPro: false
          name: amgadhasan
          type: user
        html: '<p>This is called speaker diarizatiom.<br>There are many repos on github
          for thid</p>

          '
        raw: 'This is called speaker diarizatiom.

          There are many repos on github for thid'
        updatedAt: '2023-11-20T18:02:27.555Z'
      numEdits: 0
      reactions: []
    id: 655b9f33bfb531437a3033ce
    type: comment
  author: amgadhasan
  content: 'This is called speaker diarizatiom.

    There are many repos on github for thid'
  created_at: 2023-11-20 18:02:27+00:00
  edited: false
  hidden: false
  id: 655b9f33bfb531437a3033ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
      fullname: qiuwei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andyweiqiu
      type: user
    createdAt: '2023-11-21T07:27:29.000Z'
    data:
      edited: false
      editors:
      - andyweiqiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7801192998886108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
          fullname: qiuwei
          isHf: false
          isPro: false
          name: andyweiqiu
          type: user
        html: "<blockquote>\n<p>Hey <span data-props=\"{&quot;user&quot;:&quot;andyweiqiu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andyweiqiu\"\
          >@<span class=\"underline\">andyweiqiu</span></a></span>\n\n\t</span></span>\
          \ - these are the <a href=\"https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\"\
          >sharded weights</a> split by the <code>.save_pretrained</code> method.\
          \ To use them for inference, simply follow the code snippets in the README,\
          \ replacing <code>model_id</code> with the repo id or path to your save\
          \ checkpoint: <a href=\"https://huggingface.co/openai/whisper-large-v3#usage\"\
          >https://huggingface.co/openai/whisper-large-v3#usage</a></p>\n<p>E.g. if\
          \ I trained a model and saved it under <code>sanchit-gandhi/whisper-large-v3-hi</code>,\
          \ I would set:</p>\n<pre><code class=\"language-python\">model_id = <span\
          \ class=\"hljs-string\">\"sanchit-gandhi/whisper-large-v3-hi\"</span>\n\
          </code></pre>\n<p>And keep the rest of the code example un-changed.</p>\n\
          </blockquote>\n<p>Thank you for your reply. I also want to know how to generate\
          \ the pytorch_model.bin model file after I upload the local file(eg, model-00001-of-00002.safetensors\
          \ and model-00002-of-00002.safetensors.) , or will it be generated automatically\
          \ after uploading?</p>\n"
        raw: "> Hey @andyweiqiu - these are the [sharded weights](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size)\
          \ split by the `.save_pretrained` method. To use them for inference, simply\
          \ follow the code snippets in the README, replacing `model_id` with the\
          \ repo id or path to your save checkpoint: https://huggingface.co/openai/whisper-large-v3#usage\n\
          > \n> E.g. if I trained a model and saved it under `sanchit-gandhi/whisper-large-v3-hi`,\
          \ I would set:\n> ```python\n> model_id = \"sanchit-gandhi/whisper-large-v3-hi\"\
          \n> ```\n> And keep the rest of the code example un-changed.\n\nThank you\
          \ for your reply. I also want to know how to generate the pytorch_model.bin\
          \ model file after I upload the local file(eg, model-00001-of-00002.safetensors\
          \ and model-00002-of-00002.safetensors.) , or will it be generated automatically\
          \ after uploading?"
        updatedAt: '2023-11-21T07:27:29.975Z'
      numEdits: 0
      reactions: []
    id: 655c5be1b3ed2f5d1d35529a
    type: comment
  author: andyweiqiu
  content: "> Hey @andyweiqiu - these are the [sharded weights](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size)\
    \ split by the `.save_pretrained` method. To use them for inference, simply follow\
    \ the code snippets in the README, replacing `model_id` with the repo id or path\
    \ to your save checkpoint: https://huggingface.co/openai/whisper-large-v3#usage\n\
    > \n> E.g. if I trained a model and saved it under `sanchit-gandhi/whisper-large-v3-hi`,\
    \ I would set:\n> ```python\n> model_id = \"sanchit-gandhi/whisper-large-v3-hi\"\
    \n> ```\n> And keep the rest of the code example un-changed.\n\nThank you for\
    \ your reply. I also want to know how to generate the pytorch_model.bin model\
    \ file after I upload the local file(eg, model-00001-of-00002.safetensors and\
    \ model-00002-of-00002.safetensors.) , or will it be generated automatically after\
    \ uploading?"
  created_at: 2023-11-21 07:27:29+00:00
  edited: false
  hidden: false
  id: 655c5be1b3ed2f5d1d35529a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-23T14:51:55.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7737143039703369
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;andyweiqiu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andyweiqiu\"\
          >@<span class=\"underline\">andyweiqiu</span></a></span>\n\n\t</span></span>\
          \ - may I ask why you need the <code>pytorch_model.bin</code> format? Note\
          \ that this weight format is inherently unsafe, as explained: <a href=\"\
          https://huggingface.co/blog/safetensors-security-audit#why-create-something-new\"\
          >https://huggingface.co/blog/safetensors-security-audit#why-create-something-new</a></p>\n\
          <p>Therefore, it is recommended to use weight sharding and <a href=\"https://huggingface.co/blog/safetensors-security-audit\"\
          >safetensors</a> serialisation to save PyTorch model weights: <a href=\"\
          https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\"\
          >https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size</a></p>\n\
          <p>These weights are entirely compatible with <code>from_pretrained</code>,\
          \ so there's no need to change any of your code to accommodate for them!</p>\n\
          <p>You can use the same code-snippet as in: <a href=\"https://huggingface.co/openai/whisper-large-v3#usage\"\
          >https://huggingface.co/openai/whisper-large-v3#usage</a></p>\n<p>Just replace\
          \ the <code>model_id</code> with the path (or repo id) of your model.</p>\n"
        raw: 'Hey @andyweiqiu - may I ask why you need the `pytorch_model.bin` format?
          Note that this weight format is inherently unsafe, as explained: https://huggingface.co/blog/safetensors-security-audit#why-create-something-new


          Therefore, it is recommended to use weight sharding and [safetensors](https://huggingface.co/blog/safetensors-security-audit)
          serialisation to save PyTorch model weights: https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size


          These weights are entirely compatible with `from_pretrained`, so there''s
          no need to change any of your code to accommodate for them!


          You can use the same code-snippet as in: https://huggingface.co/openai/whisper-large-v3#usage


          Just replace the `model_id` with the path (or repo id) of your model.'
        updatedAt: '2023-11-23T14:51:55.908Z'
      numEdits: 0
      reactions: []
    id: 655f670b0bda1e8ff844cf74
    type: comment
  author: sanchit-gandhi
  content: 'Hey @andyweiqiu - may I ask why you need the `pytorch_model.bin` format?
    Note that this weight format is inherently unsafe, as explained: https://huggingface.co/blog/safetensors-security-audit#why-create-something-new


    Therefore, it is recommended to use weight sharding and [safetensors](https://huggingface.co/blog/safetensors-security-audit)
    serialisation to save PyTorch model weights: https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size


    These weights are entirely compatible with `from_pretrained`, so there''s no need
    to change any of your code to accommodate for them!


    You can use the same code-snippet as in: https://huggingface.co/openai/whisper-large-v3#usage


    Just replace the `model_id` with the path (or repo id) of your model.'
  created_at: 2023-11-23 14:51:55+00:00
  edited: false
  hidden: false
  id: 655f670b0bda1e8ff844cf74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
      fullname: qiuwei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andyweiqiu
      type: user
    createdAt: '2023-11-29T07:56:09.000Z'
    data:
      edited: false
      editors:
      - andyweiqiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7910243272781372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/622b1e0f088cad9efd500bb88d8ac633.svg
          fullname: qiuwei
          isHf: false
          isPro: false
          name: andyweiqiu
          type: user
        html: "<blockquote>\n<p>Hey <span data-props=\"{&quot;user&quot;:&quot;andyweiqiu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andyweiqiu\"\
          >@<span class=\"underline\">andyweiqiu</span></a></span>\n\n\t</span></span>\
          \ - may I ask why you need the <code>pytorch_model.bin</code> format? Note\
          \ that this weight format is inherently unsafe, as explained: <a href=\"\
          https://huggingface.co/blog/safetensors-security-audit#why-create-something-new\"\
          >https://huggingface.co/blog/safetensors-security-audit#why-create-something-new</a></p>\n\
          <p>Therefore, it is recommended to use weight sharding and <a href=\"https://huggingface.co/blog/safetensors-security-audit\"\
          >safetensors</a> serialisation to save PyTorch model weights: <a href=\"\
          https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\"\
          >https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size</a></p>\n\
          <p>These weights are entirely compatible with <code>from_pretrained</code>,\
          \ so there's no need to change any of your code to accommodate for them!</p>\n\
          <p>You can use the same code-snippet as in: <a href=\"https://huggingface.co/openai/whisper-large-v3#usage\"\
          >https://huggingface.co/openai/whisper-large-v3#usage</a></p>\n<p>Just replace\
          \ the <code>model_id</code> with the path (or repo id) of your model.</p>\n\
          </blockquote>\n<p>Ok, thank you. I've got it</p>\n"
        raw: "> Hey @andyweiqiu - may I ask why you need the `pytorch_model.bin` format?\
          \ Note that this weight format is inherently unsafe, as explained: https://huggingface.co/blog/safetensors-security-audit#why-create-something-new\n\
          > \n> Therefore, it is recommended to use weight sharding and [safetensors](https://huggingface.co/blog/safetensors-security-audit)\
          \ serialisation to save PyTorch model weights: https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\n\
          > \n> These weights are entirely compatible with `from_pretrained`, so there's\
          \ no need to change any of your code to accommodate for them!\n> \n> You\
          \ can use the same code-snippet as in: https://huggingface.co/openai/whisper-large-v3#usage\n\
          > \n> Just replace the `model_id` with the path (or repo id) of your model.\n\
          \nOk, thank you. I've got it"
        updatedAt: '2023-11-29T07:56:09.518Z'
      numEdits: 0
      reactions: []
    id: 6566ee99e1604b20589273bd
    type: comment
  author: andyweiqiu
  content: "> Hey @andyweiqiu - may I ask why you need the `pytorch_model.bin` format?\
    \ Note that this weight format is inherently unsafe, as explained: https://huggingface.co/blog/safetensors-security-audit#why-create-something-new\n\
    > \n> Therefore, it is recommended to use weight sharding and [safetensors](https://huggingface.co/blog/safetensors-security-audit)\
    \ serialisation to save PyTorch model weights: https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\n\
    > \n> These weights are entirely compatible with `from_pretrained`, so there's\
    \ no need to change any of your code to accommodate for them!\n> \n> You can use\
    \ the same code-snippet as in: https://huggingface.co/openai/whisper-large-v3#usage\n\
    > \n> Just replace the `model_id` with the path (or repo id) of your model.\n\n\
    Ok, thank you. I've got it"
  created_at: 2023-11-29 07:56:09+00:00
  edited: false
  hidden: false
  id: 6566ee99e1604b20589273bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 36
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: how to evaluate use  model-00001-of-00002.safetensors and model-00002-of-00002.safetensors
