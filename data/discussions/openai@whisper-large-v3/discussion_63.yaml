!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KSDFOSSDGHJ
conflicting_files: null
created_at: 2024-01-09 16:38:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2dd227ac855a5db043edca709385aac.svg
      fullname: KSDFOSSDGHJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSDFOSSDGHJ
      type: user
    createdAt: '2024-01-09T16:38:43.000Z'
    data:
      edited: false
      editors:
      - KSDFOSSDGHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3674328327178955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2dd227ac855a5db043edca709385aac.svg
          fullname: KSDFOSSDGHJ
          isHf: false
          isPro: false
          name: KSDFOSSDGHJ
          type: user
        html: "<p>Win 10, Python 3.10.11, CUDA 12.3, CUDNN 8.9.7.29, Ampere (3060ti).</p>\n\
          <details><summary>pip list:</summary>\n<p>accelerate               0.25.0\n\
          aiohttp                  3.9.1\naiosignal                1.3.1\nanyio  \
          \                  3.6.2\nappdirs                  1.4.4\nargcomplete  \
          \            3.2.1\nargon2-cffi              21.3.0\nargon2-cffi-bindings\
          \     21.2.0\narrow                    1.2.3\nasttokens                2.2.1\n\
          async-timeout            4.0.2\nattrs                    23.1.0\naudioread\
          \                3.0.1\nbackcall                 0.2.0\nbackoff        \
          \          2.1.2\nbeautifulsoup4           4.12.2\nbleach              \
          \     6.0.0\nboto3                    1.26.120\nbotocore               \
          \  1.29.120\ncertifi                  2022.12.7\ncffi                  \
          \   1.15.1\ncharset-normalizer       3.1.0\nclick                    8.1.7\n\
          colorama                 0.4.6\ncoloredlogs              15.0.1\ncomm  \
          \                   0.1.3\ncuda-python              12.1.0\nCython     \
          \              0.29.34\ndatasets                 2.16.1\ndebugpy       \
          \           1.6.7\ndecorator                5.1.1\ndefusedxml          \
          \     0.7.1\ndill                     0.3.7\neinops                   0.6.1\n\
          encodec                  0.1.1\nexecuting                1.2.0\nfastjsonschema\
          \           2.16.3\nfilelock                 3.12.0\nflash_attn        \
          \       2.4.2\nfqdn                     1.5.1\nfrozenlist              \
          \ 1.4.1\nfsspec                   2023.10.0\nfuncy                    2.0\n\
          githubrelease            1.5.9\nhuggingface-hub          0.20.2\nhumanfriendly\
          \            10.0\nidna                     3.4\nifaddr                \
          \   0.2.0\nipykernel                6.22.0\nipython                  8.12.0\n\
          ipython-genutils         0.2.0\nipywidgets               8.0.6\nisoduration\
          \              20.11.0\njedi                     0.18.2\nJinja2        \
          \           3.1.2\njmespath                 1.0.1\njoblib              \
          \     1.3.2\njsonpointer              2.3\njsonschema               4.17.3\n\
          jupyter                  1.0.0\njupyter_client           8.2.0\njupyter-console\
          \          6.6.3\njupyter_core             5.3.0\njupyter-events       \
          \    0.6.3\njupyter_server           2.5.0\njupyter_server_terminals 0.4.4\n\
          jupyterlab-pygments      0.2.2\njupyterlab-widgets       3.0.7\nlazy_loader\
          \              0.3\nlibrespot                0.0.9\nlibrosa            \
          \      0.10.1\nLinkHeader               0.4.3\nllvmlite                \
          \ 0.41.1\nMarkupSafe               2.1.2\nmatplotlib-inline        0.1.6\n\
          mistune                  2.0.5\nmore-itertools           10.1.0\nmpmath\
          \                   1.3.0\nmsgpack                  1.0.7\nmultidict   \
          \             6.0.4\nmultiprocess             0.70.15\nmusic-tag       \
          \         0.4.3\nmutagen                  1.46.0\nnbclassic            \
          \    0.5.5\nnbclient                 0.7.4\nnbconvert                7.3.1\n\
          nbformat                 5.8.0\nnest-asyncio             1.5.6\nnetworkx\
          \                 3.1\nninja                    1.11.1.1\nnotebook     \
          \            6.5.4\nnotebook_shim            0.2.3\nnumba              \
          \      0.58.1\nnumpy                    1.24.3\nnvidia-cuda-runtime-cu12\
          \ 12.3.101\nopenai-whisper           20231117\noptimum                 \
          \ 1.16.1\npackaging                23.2\npandas                   2.1.4\n\
          pandocfilters            1.5.0\nparso                    0.8.3\npickleshare\
          \              0.7.5\nPillow                   9.5.0\npip              \
          \        23.3.2\npipx                     1.4.1\nplatformdirs          \
          \   3.4.0\npooch                    1.8.0\nprometheus-client        0.16.0\n\
          prompt-toolkit           3.0.38\nprotobuf                 3.20.1\npsutil\
          \                   5.9.5\npure-eval                0.2.2\npyarrow     \
          \             14.0.2\npyarrow-hotfix           0.6\npycparser          \
          \      2.21\npycryptodomex            3.18.0\npydub                    0.25.1\n\
          Pygments                 2.15.1\nPyOgg                    0.6.14a1\npyreadline3\
          \              3.4.1\npyrsistent               0.19.3\npython-dateutil \
          \         2.8.2\npython-json-logger       2.0.7\npytz                  \
          \   2023.3.post1\npywin32                  306\npywinpty               \
          \  2.0.10\nPyYAML                   6.0\npyzmq                    25.0.2\n\
          qtconsole                5.4.2\nQtPy                     2.3.1\nregex  \
          \                  2023.3.23\nrequests                 2.31.0\nrfc3339-validator\
          \        0.1.4\nrfc3986-validator        0.1.1\ns3transfer             \
          \  0.6.0\nsafetensors              0.4.1\nscikit-learn             1.3.2\n\
          scipy                    1.10.1\nSend2Trash               1.8.0\nsentencepiece\
          \            0.1.99\nsetuptools               69.0.3\nsix              \
          \        1.16.0\nsniffio                  1.3.0\nsoundfile             \
          \   0.12.1\nsoupsieve                2.4.1\nsoxr                     0.3.7\n\
          stack-data               0.6.2\nsuno-bark                0.0.1a0\nsympy\
          \                    1.11.1\nterminado                0.17.1\nthreadpoolctl\
          \            3.2.0\ntiktoken                 0.5.2\ntinycss2           \
          \      1.2.1\ntokenizers               0.15.0\ntomli                   \
          \ 2.0.1\ntorch                    2.1.2+cu121\ntorchaudio              \
          \ 2.1.2+cu121\ntorchvision              0.16.2+cu121\ntornado          \
          \        6.3.1\ntqdm                     4.65.0\ntraitlets             \
          \   5.9.0\ntransformers             4.37.0.dev0\ntyping_extensions     \
          \   4.5.0\ntzdata                   2023.4\nuri-template             1.2.0\n\
          urllib3                  1.26.15\nuserpath                 1.9.1\nwcwidth\
          \                  0.2.6\nwebcolors                1.13\nwebencodings  \
          \           0.5.1\nwebsocket-client         1.5.2\nwheel               \
          \     0.42.0\nwidgetsnbextension       4.0.7\nxxhash                   3.4.1\n\
          yarl                     1.9.4\nzeroconf                 0.64.0\n</p>\n\
          </details> \n\n<p>I have the following code:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\n<span class=\"hljs-keyword\">from</span> datasets\
          \ <span class=\"hljs-keyword\">import</span> load_dataset\n<span class=\"\
          hljs-keyword\">import</span> time\n\n<span class=\"hljs-comment\"># Measure\
          \ the start time</span>\nstart_time = time.time()\n\n<span class=\"hljs-comment\"\
          ># Check if GPU is available</span>\ndevice = <span class=\"hljs-string\"\
          >\"cuda:0\"</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\ntorch_dtype = torch.float16 <span class=\"hljs-keyword\">if</span>\
          \ torch.cuda.is_available() <span class=\"hljs-keyword\">else</span> torch.float32\n\
          \n<span class=\"hljs-comment\"># Load the model on the CPU</span>\nmodel_id\
          \ = <span class=\"hljs-string\">\"openai/whisper-large-v3\"</span>\nmodel\
          \ = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype,\
          \ low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>, use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>, attn_implementation=<span class=\"\
          hljs-string\">\"flash_attention_2\"</span>\n)\n\n<span class=\"hljs-comment\"\
          ># Move the model to the GPU if available</span>\nmodel.to(device)\n\nprocessor\
          \ = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    <span\
          \ class=\"hljs-string\">\"automatic-speech-recognition\"</span>,\n    model=model,\n\
          \    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=<span class=\"hljs-number\">128</span>,\n    chunk_length_s=<span\
          \ class=\"hljs-number\">30</span>,\n    batch_size=<span class=\"hljs-number\"\
          >16</span>,\n    return_timestamps=<span class=\"hljs-literal\">True</span>,\n\
          \    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(<span\
          \ class=\"hljs-string\">\"distil-whisper/librispeech_long\"</span>, <span\
          \ class=\"hljs-string\">\"clean\"</span>, split=<span class=\"hljs-string\"\
          >\"validation\"</span>)\nsample = dataset[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">\"audio\"</span>]\n\n<span class=\"hljs-comment\"\
          ># Execute the pipeline</span>\nresult = pipe(<span class=\"hljs-string\"\
          >\"1.ogg\"</span>)\n\n<span class=\"hljs-comment\"># Print the result and\
          \ execution time</span>\n<span class=\"hljs-built_in\">print</span>(result[<span\
          \ class=\"hljs-string\">\"text\"</span>])\nend_time = time.time()\nexecution_time\
          \ = end_time - start_time\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Execution time: <span class=\"hljs-subst\">{execution_time}</span>\
          \ seconds\"</span>)\n</code></pre>\n<p>Getting this error:</p>\n<p><code>You\
          \ are attempting to use Flash Attention 2.0 with a model not initialized\
          \ on GPU and with no GPU available. This is not supported yet. Please make\
          \ sure to have access to a GPU and either initialise the model on a GPU\
          \ by passing a device_map or initialising the model on CPU and then moving\
          \ it to GPU.</code></p>\n<p>\U0001F625</p>\n"
        raw: "Win 10, Python 3.10.11, CUDA 12.3, CUDNN 8.9.7.29, Ampere (3060ti).\r\
          \n<details><summary>pip list:</summary>\r\n<p>accelerate               0.25.0\r\
          \naiohttp                  3.9.1\r\naiosignal                1.3.1\r\nanyio\
          \                    3.6.2\r\nappdirs                  1.4.4\r\nargcomplete\
          \              3.2.1\r\nargon2-cffi              21.3.0\r\nargon2-cffi-bindings\
          \     21.2.0\r\narrow                    1.2.3\r\nasttokens            \
          \    2.2.1\r\nasync-timeout            4.0.2\r\nattrs                  \
          \  23.1.0\r\naudioread                3.0.1\r\nbackcall                \
          \ 0.2.0\r\nbackoff                  2.1.2\r\nbeautifulsoup4           4.12.2\r\
          \nbleach                   6.0.0\r\nboto3                    1.26.120\r\n\
          botocore                 1.29.120\r\ncertifi                  2022.12.7\r\
          \ncffi                     1.15.1\r\ncharset-normalizer       3.1.0\r\n\
          click                    8.1.7\r\ncolorama                 0.4.6\r\ncoloredlogs\
          \              15.0.1\r\ncomm                     0.1.3\r\ncuda-python \
          \             12.1.0\r\nCython                   0.29.34\r\ndatasets   \
          \              2.16.1\r\ndebugpy                  1.6.7\r\ndecorator   \
          \             5.1.1\r\ndefusedxml               0.7.1\r\ndill          \
          \           0.3.7\r\neinops                   0.6.1\r\nencodec         \
          \         0.1.1\r\nexecuting                1.2.0\r\nfastjsonschema    \
          \       2.16.3\r\nfilelock                 3.12.0\r\nflash_attn        \
          \       2.4.2\r\nfqdn                     1.5.1\r\nfrozenlist          \
          \     1.4.1\r\nfsspec                   2023.10.0\r\nfuncy             \
          \       2.0\r\ngithubrelease            1.5.9\r\nhuggingface-hub       \
          \   0.20.2\r\nhumanfriendly            10.0\r\nidna                    \
          \ 3.4\r\nifaddr                   0.2.0\r\nipykernel                6.22.0\r\
          \nipython                  8.12.0\r\nipython-genutils         0.2.0\r\n\
          ipywidgets               8.0.6\r\nisoduration              20.11.0\r\njedi\
          \                     0.18.2\r\nJinja2                   3.1.2\r\njmespath\
          \                 1.0.1\r\njoblib                   1.3.2\r\njsonpointer\
          \              2.3\r\njsonschema               4.17.3\r\njupyter       \
          \           1.0.0\r\njupyter_client           8.2.0\r\njupyter-console \
          \         6.6.3\r\njupyter_core             5.3.0\r\njupyter-events    \
          \       0.6.3\r\njupyter_server           2.5.0\r\njupyter_server_terminals\
          \ 0.4.4\r\njupyterlab-pygments      0.2.2\r\njupyterlab-widgets       3.0.7\r\
          \nlazy_loader              0.3\r\nlibrespot                0.0.9\r\nlibrosa\
          \                  0.10.1\r\nLinkHeader               0.4.3\r\nllvmlite\
          \                 0.41.1\r\nMarkupSafe               2.1.2\r\nmatplotlib-inline\
          \        0.1.6\r\nmistune                  2.0.5\r\nmore-itertools     \
          \      10.1.0\r\nmpmath                   1.3.0\r\nmsgpack             \
          \     1.0.7\r\nmultidict                6.0.4\r\nmultiprocess          \
          \   0.70.15\r\nmusic-tag                0.4.3\r\nmutagen               \
          \   1.46.0\r\nnbclassic                0.5.5\r\nnbclient               \
          \  0.7.4\r\nnbconvert                7.3.1\r\nnbformat                 5.8.0\r\
          \nnest-asyncio             1.5.6\r\nnetworkx                 3.1\r\nninja\
          \                    1.11.1.1\r\nnotebook                 6.5.4\r\nnotebook_shim\
          \            0.2.3\r\nnumba                    0.58.1\r\nnumpy         \
          \           1.24.3\r\nnvidia-cuda-runtime-cu12 12.3.101\r\nopenai-whisper\
          \           20231117\r\noptimum                  1.16.1\r\npackaging   \
          \             23.2\r\npandas                   2.1.4\r\npandocfilters  \
          \          1.5.0\r\nparso                    0.8.3\r\npickleshare      \
          \        0.7.5\r\nPillow                   9.5.0\r\npip                \
          \      23.3.2\r\npipx                     1.4.1\r\nplatformdirs        \
          \     3.4.0\r\npooch                    1.8.0\r\nprometheus-client     \
          \   0.16.0\r\nprompt-toolkit           3.0.38\r\nprotobuf              \
          \   3.20.1\r\npsutil                   5.9.5\r\npure-eval              \
          \  0.2.2\r\npyarrow                  14.0.2\r\npyarrow-hotfix          \
          \ 0.6\r\npycparser                2.21\r\npycryptodomex            3.18.0\r\
          \npydub                    0.25.1\r\nPygments                 2.15.1\r\n\
          PyOgg                    0.6.14a1\r\npyreadline3              3.4.1\r\n\
          pyrsistent               0.19.3\r\npython-dateutil          2.8.2\r\npython-json-logger\
          \       2.0.7\r\npytz                     2023.3.post1\r\npywin32      \
          \            306\r\npywinpty                 2.0.10\r\nPyYAML          \
          \         6.0\r\npyzmq                    25.0.2\r\nqtconsole          \
          \      5.4.2\r\nQtPy                     2.3.1\r\nregex                \
          \    2023.3.23\r\nrequests                 2.31.0\r\nrfc3339-validator \
          \       0.1.4\r\nrfc3986-validator        0.1.1\r\ns3transfer          \
          \     0.6.0\r\nsafetensors              0.4.1\r\nscikit-learn          \
          \   1.3.2\r\nscipy                    1.10.1\r\nSend2Trash             \
          \  1.8.0\r\nsentencepiece            0.1.99\r\nsetuptools              \
          \ 69.0.3\r\nsix                      1.16.0\r\nsniffio                 \
          \ 1.3.0\r\nsoundfile                0.12.1\r\nsoupsieve                2.4.1\r\
          \nsoxr                     0.3.7\r\nstack-data               0.6.2\r\nsuno-bark\
          \                0.0.1a0\r\nsympy                    1.11.1\r\nterminado\
          \                0.17.1\r\nthreadpoolctl            3.2.0\r\ntiktoken  \
          \               0.5.2\r\ntinycss2                 1.2.1\r\ntokenizers  \
          \             0.15.0\r\ntomli                    2.0.1\r\ntorch        \
          \            2.1.2+cu121\r\ntorchaudio               2.1.2+cu121\r\ntorchvision\
          \              0.16.2+cu121\r\ntornado                  6.3.1\r\ntqdm  \
          \                   4.65.0\r\ntraitlets                5.9.0\r\ntransformers\
          \             4.37.0.dev0\r\ntyping_extensions        4.5.0\r\ntzdata  \
          \                 2023.4\r\nuri-template             1.2.0\r\nurllib3  \
          \                1.26.15\r\nuserpath                 1.9.1\r\nwcwidth  \
          \                0.2.6\r\nwebcolors                1.13\r\nwebencodings\
          \             0.5.1\r\nwebsocket-client         1.5.2\r\nwheel         \
          \           0.42.0\r\nwidgetsnbextension       4.0.7\r\nxxhash         \
          \          3.4.1\r\nyarl                     1.9.4\r\nzeroconf         \
          \        0.64.0\r\n</p>\r\n</details> \r\n\r\nI have the following code:\r\
          \n```python\r\nimport torch\r\nfrom transformers import AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\r\nfrom datasets import load_dataset\r\nimport\
          \ time\r\n\r\n# Measure the start time\r\nstart_time = time.time()\r\n\r\
          \n# Check if GPU is available\r\ndevice = \"cuda:0\" if torch.cuda.is_available()\
          \ else \"cpu\"\r\ntorch_dtype = torch.float16 if torch.cuda.is_available()\
          \ else torch.float32\r\n\r\n# Load the model on the CPU\r\nmodel_id = \"\
          openai/whisper-large-v3\"\r\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\r\
          \n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True,\
          \ attn_implementation=\"flash_attention_2\"\r\n)\r\n\r\n# Move the model\
          \ to the GPU if available\r\nmodel.to(device)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\
          \n\r\npipe = pipeline(\r\n    \"automatic-speech-recognition\",\r\n    model=model,\r\
          \n    tokenizer=processor.tokenizer,\r\n    feature_extractor=processor.feature_extractor,\r\
          \n    max_new_tokens=128,\r\n    chunk_length_s=30,\r\n    batch_size=16,\r\
          \n    return_timestamps=True,\r\n    torch_dtype=torch_dtype,\r\n    device=device,\r\
          \n)\r\n\r\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"\
          clean\", split=\"validation\")\r\nsample = dataset[0][\"audio\"]\r\n\r\n\
          # Execute the pipeline\r\nresult = pipe(\"1.ogg\")\r\n\r\n# Print the result\
          \ and execution time\r\nprint(result[\"text\"])\r\nend_time = time.time()\r\
          \nexecution_time = end_time - start_time\r\nprint(f\"Execution time: {execution_time}\
          \ seconds\")\r\n``` \r\nGetting this error:\r\n\r\n`You are attempting to\
          \ use Flash Attention 2.0 with a model not initialized on GPU and with no\
          \ GPU available. This is not supported yet. Please make sure to have access\
          \ to a GPU and either initialise the model on a GPU by passing a device_map\
          \ or initialising the model on CPU and then moving it to GPU.`\r\n\r\n\U0001F625"
        updatedAt: '2024-01-09T16:38:43.408Z'
      numEdits: 0
      reactions: []
    id: 659d769385049e3b18f7ae73
    type: comment
  author: KSDFOSSDGHJ
  content: "Win 10, Python 3.10.11, CUDA 12.3, CUDNN 8.9.7.29, Ampere (3060ti).\r\n\
    <details><summary>pip list:</summary>\r\n<p>accelerate               0.25.0\r\n\
    aiohttp                  3.9.1\r\naiosignal                1.3.1\r\nanyio    \
    \                3.6.2\r\nappdirs                  1.4.4\r\nargcomplete      \
    \        3.2.1\r\nargon2-cffi              21.3.0\r\nargon2-cffi-bindings    \
    \ 21.2.0\r\narrow                    1.2.3\r\nasttokens                2.2.1\r\
    \nasync-timeout            4.0.2\r\nattrs                    23.1.0\r\naudioread\
    \                3.0.1\r\nbackcall                 0.2.0\r\nbackoff          \
    \        2.1.2\r\nbeautifulsoup4           4.12.2\r\nbleach                  \
    \ 6.0.0\r\nboto3                    1.26.120\r\nbotocore                 1.29.120\r\
    \ncertifi                  2022.12.7\r\ncffi                     1.15.1\r\ncharset-normalizer\
    \       3.1.0\r\nclick                    8.1.7\r\ncolorama                 0.4.6\r\
    \ncoloredlogs              15.0.1\r\ncomm                     0.1.3\r\ncuda-python\
    \              12.1.0\r\nCython                   0.29.34\r\ndatasets        \
    \         2.16.1\r\ndebugpy                  1.6.7\r\ndecorator              \
    \  5.1.1\r\ndefusedxml               0.7.1\r\ndill                     0.3.7\r\
    \neinops                   0.6.1\r\nencodec                  0.1.1\r\nexecuting\
    \                1.2.0\r\nfastjsonschema           2.16.3\r\nfilelock        \
    \         3.12.0\r\nflash_attn               2.4.2\r\nfqdn                   \
    \  1.5.1\r\nfrozenlist               1.4.1\r\nfsspec                   2023.10.0\r\
    \nfuncy                    2.0\r\ngithubrelease            1.5.9\r\nhuggingface-hub\
    \          0.20.2\r\nhumanfriendly            10.0\r\nidna                   \
    \  3.4\r\nifaddr                   0.2.0\r\nipykernel                6.22.0\r\n\
    ipython                  8.12.0\r\nipython-genutils         0.2.0\r\nipywidgets\
    \               8.0.6\r\nisoduration              20.11.0\r\njedi            \
    \         0.18.2\r\nJinja2                   3.1.2\r\njmespath               \
    \  1.0.1\r\njoblib                   1.3.2\r\njsonpointer              2.3\r\n\
    jsonschema               4.17.3\r\njupyter                  1.0.0\r\njupyter_client\
    \           8.2.0\r\njupyter-console          6.6.3\r\njupyter_core          \
    \   5.3.0\r\njupyter-events           0.6.3\r\njupyter_server           2.5.0\r\
    \njupyter_server_terminals 0.4.4\r\njupyterlab-pygments      0.2.2\r\njupyterlab-widgets\
    \       3.0.7\r\nlazy_loader              0.3\r\nlibrespot                0.0.9\r\
    \nlibrosa                  0.10.1\r\nLinkHeader               0.4.3\r\nllvmlite\
    \                 0.41.1\r\nMarkupSafe               2.1.2\r\nmatplotlib-inline\
    \        0.1.6\r\nmistune                  2.0.5\r\nmore-itertools           10.1.0\r\
    \nmpmath                   1.3.0\r\nmsgpack                  1.0.7\r\nmultidict\
    \                6.0.4\r\nmultiprocess             0.70.15\r\nmusic-tag      \
    \          0.4.3\r\nmutagen                  1.46.0\r\nnbclassic             \
    \   0.5.5\r\nnbclient                 0.7.4\r\nnbconvert                7.3.1\r\
    \nnbformat                 5.8.0\r\nnest-asyncio             1.5.6\r\nnetworkx\
    \                 3.1\r\nninja                    1.11.1.1\r\nnotebook       \
    \          6.5.4\r\nnotebook_shim            0.2.3\r\nnumba                  \
    \  0.58.1\r\nnumpy                    1.24.3\r\nnvidia-cuda-runtime-cu12 12.3.101\r\
    \nopenai-whisper           20231117\r\noptimum                  1.16.1\r\npackaging\
    \                23.2\r\npandas                   2.1.4\r\npandocfilters     \
    \       1.5.0\r\nparso                    0.8.3\r\npickleshare              0.7.5\r\
    \nPillow                   9.5.0\r\npip                      23.3.2\r\npipx  \
    \                   1.4.1\r\nplatformdirs             3.4.0\r\npooch         \
    \           1.8.0\r\nprometheus-client        0.16.0\r\nprompt-toolkit       \
    \    3.0.38\r\nprotobuf                 3.20.1\r\npsutil                   5.9.5\r\
    \npure-eval                0.2.2\r\npyarrow                  14.0.2\r\npyarrow-hotfix\
    \           0.6\r\npycparser                2.21\r\npycryptodomex            3.18.0\r\
    \npydub                    0.25.1\r\nPygments                 2.15.1\r\nPyOgg\
    \                    0.6.14a1\r\npyreadline3              3.4.1\r\npyrsistent\
    \               0.19.3\r\npython-dateutil          2.8.2\r\npython-json-logger\
    \       2.0.7\r\npytz                     2023.3.post1\r\npywin32            \
    \      306\r\npywinpty                 2.0.10\r\nPyYAML                   6.0\r\
    \npyzmq                    25.0.2\r\nqtconsole                5.4.2\r\nQtPy  \
    \                   2.3.1\r\nregex                    2023.3.23\r\nrequests  \
    \               2.31.0\r\nrfc3339-validator        0.1.4\r\nrfc3986-validator\
    \        0.1.1\r\ns3transfer               0.6.0\r\nsafetensors              0.4.1\r\
    \nscikit-learn             1.3.2\r\nscipy                    1.10.1\r\nSend2Trash\
    \               1.8.0\r\nsentencepiece            0.1.99\r\nsetuptools       \
    \        69.0.3\r\nsix                      1.16.0\r\nsniffio                \
    \  1.3.0\r\nsoundfile                0.12.1\r\nsoupsieve                2.4.1\r\
    \nsoxr                     0.3.7\r\nstack-data               0.6.2\r\nsuno-bark\
    \                0.0.1a0\r\nsympy                    1.11.1\r\nterminado     \
    \           0.17.1\r\nthreadpoolctl            3.2.0\r\ntiktoken             \
    \    0.5.2\r\ntinycss2                 1.2.1\r\ntokenizers               0.15.0\r\
    \ntomli                    2.0.1\r\ntorch                    2.1.2+cu121\r\ntorchaudio\
    \               2.1.2+cu121\r\ntorchvision              0.16.2+cu121\r\ntornado\
    \                  6.3.1\r\ntqdm                     4.65.0\r\ntraitlets     \
    \           5.9.0\r\ntransformers             4.37.0.dev0\r\ntyping_extensions\
    \        4.5.0\r\ntzdata                   2023.4\r\nuri-template            \
    \ 1.2.0\r\nurllib3                  1.26.15\r\nuserpath                 1.9.1\r\
    \nwcwidth                  0.2.6\r\nwebcolors                1.13\r\nwebencodings\
    \             0.5.1\r\nwebsocket-client         1.5.2\r\nwheel               \
    \     0.42.0\r\nwidgetsnbextension       4.0.7\r\nxxhash                   3.4.1\r\
    \nyarl                     1.9.4\r\nzeroconf                 0.64.0\r\n</p>\r\n\
    </details> \r\n\r\nI have the following code:\r\n```python\r\nimport torch\r\n\
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\r\n\
    from datasets import load_dataset\r\nimport time\r\n\r\n# Measure the start time\r\
    \nstart_time = time.time()\r\n\r\n# Check if GPU is available\r\ndevice = \"cuda:0\"\
    \ if torch.cuda.is_available() else \"cpu\"\r\ntorch_dtype = torch.float16 if\
    \ torch.cuda.is_available() else torch.float32\r\n\r\n# Load the model on the\
    \ CPU\r\nmodel_id = \"openai/whisper-large-v3\"\r\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\r\
    \n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True,\
    \ attn_implementation=\"flash_attention_2\"\r\n)\r\n\r\n# Move the model to the\
    \ GPU if available\r\nmodel.to(device)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\
    \n\r\npipe = pipeline(\r\n    \"automatic-speech-recognition\",\r\n    model=model,\r\
    \n    tokenizer=processor.tokenizer,\r\n    feature_extractor=processor.feature_extractor,\r\
    \n    max_new_tokens=128,\r\n    chunk_length_s=30,\r\n    batch_size=16,\r\n\
    \    return_timestamps=True,\r\n    torch_dtype=torch_dtype,\r\n    device=device,\r\
    \n)\r\n\r\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\"\
    , split=\"validation\")\r\nsample = dataset[0][\"audio\"]\r\n\r\n# Execute the\
    \ pipeline\r\nresult = pipe(\"1.ogg\")\r\n\r\n# Print the result and execution\
    \ time\r\nprint(result[\"text\"])\r\nend_time = time.time()\r\nexecution_time\
    \ = end_time - start_time\r\nprint(f\"Execution time: {execution_time} seconds\"\
    )\r\n``` \r\nGetting this error:\r\n\r\n`You are attempting to use Flash Attention\
    \ 2.0 with a model not initialized on GPU and with no GPU available. This is not\
    \ supported yet. Please make sure to have access to a GPU and either initialise\
    \ the model on a GPU by passing a device_map or initialising the model on CPU\
    \ and then moving it to GPU.`\r\n\r\n\U0001F625"
  created_at: 2024-01-09 16:38:43+00:00
  edited: false
  hidden: false
  id: 659d769385049e3b18f7ae73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2dd227ac855a5db043edca709385aac.svg
      fullname: KSDFOSSDGHJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSDFOSSDGHJ
      type: user
    createdAt: '2024-01-10T02:55:40.000Z'
    data:
      edited: false
      editors:
      - KSDFOSSDGHJ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5565306544303894
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2dd227ac855a5db043edca709385aac.svg
          fullname: KSDFOSSDGHJ
          isHf: false
          isPro: false
          name: KSDFOSSDGHJ
          type: user
        html: "<p>I did this:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\n<span class=\"hljs-keyword\">from</span> datasets\
          \ <span class=\"hljs-keyword\">import</span> load_dataset\n<span class=\"\
          hljs-keyword\">import</span> time\n\n<span class=\"hljs-comment\"># Measure\
          \ the start time</span>\nstart_time = time.time()\n\n<span class=\"hljs-comment\"\
          ># Check if GPU is available</span>\ndevice = <span class=\"hljs-string\"\
          >\"cuda:0\"</span> <span class=\"hljs-comment\">#if torch.cuda.is_available()\
          \ else \"cpu\"</span>\ntorch_dtype = torch.float16 <span class=\"hljs-keyword\"\
          >if</span> torch.cuda.is_available() <span class=\"hljs-keyword\">else</span>\
          \ torch.float32\n\n<span class=\"hljs-comment\"># Load the model on the\
          \ CPU</span>\nmodel_id = <span class=\"hljs-string\">\"openai/whisper-large-v3\"\
          </span>\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id,\
          \ torch_dtype=torch_dtype, low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>, use_safetensors=<span class=\"hljs-literal\">True</span>,\
          \ device_map=<span class=\"hljs-string\">\"cuda:0\"</span>, attn_implementation=<span\
          \ class=\"hljs-string\">\"flash_attention_2\"</span>\n)\n\n<span class=\"\
          hljs-comment\"># Move the model to the GPU if available</span>\n<span class=\"\
          hljs-comment\">#model.to(device)</span>\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
          \npipe = pipeline(\n    <span class=\"hljs-string\">\"automatic-speech-recognition\"\
          </span>,\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=<span class=\"hljs-number\">128</span>,\n    chunk_length_s=<span\
          \ class=\"hljs-number\">30</span>,\n    batch_size=<span class=\"hljs-number\"\
          >16</span>,\n    return_timestamps=<span class=\"hljs-literal\">True</span>,\n\
          \    torch_dtype=torch_dtype,\n    <span class=\"hljs-comment\">#device=device,</span>\n\
          )\n\ndataset = load_dataset(<span class=\"hljs-string\">\"distil-whisper/librispeech_long\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>)\nsample = dataset[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"audio\"</span>]\n\n<span class=\"\
          hljs-comment\"># Execute the pipeline</span>\nresult = pipe(<span class=\"\
          hljs-string\">\"1.ogg\"</span>)\n\n<span class=\"hljs-comment\"># Print\
          \ the result and execution time</span>\n<span class=\"hljs-built_in\">print</span>(result[<span\
          \ class=\"hljs-string\">\"text\"</span>])\nend_time = time.time()\nexecution_time\
          \ = end_time - start_time\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Execution time: <span class=\"hljs-subst\">{execution_time}</span>\
          \ seconds\"</span>)\n</code></pre>\n<p>It executes without any errors now,\
          \ but no speed improvement whatsoever.</p>\n<p>I also noticed that removing:</p>\n\
          <pre><code class=\"language-python\">dataset = load_dataset(<span class=\"\
          hljs-string\">\"distil-whisper/librispeech_long\"</span>, <span class=\"\
          hljs-string\">\"clean\"</span>, split=<span class=\"hljs-string\">\"validation\"\
          </span>)\nsample = dataset[<span class=\"hljs-number\">0</span>][<span class=\"\
          hljs-string\">\"audio\"</span>]\n</code></pre>\n<p>Makes it twice faster.</p>\n"
        raw: "I did this:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\nfrom datasets import load_dataset\nimport time\n\
          \n# Measure the start time\nstart_time = time.time()\n\n# Check if GPU is\
          \ available\ndevice = \"cuda:0\" #if torch.cuda.is_available() else \"cpu\"\
          \ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\
          \n# Load the model on the CPU\nmodel_id = \"openai/whisper-large-v3\"\n\
          model = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype,\
          \ low_cpu_mem_usage=True, use_safetensors=True, device_map=\"cuda:0\", attn_implementation=\"\
          flash_attention_2\"\n)\n\n# Move the model to the GPU if available\n#model.to(device)\n\
          \nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n\
          \    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n\
          \    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n\
          \    chunk_length_s=30,\n    batch_size=16,\n    return_timestamps=True,\n\
          \    torch_dtype=torch_dtype,\n    #device=device,\n)\n\ndataset = load_dataset(\"\
          distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample\
          \ = dataset[0][\"audio\"]\n\n# Execute the pipeline\nresult = pipe(\"1.ogg\"\
          )\n\n# Print the result and execution time\nprint(result[\"text\"])\nend_time\
          \ = time.time()\nexecution_time = end_time - start_time\nprint(f\"Execution\
          \ time: {execution_time} seconds\")\n```\nIt executes without any errors\
          \ now, but no speed improvement whatsoever.\n\nI also noticed that removing:\n\
          ```python\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"\
          clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n```\nMakes\
          \ it twice faster."
        updatedAt: '2024-01-10T02:55:40.739Z'
      numEdits: 0
      reactions: []
    id: 659e072c73410185e54867f6
    type: comment
  author: KSDFOSSDGHJ
  content: "I did this:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq,\
    \ AutoProcessor, pipeline\nfrom datasets import load_dataset\nimport time\n\n\
    # Measure the start time\nstart_time = time.time()\n\n# Check if GPU is available\n\
    device = \"cuda:0\" #if torch.cuda.is_available() else \"cpu\"\ntorch_dtype =\
    \ torch.float16 if torch.cuda.is_available() else torch.float32\n\n# Load the\
    \ model on the CPU\nmodel_id = \"openai/whisper-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
    \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True,\
    \ device_map=\"cuda:0\", attn_implementation=\"flash_attention_2\"\n)\n\n# Move\
    \ the model to the GPU if available\n#model.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
    \npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n\
    \    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
    \    max_new_tokens=128,\n    chunk_length_s=30,\n    batch_size=16,\n    return_timestamps=True,\n\
    \    torch_dtype=torch_dtype,\n    #device=device,\n)\n\ndataset = load_dataset(\"\
    distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample =\
    \ dataset[0][\"audio\"]\n\n# Execute the pipeline\nresult = pipe(\"1.ogg\")\n\n\
    # Print the result and execution time\nprint(result[\"text\"])\nend_time = time.time()\n\
    execution_time = end_time - start_time\nprint(f\"Execution time: {execution_time}\
    \ seconds\")\n```\nIt executes without any errors now, but no speed improvement\
    \ whatsoever.\n\nI also noticed that removing:\n```python\ndataset = load_dataset(\"\
    distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample =\
    \ dataset[0][\"audio\"]\n```\nMakes it twice faster."
  created_at: 2024-01-10 02:55:40+00:00
  edited: false
  hidden: false
  id: 659e072c73410185e54867f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f2dd227ac855a5db043edca709385aac.svg
      fullname: KSDFOSSDGHJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSDFOSSDGHJ
      type: user
    createdAt: '2024-01-10T13:02:48.000Z'
    data:
      status: closed
    id: 659e9578b81bfb45fb112f03
    type: status-change
  author: KSDFOSSDGHJ
  created_at: 2024-01-10 13:02:48+00:00
  id: 659e9578b81bfb45fb112f03
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 63
repo_id: openai/whisper-large-v3
repo_type: model
status: closed
target_branch: null
title: Flash Attention 2.0 does not work.
