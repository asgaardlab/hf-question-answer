!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ait-paca
conflicting_files: null
created_at: 2023-11-22 11:58:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
      fullname: Ait Paca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ait-paca
      type: user
    createdAt: '2023-11-22T11:58:34.000Z'
    data:
      edited: false
      editors:
      - ait-paca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8912054896354675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
          fullname: Ait Paca
          isHf: false
          isPro: false
          name: ait-paca
          type: user
        html: '<p>Hi,<br>Thanks for sharing this model and related work.</p>

          <p>I downloaded the Whisper-Large-V3 model using HF_hub snapshot by ignoring
          the patterns for msgpack, h5, fp32*, and safetensors. Thereby, only pytorch_model.bin
          (with dtype as float16) as the model file (~3GB), and all the rest of the
          repo files are downloaded.</p>

          <p>In config.json, the torch_dtype is given as float16. If I use default
          settings (without any mentioning of torch_dtype or with explicitly defining
          it as torch_dtype = torch.float32) for pipeline or AutoConfig... + AutoModel...
          + AutoProcessor..., it works fine. However, if I try using it with torch_dtype
          = torch.float16, it gives the following error.</p>

          <p>Input type (torch.FloatTensor) and weight type (torch.HalfTensor) should
          be the same or input should be a MKLDNN tensor and weight is a dense tensor...</p>

          <p>My setup:<br>OS: MacOS Monterey 12.7.1<br>device = "cpu"<br>Python: 3.11.6<br>Transformers:
          4.35.2</p>

          <p>2 questions:</p>

          <ol>

          <li><p>Why this model is not working with the dtype as given in config.json?
          If I explicitly define dtype for Large-V3 model, what other changes I should
          make for it to work?</p>

          </li>

          <li><p>I also tried with all other Whisper models (tiny, base, small, and
          medium, all of which have dtype as float32 in config.json). I also faced
          this problem of using (torch_dtype=torch.float16) there. For this case,
          is it even possible to use (any) Whisper pytorch model with different dtype
          than for the entry in config.json or I''m making any basic mistake? If it
          is possible to use with different dtype, what adaptation I need to make?</p>

          </li>

          </ol>

          <p>Thank you in advance.</p>

          '
        raw: "Hi,\r\nThanks for sharing this model and related work.\r\n\r\nI downloaded\
          \ the Whisper-Large-V3 model using HF_hub snapshot by ignoring the patterns\
          \ for msgpack, h5, fp32*, and safetensors. Thereby, only pytorch_model.bin\
          \ (with dtype as float16) as the model file (~3GB), and all the rest of\
          \ the repo files are downloaded.\r\n\r\nIn config.json, the torch_dtype\
          \ is given as float16. If I use default settings (without any mentioning\
          \ of torch_dtype or with explicitly defining it as torch_dtype = torch.float32)\
          \ for pipeline or AutoConfig... + AutoModel... + AutoProcessor..., it works\
          \ fine. However, if I try using it with torch_dtype = torch.float16, it\
          \ gives the following error.\r\n\r\nInput type (torch.FloatTensor) and weight\
          \ type (torch.HalfTensor) should be the same or input should be a MKLDNN\
          \ tensor and weight is a dense tensor...\r\n\r\nMy setup:\r\nOS: MacOS Monterey\
          \ 12.7.1\r\ndevice = \"cpu\"\r\nPython: 3.11.6\r\nTransformers: 4.35.2\r\
          \n\r\n2 questions:\r\n\r\n1. Why this model is not working with the dtype\
          \ as given in config.json? If I explicitly define dtype for Large-V3 model,\
          \ what other changes I should make for it to work?\r\n\r\n2. I also tried\
          \ with all other Whisper models (tiny, base, small, and medium, all of which\
          \ have dtype as float32 in config.json). I also faced this problem of using\
          \ (torch_dtype=torch.float16) there. For this case, is it even possible\
          \ to use (any) Whisper pytorch model with different dtype than for the entry\
          \ in config.json or I'm making any basic mistake? If it is possible to use\
          \ with different dtype, what adaptation I need to make?\r\n\r\nThank you\
          \ in advance."
        updatedAt: '2023-11-22T11:58:34.375Z'
      numEdits: 0
      reactions: []
    id: 655decea9703eb0dad494db9
    type: comment
  author: ait-paca
  content: "Hi,\r\nThanks for sharing this model and related work.\r\n\r\nI downloaded\
    \ the Whisper-Large-V3 model using HF_hub snapshot by ignoring the patterns for\
    \ msgpack, h5, fp32*, and safetensors. Thereby, only pytorch_model.bin (with dtype\
    \ as float16) as the model file (~3GB), and all the rest of the repo files are\
    \ downloaded.\r\n\r\nIn config.json, the torch_dtype is given as float16. If I\
    \ use default settings (without any mentioning of torch_dtype or with explicitly\
    \ defining it as torch_dtype = torch.float32) for pipeline or AutoConfig... +\
    \ AutoModel... + AutoProcessor..., it works fine. However, if I try using it with\
    \ torch_dtype = torch.float16, it gives the following error.\r\n\r\nInput type\
    \ (torch.FloatTensor) and weight type (torch.HalfTensor) should be the same or\
    \ input should be a MKLDNN tensor and weight is a dense tensor...\r\n\r\nMy setup:\r\
    \nOS: MacOS Monterey 12.7.1\r\ndevice = \"cpu\"\r\nPython: 3.11.6\r\nTransformers:\
    \ 4.35.2\r\n\r\n2 questions:\r\n\r\n1. Why this model is not working with the\
    \ dtype as given in config.json? If I explicitly define dtype for Large-V3 model,\
    \ what other changes I should make for it to work?\r\n\r\n2. I also tried with\
    \ all other Whisper models (tiny, base, small, and medium, all of which have dtype\
    \ as float32 in config.json). I also faced this problem of using (torch_dtype=torch.float16)\
    \ there. For this case, is it even possible to use (any) Whisper pytorch model\
    \ with different dtype than for the entry in config.json or I'm making any basic\
    \ mistake? If it is possible to use with different dtype, what adaptation I need\
    \ to make?\r\n\r\nThank you in advance."
  created_at: 2023-11-22 11:58:34+00:00
  edited: false
  hidden: false
  id: 655decea9703eb0dad494db9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-27T15:57:26.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8002004027366638
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>The Whisper model was trained in <code>float16</code>, hence the
          weights are in <code>float16</code> on the Hub. When we call <code>from_pretrained</code>,
          we automatically upcast to <code>float32</code>, unless you specify <code>torch_dtype=torch.float16</code>
          as you have done: <a href="https://huggingface.co/docs/transformers/main_classes/model#model-instantiation-dtype">https://huggingface.co/docs/transformers/main_classes/model#model-instantiation-dtype</a></p>

          <p>To fix your issue:</p>

          <ol>

          <li>You also need to convert your inputs to the same dtype as the model,
          i.e. <code>input_features = input_features.to(torch.float16)</code>. If
          you can share a code snippet of how you''re running the model, I can show
          you where to add this line</li>

          <li>Yes - according to 1 you can run any Whisper model in your desired dtype,
          provided the <code>input_features</code> are the same dtype as the <code>model</code></li>

          </ol>

          <p>You can see an example for <code>float16</code> evaluation for distil-whisper
          here: <a href="https://huggingface.co/distil-whisper/distil-large-v2#evaluation">https://huggingface.co/distil-whisper/distil-large-v2#evaluation</a>
          </p>

          '
        raw: 'The Whisper model was trained in `float16`, hence the weights are in
          `float16` on the Hub. When we call `from_pretrained`, we automatically upcast
          to `float32`, unless you specify `torch_dtype=torch.float16` as you have
          done: https://huggingface.co/docs/transformers/main_classes/model#model-instantiation-dtype


          To fix your issue:

          1. You also need to convert your inputs to the same dtype as the model,
          i.e. `input_features = input_features.to(torch.float16)`. If you can share
          a code snippet of how you''re running the model, I can show you where to
          add this line

          2. Yes - according to 1 you can run any Whisper model in your desired dtype,
          provided the `input_features` are the same dtype as the `model`


          You can see an example for `float16` evaluation for distil-whisper here:
          https://huggingface.co/distil-whisper/distil-large-v2#evaluation '
        updatedAt: '2023-11-27T15:57:26.952Z'
      numEdits: 0
      reactions: []
    id: 6564bc66c5f4dd9565acf8ff
    type: comment
  author: sanchit-gandhi
  content: 'The Whisper model was trained in `float16`, hence the weights are in `float16`
    on the Hub. When we call `from_pretrained`, we automatically upcast to `float32`,
    unless you specify `torch_dtype=torch.float16` as you have done: https://huggingface.co/docs/transformers/main_classes/model#model-instantiation-dtype


    To fix your issue:

    1. You also need to convert your inputs to the same dtype as the model, i.e. `input_features
    = input_features.to(torch.float16)`. If you can share a code snippet of how you''re
    running the model, I can show you where to add this line

    2. Yes - according to 1 you can run any Whisper model in your desired dtype, provided
    the `input_features` are the same dtype as the `model`


    You can see an example for `float16` evaluation for distil-whisper here: https://huggingface.co/distil-whisper/distil-large-v2#evaluation '
  created_at: 2023-11-27 15:57:26+00:00
  edited: false
  hidden: false
  id: 6564bc66c5f4dd9565acf8ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
      fullname: Ait Paca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ait-paca
      type: user
    createdAt: '2023-11-28T17:22:54.000Z'
    data:
      edited: false
      editors:
      - ait-paca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.864980161190033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c47b09195e6010a5ed591fd134a69709.svg
          fullname: Ait Paca
          isHf: false
          isPro: false
          name: ait-paca
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  for your response.</p>\n<p>I share my little Colab notebook.<br><a rel=\"\
          nofollow\" href=\"https://colab.research.google.com/drive/1uNCpZd6_g2MeuRn20AOW8cXs7Dbinoff\"\
          >https://colab.research.google.com/drive/1uNCpZd6_g2MeuRn20AOW8cXs7Dbinoff</a></p>\n\
          <p>I'm using simple pipeline call. The types for data and device are defined\
          \ in the 5th cell. As such, for a given media file (an audio file, I've\
          \ tested it on mp4/webm/mp3 formats), this code runs perfectly fine with\
          \ <code>u_torch_dtype = torch.float32</code>. However, any change here to\
          \ float16 raises the issue I mentioned.</p>\n<p>Please let me know what/where\
          \ changes are required pertaining to <code>input_features</code> as you\
          \ suggested above.</p>\n"
        raw: 'Thank you @sanchit-gandhi  for your response.


          I share my little Colab notebook.

          https://colab.research.google.com/drive/1uNCpZd6_g2MeuRn20AOW8cXs7Dbinoff


          I''m using simple pipeline call. The types for data and device are defined
          in the 5th cell. As such, for a given media file (an audio file, I''ve tested
          it on mp4/webm/mp3 formats), this code runs perfectly fine with `u_torch_dtype
          = torch.float32`. However, any change here to float16 raises the issue I
          mentioned.


          Please let me know what/where changes are required pertaining to `input_features`
          as you suggested above.'
        updatedAt: '2023-11-28T17:22:54.383Z'
      numEdits: 0
      reactions: []
    id: 656621ee7b5ed07358023fcc
    type: comment
  author: ait-paca
  content: 'Thank you @sanchit-gandhi  for your response.


    I share my little Colab notebook.

    https://colab.research.google.com/drive/1uNCpZd6_g2MeuRn20AOW8cXs7Dbinoff


    I''m using simple pipeline call. The types for data and device are defined in
    the 5th cell. As such, for a given media file (an audio file, I''ve tested it
    on mp4/webm/mp3 formats), this code runs perfectly fine with `u_torch_dtype =
    torch.float32`. However, any change here to float16 raises the issue I mentioned.


    Please let me know what/where changes are required pertaining to `input_features`
    as you suggested above.'
  created_at: 2023-11-28 17:22:54+00:00
  edited: false
  hidden: false
  id: 656621ee7b5ed07358023fcc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Whisper-Large-V3 does not work with explicit use of dtype which is given in
  config.json
