!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rizwanishaq
conflicting_files: null
created_at: 2023-11-15 15:23:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48ac305c0d901b362346a2773f0f513c.svg
      fullname: Rizwan Ishaq
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rizwanishaq
      type: user
    createdAt: '2023-11-15T15:23:32.000Z'
    data:
      edited: false
      editors:
      - rizwanishaq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8979158401489258
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48ac305c0d901b362346a2773f0f513c.svg
          fullname: Rizwan Ishaq
          isHf: false
          isPro: false
          name: rizwanishaq
          type: user
        html: '<p>How we can get the no_speech_prob in the pipeline?</p>

          '
        raw: How we can get the no_speech_prob in the pipeline?
        updatedAt: '2023-11-15T15:23:32.075Z'
      numEdits: 0
      reactions: []
    id: 6554e274fcb10ad33d88d085
    type: comment
  author: rizwanishaq
  content: How we can get the no_speech_prob in the pipeline?
  created_at: 2023-11-15 15:23:32+00:00
  edited: false
  hidden: false
  id: 6554e274fcb10ad33d88d085
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-11-15T15:57:38.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6146273612976074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>Hey!  Currently, these outputs are not yet supported mostly because\
          \ they are whisper specific. Would recommend you to use the following code\
          \ to get them: </p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ WhisperTokenizerFast, WhisperForConditionalGeneration, WhisperProcessor\n\
          <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\"\
          >import</span> load_dataset\n<span class=\"hljs-keyword\">import</span>\
          \ datasets\n<span class=\"hljs-keyword\">import</span> torch \n\n<span class=\"\
          hljs-comment\"># Load the sample audio from common_voice dataset</span>\n\
          ds = load_dataset(<span class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>).sort(<span class=\"hljs-string\">\"\
          id\"</span>)\ninput_speech = ds[<span class=\"hljs-number\">40</span>][<span\
          \ class=\"hljs-string\">\"audio\"</span>][<span class=\"hljs-string\">\"\
          array\"</span>]\n\n<span class=\"hljs-comment\"># Initialize  the model\
          \ and the processor</span>\nmodel = WhisperForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"openai/whisper-large-v3\"</span>)\nprocessor =\
          \ WhisperProcessor.from_pretrained(<span class=\"hljs-string\">\"openai/whisper-large-v3\"\
          </span>)\n\n<span class=\"hljs-comment\"># Extract input features from the\
          \ raw speech</span>\ninput_features = processor(raw_speech=input_speech,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\n<span class=\"\
          hljs-comment\"># Generate output logits with output_scores set to True</span>\n\
          output = model.generate(**input_features,  output_scores=<span class=\"\
          hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># Access the\
          \ last logits and calculate the probability of the target token</span>\n\
          last_logits = torch.cat(output.scores)\n\ntarget_token = <span class=\"\
          hljs-string\">\"&lt;|nospeech|&gt;\"</span>\ntarget_token_id = processor.tokenizer.convert_tokens_to_ids(target_token)\n\
          target_token_prob = last_logits[:, target_token_id]\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Probability of <span class=\"\
          hljs-subst\">{target_token}</span>: <span class=\"hljs-subst\">{target_token_prob}</span>\"\
          </span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output.sequences[<span\
          \ class=\"hljs-number\">0</span>])<span class=\"hljs-string\">\"</span>\n\
          <span class=\"hljs-string\">#  '&lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|startoflm|&gt;\
          \ A man said to the universe, Sir, I exist.&lt;|endoftext|&gt;' </span>\n\
          </code></pre>\n"
        raw: "Hey!  Currently, these outputs are not yet supported mostly because\
          \ they are whisper specific. Would recommend you to use the following code\
          \ to get them: \n```python \nfrom transformers import WhisperTokenizerFast,\
          \ WhisperForConditionalGeneration, WhisperProcessor\nfrom datasets import\
          \ load_dataset\nimport datasets\nimport torch \n\n# Load the sample audio\
          \ from common_voice dataset\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\").sort(\"id\")\ninput_speech = ds[40][\"\
          audio\"][\"array\"]\n\n# Initialize  the model and the processor\nmodel\
          \ = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\"\
          )\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\"\
          )\n\n# Extract input features from the raw speech\ninput_features = processor(raw_speech=input_speech,\
          \ return_tensors=\"pt\")\n\n# Generate output logits with output_scores\
          \ set to True\noutput = model.generate(**input_features,  output_scores=True)\n\
          \n# Access the last logits and calculate the probability of the target token\n\
          last_logits = torch.cat(output.scores)\n\ntarget_token = \"<|nospeech|>\"\
          \ntarget_token_id = processor.tokenizer.convert_tokens_to_ids(target_token)\n\
          target_token_prob = last_logits[:, target_token_id]\nprint(f\"Probability\
          \ of {target_token}: {target_token_prob}\")\nprint(tokenizer.decode(output.sequences[0])\"\
          \n#  '<|startoftranscript|><|en|><|startoflm|> A man said to the universe,\
          \ Sir, I exist.<|endoftext|>' \n```"
        updatedAt: '2023-11-15T15:57:38.618Z'
      numEdits: 0
      reactions: []
    id: 6554ea72710bb1dad2ffa7ed
    type: comment
  author: ArthurZ
  content: "Hey!  Currently, these outputs are not yet supported mostly because they\
    \ are whisper specific. Would recommend you to use the following code to get them:\
    \ \n```python \nfrom transformers import WhisperTokenizerFast, WhisperForConditionalGeneration,\
    \ WhisperProcessor\nfrom datasets import load_dataset\nimport datasets\nimport\
    \ torch \n\n# Load the sample audio from common_voice dataset\nds = load_dataset(\"\
    hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\").sort(\"\
    id\")\ninput_speech = ds[40][\"audio\"][\"array\"]\n\n# Initialize  the model\
    \ and the processor\nmodel = WhisperForConditionalGeneration.from_pretrained(\"\
    openai/whisper-large-v3\")\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\"\
    )\n\n# Extract input features from the raw speech\ninput_features = processor(raw_speech=input_speech,\
    \ return_tensors=\"pt\")\n\n# Generate output logits with output_scores set to\
    \ True\noutput = model.generate(**input_features,  output_scores=True)\n\n# Access\
    \ the last logits and calculate the probability of the target token\nlast_logits\
    \ = torch.cat(output.scores)\n\ntarget_token = \"<|nospeech|>\"\ntarget_token_id\
    \ = processor.tokenizer.convert_tokens_to_ids(target_token)\ntarget_token_prob\
    \ = last_logits[:, target_token_id]\nprint(f\"Probability of {target_token}: {target_token_prob}\"\
    )\nprint(tokenizer.decode(output.sequences[0])\"\n#  '<|startoftranscript|><|en|><|startoflm|>\
    \ A man said to the universe, Sir, I exist.<|endoftext|>' \n```"
  created_at: 2023-11-15 15:57:38+00:00
  edited: false
  hidden: false
  id: 6554ea72710bb1dad2ffa7ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7756c8277c47756d831760e22d43493a.svg
      fullname: Mingqi Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ac3eee
      type: user
    createdAt: '2023-12-14T19:29:07.000Z'
    data:
      edited: false
      editors:
      - Ac3eee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6261668801307678
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7756c8277c47756d831760e22d43493a.svg
          fullname: Mingqi Yu
          isHf: false
          isPro: false
          name: Ac3eee
          type: user
        html: "<blockquote>\n<p>Hey!  Currently, these outputs are not yet supported\
          \ mostly because they are whisper specific. Would recommend you to use the\
          \ following code to get them: </p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> WhisperTokenizerFast, WhisperForConditionalGeneration, WhisperProcessor\n\
          <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\"\
          >import</span> load_dataset\n<span class=\"hljs-keyword\">import</span>\
          \ datasets\n<span class=\"hljs-keyword\">import</span> torch \n\n<span class=\"\
          hljs-comment\"># Load the sample audio from common_voice dataset</span>\n\
          ds = load_dataset(<span class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>).sort(<span class=\"hljs-string\">\"\
          id\"</span>)\ninput_speech = ds[<span class=\"hljs-number\">40</span>][<span\
          \ class=\"hljs-string\">\"audio\"</span>][<span class=\"hljs-string\">\"\
          array\"</span>]\n\n<span class=\"hljs-comment\"># Initialize  the model\
          \ and the processor</span>\nmodel = WhisperForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"openai/whisper-large-v3\"</span>)\nprocessor =\
          \ WhisperProcessor.from_pretrained(<span class=\"hljs-string\">\"openai/whisper-large-v3\"\
          </span>)\n\n<span class=\"hljs-comment\"># Extract input features from the\
          \ raw speech</span>\ninput_features = processor(raw_speech=input_speech,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\n<span class=\"\
          hljs-comment\"># Generate output logits with output_scores set to True</span>\n\
          output = model.generate(**input_features,  output_scores=<span class=\"\
          hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># Access the\
          \ last logits and calculate the probability of the target token</span>\n\
          last_logits = torch.cat(output.scores)\n\ntarget_token = <span class=\"\
          hljs-string\">\"&lt;|nospeech|&gt;\"</span>\ntarget_token_id = processor.tokenizer.convert_tokens_to_ids(target_token)\n\
          target_token_prob = last_logits[:, target_token_id]\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Probability of <span class=\"\
          hljs-subst\">{target_token}</span>: <span class=\"hljs-subst\">{target_token_prob}</span>\"\
          </span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output.sequences[<span\
          \ class=\"hljs-number\">0</span>])<span class=\"hljs-string\">\"</span>\n\
          <span class=\"hljs-string\">#  '&lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|startoflm|&gt;\
          \ A man said to the universe, Sir, I exist.&lt;|endoftext|&gt;' </span>\n\
          </code></pre>\n</blockquote>\n<p>Is there a way to set the compression ratio\
          \ in pipe as well?</p>\n"
        raw: "> Hey!  Currently, these outputs are not yet supported mostly because\
          \ they are whisper specific. Would recommend you to use the following code\
          \ to get them: \n> ```python \n> from transformers import WhisperTokenizerFast,\
          \ WhisperForConditionalGeneration, WhisperProcessor\n> from datasets import\
          \ load_dataset\n> import datasets\n> import torch \n> \n> # Load the sample\
          \ audio from common_voice dataset\n> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\").sort(\"id\")\n> input_speech = ds[40][\"\
          audio\"][\"array\"]\n> \n> # Initialize  the model and the processor\n>\
          \ model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\"\
          )\n> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\"\
          )\n> \n> # Extract input features from the raw speech\n> input_features\
          \ = processor(raw_speech=input_speech, return_tensors=\"pt\")\n> \n> # Generate\
          \ output logits with output_scores set to True\n> output = model.generate(**input_features,\
          \  output_scores=True)\n> \n> # Access the last logits and calculate the\
          \ probability of the target token\n> last_logits = torch.cat(output.scores)\n\
          > \n> target_token = \"<|nospeech|>\"\n> target_token_id = processor.tokenizer.convert_tokens_to_ids(target_token)\n\
          > target_token_prob = last_logits[:, target_token_id]\n> print(f\"Probability\
          \ of {target_token}: {target_token_prob}\")\n> print(tokenizer.decode(output.sequences[0])\"\
          \n> #  '<|startoftranscript|><|en|><|startoflm|> A man said to the universe,\
          \ Sir, I exist.<|endoftext|>' \n> ```\n\nIs there a way to set the compression\
          \ ratio in pipe as well?"
        updatedAt: '2023-12-14T19:29:07.869Z'
      numEdits: 0
      reactions: []
    id: 657b578310609bba272bb4fc
    type: comment
  author: Ac3eee
  content: "> Hey!  Currently, these outputs are not yet supported mostly because\
    \ they are whisper specific. Would recommend you to use the following code to\
    \ get them: \n> ```python \n> from transformers import WhisperTokenizerFast, WhisperForConditionalGeneration,\
    \ WhisperProcessor\n> from datasets import load_dataset\n> import datasets\n>\
    \ import torch \n> \n> # Load the sample audio from common_voice dataset\n> ds\
    \ = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"\
    validation\").sort(\"id\")\n> input_speech = ds[40][\"audio\"][\"array\"]\n> \n\
    > # Initialize  the model and the processor\n> model = WhisperForConditionalGeneration.from_pretrained(\"\
    openai/whisper-large-v3\")\n> processor = WhisperProcessor.from_pretrained(\"\
    openai/whisper-large-v3\")\n> \n> # Extract input features from the raw speech\n\
    > input_features = processor(raw_speech=input_speech, return_tensors=\"pt\")\n\
    > \n> # Generate output logits with output_scores set to True\n> output = model.generate(**input_features,\
    \  output_scores=True)\n> \n> # Access the last logits and calculate the probability\
    \ of the target token\n> last_logits = torch.cat(output.scores)\n> \n> target_token\
    \ = \"<|nospeech|>\"\n> target_token_id = processor.tokenizer.convert_tokens_to_ids(target_token)\n\
    > target_token_prob = last_logits[:, target_token_id]\n> print(f\"Probability\
    \ of {target_token}: {target_token_prob}\")\n> print(tokenizer.decode(output.sequences[0])\"\
    \n> #  '<|startoftranscript|><|en|><|startoflm|> A man said to the universe, Sir,\
    \ I exist.<|endoftext|>' \n> ```\n\nIs there a way to set the compression ratio\
    \ in pipe as well?"
  created_at: 2023-12-14 19:29:07+00:00
  edited: false
  hidden: false
  id: 657b578310609bba272bb4fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: no_speech_prob in pipeline?
