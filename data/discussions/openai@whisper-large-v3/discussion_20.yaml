!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nateraw
conflicting_files: null
created_at: 2023-11-13 21:03:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
      fullname: Nate Raw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nateraw
      type: user
    createdAt: '2023-11-13T21:03:17.000Z'
    data:
      edited: false
      editors:
      - nateraw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9142076969146729
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
          fullname: Nate Raw
          isHf: false
          isPro: false
          name: nateraw
          type: user
        html: "<p>Hi there! Seems the snippet under the <a href=\"https://huggingface.co/openai/whisper-large-v3#speculative-decoding\"\
          >speculative decoding section</a> does not work. I just ran this in a new\
          \ colab notebook (link <a rel=\"nofollow\" href=\"https://gist.github.com/nateraw/2ec99277d11404292a6eb54213953f3a\"\
          >here</a>) and got the following:</p>\n<pre><code>TypeError: linear(): argument\
          \ 'input' (position 1) must be Tensor, not BaseModelOutput\n</code></pre>\n\
          <p>I meant to report this issue last week, but didn't get around to it.\
          \ the error, if I remember correctly, was different then than it is now.</p>\n\
          <p>Related...it seems the explanation in that section starts by saying you\
          \ can use <code>whisper-tiny</code>, but then also mentions using <code>distil-whisper</code>,\
          \ which isn't used in the snippet at all, so kind of confusing (unless this\
          \ <em>is</em> <code>whisper-tiny</code>??). Not sure! </p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;sanchit-gandhi&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/sanchit-gandhi\">@<span class=\"underline\"\
          >sanchit-gandhi</span></a></span>\n\n\t</span></span> / <span data-props=\"\
          {&quot;user&quot;:&quot;patrickvonplaten&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/patrickvonplaten\">@<span class=\"\
          underline\">patrickvonplaten</span></a></span>\n\n\t</span></span> can you\
          \ please have a look when you get the chance? :) Would really love to try\
          \ this out.</p>\n"
        raw: "Hi there! Seems the snippet under the [speculative decoding section](https://huggingface.co/openai/whisper-large-v3#speculative-decoding)\
          \ does not work. I just ran this in a new colab notebook (link [here](https://gist.github.com/nateraw/2ec99277d11404292a6eb54213953f3a))\
          \ and got the following:\r\n\r\n```\r\nTypeError: linear(): argument 'input'\
          \ (position 1) must be Tensor, not BaseModelOutput\r\n```\r\n\r\nI meant\
          \ to report this issue last week, but didn't get around to it. the error,\
          \ if I remember correctly, was different then than it is now.\r\n\r\nRelated...it\
          \ seems the explanation in that section starts by saying you can use `whisper-tiny`,\
          \ but then also mentions using `distil-whisper`, which isn't used in the\
          \ snippet at all, so kind of confusing (unless this *is* `whisper-tiny`??).\
          \ Not sure! \r\n\r\n@sanchit-gandhi / @patrickvonplaten can you please have\
          \ a look when you get the chance? :) Would really love to try this out."
        updatedAt: '2023-11-13T21:03:17.581Z'
      numEdits: 0
      reactions: []
    id: 65528f1574765dbef32776e1
    type: comment
  author: nateraw
  content: "Hi there! Seems the snippet under the [speculative decoding section](https://huggingface.co/openai/whisper-large-v3#speculative-decoding)\
    \ does not work. I just ran this in a new colab notebook (link [here](https://gist.github.com/nateraw/2ec99277d11404292a6eb54213953f3a))\
    \ and got the following:\r\n\r\n```\r\nTypeError: linear(): argument 'input' (position\
    \ 1) must be Tensor, not BaseModelOutput\r\n```\r\n\r\nI meant to report this\
    \ issue last week, but didn't get around to it. the error, if I remember correctly,\
    \ was different then than it is now.\r\n\r\nRelated...it seems the explanation\
    \ in that section starts by saying you can use `whisper-tiny`, but then also mentions\
    \ using `distil-whisper`, which isn't used in the snippet at all, so kind of confusing\
    \ (unless this *is* `whisper-tiny`??). Not sure! \r\n\r\n@sanchit-gandhi / @patrickvonplaten\
    \ can you please have a look when you get the chance? :) Would really love to\
    \ try this out."
  created_at: 2023-11-13 21:03:17+00:00
  edited: false
  hidden: false
  id: 65528f1574765dbef32776e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2760fc2904e113c19e2d57c9ec2c105a.svg
      fullname: Fugu Tech
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FU-Tech
      type: user
    createdAt: '2023-11-14T00:40:18.000Z'
    data:
      edited: false
      editors:
      - FU-Tech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.771479606628418
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2760fc2904e113c19e2d57c9ec2c105a.svg
          fullname: Fugu Tech
          isHf: false
          isPro: false
          name: FU-Tech
          type: user
        html: '<p>Same question here. I saw "distil-whisper" part coming from <a rel="nofollow"
          href="https://github.com/huggingface/distil-whisper">https://github.com/huggingface/distil-whisper</a>.
          Also, adding "return_dict=True" will pass this error, but get stuck in another
          error.<br>  File "/home/dev/.cache/pypoetry/virtualenvs/playground-_ijcKkog-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py",
          line 306, in _conv_forward<br>    return F.conv1d(input, weight, bias, self.stride,<br>RuntimeError:
          Given groups=1, weight of size [384, 80, 3], expected input[1, 1, 1500]
          to have 80 channels, but got 1 channels instead</p>

          '
        raw: "Same question here. I saw \"distil-whisper\" part coming from https://github.com/huggingface/distil-whisper.\
          \ Also, adding \"return_dict=True\" will pass this error, but get stuck\
          \ in another error.\n  File \"/home/dev/.cache/pypoetry/virtualenvs/playground-_ijcKkog-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
          , line 306, in _conv_forward\n    return F.conv1d(input, weight, bias, self.stride,\n\
          RuntimeError: Given groups=1, weight of size [384, 80, 3], expected input[1,\
          \ 1, 1500] to have 80 channels, but got 1 channels instead"
        updatedAt: '2023-11-14T00:40:18.318Z'
      numEdits: 0
      reactions: []
    id: 6552c1f2d05fe94e01af72da
    type: comment
  author: FU-Tech
  content: "Same question here. I saw \"distil-whisper\" part coming from https://github.com/huggingface/distil-whisper.\
    \ Also, adding \"return_dict=True\" will pass this error, but get stuck in another\
    \ error.\n  File \"/home/dev/.cache/pypoetry/virtualenvs/playground-_ijcKkog-py3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
    , line 306, in _conv_forward\n    return F.conv1d(input, weight, bias, self.stride,\n\
    RuntimeError: Given groups=1, weight of size [384, 80, 3], expected input[1, 1,\
    \ 1500] to have 80 channels, but got 1 channels instead"
  created_at: 2023-11-14 00:40:18+00:00
  edited: false
  hidden: false
  id: 6552c1f2d05fe94e01af72da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/020a1f243af516a992428d4687757a9e.svg
      fullname: Piotr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: piotrpolska
      type: user
    createdAt: '2023-11-15T18:20:09.000Z'
    data:
      edited: true
      editors:
      - patrickvonplaten
      - piotrpolska
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4678818881511688
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Ive also tried distil-whisper and had issues. This code works:</p>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> pipeline, AutoModelForCausalLM,\
          \ AutoModelForSpeechSeq2Seq, AutoProcessor\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-comment\"># from datasets import\
          \ load_dataset</span>\n\ndevice = <span class=\"hljs-string\">\"cuda:0\"\
          </span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\ntorch_dtype = torch.float16 <span class=\"hljs-keyword\">if</span>\
          \ torch.cuda.is_available() <span class=\"hljs-keyword\">else</span> torch.float32\n\
          \nassistant_model_id = <span class=\"hljs-string\">\"openai/whisper-tiny\"\
          </span>\n\nassistant_model = AutoModelForCausalLM.from_pretrained(\n   \
          \ assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>, use_safetensors=<span class=\"hljs-literal\"\
          >True</span>\n)\nassistant_model.to(device)\n\nmodel_id = <span class=\"\
          hljs-string\">\"openai/whisper-large-v3\"</span>\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>, use_safetensors=<span class=\"hljs-literal\"\
          >True</span>\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
          \npipe = pipeline(\n    <span class=\"hljs-string\">\"automatic-speech-recognition\"\
          </span>,\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=<span class=\"hljs-number\">128</span>,\n    generate_kwargs={<span\
          \ class=\"hljs-string\">\"assistant_model\"</span>: assistant_model},\n\
          \    torch_dtype=torch_dtype,\n    device=device,\n)\n\n<span class=\"hljs-comment\"\
          >#dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\",\
          \ \"clean\", split=\"validation\")</span>\n<span class=\"hljs-comment\"\
          >#sample = dataset[0][\"audio\"]</span>\n\n<span class=\"hljs-comment\"\
          >#result = pipe(sample)</span>\n<span class=\"hljs-comment\">#print(result[\"\
          text\"])</span>\n</code></pre>\n"
        raw: "Ive also tried distil-whisper and had issues. This code works:\n\n```py\n\
          from transformers import pipeline, AutoModelForCausalLM, AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor\nimport torch\n# from datasets import load_dataset\n\ndevice\
          \ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype =\
          \ torch.float16 if torch.cuda.is_available() else torch.float32\n\nassistant_model_id\
          \ = \"openai/whisper-tiny\"\n\nassistant_model = AutoModelForCausalLM.from_pretrained(\n\
          \    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True,\
          \ use_safetensors=True\n)\nassistant_model.to(device)\n\nmodel_id = \"openai/whisper-large-v3\"\
          \n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype,\
          \ low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\n\
          processor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n\
          \    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n\
          \    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n\
          \    generate_kwargs={\"assistant_model\": assistant_model},\n    torch_dtype=torch_dtype,\n\
          \    device=device,\n)\n\n#dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")\n#sample = dataset[0][\"audio\"]\n\n\
          #result = pipe(sample)\n#print(result[\"text\"])\n```"
        updatedAt: '2023-11-15T23:19:53.045Z'
      numEdits: 1
      reactions: []
    id: 65550bd9e0169cf32ccf98f2
    type: comment
  author: piotrpolska
  content: "Ive also tried distil-whisper and had issues. This code works:\n\n```py\n\
    from transformers import pipeline, AutoModelForCausalLM, AutoModelForSpeechSeq2Seq,\
    \ AutoProcessor\nimport torch\n# from datasets import load_dataset\n\ndevice =\
    \ \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16\
    \ if torch.cuda.is_available() else torch.float32\n\nassistant_model_id = \"openai/whisper-tiny\"\
    \n\nassistant_model = AutoModelForCausalLM.from_pretrained(\n    assistant_model_id,\
    \ torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nassistant_model.to(device)\n\
    \nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
    \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
    )\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n\
    pipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n  \
    \  tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
    \    max_new_tokens=128,\n    generate_kwargs={\"assistant_model\": assistant_model},\n\
    \    torch_dtype=torch_dtype,\n    device=device,\n)\n\n#dataset = load_dataset(\"\
    hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\
    #sample = dataset[0][\"audio\"]\n\n#result = pipe(sample)\n#print(result[\"text\"\
    ])\n```"
  created_at: 2023-11-15 18:20:09+00:00
  edited: true
  hidden: false
  id: 65550bd9e0169cf32ccf98f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-11-15T23:43:29.000Z'
    data:
      edited: true
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.869998574256897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>Aiii it seems like speculative decoding is accidentally broken on
          "main": <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/26892#issuecomment-1813470053">https://github.com/huggingface/transformers/pull/26892#issuecomment-1813470053</a></p>

          <p>Looking into reverting the PR: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/27523">https://github.com/huggingface/transformers/pull/27523</a></p>

          '
        raw: 'Aiii it seems like speculative decoding is accidentally broken on "main":
          https://github.com/huggingface/transformers/pull/26892#issuecomment-1813470053


          Looking into reverting the PR: https://github.com/huggingface/transformers/pull/27523'
        updatedAt: '2023-11-15T23:49:18.576Z'
      numEdits: 1
      reactions: []
    id: 655557a1dcf410fd07385e8c
    type: comment
  author: patrickvonplaten
  content: 'Aiii it seems like speculative decoding is accidentally broken on "main":
    https://github.com/huggingface/transformers/pull/26892#issuecomment-1813470053


    Looking into reverting the PR: https://github.com/huggingface/transformers/pull/27523'
  created_at: 2023-11-15 23:43:29+00:00
  edited: true
  hidden: false
  id: 655557a1dcf410fd07385e8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-11-16T00:00:03.000Z'
    data:
      edited: true
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9258220791816711
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>Also Whisper-v3 doesn''t yet work for speculative decoding because
          the model takes mel features of size 128 and not 80 and therefore there
          is a mismatch. We''re working on getting Distil-Whisper-v3 out somewhat
          soon which should better enable speculative decoding.</p>

          <p>Until Distil-Whisper-v3 is out, we''ll have to stick to Whisper-v2 for
          speculative decoding, I''m afraid, see code here: <a rel="nofollow" href="https://github.com/huggingface/distil-whisper#speculative-decoding">https://github.com/huggingface/distil-whisper#speculative-decoding</a></p>

          '
        raw: 'Also Whisper-v3 doesn''t yet work for speculative decoding because the
          model takes mel features of size 128 and not 80 and therefore there is a
          mismatch. We''re working on getting Distil-Whisper-v3 out somewhat soon
          which should better enable speculative decoding.


          Until Distil-Whisper-v3 is out, we''ll have to stick to Whisper-v2 for speculative
          decoding, I''m afraid, see code here: https://github.com/huggingface/distil-whisper#speculative-decoding'
        updatedAt: '2023-11-16T00:00:47.557Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FU-Tech
    id: 65555b83a0c34cd61a502c45
    type: comment
  author: patrickvonplaten
  content: 'Also Whisper-v3 doesn''t yet work for speculative decoding because the
    model takes mel features of size 128 and not 80 and therefore there is a mismatch.
    We''re working on getting Distil-Whisper-v3 out somewhat soon which should better
    enable speculative decoding.


    Until Distil-Whisper-v3 is out, we''ll have to stick to Whisper-v2 for speculative
    decoding, I''m afraid, see code here: https://github.com/huggingface/distil-whisper#speculative-decoding'
  created_at: 2023-11-16 00:00:03+00:00
  edited: true
  hidden: false
  id: 65555b83a0c34cd61a502c45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Speculative Decoding Snippet Not Working
