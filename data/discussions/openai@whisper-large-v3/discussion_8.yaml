!!python/object:huggingface_hub.community.DiscussionWithDetails
author: agonben23
conflicting_files: null
created_at: 2023-11-09 16:28:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17b18d876f2d41c3cfa5b04544325495.svg
      fullname: "Alejandro Gonz\xE1lez Ben\xEDtez"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agonben23
      type: user
    createdAt: '2023-11-09T16:28:05.000Z'
    data:
      edited: false
      editors:
      - agonben23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5242834687232971
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17b18d876f2d41c3cfa5b04544325495.svg
          fullname: "Alejandro Gonz\xE1lez Ben\xEDtez"
          isHf: false
          isPro: false
          name: agonben23
          type: user
        html: '<p>When executing the following code, an error appears that prevents
          using the model</p>

          <p>from transformers import pipeline<br>import torch </p>

          <p>pipe = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3")</p>

          <p>audio_file_path = "/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav"</p>

          <p>with open(audio_file_path, "rb") as audio_file:<br>    audio_content
          = audio_file.read()</p>

          <p>transcription = pipe(audio_content, lang="es")</p>

          <p>print(transcription)</p>

          <p>The complete traceback is specified below :</p>

          <hr>

          <p>ValueError                                Traceback (most recent call
          last)<br>Cell In[1], line 4<br>      1 from transformers import pipeline<br>      2
          import torch<br>----&gt; 4 pipe = pipeline("automatic-speech-recognition",
          model="openai/whisper-large-v3")<br>      6 audio_file_path = "/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav"<br>      8
          with open(audio_file_path, "rb") as audio_file:</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py:921,
          in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,
          framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,
          model_kwargs, pipeline_class, **kwargs)<br>    918             tokenizer_kwargs
          = model_kwargs.copy()<br>    919             tokenizer_kwargs.pop("torch_dtype",
          None)<br>--&gt; 921         tokenizer = AutoTokenizer.from_pretrained(<br>    922             tokenizer_identifier,
          use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs<br>    923         )<br>    925
          if load_image_processor:<br>    926     # Try to infer image processor from
          model or config name (if provided as str)<br>    927     if image_processor
          is None:</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:736,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    732     if tokenizer_class is None:<br>    733         raise
          ValueError(<br>    734             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    735         )<br>--&gt;
          736     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    738 # Otherwise we have to be creative.<br>    739
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    740 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          cache_dir, force_download, local_files_only, token, revision, *init_inputs,
          **kwargs)<br>   1851     else:<br>   1852         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1854 return cls._from_pretrained(<br>   1855     resolved_vocab_files,<br>   1856     pretrained_model_name_or_path,<br>   1857     init_configuration,<br>   1858     *init_inputs,<br>   1859     token=token,<br>   1860     cache_dir=cache_dir,<br>   1861     local_files_only=local_files_only,<br>   1862     _commit_hash=commit_hash,<br>   1863     _is_local=is_local,<br>   1864     **kwargs,<br>   1865
          )</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1886,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,
          *init_inputs, **kwargs)<br>   1884 has_tokenizer_file = resolved_vocab_files.get("tokenizer_file",
          None) is not None<br>   1885 if (from_slow or not has_tokenizer_file) and
          cls.slow_tokenizer_class is not None:<br>-&gt; 1886     slow_tokenizer =
          (cls.slow_tokenizer_class)._from_pretrained(<br>   1887         copy.deepcopy(resolved_vocab_files),<br>   1888         pretrained_model_name_or_path,<br>   1889         copy.deepcopy(init_configuration),<br>   1890         *init_inputs,<br>   1891         token=token,<br>   1892         cache_dir=cache_dir,<br>   1893         local_files_only=local_files_only,<br>   1894         _commit_hash=_commit_hash,<br>   1895         **(copy.deepcopy(kwargs)),<br>   1896     )<br>   1897
          else:<br>   1898     slow_tokenizer = None</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2073,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,
          *init_inputs, **kwargs)<br>   2066     raise ValueError(<br>   2067         f"Wrong
          index found for {token}: should be {tokenizer.convert_tokens_to_ids(token)}
          but found "<br>   2068         f"{index}."<br>   2069     )<br>   2070 elif
          not has_tokenizer_file and index != current_index:<br>   2071     # Tokenizer
          slow: added token cannot already be in the vocabulary so its index needs
          to be the<br>   2072     # current length of the tokenizer.<br>-&gt; 2073     raise
          ValueError(<br>   2074         f"Non-consecutive added token ''{token}''
          found. "<br>   2075         f"Should have index {current_index} but has
          index {index} in saved vocabulary."<br>   2076     )<br>   2078 is_special
          = bool(token in special_tokens)<br>   2079 if is_last_special is None or
          is_last_special == is_special:</p>

          <p>ValueError: Non-consecutive added token ''&lt;|0.02|&gt;'' found. Should
          have index 50365 but has index 50366 in saved vocabulary.</p>

          '
        raw: "When executing the following code, an error appears that prevents using\
          \ the model\r\n\r\nfrom transformers import pipeline\r\nimport torch \r\n\
          \r\npipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\
          )\r\n\r\naudio_file_path = \"/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav\"\
          \r\n\r\nwith open(audio_file_path, \"rb\") as audio_file:\r\n    audio_content\
          \ = audio_file.read()\r\n\r\ntranscription = pipe(audio_content, lang=\"\
          es\")\r\n\r\nprint(transcription)\r\n\r\nThe complete traceback is specified\
          \ below :\r\n\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[1], line 4\r\n      1 from transformers import pipeline\r\
          \n      2 import torch \r\n----> 4 pipe = pipeline(\"automatic-speech-recognition\"\
          , model=\"openai/whisper-large-v3\")\r\n      6 audio_file_path = \"/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav\"\
          \r\n      8 with open(audio_file_path, \"rb\") as audio_file:\r\n\r\nFile\
          \ /opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:921,\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    918\
          \             tokenizer_kwargs = model_kwargs.copy()\r\n    919        \
          \     tokenizer_kwargs.pop(\"torch_dtype\", None)\r\n--> 921         tokenizer\
          \ = AutoTokenizer.from_pretrained(\r\n    922             tokenizer_identifier,\
          \ use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs\r\
          \n    923         )\r\n    925 if load_image_processor:\r\n    926     #\
          \ Try to infer image processor from model or config name (if provided as\
          \ str)\r\n    927     if image_processor is None:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:736,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    732     if tokenizer_class is None:\r\n    733      \
          \   raise ValueError(\r\n    734             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    735         )\r\n\
          --> 736     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    738 # Otherwise we have to be creative.\r\n\
          \    739 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    740 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 1854 return cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\
          \n   1856     pretrained_model_name_or_path,\r\n   1857     init_configuration,\r\
          \n   1858     *init_inputs,\r\n   1859     token=token,\r\n   1860     cache_dir=cache_dir,\r\
          \n   1861     local_files_only=local_files_only,\r\n   1862     _commit_hash=commit_hash,\r\
          \n   1863     _is_local=is_local,\r\n   1864     **kwargs,\r\n   1865 )\r\
          \n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1886,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   1884 has_tokenizer_file\
          \ = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\r\n \
          \  1885 if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class\
          \ is not None:\r\n-> 1886     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
          \n   1887         copy.deepcopy(resolved_vocab_files),\r\n   1888      \
          \   pretrained_model_name_or_path,\r\n   1889         copy.deepcopy(init_configuration),\r\
          \n   1890         *init_inputs,\r\n   1891         token=token,\r\n   1892\
          \         cache_dir=cache_dir,\r\n   1893         local_files_only=local_files_only,\r\
          \n   1894         _commit_hash=_commit_hash,\r\n   1895         **(copy.deepcopy(kwargs)),\r\
          \n   1896     )\r\n   1897 else:\r\n   1898     slow_tokenizer = None\r\n\
          \r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2073,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   2066     raise\
          \ ValueError(\r\n   2067         f\"Wrong index found for {token}: should\
          \ be {tokenizer.convert_tokens_to_ids(token)} but found \"\r\n   2068  \
          \       f\"{index}.\"\r\n   2069     )\r\n   2070 elif not has_tokenizer_file\
          \ and index != current_index:\r\n   2071     # Tokenizer slow: added token\
          \ cannot already be in the vocabulary so its index needs to be the\r\n \
          \  2072     # current length of the tokenizer.\r\n-> 2073     raise ValueError(\r\
          \n   2074         f\"Non-consecutive added token '{token}' found. \"\r\n\
          \   2075         f\"Should have index {current_index} but has index {index}\
          \ in saved vocabulary.\"\r\n   2076     )\r\n   2078 is_special = bool(token\
          \ in special_tokens)\r\n   2079 if is_last_special is None or is_last_special\
          \ == is_special:\r\n\r\nValueError: Non-consecutive added token '<|0.02|>'\
          \ found. Should have index 50365 but has index 50366 in saved vocabulary."
        updatedAt: '2023-11-09T16:28:05.330Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - khassanoff
        - Usman8433
    id: 654d089571a30c4bcaf5f7ad
    type: comment
  author: agonben23
  content: "When executing the following code, an error appears that prevents using\
    \ the model\r\n\r\nfrom transformers import pipeline\r\nimport torch \r\n\r\n\
    pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\
    )\r\n\r\naudio_file_path = \"/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav\"\
    \r\n\r\nwith open(audio_file_path, \"rb\") as audio_file:\r\n    audio_content\
    \ = audio_file.read()\r\n\r\ntranscription = pipe(audio_content, lang=\"es\")\r\
    \n\r\nprint(transcription)\r\n\r\nThe complete traceback is specified below :\r\
    \n\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[1], line 4\r\n      1 from transformers import pipeline\r\n      2 import\
    \ torch \r\n----> 4 pipe = pipeline(\"automatic-speech-recognition\", model=\"\
    openai/whisper-large-v3\")\r\n      6 audio_file_path = \"/kaggle/input/audios3/6d1a6405e10d3a882536bd8363ace064e6cd80236e49d7e8a574c392ecc3ae05.wav\"\
    \r\n      8 with open(audio_file_path, \"rb\") as audio_file:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:921,\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,\
    \ model_kwargs, pipeline_class, **kwargs)\r\n    918             tokenizer_kwargs\
    \ = model_kwargs.copy()\r\n    919             tokenizer_kwargs.pop(\"torch_dtype\"\
    , None)\r\n--> 921         tokenizer = AutoTokenizer.from_pretrained(\r\n    922\
    \             tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs,\
    \ **tokenizer_kwargs\r\n    923         )\r\n    925 if load_image_processor:\r\
    \n    926     # Try to infer image processor from model or config name (if provided\
    \ as str)\r\n    927     if image_processor is None:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:736,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    732     if tokenizer_class is None:\r\n    733         raise\
    \ ValueError(\r\n    734             f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is not currently imported.\"\r\n    735         )\r\n--> 736\
    \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    738 # Otherwise we have to be creative.\r\n    739 # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default\r\n  \
    \  740 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 1854 return\
    \ cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\n   1856     pretrained_model_name_or_path,\r\
    \n   1857     init_configuration,\r\n   1858     *init_inputs,\r\n   1859    \
    \ token=token,\r\n   1860     cache_dir=cache_dir,\r\n   1861     local_files_only=local_files_only,\r\
    \n   1862     _commit_hash=commit_hash,\r\n   1863     _is_local=is_local,\r\n\
    \   1864     **kwargs,\r\n   1865 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1886,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   1884 has_tokenizer_file = resolved_vocab_files.get(\"\
    tokenizer_file\", None) is not None\r\n   1885 if (from_slow or not has_tokenizer_file)\
    \ and cls.slow_tokenizer_class is not None:\r\n-> 1886     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
    \n   1887         copy.deepcopy(resolved_vocab_files),\r\n   1888         pretrained_model_name_or_path,\r\
    \n   1889         copy.deepcopy(init_configuration),\r\n   1890         *init_inputs,\r\
    \n   1891         token=token,\r\n   1892         cache_dir=cache_dir,\r\n   1893\
    \         local_files_only=local_files_only,\r\n   1894         _commit_hash=_commit_hash,\r\
    \n   1895         **(copy.deepcopy(kwargs)),\r\n   1896     )\r\n   1897 else:\r\
    \n   1898     slow_tokenizer = None\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2073,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2066     raise ValueError(\r\n   2067        \
    \ f\"Wrong index found for {token}: should be {tokenizer.convert_tokens_to_ids(token)}\
    \ but found \"\r\n   2068         f\"{index}.\"\r\n   2069     )\r\n   2070 elif\
    \ not has_tokenizer_file and index != current_index:\r\n   2071     # Tokenizer\
    \ slow: added token cannot already be in the vocabulary so its index needs to\
    \ be the\r\n   2072     # current length of the tokenizer.\r\n-> 2073     raise\
    \ ValueError(\r\n   2074         f\"Non-consecutive added token '{token}' found.\
    \ \"\r\n   2075         f\"Should have index {current_index} but has index {index}\
    \ in saved vocabulary.\"\r\n   2076     )\r\n   2078 is_special = bool(token in\
    \ special_tokens)\r\n   2079 if is_last_special is None or is_last_special ==\
    \ is_special:\r\n\r\nValueError: Non-consecutive added token '<|0.02|>' found.\
    \ Should have index 50365 but has index 50366 in saved vocabulary."
  created_at: 2023-11-09 16:28:05+00:00
  edited: false
  hidden: false
  id: 654d089571a30c4bcaf5f7ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651fa6c44ae83c6592c62494/eiVe2wAYDPTuLb61PUgRV.png?w=200&h=200&f=face
      fullname: Muhammad Usman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Usman8433
      type: user
    createdAt: '2023-11-16T07:31:16.000Z'
    data:
      edited: false
      editors:
      - Usman8433
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463493824005127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651fa6c44ae83c6592c62494/eiVe2wAYDPTuLb61PUgRV.png?w=200&h=200&f=face
          fullname: Muhammad Usman
          isHf: false
          isPro: false
          name: Usman8433
          type: user
        html: '<p>I am also getting the same error.</p>

          '
        raw: I am also getting the same error.
        updatedAt: '2023-11-16T07:31:16.228Z'
      numEdits: 0
      reactions: []
    id: 6555c5441c523d260151401d
    type: comment
  author: Usman8433
  content: I am also getting the same error.
  created_at: 2023-11-16 07:31:16+00:00
  edited: false
  hidden: false
  id: 6555c5441c523d260151401d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-16T11:14:06.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8487722873687744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;agonben23&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/agonben23\"\
          >@<span class=\"underline\">agonben23</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;Usman8433&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Usman8433\">@<span class=\"\
          underline\">Usman8433</span></a></span>\n\n\t</span></span>! Could you both\
          \ confirm that you're running the latest version of the transformers package?\
          \ i.e. the version you get with:</p>\n<pre><code>pip install -U transformers\n\
          </code></pre>\n<p>I am not able to reproduce the error with the latest version\
          \ of the package. If it still persists after update, could you post the\
          \ output of the following command please:</p>\n<pre><code>transformers-cli\
          \ env\n</code></pre>\n"
        raw: 'Hey @agonben23 and @Usman8433! Could you both confirm that you''re running
          the latest version of the transformers package? i.e. the version you get
          with:

          ```

          pip install -U transformers

          ```


          I am not able to reproduce the error with the latest version of the package.
          If it still persists after update, could you post the output of the following
          command please:

          ```

          transformers-cli env

          ```'
        updatedAt: '2023-11-16T11:14:06.873Z'
      numEdits: 0
      reactions: []
    id: 6555f97ee3772d4af00db50f
    type: comment
  author: sanchit-gandhi
  content: 'Hey @agonben23 and @Usman8433! Could you both confirm that you''re running
    the latest version of the transformers package? i.e. the version you get with:

    ```

    pip install -U transformers

    ```


    I am not able to reproduce the error with the latest version of the package. If
    it still persists after update, could you post the output of the following command
    please:

    ```

    transformers-cli env

    ```'
  created_at: 2023-11-16 11:14:06+00:00
  edited: false
  hidden: false
  id: 6555f97ee3772d4af00db50f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e0930e068b065336b1d308d0e621e.svg
      fullname: louiesheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthew2023
      type: user
    createdAt: '2023-11-28T04:19:27.000Z'
    data:
      edited: false
      editors:
      - matthew2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6958611011505127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e0930e068b065336b1d308d0e621e.svg
          fullname: louiesheng
          isHf: false
          isPro: false
          name: matthew2023
          type: user
        html: "<blockquote>\n<p>Hey <span data-props=\"{&quot;user&quot;:&quot;agonben23&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/agonben23\"\
          >@<span class=\"underline\">agonben23</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;Usman8433&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Usman8433\">@<span class=\"\
          underline\">Usman8433</span></a></span>\n\n\t</span></span>! Could you both\
          \ confirm that you're running the latest version of the transformers package?\
          \ i.e. the version you get with:</p>\n<pre><code>pip install -U transformers\n\
          </code></pre>\n<p>I am not able to reproduce the error with the latest version\
          \ of the package. If it still persists after update, could you post the\
          \ output of the following command please:</p>\n<pre><code>transformers-cli\
          \ env\n</code></pre>\n</blockquote>\n<ul>\n<li><code>transformers</code>\
          \ version: 4.36.0.dev0</li>\n<li>Platform: Windows-10-10.0.22631-SP0</li>\n\
          <li>Python version: 3.11.5</li>\n<li>Huggingface_hub version: 0.19.4</li>\n\
          <li>Safetensors version: 0.4.0</li>\n<li>Accelerate version: 0.24.1</li>\n\
          <li>Accelerate config:    not found</li>\n<li>PyTorch version (GPU?): 2.1.1\
          \ (True)</li>\n<li>Tensorflow version (GPU?): not installed (NA)</li>\n\
          <li>Flax version (CPU?/GPU?/TPU?): not installed (NA)</li>\n<li>Jax version:\
          \ not installed</li>\n<li>JaxLib version: not installed</li>\n<li>Using\
          \ GPU in script?: </li>\n<li>Using distributed or parallel set-up in script?:\
          \ </li>\n</ul>\n"
        raw: "> Hey @agonben23 and @Usman8433! Could you both confirm that you're\
          \ running the latest version of the transformers package? i.e. the version\
          \ you get with:\n> ```\n> pip install -U transformers\n> ```\n> \n> I am\
          \ not able to reproduce the error with the latest version of the package.\
          \ If it still persists after update, could you post the output of the following\
          \ command please:\n> ```\n> transformers-cli env\n> ```\n\n- `transformers`\
          \ version: 4.36.0.dev0\n- Platform: Windows-10-10.0.22631-SP0\n- Python\
          \ version: 3.11.5\n- Huggingface_hub version: 0.19.4\n- Safetensors version:\
          \ 0.4.0\n- Accelerate version: 0.24.1\n- Accelerate config:    not found\n\
          - PyTorch version (GPU?): 2.1.1 (True)\n- Tensorflow version (GPU?): not\
          \ installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n\
          - Jax version: not installed\n- JaxLib version: not installed\n- Using GPU\
          \ in script?: <fill in>\n- Using distributed or parallel set-up in script?:\
          \ <fill in>"
        updatedAt: '2023-11-28T04:19:27.837Z'
      numEdits: 0
      reactions: []
    id: 65656a4fbfb71b4ea3a4797b
    type: comment
  author: matthew2023
  content: "> Hey @agonben23 and @Usman8433! Could you both confirm that you're running\
    \ the latest version of the transformers package? i.e. the version you get with:\n\
    > ```\n> pip install -U transformers\n> ```\n> \n> I am not able to reproduce\
    \ the error with the latest version of the package. If it still persists after\
    \ update, could you post the output of the following command please:\n> ```\n\
    > transformers-cli env\n> ```\n\n- `transformers` version: 4.36.0.dev0\n- Platform:\
    \ Windows-10-10.0.22631-SP0\n- Python version: 3.11.5\n- Huggingface_hub version:\
    \ 0.19.4\n- Safetensors version: 0.4.0\n- Accelerate version: 0.24.1\n- Accelerate\
    \ config:    not found\n- PyTorch version (GPU?): 2.1.1 (True)\n- Tensorflow version\
    \ (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n\
    - Jax version: not installed\n- JaxLib version: not installed\n- Using GPU in\
    \ script?: <fill in>\n- Using distributed or parallel set-up in script?: <fill\
    \ in>"
  created_at: 2023-11-28 04:19:27+00:00
  edited: false
  hidden: false
  id: 65656a4fbfb71b4ea3a4797b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7e0930e068b065336b1d308d0e621e.svg
      fullname: louiesheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthew2023
      type: user
    createdAt: '2023-11-28T04:22:11.000Z'
    data:
      edited: false
      editors:
      - matthew2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.27734050154685974
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7e0930e068b065336b1d308d0e621e.svg
          fullname: louiesheng
          isHf: false
          isPro: false
          name: matthew2023
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\connectionpool.py",
          line 715, in urlopen<br>    httplib_response = self._make_request(<br>                       ^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\connectionpool.py",
          line 404, in _make_request<br>    self._validate_conn(conn)<br>  File "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\connectionpool.py",
          line 1058, in <em>validate_conn<br>    conn.connect()<br>  File "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\connection.py",
          line 419, in connect<br>    self.sock = ssl_wrap_socket(<br>                ^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\util\ssl</em>.py",
          line 449, in ssl_wrap_socket<br>    ssl_sock = <em>ssl_wrap_socket_impl(<br>               ^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\util\ssl</em>.py",
          line 493, in _ssl_wrap_socket_impl<br>    return ssl_context.wrap_socket(sock,
          server_hostname=server_hostname)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\ssl.py", line 517, in wrap_socket<br>    return
          self.sslsocket_class._create(<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\ssl.py", line 1108, in _create<br>    self.do_handshake()<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\ssl.py", line 1379, in do_handshake<br>    self._sslobj.do_handshake()<br>ssl.SSLEOFError:
          [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol
          (_ssl.c:1006)</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\requests\adapters.py",
          line 486, in send<br>    resp = conn.urlopen(<br>           ^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\connectionpool.py",
          line 799, in urlopen<br>    retries = retries.increment(<br>              ^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\urllib3\util\retry.py",
          line 592, in increment<br>    raise MaxRetryError(_pool, url, error or ResponseError(cause))<br>urllib3.exceptions.MaxRetryError:
          HTTPSConnectionPool(host=''huggingface.co'', port=443): Max retries exceeded
          with url: /openai/whisper-large-v3/resolve/main/config.json (Caused by SSLError(SSLEOFError(8,
          ''[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol
          (_ssl.c:1006)'')))</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\l''t''s\code\whisper\fast_tr.py",
          line 12, in <br>    model = AutoModelForSpeechSeq2Seq.from_pretrained(<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\transformers\models\auto\auto_factory.py",
          line 488, in from_pretrained<br>    resolved_config_file = cached_file(<br>                           ^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\transformers\utils\hub.py",
          line 389, in cached_file<br>    resolved_file = hf_hub_download(<br>                    ^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\utils_validators.py",
          line 118, in _inner_fn<br>    return fn(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\file_download.py",
          line 1247, in hf_hub_download<br>    metadata = get_hf_file_metadata(<br>               ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\utils_validators.py",
          line 118, in _inner_fn<br>    return fn(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\file_download.py",
          line 1624, in get_hf_file_metadata<br>    r = _request_wrapper(<br>        ^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\file_download.py",
          line 402, in _request_wrapper<br>    response = _request_wrapper(<br>               ^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\file_download.py",
          line 425, in _request_wrapper<br>    response = get_session().request(method=method,
          url=url, **params)<br>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\requests\sessions.py",
          line 589, in request<br>    resp = self.send(prep, **send_kwargs)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\requests\sessions.py",
          line 703, in send<br>    r = adapter.send(request, **kwargs)<br>        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\huggingface_hub\utils_http.py",
          line 63, in send<br>    return super().send(request, *args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\l''t''s.conda\envs\py311\Lib\site-packages\requests\adapters.py",
          line 517, in send<br>    raise SSLError(e, request=request)<br>requests.exceptions.SSLError:
          (MaxRetryError("HTTPSConnectionPool(host=''huggingface.co'', port=443):
          Max retries exceeded with url: /openai/whisper-large-v3/resolve/main/config.json
          (Caused by SSLError(SSLEOFError(8, ''[SSL: UNEXPECTED_EOF_WHILE_READING]
          EOF occurred in violation of protocol (_ssl.c:1006)'')))"), ''(Request ID:
          2871eed3-0659-4e0a-93ba-2c8d16c599af)'')</p>

          '
        raw: "Traceback (most recent call last):\n  File \"C:\\Users\\l't's\\.conda\\\
          envs\\py311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 715,\
          \ in urlopen\n    httplib_response = self._make_request(\n             \
          \          ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
          py311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 404, in _make_request\n\
          \    self._validate_conn(conn)\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
          py311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1058, in _validate_conn\n\
          \    conn.connect()\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\\
          site-packages\\urllib3\\connection.py\", line 419, in connect\n    self.sock\
          \ = ssl_wrap_socket(\n                ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\\
          l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\"\
          , line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\\
          envs\\py311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in\
          \ _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\ssl.py\", line 517,\
          \ in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\ssl.py\", line 1108,\
          \ in _create\n    self.do_handshake()\n  File \"C:\\Users\\l't's\\.conda\\\
          envs\\py311\\Lib\\ssl.py\", line 1379, in do_handshake\n    self._sslobj.do_handshake()\n\
          ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation\
          \ of protocol (_ssl.c:1006)\n\nDuring handling of the above exception, another\
          \ exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\\
          Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\adapters.py\"\
          , line 486, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\\
          connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n\
          \              ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
          py311\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n\
          \    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError:\
          \ HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded\
          \ with url: /openai/whisper-large-v3/resolve/main/config.json (Caused by\
          \ SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred\
          \ in violation of protocol (_ssl.c:1006)')))\n\nDuring handling of the above\
          \ exception, another exception occurred:\n\nTraceback (most recent call\
          \ last):\n  File \"C:\\Users\\l't's\\code\\whisper\\fast_tr.py\", line 12,\
          \ in <module>\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\\
          l't's\\.conda\\envs\\py311\\Lib\\site-packages\\transformers\\models\\auto\\\
          auto_factory.py\", line 488, in from_pretrained\n    resolved_config_file\
          \ = cached_file(\n                           ^^^^^^^^^^^^\n  File \"C:\\\
          Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\\
          hub.py\", line 389, in cached_file\n    resolved_file = hf_hub_download(\n\
          \                    ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\\
          envs\\py311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\"\
          , line 118, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
          file_download.py\", line 1247, in hf_hub_download\n    metadata = get_hf_file_metadata(\n\
          \               ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\\
          envs\\py311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\"\
          , line 118, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
          file_download.py\", line 1624, in get_hf_file_metadata\n    r = _request_wrapper(\n\
          \        ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\\
          Lib\\site-packages\\huggingface_hub\\file_download.py\", line 402, in _request_wrapper\n\
          \    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File\
          \ \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
          file_download.py\", line 425, in _request_wrapper\n    response = get_session().request(method=method,\
          \ url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\\
          sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\\
          .conda\\envs\\py311\\Lib\\site-packages\\requests\\sessions.py\", line 703,\
          \ in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
          utils\\_http.py\", line 63, in send\n    return super().send(request, *args,\
          \ **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\adapters.py\"\
          , line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError:\
          \ (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443):\
          \ Max retries exceeded with url: /openai/whisper-large-v3/resolve/main/config.json\
          \ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING]\
          \ EOF occurred in violation of protocol (_ssl.c:1006)')))\"), '(Request\
          \ ID: 2871eed3-0659-4e0a-93ba-2c8d16c599af)')"
        updatedAt: '2023-11-28T04:22:11.923Z'
      numEdits: 0
      reactions: []
    id: 65656af3e09d6576686de759
    type: comment
  author: matthew2023
  content: "Traceback (most recent call last):\n  File \"C:\\Users\\l't's\\.conda\\\
    envs\\py311\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 715, in urlopen\n\
    \    httplib_response = self._make_request(\n                       ^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\\
    connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\\
    connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File\
    \ \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\connection.py\"\
    , line 419, in connect\n    self.sock = ssl_wrap_socket(\n                ^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\\
    util\\ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n\
    \               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
    py311\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n\
    \    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n \
    \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
    \ \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\ssl.py\", line 517, in wrap_socket\n\
    \    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\ssl.py\", line 1108, in\
    \ _create\n    self.do_handshake()\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
    py311\\Lib\\ssl.py\", line 1379, in do_handshake\n    self._sslobj.do_handshake()\n\
    ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation\
    \ of protocol (_ssl.c:1006)\n\nDuring handling of the above exception, another\
    \ exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\\
    l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\adapters.py\", line\
    \ 486, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"\
    C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\connectionpool.py\"\
    , line 799, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\urllib3\\\
    util\\retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url,\
    \ error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co',\
    \ port=443): Max retries exceeded with url: /openai/whisper-large-v3/resolve/main/config.json\
    \ (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF\
    \ occurred in violation of protocol (_ssl.c:1006)')))\n\nDuring handling of the\
    \ above exception, another exception occurred:\n\nTraceback (most recent call\
    \ last):\n  File \"C:\\Users\\l't's\\code\\whisper\\fast_tr.py\", line 12, in\
    \ <module>\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n         \
    \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\\
    envs\\py311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 488, in from_pretrained\n    resolved_config_file = cached_file(\n    \
    \                       ^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\\
    py311\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 389, in cached_file\n\
    \    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
    utils\\_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n\
    \           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\\
    Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1247, in hf_hub_download\n\
    \    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\\
    utils\\_validators.py\", line 118, in _inner_fn\n    return fn(*args, **kwargs)\n\
    \           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\\
    Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1624, in get_hf_file_metadata\n\
    \    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\\
    .conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\file_download.py\",\
    \ line 402, in _request_wrapper\n    response = _request_wrapper(\n          \
    \     ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\\
    site-packages\\huggingface_hub\\file_download.py\", line 425, in _request_wrapper\n\
    \    response = get_session().request(method=method, url=url, **params)\n    \
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\sessions.py\"\
    , line 589, in request\n    resp = self.send(prep, **send_kwargs)\n          \
    \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\\
    Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request,\
    \ **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\l't's\\\
    .conda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line\
    \ 63, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\l't's\\.conda\\envs\\py311\\Lib\\site-packages\\requests\\\
    adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError:\
    \ (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries\
    \ exceeded with url: /openai/whisper-large-v3/resolve/main/config.json (Caused\
    \ by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred\
    \ in violation of protocol (_ssl.c:1006)')))\"), '(Request ID: 2871eed3-0659-4e0a-93ba-2c8d16c599af)')"
  created_at: 2023-11-28 04:22:11+00:00
  edited: false
  hidden: false
  id: 65656af3e09d6576686de759
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Error when running a model inference
