!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ctranslate2-4you
conflicting_files: null
created_at: 2023-11-07 23:04:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec5cbabfb5c46d559979a7487963d1ec.svg
      fullname: Blair Chintella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ctranslate2-4you
      type: user
    createdAt: '2023-11-07T23:04:49.000Z'
    data:
      edited: false
      editors:
      - ctranslate2-4you
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755660891532898
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec5cbabfb5c46d559979a7487963d1ec.svg
          fullname: Blair Chintella
          isHf: false
          isPro: false
          name: ctranslate2-4you
          type: user
        html: '<p>So why was the decision made to only release the float16 version,
          or is that the way it was trained from the get to?  All other whisper models
          are in float32, everything up to large-v2?  Any admins care to enlighten
          the rest of us?  Thanks.</p>

          '
        raw: So why was the decision made to only release the float16 version, or
          is that the way it was trained from the get to?  All other whisper models
          are in float32, everything up to large-v2?  Any admins care to enlighten
          the rest of us?  Thanks.
        updatedAt: '2023-11-07T23:04:49.090Z'
      numEdits: 0
      reactions: []
    id: 654ac2917e27d1568164e1df
    type: comment
  author: ctranslate2-4you
  content: So why was the decision made to only release the float16 version, or is
    that the way it was trained from the get to?  All other whisper models are in
    float32, everything up to large-v2?  Any admins care to enlighten the rest of
    us?  Thanks.
  created_at: 2023-11-07 23:04:49+00:00
  edited: false
  hidden: false
  id: 654ac2917e27d1568164e1df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-08T12:10:49.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8693858981132507
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>OpenAI only publish fp16 weights, so we know the weights work as
          intended in half-precision. To improve the download speed for users, the
          main <code>transformers</code> weights are also fp16 (half the size of fp32
          weights =&gt; half the download time). So that fp32 weights can be used,
          I''ve also pushed fp32 weights to this repo: <a href="/openai/whisper-large-v3/discussions/5">#5</a>.
          You can load them by passing <code>variant="fp32"</code> when you call <code>from_pretrained</code>:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> WhisperForConditionalGeneration


          model = WhisperForConditionalGeneration.from_pretrained(<span class="hljs-string">"openai/whisper-large-v3"</span>,
          variant=<span class="hljs-string">"fp32"</span>)

          </code></pre>

          '
        raw: 'OpenAI only publish fp16 weights, so we know the weights work as intended
          in half-precision. To improve the download speed for users, the main `transformers`
          weights are also fp16 (half the size of fp32 weights => half the download
          time). So that fp32 weights can be used, I''ve also pushed fp32 weights
          to this repo: #5. You can load them by passing `variant="fp32"` when you
          call `from_pretrained`:

          ```python

          from transformers import WhisperForConditionalGeneration


          model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3",
          variant="fp32")

          ```'
        updatedAt: '2023-11-08T12:10:49.412Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ifredom
        - yehiaserag
        - MaxB629
        - javadr
    id: 654b7ac93ef2f1e3d316246b
    type: comment
  author: sanchit-gandhi
  content: 'OpenAI only publish fp16 weights, so we know the weights work as intended
    in half-precision. To improve the download speed for users, the main `transformers`
    weights are also fp16 (half the size of fp32 weights => half the download time).
    So that fp32 weights can be used, I''ve also pushed fp32 weights to this repo:
    #5. You can load them by passing `variant="fp32"` when you call `from_pretrained`:

    ```python

    from transformers import WhisperForConditionalGeneration


    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3",
    variant="fp32")

    ```'
  created_at: 2023-11-08 12:10:49+00:00
  edited: false
  hidden: false
  id: 654b7ac93ef2f1e3d316246b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eaff66003bc3d776d93e6c5c2024a824.svg
      fullname: Isaac Weingarten
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: isaac-telebroad
      type: user
    createdAt: '2023-12-21T23:20:23.000Z'
    data:
      edited: true
      editors:
      - isaac-telebroad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4681847393512726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eaff66003bc3d776d93e6c5c2024a824.svg
          fullname: Isaac Weingarten
          isHf: false
          isPro: false
          name: isaac-telebroad
          type: user
        html: "<p>with attn_implementation=\"flash_attention_2\" i dont see any performance\
          \ boost when using the  fp32 vs not</p>\n<pre><code class=\"language-python\"\
          >model = WhisperForConditionalGeneration.from_pretrained(\n            model_id,\n\
          \            variant=<span class=\"hljs-string\">\"fp32\"</span> <span class=\"\
          hljs-keyword\">if</span> on_gpu_float_size == <span class=\"hljs-number\"\
          >32</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\"\
          >\"\"</span>,\n            torch_dtype=torch_dtype,\n            low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>,\n            use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>,\n            attn_implementation=<span\
          \ class=\"hljs-string\">\"flash_attention_2\"</span> \n)\n</code></pre>\n"
        raw: "with attn_implementation=\"flash_attention_2\" i dont see any performance\
          \ boost when using the  fp32 vs not\n````python\nmodel = WhisperForConditionalGeneration.from_pretrained(\n\
          \            model_id,\n            variant=\"fp32\" if on_gpu_float_size\
          \ == 32 else \"\",\n            torch_dtype=torch_dtype,\n            low_cpu_mem_usage=True,\n\
          \            use_safetensors=True,\n            attn_implementation=\"flash_attention_2\"\
          \ \n)\n````"
        updatedAt: '2023-12-21T23:21:38.162Z'
      numEdits: 2
      reactions: []
    id: 6584c837f5358611c0818aa3
    type: comment
  author: isaac-telebroad
  content: "with attn_implementation=\"flash_attention_2\" i dont see any performance\
    \ boost when using the  fp32 vs not\n````python\nmodel = WhisperForConditionalGeneration.from_pretrained(\n\
    \            model_id,\n            variant=\"fp32\" if on_gpu_float_size == 32\
    \ else \"\",\n            torch_dtype=torch_dtype,\n            low_cpu_mem_usage=True,\n\
    \            use_safetensors=True,\n            attn_implementation=\"flash_attention_2\"\
    \ \n)\n````"
  created_at: 2023-12-21 23:20:23+00:00
  edited: true
  hidden: false
  id: 6584c837f5358611c0818aa3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2024-01-04T11:53:35.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737815260887146
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>This is as expected: there shouldn't be any performance boost in\
          \ using the fp32 weights <span data-props=\"{&quot;user&quot;:&quot;isaac-telebroad&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/isaac-telebroad\"\
          >@<span class=\"underline\">isaac-telebroad</span></a></span>\n\n\t</span></span>.\
          \ Since the model was only trained in fp16, upcasting the weights from fp16\
          \ -&gt; fp32 for inference won't give any performance gain (no free lunches!)</p>\n"
        raw: 'This is as expected: there shouldn''t be any performance boost in using
          the fp32 weights @isaac-telebroad. Since the model was only trained in fp16,
          upcasting the weights from fp16 -> fp32 for inference won''t give any performance
          gain (no free lunches!)'
        updatedAt: '2024-01-04T11:53:35.225Z'
      numEdits: 0
      reactions: []
    id: 65969c3f994d0ef58183eb73
    type: comment
  author: sanchit-gandhi
  content: 'This is as expected: there shouldn''t be any performance boost in using
    the fp32 weights @isaac-telebroad. Since the model was only trained in fp16, upcasting
    the weights from fp16 -> fp32 for inference won''t give any performance gain (no
    free lunches!)'
  created_at: 2024-01-04 11:53:35+00:00
  edited: false
  hidden: false
  id: 65969c3f994d0ef58183eb73
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: openai/whisper-large-v3
repo_type: model
status: open
target_branch: null
title: Why float16?  Why not Float32
