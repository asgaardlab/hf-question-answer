!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BMU
conflicting_files: null
created_at: 2022-07-18 07:28:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dd34a7894638cc15516621eb4993c00.svg
      fullname: Benat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BMU
      type: user
    createdAt: '2022-07-18T08:28:59.000Z'
    data:
      edited: false
      editors:
      - BMU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dd34a7894638cc15516621eb4993c00.svg
          fullname: Benat
          isHf: false
          isPro: false
          name: BMU
          type: user
        html: '<p>I am trying to speed up the inference. Because of the limit in the
          number of tokens of the input text, I split the input text before hand into
          chunks. I then have a for loop to classify each chunk:</p>

          <p><code>result_list  = list()</code><br><code>for text in text_list:</code><br>               <code>results
          = pipeline(text)</code><br>              <code> result_list.append(results)</code></p>

          <p>I tried with the solution by tyrex in <a rel="nofollow" href="https://stackoverflow.com/questions/9786102/how-do-i-parallelize-a-simple-python-loop">https://stackoverflow.com/questions/9786102/how-do-i-parallelize-a-simple-python-loop</a>.
          But I get this warning:</p>

          <p><code>huggingface/tokenizers: The current process just got forked, after
          parallelism has already been used. Disabling parallelism to avoid deadlocks...</code><br><code>To
          disable this warning, you can either:</code><br>    <code>    - Avoid using
          `tokenizers` before the fork if possible</code><br>        <code>- Explicitly
          set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></p>

          <p>I did some research about parallelization of inference of transformers
          models. There is <code>parallelformers</code><br> (<a rel="nofollow" href="https://github.com/tunib-ai/parallelformers">https://github.com/tunib-ai/parallelformers</a>),
          but ESG-BERT is not in the current list of supported models. I also found
          this article on how to parallelise the inference of transformers models
          on CPU: <a rel="nofollow" href="https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23">https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23</a>.</p>

          <p>Any advice on the best way to speed up the inference of ESG-BERT?</p>

          '
        raw: "I am trying to speed up the inference. Because of the limit in the number\
          \ of tokens of the input text, I split the input text before hand into chunks.\
          \ I then have a for loop to classify each chunk:\r\n\r\n```result_list \
          \ = list()```\r\n```for text in text_list:```\r\n               ```results\
          \ = pipeline(text)```\r\n              ``` result_list.append(results)```\r\
          \n\r\nI tried with the solution by tyrex in https://stackoverflow.com/questions/9786102/how-do-i-parallelize-a-simple-python-loop.\
          \ But I get this warning:\r\n\r\n```huggingface/tokenizers: The current\
          \ process just got forked, after parallelism has already been used. Disabling\
          \ parallelism to avoid deadlocks...```\r\n```To disable this warning, you\
          \ can either:```\r\n    ```    - Avoid using `tokenizers` before the fork\
          \ if possible```\r\n        ```- Explicitly set the environment variable\
          \ TOKENIZERS_PARALLELISM=(true | false)```\r\n\r\nI did some research about\
          \ parallelization of inference of transformers models. There is ```parallelformers```\r\
          \n (https://github.com/tunib-ai/parallelformers), but ESG-BERT is not in\
          \ the current list of supported models. I also found this article on how\
          \ to parallelise the inference of transformers models on CPU: https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23.\r\
          \n\r\nAny advice on the best way to speed up the inference of ESG-BERT?\r\
          \n"
        updatedAt: '2022-07-18T08:28:59.735Z'
      numEdits: 0
      reactions: []
    id: 62d519cba16e421ae484ee81
    type: comment
  author: BMU
  content: "I am trying to speed up the inference. Because of the limit in the number\
    \ of tokens of the input text, I split the input text before hand into chunks.\
    \ I then have a for loop to classify each chunk:\r\n\r\n```result_list  = list()```\r\
    \n```for text in text_list:```\r\n               ```results = pipeline(text)```\r\
    \n              ``` result_list.append(results)```\r\n\r\nI tried with the solution\
    \ by tyrex in https://stackoverflow.com/questions/9786102/how-do-i-parallelize-a-simple-python-loop.\
    \ But I get this warning:\r\n\r\n```huggingface/tokenizers: The current process\
    \ just got forked, after parallelism has already been used. Disabling parallelism\
    \ to avoid deadlocks...```\r\n```To disable this warning, you can either:```\r\
    \n    ```    - Avoid using `tokenizers` before the fork if possible```\r\n   \
    \     ```- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true\
    \ | false)```\r\n\r\nI did some research about parallelization of inference of\
    \ transformers models. There is ```parallelformers```\r\n (https://github.com/tunib-ai/parallelformers),\
    \ but ESG-BERT is not in the current list of supported models. I also found this\
    \ article on how to parallelise the inference of transformers models on CPU: https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23.\r\
    \n\r\nAny advice on the best way to speed up the inference of ESG-BERT?\r\n"
  created_at: 2022-07-18 07:28:59+00:00
  edited: false
  hidden: false
  id: 62d519cba16e421ae484ee81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
      fullname: Nicholas Broad
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: true
      name: nbroad
      type: user
    createdAt: '2022-07-18T21:12:38.000Z'
    data:
      edited: false
      editors:
      - nbroad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
          fullname: Nicholas Broad
          isHf: true
          isPro: true
          name: nbroad
          type: user
        html: '<p>I wouldn''t recommend parallelizing.</p>

          <p>Hugging Face has another library called <a rel="nofollow" href="https://github.com/huggingface/optimum/">optimum</a>
          that can help you speed up your model.</p>

          <p>See here for an article explaining how to use it: <a rel="nofollow" href="https://www.philschmid.de/optimizing-transformers-with-optimum">https://www.philschmid.de/optimizing-transformers-with-optimum</a></p>

          '
        raw: 'I wouldn''t recommend parallelizing.


          Hugging Face has another library called [optimum](https://github.com/huggingface/optimum/)
          that can help you speed up your model.


          See here for an article explaining how to use it: https://www.philschmid.de/optimizing-transformers-with-optimum'
        updatedAt: '2022-07-18T21:12:38.871Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BMU
    id: 62d5ccc605461e9459d01944
    type: comment
  author: nbroad
  content: 'I wouldn''t recommend parallelizing.


    Hugging Face has another library called [optimum](https://github.com/huggingface/optimum/)
    that can help you speed up your model.


    See here for an article explaining how to use it: https://www.philschmid.de/optimizing-transformers-with-optimum'
  created_at: 2022-07-18 20:12:38+00:00
  edited: false
  hidden: false
  id: 62d5ccc605461e9459d01944
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
      fullname: Nicholas Broad
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: true
      name: nbroad
      type: user
    createdAt: '2023-04-26T04:52:52.000Z'
    data:
      status: closed
    id: 6448ae243e498d6691a3d535
    type: status-change
  author: nbroad
  created_at: 2023-04-26 03:52:52+00:00
  id: 6448ae243e498d6691a3d535
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: nbroad/ESG-BERT
repo_type: model
status: closed
target_branch: null
title: Best way to parallelize the inference of ESG-BERT?
