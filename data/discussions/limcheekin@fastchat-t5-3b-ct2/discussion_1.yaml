!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vasilee
conflicting_files: null
created_at: 2023-06-08 06:10:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-08T07:10:49.000Z'
    data:
      edited: false
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16160224378108978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>did you test the result?<br>for me for the exact code from the model
          card (including prompt)<br>it answers with :<br>................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................</p>

          <p>and for input "how are you?"<br>it says:<br>oldsoldssabsarmsarmsarmsarmsarmsarmarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsrachrachsrachrachsrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrach
          rachrachrachrachrachrachrachrachrachrachrachrachrachrachrach rachrachrach</p>

          '
        raw: "did you test the result?\r\nfor me for the exact code from the model\
          \ card (including prompt)\r\nit answers with :\r\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\
          \n\r\nand for input \"how are you?\"\r\nit says:\r\noldsoldssabsarmsarmsarmsarmsarmsarmarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsrachrachsrachrachsrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrach\
          \ rachrachrachrachrachrachrachrachrachrachrachrachrachrachrach rachrachrach"
        updatedAt: '2023-06-08T07:10:49.322Z'
      numEdits: 0
      reactions: []
    id: 64817ef950b759c75d5d378d
    type: comment
  author: vasilee
  content: "did you test the result?\r\nfor me for the exact code from the model card\
    \ (including prompt)\r\nit answers with :\r\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\
    \n\r\nand for input \"how are you?\"\r\nit says:\r\noldsoldssabsarmsarmsarmsarmsarmsarmarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsarmsrachrachsrachrachsrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrachrach\
    \ rachrachrachrachrachrachrachrachrachrachrachrachrachrachrach rachrachrach"
  created_at: 2023-06-08 06:10:49+00:00
  edited: false
  hidden: false
  id: 64817ef950b759c75d5d378d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
      fullname: Lim Chee Kin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: limcheekin
      type: user
    createdAt: '2023-06-09T07:53:01.000Z'
    data:
      edited: false
      editors:
      - limcheekin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9435494542121887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
          fullname: Lim Chee Kin
          isHf: false
          isPro: false
          name: limcheekin
          type: user
        html: '<p>Yes, thanks for raising the issue.</p>

          <p>The same problem happened to limcheekin/mpt-7b-storywriter-ct2.<br>I
          not sure whether there''s something missing in my conversion process or
          the models are not supported by CTranslate2.</p>

          <p>You can try to run the conversion yourself and test it out. Let''s me
          know if you managed to get it working in your PC.</p>

          '
        raw: 'Yes, thanks for raising the issue.


          The same problem happened to limcheekin/mpt-7b-storywriter-ct2.

          I not sure whether there''s something missing in my conversion process or
          the models are not supported by CTranslate2.


          You can try to run the conversion yourself and test it out. Let''s me know
          if you managed to get it working in your PC.'
        updatedAt: '2023-06-09T07:53:01.956Z'
      numEdits: 0
      reactions: []
    id: 6482da5d875632d68ee62cdc
    type: comment
  author: limcheekin
  content: 'Yes, thanks for raising the issue.


    The same problem happened to limcheekin/mpt-7b-storywriter-ct2.

    I not sure whether there''s something missing in my conversion process or the
    models are not supported by CTranslate2.


    You can try to run the conversion yourself and test it out. Let''s me know if
    you managed to get it working in your PC.'
  created_at: 2023-06-09 06:53:01+00:00
  edited: false
  hidden: false
  id: 6482da5d875632d68ee62cdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
      fullname: Lim Chee Kin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: limcheekin
      type: user
    createdAt: '2023-06-09T07:57:19.000Z'
    data:
      edited: true
      editors:
      - limcheekin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8541023135185242
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
          fullname: Lim Chee Kin
          isHf: false
          isPro: false
          name: limcheekin
          type: user
        html: '<p>I suggest you try out the following repo (a different but similar
          model) if you don''t want to do the model conversion yourself published
          by another CTranslate2 supporter:<br><a href="https://huggingface.co/michaelfeil/ct2fast-RedPajama-INCITE-7B-Chat">https://huggingface.co/michaelfeil/ct2fast-RedPajama-INCITE-7B-Chat</a></p>

          '
        raw: 'I suggest you try out the following repo (a different but similar model)
          if you don''t want to do the model conversion yourself published by another
          CTranslate2 supporter:

          https://huggingface.co/michaelfeil/ct2fast-RedPajama-INCITE-7B-Chat'
        updatedAt: '2023-06-09T09:11:50.871Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vasilee
    id: 6482db5f57d6e62bf4d7d9b9
    type: comment
  author: limcheekin
  content: 'I suggest you try out the following repo (a different but similar model)
    if you don''t want to do the model conversion yourself published by another CTranslate2
    supporter:

    https://huggingface.co/michaelfeil/ct2fast-RedPajama-INCITE-7B-Chat'
  created_at: 2023-06-09 06:57:19+00:00
  edited: true
  hidden: false
  id: 6482db5f57d6e62bf4d7d9b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-13T06:31:31.000Z'
    data:
      status: closed
    id: 64880d433126307b34fa95f9
    type: status-change
  author: vasilee
  created_at: 2023-06-13 05:31:31+00:00
  id: 64880d433126307b34fa95f9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-14T22:48:16.000Z'
    data:
      status: open
    id: 648a43b0c497a983618ead82
    type: status-change
  author: vasilee
  created_at: 2023-06-14 21:48:16+00:00
  id: 648a43b0c497a983618ead82
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-14T23:03:07.000Z'
    data:
      edited: true
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9524915218353271
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>I tried many small models &lt;=7b, and  WizardLM-7b  and fastchat
          give me the best summaries and answers, but fastchat wins on a couple of
          things</p>

          <ol>

          <li>license<br>flan-t5 models are really open-source (apache2), unlike llama
          models which are not available for commercial usage</li>

          <li>context<br>flan-t5 models (including fastchat) seems not too have a
          problem with longer contexts than specified,<br>I used 3K+ tokens (which
          is more than 2048 specified in the config)</li>

          <li>speed<br>maybe because it is smaller (only 3B), but it is faster than
          quantized 4-bit wizardlm-7b, especially when testing with longer contexts</li>

          </ol>

          <p>so my question :)<br>are you going to upload a new working version? </p>

          <p>PS: in my tests, flan-alpaca-xl is not on par with fastchat, so it is
          not an option even if they were trained from the same base model</p>

          '
        raw: "I tried many small models <=7b, and  WizardLM-7b  and fastchat give\
          \ me the best summaries and answers, but fastchat wins on a couple of things\n\
          1) license\nflan-t5 models are really open-source (apache2), unlike llama\
          \ models which are not available for commercial usage\n2) context\nflan-t5\
          \ models (including fastchat) seems not too have a problem with longer contexts\
          \ than specified, \nI used 3K+ tokens (which is more than 2048 specified\
          \ in the config)\n3) speed\nmaybe because it is smaller (only 3B), but it\
          \ is faster than quantized 4-bit wizardlm-7b, especially when testing with\
          \ longer contexts\n\nso my question :)\nare you going to upload a new working\
          \ version? \n\n\nPS: in my tests, flan-alpaca-xl is not on par with fastchat,\
          \ so it is not an option even if they were trained from the same base model\n"
        updatedAt: '2023-06-14T23:42:49.776Z'
      numEdits: 1
      reactions: []
    id: 648a472b9c7c32ce0bb08b42
    type: comment
  author: vasilee
  content: "I tried many small models <=7b, and  WizardLM-7b  and fastchat give me\
    \ the best summaries and answers, but fastchat wins on a couple of things\n1)\
    \ license\nflan-t5 models are really open-source (apache2), unlike llama models\
    \ which are not available for commercial usage\n2) context\nflan-t5 models (including\
    \ fastchat) seems not too have a problem with longer contexts than specified,\
    \ \nI used 3K+ tokens (which is more than 2048 specified in the config)\n3) speed\n\
    maybe because it is smaller (only 3B), but it is faster than quantized 4-bit wizardlm-7b,\
    \ especially when testing with longer contexts\n\nso my question :)\nare you going\
    \ to upload a new working version? \n\n\nPS: in my tests, flan-alpaca-xl is not\
    \ on par with fastchat, so it is not an option even if they were trained from\
    \ the same base model\n"
  created_at: 2023-06-14 22:03:07+00:00
  edited: true
  hidden: false
  id: 648a472b9c7c32ce0bb08b42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-15T05:45:20.000Z'
    data:
      edited: false
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9558077454566956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>UPDATE:<br>flan-alpaca-gpt4-xl works as good as fastchat,<br>thanks
          for quantizing it!  </p>

          '
        raw: 'UPDATE:

          flan-alpaca-gpt4-xl works as good as fastchat,

          thanks for quantizing it!  '
        updatedAt: '2023-06-15T05:45:20.213Z'
      numEdits: 0
      reactions: []
    id: 648aa570fe69a22cb60e061e
    type: comment
  author: vasilee
  content: 'UPDATE:

    flan-alpaca-gpt4-xl works as good as fastchat,

    thanks for quantizing it!  '
  created_at: 2023-06-15 04:45:20+00:00
  edited: false
  hidden: false
  id: 648aa570fe69a22cb60e061e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
      fullname: Lim Chee Kin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: limcheekin
      type: user
    createdAt: '2023-06-15T07:07:59.000Z'
    data:
      edited: false
      editors:
      - limcheekin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408978819847107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
          fullname: Lim Chee Kin
          isHf: false
          isPro: false
          name: limcheekin
          type: user
        html: '<p>You''re mostly welcome on the quantize version of the flan-alpaca-gpt4-xl.
          I glad that it help.</p>

          <p>Thanks for sharing the testing outcomes of the fastchat-t5 model. It
          seems worth to take another look into the issue.<br>By the way, did you
          tried to run the conversion and quantization on fastchat-t5 yourself and
          test it out?</p>

          <p>Created an issue regarding this matter at <a rel="nofollow" href="https://github.com/OpenNMT/CTranslate2/issues/1295">https://github.com/OpenNMT/CTranslate2/issues/1295</a></p>

          '
        raw: 'You''re mostly welcome on the quantize version of the flan-alpaca-gpt4-xl.
          I glad that it help.


          Thanks for sharing the testing outcomes of the fastchat-t5 model. It seems
          worth to take another look into the issue.

          By the way, did you tried to run the conversion and quantization on fastchat-t5
          yourself and test it out?


          Created an issue regarding this matter at https://github.com/OpenNMT/CTranslate2/issues/1295'
        updatedAt: '2023-06-15T07:07:59.321Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - vasilee
    id: 648ab8cf1680fe31c1f227d6
    type: comment
  author: limcheekin
  content: 'You''re mostly welcome on the quantize version of the flan-alpaca-gpt4-xl.
    I glad that it help.


    Thanks for sharing the testing outcomes of the fastchat-t5 model. It seems worth
    to take another look into the issue.

    By the way, did you tried to run the conversion and quantization on fastchat-t5
    yourself and test it out?


    Created an issue regarding this matter at https://github.com/OpenNMT/CTranslate2/issues/1295'
  created_at: 2023-06-15 06:07:59+00:00
  edited: false
  hidden: false
  id: 648ab8cf1680fe31c1f227d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-06-15T10:59:03.000Z'
    data:
      edited: true
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9451017379760742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>after latest changes it seems to work, but it is appending a quote
          (`) to every word.</p>

          <p>the response for "hi, how are you" is:</p>

          <pre><code>I?` I''m` good!` How` about` you?` How` are` you?

          </code></pre>

          <p>the response for translation to German is:</p>

          <pre><code>Die` Haus` ist` wunderbar.

          </code></pre>

          <p>After following the suggestion from that thread you posted, and switched
          the tokenizer to the one from flan-alpaca-gpt4-xl-ct2 it works fine</p>

          '
        raw: 'after latest changes it seems to work, but it is appending a quote (`)
          to every word.


          the response for "hi, how are you" is:

          ```

          I?` I''m` good!` How` about` you?` How` are` you?

          ```


          the response for translation to German is:

          ```

          Die` Haus` ist` wunderbar.

          ```


          After following the suggestion from that thread you posted, and switched
          the tokenizer to the one from flan-alpaca-gpt4-xl-ct2 it works fine'
        updatedAt: '2023-06-15T11:03:10.356Z'
      numEdits: 1
      reactions: []
    id: 648aeef7836be95a1245c537
    type: comment
  author: vasilee
  content: 'after latest changes it seems to work, but it is appending a quote (`)
    to every word.


    the response for "hi, how are you" is:

    ```

    I?` I''m` good!` How` about` you?` How` are` you?

    ```


    the response for translation to German is:

    ```

    Die` Haus` ist` wunderbar.

    ```


    After following the suggestion from that thread you posted, and switched the tokenizer
    to the one from flan-alpaca-gpt4-xl-ct2 it works fine'
  created_at: 2023-06-15 09:59:03+00:00
  edited: true
  hidden: false
  id: 648aeef7836be95a1245c537
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
      fullname: Lim Chee Kin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: limcheekin
      type: user
    createdAt: '2023-06-16T03:53:18.000Z'
    data:
      edited: true
      editors:
      - limcheekin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7925520539283752
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
          fullname: Lim Chee Kin
          isHf: false
          isPro: false
          name: limcheekin
          type: user
        html: '<p>Thanks for sharing the solution. I updated the repo to use the tokenizer
          of the flan-alpaca-gpt4-xl-ct2, no switching required.</p>

          <p>Please verify and close the issue if there''s no problem.</p>

          <p>Thanks.</p>

          '
        raw: 'Thanks for sharing the solution. I updated the repo to use the tokenizer
          of the flan-alpaca-gpt4-xl-ct2, no switching required.


          Please verify and close the issue if there''s no problem.


          Thanks.'
        updatedAt: '2023-06-16T06:42:30.345Z'
      numEdits: 2
      reactions: []
    id: 648bdcae7becd01f72b435f2
    type: comment
  author: limcheekin
  content: 'Thanks for sharing the solution. I updated the repo to use the tokenizer
    of the flan-alpaca-gpt4-xl-ct2, no switching required.


    Please verify and close the issue if there''s no problem.


    Thanks.'
  created_at: 2023-06-16 02:53:18+00:00
  edited: true
  hidden: false
  id: 648bdcae7becd01f72b435f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52394b21e9e276e1e954cd70f36c099d.svg
      fullname: Filipe Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: filipemesquita
      type: user
    createdAt: '2023-08-15T20:30:06.000Z'
    data:
      edited: false
      editors:
      - filipemesquita
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5887712240219116
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52394b21e9e276e1e954cd70f36c099d.svg
          fullname: Filipe Mesquita
          isHf: false
          isPro: false
          name: filipemesquita
          type: user
        html: "<p>The new tokenizer partially works but it doesn't recognize newlines\
          \ (\\n), which is expected from the original Flan T5 model. But the fastchat-t5\
          \ tokenizer uses a special encoding to represent newlines.</p>\n<p>Example:</p>\n\
          <pre><code>input_text = \"line1\\nline2\"\nFlan T5 tokenizer: ['\u2581line',\
          \ '1', '\u2581line', '2', '&lt;/s&gt;']\nfastchat-t5-3b tokenizer: ['\u2581\
          line', '1', '\\n', '\u2581line', '2', '&lt;/s&gt;']\n</code></pre>\n"
        raw: "The new tokenizer partially works but it doesn't recognize newlines\
          \ (\\n), which is expected from the original Flan T5 model. But the fastchat-t5\
          \ tokenizer uses a special encoding to represent newlines.\n\nExample:\n\
          ```\ninput_text = \"line1\\nline2\"\nFlan T5 tokenizer: ['\u2581line', '1',\
          \ '\u2581line', '2', '</s>']\nfastchat-t5-3b tokenizer: ['\u2581line', '1',\
          \ '\\n', '\u2581line', '2', '</s>']\n```"
        updatedAt: '2023-08-15T20:30:06.379Z'
      numEdits: 0
      reactions: []
    id: 64dbe04ea34448aee684ecf7
    type: comment
  author: filipemesquita
  content: "The new tokenizer partially works but it doesn't recognize newlines (\\\
    n), which is expected from the original Flan T5 model. But the fastchat-t5 tokenizer\
    \ uses a special encoding to represent newlines.\n\nExample:\n```\ninput_text\
    \ = \"line1\\nline2\"\nFlan T5 tokenizer: ['\u2581line', '1', '\u2581line', '2',\
    \ '</s>']\nfastchat-t5-3b tokenizer: ['\u2581line', '1', '\\n', '\u2581line',\
    \ '2', '</s>']\n```"
  created_at: 2023-08-15 19:30:06+00:00
  edited: false
  hidden: false
  id: 64dbe04ea34448aee684ecf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52394b21e9e276e1e954cd70f36c099d.svg
      fullname: Filipe Mesquita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: filipemesquita
      type: user
    createdAt: '2023-08-16T00:39:38.000Z'
    data:
      edited: false
      editors:
      - filipemesquita
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6036280393600464
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52394b21e9e276e1e954cd70f36c099d.svg
          fullname: Filipe Mesquita
          isHf: false
          isPro: false
          name: filipemesquita
          type: user
        html: '<p>More information on how to use the fastchat tokenizer: <a rel="nofollow"
          href="https://github.com/OpenNMT/CTranslate2/issues/1220#issuecomment-1679749680">https://github.com/OpenNMT/CTranslate2/issues/1220#issuecomment-1679749680</a></p>

          '
        raw: 'More information on how to use the fastchat tokenizer: https://github.com/OpenNMT/CTranslate2/issues/1220#issuecomment-1679749680'
        updatedAt: '2023-08-16T00:39:38.704Z'
      numEdits: 0
      reactions: []
    id: 64dc1aca5f144aa29f14db29
    type: comment
  author: filipemesquita
  content: 'More information on how to use the fastchat tokenizer: https://github.com/OpenNMT/CTranslate2/issues/1220#issuecomment-1679749680'
  created_at: 2023-08-15 23:39:38+00:00
  edited: false
  hidden: false
  id: 64dc1aca5f144aa29f14db29
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: limcheekin/fastchat-t5-3b-ct2
repo_type: model
status: open
target_branch: null
title: generates non-sense response
