!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gagan3012
conflicting_files: null
created_at: 2023-11-17 02:34:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614366097007-noauth.jpeg?w=200&h=200&f=face
      fullname: Gagan Bhatia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gagan3012
      type: user
    createdAt: '2023-11-17T02:34:48.000Z'
    data:
      edited: false
      editors:
      - gagan3012
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9793268442153931
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614366097007-noauth.jpeg?w=200&h=200&f=face
          fullname: Gagan Bhatia
          isHf: false
          isPro: false
          name: gagan3012
          type: user
        html: '<p>Hello,<br>I am trying to understand model merging, and I was wondering
          how this model was created and why its not 140B, considering its 2 70b llamas.
          Also can you share the model creation scripts if not can you explain the
          process?</p>

          '
        raw: "Hello, \r\nI am trying to understand model merging, and I was wondering\
          \ how this model was created and why its not 140B, considering its 2 70b\
          \ llamas. Also can you share the model creation scripts if not can you explain\
          \ the process?"
        updatedAt: '2023-11-17T02:34:48.434Z'
      numEdits: 0
      reactions: []
    id: 6556d148bcdc315c0a0300b4
    type: comment
  author: gagan3012
  content: "Hello, \r\nI am trying to understand model merging, and I was wondering\
    \ how this model was created and why its not 140B, considering its 2 70b llamas.\
    \ Also can you share the model creation scripts if not can you explain the process?"
  created_at: 2023-11-17 02:34:48+00:00
  edited: false
  hidden: false
  id: 6556d148bcdc315c0a0300b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-11-23T20:31:27.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.982177734375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<p>Goliath wasn''t created by simply stacking two models <em>on top
          of each other</em>. The merge process was essentially taking slices from
          various layer ranges from each model, then interleaving those slices into
          a final model. It didn''t turn out to be 140B because not all layers were
          included in the slices. For example, the input and output layers were not
          stacked, as they need to be unique. The final size is 118B, but I named
          it 120B because it sounds better.</p>

          '
        raw: Goliath wasn't created by simply stacking two models *on top of each
          other*. The merge process was essentially taking slices from various layer
          ranges from each model, then interleaving those slices into a final model.
          It didn't turn out to be 140B because not all layers were included in the
          slices. For example, the input and output layers were not stacked, as they
          need to be unique. The final size is 118B, but I named it 120B because it
          sounds better.
        updatedAt: '2023-11-23T20:31:27.162Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - homosapienLCY
        - gagan3012
        - MatthewK
        - phi0112358
        - zereraz
        - MaziyarPanahi
    id: 655fb69fd69284e31fd94e86
    type: comment
  author: alpindale
  content: Goliath wasn't created by simply stacking two models *on top of each other*.
    The merge process was essentially taking slices from various layer ranges from
    each model, then interleaving those slices into a final model. It didn't turn
    out to be 140B because not all layers were included in the slices. For example,
    the input and output layers were not stacked, as they need to be unique. The final
    size is 118B, but I named it 120B because it sounds better.
  created_at: 2023-11-23 20:31:27+00:00
  edited: false
  hidden: false
  id: 655fb69fd69284e31fd94e86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c44fbb32cb134259bdadf84c61efa43c.svg
      fullname: LCY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: homosapienLCY
      type: user
    createdAt: '2023-11-24T12:37:42.000Z'
    data:
      edited: false
      editors:
      - homosapienLCY
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497664570808411
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c44fbb32cb134259bdadf84c61efa43c.svg
          fullname: LCY
          isHf: false
          isPro: false
          name: homosapienLCY
          type: user
        html: '<blockquote>

          <p>Goliath wasn''t created by simply stacking two models <em>on top of each
          other</em>. The merge process was essentially taking slices from various
          layer ranges from each model, then interleaving those slices into a final
          model. It didn''t turn out to be 140B because not all layers were included
          in the slices. For example, the input and output layers were not stacked,
          as they need to be unique. The final size is 118B, but I named it 120B because
          it sounds better.</p>

          </blockquote>

          <p>Thanks for the explanation (and the great model, it works so nicely!)!<br>But
          the mergekit provides 3 types of merges: Linear, Slerp and Passthrough.
          Are you using the passthrough merge as shown here: <a rel="nofollow" href="https://github.com/cg123/mergekit/blob/main/examples/orcamini-platy-44layer.yml">https://github.com/cg123/mergekit/blob/main/examples/orcamini-platy-44layer.yml</a><br>Another
          question I have is what''s your strategy of finding the way to stack the
          model? Do you always separate them into 16-layer groups and interleaving
          them?</p>

          <p>Best<br>HomoSapien</p>

          '
        raw: '> Goliath wasn''t created by simply stacking two models *on top of each
          other*. The merge process was essentially taking slices from various layer
          ranges from each model, then interleaving those slices into a final model.
          It didn''t turn out to be 140B because not all layers were included in the
          slices. For example, the input and output layers were not stacked, as they
          need to be unique. The final size is 118B, but I named it 120B because it
          sounds better.


          Thanks for the explanation (and the great model, it works so nicely!)!

          But the mergekit provides 3 types of merges: Linear, Slerp and Passthrough.
          Are you using the passthrough merge as shown here: https://github.com/cg123/mergekit/blob/main/examples/orcamini-platy-44layer.yml

          Another question I have is what''s your strategy of finding the way to stack
          the model? Do you always separate them into 16-layer groups and interleaving
          them?


          Best

          HomoSapien'
        updatedAt: '2023-11-24T12:37:42.806Z'
      numEdits: 0
      reactions: []
    id: 656099165475849b8263772d
    type: comment
  author: homosapienLCY
  content: '> Goliath wasn''t created by simply stacking two models *on top of each
    other*. The merge process was essentially taking slices from various layer ranges
    from each model, then interleaving those slices into a final model. It didn''t
    turn out to be 140B because not all layers were included in the slices. For example,
    the input and output layers were not stacked, as they need to be unique. The final
    size is 118B, but I named it 120B because it sounds better.


    Thanks for the explanation (and the great model, it works so nicely!)!

    But the mergekit provides 3 types of merges: Linear, Slerp and Passthrough. Are
    you using the passthrough merge as shown here: https://github.com/cg123/mergekit/blob/main/examples/orcamini-platy-44layer.yml

    Another question I have is what''s your strategy of finding the way to stack the
    model? Do you always separate them into 16-layer groups and interleaving them?


    Best

    HomoSapien'
  created_at: 2023-11-24 12:37:42+00:00
  edited: false
  hidden: false
  id: 656099165475849b8263772d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: alpindale/goliath-120b
repo_type: model
status: open
target_branch: null
title: How was this created?
