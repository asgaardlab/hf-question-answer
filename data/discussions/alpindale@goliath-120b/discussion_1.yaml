!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aslawliet
conflicting_files: null
created_at: 2023-11-14 05:52:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-14T05:52:31.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9175758361816406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: '<p>What is amount of CPU ram will I require to run this model on ubuntu
          with ram 8gb and swap 150gb?</p>

          '
        raw: What is amount of CPU ram will I require to run this model on ubuntu
          with ram 8gb and swap 150gb?
        updatedAt: '2023-11-14T05:52:31.586Z'
      numEdits: 0
      reactions: []
    id: 65530b1f86270cc7f8d0fd70
    type: comment
  author: aslawliet
  content: What is amount of CPU ram will I require to run this model on ubuntu with
    ram 8gb and swap 150gb?
  created_at: 2023-11-14 05:52:31+00:00
  edited: false
  hidden: false
  id: 65530b1f86270cc7f8d0fd70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-14T23:51:13.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9691548347473145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Depends on the quantization you use. With 16-bit precision, which
          is the model in this repository, you would need about 240-260GB. Some people
          have good results with q2_k quantization that is about 50GB in size, about
          55GB RAM use. Still, if you plan to use swap as your primary place for storing
          the weights during inference, you will have a really bad time running it.
          Basically, all of the weights need to be read once to generate one token.
          If you plan to use a SATA 6Gb/s SSD, which is likely given that you have
          8GB of RAM, you are looking at read speed of about 500MB/s. Meaning that
          it would take you 100 seconds to read the weights once, so generating a
          sentence would take you about 1800 seconds - half an hour. </p>

          '
        raw: 'Depends on the quantization you use. With 16-bit precision, which is
          the model in this repository, you would need about 240-260GB. Some people
          have good results with q2_k quantization that is about 50GB in size, about
          55GB RAM use. Still, if you plan to use swap as your primary place for storing
          the weights during inference, you will have a really bad time running it.
          Basically, all of the weights need to be read once to generate one token.
          If you plan to use a SATA 6Gb/s SSD, which is likely given that you have
          8GB of RAM, you are looking at read speed of about 500MB/s. Meaning that
          it would take you 100 seconds to read the weights once, so generating a
          sentence would take you about 1800 seconds - half an hour. '
        updatedAt: '2023-11-14T23:51:13.393Z'
      numEdits: 0
      reactions: []
    id: 655407f1822892a9251dedc4
    type: comment
  author: adamo1139
  content: 'Depends on the quantization you use. With 16-bit precision, which is the
    model in this repository, you would need about 240-260GB. Some people have good
    results with q2_k quantization that is about 50GB in size, about 55GB RAM use.
    Still, if you plan to use swap as your primary place for storing the weights during
    inference, you will have a really bad time running it. Basically, all of the weights
    need to be read once to generate one token. If you plan to use a SATA 6Gb/s SSD,
    which is likely given that you have 8GB of RAM, you are looking at read speed
    of about 500MB/s. Meaning that it would take you 100 seconds to read the weights
    once, so generating a sentence would take you about 1800 seconds - half an hour. '
  created_at: 2023-11-14 23:51:13+00:00
  edited: false
  hidden: false
  id: 655407f1822892a9251dedc4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: alpindale/goliath-120b
repo_type: model
status: open
target_branch: null
title: How much CPU ram will it take to load and for inference?
