!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JL-er
conflicting_files: null
created_at: 2023-12-07 06:58:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1b5851e6e48787e70c617da49d254fe.svg
      fullname: JL-er
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JL-er
      type: user
    createdAt: '2023-12-07T06:58:37.000Z'
    data:
      edited: false
      editors:
      - JL-er
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8600987792015076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1b5851e6e48787e70c617da49d254fe.svg
          fullname: JL-er
          isHf: false
          isPro: false
          name: JL-er
          type: user
        html: '<p>Is it effective to merge models of different architectures using
          mergekit?</p>

          '
        raw: "\r\nIs it effective to merge models of different architectures using\
          \ mergekit?"
        updatedAt: '2023-12-07T06:58:37.983Z'
      numEdits: 0
      reactions: []
    id: 65716d1dc8018fe64098b5ee
    type: comment
  author: JL-er
  content: "\r\nIs it effective to merge models of different architectures using mergekit?"
  created_at: 2023-12-07 06:58:37+00:00
  edited: false
  hidden: false
  id: 65716d1dc8018fe64098b5ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-12-08T15:51:15.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.702092707157135
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: "<p>AFAIK mergekit only supports transformer models, as it imports from\
          \ HF transformers and manually specifies the different layer names in its\
          \ architecture.py. You can try inspecting all the module names for mamba,\
          \ and map them appropriately in mergekit, and have it load from the mamba_ssm\
          \ package if it encounters a mamba model. I grabbed the module names yesterday:</p>\n\
          <pre><code class=\"language-py\">MAMBA_INFO = StaticTensorNames(\n    name=<span\
          \ class=\"hljs-string\">\"MambaLMHeadModel\"</span>,\n    pre_weight_names=[<span\
          \ class=\"hljs-string\">\"backbone.embedding.weight\"</span>],\n    post_weight_names=[<span\
          \ class=\"hljs-string\">\"backbone.norm_f.weight\"</span>, <span class=\"\
          hljs-string\">\"lm_head.weight\"</span>],\n    embed_weight_names=[<span\
          \ class=\"hljs-string\">\"backbone.embedding.weight\"</span>, <span class=\"\
          hljs-string\">\"lm_head.weight\"</span>],\n    layer_prefix_format=<span\
          \ class=\"hljs-string\">\"backbone.layers.{idx}\"</span>,\n    layer_weight_suffixes=[\n\
          \        <span class=\"hljs-string\">\"mixer.A_log\"</span>,\n        <span\
          \ class=\"hljs-string\">\"mixer.D\"</span>,\n        <span class=\"hljs-string\"\
          >\"mixer.in_proj.weight\"</span>,\n        <span class=\"hljs-string\">\"\
          conv1d.weight\"</span>,\n        <span class=\"hljs-string\">\"conv1d.bias\"\
          </span>,\n        <span class=\"hljs-string\">\"x_proj.weight\"</span>,\n\
          \        <span class=\"hljs-string\">\"dt_proj.weight\"</span>,\n      \
          \  <span class=\"hljs-string\">\"dt_proj.bias\"</span>,\n        <span class=\"\
          hljs-string\">\"out_proj.weight\"</span>,\n        <span class=\"hljs-string\"\
          >\"norm.weight\"</span>,\n    ],\n)\n</code></pre>\n"
        raw: "AFAIK mergekit only supports transformer models, as it imports from\
          \ HF transformers and manually specifies the different layer names in its\
          \ architecture.py. You can try inspecting all the module names for mamba,\
          \ and map them appropriately in mergekit, and have it load from the mamba_ssm\
          \ package if it encounters a mamba model. I grabbed the module names yesterday:\n\
          \n```py\nMAMBA_INFO = StaticTensorNames(\n    name=\"MambaLMHeadModel\"\
          ,\n    pre_weight_names=[\"backbone.embedding.weight\"],\n    post_weight_names=[\"\
          backbone.norm_f.weight\", \"lm_head.weight\"],\n    embed_weight_names=[\"\
          backbone.embedding.weight\", \"lm_head.weight\"],\n    layer_prefix_format=\"\
          backbone.layers.{idx}\",\n    layer_weight_suffixes=[\n        \"mixer.A_log\"\
          ,\n        \"mixer.D\",\n        \"mixer.in_proj.weight\",\n        \"conv1d.weight\"\
          ,\n        \"conv1d.bias\",\n        \"x_proj.weight\",\n        \"dt_proj.weight\"\
          ,\n        \"dt_proj.bias\",\n        \"out_proj.weight\",\n        \"norm.weight\"\
          ,\n    ],\n)\n```\n\n"
        updatedAt: '2023-12-08T15:51:15.991Z'
      numEdits: 0
      reactions: []
    id: 65733b733e0cb21bc7b02065
    type: comment
  author: alpindale
  content: "AFAIK mergekit only supports transformer models, as it imports from HF\
    \ transformers and manually specifies the different layer names in its architecture.py.\
    \ You can try inspecting all the module names for mamba, and map them appropriately\
    \ in mergekit, and have it load from the mamba_ssm package if it encounters a\
    \ mamba model. I grabbed the module names yesterday:\n\n```py\nMAMBA_INFO = StaticTensorNames(\n\
    \    name=\"MambaLMHeadModel\",\n    pre_weight_names=[\"backbone.embedding.weight\"\
    ],\n    post_weight_names=[\"backbone.norm_f.weight\", \"lm_head.weight\"],\n\
    \    embed_weight_names=[\"backbone.embedding.weight\", \"lm_head.weight\"],\n\
    \    layer_prefix_format=\"backbone.layers.{idx}\",\n    layer_weight_suffixes=[\n\
    \        \"mixer.A_log\",\n        \"mixer.D\",\n        \"mixer.in_proj.weight\"\
    ,\n        \"conv1d.weight\",\n        \"conv1d.bias\",\n        \"x_proj.weight\"\
    ,\n        \"dt_proj.weight\",\n        \"dt_proj.bias\",\n        \"out_proj.weight\"\
    ,\n        \"norm.weight\",\n    ],\n)\n```\n\n"
  created_at: 2023-12-08 15:51:15+00:00
  edited: false
  hidden: false
  id: 65733b733e0cb21bc7b02065
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: alpindale/goliath-120b
repo_type: model
status: open
target_branch: null
title: Can I merge models using architectures other than Llama, such as Memba?
