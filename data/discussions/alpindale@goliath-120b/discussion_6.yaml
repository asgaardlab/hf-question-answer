!!python/object:huggingface_hub.community.DiscussionWithDetails
author: edwardDali
conflicting_files: null
created_at: 2023-11-18 18:36:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d1de0691cd0f64062591a5a96d09239.svg
      fullname: eduardT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edwardDali
      type: user
    createdAt: '2023-11-18T18:36:30.000Z'
    data:
      edited: true
      editors:
      - edwardDali
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176260232925415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d1de0691cd0f64062591a5a96d09239.svg
          fullname: eduardT
          isHf: false
          isPro: false
          name: edwardDali
          type: user
        html: '<p>I heard goliath 120b is at GPT4 level for some benchmarks, it it
          possible to use the same merge techniques and generate a merge of 2 mistral
          models? should be interesting if same capabilities are amplified as well.
          Maybe a merge of 3 models would be even stronger :)</p>

          '
        raw: I heard goliath 120b is at GPT4 level for some benchmarks, it it possible
          to use the same merge techniques and generate a merge of 2 mistral models?
          should be interesting if same capabilities are amplified as well. Maybe
          a merge of 3 models would be even stronger :)
        updatedAt: '2023-11-18T18:40:36.323Z'
      numEdits: 3
      reactions: []
    id: 6559042e04a63a0dfbdece39
    type: comment
  author: edwardDali
  content: I heard goliath 120b is at GPT4 level for some benchmarks, it it possible
    to use the same merge techniques and generate a merge of 2 mistral models? should
    be interesting if same capabilities are amplified as well. Maybe a merge of 3
    models would be even stronger :)
  created_at: 2023-11-18 18:36:30+00:00
  edited: true
  hidden: false
  id: 6559042e04a63a0dfbdece39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-11-20T02:24:14.000Z'
    data:
      edited: false
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9261658191680908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: '<blockquote>

          <p>I heard goliath 120b is at GPT4 level for some benchmarks, it it possible
          to use the same merge techniques and generate a merge of 2 mistral models?
          should be interesting if same capabilities are amplified as well. Maybe
          a merge of 3 models would be even stronger :)</p>

          </blockquote>

          <p>There is no publically available 70B Mistral yet though.</p>

          '
        raw: '> I heard goliath 120b is at GPT4 level for some benchmarks, it it possible
          to use the same merge techniques and generate a merge of 2 mistral models?
          should be interesting if same capabilities are amplified as well. Maybe
          a merge of 3 models would be even stronger :)


          There is no publically available 70B Mistral yet though.'
        updatedAt: '2023-11-20T02:24:14.759Z'
      numEdits: 0
      reactions: []
    id: 655ac34e727df37c779e5125
    type: comment
  author: acrastt
  content: '> I heard goliath 120b is at GPT4 level for some benchmarks, it it possible
    to use the same merge techniques and generate a merge of 2 mistral models? should
    be interesting if same capabilities are amplified as well. Maybe a merge of 3
    models would be even stronger :)


    There is no publically available 70B Mistral yet though.'
  created_at: 2023-11-20 02:24:14+00:00
  edited: false
  hidden: false
  id: 655ac34e727df37c779e5125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d1de0691cd0f64062591a5a96d09239.svg
      fullname: eduardT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edwardDali
      type: user
    createdAt: '2023-11-20T08:06:25.000Z'
    data:
      edited: false
      editors:
      - edwardDali
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496525526046753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d1de0691cd0f64062591a5a96d09239.svg
          fullname: eduardT
          isHf: false
          isPro: false
          name: edwardDali
          type: user
        html: '<p>The 7b models are quite strong. Maybe merging multiple small models
          will improve the overall result. I don''t know... newbie here. But your
          Goliath experiment seems to indicate a valid path.</p>

          '
        raw: The 7b models are quite strong. Maybe merging multiple small models will
          improve the overall result. I don't know... newbie here. But your Goliath
          experiment seems to indicate a valid path.
        updatedAt: '2023-11-20T08:06:25.143Z'
      numEdits: 0
      reactions: []
    id: 655b1381b158b94c3aeec3e1
    type: comment
  author: edwardDali
  content: The 7b models are quite strong. Maybe merging multiple small models will
    improve the overall result. I don't know... newbie here. But your Goliath experiment
    seems to indicate a valid path.
  created_at: 2023-11-20 08:06:25+00:00
  edited: false
  hidden: false
  id: 655b1381b158b94c3aeec3e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-29T12:14:46.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9634037613868713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: '<blockquote>

          <p>The 7b models are quite strong. Maybe merging multiple small models will
          improve the overall result. I don''t know... newbie here. But your Goliath
          experiment seems to indicate a valid path.</p>

          </blockquote>

          <p>It''s not the way you think it works, adding like 3 same models, would
          be quite a amount to layer duplicasy which would eventually lead to a garbage
          model if not fine tuned further</p>

          '
        raw: '> The 7b models are quite strong. Maybe merging multiple small models
          will improve the overall result. I don''t know... newbie here. But your
          Goliath experiment seems to indicate a valid path.


          It''s not the way you think it works, adding like 3 same models, would be
          quite a amount to layer duplicasy which would eventually lead to a garbage
          model if not fine tuned further'
        updatedAt: '2023-11-29T12:14:46.024Z'
      numEdits: 0
      reactions: []
    id: 65672b369593e6086a3743a8
    type: comment
  author: aslawliet
  content: '> The 7b models are quite strong. Maybe merging multiple small models
    will improve the overall result. I don''t know... newbie here. But your Goliath
    experiment seems to indicate a valid path.


    It''s not the way you think it works, adding like 3 same models, would be quite
    a amount to layer duplicasy which would eventually lead to a garbage model if
    not fine tuned further'
  created_at: 2023-11-29 12:14:46+00:00
  edited: false
  hidden: false
  id: 65672b369593e6086a3743a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: alpindale/goliath-120b
repo_type: model
status: open
target_branch: null
title: mistral-goliath-12b ?
