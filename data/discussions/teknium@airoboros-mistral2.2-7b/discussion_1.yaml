!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spaceemotion
conflicting_files: null
created_at: 2023-10-03 20:00:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/pD8mJ8gGYuzGafoacCcHO.png?w=200&h=200&f=face
      fullname: Leonie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spaceemotion
      type: user
    createdAt: '2023-10-03T21:00:48.000Z'
    data:
      edited: false
      editors:
      - spaceemotion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6022053956985474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/pD8mJ8gGYuzGafoacCcHO.png?w=200&h=200&f=face
          fullname: Leonie
          isHf: false
          isPro: false
          name: spaceemotion
          type: user
        html: "<p>Hi there! I am quite new to converting models, so please excuse\
          \ me if i'm doing something totally wrong in the first place.<br>I want\
          \ to run this airoboros-trained version using ollama.ai/llama.cpp which\
          \ requires a gguf version as it seems.</p>\n<p>So far, I've downloaded the\
          \ model and tried to start the conversion process like so:</p>\n<pre><code\
          \ class=\"language-bash\">python llama.cpp/convert.py airoboros-mistral2.2-7b\
          \ --outfile airoboros-mistral2.2-7b.gguf\n</code></pre>\n<p>However, I am\
          \ getting the following error back:</p>\n<pre><code>Loading model file airoboros-mistral2.2-7b\\\
          pytorch_model-00001-of-00002.bin\nLoading model file airoboros-mistral2.2-7b\\\
          pytorch_model-00001-of-00002.bin\nLoading model file airoboros-mistral2.2-7b\\\
          pytorch_model-00002-of-00002.bin\nparams = Params(n_vocab=32000, n_embd=4096,\
          \ n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05,\
          \ f_rope_freq_base=10000.0, f_rope_scale=None, ftype=None, path_model=WindowsPath('airoboros-mistral2.2-7b'))\n\
          Loading vocab file 'airoboros-mistral2.2-7b\\tokenizer.model', type 'spm'\n\
          Traceback (most recent call last):\n  File \"C:\\Users\\spaceemotion\\code\\\
          gguf\\llama.cpp\\convert.py\", line 1193, in &lt;module&gt;\n    main()\n\
          \  File \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\",\
          \ line 1175, in main\n    vocab = load_vocab(vocab_dir, args.vocabtype)\n\
          \  File \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\",\
          \ line 1086, in load_vocab\n    return SentencePieceVocab(path, added_tokens_path\
          \ if added_tokens_path.exists() else None)\n  File \"C:\\Users\\spaceemotion\\\
          code\\gguf\\llama.cpp\\convert.py\", line 372, in __init__\n    raise Exception(f\"\
          Expected added token IDs to be sequential and start at {len(added_tokens)};\
          \ got {actual_ids}\")\nException: Expected added token IDs to be sequential\
          \ and start at 3; got [0, 1, 2]\n</code></pre>\n<p>Is this something because\
          \ of my setup or how can I go about fixing this? Thanks in advance!</p>\n"
        raw: "Hi there! I am quite new to converting models, so please excuse me if\
          \ i'm doing something totally wrong in the first place.\r\nI want to run\
          \ this airoboros-trained version using ollama.ai/llama.cpp which requires\
          \ a gguf version as it seems.\r\n\r\nSo far, I've downloaded the model and\
          \ tried to start the conversion process like so:\r\n\r\n```bash\r\npython\
          \ llama.cpp/convert.py airoboros-mistral2.2-7b --outfile airoboros-mistral2.2-7b.gguf\r\
          \n```\r\n\r\nHowever, I am getting the following error back:\r\n```\r\n\
          Loading model file airoboros-mistral2.2-7b\\pytorch_model-00001-of-00002.bin\r\
          \nLoading model file airoboros-mistral2.2-7b\\pytorch_model-00001-of-00002.bin\r\
          \nLoading model file airoboros-mistral2.2-7b\\pytorch_model-00002-of-00002.bin\r\
          \nparams = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336,\
          \ n_head=32, n_head_kv=8, f_norm_eps=1e-05, f_rope_freq_base=10000.0, f_rope_scale=None,\
          \ ftype=None, path_model=WindowsPath('airoboros-mistral2.2-7b'))\r\nLoading\
          \ vocab file 'airoboros-mistral2.2-7b\\tokenizer.model', type 'spm'\r\n\
          Traceback (most recent call last):\r\n  File \"C:\\Users\\spaceemotion\\\
          code\\gguf\\llama.cpp\\convert.py\", line 1193, in <module>\r\n    main()\r\
          \n  File \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\"\
          , line 1175, in main\r\n    vocab = load_vocab(vocab_dir, args.vocabtype)\r\
          \n  File \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\"\
          , line 1086, in load_vocab\r\n    return SentencePieceVocab(path, added_tokens_path\
          \ if added_tokens_path.exists() else None)\r\n  File \"C:\\Users\\spaceemotion\\\
          code\\gguf\\llama.cpp\\convert.py\", line 372, in __init__\r\n    raise\
          \ Exception(f\"Expected added token IDs to be sequential and start at {len(added_tokens)};\
          \ got {actual_ids}\")\r\nException: Expected added token IDs to be sequential\
          \ and start at 3; got [0, 1, 2]\r\n```\r\n\r\nIs this something because\
          \ of my setup or how can I go about fixing this? Thanks in advance!"
        updatedAt: '2023-10-03T21:00:48.593Z'
      numEdits: 0
      reactions: []
    id: 651c8100e648c876f1478afe
    type: comment
  author: spaceemotion
  content: "Hi there! I am quite new to converting models, so please excuse me if\
    \ i'm doing something totally wrong in the first place.\r\nI want to run this\
    \ airoboros-trained version using ollama.ai/llama.cpp which requires a gguf version\
    \ as it seems.\r\n\r\nSo far, I've downloaded the model and tried to start the\
    \ conversion process like so:\r\n\r\n```bash\r\npython llama.cpp/convert.py airoboros-mistral2.2-7b\
    \ --outfile airoboros-mistral2.2-7b.gguf\r\n```\r\n\r\nHowever, I am getting the\
    \ following error back:\r\n```\r\nLoading model file airoboros-mistral2.2-7b\\\
    pytorch_model-00001-of-00002.bin\r\nLoading model file airoboros-mistral2.2-7b\\\
    pytorch_model-00001-of-00002.bin\r\nLoading model file airoboros-mistral2.2-7b\\\
    pytorch_model-00002-of-00002.bin\r\nparams = Params(n_vocab=32000, n_embd=4096,\
    \ n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05,\
    \ f_rope_freq_base=10000.0, f_rope_scale=None, ftype=None, path_model=WindowsPath('airoboros-mistral2.2-7b'))\r\
    \nLoading vocab file 'airoboros-mistral2.2-7b\\tokenizer.model', type 'spm'\r\n\
    Traceback (most recent call last):\r\n  File \"C:\\Users\\spaceemotion\\code\\\
    gguf\\llama.cpp\\convert.py\", line 1193, in <module>\r\n    main()\r\n  File\
    \ \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\", line 1175, in\
    \ main\r\n    vocab = load_vocab(vocab_dir, args.vocabtype)\r\n  File \"C:\\Users\\\
    spaceemotion\\code\\gguf\\llama.cpp\\convert.py\", line 1086, in load_vocab\r\n\
    \    return SentencePieceVocab(path, added_tokens_path if added_tokens_path.exists()\
    \ else None)\r\n  File \"C:\\Users\\spaceemotion\\code\\gguf\\llama.cpp\\convert.py\"\
    , line 372, in __init__\r\n    raise Exception(f\"Expected added token IDs to\
    \ be sequential and start at {len(added_tokens)}; got {actual_ids}\")\r\nException:\
    \ Expected added token IDs to be sequential and start at 3; got [0, 1, 2]\r\n\
    ```\r\n\r\nIs this something because of my setup or how can I go about fixing\
    \ this? Thanks in advance!"
  created_at: 2023-10-03 20:00:48+00:00
  edited: false
  hidden: false
  id: 651c8100e648c876f1478afe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
      fullname: John Steward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HDiffusion
      type: user
    createdAt: '2023-10-04T01:03:28.000Z'
    data:
      edited: false
      editors:
      - HDiffusion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487593173980713
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
          fullname: John Steward
          isHf: false
          isPro: false
          name: HDiffusion
          type: user
        html: '<p>The bloke has already done it and uploaded it here: <a href="https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF">https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF</a>.
          You might have better luck asking there or in their discord server.</p>

          '
        raw: 'The bloke has already done it and uploaded it here: https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF.
          You might have better luck asking there or in their discord server.'
        updatedAt: '2023-10-04T01:03:28.439Z'
      numEdits: 0
      reactions: []
    id: 651cb9e0c0dfd35206d7df5e
    type: comment
  author: HDiffusion
  content: 'The bloke has already done it and uploaded it here: https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF.
    You might have better luck asking there or in their discord server.'
  created_at: 2023-10-04 00:03:28+00:00
  edited: false
  hidden: false
  id: 651cb9e0c0dfd35206d7df5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-10-04T02:20:04.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9309238791465759
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>The bloke has already done it and uploaded it here:\
          \ <a href=\"https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF\"\
          >https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF</a>. You might\
          \ have better luck asking there or in their discord server.</p>\n</blockquote>\n\
          <p>Oh nice, thank you <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ :]</p>\n"
        raw: '> The bloke has already done it and uploaded it here: https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF.
          You might have better luck asking there or in their discord server.


          Oh nice, thank you @TheBloke :]'
        updatedAt: '2023-10-04T02:20:04.782Z'
      numEdits: 0
      reactions: []
    id: 651ccbd409debbe627b893d1
    type: comment
  author: teknium
  content: '> The bloke has already done it and uploaded it here: https://huggingface.co/TheBloke/airoboros-mistral2.2-7B-GGUF.
    You might have better luck asking there or in their discord server.


    Oh nice, thank you @TheBloke :]'
  created_at: 2023-10-04 01:20:04+00:00
  edited: false
  hidden: false
  id: 651ccbd409debbe627b893d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22f7db5753d0ca0f494537e9ce0f2c07.svg
      fullname: d6e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: d6e
      type: user
    createdAt: '2023-12-27T23:55:12.000Z'
    data:
      edited: false
      editors:
      - d6e
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9960688948631287
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22f7db5753d0ca0f494537e9ce0f2c07.svg
          fullname: d6e
          isHf: false
          isPro: false
          name: d6e
          type: user
        html: '<p>Did you end up finding out how they did it?</p>

          '
        raw: Did you end up finding out how they did it?
        updatedAt: '2023-12-27T23:55:12.256Z'
      numEdits: 0
      reactions: []
    id: 658cb960f075fa6df1a3cc72
    type: comment
  author: d6e
  content: Did you end up finding out how they did it?
  created_at: 2023-12-27 23:55:12+00:00
  edited: false
  hidden: false
  id: 658cb960f075fa6df1a3cc72
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: teknium/airoboros-mistral2.2-7b
repo_type: model
status: open
target_branch: null
title: How to convert to GGUF?
