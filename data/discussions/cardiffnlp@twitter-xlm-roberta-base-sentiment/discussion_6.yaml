!!python/object:huggingface_hub.community.DiscussionWithDetails
author: k3ybladewielder
conflicting_files: null
created_at: 2023-02-27 18:55:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/464bdd1958cdd98a3026af5e081d8714.svg
      fullname: "Alysson Guimar\xE3es"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: k3ybladewielder
      type: user
    createdAt: '2023-02-27T18:55:12.000Z'
    data:
      edited: false
      editors:
      - k3ybladewielder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/464bdd1958cdd98a3026af5e081d8714.svg
          fullname: "Alysson Guimar\xE3es"
          isHf: false
          isPro: false
          name: k3ybladewielder
          type: user
        html: "<p>Using the code:</p>\n<pre><code>model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\
          \nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n\
          </code></pre>\n<p>Im facing the issue below:</p>\n<pre><code>AttributeError:\
          \ module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\n\
          ---------------------------------------------------------------------------\n\
          AttributeError                            Traceback (most recent call last)\n\
          &lt;command-798773577171854&gt; in &lt;module&gt;\n     15 \n     16 model_path\
          \ = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n---&gt; 17 sentiment_task\
          \ = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n\
          \     18 \n     19 df_group_notna['sentiment'] = df_group_notna['Feedback'].apply(lambda\
          \ x: sentiment_task(x)[0]['label'])\n\n/databricks/python/lib/python3.7/site-packages/transformers/pipelines/__init__.py\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, framework,\
          \ revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code,\
          \ model_kwargs, pipeline_class, **kwargs)\n    827 \n    828           \
          \  tokenizer = AutoTokenizer.from_pretrained(\n--&gt; 829              \
          \   tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs,\
          \ **tokenizer_kwargs\n    830             )\n    831 \n/databricks/python/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\n\
          \    674             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\n\
          \    675             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
          \ is None):\n--&gt; 676                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    677             else:\n    678               \
          \  if tokenizer_class_py is not None:\n\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\n\
          \   1811             local_files_only=local_files_only,\n   1812       \
          \      _commit_hash=commit_hash,\n-&gt; 1813             **kwargs,\n   1814\
          \         )\n   1815 \n\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ *init_inputs, **kwargs)\n   1957         # Instantiate tokenizer.\n  \
          \ 1958         try:\n-&gt; 1959             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   1960         except OSError:\n   1961             raise\
          \ OSError(\n\n/databricks/python/lib/python3.7/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token,\
          \ cls_token, unk_token, pad_token, mask_token, **kwargs)\n    163      \
          \       pad_token=pad_token,\n    164             mask_token=mask_token,\n\
          --&gt; 165             **kwargs,\n    166         )\n    167 \n\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\
          \ in __init__(self, *args, **kwargs)\n    112         elif slow_tokenizer\
          \ is not None:\n    113             # We need to convert a slow tokenizer\
          \ to build the backend\n--&gt; 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n\
          \    115         elif self.slow_tokenizer_class is not None:\n    116  \
          \           # We need to create and convert a slow tokenizer to build the\
          \ backend\n\n/databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
          \ in convert_slow_tokenizer(transformer_tokenizer)\n   1160     converter_class\
          \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n   1161 \n-&gt; 1162\
          \     return converter_class(transformer_tokenizer).converted()\n\n/databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
          \ in __init__(self, *args)\n    436         super().__init__(*args)\n  \
          \  437 \n--&gt; 438         from .utils import sentencepiece_model_pb2 as\
          \ model_pb2\n    439 \n    440         m = model_pb2.ModelProto()\n\n/databricks/python/lib/python3.7/site-packages/transformers/utils/sentencepiece_model_pb2.py\
          \ in &lt;module&gt;\n     32     syntax=\"proto2\",\n     33     serialized_options=b\"\
          H\\003\",\n---&gt; 34     create_key=_descriptor._internal_create_key,\n\
          \     35     serialized_pb=(\n     36         b'\\n\\x19sentencepiece_model.proto\\\
          x12\\rsentencepiece\"\\xa1\\n\\n\\x0bTrainerSpec\\x12\\r\\n\\x05input\\\
          x18\\x01'\n\nAttributeError: module 'google.protobuf.descriptor' has no\
          \ attribute '_internal_create_key'\n</code></pre>\n"
        raw: "Using the code:\r\n```\r\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\
          \r\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path,\
          \ tokenizer=model_path)\r\n```\r\nIm facing the issue below:\r\n```\r\n\
          AttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\r\
          \n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\n<command-798773577171854> in <module>\r\n     15 \r\n     16\
          \ model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\r\n--->\
          \ 17 sentiment_task = pipeline(\"sentiment-analysis\", model=model_path,\
          \ tokenizer=model_path)\r\n     18 \r\n     19 df_group_notna['sentiment']\
          \ = df_group_notna['Feedback'].apply(lambda x: sentiment_task(x)[0]['label'])\r\
          \n\r\n/databricks/python/lib/python3.7/site-packages/transformers/pipelines/__init__.py\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, framework,\
          \ revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code,\
          \ model_kwargs, pipeline_class, **kwargs)\r\n    827 \r\n    828       \
          \      tokenizer = AutoTokenizer.from_pretrained(\r\n--> 829           \
          \      tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs,\
          \ **tokenizer_kwargs\r\n    830             )\r\n    831 \r\n/databricks/python/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\
          \n    674             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
          \n    675             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
          \ is None):\r\n--> 676                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    677             else:\r\n    678           \
          \      if tokenizer_class_py is not None:\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
          \n   1811             local_files_only=local_files_only,\r\n   1812    \
          \         _commit_hash=commit_hash,\r\n-> 1813             **kwargs,\r\n\
          \   1814         )\r\n   1815 \r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ *init_inputs, **kwargs)\r\n   1957         # Instantiate tokenizer.\r\n\
          \   1958         try:\r\n-> 1959             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   1960         except OSError:\r\n   1961         \
          \    raise OSError(\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token,\
          \ cls_token, unk_token, pad_token, mask_token, **kwargs)\r\n    163    \
          \         pad_token=pad_token,\r\n    164             mask_token=mask_token,\r\
          \n--> 165             **kwargs,\r\n    166         )\r\n    167 \r\n\r\n\
          /databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\
          \ in __init__(self, *args, **kwargs)\r\n    112         elif slow_tokenizer\
          \ is not None:\r\n    113             # We need to convert a slow tokenizer\
          \ to build the backend\r\n--> 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
          \n    115         elif self.slow_tokenizer_class is not None:\r\n    116\
          \             # We need to create and convert a slow tokenizer to build\
          \ the backend\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
          \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1160     converter_class\
          \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\n   1161 \r\n-> 1162\
          \     return converter_class(transformer_tokenizer).converted()\r\n\r\n\
          /databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
          \ in __init__(self, *args)\r\n    436         super().__init__(*args)\r\n\
          \    437 \r\n--> 438         from .utils import sentencepiece_model_pb2\
          \ as model_pb2\r\n    439 \r\n    440         m = model_pb2.ModelProto()\r\
          \n\r\n/databricks/python/lib/python3.7/site-packages/transformers/utils/sentencepiece_model_pb2.py\
          \ in <module>\r\n     32     syntax=\"proto2\",\r\n     33     serialized_options=b\"\
          H\\003\",\r\n---> 34     create_key=_descriptor._internal_create_key,\r\n\
          \     35     serialized_pb=(\r\n     36         b'\\n\\x19sentencepiece_model.proto\\\
          x12\\rsentencepiece\"\\xa1\\n\\n\\x0bTrainerSpec\\x12\\r\\n\\x05input\\\
          x18\\x01'\r\n\r\nAttributeError: module 'google.protobuf.descriptor' has\
          \ no attribute '_internal_create_key'\r\n```"
        updatedAt: '2023-02-27T18:55:12.644Z'
      numEdits: 0
      reactions: []
    id: 63fcfc90d9818304e8521945
    type: comment
  author: k3ybladewielder
  content: "Using the code:\r\n```\r\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\
    \r\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\r\
    \n```\r\nIm facing the issue below:\r\n```\r\nAttributeError: module 'google.protobuf.descriptor'\
    \ has no attribute '_internal_create_key'\r\n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \n<command-798773577171854> in <module>\r\n     15 \r\n     16 model_path = \"\
    cardiffnlp/twitter-xlm-roberta-base-sentiment\"\r\n---> 17 sentiment_task = pipeline(\"\
    sentiment-analysis\", model=model_path, tokenizer=model_path)\r\n     18 \r\n\
    \     19 df_group_notna['sentiment'] = df_group_notna['Feedback'].apply(lambda\
    \ x: sentiment_task(x)[0]['label'])\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/pipelines/__init__.py\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision,\
    \ use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code,\
    \ model_kwargs, pipeline_class, **kwargs)\r\n    827 \r\n    828             tokenizer\
    \ = AutoTokenizer.from_pretrained(\r\n--> 829                 tokenizer_identifier,\
    \ use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs\r\n\
    \    830             )\r\n    831 \r\n/databricks/python/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n\
    \    674             tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
    \n    675             if tokenizer_class_fast and (use_fast or tokenizer_class_py\
    \ is None):\r\n--> 676                 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    677             else:\r\n    678                 if\
    \ tokenizer_class_py is not None:\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
    \n   1811             local_files_only=local_files_only,\r\n   1812          \
    \   _commit_hash=commit_hash,\r\n-> 1813             **kwargs,\r\n   1814    \
    \     )\r\n   1815 \r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\
    \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ *init_inputs, **kwargs)\r\n   1957         # Instantiate tokenizer.\r\n   1958\
    \         try:\r\n-> 1959             tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n   1960         except OSError:\r\n   1961             raise OSError(\r\n\r\n\
    /databricks/python/lib/python3.7/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\
    \ in __init__(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token,\
    \ cls_token, unk_token, pad_token, mask_token, **kwargs)\r\n    163          \
    \   pad_token=pad_token,\r\n    164             mask_token=mask_token,\r\n-->\
    \ 165             **kwargs,\r\n    166         )\r\n    167 \r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\
    \ in __init__(self, *args, **kwargs)\r\n    112         elif slow_tokenizer is\
    \ not None:\r\n    113             # We need to convert a slow tokenizer to build\
    \ the backend\r\n--> 114             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n    115         elif self.slow_tokenizer_class is not None:\r\n    116     \
    \        # We need to create and convert a slow tokenizer to build the backend\r\
    \n\r\n/databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
    \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1160     converter_class\
    \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\n   1161 \r\n-> 1162     return\
    \ converter_class(transformer_tokenizer).converted()\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py\
    \ in __init__(self, *args)\r\n    436         super().__init__(*args)\r\n    437\
    \ \r\n--> 438         from .utils import sentencepiece_model_pb2 as model_pb2\r\
    \n    439 \r\n    440         m = model_pb2.ModelProto()\r\n\r\n/databricks/python/lib/python3.7/site-packages/transformers/utils/sentencepiece_model_pb2.py\
    \ in <module>\r\n     32     syntax=\"proto2\",\r\n     33     serialized_options=b\"\
    H\\003\",\r\n---> 34     create_key=_descriptor._internal_create_key,\r\n    \
    \ 35     serialized_pb=(\r\n     36         b'\\n\\x19sentencepiece_model.proto\\\
    x12\\rsentencepiece\"\\xa1\\n\\n\\x0bTrainerSpec\\x12\\r\\n\\x05input\\x18\\x01'\r\
    \n\r\nAttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\r\
    \n```"
  created_at: 2023-02-27 18:55:12+00:00
  edited: false
  hidden: false
  id: 63fcfc90d9818304e8521945
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: cardiffnlp/twitter-xlm-roberta-base-sentiment
repo_type: model
status: open
target_branch: null
title: ' AttributeError: module ''google.protobuf.descriptor'' has no attribute ''_internal_create_key'''
