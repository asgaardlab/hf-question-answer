!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sophosympatheia
conflicting_files: null
created_at: 2024-01-19 02:17:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-19T02:17:47.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669678211212158
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>Hey, Grimulkan. Thanks again for working on this excellent model!
          I am eager to incorporate it into my merging process, but my first experiment
          with it last night produced a failed mess of a model. I suspect that''s
          because you''ve trained Aurelian on top of a modified base version of Llama2
          which unfortunately doesn''t seem to play nicely with the other models that
          were trained on the stock Llama2 base model, at least when merged together
          naively.<br>Do you have any advice for a mergekit monkey like me who wants
          to merge your model with stock Llama2 models? I appreciate any you can spare.</p>

          '
        raw: "Hey, Grimulkan. Thanks again for working on this excellent model! I\
          \ am eager to incorporate it into my merging process, but my first experiment\
          \ with it last night produced a failed mess of a model. I suspect that's\
          \ because you've trained Aurelian on top of a modified base version of Llama2\
          \ which unfortunately doesn't seem to play nicely with the other models\
          \ that were trained on the stock Llama2 base model, at least when merged\
          \ together naively.\r\nDo you have any advice for a mergekit monkey like\
          \ me who wants to merge your model with stock Llama2 models? I appreciate\
          \ any you can spare."
        updatedAt: '2024-01-19T02:17:47.610Z'
      numEdits: 0
      reactions: []
    id: 65a9dbcb2506426b812f8f3a
    type: comment
  author: sophosympatheia
  content: "Hey, Grimulkan. Thanks again for working on this excellent model! I am\
    \ eager to incorporate it into my merging process, but my first experiment with\
    \ it last night produced a failed mess of a model. I suspect that's because you've\
    \ trained Aurelian on top of a modified base version of Llama2 which unfortunately\
    \ doesn't seem to play nicely with the other models that were trained on the stock\
    \ Llama2 base model, at least when merged together naively.\r\nDo you have any\
    \ advice for a mergekit monkey like me who wants to merge your model with stock\
    \ Llama2 models? I appreciate any you can spare."
  created_at: 2024-01-19 02:17:47+00:00
  edited: false
  hidden: false
  id: 65a9dbcb2506426b812f8f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-19T15:04:26.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8216342329978943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<p>That's a good point. When merging models of longLORA descendancy,\
          \ you probably want to treat the embed and norm layers differently. I am\
          \ not sure if mergekit supports that directly.</p>\n<p>I can think of two\
          \ options:</p>\n<ul>\n<li>Replace the embed/norm layers of all your non-longLORA\
          \ participants with those from <a href=\"https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8\"\
          >base longLORA</a> (Aurelian itself will not need the replacement), then\
          \ merge using mergekit normally; OR</li>\n<li>Merge as normal, but you don't\
          \ want to retain the blended the embed &amp; norm layers. Instead, just\
          \ replace them with Aurelian's layers after merging. This is probably easiest,\
          \ but unsure if it works.</li>\n</ul>\n<p>EDIT: Actually, I'm not sure what\
          \ mergekit does with embed &amp; norm layers: if it blends them or just\
          \ keeps them from the first model. I'm guessing it blends them.</p>\n<p>Some\
          \ useful code snippets (assumes <code>model</code> is already loaded using\
          \ <code>.from_pretrained()</code>:<br>To extract and save the embed &amp;\
          \  norm layers from any model in a file called <code>trainable_params.bin</code>\
          \ in the folder stored in <code>trainable_params_dir</code>:</p>\n<pre><code>#Hack\
          \ to save trainable params\nmodules_to_save = [\"embed\", \"norm\"]\nstate_dict\
          \ = model.state_dict() #use trainer.model if it is a newly trained model\
          \ coming out of the trainer class\nto_save = {}\nfor key, value in state_dict.items():\n\
          \    if any(module_name in key for module_name in modules_to_save):\n  \
          \      to_save[key.replace(\"base_model.model.\", \"\")] = value\ntorch.save(to_save,\
          \ os.path.join(trainable_params_dir, \"trainable_params.bin\"))  \n</code></pre>\n\
          <p>To load (and replace) those layers onto any other model:</p>\n<pre><code>#trainable_params_dir\
          \ is wherever trainable_params.bin was saved\nstate_dict = torch.load(trainable_params_dir,\
          \ map_location=model.device)\nmodel.load_state_dict(state_dict, strict=False)\n\
          </code></pre>\n<p>Note that for the first bullet, if you use the <a href=\"\
          https://huggingface.co/Yukang/Llama-2-70b-longlora-32k\">original LongLORA\
          \ base</a>, it has the pre-extracted <code>trainable_params.bin</code>,\
          \ but the vocab may not match. My <a href=\"https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8\"\
          >modified version of the base</a> will have the same vocab as base Llama\
          \ (and Aurelian is derived from this modified base), but you need to extract\
          \ <code>trainable_params.bin</code> using the above code. But maybe mergekit\
          \ already knows what to do with vocab count mismatch (egs., Nous Hermes\
          \ 70B also has added tokens).</p>\n<p>If you extract <code>trainable_params.bin</code>\
          \ from Aurelian for the method in the first bullet above, it should be identical\
          \ to the approach in the second bullet (only slower). So first bullet makes\
          \ sense if you want to combine the embed &amp; norm from original longLORA\
          \ with those from Aurelian. Second bullet is more if you want to just use\
          \ Aurelian's. I don't know the effect.</p>\n<p>I haven't tested all this,\
          \ just my thoughts. Let me know how it goes! If it is still broken, maybe\
          \ these longLORA models need a small amount of fine-tuning to glue everything,\
          \ which I could do, but it would make it quite annoying to merge and experiment.\
          \ So I hope the above workarounds work.</p>\n<p>If you don't want to write\
          \ some code, let me know and I could give you a python script to do the\
          \ above things (basically same code as above with the proper scaffolding\
          \ and imports).</p>\n"
        raw: "That's a good point. When merging models of longLORA descendancy, you\
          \ probably want to treat the embed and norm layers differently. I am not\
          \ sure if mergekit supports that directly.\n\nI can think of two options:\n\
          * Replace the embed/norm layers of all your non-longLORA participants with\
          \ those from [base longLORA](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
          \ (Aurelian itself will not need the replacement), then merge using mergekit\
          \ normally; OR\n* Merge as normal, but you don't want to retain the blended\
          \ the embed & norm layers. Instead, just replace them with Aurelian's layers\
          \ after merging. This is probably easiest, but unsure if it works.\n\nEDIT:\
          \ Actually, I'm not sure what mergekit does with embed & norm layers: if\
          \ it blends them or just keeps them from the first model. I'm guessing it\
          \ blends them.\n\nSome useful code snippets (assumes `model` is already\
          \ loaded using `.from_pretrained()`:\nTo extract and save the embed &  norm\
          \ layers from any model in a file called `trainable_params.bin` in the folder\
          \ stored in `trainable_params_dir`:\n```\n#Hack to save trainable params\n\
          modules_to_save = [\"embed\", \"norm\"]\nstate_dict = model.state_dict()\
          \ #use trainer.model if it is a newly trained model coming out of the trainer\
          \ class\nto_save = {}\nfor key, value in state_dict.items():\n\tif any(module_name\
          \ in key for module_name in modules_to_save):\n\t\tto_save[key.replace(\"\
          base_model.model.\", \"\")] = value\ntorch.save(to_save, os.path.join(trainable_params_dir,\
          \ \"trainable_params.bin\"))  \n```\nTo load (and replace) those layers\
          \ onto any other model:\n```\n#trainable_params_dir is wherever trainable_params.bin\
          \ was saved\nstate_dict = torch.load(trainable_params_dir, map_location=model.device)\n\
          model.load_state_dict(state_dict, strict=False)\n```\n\nNote that for the\
          \ first bullet, if you use the [original LongLORA base](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k),\
          \ it has the pre-extracted `trainable_params.bin`, but the vocab may not\
          \ match. My [modified version of the base](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
          \ will have the same vocab as base Llama (and Aurelian is derived from this\
          \ modified base), but you need to extract `trainable_params.bin` using the\
          \ above code. But maybe mergekit already knows what to do with vocab count\
          \ mismatch (egs., Nous Hermes 70B also has added tokens).\n\nIf you extract\
          \ `trainable_params.bin` from Aurelian for the method in the first bullet\
          \ above, it should be identical to the approach in the second bullet (only\
          \ slower). So first bullet makes sense if you want to combine the embed\
          \ & norm from original longLORA with those from Aurelian. Second bullet\
          \ is more if you want to just use Aurelian's. I don't know the effect.\n\
          \nI haven't tested all this, just my thoughts. Let me know how it goes!\
          \ If it is still broken, maybe these longLORA models need a small amount\
          \ of fine-tuning to glue everything, which I could do, but it would make\
          \ it quite annoying to merge and experiment. So I hope the above workarounds\
          \ work.\n\nIf you don't want to write some code, let me know and I could\
          \ give you a python script to do the above things (basically same code as\
          \ above with the proper scaffolding and imports)."
        updatedAt: '2024-01-19T21:41:15.294Z'
      numEdits: 11
      reactions: []
    id: 65aa8f7ac8903e28aeac6b80
    type: comment
  author: grimulkan
  content: "That's a good point. When merging models of longLORA descendancy, you\
    \ probably want to treat the embed and norm layers differently. I am not sure\
    \ if mergekit supports that directly.\n\nI can think of two options:\n* Replace\
    \ the embed/norm layers of all your non-longLORA participants with those from\
    \ [base longLORA](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
    \ (Aurelian itself will not need the replacement), then merge using mergekit normally;\
    \ OR\n* Merge as normal, but you don't want to retain the blended the embed &\
    \ norm layers. Instead, just replace them with Aurelian's layers after merging.\
    \ This is probably easiest, but unsure if it works.\n\nEDIT: Actually, I'm not\
    \ sure what mergekit does with embed & norm layers: if it blends them or just\
    \ keeps them from the first model. I'm guessing it blends them.\n\nSome useful\
    \ code snippets (assumes `model` is already loaded using `.from_pretrained()`:\n\
    To extract and save the embed &  norm layers from any model in a file called `trainable_params.bin`\
    \ in the folder stored in `trainable_params_dir`:\n```\n#Hack to save trainable\
    \ params\nmodules_to_save = [\"embed\", \"norm\"]\nstate_dict = model.state_dict()\
    \ #use trainer.model if it is a newly trained model coming out of the trainer\
    \ class\nto_save = {}\nfor key, value in state_dict.items():\n\tif any(module_name\
    \ in key for module_name in modules_to_save):\n\t\tto_save[key.replace(\"base_model.model.\"\
    , \"\")] = value\ntorch.save(to_save, os.path.join(trainable_params_dir, \"trainable_params.bin\"\
    ))  \n```\nTo load (and replace) those layers onto any other model:\n```\n#trainable_params_dir\
    \ is wherever trainable_params.bin was saved\nstate_dict = torch.load(trainable_params_dir,\
    \ map_location=model.device)\nmodel.load_state_dict(state_dict, strict=False)\n\
    ```\n\nNote that for the first bullet, if you use the [original LongLORA base](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k),\
    \ it has the pre-extracted `trainable_params.bin`, but the vocab may not match.\
    \ My [modified version of the base](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
    \ will have the same vocab as base Llama (and Aurelian is derived from this modified\
    \ base), but you need to extract `trainable_params.bin` using the above code.\
    \ But maybe mergekit already knows what to do with vocab count mismatch (egs.,\
    \ Nous Hermes 70B also has added tokens).\n\nIf you extract `trainable_params.bin`\
    \ from Aurelian for the method in the first bullet above, it should be identical\
    \ to the approach in the second bullet (only slower). So first bullet makes sense\
    \ if you want to combine the embed & norm from original longLORA with those from\
    \ Aurelian. Second bullet is more if you want to just use Aurelian's. I don't\
    \ know the effect.\n\nI haven't tested all this, just my thoughts. Let me know\
    \ how it goes! If it is still broken, maybe these longLORA models need a small\
    \ amount of fine-tuning to glue everything, which I could do, but it would make\
    \ it quite annoying to merge and experiment. So I hope the above workarounds work.\n\
    \nIf you don't want to write some code, let me know and I could give you a python\
    \ script to do the above things (basically same code as above with the proper\
    \ scaffolding and imports)."
  created_at: 2024-01-19 15:04:26+00:00
  edited: true
  hidden: false
  id: 65aa8f7ac8903e28aeac6b80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-20T00:51:01.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9392915368080139
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>My friend, you have provided so much more than I expected in your
          response. Thank you for taking the time to explain everything in such depth.</p>

          <p>Do you think the easy way out here might be to use <a rel="nofollow"
          href="https://github.com/cg123/mergekit#tokenizer-source">mergekit''s tokenizer
          source settings</a>? When I did my merge, I didn''t specify a strategy so
          I think it just copied everything over from Llama2. I can try the other
          two approaches, either pulling from Aurelian or having mergekit try to make
          a union, and report back what happens.</p>

          <p>If that doesn''t work, I think you gave me enough information (and example
          code!) to try the method you described. Thank you so much!</p>

          '
        raw: 'My friend, you have provided so much more than I expected in your response.
          Thank you for taking the time to explain everything in such depth.


          Do you think the easy way out here might be to use [mergekit''s tokenizer
          source settings](https://github.com/cg123/mergekit#tokenizer-source)? When
          I did my merge, I didn''t specify a strategy so I think it just copied everything
          over from Llama2. I can try the other two approaches, either pulling from
          Aurelian or having mergekit try to make a union, and report back what happens.


          If that doesn''t work, I think you gave me enough information (and example
          code!) to try the method you described. Thank you so much!'
        updatedAt: '2024-01-20T00:51:01.050Z'
      numEdits: 0
      reactions: []
    id: 65ab18f5a8f716b32e0d3dcc
    type: comment
  author: sophosympatheia
  content: 'My friend, you have provided so much more than I expected in your response.
    Thank you for taking the time to explain everything in such depth.


    Do you think the easy way out here might be to use [mergekit''s tokenizer source
    settings](https://github.com/cg123/mergekit#tokenizer-source)? When I did my merge,
    I didn''t specify a strategy so I think it just copied everything over from Llama2.
    I can try the other two approaches, either pulling from Aurelian or having mergekit
    try to make a union, and report back what happens.


    If that doesn''t work, I think you gave me enough information (and example code!)
    to try the method you described. Thank you so much!'
  created_at: 2024-01-20 00:51:01+00:00
  edited: false
  hidden: false
  id: 65ab18f5a8f716b32e0d3dcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-20T01:17:09.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9749301075935364
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>The issue of tokenizer vocab mismatch is separate from the issue
          of merging embed/norm layers. Yes, you could try union and see if that helps.</p>

          <p>I only mentioned tokenizer vocab size because original longLORA has an
          extra added pad token (Aurelian does not). But other non-longLORA models
          also have this, so yeah, union seems like it was designed for that case.</p>

          <p>But if you were trying to already merge with same vocab, like base Llama
          and Aurelian, or Aurelian and lzlv, then the issue is something else (could
          be embed/norm like I mentioned).</p>

          '
        raw: 'The issue of tokenizer vocab mismatch is separate from the issue of
          merging embed/norm layers. Yes, you could try union and see if that helps.


          I only mentioned tokenizer vocab size because original longLORA has an extra
          added pad token (Aurelian does not). But other non-longLORA models also
          have this, so yeah, union seems like it was designed for that case.


          But if you were trying to already merge with same vocab, like base Llama
          and Aurelian, or Aurelian and lzlv, then the issue is something else (could
          be embed/norm like I mentioned).'
        updatedAt: '2024-01-20T01:17:09.669Z'
      numEdits: 0
      reactions: []
    id: 65ab1f15c3fa44c710b05578
    type: comment
  author: grimulkan
  content: 'The issue of tokenizer vocab mismatch is separate from the issue of merging
    embed/norm layers. Yes, you could try union and see if that helps.


    I only mentioned tokenizer vocab size because original longLORA has an extra added
    pad token (Aurelian does not). But other non-longLORA models also have this, so
    yeah, union seems like it was designed for that case.


    But if you were trying to already merge with same vocab, like base Llama and Aurelian,
    or Aurelian and lzlv, then the issue is something else (could be embed/norm like
    I mentioned).'
  created_at: 2024-01-20 01:17:09+00:00
  edited: false
  hidden: false
  id: 65ab1f15c3fa44c710b05578
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-21T19:10:31.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.931609034538269
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I was testing lzlv and found a third option to prepare any model
          for merging with Aurelian (or any other future longLORA models):</p>

          <ul>

          <li>Extract the embed/norm weights from the <a href="https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8">modified
          longLORA base</a> and replace the corresponding ones in lzlv (or whichever
          model you want to merge with Aurelian).</li>

          <li>Merge the <a href="https://huggingface.co/Yukang/Llama-2-70b-longlora-32k">original
          LongLORA</a> LORA onto lzlv (<code>adapter_model.bin</code>).<ul>

          <li>You can also directly use the <code>trainable_params.bin</code> from
          that repo to merge along with the LORA, but the tokenizer vocab size may
          not match (they have an extra row).</li>

          </ul>

          </li>

          </ul>

          <p>With this method, I was able to get 32K versions of lzlv and Euryale
          to work, with no fine-tuning. They worked ''fine'' with rope scaling 8 up
          to 32K (well, at least they weren''t garbage). That tells me it is totally
          possible to merge longLORA into an existing model to give it 32K capabilities
          to an extent, and it will probably make it better for merging with Aurelian
          as well.</p>

          '
        raw: "I was testing lzlv and found a third option to prepare any model for\
          \ merging with Aurelian (or any other future longLORA models):\n* Extract\
          \ the embed/norm weights from the [modified longLORA base](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
          \ and replace the corresponding ones in lzlv (or whichever model you want\
          \ to merge with Aurelian).\n* Merge the [original LongLORA](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k)\
          \ LORA onto lzlv (`adapter_model.bin`).\n     * You can also directly use\
          \ the `trainable_params.bin` from that repo to merge along with the LORA,\
          \ but the tokenizer vocab size may not match (they have an extra row).\n\
          \nWith this method, I was able to get 32K versions of lzlv and Euryale to\
          \ work, with no fine-tuning. They worked 'fine' with rope scaling 8 up to\
          \ 32K (well, at least they weren't garbage). That tells me it is totally\
          \ possible to merge longLORA into an existing model to give it 32K capabilities\
          \ to an extent, and it will probably make it better for merging with Aurelian\
          \ as well."
        updatedAt: '2024-01-21T19:10:31.479Z'
      numEdits: 0
      reactions: []
    id: 65ad6c27819fbfaf49505aba
    type: comment
  author: grimulkan
  content: "I was testing lzlv and found a third option to prepare any model for merging\
    \ with Aurelian (or any other future longLORA models):\n* Extract the embed/norm\
    \ weights from the [modified longLORA base](https://huggingface.co/grimulkan/llama2_70b_longlora_fp16_32k_ROPE8)\
    \ and replace the corresponding ones in lzlv (or whichever model you want to merge\
    \ with Aurelian).\n* Merge the [original LongLORA](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k)\
    \ LORA onto lzlv (`adapter_model.bin`).\n     * You can also directly use the\
    \ `trainable_params.bin` from that repo to merge along with the LORA, but the\
    \ tokenizer vocab size may not match (they have an extra row).\n\nWith this method,\
    \ I was able to get 32K versions of lzlv and Euryale to work, with no fine-tuning.\
    \ They worked 'fine' with rope scaling 8 up to 32K (well, at least they weren't\
    \ garbage). That tells me it is totally possible to merge longLORA into an existing\
    \ model to give it 32K capabilities to an extent, and it will probably make it\
    \ better for merging with Aurelian as well."
  created_at: 2024-01-21 19:10:31+00:00
  edited: false
  hidden: false
  id: 65ad6c27819fbfaf49505aba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-21T19:15:07.000Z'
    data:
      edited: true
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48543089628219604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: "<p><strong>EDIT</strong>: I missed your previous message where you\
          \ mentioned merging the original LongLORA adapter_model.bin onto the model\
          \ after replacing the embed/norm weights. If the script below looks like\
          \ it's doing what it should for those weights, I can add the remaining code\
          \ to load the LongLORA PEFT adapter and merge that in before saving the\
          \ resultant model. (<strong>EDIT3</strong>: I updated the script with the\
          \ PEFT code.) What's your take on applying this approach <em>after</em>\
          \ merging Aurelian with some other models? Do you think it would be better\
          \ to prep all the Llama2 models <em>before</em> the merge rather than applying\
          \ it once to the resultant blend after the merge with Aurelian? (<strong>EDIT4</strong>:\
          \ I intend to test it both ways, but I'm curious what you think about it.)</p>\n\
          <p><strong>EDIT2</strong>: A thought just occurred to me. Do you want to\
          \ make the trainable_params.bin file containing the embed/norm weights extracted\
          \ from your modified longLORA base available in your HF repo for it? It\
          \ would save other people having to download the full weights to extract\
          \ it themselves, and everyone could rest assured that they're working with\
          \ a version that was extracted properly.</p>\n<p>I wrote up a script that\
          \ implements the code you suggested. I'm including a sample of its output\
          \ to show you what it is saving to the trainable_params.bin file.</p>\n\
          <pre><code>2024-01-21 11:00:08,421 - __main__ - DEBUG - Saving key  model.embed_tokens.weight\
          \  with value  tensor([[-0.0004, -0.0012, -0.0011,  ...,  0.0002, -0.0013,\
          \  0.0008],\n        [-0.0003, -0.0013,  0.0017,  ..., -0.0019,  0.0032,\
          \  0.0033],\n        [ 0.0050,  0.0029, -0.0038,  ...,  0.0005, -0.0082,\
          \  0.0110],\n        ...,\n        [ 0.0057,  0.0189,  0.0099,  ..., -0.0154,\
          \  0.0043, -0.0136],\n        [-0.0028, -0.0086,  0.0019,  ...,  0.0176,\
          \ -0.0158,  0.0172],\n        [ 0.0102, -0.0007,  0.0031,  ...,  0.0094,\
          \ -0.0045,  0.0045]],\n       dtype=torch.float16). Saved key name is: model.embed_tokens.weight\n\
          </code></pre>\n<p>Then for each layer it's saving input_layernorm.weight\
          \ and post_attention_layernorm.weight.</p>\n<pre><code>2024-01-21 11:00:08,596\
          \ - __main__ - DEBUG - Saving key  model.layers.79.input_layernorm.weight\
          \  with value  tensor([0.2903, 0.2993, 0.2812,  ..., 0.2815, 0.1703, 0.1708],\n\
          \       dtype=torch.float16). Saved key name is: model.layers.79.input_layernorm.weight\n\
          2024-01-21 11:00:08,596 - __main__ - DEBUG - Saving key  model.layers.79.post_attention_layernorm.weight\
          \  with value  tensor([0.3440, 0.3298, 0.3386,  ..., 0.3645, 0.1852, 0.2449],\n\
          \       dtype=torch.float16). Saved key name is: model.layers.79.post_attention_layernorm.weight\n\
          </code></pre>\n<p>Finally, it saves the norm.weight.</p>\n<pre><code>2024-01-21\
          \ 11:00:08,596 - __main__ - DEBUG - Saving key  model.norm.weight  with\
          \ value  tensor([1.1504, 1.0186, 1.1230,  ..., 1.1084, 1.4932, 1.3701],\n\
          \       dtype=torch.float16). Saved key name is: model.norm.weight\n</code></pre>\n\
          <p>It doesn't appear that the key.replace(\"base_model.model.\", \"\") code\
          \ is doing anything. Is that a problem?</p>\n<p>Here is the full script.\
          \ <strong>EDIT5</strong>: Sorry for so many edits haha. I had to make some\
          \ fixes.</p>\n<pre><code>import os\nimport argparse\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModelForCausalLM\n\
          import torch\nimport logging\nfrom colorama import Fore\n\nlogging.basicConfig(level=logging.DEBUG,\n\
          \                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\
          logger = logging.getLogger(__name__)\n\ndef get_layers_to_save(model):\n\
          \    modules_to_save = [\"embed\", \"norm\"]\n    state_dict = model.state_dict()\n\
          \    to_save = {}\n    for key, value in state_dict.items():\n        if\
          \ any(module_name in key for module_name in modules_to_save):\n        \
          \    key_name = key.replace(\"base_model.model.\", \"\")\n            logger.debug(f\"\
          {Fore.GREEN}Saving key  {key}  with value  {value}. Saved key name is: {key_name}{Fore.RESET}\"\
          )\n            to_save[key_name] = value\n        else:\n            logger.debug(f\"\
          Skipping key  {key}  because it is not in the list of modules to save\"\
          )\n    return to_save\n\ndef main(args):\n   \n    if args.dtype == \"float16\"\
          :\n        dtype = torch.float16\n    elif args.dtype == \"float32\":\n\
          \        dtype = torch.float32\n    else:\n        raise ValueError(f\"\
          Please provide an appropriate value for dtype. Value: {args.dtype}\")\n\
          \    \n    source_model_dir = os.path.abspath(args.source)\n    destination_dir\
          \ = os.path.abspath(args.destination)\n    trainable_params_path = os.path.join(source_model_dir,\
          \ \"trainable_params/trainable_params.bin\")\n\n    logger.debug(f\"Source\
          \ dir: {source_model_dir}\\nDestination dir: {destination_dir}\\nTrainable\
          \ Params dir: {trainable_params_path}\")\n\n    logger.info(f\"Trainable\
          \ param path: {trainable_params_path}\")\n\n    # Check if trainable_params.bin\
          \ exists in the source model directory\n    if os.path.exists(trainable_params_path):\n\
          \        logger.info(\"Found existing trainable_params.bin. Using this file.\"\
          )\n    else:\n        if not os.path.exists(args.source):\n            raise\
          \ FileNotFoundError(f\"Source model not found at {args.source}\")\n\n  \
          \      # Load source model\n        logger.info(f\"Loading source model\
          \ from {args.source}\")\n        source_model = AutoModelForCausalLM.from_pretrained(args.source,\
          \ torch_dtype=dtype)\n\n        # Save the embed and norm layers from the\
          \ source model\n        to_save = get_layers_to_save(source_model)\n   \
          \     \n        torch.save(to_save, trainable_params_path)\n        logger.info(f\"\
          trainable_params.bin saved to {trainable_params_path}\")\n        del source_model\
          \ # free up resources\n\n    # Load target model and tokenizer\n    if not\
          \ os.path.exists(args.target):\n        raise FileNotFoundError(f\"Target\
          \ model not found at {args.target}\")\n    if not os.path.exists(args.lora):\n\
          \        raise FileNotFoundError(f\"LoRA not found at {args.lora}\")\n \
          \   logger.info(f\"Loading target model from {args.target}\")\n    target_model\
          \ = AutoModelForCausalLM.from_pretrained(args.target, torch_dtype=dtype)\n\
          \    tokenizer = AutoTokenizer.from_pretrained(args.target)\n\n    # Load\
          \ the saved layers onto the target model\n    logger.info(\"Loading state\
          \ dict...\")\n    state_dict = torch.load(trainable_params_path, map_location=target_model.device)\n\
          \    target_model.load_state_dict(state_dict, strict=False)\n\n    # Merge\
          \ in the LoRA\n    logger.info(f\"Loading LoRA adapter from {args.lora}\
          \ and merging with the target model before saving\")\n    target_model =\
          \ PeftModelForCausalLM.from_pretrained(target_model, args.lora, dtype=dtype)\n\
          \    target_model = target_model.merge_and_unload(progressbar=True) \n\n\
          \    # Save the updated model to the destination path, including its tokenizer\
          \ settings\n    logger.info(f\"Saving resultant blend to {destination_dir}.\
          \ This could take a while...\")\n    target_model.save_pretrained(save_directory=destination_dir,\
          \ safe_serialization=True, max_shard_size=f\"{args.shard_size}MiB\")\n \
          \   tokenizer.save_pretrained(destination_dir)\n    logger.info(f\"Updated\
          \ model saved successfully at {destination_dir}\")\n\nif __name__ == \"\
          __main__\":\n    parser = argparse.ArgumentParser(description=\"Transfer\
          \ trainable parameters (embed and norm layers) from a source model to a\
          \ target model using the Transformers library.\")\n    parser.add_argument(\"\
          -s\", \"--source\", required=True, help=\"Path to or identifier of the source\
          \ model.\")\n    parser.add_argument(\"-t\", \"--target\", required=True,\
          \ help=\"Path to or identifier of the target model.\")\n    parser.add_argument(\"\
          -d\", \"--destination\", required=True, help=\"Directory to save the updated\
          \ model.\")\n    parser.add_argument('-l', '--lora', required=True, type=str,\
          \ help=\"A path to the adapter_model.bin file corresponding to original\
          \ LongLORA base (e.g. Yukang/Llama-2-70b-longlora-32k)\")\n    parser.add_argument(\"\
          --dtype\", type=str, default=\"float16\", choices=['float16', 'float32'],\
          \ help=\"The torch data type to use for loading the models. Defaults to\
          \ float16.\")\n    parser.add_argument(\"--shard_size\", type=int, default=8000,\
          \ help=\"Size of shards for saving the model tensors in MiB. Defaults to\
          \ 8000.\")\n\n    args = parser.parse_args()\n    main(args)\n</code></pre>\n"
        raw: "**EDIT**: I missed your previous message where you mentioned merging\
          \ the original LongLORA adapter_model.bin onto the model after replacing\
          \ the embed/norm weights. If the script below looks like it's doing what\
          \ it should for those weights, I can add the remaining code to load the\
          \ LongLORA PEFT adapter and merge that in before saving the resultant model.\
          \ (**EDIT3**: I updated the script with the PEFT code.) What's your take\
          \ on applying this approach *after* merging Aurelian with some other models?\
          \ Do you think it would be better to prep all the Llama2 models *before*\
          \ the merge rather than applying it once to the resultant blend after the\
          \ merge with Aurelian? (**EDIT4**: I intend to test it both ways, but I'm\
          \ curious what you think about it.)\n\n**EDIT2**: A thought just occurred\
          \ to me. Do you want to make the trainable_params.bin file containing the\
          \ embed/norm weights extracted from your modified longLORA base available\
          \ in your HF repo for it? It would save other people having to download\
          \ the full weights to extract it themselves, and everyone could rest assured\
          \ that they're working with a version that was extracted properly.\n\nI\
          \ wrote up a script that implements the code you suggested. I'm including\
          \ a sample of its output to show you what it is saving to the trainable_params.bin\
          \ file.\n\n```\n2024-01-21 11:00:08,421 - __main__ - DEBUG - Saving key\
          \  model.embed_tokens.weight  with value  tensor([[-0.0004, -0.0012, -0.0011,\
          \  ...,  0.0002, -0.0013,  0.0008],\n        [-0.0003, -0.0013,  0.0017,\
          \  ..., -0.0019,  0.0032,  0.0033],\n        [ 0.0050,  0.0029, -0.0038,\
          \  ...,  0.0005, -0.0082,  0.0110],\n        ...,\n        [ 0.0057,  0.0189,\
          \  0.0099,  ..., -0.0154,  0.0043, -0.0136],\n        [-0.0028, -0.0086,\
          \  0.0019,  ...,  0.0176, -0.0158,  0.0172],\n        [ 0.0102, -0.0007,\
          \  0.0031,  ...,  0.0094, -0.0045,  0.0045]],\n       dtype=torch.float16).\
          \ Saved key name is: model.embed_tokens.weight\n```\n\nThen for each layer\
          \ it's saving input_layernorm.weight and post_attention_layernorm.weight.\n\
          \n```\n2024-01-21 11:00:08,596 - __main__ - DEBUG - Saving key  model.layers.79.input_layernorm.weight\
          \  with value  tensor([0.2903, 0.2993, 0.2812,  ..., 0.2815, 0.1703, 0.1708],\n\
          \       dtype=torch.float16). Saved key name is: model.layers.79.input_layernorm.weight\n\
          2024-01-21 11:00:08,596 - __main__ - DEBUG - Saving key  model.layers.79.post_attention_layernorm.weight\
          \  with value  tensor([0.3440, 0.3298, 0.3386,  ..., 0.3645, 0.1852, 0.2449],\n\
          \       dtype=torch.float16). Saved key name is: model.layers.79.post_attention_layernorm.weight\n\
          ```\n\nFinally, it saves the norm.weight.\n\n```\n2024-01-21 11:00:08,596\
          \ - __main__ - DEBUG - Saving key  model.norm.weight  with value  tensor([1.1504,\
          \ 1.0186, 1.1230,  ..., 1.1084, 1.4932, 1.3701],\n       dtype=torch.float16).\
          \ Saved key name is: model.norm.weight\n```\n\nIt doesn't appear that the\
          \ key.replace(\"base_model.model.\", \"\") code is doing anything. Is that\
          \ a problem?\n\nHere is the full script. **EDIT5**: Sorry for so many edits\
          \ haha. I had to make some fixes.\n\n```\nimport os\nimport argparse\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import\
          \ PeftModelForCausalLM\nimport torch\nimport logging\nfrom colorama import\
          \ Fore\n\nlogging.basicConfig(level=logging.DEBUG,\n                   \
          \ format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger\
          \ = logging.getLogger(__name__)\n\ndef get_layers_to_save(model):\n    modules_to_save\
          \ = [\"embed\", \"norm\"]\n    state_dict = model.state_dict()\n    to_save\
          \ = {}\n    for key, value in state_dict.items():\n        if any(module_name\
          \ in key for module_name in modules_to_save):\n            key_name = key.replace(\"\
          base_model.model.\", \"\")\n            logger.debug(f\"{Fore.GREEN}Saving\
          \ key  {key}  with value  {value}. Saved key name is: {key_name}{Fore.RESET}\"\
          )\n            to_save[key_name] = value\n        else:\n            logger.debug(f\"\
          Skipping key  {key}  because it is not in the list of modules to save\"\
          )\n    return to_save\n\ndef main(args):\n   \n    if args.dtype == \"float16\"\
          :\n        dtype = torch.float16\n    elif args.dtype == \"float32\":\n\
          \        dtype = torch.float32\n    else:\n        raise ValueError(f\"\
          Please provide an appropriate value for dtype. Value: {args.dtype}\")\n\
          \    \n    source_model_dir = os.path.abspath(args.source)\n    destination_dir\
          \ = os.path.abspath(args.destination)\n    trainable_params_path = os.path.join(source_model_dir,\
          \ \"trainable_params/trainable_params.bin\")\n\n    logger.debug(f\"Source\
          \ dir: {source_model_dir}\\nDestination dir: {destination_dir}\\nTrainable\
          \ Params dir: {trainable_params_path}\")\n\n    logger.info(f\"Trainable\
          \ param path: {trainable_params_path}\")\n\n    # Check if trainable_params.bin\
          \ exists in the source model directory\n    if os.path.exists(trainable_params_path):\n\
          \        logger.info(\"Found existing trainable_params.bin. Using this file.\"\
          )\n    else:\n        if not os.path.exists(args.source):\n            raise\
          \ FileNotFoundError(f\"Source model not found at {args.source}\")\n\n  \
          \      # Load source model\n        logger.info(f\"Loading source model\
          \ from {args.source}\")\n        source_model = AutoModelForCausalLM.from_pretrained(args.source,\
          \ torch_dtype=dtype)\n\n        # Save the embed and norm layers from the\
          \ source model\n        to_save = get_layers_to_save(source_model)\n   \
          \     \n        torch.save(to_save, trainable_params_path)\n        logger.info(f\"\
          trainable_params.bin saved to {trainable_params_path}\")\n        del source_model\
          \ # free up resources\n\n    # Load target model and tokenizer\n    if not\
          \ os.path.exists(args.target):\n        raise FileNotFoundError(f\"Target\
          \ model not found at {args.target}\")\n    if not os.path.exists(args.lora):\n\
          \        raise FileNotFoundError(f\"LoRA not found at {args.lora}\")\n \
          \   logger.info(f\"Loading target model from {args.target}\")\n    target_model\
          \ = AutoModelForCausalLM.from_pretrained(args.target, torch_dtype=dtype)\n\
          \    tokenizer = AutoTokenizer.from_pretrained(args.target)\n\n    # Load\
          \ the saved layers onto the target model\n    logger.info(\"Loading state\
          \ dict...\")\n    state_dict = torch.load(trainable_params_path, map_location=target_model.device)\n\
          \    target_model.load_state_dict(state_dict, strict=False)\n\n    # Merge\
          \ in the LoRA\n    logger.info(f\"Loading LoRA adapter from {args.lora}\
          \ and merging with the target model before saving\")\n    target_model =\
          \ PeftModelForCausalLM.from_pretrained(target_model, args.lora, dtype=dtype)\n\
          \    target_model = target_model.merge_and_unload(progressbar=True) \n\n\
          \    # Save the updated model to the destination path, including its tokenizer\
          \ settings\n    logger.info(f\"Saving resultant blend to {destination_dir}.\
          \ This could take a while...\")\n    target_model.save_pretrained(save_directory=destination_dir,\
          \ safe_serialization=True, max_shard_size=f\"{args.shard_size}MiB\")\n \
          \   tokenizer.save_pretrained(destination_dir)\n    logger.info(f\"Updated\
          \ model saved successfully at {destination_dir}\")\n\nif __name__ == \"\
          __main__\":\n    parser = argparse.ArgumentParser(description=\"Transfer\
          \ trainable parameters (embed and norm layers) from a source model to a\
          \ target model using the Transformers library.\")\n    parser.add_argument(\"\
          -s\", \"--source\", required=True, help=\"Path to or identifier of the source\
          \ model.\")\n    parser.add_argument(\"-t\", \"--target\", required=True,\
          \ help=\"Path to or identifier of the target model.\")\n    parser.add_argument(\"\
          -d\", \"--destination\", required=True, help=\"Directory to save the updated\
          \ model.\")\n    parser.add_argument('-l', '--lora', required=True, type=str,\
          \ help=\"A path to the adapter_model.bin file corresponding to original\
          \ LongLORA base (e.g. Yukang/Llama-2-70b-longlora-32k)\")\n    parser.add_argument(\"\
          --dtype\", type=str, default=\"float16\", choices=['float16', 'float32'],\
          \ help=\"The torch data type to use for loading the models. Defaults to\
          \ float16.\")\n    parser.add_argument(\"--shard_size\", type=int, default=8000,\
          \ help=\"Size of shards for saving the model tensors in MiB. Defaults to\
          \ 8000.\")\n\n    args = parser.parse_args()\n    main(args)\n\n```\n"
        updatedAt: '2024-01-21T21:40:24.347Z'
      numEdits: 4
      reactions: []
    id: 65ad6d3bdf46651406ff250f
    type: comment
  author: sophosympatheia
  content: "**EDIT**: I missed your previous message where you mentioned merging the\
    \ original LongLORA adapter_model.bin onto the model after replacing the embed/norm\
    \ weights. If the script below looks like it's doing what it should for those\
    \ weights, I can add the remaining code to load the LongLORA PEFT adapter and\
    \ merge that in before saving the resultant model. (**EDIT3**: I updated the script\
    \ with the PEFT code.) What's your take on applying this approach *after* merging\
    \ Aurelian with some other models? Do you think it would be better to prep all\
    \ the Llama2 models *before* the merge rather than applying it once to the resultant\
    \ blend after the merge with Aurelian? (**EDIT4**: I intend to test it both ways,\
    \ but I'm curious what you think about it.)\n\n**EDIT2**: A thought just occurred\
    \ to me. Do you want to make the trainable_params.bin file containing the embed/norm\
    \ weights extracted from your modified longLORA base available in your HF repo\
    \ for it? It would save other people having to download the full weights to extract\
    \ it themselves, and everyone could rest assured that they're working with a version\
    \ that was extracted properly.\n\nI wrote up a script that implements the code\
    \ you suggested. I'm including a sample of its output to show you what it is saving\
    \ to the trainable_params.bin file.\n\n```\n2024-01-21 11:00:08,421 - __main__\
    \ - DEBUG - Saving key  model.embed_tokens.weight  with value  tensor([[-0.0004,\
    \ -0.0012, -0.0011,  ...,  0.0002, -0.0013,  0.0008],\n        [-0.0003, -0.0013,\
    \  0.0017,  ..., -0.0019,  0.0032,  0.0033],\n        [ 0.0050,  0.0029, -0.0038,\
    \  ...,  0.0005, -0.0082,  0.0110],\n        ...,\n        [ 0.0057,  0.0189,\
    \  0.0099,  ..., -0.0154,  0.0043, -0.0136],\n        [-0.0028, -0.0086,  0.0019,\
    \  ...,  0.0176, -0.0158,  0.0172],\n        [ 0.0102, -0.0007,  0.0031,  ...,\
    \  0.0094, -0.0045,  0.0045]],\n       dtype=torch.float16). Saved key name is:\
    \ model.embed_tokens.weight\n```\n\nThen for each layer it's saving input_layernorm.weight\
    \ and post_attention_layernorm.weight.\n\n```\n2024-01-21 11:00:08,596 - __main__\
    \ - DEBUG - Saving key  model.layers.79.input_layernorm.weight  with value  tensor([0.2903,\
    \ 0.2993, 0.2812,  ..., 0.2815, 0.1703, 0.1708],\n       dtype=torch.float16).\
    \ Saved key name is: model.layers.79.input_layernorm.weight\n2024-01-21 11:00:08,596\
    \ - __main__ - DEBUG - Saving key  model.layers.79.post_attention_layernorm.weight\
    \  with value  tensor([0.3440, 0.3298, 0.3386,  ..., 0.3645, 0.1852, 0.2449],\n\
    \       dtype=torch.float16). Saved key name is: model.layers.79.post_attention_layernorm.weight\n\
    ```\n\nFinally, it saves the norm.weight.\n\n```\n2024-01-21 11:00:08,596 - __main__\
    \ - DEBUG - Saving key  model.norm.weight  with value  tensor([1.1504, 1.0186,\
    \ 1.1230,  ..., 1.1084, 1.4932, 1.3701],\n       dtype=torch.float16). Saved key\
    \ name is: model.norm.weight\n```\n\nIt doesn't appear that the key.replace(\"\
    base_model.model.\", \"\") code is doing anything. Is that a problem?\n\nHere\
    \ is the full script. **EDIT5**: Sorry for so many edits haha. I had to make some\
    \ fixes.\n\n```\nimport os\nimport argparse\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\nfrom peft import PeftModelForCausalLM\nimport torch\nimport logging\n\
    from colorama import Fore\n\nlogging.basicConfig(level=logging.DEBUG,\n      \
    \              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\
    logger = logging.getLogger(__name__)\n\ndef get_layers_to_save(model):\n    modules_to_save\
    \ = [\"embed\", \"norm\"]\n    state_dict = model.state_dict()\n    to_save =\
    \ {}\n    for key, value in state_dict.items():\n        if any(module_name in\
    \ key for module_name in modules_to_save):\n            key_name = key.replace(\"\
    base_model.model.\", \"\")\n            logger.debug(f\"{Fore.GREEN}Saving key\
    \  {key}  with value  {value}. Saved key name is: {key_name}{Fore.RESET}\")\n\
    \            to_save[key_name] = value\n        else:\n            logger.debug(f\"\
    Skipping key  {key}  because it is not in the list of modules to save\")\n   \
    \ return to_save\n\ndef main(args):\n   \n    if args.dtype == \"float16\":\n\
    \        dtype = torch.float16\n    elif args.dtype == \"float32\":\n        dtype\
    \ = torch.float32\n    else:\n        raise ValueError(f\"Please provide an appropriate\
    \ value for dtype. Value: {args.dtype}\")\n    \n    source_model_dir = os.path.abspath(args.source)\n\
    \    destination_dir = os.path.abspath(args.destination)\n    trainable_params_path\
    \ = os.path.join(source_model_dir, \"trainable_params/trainable_params.bin\")\n\
    \n    logger.debug(f\"Source dir: {source_model_dir}\\nDestination dir: {destination_dir}\\\
    nTrainable Params dir: {trainable_params_path}\")\n\n    logger.info(f\"Trainable\
    \ param path: {trainable_params_path}\")\n\n    # Check if trainable_params.bin\
    \ exists in the source model directory\n    if os.path.exists(trainable_params_path):\n\
    \        logger.info(\"Found existing trainable_params.bin. Using this file.\"\
    )\n    else:\n        if not os.path.exists(args.source):\n            raise FileNotFoundError(f\"\
    Source model not found at {args.source}\")\n\n        # Load source model\n  \
    \      logger.info(f\"Loading source model from {args.source}\")\n        source_model\
    \ = AutoModelForCausalLM.from_pretrained(args.source, torch_dtype=dtype)\n\n \
    \       # Save the embed and norm layers from the source model\n        to_save\
    \ = get_layers_to_save(source_model)\n        \n        torch.save(to_save, trainable_params_path)\n\
    \        logger.info(f\"trainable_params.bin saved to {trainable_params_path}\"\
    )\n        del source_model # free up resources\n\n    # Load target model and\
    \ tokenizer\n    if not os.path.exists(args.target):\n        raise FileNotFoundError(f\"\
    Target model not found at {args.target}\")\n    if not os.path.exists(args.lora):\n\
    \        raise FileNotFoundError(f\"LoRA not found at {args.lora}\")\n    logger.info(f\"\
    Loading target model from {args.target}\")\n    target_model = AutoModelForCausalLM.from_pretrained(args.target,\
    \ torch_dtype=dtype)\n    tokenizer = AutoTokenizer.from_pretrained(args.target)\n\
    \n    # Load the saved layers onto the target model\n    logger.info(\"Loading\
    \ state dict...\")\n    state_dict = torch.load(trainable_params_path, map_location=target_model.device)\n\
    \    target_model.load_state_dict(state_dict, strict=False)\n\n    # Merge in\
    \ the LoRA\n    logger.info(f\"Loading LoRA adapter from {args.lora} and merging\
    \ with the target model before saving\")\n    target_model = PeftModelForCausalLM.from_pretrained(target_model,\
    \ args.lora, dtype=dtype)\n    target_model = target_model.merge_and_unload(progressbar=True)\
    \ \n\n    # Save the updated model to the destination path, including its tokenizer\
    \ settings\n    logger.info(f\"Saving resultant blend to {destination_dir}. This\
    \ could take a while...\")\n    target_model.save_pretrained(save_directory=destination_dir,\
    \ safe_serialization=True, max_shard_size=f\"{args.shard_size}MiB\")\n    tokenizer.save_pretrained(destination_dir)\n\
    \    logger.info(f\"Updated model saved successfully at {destination_dir}\")\n\
    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\
    Transfer trainable parameters (embed and norm layers) from a source model to a\
    \ target model using the Transformers library.\")\n    parser.add_argument(\"\
    -s\", \"--source\", required=True, help=\"Path to or identifier of the source\
    \ model.\")\n    parser.add_argument(\"-t\", \"--target\", required=True, help=\"\
    Path to or identifier of the target model.\")\n    parser.add_argument(\"-d\"\
    , \"--destination\", required=True, help=\"Directory to save the updated model.\"\
    )\n    parser.add_argument('-l', '--lora', required=True, type=str, help=\"A path\
    \ to the adapter_model.bin file corresponding to original LongLORA base (e.g.\
    \ Yukang/Llama-2-70b-longlora-32k)\")\n    parser.add_argument(\"--dtype\", type=str,\
    \ default=\"float16\", choices=['float16', 'float32'], help=\"The torch data type\
    \ to use for loading the models. Defaults to float16.\")\n    parser.add_argument(\"\
    --shard_size\", type=int, default=8000, help=\"Size of shards for saving the model\
    \ tensors in MiB. Defaults to 8000.\")\n\n    args = parser.parse_args()\n   \
    \ main(args)\n\n```\n"
  created_at: 2024-01-21 19:15:07+00:00
  edited: true
  hidden: false
  id: 65ad6d3bdf46651406ff250f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-21T19:33:14.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.971947431564331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<blockquote>\n<p>What's your take on applying this approach after merging\
          \ Aurelian with some other models? Do you think it would be better to prep\
          \ all the Llama2 models before the merge rather than applying it once to\
          \ the resultant blend after the merge with Aurelian?</p>\n</blockquote>\n\
          <p>I only tested merging the LORA and embed/norm directly onto lzlv and\
          \ Euryale, without anything to do with Aurelian. I'd imagine you'd want\
          \ to do that <em>before</em> merging with Aurelian, as a prep step. But\
          \ who knows... experiment. Applying the LORA to the resultant doesn't seem\
          \ right as it will undo part of what Aurelian has done (I'm thinking you\
          \ don't want it to affect Aurelian).</p>\n<p>Code looks right. I don't remember\
          \ if Aurelian and/or longLORA were saved as bfloat16 or float16. Your debug\
          \ dump says float16 though.</p>\n<p>I'm trying to remember why I stuck that\
          \ <code>key.replace</code> in there. Probably from my code where I saved\
          \ the model out of the <code>trainer</code> class, which renames some of\
          \ the layers, and it does nothing bad if that isn't the case. You want the\
          \ key to look like <code>model.embed_tokens.weight</code>, and it does.</p>\n\
          <p>EDIT:</p>\n<blockquote>\n<p>A thought just occurred to me. Do you want\
          \ to make the trainable_params.bin file containing the embed/norm weights\
          \ extracted from your modified longLORA base available in your HF repo for\
          \ it? It would save other people having to download the full weights to\
          \ extract it themselves, and everyone could rest assured that they're working\
          \ with a version that was extracted properly.</p>\n</blockquote>\n<p>Yeah,\
          \ probably a good idea. It's small, I'll just add it to the fp16 repo.</p>\n\
          <p>EDIT2: You can also easily just modify <code>trainable_params.bin</code>\
          \ from the <a href=\"https://huggingface.co/Yukang/Llama-2-70b-longlora-32k\"\
          >original LongLORA repo</a> as follows, to strip out the extra row without\
          \ downloading the weights:</p>\n<pre><code>state_dict = torch.load(trainable_params_path,\
          \ map_location=model.device)\n#Remove extra [PAD] token (last row)\nif state_dict['model.embed_tokens.weight'].shape[0]\
          \ == 32001: #Check that we have the single extra row\n    state_dict['model.embed_tokens.weight']\
          \ = state_dict['model.embed_tokens.weight'][:-1, :]\n#Do whatever with modified\
          \ state_dict...\n</code></pre>\n<p>I basically did this + change <code>config.json</code>\
          \ to rope 8 + remove references to the extra token in the other jsons, for\
          \ my modified version.</p>\n"
        raw: "> What's your take on applying this approach after merging Aurelian\
          \ with some other models? Do you think it would be better to prep all the\
          \ Llama2 models before the merge rather than applying it once to the resultant\
          \ blend after the merge with Aurelian?\n\nI only tested merging the LORA\
          \ and embed/norm directly onto lzlv and Euryale, without anything to do\
          \ with Aurelian. I'd imagine you'd want to do that _before_ merging with\
          \ Aurelian, as a prep step. But who knows... experiment. Applying the LORA\
          \ to the resultant doesn't seem right as it will undo part of what Aurelian\
          \ has done (I'm thinking you don't want it to affect Aurelian).\n\nCode\
          \ looks right. I don't remember if Aurelian and/or longLORA were saved as\
          \ bfloat16 or float16. Your debug dump says float16 though.\n\nI'm trying\
          \ to remember why I stuck that `key.replace` in there. Probably from my\
          \ code where I saved the model out of the `trainer` class, which renames\
          \ some of the layers, and it does nothing bad if that isn't the case. You\
          \ want the key to look like `model.embed_tokens.weight`, and it does.\n\n\
          EDIT:\n> A thought just occurred to me. Do you want to make the trainable_params.bin\
          \ file containing the embed/norm weights extracted from your modified longLORA\
          \ base available in your HF repo for it? It would save other people having\
          \ to download the full weights to extract it themselves, and everyone could\
          \ rest assured that they're working with a version that was extracted properly.\n\
          \nYeah, probably a good idea. It's small, I'll just add it to the fp16 repo.\n\
          \nEDIT2: You can also easily just modify `trainable_params.bin` from the\
          \ [original LongLORA repo](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k)\
          \ as follows, to strip out the extra row without downloading the weights:\n\
          ```\nstate_dict = torch.load(trainable_params_path, map_location=model.device)\n\
          #Remove extra [PAD] token (last row)\nif state_dict['model.embed_tokens.weight'].shape[0]\
          \ == 32001: #Check that we have the single extra row\n    state_dict['model.embed_tokens.weight']\
          \ = state_dict['model.embed_tokens.weight'][:-1, :]\n#Do whatever with modified\
          \ state_dict...\n```\nI basically did this + change `config.json` to rope\
          \ 8 + remove references to the extra token in the other jsons, for my modified\
          \ version."
        updatedAt: '2024-01-21T19:43:42.971Z'
      numEdits: 6
      reactions: []
    id: 65ad717ab68db4f26ee3a89c
    type: comment
  author: grimulkan
  content: "> What's your take on applying this approach after merging Aurelian with\
    \ some other models? Do you think it would be better to prep all the Llama2 models\
    \ before the merge rather than applying it once to the resultant blend after the\
    \ merge with Aurelian?\n\nI only tested merging the LORA and embed/norm directly\
    \ onto lzlv and Euryale, without anything to do with Aurelian. I'd imagine you'd\
    \ want to do that _before_ merging with Aurelian, as a prep step. But who knows...\
    \ experiment. Applying the LORA to the resultant doesn't seem right as it will\
    \ undo part of what Aurelian has done (I'm thinking you don't want it to affect\
    \ Aurelian).\n\nCode looks right. I don't remember if Aurelian and/or longLORA\
    \ were saved as bfloat16 or float16. Your debug dump says float16 though.\n\n\
    I'm trying to remember why I stuck that `key.replace` in there. Probably from\
    \ my code where I saved the model out of the `trainer` class, which renames some\
    \ of the layers, and it does nothing bad if that isn't the case. You want the\
    \ key to look like `model.embed_tokens.weight`, and it does.\n\nEDIT:\n> A thought\
    \ just occurred to me. Do you want to make the trainable_params.bin file containing\
    \ the embed/norm weights extracted from your modified longLORA base available\
    \ in your HF repo for it? It would save other people having to download the full\
    \ weights to extract it themselves, and everyone could rest assured that they're\
    \ working with a version that was extracted properly.\n\nYeah, probably a good\
    \ idea. It's small, I'll just add it to the fp16 repo.\n\nEDIT2: You can also\
    \ easily just modify `trainable_params.bin` from the [original LongLORA repo](https://huggingface.co/Yukang/Llama-2-70b-longlora-32k)\
    \ as follows, to strip out the extra row without downloading the weights:\n```\n\
    state_dict = torch.load(trainable_params_path, map_location=model.device)\n#Remove\
    \ extra [PAD] token (last row)\nif state_dict['model.embed_tokens.weight'].shape[0]\
    \ == 32001: #Check that we have the single extra row\n    state_dict['model.embed_tokens.weight']\
    \ = state_dict['model.embed_tokens.weight'][:-1, :]\n#Do whatever with modified\
    \ state_dict...\n```\nI basically did this + change `config.json` to rope 8 +\
    \ remove references to the extra token in the other jsons, for my modified version."
  created_at: 2024-01-21 19:33:14+00:00
  edited: true
  hidden: false
  id: 65ad717ab68db4f26ee3a89c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-21T21:44:40.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9724401831626892
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>Thanks for sharing all these tips and tricks. I feel like I''m getting
          an advanced crash course in Llama2 hacking!</p>

          <p>I''ll keep you posted on my merge results. I''m trying to blend Midnight
          Rose with your Aurelian model because I have a good feeling about what that
          merge could be like.</p>

          '
        raw: 'Thanks for sharing all these tips and tricks. I feel like I''m getting
          an advanced crash course in Llama2 hacking!


          I''ll keep you posted on my merge results. I''m trying to blend Midnight
          Rose with your Aurelian model because I have a good feeling about what that
          merge could be like.'
        updatedAt: '2024-01-21T21:44:40.464Z'
      numEdits: 0
      reactions: []
    id: 65ad9048b0b0876790723065
    type: comment
  author: sophosympatheia
  content: 'Thanks for sharing all these tips and tricks. I feel like I''m getting
    an advanced crash course in Llama2 hacking!


    I''ll keep you posted on my merge results. I''m trying to blend Midnight Rose
    with your Aurelian model because I have a good feeling about what that merge could
    be like.'
  created_at: 2024-01-21 21:44:40+00:00
  edited: false
  hidden: false
  id: 65ad9048b0b0876790723065
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-22T23:28:45.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8039467334747314
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I uploaded LongLORA merged versions of 70B 32K <a href="https://huggingface.co/grimulkan/lzlv-longLORA-70b-rope8-32k-fp16">lzlv</a>,
          <a href="https://huggingface.co/grimulkan/Euryale-1.3-longLORA-70b-rope8-32k-fp16">Euryale
          1.3</a> and <a href="https://huggingface.co/grimulkan/Aetheria-longLORA-70b-rope8-32k-fp16">Aetheria</a>
          if that''s useful. No idea how well these work, but they seem to not be
          broken at least.</p>

          '
        raw: I uploaded LongLORA merged versions of 70B 32K [lzlv](https://huggingface.co/grimulkan/lzlv-longLORA-70b-rope8-32k-fp16),
          [Euryale 1.3](https://huggingface.co/grimulkan/Euryale-1.3-longLORA-70b-rope8-32k-fp16)
          and [Aetheria](https://huggingface.co/grimulkan/Aetheria-longLORA-70b-rope8-32k-fp16)
          if that's useful. No idea how well these work, but they seem to not be broken
          at least.
        updatedAt: '2024-01-22T23:28:45.231Z'
      numEdits: 0
      reactions: []
    id: 65aefa2d101482afcc4dad2c
    type: comment
  author: grimulkan
  content: I uploaded LongLORA merged versions of 70B 32K [lzlv](https://huggingface.co/grimulkan/lzlv-longLORA-70b-rope8-32k-fp16),
    [Euryale 1.3](https://huggingface.co/grimulkan/Euryale-1.3-longLORA-70b-rope8-32k-fp16)
    and [Aetheria](https://huggingface.co/grimulkan/Aetheria-longLORA-70b-rope8-32k-fp16)
    if that's useful. No idea how well these work, but they seem to not be broken
    at least.
  created_at: 2024-01-22 23:28:45+00:00
  edited: false
  hidden: false
  id: 65aefa2d101482afcc4dad2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
      fullname: Charles McSneed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChuckMcSneed
      type: user
    createdAt: '2024-01-23T05:02:08.000Z'
    data:
      edited: false
      editors:
      - ChuckMcSneed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2763836085796356
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
          fullname: Charles McSneed
          isHf: false
          isPro: false
          name: ChuckMcSneed
          type: user
        html: "<p>In the past I also tried merging models with LongLORA.</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/MU2-nhoK9qzVN-FpJrlQC.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/MU2-nhoK9qzVN-FpJrlQC.png\"\
          ></a></p>\n<p>I found that merging  chat-longlora had the least impact on\
          \ \"creativity\"(SP), but still was severely damaging to the model.<br>I\
          \ used this script, I'm not sure if I did it right, or if I should redo\
          \ the tests.</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nimport os\n\
          import argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\
          \    parser.add_argument(\"--base_model_name_or_path\", type=str)\n    parser.add_argument(\"\
          --peft_model_path\", type=str)\n    parser.add_argument(\"--output_dir\"\
          , type=str)\n    parser.add_argument(\"--device\", type=str, default=\"\
          auto\")\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\"\
          )\n\n    return parser.parse_args()\n\ndef main():\n    args = get_args()\n\
          \n    if args.device == 'auto':\n        device_arg = { 'device_map': 'auto'\
          \ }\n    else:\n        device_arg = { 'device_map': { \"\": args.device}\
          \ }\n\n    print(f\"Loading base model: {args.base_model_name_or_path}\"\
          )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
          \        return_dict=True,\n        torch_dtype=torch.float16,\n       \
          \ **device_arg\n    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\"\
          )\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ offload_folder = \"offload/\", **device_arg)\n    print(f\"Running merge_and_unload\"\
          )\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n    \
          \    model.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n   \
          \     tokenizer.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n\
          \    else:\n        model.save_pretrained(f\"{args.output_dir}\")\n    \
          \    tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
          Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n \
          \   main()\n</code></pre>\n"
        raw: "In the past I also tried merging models with LongLORA.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/MU2-nhoK9qzVN-FpJrlQC.png)\n\
          \nI found that merging  chat-longlora had the least impact on \"creativity\"\
          (SP), but still was severely damaging to the model.\nI used this script,\
          \ I'm not sure if I did it right, or if I should redo the tests.\n```\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft\
          \ import PeftModel\nimport torch\n\nimport os\nimport argparse\n\ndef get_args():\n\
          \    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base_model_name_or_path\"\
          , type=str)\n    parser.add_argument(\"--peft_model_path\", type=str)\n\
          \    parser.add_argument(\"--output_dir\", type=str)\n    parser.add_argument(\"\
          --device\", type=str, default=\"auto\")\n    parser.add_argument(\"--push_to_hub\"\
          , action=\"store_true\")\n\n    return parser.parse_args()\n\ndef main():\n\
          \    args = get_args()\n\n    if args.device == 'auto':\n        device_arg\
          \ = { 'device_map': 'auto' }\n    else:\n        device_arg = { 'device_map':\
          \ { \"\": args.device} }\n\n    print(f\"Loading base model: {args.base_model_name_or_path}\"\
          )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
          \        return_dict=True,\n        torch_dtype=torch.float16,\n       \
          \ **device_arg\n    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\"\
          )\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ offload_folder = \"offload/\", **device_arg)\n    print(f\"Running merge_and_unload\"\
          )\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n    \
          \    model.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n   \
          \     tokenizer.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n\
          \    else:\n        model.save_pretrained(f\"{args.output_dir}\")\n    \
          \    tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
          Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n \
          \   main()\n\n```"
        updatedAt: '2024-01-23T05:02:08.890Z'
      numEdits: 0
      reactions: []
    id: 65af485048481b399a412d8a
    type: comment
  author: ChuckMcSneed
  content: "In the past I also tried merging models with LongLORA.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/MU2-nhoK9qzVN-FpJrlQC.png)\n\
    \nI found that merging  chat-longlora had the least impact on \"creativity\"(SP),\
    \ but still was severely damaging to the model.\nI used this script, I'm not sure\
    \ if I did it right, or if I should redo the tests.\n```\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\
    \nimport os\nimport argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\
    \    parser.add_argument(\"--base_model_name_or_path\", type=str)\n    parser.add_argument(\"\
    --peft_model_path\", type=str)\n    parser.add_argument(\"--output_dir\", type=str)\n\
    \    parser.add_argument(\"--device\", type=str, default=\"auto\")\n    parser.add_argument(\"\
    --push_to_hub\", action=\"store_true\")\n\n    return parser.parse_args()\n\n\
    def main():\n    args = get_args()\n\n    if args.device == 'auto':\n        device_arg\
    \ = { 'device_map': 'auto' }\n    else:\n        device_arg = { 'device_map':\
    \ { \"\": args.device} }\n\n    print(f\"Loading base model: {args.base_model_name_or_path}\"\
    )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
    \        return_dict=True,\n        torch_dtype=torch.float16,\n        **device_arg\n\
    \    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\")\n    model = PeftModel.from_pretrained(base_model,\
    \ args.peft_model_path, offload_folder = \"offload/\", **device_arg)\n    print(f\"\
    Running merge_and_unload\")\n    model = model.merge_and_unload()\n\n    tokenizer\
    \ = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\n    if args.push_to_hub:\n\
    \        print(f\"Saving to hub ...\")\n        model.push_to_hub(f\"{args.output_dir}\"\
    , use_temp_dir=False)\n        tokenizer.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n\
    \    else:\n        model.save_pretrained(f\"{args.output_dir}\")\n        tokenizer.save_pretrained(f\"\
    {args.output_dir}\")\n        print(f\"Model saved to {args.output_dir}\")\n\n\
    if __name__ == \"__main__\" :\n    main()\n\n```"
  created_at: 2024-01-23 05:02:08+00:00
  edited: false
  hidden: false
  id: 65af485048481b399a412d8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-23T06:23:12.000Z'
    data:
      edited: true
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9525660276412964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;grimulkan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/grimulkan\">@<span class=\"\
          underline\">grimulkan</span></a></span>\n\n\t</span></span><br>I tried a\
          \ ties merge between two longLorafied versions of Midnight Rose 70b and\
          \ Aurelian, using your modified longLora Llama2 model as the base model\
          \ to receive the deltas, and the end result was a hot mess. Tonight I'm\
          \ going to quantize the longLora version of Midnight Rose 70b so I can verify\
          \ that modified version works. (I need to rule out the possibility that\
          \ I made a mistake with the longLorafication process, although I doubt that's\
          \ it.) If that model works, then something about the merge process is what\
          \ toasted the resulting model.<br>I think I'll try a ties merge / task_arithmetic\
          \ merge again but using regular Llama2 as the base model. If that doesn't\
          \ work, I'll explore other merge methods. Maybe a straight linear merge\
          \ or a slerp merge would be better for this particular use case.<br>I'll\
          \ keep you posted.</p>\n<p><strong>EDIT:</strong> I confirmed this morning\
          \ that the longLora version of Midnight Rose 70b works! Even if that is\
          \ all that comes of these experiments, I'll be happy. It's great to see\
          \ it producing good results out to 12K context. (Just to clarify, 12K is\
          \ all the context I can fit into my available memory at 4.85bpw. Presumably\
          \ it can go longer and still hold up.) My initial impression is the quality\
          \ of the output is holding up too. I'll merge it with Aurelian now and we'll\
          \ see what I get.</p>\n"
        raw: "@grimulkan \nI tried a ties merge between two longLorafied versions\
          \ of Midnight Rose 70b and Aurelian, using your modified longLora Llama2\
          \ model as the base model to receive the deltas, and the end result was\
          \ a hot mess. Tonight I'm going to quantize the longLora version of Midnight\
          \ Rose 70b so I can verify that modified version works. (I need to rule\
          \ out the possibility that I made a mistake with the longLorafication process,\
          \ although I doubt that's it.) If that model works, then something about\
          \ the merge process is what toasted the resulting model.\nI think I'll try\
          \ a ties merge / task_arithmetic merge again but using regular Llama2 as\
          \ the base model. If that doesn't work, I'll explore other merge methods.\
          \ Maybe a straight linear merge or a slerp merge would be better for this\
          \ particular use case.\nI'll keep you posted.\n\n**EDIT:** I confirmed this\
          \ morning that the longLora version of Midnight Rose 70b works! Even if\
          \ that is all that comes of these experiments, I'll be happy. It's great\
          \ to see it producing good results out to 12K context. (Just to clarify,\
          \ 12K is all the context I can fit into my available memory at 4.85bpw.\
          \ Presumably it can go longer and still hold up.) My initial impression\
          \ is the quality of the output is holding up too. I'll merge it with Aurelian\
          \ now and we'll see what I get."
        updatedAt: '2024-01-23T15:16:11.145Z'
      numEdits: 2
      reactions: []
    id: 65af5b50308fc2952b7507b9
    type: comment
  author: sophosympatheia
  content: "@grimulkan \nI tried a ties merge between two longLorafied versions of\
    \ Midnight Rose 70b and Aurelian, using your modified longLora Llama2 model as\
    \ the base model to receive the deltas, and the end result was a hot mess. Tonight\
    \ I'm going to quantize the longLora version of Midnight Rose 70b so I can verify\
    \ that modified version works. (I need to rule out the possibility that I made\
    \ a mistake with the longLorafication process, although I doubt that's it.) If\
    \ that model works, then something about the merge process is what toasted the\
    \ resulting model.\nI think I'll try a ties merge / task_arithmetic merge again\
    \ but using regular Llama2 as the base model. If that doesn't work, I'll explore\
    \ other merge methods. Maybe a straight linear merge or a slerp merge would be\
    \ better for this particular use case.\nI'll keep you posted.\n\n**EDIT:** I confirmed\
    \ this morning that the longLora version of Midnight Rose 70b works! Even if that\
    \ is all that comes of these experiments, I'll be happy. It's great to see it\
    \ producing good results out to 12K context. (Just to clarify, 12K is all the\
    \ context I can fit into my available memory at 4.85bpw. Presumably it can go\
    \ longer and still hold up.) My initial impression is the quality of the output\
    \ is holding up too. I'll merge it with Aurelian now and we'll see what I get."
  created_at: 2024-01-23 06:23:12+00:00
  edited: true
  hidden: false
  id: 65af5b50308fc2952b7507b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-23T18:12:44.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9883098006248474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ChuckMcSneed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ChuckMcSneed\"\
          >@<span class=\"underline\">ChuckMcSneed</span></a></span>\n\n\t</span></span>\
          \ That script doesn't seem to insert the embed and norm layers. It's what\
          \ I did in the lzlv, etc., uploads in the prior post (and in the code earlier\
          \ in this topic). The resulting models seem to at least not be completely\
          \ broken (I checked up to the full 32K context). So perhaps that was the\
          \ missing piece for merging LongLORA into other base models.</p>\n<p>EDIT:\
          \ <span data-props=\"{&quot;user&quot;:&quot;sophosympatheia&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sophosympatheia\">@<span\
          \ class=\"underline\">sophosympatheia</span></a></span>\n\n\t</span></span>\
          \ also seems to have replicated that with Midnight Rose! So it's a start.\
          \ Hopefully it also unlocks other merging methods with models like Aurelian.</p>\n\
          <p>EDIT2: <span data-props=\"{&quot;user&quot;:&quot;sophosympatheia&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sophosympatheia\"\
          >@<span class=\"underline\">sophosympatheia</span></a></span>\n\n\t</span></span>\
          \ Just to confirm which method worked: you merged longLORA LORA and embed+norm\
          \ layers into Midnight Rose, and that worked? That would be the same I did\
          \ for lzlv, Euryale, etc.</p>\n"
        raw: '@ChuckMcSneed That script doesn''t seem to insert the embed and norm
          layers. It''s what I did in the lzlv, etc., uploads in the prior post (and
          in the code earlier in this topic). The resulting models seem to at least
          not be completely broken (I checked up to the full 32K context). So perhaps
          that was the missing piece for merging LongLORA into other base models.


          EDIT: @sophosympatheia also seems to have replicated that with Midnight
          Rose! So it''s a start. Hopefully it also unlocks other merging methods
          with models like Aurelian.


          EDIT2: @sophosympatheia Just to confirm which method worked: you merged
          longLORA LORA and embed+norm layers into Midnight Rose, and that worked?
          That would be the same I did for lzlv, Euryale, etc.'
        updatedAt: '2024-01-23T18:15:21.005Z'
      numEdits: 3
      reactions: []
    id: 65b0019c2e39165640d71069
    type: comment
  author: grimulkan
  content: '@ChuckMcSneed That script doesn''t seem to insert the embed and norm layers.
    It''s what I did in the lzlv, etc., uploads in the prior post (and in the code
    earlier in this topic). The resulting models seem to at least not be completely
    broken (I checked up to the full 32K context). So perhaps that was the missing
    piece for merging LongLORA into other base models.


    EDIT: @sophosympatheia also seems to have replicated that with Midnight Rose!
    So it''s a start. Hopefully it also unlocks other merging methods with models
    like Aurelian.


    EDIT2: @sophosympatheia Just to confirm which method worked: you merged longLORA
    LORA and embed+norm layers into Midnight Rose, and that worked? That would be
    the same I did for lzlv, Euryale, etc.'
  created_at: 2024-01-23 18:12:44+00:00
  edited: true
  hidden: false
  id: 65b0019c2e39165640d71069
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-24T06:17:31.000Z'
    data:
      edited: true
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6044620871543884
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;grimulkan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/grimulkan\">@<span class=\"\
          underline\">grimulkan</span></a></span>\n\n\t</span></span> Yes, I merged\
          \ the embed+norm layers into Midnight Rose, then applied the longLORA LoRA.\
          \ It seems to be working, but I still can't get a viable merge with Aurelian.</p>\n\
          <p>This time the merge was closer in the respect that at the end of exllama2's\
          \ quantization process, it gave the result a perplexity score around 9,\
          \ whereas last time it was in the thousands.<br>However, I can't get the\
          \ resultant merge to load in Textgen WebUI using the Exllamav2_HF loader\
          \ or the non-HF Exllamav2 loader. I get a 'safetensors_rust.SafetensorError:\
          \ Error while deserializing header: MetadataIncompleteBuffer' error.</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\"\
          , line 213, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(selected_model, loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\"\
          , line 87, in load_model\n    output = load_func_map[loader](model_name)\n\
          \  File \"/home/llm/text-generation-webui/modules/models.py\", line 389,\
          \ in ExLlamav2_HF_loader\n    return Exllamav2HF.from_pretrained(model_name)\n\
          \  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\", line\
          \ 170, in from_pretrained\n    return Exllamav2HF(config)\n  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\"\
          , line 44, in __init__\n    self.ex_model.load(split)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
          , line 248, in load\n    for item in f: return item\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
          , line 266, in load_gen\n    module.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/mlp.py\"\
          , line 77, in load\n    self.down_proj.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/linear.py\"\
          , line 45, in load\n    if w is None: w = self.load_weight()\n  File \"\
          /home/llm/mergequant/exllamav2/exllamav2/module.py\", line 96, in load_weight\n\
          \    qtensors = self.load_multi([\"q_weight\", \"q_invperm\", \"q_scale\"\
          , \"q_scale_max\", \"q_groups\", \"q_perm\"], override_key = override_key)\n\
          \  File \"/home/llm/mergequant/exllamav2/exllamav2/module.py\", line 77,\
          \ in load_multi\n    tensors[k] = stfile.get_tensor(key + \".\" + k, device\
          \ = self.device())\n  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\"\
          , line 116, in get_tensor\n    f = self.get_cm(device)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\"\
          , line 107, in get_cm\n    f = safe_open(self.filename, framework = \"pt\"\
          , device = device)\nsafetensors_rust.SafetensorError: Error while deserializing\
          \ header: MetadataIncompleteBuffer\n</code></pre>\n<p>What's interesting\
          \ is the error occurs in Exllamav2's fasttensors.py. If I set the \"no_use_fast\"\
          \ option with the Exllamav2_HF loader, it produces the exact same error\
          \ on the same line in fasttensors.py.<br>The only reason I find that interesting\
          \ is because I also have issues with the fast tokenizer setting when loading\
          \ my longLORA version of Midnight Rose, but in that case I can get it to\
          \ load by using the \"no_use_fast\" option with the Exllamav2_HF loader.\
          \ Did you encounter that with your longLORA versions of lzlv and Euryale?</p>\n\
          <p>Here is the error I get when I try to load my longLORA Midnight Rose\
          \ version using the fast tokenizer setting:</p>\n<pre><code>Traceback (most\
          \ recent call last):\n  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\"\
          , line 213, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(selected_model, loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\"\
          , line 95, in load_model\n    tokenizer = load_tokenizer(model_name, model)\n\
          \  File \"/home/llm/text-generation-webui/modules/models.py\", line 119,\
          \ in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n  File\
          \ \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 814, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2029, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2261, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 124, in __init__\n    super().__init__(\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 111, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
          Exception: No such file or directory (os error 2)\n</code></pre>\n<p>I'll\
          \ dig more into this when I have time. I just thought I'd share the results\
          \ so far.</p>\n<p><strong>UPDATE:</strong> Copying tokenizer files from\
          \ a previous version of Midnight Rose fixed the fast tensor issue with the\
          \ longLORA Midnight Rose version. The same trick didn't help with the MR-Aurelian\
          \ hybrid model.</p>\n"
        raw: "@grimulkan Yes, I merged the embed+norm layers into Midnight Rose, then\
          \ applied the longLORA LoRA. It seems to be working, but I still can't get\
          \ a viable merge with Aurelian.\n\nThis time the merge was closer in the\
          \ respect that at the end of exllama2's quantization process, it gave the\
          \ result a perplexity score around 9, whereas last time it was in the thousands.\n\
          However, I can't get the resultant merge to load in Textgen WebUI using\
          \ the Exllamav2_HF loader or the non-HF Exllamav2 loader. I get a 'safetensors_rust.SafetensorError:\
          \ Error while deserializing header: MetadataIncompleteBuffer' error.\n\n\
          ```\nTraceback (most recent call last):\n  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\"\
          , line 213, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(selected_model, loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\"\
          , line 87, in load_model\n    output = load_func_map[loader](model_name)\n\
          \  File \"/home/llm/text-generation-webui/modules/models.py\", line 389,\
          \ in ExLlamav2_HF_loader\n    return Exllamav2HF.from_pretrained(model_name)\n\
          \  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\", line\
          \ 170, in from_pretrained\n    return Exllamav2HF(config)\n  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\"\
          , line 44, in __init__\n    self.ex_model.load(split)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
          , line 248, in load\n    for item in f: return item\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
          , line 266, in load_gen\n    module.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/mlp.py\"\
          , line 77, in load\n    self.down_proj.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/linear.py\"\
          , line 45, in load\n    if w is None: w = self.load_weight()\n  File \"\
          /home/llm/mergequant/exllamav2/exllamav2/module.py\", line 96, in load_weight\n\
          \    qtensors = self.load_multi([\"q_weight\", \"q_invperm\", \"q_scale\"\
          , \"q_scale_max\", \"q_groups\", \"q_perm\"], override_key = override_key)\n\
          \  File \"/home/llm/mergequant/exllamav2/exllamav2/module.py\", line 77,\
          \ in load_multi\n    tensors[k] = stfile.get_tensor(key + \".\" + k, device\
          \ = self.device())\n  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\"\
          , line 116, in get_tensor\n    f = self.get_cm(device)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\"\
          , line 107, in get_cm\n    f = safe_open(self.filename, framework = \"pt\"\
          , device = device)\nsafetensors_rust.SafetensorError: Error while deserializing\
          \ header: MetadataIncompleteBuffer\n```\n\nWhat's interesting is the error\
          \ occurs in Exllamav2's fasttensors.py. If I set the \"no_use_fast\" option\
          \ with the Exllamav2_HF loader, it produces the exact same error on the\
          \ same line in fasttensors.py. \nThe only reason I find that interesting\
          \ is because I also have issues with the fast tokenizer setting when loading\
          \ my longLORA version of Midnight Rose, but in that case I can get it to\
          \ load by using the \"no_use_fast\" option with the Exllamav2_HF loader.\
          \ Did you encounter that with your longLORA versions of lzlv and Euryale?\n\
          \nHere is the error I get when I try to load my longLORA Midnight Rose version\
          \ using the fast tokenizer setting:\n\n```\nTraceback (most recent call\
          \ last):\n  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\"\
          , line 213, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(selected_model, loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\"\
          , line 95, in load_model\n    tokenizer = load_tokenizer(model_name, model)\n\
          \  File \"/home/llm/text-generation-webui/modules/models.py\", line 119,\
          \ in load_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n  File\
          \ \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 814, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2029, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2261, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 124, in __init__\n    super().__init__(\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 111, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
          Exception: No such file or directory (os error 2)\n```\nI'll dig more into\
          \ this when I have time. I just thought I'd share the results so far.\n\n\
          **UPDATE:** Copying tokenizer files from a previous version of Midnight\
          \ Rose fixed the fast tensor issue with the longLORA Midnight Rose version.\
          \ The same trick didn't help with the MR-Aurelian hybrid model."
        updatedAt: '2024-01-24T06:25:16.879Z'
      numEdits: 1
      reactions: []
    id: 65b0ab7b9708887997fa2212
    type: comment
  author: sophosympatheia
  content: "@grimulkan Yes, I merged the embed+norm layers into Midnight Rose, then\
    \ applied the longLORA LoRA. It seems to be working, but I still can't get a viable\
    \ merge with Aurelian.\n\nThis time the merge was closer in the respect that at\
    \ the end of exllama2's quantization process, it gave the result a perplexity\
    \ score around 9, whereas last time it was in the thousands.\nHowever, I can't\
    \ get the resultant merge to load in Textgen WebUI using the Exllamav2_HF loader\
    \ or the non-HF Exllamav2 loader. I get a 'safetensors_rust.SafetensorError: Error\
    \ while deserializing header: MetadataIncompleteBuffer' error.\n\n```\nTraceback\
    \ (most recent call last):\n  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\"\
    , line 213, in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\", line\
    \ 87, in load_model\n    output = load_func_map[loader](model_name)\n  File \"\
    /home/llm/text-generation-webui/modules/models.py\", line 389, in ExLlamav2_HF_loader\n\
    \    return Exllamav2HF.from_pretrained(model_name)\n  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\"\
    , line 170, in from_pretrained\n    return Exllamav2HF(config)\n  File \"/home/llm/text-generation-webui/modules/exllamav2_hf.py\"\
    , line 44, in __init__\n    self.ex_model.load(split)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
    , line 248, in load\n    for item in f: return item\n  File \"/home/llm/mergequant/exllamav2/exllamav2/model.py\"\
    , line 266, in load_gen\n    module.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/mlp.py\"\
    , line 77, in load\n    self.down_proj.load()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/linear.py\"\
    , line 45, in load\n    if w is None: w = self.load_weight()\n  File \"/home/llm/mergequant/exllamav2/exllamav2/module.py\"\
    , line 96, in load_weight\n    qtensors = self.load_multi([\"q_weight\", \"q_invperm\"\
    , \"q_scale\", \"q_scale_max\", \"q_groups\", \"q_perm\"], override_key = override_key)\n\
    \  File \"/home/llm/mergequant/exllamav2/exllamav2/module.py\", line 77, in load_multi\n\
    \    tensors[k] = stfile.get_tensor(key + \".\" + k, device = self.device())\n\
    \  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\", line 116,\
    \ in get_tensor\n    f = self.get_cm(device)\n  File \"/home/llm/mergequant/exllamav2/exllamav2/fasttensors.py\"\
    , line 107, in get_cm\n    f = safe_open(self.filename, framework = \"pt\", device\
    \ = device)\nsafetensors_rust.SafetensorError: Error while deserializing header:\
    \ MetadataIncompleteBuffer\n```\n\nWhat's interesting is the error occurs in Exllamav2's\
    \ fasttensors.py. If I set the \"no_use_fast\" option with the Exllamav2_HF loader,\
    \ it produces the exact same error on the same line in fasttensors.py. \nThe only\
    \ reason I find that interesting is because I also have issues with the fast tokenizer\
    \ setting when loading my longLORA version of Midnight Rose, but in that case\
    \ I can get it to load by using the \"no_use_fast\" option with the Exllamav2_HF\
    \ loader. Did you encounter that with your longLORA versions of lzlv and Euryale?\n\
    \nHere is the error I get when I try to load my longLORA Midnight Rose version\
    \ using the fast tokenizer setting:\n\n```\nTraceback (most recent call last):\n\
    \  File \"/home/llm/text-generation-webui/modules/ui_model_menu.py\", line 213,\
    \ in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\n  File \"/home/llm/text-generation-webui/modules/models.py\", line\
    \ 95, in load_model\n    tokenizer = load_tokenizer(model_name, model)\n  File\
    \ \"/home/llm/text-generation-webui/modules/models.py\", line 119, in load_tokenizer\n\
    \    tokenizer = AutoTokenizer.from_pretrained(\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 814, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2029, in from_pretrained\n    return cls._from_pretrained(\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2261, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
    , line 124, in __init__\n    super().__init__(\n  File \"/home/llm/.miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 111, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
    Exception: No such file or directory (os error 2)\n```\nI'll dig more into this\
    \ when I have time. I just thought I'd share the results so far.\n\n**UPDATE:**\
    \ Copying tokenizer files from a previous version of Midnight Rose fixed the fast\
    \ tensor issue with the longLORA Midnight Rose version. The same trick didn't\
    \ help with the MR-Aurelian hybrid model."
  created_at: 2024-01-24 06:17:31+00:00
  edited: true
  hidden: false
  id: 65b0ab7b9708887997fa2212
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-24T09:02:33.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9306650757789612
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<p>I didn\u2019t encounter this error with the merges I made. For MR-Aurelian,\
          \ is it now the same tokenizer error, or does the model produce nonsense?</p>\n\
          <p>Can you see if there are any added tokens in MR alone (you can check\
          \ the accompanying jsons)? The longlora merge code may not have accounted\
          \ for that. I could not merge with Nous Hermes directly for instance, because\
          \ it had added special tokens. There would be no embed row for that token\
          \ from the long context. If that\u2019s the case with MR also, then it can\
          \ get a bit hacky where we need to reconstruct that row somehow without\
          \ finetuning.</p>\n"
        raw: "I didn\u2019t encounter this error with the merges I made. For MR-Aurelian,\
          \ is it now the same tokenizer error, or does the model produce nonsense?\n\
          \nCan you see if there are any added tokens in MR alone (you can check the\
          \ accompanying jsons)? The longlora merge code may not have accounted for\
          \ that. I could not merge with Nous Hermes directly for instance, because\
          \ it had added special tokens. There would be no embed row for that token\
          \ from the long context. If that\u2019s the case with MR also, then it can\
          \ get a bit hacky where we need to reconstruct that row somehow without\
          \ finetuning."
        updatedAt: '2024-01-24T09:07:54.505Z'
      numEdits: 1
      reactions: []
    id: 65b0d2292ffd1976c3eca28e
    type: comment
  author: grimulkan
  content: "I didn\u2019t encounter this error with the merges I made. For MR-Aurelian,\
    \ is it now the same tokenizer error, or does the model produce nonsense?\n\n\
    Can you see if there are any added tokens in MR alone (you can check the accompanying\
    \ jsons)? The longlora merge code may not have accounted for that. I could not\
    \ merge with Nous Hermes directly for instance, because it had added special tokens.\
    \ There would be no embed row for that token from the long context. If that\u2019\
    s the case with MR also, then it can get a bit hacky where we need to reconstruct\
    \ that row somehow without finetuning."
  created_at: 2024-01-24 09:02:33+00:00
  edited: true
  hidden: false
  id: 65b0d2292ffd1976c3eca28e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
      fullname: Charles McSneed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChuckMcSneed
      type: user
    createdAt: '2024-01-24T17:08:18.000Z'
    data:
      edited: false
      editors:
      - ChuckMcSneed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8746451139450073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4VOzArmrRaX_DUTxGmm59.jpeg?w=200&h=200&f=face
          fullname: Charles McSneed
          isHf: false
          isPro: false
          name: ChuckMcSneed
          type: user
        html: '<p>Update on the benchmarks: I tried merging xwin the right way, and
          the results are much better than before:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/R5zr-vAuIvi1elNNpFQSC.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/R5zr-vAuIvi1elNNpFQSC.png"></a><br>It
          seems that this type of extention degrades SP score by ~30%. Not bad, if
          you consider 4x the context.</p>

          '
        raw: 'Update on the benchmarks: I tried merging xwin the right way, and the
          results are much better than before:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/R5zr-vAuIvi1elNNpFQSC.png)

          It seems that this type of extention degrades SP score by ~30%. Not bad,
          if you consider 4x the context.'
        updatedAt: '2024-01-24T17:08:18.348Z'
      numEdits: 0
      reactions: []
    id: 65b14402ce23cf412f1ced39
    type: comment
  author: ChuckMcSneed
  content: 'Update on the benchmarks: I tried merging xwin the right way, and the
    results are much better than before:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/R5zr-vAuIvi1elNNpFQSC.png)

    It seems that this type of extention degrades SP score by ~30%. Not bad, if you
    consider 4x the context.'
  created_at: 2024-01-24 17:08:18+00:00
  edited: false
  hidden: false
  id: 65b14402ce23cf412f1ced39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-24T22:24:45.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9316263794898987
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Actually, 8x the context?</p>

          <p>Good to see. I will look into doing a small amount of 32K fine-tuning
          to try and glue the model better, and see if the degradation improves.</p>

          <p>Edit: Also, uploaded Xwin and Goliath 120b merged with this method. It
          seems to work.</p>

          '
        raw: 'Actually, 8x the context?


          Good to see. I will look into doing a small amount of 32K fine-tuning to
          try and glue the model better, and see if the degradation improves.


          Edit: Also, uploaded Xwin and Goliath 120b merged with this method. It seems
          to work.'
        updatedAt: '2024-01-24T23:31:57.883Z'
      numEdits: 1
      reactions: []
    id: 65b18e2d61eab097910de55c
    type: comment
  author: grimulkan
  content: 'Actually, 8x the context?


    Good to see. I will look into doing a small amount of 32K fine-tuning to try and
    glue the model better, and see if the degradation improves.


    Edit: Also, uploaded Xwin and Goliath 120b merged with this method. It seems to
    work.'
  created_at: 2024-01-24 22:24:45+00:00
  edited: true
  hidden: false
  id: 65b18e2d61eab097910de55c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2024-01-25T03:18:57.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467955827713013
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;grimulkan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/grimulkan\">@<span class=\"\
          underline\">grimulkan</span></a></span>\n\n\t</span></span> Nope, no added\
          \ tokens, but there are some differences in the tokenizer configs that I\
          \ suspect are causing all the problems. I'll keep you posted if I figure\
          \ it out. Right now I'm stuck haha.</p>\n"
        raw: '@grimulkan Nope, no added tokens, but there are some differences in
          the tokenizer configs that I suspect are causing all the problems. I''ll
          keep you posted if I figure it out. Right now I''m stuck haha.'
        updatedAt: '2024-01-25T03:18:57.567Z'
      numEdits: 0
      reactions: []
    id: 65b1d3213a41095a56410664
    type: comment
  author: sophosympatheia
  content: '@grimulkan Nope, no added tokens, but there are some differences in the
    tokenizer configs that I suspect are causing all the problems. I''ll keep you
    posted if I figure it out. Right now I''m stuck haha.'
  created_at: 2024-01-25 03:18:57+00:00
  edited: false
  hidden: false
  id: 65b1d3213a41095a56410664
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: grimulkan/aurelian-v0.5-70b-rope8-32K-fp16
repo_type: model
status: open
target_branch: null
title: Merging advice?
