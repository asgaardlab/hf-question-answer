!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jspr
conflicting_files: null
created_at: 2024-01-18 23:47:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
      fullname: Jasper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jspr
      type: user
    createdAt: '2024-01-18T23:47:35.000Z'
    data:
      edited: false
      editors:
      - jspr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9818624258041382
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
          fullname: Jasper
          isHf: false
          isPro: false
          name: jspr
          type: user
        html: '<p>This model is incredible. It''s simply the best out there for the
          task for which it was designed. However, I''ve found that after a while
          (maybe a few hundred to a few thousand tokens in), it can tend to devolve
          into consecutive long descriptive words with no punctuation. What might
          this be an artifact of? I''ve found that turning down the repetition penalty
          to 1 from tgui''s default of 1.15 can help.</p>

          '
        raw: This model is incredible. It's simply the best out there for the task
          for which it was designed. However, I've found that after a while (maybe
          a few hundred to a few thousand tokens in), it can tend to devolve into
          consecutive long descriptive words with no punctuation. What might this
          be an artifact of? I've found that turning down the repetition penalty to
          1 from tgui's default of 1.15 can help.
        updatedAt: '2024-01-18T23:47:35.008Z'
      numEdits: 0
      reactions: []
    id: 65a9b897dd55025184e50e38
    type: comment
  author: jspr
  content: This model is incredible. It's simply the best out there for the task for
    which it was designed. However, I've found that after a while (maybe a few hundred
    to a few thousand tokens in), it can tend to devolve into consecutive long descriptive
    words with no punctuation. What might this be an artifact of? I've found that
    turning down the repetition penalty to 1 from tgui's default of 1.15 can help.
  created_at: 2024-01-18 23:47:35+00:00
  edited: false
  hidden: false
  id: 65a9b897dd55025184e50e38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-18T23:57:41.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7525551319122314
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Interesting. Can you post your generation settings? Which quantization
          were you testing?</p>

          <p>I''ve definitely tested many generations up to the 32K limit and haven''t
          found that particular issue. Is it possible for you to share an example
          so I can replicate it?</p>

          <p>I often test with the following settings. It is true that I did not go
          to a very high rep penalty (but 1.15 doesn''t seem that high). Does what
          you describe happen with mirostat as well?</p>

          <p><strong>Standard sampling</strong>:</p>

          <pre><code>''temperature'': 0.7-0.8

          ''top_p'': 0.6

          ''min_p'': 0

          ''top_k'': 40

          ''repetition_penalty'': 1.12

          ''presence_penalty'': 0

          ''frequency_penalty'': 0

          ''repetition_penalty_range'': 1024

          ''typical_p'': 1

          ''tfs'': 1

          ''top_a'': 0

          </code></pre>

          <p><strong>Mirostat</strong>: (Oobabooga settings)</p>

          <pre><code>''mirostat_mode'': 2 (set to 1 if using GGUF)

          ''mirostat_tau'': 1.5 to 2

          ''mirostat_eta'': 0.1

          </code></pre>

          <p>with the other settings set to defaults:</p>

          <pre><code>''temperature'': 1

          ''top_p'': 1

          ''min_p'': 0

          ''top_k'': 0

          ''repetition_penalty'': 1

          ''presence_penalty'': 0

          ''frequency_penalty'': 0

          ''repetition_penalty_range'': 1024

          ''typical_p'': 1

          ''tfs'': 1

          ''top_a'': 0

          </code></pre>

          '
        raw: "Interesting. Can you post your generation settings? Which quantization\
          \ were you testing?\n\nI've definitely tested many generations up to the\
          \ 32K limit and haven't found that particular issue. Is it possible for\
          \ you to share an example so I can replicate it?\n\nI often test with the\
          \ following settings. It is true that I did not go to a very high rep penalty\
          \ (but 1.15 doesn't seem that high). Does what you describe happen with\
          \ mirostat as well?\n\n**Standard sampling**:\n\n    'temperature': 0.7-0.8\n\
          \    'top_p': 0.6\n    'min_p': 0\n    'top_k': 40\n    'repetition_penalty':\
          \ 1.12\n    'presence_penalty': 0\n    'frequency_penalty': 0\n    'repetition_penalty_range':\
          \ 1024\n    'typical_p': 1\n    'tfs': 1\n    'top_a': 0\n\n**Mirostat**:\
          \ (Oobabooga settings)\n\n    'mirostat_mode': 2 (set to 1 if using GGUF)\n\
          \    'mirostat_tau': 1.5 to 2\n    'mirostat_eta': 0.1\n\nwith the other\
          \ settings set to defaults:\n\n    'temperature': 1\n    'top_p': 1\n  \
          \  'min_p': 0\n    'top_k': 0\n    'repetition_penalty': 1\n    'presence_penalty':\
          \ 0\n    'frequency_penalty': 0\n    'repetition_penalty_range': 1024\n\
          \    'typical_p': 1\n    'tfs': 1\n    'top_a': 0\n"
        updatedAt: '2024-01-18T23:57:41.387Z'
      numEdits: 0
      reactions: []
    id: 65a9baf5cb5b4fb08e8698c5
    type: comment
  author: grimulkan
  content: "Interesting. Can you post your generation settings? Which quantization\
    \ were you testing?\n\nI've definitely tested many generations up to the 32K limit\
    \ and haven't found that particular issue. Is it possible for you to share an\
    \ example so I can replicate it?\n\nI often test with the following settings.\
    \ It is true that I did not go to a very high rep penalty (but 1.15 doesn't seem\
    \ that high). Does what you describe happen with mirostat as well?\n\n**Standard\
    \ sampling**:\n\n    'temperature': 0.7-0.8\n    'top_p': 0.6\n    'min_p': 0\n\
    \    'top_k': 40\n    'repetition_penalty': 1.12\n    'presence_penalty': 0\n\
    \    'frequency_penalty': 0\n    'repetition_penalty_range': 1024\n    'typical_p':\
    \ 1\n    'tfs': 1\n    'top_a': 0\n\n**Mirostat**: (Oobabooga settings)\n\n  \
    \  'mirostat_mode': 2 (set to 1 if using GGUF)\n    'mirostat_tau': 1.5 to 2\n\
    \    'mirostat_eta': 0.1\n\nwith the other settings set to defaults:\n\n    'temperature':\
    \ 1\n    'top_p': 1\n    'min_p': 0\n    'top_k': 0\n    'repetition_penalty':\
    \ 1\n    'presence_penalty': 0\n    'frequency_penalty': 0\n    'repetition_penalty_range':\
    \ 1024\n    'typical_p': 1\n    'tfs': 1\n    'top_a': 0\n"
  created_at: 2024-01-18 23:57:41+00:00
  edited: false
  hidden: false
  id: 65a9baf5cb5b4fb08e8698c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
      fullname: Jasper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jspr
      type: user
    createdAt: '2024-01-19T00:20:25.000Z'
    data:
      edited: false
      editors:
      - jspr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9066375494003296
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
          fullname: Jasper
          isHf: false
          isPro: false
          name: jspr
          type: user
        html: '<p>Super helpful, thanks! Here are the settings I''ve been using -
          they''re mostly the text-generation-webui defaults.</p>

          <pre><code>temperature 0.7

          top_p 0.9

          top_k 20

          min_p 0

          repetition penalty 1.15

          frequency and presence penalties 0

          repetition penalty range 1024

          guidance scale 1

          mirostat mode 0

          mirostat tau 5

          mirostat eta 0.1

          typical_p 1

          tfs 1

          top_a 0

          </code></pre>

          <p>for model loading, I''ve got <code>compress_pos_emb</code> at 8.</p>

          <p>The only major differences are top_p, top_k, mirostat mode, and mirostat
          tau, I think. I''m not super familiar with Mirostat, but maybe 5 is too
          high? or maybe mirostat mode == 0 means it''s off entirely?</p>

          <p>I''ve also mostly been using the elx2 quants, usually the 4 bpw one,
          but I doubt that was introducing performance problems.</p>

          '
        raw: 'Super helpful, thanks! Here are the settings I''ve been using - they''re
          mostly the text-generation-webui defaults.


          ```

          temperature 0.7

          top_p 0.9

          top_k 20

          min_p 0

          repetition penalty 1.15

          frequency and presence penalties 0

          repetition penalty range 1024

          guidance scale 1

          mirostat mode 0

          mirostat tau 5

          mirostat eta 0.1

          typical_p 1

          tfs 1

          top_a 0

          ```


          for model loading, I''ve got `compress_pos_emb` at 8.


          The only major differences are top_p, top_k, mirostat mode, and mirostat
          tau, I think. I''m not super familiar with Mirostat, but maybe 5 is too
          high? or maybe mirostat mode == 0 means it''s off entirely?


          I''ve also mostly been using the elx2 quants, usually the 4 bpw one, but
          I doubt that was introducing performance problems.'
        updatedAt: '2024-01-19T00:20:25.328Z'
      numEdits: 0
      reactions: []
    id: 65a9c0491145be22c8bd3b43
    type: comment
  author: jspr
  content: 'Super helpful, thanks! Here are the settings I''ve been using - they''re
    mostly the text-generation-webui defaults.


    ```

    temperature 0.7

    top_p 0.9

    top_k 20

    min_p 0

    repetition penalty 1.15

    frequency and presence penalties 0

    repetition penalty range 1024

    guidance scale 1

    mirostat mode 0

    mirostat tau 5

    mirostat eta 0.1

    typical_p 1

    tfs 1

    top_a 0

    ```


    for model loading, I''ve got `compress_pos_emb` at 8.


    The only major differences are top_p, top_k, mirostat mode, and mirostat tau,
    I think. I''m not super familiar with Mirostat, but maybe 5 is too high? or maybe
    mirostat mode == 0 means it''s off entirely?


    I''ve also mostly been using the elx2 quants, usually the 4 bpw one, but I doubt
    that was introducing performance problems.'
  created_at: 2024-01-19 00:20:25+00:00
  edited: false
  hidden: false
  id: 65a9c0491145be22c8bd3b43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-19T00:23:40.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8862051367759705
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Yeah mirostat mode = 0 implies it is off. You could try picking
          the default mirostat preset in the dropdown, and then change tau to 1.5-2
          (5 is meant for smaller/dumber models).</p>

          <p>Also, are you using the included <code>Aurelian.yaml</code> for the prompt
          template (or maybe manually set up the prompt format)?</p>

          <p>Edit: Just tested some generations that were ~4k tokens in length with
          the generation settings you used and it seemed fine, so that can''t be <em>that</em>
          wrong.</p>

          '
        raw: 'Yeah mirostat mode = 0 implies it is off. You could try picking the
          default mirostat preset in the dropdown, and then change tau to 1.5-2 (5
          is meant for smaller/dumber models).


          Also, are you using the included `Aurelian.yaml` for the prompt template
          (or maybe manually set up the prompt format)?


          Edit: Just tested some generations that were ~4k tokens in length with the
          generation settings you used and it seemed fine, so that can''t be _that_
          wrong.'
        updatedAt: '2024-01-19T00:29:54.931Z'
      numEdits: 4
      reactions: []
    id: 65a9c10c15102fd6592c16c5
    type: comment
  author: grimulkan
  content: 'Yeah mirostat mode = 0 implies it is off. You could try picking the default
    mirostat preset in the dropdown, and then change tau to 1.5-2 (5 is meant for
    smaller/dumber models).


    Also, are you using the included `Aurelian.yaml` for the prompt template (or maybe
    manually set up the prompt format)?


    Edit: Just tested some generations that were ~4k tokens in length with the generation
    settings you used and it seemed fine, so that can''t be _that_ wrong.'
  created_at: 2024-01-19 00:23:40+00:00
  edited: true
  hidden: false
  id: 65a9c10c15102fd6592c16c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
      fullname: Jasper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jspr
      type: user
    createdAt: '2024-01-19T22:32:20.000Z'
    data:
      edited: false
      editors:
      - jspr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9743693470954895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
          fullname: Jasper
          isHf: false
          isPro: false
          name: jspr
          type: user
        html: "<p>Yep, I made sure that the prompt template was the same as <code>Aurelian.yaml</code>.\
          \ Sounds like it's likely to be a mirostat thing, I'll re-run with that\
          \ enabled and the config that you suggested above. It definitely worked\
          \ like 95% of the time, so not a huge blocker or anything. Thanks <span\
          \ data-props=\"{&quot;user&quot;:&quot;grimulkan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/grimulkan\">@<span class=\"\
          underline\">grimulkan</span></a></span>\n\n\t</span></span>!</p>\n"
        raw: Yep, I made sure that the prompt template was the same as `Aurelian.yaml`.
          Sounds like it's likely to be a mirostat thing, I'll re-run with that enabled
          and the config that you suggested above. It definitely worked like 95% of
          the time, so not a huge blocker or anything. Thanks @grimulkan!
        updatedAt: '2024-01-19T22:32:20.460Z'
      numEdits: 0
      reactions: []
    id: 65aaf874f9e55b5f75c60df0
    type: comment
  author: jspr
  content: Yep, I made sure that the prompt template was the same as `Aurelian.yaml`.
    Sounds like it's likely to be a mirostat thing, I'll re-run with that enabled
    and the config that you suggested above. It definitely worked like 95% of the
    time, so not a huge blocker or anything. Thanks @grimulkan!
  created_at: 2024-01-19 22:32:20+00:00
  edited: false
  hidden: false
  id: 65aaf874f9e55b5f75c60df0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-19T22:35:45.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.97305828332901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Great! Do let me know if you can consistently trigger that 5% that
          doesn''t work though. It might be something I can fix in a later finetune.</p>

          '
        raw: Great! Do let me know if you can consistently trigger that 5% that doesn't
          work though. It might be something I can fix in a later finetune.
        updatedAt: '2024-01-19T22:35:45.019Z'
      numEdits: 0
      reactions: []
    id: 65aaf941c3fa44c710a1f4bc
    type: comment
  author: grimulkan
  content: Great! Do let me know if you can consistently trigger that 5% that doesn't
    work though. It might be something I can fix in a later finetune.
  created_at: 2024-01-19 22:35:45+00:00
  edited: false
  hidden: false
  id: 65aaf941c3fa44c710a1f4bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
      fullname: Jasper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jspr
      type: user
    createdAt: '2024-01-20T00:19:46.000Z'
    data:
      edited: false
      editors:
      - jspr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8937113285064697
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
          fullname: Jasper
          isHf: false
          isPro: false
          name: jspr
          type: user
        html: '<p>Yep, looks like enabling  mirostat mode solves it up through at
          least 8k tokens. Thanks for the help!</p>

          '
        raw: Yep, looks like enabling  mirostat mode solves it up through at least
          8k tokens. Thanks for the help!
        updatedAt: '2024-01-20T00:19:46.520Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65ab11a2819fbfaf498ebca0
    id: 65ab11a2819fbfaf498ebc9e
    type: comment
  author: jspr
  content: Yep, looks like enabling  mirostat mode solves it up through at least 8k
    tokens. Thanks for the help!
  created_at: 2024-01-20 00:19:46+00:00
  edited: false
  hidden: false
  id: 65ab11a2819fbfaf498ebc9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660326608235-62d9e75928f71aa2a180891e.jpeg?w=200&h=200&f=face
      fullname: Jasper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jspr
      type: user
    createdAt: '2024-01-20T00:19:46.000Z'
    data:
      status: closed
    id: 65ab11a2819fbfaf498ebca0
    type: status-change
  author: jspr
  created_at: 2024-01-20 00:19:46+00:00
  id: 65ab11a2819fbfaf498ebca0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: grimulkan/aurelian-v0.5-70b-rope8-32K-fp16
repo_type: model
status: closed
target_branch: null
title: Devolution into semi-nonsensical long words
