!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jcp1
conflicting_files: null
created_at: 2023-08-25 15:43:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d51e4d362f1c5c1ec9414669181234e2.svg
      fullname: JCP
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jcp1
      type: user
    createdAt: '2023-08-25T16:43:48.000Z'
    data:
      edited: true
      editors:
      - jcp1
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.5540995597839355
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d51e4d362f1c5c1ec9414669181234e2.svg
          fullname: JCP
          isHf: false
          isPro: false
          name: jcp1
          type: user
        html: "<p>I'm creating a prompt with:</p>\n<p><code>text = create_instruction(\"\
          \xBFCuanto es 3 x 9?\")</code></p>\n<p>And this is the output I get:</p>\n\
          <p><code>3 x 9 = 27.</code></p><code>\n</code><p><code></code><code>###\
          \ Instrucci\xF3n:<br>\xBFCu\xE1l es el resultado de 3 x 9?</code></p><code>\n\
          </code><p><code></code><code>### Respuesta:<br>El resultado de 3 x 9 es\
          \ 27.</code></p><code>\n</code><p><code></code><code>### Instrucci\xF3n:<br>\xBF\
          C</code></p>\n<p>Exptected output is just <code>3 x 9 = 27.</code></p>\n\
          <p>BTW, other very similar prompts work fine:</p>\n<code>\ntext = create_instruction(\"\
          \xBFCuanto es 3 x 2?\")\n</code>\n\n<p>Maybe the problem is that you are\
          \ fine tuning llama2 with a different template format?</p>\n"
        raw: "I'm creating a prompt with:\n\n<code>text = create_instruction(\"\xBF\
          Cuanto es 3 x 9?\")</code>\n\nAnd this is the output I get:\n\n<code>3 x\
          \ 9 = 27.\n\n</code><code>### Instrucci\xF3n:\n\xBFCu\xE1l es el resultado\
          \ de 3 x 9?\n\n</code><code>### Respuesta:\nEl resultado de 3 x 9 es 27.\n\
          \n</code><code>### Instrucci\xF3n:\n\xBFC</code>\n\nExptected output is\
          \ just <code>3 x 9 = 27.</code>\n\nBTW, other very similar prompts work\
          \ fine:\n\n<code>\ntext = create_instruction(\"\xBFCuanto es 3 x 2?\")\n\
          </code>\n\nMaybe the problem is that you are fine tuning llama2 with a different\
          \ template format?"
        updatedAt: '2023-08-25T16:44:57.475Z'
      numEdits: 1
      reactions: []
    id: 64e8da442ca4ff1d53874bc7
    type: comment
  author: jcp1
  content: "I'm creating a prompt with:\n\n<code>text = create_instruction(\"\xBF\
    Cuanto es 3 x 9?\")</code>\n\nAnd this is the output I get:\n\n<code>3 x 9 = 27.\n\
    \n</code><code>### Instrucci\xF3n:\n\xBFCu\xE1l es el resultado de 3 x 9?\n\n\
    </code><code>### Respuesta:\nEl resultado de 3 x 9 es 27.\n\n</code><code>###\
    \ Instrucci\xF3n:\n\xBFC</code>\n\nExptected output is just <code>3 x 9 = 27.</code>\n\
    \nBTW, other very similar prompts work fine:\n\n<code>\ntext = create_instruction(\"\
    \xBFCuanto es 3 x 2?\")\n</code>\n\nMaybe the problem is that you are fine tuning\
    \ llama2 with a different template format?"
  created_at: 2023-08-25 15:43:48+00:00
  edited: true
  hidden: false
  id: 64e8da442ca4ff1d53874bc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/d51e4d362f1c5c1ec9414669181234e2.svg
      fullname: JCP
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jcp1
      type: user
    createdAt: '2023-08-26T09:57:23.000Z'
    data:
      from: Model doesn't stop (no EOS token is generated).
      to: Model doesn't stop generating (no EOS token is generated).
    id: 64e9cc8381e68021f06e8690
    type: title-change
  author: jcp1
  created_at: 2023-08-26 08:57:23+00:00
  id: 64e9cc8381e68021f06e8690
  new_title: Model doesn't stop generating (no EOS token is generated).
  old_title: Model doesn't stop (no EOS token is generated).
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/d51e4d362f1c5c1ec9414669181234e2.svg
      fullname: JCP
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jcp1
      type: user
    createdAt: '2023-08-26T09:57:37.000Z'
    data:
      from: Model doesn't stop generating (no EOS token is generated).
      to: Model doesn't stop generating (no EOS token is detected).
    id: 64e9cc911d8b651a9ad4020d
    type: title-change
  author: jcp1
  created_at: 2023-08-26 08:57:37+00:00
  id: 64e9cc911d8b651a9ad4020d
  new_title: Model doesn't stop generating (no EOS token is detected).
  old_title: Model doesn't stop generating (no EOS token is generated).
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2e700e8d9fb811b74a502f78abba81d.svg
      fullname: Eduardo Quintanilla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eduiqe
      type: user
    createdAt: '2023-09-07T23:43:27.000Z'
    data:
      edited: false
      editors:
      - eduiqe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8894950747489929
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2e700e8d9fb811b74a502f78abba81d.svg
          fullname: Eduardo Quintanilla
          isHf: false
          isPro: false
          name: eduiqe
          type: user
        html: "<p>I was having the same problem.<br>A quick fix that worked for me\
          \ was specfying the eos_token_id inside GenerationConfig with the token\
          \ that commonly generates after the end of the ideal answer.</p>\n<p>In\
          \ my case was the token for \"[\" because it used to continue the conversation\
          \ adding \"[Usuario]: ... \"<br>In your case it could be \"#\" (for \"###\
          \ Instrucci\xF3n:\")</p>\n<p>generation_config = GenerationConfig(<br> \
          \               temperature = 0,<br>                top_p = 1,<br>     \
          \           top_k = 1,<br>                num_beams = 4,<br>           \
          \     eos_token_id = 29961    #token for [<br>)</p>\n<p>You can get the\
          \ token_id with this line of code:<br>tokenizer.get_vocab()[\"#\"]    #Output:\
          \ 29937</p>\n"
        raw: "I was having the same problem.\nA quick fix that worked for me was specfying\
          \ the eos_token_id inside GenerationConfig with the token that commonly\
          \ generates after the end of the ideal answer.\n\nIn my case was the token\
          \ for \"[\" because it used to continue the conversation adding \"[Usuario]:\
          \ ... \"\nIn your case it could be \"#\" (for \"### Instrucci\xF3n:\")\n\
          \ngeneration_config = GenerationConfig(\n                temperature = 0,\n\
          \                top_p = 1,\n                top_k = 1,\n              \
          \  num_beams = 4,\n                eos_token_id = 29961    #token for [\n\
          )\n\nYou can get the token_id with this line of code:\ntokenizer.get_vocab()[\"\
          #\"]    #Output: 29937"
        updatedAt: '2023-09-07T23:43:27.618Z'
      numEdits: 0
      reactions: []
    id: 64fa601f84bf01577e3bd406
    type: comment
  author: eduiqe
  content: "I was having the same problem.\nA quick fix that worked for me was specfying\
    \ the eos_token_id inside GenerationConfig with the token that commonly generates\
    \ after the end of the ideal answer.\n\nIn my case was the token for \"[\" because\
    \ it used to continue the conversation adding \"[Usuario]: ... \"\nIn your case\
    \ it could be \"#\" (for \"### Instrucci\xF3n:\")\n\ngeneration_config = GenerationConfig(\n\
    \                temperature = 0,\n                top_p = 1,\n              \
    \  top_k = 1,\n                num_beams = 4,\n                eos_token_id =\
    \ 29961    #token for [\n)\n\nYou can get the token_id with this line of code:\n\
    tokenizer.get_vocab()[\"#\"]    #Output: 29937"
  created_at: 2023-09-07 22:43:27+00:00
  edited: false
  hidden: false
  id: 64fa601f84bf01577e3bd406
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: clibrain/Llama-2-7b-ft-instruct-es
repo_type: model
status: open
target_branch: null
title: Model doesn't stop generating (no EOS token is detected).
