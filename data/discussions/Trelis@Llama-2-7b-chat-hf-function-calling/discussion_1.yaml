!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-07-31 17:42:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-31T18:42:51.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7945200204849243
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>This model will be improved as the underlying fine-tuning dataset\
          \ improves and expands.</p>\n<p>The 7B model already does well for simple\
          \ functions, whereas the 13B model can capture more complexity (i.e. more\
          \ data structures as function arguments).</p>\n<p>Using the \"test\" split\
          \ of the <a href=\"https://huggingface.co/datasets/Trelis/function_calling_extended\"\
          >Function Calling Extended</a> dataset, I'm scoring the performance as\"\
          :</p>\n<ul>\n<li>7B = 5 out of 7 (see below for an example I consider a\
          \ failure).</li>\n<li>13B = 6 out of 7.</li>\n</ul>\n<p>Note that the \"\
          test\" split was not used for training.</p>\n<h2 id=\"sample--fllama-13b-prompt-7b-does-well-on-this-too\"\
          >Sample  fLlama-13B prompt (7B does well on this too)</h2>\n<pre><code>Prompt:\n\
          [INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful research assistant. The following\
          \ functions are available for you to fetch further data to answer user questions,\
          \ if relevant:\n\n{\n    \"function\": \"save_chat\",\n    \"description\"\
          : \"This function saves content from current chat to a user's files. Use\
          \ this function if the user asks to save this chat or content from this\
          \ chat. If the user doesn't specify a name or description, use the content\
          \ of the chat to determine a suitable chat name and description.\",\n  \
          \  \"arguments\": [\n        {\n            \"name\": \"fileName\",\n  \
          \          \"type\": \"string\",\n            \"description\": \"A suitable\
          \ name for the file where the chat history will be saved.\"\n        },\n\
          \        {\n            \"name\": \"fileDescription\",\n            \"type\"\
          : \"string\",\n            \"description\": \"A brief description of the\
          \ chat history\"\n        },\n        {\n            \"name\": \"fileContent\"\
          ,\n            \"type\": \"string\",\n            \"description\": \"Content\
          \ to save. Format the content in markdown. Do not include system messages\"\
          \n        }\n    ]\n}\n\n{\n    \"function\": \"delete_file\",\n    \"description\"\
          : \"This function provides the ability to delete files in a user's folder.\
          \ It doesn't delete 'files.json'. It updates 'files.json' to remove entries\
          \ of deleted files.\",\n    \"arguments\": [\n        {\n            \"\
          name\": \"fileNames\",\n            \"type\": \"array\",\n            \"\
          items\": {\n                \"type\": \"string\"\n            },\n     \
          \       \"description\": \"An array of file names to be deleted\"\n    \
          \    }\n    ]\n}\n                \n\nTo call a function, respond - immediately\
          \ and only - with a JSON object of the following format:\n{\n    \"function\"\
          : \"function_name\",\n    \"arguments\": {\n        \"argument1\": value1,\n\
          \        \"argument2\": value2\n    }\n}\n&lt;&lt;/SYS&gt;&gt;\n\nThere\
          \ are one and two string kites. The two string ones are easier to control,\
          \ although you can get the cords tangled. The one-stringed ones are sometimes\
          \ used for kite fights, and you lose the kite and have to run after it if\
          \ the string breaks. \n\n Save this discussion of kites as KiteDiscussion.\
          \ [/INST]\n\nGenerated Assistant Response:\n {\n    \"function\": \"save_chat\"\
          ,\n    \"arguments\": {\n        \"fileName\": \"KiteDiscussion\",\n   \
          \     \"fileDescription\": \"A discussion about kites and their different\
          \ types\",\n        \"fileContent\": \"--- **KiteDiscussion** Kites are\
          \ a popular toy for children and adults alike. There are many different\
          \ types of kites, including one-stringed and two-stringed kites. One-stringed\
          \ kites are easier to control, but two-stringed kites are more stable and\
          \ easier to fly. Two-stringed kites are also easier to control, but they\
          \ can be more difficult to maneuver. Kite fights are a popular activity,\
          \ but you risk losing your kite if the string breaks. ---\"\n    }\n}&lt;/s&gt;\n\
          \nCorrect Assistant Response:\n{\n    \"function\": \"save_chat\",\n   \
          \ \"arguments\": {\n        \"fileName\": \"KiteDiscussion\",\n        \"\
          fileDescription\": \"Notes on one and two stringed kites\",\n        \"\
          fileContent\": \"--- **Types of Kite** There are one and two string kites.\
          \ The two string ones are easier to control, although you can get the cords\
          \ tangled. The one-stringed ones are sometimes used for kite fights, and\
          \ you lose the kite and have to run after it if the string breaks. ---\"\
          \n    }\n}\n</code></pre>\n<h2 id=\"fllama-7b-performance-on-a-tricky-prompt\"\
          >fLlama-7B performance on a tricky prompt</h2>\n<pre><code>Prompt:\n[INST]\
          \ &lt;&lt;SYS&gt;&gt;\nYou are a helpful research assistant. The following\
          \ functions are available for you to fetch further data to answer user questions,\
          \ if relevant:\n\n{\n    \"function\": \"delete_file\",\n    \"description\"\
          : \"This function provides the ability to delete files in a user's folder.\
          \ It doesn't delete 'files.json'. It updates 'files.json' to remove entries\
          \ of deleted files.\",\n    \"arguments\": [\n        {\n            \"\
          name\": \"fileNames\",\n            \"type\": \"array\",\n            \"\
          items\": {\n                \"type\": \"string\"\n            },\n     \
          \       \"description\": \"An array of file names to be deleted\"\n    \
          \    }\n    ]\n}\n\n{\n    \"function\": \"save_chat\",\n    \"description\"\
          : \"This function saves content from current chat to a user's files. Use\
          \ this function if the user asks to save this chat or content from this\
          \ chat. If the user doesn't specify a name or description, use the content\
          \ of the chat to determine a suitable chat name and description.\",\n  \
          \  \"arguments\": [\n        {\n            \"name\": \"fileName\",\n  \
          \          \"type\": \"string\",\n            \"description\": \"A suitable\
          \ name for the file where the chat history will be saved.\"\n        },\n\
          \        {\n            \"name\": \"fileDescription\",\n            \"type\"\
          : \"string\",\n            \"description\": \"A brief description of the\
          \ chat history\"\n        },\n        {\n            \"name\": \"fileContent\"\
          ,\n            \"type\": \"string\",\n            \"description\": \"Content\
          \ to save. Format the content in markdown. Do not include system messages\"\
          \n        }\n    ]\n}\n                \n\nTo call a function, respond -\
          \ immediately and only - with a JSON object of the following format:\n{\n\
          \    \"function\": \"function_name\",\n    \"arguments\": {\n        \"\
          argument1\": value1,\n        \"argument2\": value2\n    }\n}\n&lt;&lt;/SYS&gt;&gt;\n\
          \nIrish stew is typically made using beef, onions and potatoes. Start by\
          \ frying some onions in some butter in a pan. Add in and brown some cubes\
          \ of beer - very lightly. Add salt, pepper and some herbs. Then add the\
          \ sliced potatoes on top. Simmer over a medium heat for 15 minutes and then\
          \ leave to simmer on low heat for another hour. \n\n Save this Irish stew\
          \ recipe as irish_stew_recipes. [/INST]\n\nGenerated Assistant Response:\n\
          \ {\n    \"function\": \"save_chat\",\n    \"arguments\": {\n        \"\
          fileName\": \"irish_stew_recipes\",\n        \"fileDescription\": \"Irish\
          \ stew recipe\",\n        \"fileContent\": [\n            \"Beef, onions\
          \ and potatoes\",\n            \"Fry onions in butter\",\n            \"\
          Add cubes of beef and brown lightly\",\n            \"Add salt, pepper and\
          \ herbs\",\n            \"Add sliced potatoes and simmer for 15 minutes\"\
          ,\n            \"Leave to simmer on low heat for another hour\"\n      \
          \  ]\n    }\n}&lt;/s&gt;\n\nCorrect Assistant Response:\n{\n    \"function\"\
          : \"save_chat\",\n    \"arguments\": {\n        \"fileName\": \"irish_stew_recipes\"\
          ,\n        \"fileDescription\": \"Some recipes for Irish stew\",\n     \
          \   \"fileContent\": \"--- **Irish Stew Recipe** Irish stew is typically\
          \ made using beef, onions and potatoes. Start by frying some onions in some\
          \ butter in a pan. Add in and brown some cubes of beer - very lightly. Add\
          \ salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
          \ over a medium heat for 15 minutes and then leave to simmer on low heat\
          \ for another hour. \\n\\n Save this Irish stew recipe as irish_stew_recipes.\
          \ ---\"\n    }\n}\n</code></pre>\n<h2 id=\"fllama-13b-performance-on-that-same-prompt-above-not-100-perfect-but-very-good\"\
          >fLlama-13b performance on that same prompt above (not 100% perfect, but\
          \ very good):</h2>\n<pre><code>Prompt:\n[INST] &lt;&lt;SYS&gt;&gt;\nYou\
          \ are a helpful research assistant. The following functions are available\
          \ for you to fetch further data to answer user questions, if relevant:\n\
          \n{\n    \"function\": \"delete_file\",\n    \"description\": \"This function\
          \ provides the ability to delete files in a user's folder. It doesn't delete\
          \ 'files.json'. It updates 'files.json' to remove entries of deleted files.\"\
          ,\n    \"arguments\": [\n        {\n            \"name\": \"fileNames\"\
          ,\n            \"type\": \"array\",\n            \"items\": {\n        \
          \        \"type\": \"string\"\n            },\n            \"description\"\
          : \"An array of file names to be deleted\"\n        }\n    ]\n}\n\n{\n \
          \   \"function\": \"save_chat\",\n    \"description\": \"This function saves\
          \ content from current chat to a user's files. Use this function if the\
          \ user asks to save this chat or content from this chat. If the user doesn't\
          \ specify a name or description, use the content of the chat to determine\
          \ a suitable chat name and description.\",\n    \"arguments\": [\n     \
          \   {\n            \"name\": \"fileName\",\n            \"type\": \"string\"\
          ,\n            \"description\": \"A suitable name for the file where the\
          \ chat history will be saved.\"\n        },\n        {\n            \"name\"\
          : \"fileDescription\",\n            \"type\": \"string\",\n            \"\
          description\": \"A brief description of the chat history\"\n        },\n\
          \        {\n            \"name\": \"fileContent\",\n            \"type\"\
          : \"string\",\n            \"description\": \"Content to save. Format the\
          \ content in markdown. Do not include system messages\"\n        }\n   \
          \ ]\n}\n                \n\nTo call a function, respond - immediately and\
          \ only - with a JSON object of the following format:\n{\n    \"function\"\
          : \"function_name\",\n    \"arguments\": {\n        \"argument1\": value1,\n\
          \        \"argument2\": value2\n    }\n}\n&lt;&lt;/SYS&gt;&gt;\n\nIrish\
          \ stew is typically made using beef, onions and potatoes. Start by frying\
          \ some onions in some butter in a pan. Add in and brown some cubes of beer\
          \ - very lightly. Add salt, pepper and some herbs. Then add the sliced potatoes\
          \ on top. Simmer over a medium heat for 15 minutes and then leave to simmer\
          \ on low heat for another hour. \n\n Save this Irish stew recipe as irish_stew_recipes.\
          \ [/INST]\n\nGenerated Assistant Response:\n{\n    \"function\": \"save_chat\"\
          ,\n    \"arguments\": {\n        \"fileName\": \"irish_stew_recipes\",\n\
          \        \"fileDescription\": \"Irish stew recipe\",\n        \"fileContent\"\
          : \"--- Irish stew recipe Irish stew is typically made using beef, onions\
          \ and potatoes. Start by frying some onions in some butter in a pan. Add\
          \ in and brown some cubes of beer - very lightly. Add salt, pepper and some\
          \ herbs. Then add the sliced potatoes on top. Simmer over a medium heat\
          \ for 15 minutes and then leave to simmer on low heat for another hour.\
          \ ---\"\n    }\n}&lt;/s&gt;\n\nCorrect Assistant Response:\n{\n    \"function\"\
          : \"save_chat\",\n    \"arguments\": {\n        \"fileName\": \"irish_stew_recipes\"\
          ,\n        \"fileDescription\": \"Some recipes for Irish stew\",\n     \
          \   \"fileContent\": \"--- **Irish Stew Recipe** Irish stew is typically\
          \ made using beef, onions and potatoes. Start by frying some onions in some\
          \ butter in a pan. Add in and brown some cubes of beer - very lightly. Add\
          \ salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
          \ over a medium heat for 15 minutes and then leave to simmer on low heat\
          \ for another hour. \\n\\n Save this Irish stew recipe as irish_stew_recipes.\
          \ ---\"\n    }\n}\n</code></pre>\n"
        raw: "This model will be improved as the underlying fine-tuning dataset improves\
          \ and expands.\r\n\r\nThe 7B model already does well for simple functions,\
          \ whereas the 13B model can capture more complexity (i.e. more data structures\
          \ as function arguments).\r\n\r\nUsing the \"test\" split of the [Function\
          \ Calling Extended](https://huggingface.co/datasets/Trelis/function_calling_extended)\
          \ dataset, I'm scoring the performance as\":\r\n- 7B = 5 out of 7 (see below\
          \ for an example I consider a failure).\r\n- 13B = 6 out of 7.\r\n\r\nNote\
          \ that the \"test\" split was not used for training.\r\n\r\n## Sample  fLlama-13B\
          \ prompt (7B does well on this too)\r\n```\r\nPrompt:\r\n[INST] <<SYS>>\r\
          \nYou are a helpful research assistant. The following functions are available\
          \ for you to fetch further data to answer user questions, if relevant:\r\
          \n\r\n{\r\n    \"function\": \"save_chat\",\r\n    \"description\": \"This\
          \ function saves content from current chat to a user's files. Use this function\
          \ if the user asks to save this chat or content from this chat. If the user\
          \ doesn't specify a name or description, use the content of the chat to\
          \ determine a suitable chat name and description.\",\r\n    \"arguments\"\
          : [\r\n        {\r\n            \"name\": \"fileName\",\r\n            \"\
          type\": \"string\",\r\n            \"description\": \"A suitable name for\
          \ the file where the chat history will be saved.\"\r\n        },\r\n   \
          \     {\r\n            \"name\": \"fileDescription\",\r\n            \"\
          type\": \"string\",\r\n            \"description\": \"A brief description\
          \ of the chat history\"\r\n        },\r\n        {\r\n            \"name\"\
          : \"fileContent\",\r\n            \"type\": \"string\",\r\n            \"\
          description\": \"Content to save. Format the content in markdown. Do not\
          \ include system messages\"\r\n        }\r\n    ]\r\n}\r\n\r\n{\r\n    \"\
          function\": \"delete_file\",\r\n    \"description\": \"This function provides\
          \ the ability to delete files in a user's folder. It doesn't delete 'files.json'.\
          \ It updates 'files.json' to remove entries of deleted files.\",\r\n   \
          \ \"arguments\": [\r\n        {\r\n            \"name\": \"fileNames\",\r\
          \n            \"type\": \"array\",\r\n            \"items\": {\r\n     \
          \           \"type\": \"string\"\r\n            },\r\n            \"description\"\
          : \"An array of file names to be deleted\"\r\n        }\r\n    ]\r\n}\r\n\
          \                \r\n\r\nTo call a function, respond - immediately and only\
          \ - with a JSON object of the following format:\r\n{\r\n    \"function\"\
          : \"function_name\",\r\n    \"arguments\": {\r\n        \"argument1\": value1,\r\
          \n        \"argument2\": value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nThere are\
          \ one and two string kites. The two string ones are easier to control, although\
          \ you can get the cords tangled. The one-stringed ones are sometimes used\
          \ for kite fights, and you lose the kite and have to run after it if the\
          \ string breaks. \r\n\r\n Save this discussion of kites as KiteDiscussion.\
          \ [/INST]\r\n\r\nGenerated Assistant Response:\r\n {\r\n    \"function\"\
          : \"save_chat\",\r\n    \"arguments\": {\r\n        \"fileName\": \"KiteDiscussion\"\
          ,\r\n        \"fileDescription\": \"A discussion about kites and their different\
          \ types\",\r\n        \"fileContent\": \"--- **KiteDiscussion** Kites are\
          \ a popular toy for children and adults alike. There are many different\
          \ types of kites, including one-stringed and two-stringed kites. One-stringed\
          \ kites are easier to control, but two-stringed kites are more stable and\
          \ easier to fly. Two-stringed kites are also easier to control, but they\
          \ can be more difficult to maneuver. Kite fights are a popular activity,\
          \ but you risk losing your kite if the string breaks. ---\"\r\n    }\r\n\
          }</s>\r\n\r\nCorrect Assistant Response:\r\n{\r\n    \"function\": \"save_chat\"\
          ,\r\n    \"arguments\": {\r\n        \"fileName\": \"KiteDiscussion\",\r\
          \n        \"fileDescription\": \"Notes on one and two stringed kites\",\r\
          \n        \"fileContent\": \"--- **Types of Kite** There are one and two\
          \ string kites. The two string ones are easier to control, although you\
          \ can get the cords tangled. The one-stringed ones are sometimes used for\
          \ kite fights, and you lose the kite and have to run after it if the string\
          \ breaks. ---\"\r\n    }\r\n}\r\n```\r\n\r\n## fLlama-7B performance on\
          \ a tricky prompt\r\n\r\n```\r\nPrompt:\r\n[INST] <<SYS>>\r\nYou are a helpful\
          \ research assistant. The following functions are available for you to fetch\
          \ further data to answer user questions, if relevant:\r\n\r\n{\r\n    \"\
          function\": \"delete_file\",\r\n    \"description\": \"This function provides\
          \ the ability to delete files in a user's folder. It doesn't delete 'files.json'.\
          \ It updates 'files.json' to remove entries of deleted files.\",\r\n   \
          \ \"arguments\": [\r\n        {\r\n            \"name\": \"fileNames\",\r\
          \n            \"type\": \"array\",\r\n            \"items\": {\r\n     \
          \           \"type\": \"string\"\r\n            },\r\n            \"description\"\
          : \"An array of file names to be deleted\"\r\n        }\r\n    ]\r\n}\r\n\
          \r\n{\r\n    \"function\": \"save_chat\",\r\n    \"description\": \"This\
          \ function saves content from current chat to a user's files. Use this function\
          \ if the user asks to save this chat or content from this chat. If the user\
          \ doesn't specify a name or description, use the content of the chat to\
          \ determine a suitable chat name and description.\",\r\n    \"arguments\"\
          : [\r\n        {\r\n            \"name\": \"fileName\",\r\n            \"\
          type\": \"string\",\r\n            \"description\": \"A suitable name for\
          \ the file where the chat history will be saved.\"\r\n        },\r\n   \
          \     {\r\n            \"name\": \"fileDescription\",\r\n            \"\
          type\": \"string\",\r\n            \"description\": \"A brief description\
          \ of the chat history\"\r\n        },\r\n        {\r\n            \"name\"\
          : \"fileContent\",\r\n            \"type\": \"string\",\r\n            \"\
          description\": \"Content to save. Format the content in markdown. Do not\
          \ include system messages\"\r\n        }\r\n    ]\r\n}\r\n             \
          \   \r\n\r\nTo call a function, respond - immediately and only - with a\
          \ JSON object of the following format:\r\n{\r\n    \"function\": \"function_name\"\
          ,\r\n    \"arguments\": {\r\n        \"argument1\": value1,\r\n        \"\
          argument2\": value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nIrish stew is typically\
          \ made using beef, onions and potatoes. Start by frying some onions in some\
          \ butter in a pan. Add in and brown some cubes of beer - very lightly. Add\
          \ salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
          \ over a medium heat for 15 minutes and then leave to simmer on low heat\
          \ for another hour. \r\n\r\n Save this Irish stew recipe as irish_stew_recipes.\
          \ [/INST]\r\n\r\nGenerated Assistant Response:\r\n {\r\n    \"function\"\
          : \"save_chat\",\r\n    \"arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\"\
          ,\r\n        \"fileDescription\": \"Irish stew recipe\",\r\n        \"fileContent\"\
          : [\r\n            \"Beef, onions and potatoes\",\r\n            \"Fry onions\
          \ in butter\",\r\n            \"Add cubes of beef and brown lightly\",\r\
          \n            \"Add salt, pepper and herbs\",\r\n            \"Add sliced\
          \ potatoes and simmer for 15 minutes\",\r\n            \"Leave to simmer\
          \ on low heat for another hour\"\r\n        ]\r\n    }\r\n}</s>\r\n\r\n\
          Correct Assistant Response:\r\n{\r\n    \"function\": \"save_chat\",\r\n\
          \    \"arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\",\r\
          \n        \"fileDescription\": \"Some recipes for Irish stew\",\r\n    \
          \    \"fileContent\": \"--- **Irish Stew Recipe** Irish stew is typically\
          \ made using beef, onions and potatoes. Start by frying some onions in some\
          \ butter in a pan. Add in and brown some cubes of beer - very lightly. Add\
          \ salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
          \ over a medium heat for 15 minutes and then leave to simmer on low heat\
          \ for another hour. \\n\\n Save this Irish stew recipe as irish_stew_recipes.\
          \ ---\"\r\n    }\r\n}\r\n```\r\n\r\n## fLlama-13b performance on that same\
          \ prompt above (not 100% perfect, but very good):\r\n```\r\nPrompt:\r\n\
          [INST] <<SYS>>\r\nYou are a helpful research assistant. The following functions\
          \ are available for you to fetch further data to answer user questions,\
          \ if relevant:\r\n\r\n{\r\n    \"function\": \"delete_file\",\r\n    \"\
          description\": \"This function provides the ability to delete files in a\
          \ user's folder. It doesn't delete 'files.json'. It updates 'files.json'\
          \ to remove entries of deleted files.\",\r\n    \"arguments\": [\r\n   \
          \     {\r\n            \"name\": \"fileNames\",\r\n            \"type\"\
          : \"array\",\r\n            \"items\": {\r\n                \"type\": \"\
          string\"\r\n            },\r\n            \"description\": \"An array of\
          \ file names to be deleted\"\r\n        }\r\n    ]\r\n}\r\n\r\n{\r\n   \
          \ \"function\": \"save_chat\",\r\n    \"description\": \"This function saves\
          \ content from current chat to a user's files. Use this function if the\
          \ user asks to save this chat or content from this chat. If the user doesn't\
          \ specify a name or description, use the content of the chat to determine\
          \ a suitable chat name and description.\",\r\n    \"arguments\": [\r\n \
          \       {\r\n            \"name\": \"fileName\",\r\n            \"type\"\
          : \"string\",\r\n            \"description\": \"A suitable name for the\
          \ file where the chat history will be saved.\"\r\n        },\r\n       \
          \ {\r\n            \"name\": \"fileDescription\",\r\n            \"type\"\
          : \"string\",\r\n            \"description\": \"A brief description of the\
          \ chat history\"\r\n        },\r\n        {\r\n            \"name\": \"\
          fileContent\",\r\n            \"type\": \"string\",\r\n            \"description\"\
          : \"Content to save. Format the content in markdown. Do not include system\
          \ messages\"\r\n        }\r\n    ]\r\n}\r\n                \r\n\r\nTo call\
          \ a function, respond - immediately and only - with a JSON object of the\
          \ following format:\r\n{\r\n    \"function\": \"function_name\",\r\n   \
          \ \"arguments\": {\r\n        \"argument1\": value1,\r\n        \"argument2\"\
          : value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nIrish stew is typically made using\
          \ beef, onions and potatoes. Start by frying some onions in some butter\
          \ in a pan. Add in and brown some cubes of beer - very lightly. Add salt,\
          \ pepper and some herbs. Then add the sliced potatoes on top. Simmer over\
          \ a medium heat for 15 minutes and then leave to simmer on low heat for\
          \ another hour. \r\n\r\n Save this Irish stew recipe as irish_stew_recipes.\
          \ [/INST]\r\n\r\nGenerated Assistant Response:\r\n{\r\n    \"function\"\
          : \"save_chat\",\r\n    \"arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\"\
          ,\r\n        \"fileDescription\": \"Irish stew recipe\",\r\n        \"fileContent\"\
          : \"--- Irish stew recipe Irish stew is typically made using beef, onions\
          \ and potatoes. Start by frying some onions in some butter in a pan. Add\
          \ in and brown some cubes of beer - very lightly. Add salt, pepper and some\
          \ herbs. Then add the sliced potatoes on top. Simmer over a medium heat\
          \ for 15 minutes and then leave to simmer on low heat for another hour.\
          \ ---\"\r\n    }\r\n}</s>\r\n\r\nCorrect Assistant Response:\r\n{\r\n  \
          \  \"function\": \"save_chat\",\r\n    \"arguments\": {\r\n        \"fileName\"\
          : \"irish_stew_recipes\",\r\n        \"fileDescription\": \"Some recipes\
          \ for Irish stew\",\r\n        \"fileContent\": \"--- **Irish Stew Recipe**\
          \ Irish stew is typically made using beef, onions and potatoes. Start by\
          \ frying some onions in some butter in a pan. Add in and brown some cubes\
          \ of beer - very lightly. Add salt, pepper and some herbs. Then add the\
          \ sliced potatoes on top. Simmer over a medium heat for 15 minutes and then\
          \ leave to simmer on low heat for another hour. \\n\\n Save this Irish stew\
          \ recipe as irish_stew_recipes. ---\"\r\n    }\r\n}\r\n```"
        updatedAt: '2023-07-31T18:42:51.145Z'
      numEdits: 0
      reactions: []
    id: 64c800abca9e7f2fb6726a5c
    type: comment
  author: RonanMcGovern
  content: "This model will be improved as the underlying fine-tuning dataset improves\
    \ and expands.\r\n\r\nThe 7B model already does well for simple functions, whereas\
    \ the 13B model can capture more complexity (i.e. more data structures as function\
    \ arguments).\r\n\r\nUsing the \"test\" split of the [Function Calling Extended](https://huggingface.co/datasets/Trelis/function_calling_extended)\
    \ dataset, I'm scoring the performance as\":\r\n- 7B = 5 out of 7 (see below for\
    \ an example I consider a failure).\r\n- 13B = 6 out of 7.\r\n\r\nNote that the\
    \ \"test\" split was not used for training.\r\n\r\n## Sample  fLlama-13B prompt\
    \ (7B does well on this too)\r\n```\r\nPrompt:\r\n[INST] <<SYS>>\r\nYou are a\
    \ helpful research assistant. The following functions are available for you to\
    \ fetch further data to answer user questions, if relevant:\r\n\r\n{\r\n    \"\
    function\": \"save_chat\",\r\n    \"description\": \"This function saves content\
    \ from current chat to a user's files. Use this function if the user asks to save\
    \ this chat or content from this chat. If the user doesn't specify a name or description,\
    \ use the content of the chat to determine a suitable chat name and description.\"\
    ,\r\n    \"arguments\": [\r\n        {\r\n            \"name\": \"fileName\",\r\
    \n            \"type\": \"string\",\r\n            \"description\": \"A suitable\
    \ name for the file where the chat history will be saved.\"\r\n        },\r\n\
    \        {\r\n            \"name\": \"fileDescription\",\r\n            \"type\"\
    : \"string\",\r\n            \"description\": \"A brief description of the chat\
    \ history\"\r\n        },\r\n        {\r\n            \"name\": \"fileContent\"\
    ,\r\n            \"type\": \"string\",\r\n            \"description\": \"Content\
    \ to save. Format the content in markdown. Do not include system messages\"\r\n\
    \        }\r\n    ]\r\n}\r\n\r\n{\r\n    \"function\": \"delete_file\",\r\n  \
    \  \"description\": \"This function provides the ability to delete files in a\
    \ user's folder. It doesn't delete 'files.json'. It updates 'files.json' to remove\
    \ entries of deleted files.\",\r\n    \"arguments\": [\r\n        {\r\n      \
    \      \"name\": \"fileNames\",\r\n            \"type\": \"array\",\r\n      \
    \      \"items\": {\r\n                \"type\": \"string\"\r\n            },\r\
    \n            \"description\": \"An array of file names to be deleted\"\r\n  \
    \      }\r\n    ]\r\n}\r\n                \r\n\r\nTo call a function, respond\
    \ - immediately and only - with a JSON object of the following format:\r\n{\r\n\
    \    \"function\": \"function_name\",\r\n    \"arguments\": {\r\n        \"argument1\"\
    : value1,\r\n        \"argument2\": value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nThere\
    \ are one and two string kites. The two string ones are easier to control, although\
    \ you can get the cords tangled. The one-stringed ones are sometimes used for\
    \ kite fights, and you lose the kite and have to run after it if the string breaks.\
    \ \r\n\r\n Save this discussion of kites as KiteDiscussion. [/INST]\r\n\r\nGenerated\
    \ Assistant Response:\r\n {\r\n    \"function\": \"save_chat\",\r\n    \"arguments\"\
    : {\r\n        \"fileName\": \"KiteDiscussion\",\r\n        \"fileDescription\"\
    : \"A discussion about kites and their different types\",\r\n        \"fileContent\"\
    : \"--- **KiteDiscussion** Kites are a popular toy for children and adults alike.\
    \ There are many different types of kites, including one-stringed and two-stringed\
    \ kites. One-stringed kites are easier to control, but two-stringed kites are\
    \ more stable and easier to fly. Two-stringed kites are also easier to control,\
    \ but they can be more difficult to maneuver. Kite fights are a popular activity,\
    \ but you risk losing your kite if the string breaks. ---\"\r\n    }\r\n}</s>\r\
    \n\r\nCorrect Assistant Response:\r\n{\r\n    \"function\": \"save_chat\",\r\n\
    \    \"arguments\": {\r\n        \"fileName\": \"KiteDiscussion\",\r\n       \
    \ \"fileDescription\": \"Notes on one and two stringed kites\",\r\n        \"\
    fileContent\": \"--- **Types of Kite** There are one and two string kites. The\
    \ two string ones are easier to control, although you can get the cords tangled.\
    \ The one-stringed ones are sometimes used for kite fights, and you lose the kite\
    \ and have to run after it if the string breaks. ---\"\r\n    }\r\n}\r\n```\r\n\
    \r\n## fLlama-7B performance on a tricky prompt\r\n\r\n```\r\nPrompt:\r\n[INST]\
    \ <<SYS>>\r\nYou are a helpful research assistant. The following functions are\
    \ available for you to fetch further data to answer user questions, if relevant:\r\
    \n\r\n{\r\n    \"function\": \"delete_file\",\r\n    \"description\": \"This function\
    \ provides the ability to delete files in a user's folder. It doesn't delete 'files.json'.\
    \ It updates 'files.json' to remove entries of deleted files.\",\r\n    \"arguments\"\
    : [\r\n        {\r\n            \"name\": \"fileNames\",\r\n            \"type\"\
    : \"array\",\r\n            \"items\": {\r\n                \"type\": \"string\"\
    \r\n            },\r\n            \"description\": \"An array of file names to\
    \ be deleted\"\r\n        }\r\n    ]\r\n}\r\n\r\n{\r\n    \"function\": \"save_chat\"\
    ,\r\n    \"description\": \"This function saves content from current chat to a\
    \ user's files. Use this function if the user asks to save this chat or content\
    \ from this chat. If the user doesn't specify a name or description, use the content\
    \ of the chat to determine a suitable chat name and description.\",\r\n    \"\
    arguments\": [\r\n        {\r\n            \"name\": \"fileName\",\r\n       \
    \     \"type\": \"string\",\r\n            \"description\": \"A suitable name\
    \ for the file where the chat history will be saved.\"\r\n        },\r\n     \
    \   {\r\n            \"name\": \"fileDescription\",\r\n            \"type\": \"\
    string\",\r\n            \"description\": \"A brief description of the chat history\"\
    \r\n        },\r\n        {\r\n            \"name\": \"fileContent\",\r\n    \
    \        \"type\": \"string\",\r\n            \"description\": \"Content to save.\
    \ Format the content in markdown. Do not include system messages\"\r\n       \
    \ }\r\n    ]\r\n}\r\n                \r\n\r\nTo call a function, respond - immediately\
    \ and only - with a JSON object of the following format:\r\n{\r\n    \"function\"\
    : \"function_name\",\r\n    \"arguments\": {\r\n        \"argument1\": value1,\r\
    \n        \"argument2\": value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nIrish stew is\
    \ typically made using beef, onions and potatoes. Start by frying some onions\
    \ in some butter in a pan. Add in and brown some cubes of beer - very lightly.\
    \ Add salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
    \ over a medium heat for 15 minutes and then leave to simmer on low heat for another\
    \ hour. \r\n\r\n Save this Irish stew recipe as irish_stew_recipes. [/INST]\r\n\
    \r\nGenerated Assistant Response:\r\n {\r\n    \"function\": \"save_chat\",\r\n\
    \    \"arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\",\r\n   \
    \     \"fileDescription\": \"Irish stew recipe\",\r\n        \"fileContent\":\
    \ [\r\n            \"Beef, onions and potatoes\",\r\n            \"Fry onions\
    \ in butter\",\r\n            \"Add cubes of beef and brown lightly\",\r\n   \
    \         \"Add salt, pepper and herbs\",\r\n            \"Add sliced potatoes\
    \ and simmer for 15 minutes\",\r\n            \"Leave to simmer on low heat for\
    \ another hour\"\r\n        ]\r\n    }\r\n}</s>\r\n\r\nCorrect Assistant Response:\r\
    \n{\r\n    \"function\": \"save_chat\",\r\n    \"arguments\": {\r\n        \"\
    fileName\": \"irish_stew_recipes\",\r\n        \"fileDescription\": \"Some recipes\
    \ for Irish stew\",\r\n        \"fileContent\": \"--- **Irish Stew Recipe** Irish\
    \ stew is typically made using beef, onions and potatoes. Start by frying some\
    \ onions in some butter in a pan. Add in and brown some cubes of beer - very lightly.\
    \ Add salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
    \ over a medium heat for 15 minutes and then leave to simmer on low heat for another\
    \ hour. \\n\\n Save this Irish stew recipe as irish_stew_recipes. ---\"\r\n  \
    \  }\r\n}\r\n```\r\n\r\n## fLlama-13b performance on that same prompt above (not\
    \ 100% perfect, but very good):\r\n```\r\nPrompt:\r\n[INST] <<SYS>>\r\nYou are\
    \ a helpful research assistant. The following functions are available for you\
    \ to fetch further data to answer user questions, if relevant:\r\n\r\n{\r\n  \
    \  \"function\": \"delete_file\",\r\n    \"description\": \"This function provides\
    \ the ability to delete files in a user's folder. It doesn't delete 'files.json'.\
    \ It updates 'files.json' to remove entries of deleted files.\",\r\n    \"arguments\"\
    : [\r\n        {\r\n            \"name\": \"fileNames\",\r\n            \"type\"\
    : \"array\",\r\n            \"items\": {\r\n                \"type\": \"string\"\
    \r\n            },\r\n            \"description\": \"An array of file names to\
    \ be deleted\"\r\n        }\r\n    ]\r\n}\r\n\r\n{\r\n    \"function\": \"save_chat\"\
    ,\r\n    \"description\": \"This function saves content from current chat to a\
    \ user's files. Use this function if the user asks to save this chat or content\
    \ from this chat. If the user doesn't specify a name or description, use the content\
    \ of the chat to determine a suitable chat name and description.\",\r\n    \"\
    arguments\": [\r\n        {\r\n            \"name\": \"fileName\",\r\n       \
    \     \"type\": \"string\",\r\n            \"description\": \"A suitable name\
    \ for the file where the chat history will be saved.\"\r\n        },\r\n     \
    \   {\r\n            \"name\": \"fileDescription\",\r\n            \"type\": \"\
    string\",\r\n            \"description\": \"A brief description of the chat history\"\
    \r\n        },\r\n        {\r\n            \"name\": \"fileContent\",\r\n    \
    \        \"type\": \"string\",\r\n            \"description\": \"Content to save.\
    \ Format the content in markdown. Do not include system messages\"\r\n       \
    \ }\r\n    ]\r\n}\r\n                \r\n\r\nTo call a function, respond - immediately\
    \ and only - with a JSON object of the following format:\r\n{\r\n    \"function\"\
    : \"function_name\",\r\n    \"arguments\": {\r\n        \"argument1\": value1,\r\
    \n        \"argument2\": value2\r\n    }\r\n}\r\n<</SYS>>\r\n\r\nIrish stew is\
    \ typically made using beef, onions and potatoes. Start by frying some onions\
    \ in some butter in a pan. Add in and brown some cubes of beer - very lightly.\
    \ Add salt, pepper and some herbs. Then add the sliced potatoes on top. Simmer\
    \ over a medium heat for 15 minutes and then leave to simmer on low heat for another\
    \ hour. \r\n\r\n Save this Irish stew recipe as irish_stew_recipes. [/INST]\r\n\
    \r\nGenerated Assistant Response:\r\n{\r\n    \"function\": \"save_chat\",\r\n\
    \    \"arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\",\r\n   \
    \     \"fileDescription\": \"Irish stew recipe\",\r\n        \"fileContent\":\
    \ \"--- Irish stew recipe Irish stew is typically made using beef, onions and\
    \ potatoes. Start by frying some onions in some butter in a pan. Add in and brown\
    \ some cubes of beer - very lightly. Add salt, pepper and some herbs. Then add\
    \ the sliced potatoes on top. Simmer over a medium heat for 15 minutes and then\
    \ leave to simmer on low heat for another hour. ---\"\r\n    }\r\n}</s>\r\n\r\n\
    Correct Assistant Response:\r\n{\r\n    \"function\": \"save_chat\",\r\n    \"\
    arguments\": {\r\n        \"fileName\": \"irish_stew_recipes\",\r\n        \"\
    fileDescription\": \"Some recipes for Irish stew\",\r\n        \"fileContent\"\
    : \"--- **Irish Stew Recipe** Irish stew is typically made using beef, onions\
    \ and potatoes. Start by frying some onions in some butter in a pan. Add in and\
    \ brown some cubes of beer - very lightly. Add salt, pepper and some herbs. Then\
    \ add the sliced potatoes on top. Simmer over a medium heat for 15 minutes and\
    \ then leave to simmer on low heat for another hour. \\n\\n Save this Irish stew\
    \ recipe as irish_stew_recipes. ---\"\r\n    }\r\n}\r\n```"
  created_at: 2023-07-31 17:42:51+00:00
  edited: false
  hidden: false
  id: 64c800abca9e7f2fb6726a5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-08-10T17:44:49.000Z'
    data:
      edited: true
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438448548316956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: '<p>This looks cool, but may I ask how to use this in <a rel="nofollow"
          href="https://github.com/huggingface/chat-ui">chat-ui</a> ? I don''t know
          how to generate the specifical words like <code>[INST]</code> <code>&lt;&lt;SYS&gt;&gt;</code>,
          just copy paste the data seems not working.</p>

          '
        raw: This looks cool, but may I ask how to use this in [chat-ui](https://github.com/huggingface/chat-ui)
          ? I don't know how to generate the specifical words like `[INST]` `<<SYS>>`,
          just copy paste the data seems not working.
        updatedAt: '2023-08-10T17:45:40.988Z'
      numEdits: 1
      reactions: []
    id: 64d522119fef656cfdd8a334
    type: comment
  author: aisensiy
  content: This looks cool, but may I ask how to use this in [chat-ui](https://github.com/huggingface/chat-ui)
    ? I don't know how to generate the specifical words like `[INST]` `<<SYS>>`, just
    copy paste the data seems not working.
  created_at: 2023-08-10 16:44:49+00:00
  edited: true
  hidden: false
  id: 64d522119fef656cfdd8a334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-10T18:30:31.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8821693062782288
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;aisensiy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aisensiy\"\
          >@<span class=\"underline\">aisensiy</span></a></span>\n\n\t</span></span>.</p>\n\
          <p>I'm not familiar with chat-ui, but am willing to learn. How do you typically\
          \ run it - in a private space? What hardware do you choose? I can try to\
          \ replicate and debug.</p>\n<p>Your best bet for trying out fLlama is probably\
          \ to use <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing\"\
          >Jupyter fLlama</a>, which you can run for free in colab. Scroll down and\
          \ uncomment the function calling model. BTW, actually executing the function\
          \ calls is not yet (but will be soon) implemented in that notebook.</p>\n"
        raw: 'Howdy @aisensiy.


          I''m not familiar with chat-ui, but am willing to learn. How do you typically
          run it - in a private space? What hardware do you choose? I can try to replicate
          and debug.


          Your best bet for trying out fLlama is probably to use [Jupyter fLlama](https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing),
          which you can run for free in colab. Scroll down and uncomment the function
          calling model. BTW, actually executing the function calls is not yet (but
          will be soon) implemented in that notebook.'
        updatedAt: '2023-08-10T18:30:31.691Z'
      numEdits: 0
      reactions: []
    id: 64d52cc7c13c27a7013b7dbd
    type: comment
  author: RonanMcGovern
  content: 'Howdy @aisensiy.


    I''m not familiar with chat-ui, but am willing to learn. How do you typically
    run it - in a private space? What hardware do you choose? I can try to replicate
    and debug.


    Your best bet for trying out fLlama is probably to use [Jupyter fLlama](https://colab.research.google.com/drive/1iRNIVACiRGjvCrCn4ClAAW9-KoHj9W2z?usp=sharing),
    which you can run for free in colab. Scroll down and uncomment the function calling
    model. BTW, actually executing the function calls is not yet (but will be soon)
    implemented in that notebook.'
  created_at: 2023-08-10 17:30:31+00:00
  edited: false
  hidden: false
  id: 64d52cc7c13c27a7013b7dbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-08-11T02:46:30.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7595861554145813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I am using chat-ui with <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >tgi</a>.</p>\n<ol>\n<li>Run tgi and give the model-id=Trelis/Llama-2-7b-chat-hf-function-calling,\
          \ it will give me a llm backend</li>\n<li>Run chat-ui follow the Readme\
          \ of the repository</li>\n<li>Follow th instruction of <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/chat-ui#custom-models\">https://github.com/huggingface/chat-ui#custom-models</a>\
          \ and give a config of <code>.env.local</code> like this:</li>\n</ol>\n\
          <pre><code>  {\n    \"name\": \"ToolLLaMA-2-7b\",\n    \"datasetName\":\
          \ \"ToolBench/ToolLLaMA-2-7b\",\n    \"description\": \"A good alternative\
          \ to ChatGPT\",\n    \"endpoints\": [{\"url\": \"https://localhost:8080/generate_stream\"\
          }],\n    \"userMessageToken\": \"[INST]\",\n    \"assistantMessageToken\"\
          : \"[/INST]\",\n    \"messageEndToken\": \"&lt;/s&gt;\",\n    \"preprompt\"\
          : \"[INST]&lt;&lt;SYS&gt;&gt;\\nYou are a helpful, respectful and honest\
          \ assistant. Always answer as helpfully as possible, while being safe. Your\
          \ answers should not include any harmful, unethical, racist, sexist, toxic,\
          \ dangerous, or illegal content. Please ensure that your responses are socially\
          \ unbiased and positive in nature.\\n\\nIf a question does not make any\
          \ sense, or is not factually coherent, explain why instead of answering\
          \ something not correct. If you don't know the answer to a question, please\
          \ don't share false information.&lt;&lt;/SYS&gt;&gt;\\n\\n[/INST]\",\n \
          \   \"promptExamples\": [\n      {\n        \"title\": \"Write an email\
          \ from bullet list\",\n        \"prompt\": \"As a restaurant owner, write\
          \ a professional email to the supplier to get these products every week:\
          \ \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread (x12)\"\n      }, {\n    \
          \    \"title\": \"Code a snake game\",\n        \"prompt\": \"Code a basic\
          \ snake game in python, give explanations for each step.\"\n      }, {\n\
          \        \"title\": \"Assist in a task\",\n        \"prompt\": \"How do\
          \ I make a delicious lemon cheesecake?\"\n      }\n    ],\n    \"parameters\"\
          : {\n      \"temperature\": 0.3,\n      \"top_p\": 0.95,\n      \"repetition_penalty\"\
          : 1.2,\n      \"top_k\": 50,\n      \"truncate\": 2000,\n      \"max_new_tokens\"\
          : 2048\n    }\n  },\n</code></pre>\n"
        raw: "@RonanMcGovern \n\nI am using chat-ui with [tgi](https://github.com/huggingface/text-generation-inference).\n\
          \n1. Run tgi and give the model-id=Trelis/Llama-2-7b-chat-hf-function-calling,\
          \ it will give me a llm backend\n2. Run chat-ui follow the Readme of the\
          \ repository\n3. Follow th instruction of https://github.com/huggingface/chat-ui#custom-models\
          \ and give a config of `.env.local` like this:\n\n```\n  {\n    \"name\"\
          : \"ToolLLaMA-2-7b\",\n    \"datasetName\": \"ToolBench/ToolLLaMA-2-7b\"\
          ,\n    \"description\": \"A good alternative to ChatGPT\",\n    \"endpoints\"\
          : [{\"url\": \"https://localhost:8080/generate_stream\"}],\n    \"userMessageToken\"\
          : \"[INST]\",\n    \"assistantMessageToken\": \"[/INST]\",\n    \"messageEndToken\"\
          : \"</s>\",\n    \"preprompt\": \"[INST]<<SYS>>\\nYou are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe. Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature.\\n\\nIf a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.<</SYS>>\\n\\n[/INST]\"\
          ,\n    \"promptExamples\": [\n      {\n        \"title\": \"Write an email\
          \ from bullet list\",\n        \"prompt\": \"As a restaurant owner, write\
          \ a professional email to the supplier to get these products every week:\
          \ \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread (x12)\"\n      }, {\n    \
          \    \"title\": \"Code a snake game\",\n        \"prompt\": \"Code a basic\
          \ snake game in python, give explanations for each step.\"\n      }, {\n\
          \        \"title\": \"Assist in a task\",\n        \"prompt\": \"How do\
          \ I make a delicious lemon cheesecake?\"\n      }\n    ],\n    \"parameters\"\
          : {\n      \"temperature\": 0.3,\n      \"top_p\": 0.95,\n      \"repetition_penalty\"\
          : 1.2,\n      \"top_k\": 50,\n      \"truncate\": 2000,\n      \"max_new_tokens\"\
          : 2048\n    }\n  },\n```"
        updatedAt: '2023-08-11T02:46:30.301Z'
      numEdits: 0
      reactions: []
    id: 64d5a1061a81ece17d720ba0
    type: comment
  author: aisensiy
  content: "@RonanMcGovern \n\nI am using chat-ui with [tgi](https://github.com/huggingface/text-generation-inference).\n\
    \n1. Run tgi and give the model-id=Trelis/Llama-2-7b-chat-hf-function-calling,\
    \ it will give me a llm backend\n2. Run chat-ui follow the Readme of the repository\n\
    3. Follow th instruction of https://github.com/huggingface/chat-ui#custom-models\
    \ and give a config of `.env.local` like this:\n\n```\n  {\n    \"name\": \"ToolLLaMA-2-7b\"\
    ,\n    \"datasetName\": \"ToolBench/ToolLLaMA-2-7b\",\n    \"description\": \"\
    A good alternative to ChatGPT\",\n    \"endpoints\": [{\"url\": \"https://localhost:8080/generate_stream\"\
    }],\n    \"userMessageToken\": \"[INST]\",\n    \"assistantMessageToken\": \"\
    [/INST]\",\n    \"messageEndToken\": \"</s>\",\n    \"preprompt\": \"[INST]<<SYS>>\\\
    nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe. Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature.\\n\\nIf a\
    \ question does not make any sense, or is not factually coherent, explain why\
    \ instead of answering something not correct. If you don't know the answer to\
    \ a question, please don't share false information.<</SYS>>\\n\\n[/INST]\",\n\
    \    \"promptExamples\": [\n      {\n        \"title\": \"Write an email from\
    \ bullet list\",\n        \"prompt\": \"As a restaurant owner, write a professional\
    \ email to the supplier to get these products every week: \\n\\n- Wine (x10)\\\
    n- Eggs (x24)\\n- Bread (x12)\"\n      }, {\n        \"title\": \"Code a snake\
    \ game\",\n        \"prompt\": \"Code a basic snake game in python, give explanations\
    \ for each step.\"\n      }, {\n        \"title\": \"Assist in a task\",\n   \
    \     \"prompt\": \"How do I make a delicious lemon cheesecake?\"\n      }\n \
    \   ],\n    \"parameters\": {\n      \"temperature\": 0.3,\n      \"top_p\": 0.95,\n\
    \      \"repetition_penalty\": 1.2,\n      \"top_k\": 50,\n      \"truncate\"\
    : 2000,\n      \"max_new_tokens\": 2048\n    }\n  },\n```"
  created_at: 2023-08-11 01:46:30+00:00
  edited: false
  hidden: false
  id: 64d5a1061a81ece17d720ba0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-08-11T03:32:22.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.769363522529602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: "<p>BTW, just paste prompt:</p>\n<pre><code>[INST] &lt;&lt;SYS&gt;&gt;\n\
          You are a helpful research assistant. The following functions are available\
          \ for you to fetch further data to answer user questions, if relevant:\n\
          \n{\n    \"function\": \"delete_file\",\n    \"description\": \"This function\
          \ provides the ability to delete files in a user's folder. It doesn't delete\
          \ 'files.json'. It updates 'files.json' to remove entries of deleted files.\"\
          ,\n    \"arguments\": [\n        {\n            \"name\": \"fileNames\"\
          ,\n            \"type\": \"array\",\n            \"items\": {\n        \
          \        \"type\": \"string\"\n            },\n            \"description\"\
          : \"An array of file names to be deleted\"\n        }\n    ]\n}\n\n{\n \
          \   \"function\": \"save_chat\",\n    \"description\": \"This function saves\
          \ content from current chat to a user's files. Use this function if the\
          \ user asks to save this chat or content from this chat. If the user doesn't\
          \ specify a name or description, use the content of the chat to determine\
          \ a suitable chat name and description.\",\n    \"arguments\": [\n     \
          \   {\n            \"name\": \"fileName\",\n            \"type\": \"string\"\
          ,\n            \"description\": \"A suitable name for the file where the\
          \ chat history will be saved.\"\n        },\n        {\n            \"name\"\
          : \"fileDescription\",\n            \"type\": \"string\",\n            \"\
          description\": \"A brief description of the chat history\"\n        },\n\
          \        {\n            \"name\": \"fileContent\",\n            \"type\"\
          : \"string\",\n            \"description\": \"Content to save. Format the\
          \ content in markdown. Do not include system messages\"\n        }\n   \
          \ ]\n}\n                \n\nTo call a function, respond - immediately and\
          \ only - with a JSON object of the following format:\n{\n    \"function\"\
          : \"function_name\",\n    \"arguments\": {\n        \"argument1\": value1,\n\
          \        \"argument2\": value2\n    }\n}\n&lt;&lt;/SYS&gt;&gt;\n\nIrish\
          \ stew is typically made using beef, onions and potatoes. Start by frying\
          \ some onions in some butter in a pan. Add in and brown some cubes of beer\
          \ - very lightly. Add salt, pepper and some herbs. Then add the sliced potatoes\
          \ on top. Simmer over a medium heat for 15 minutes and then leave to simmer\
          \ on low heat for another hour. \n\n Save this Irish stew recipe as irish_stew_recipes.\
          \ [/INST]\n</code></pre>\n<p>is not working in <code>Trelis/Llama-2-7b-chat-hf-function-calling</code>\
          \ model. But if I paste it for <code>meta-llama/Llama-2-70b-chat-hf</code>\
          \ model, the result looks like this:</p>\n<pre><code>{\n\"function\": \"\
          save_chat\",\n\"arguments\": {\n\"fileName\": \"irish_stew_recipes\",\n\"\
          fileDescription\": \"A simple recipe for traditional Irish stew\",\n\"fileContent\"\
          : \"Irish stew is typically made using beef, onions, and potatoes.\\nStart\
          \ by frying some onions in some butter in a pan. Add in and brown some cubes\
          \ of beef - very lightly. Add salt, pepper, and some herbs. Then add the\
          \ sliced potatoes on top. Simmer over a medium heat for 15 minutes and then\
          \ leave to simmer on low heat for another hour.\"\n}\n}\n</code></pre>\n\
          <p>Sooo fancy...</p>\n"
        raw: "BTW, just paste prompt:\n\n```\n[INST] <<SYS>>\nYou are a helpful research\
          \ assistant. The following functions are available for you to fetch further\
          \ data to answer user questions, if relevant:\n\n{\n    \"function\": \"\
          delete_file\",\n    \"description\": \"This function provides the ability\
          \ to delete files in a user's folder. It doesn't delete 'files.json'. It\
          \ updates 'files.json' to remove entries of deleted files.\",\n    \"arguments\"\
          : [\n        {\n            \"name\": \"fileNames\",\n            \"type\"\
          : \"array\",\n            \"items\": {\n                \"type\": \"string\"\
          \n            },\n            \"description\": \"An array of file names\
          \ to be deleted\"\n        }\n    ]\n}\n\n{\n    \"function\": \"save_chat\"\
          ,\n    \"description\": \"This function saves content from current chat\
          \ to a user's files. Use this function if the user asks to save this chat\
          \ or content from this chat. If the user doesn't specify a name or description,\
          \ use the content of the chat to determine a suitable chat name and description.\"\
          ,\n    \"arguments\": [\n        {\n            \"name\": \"fileName\",\n\
          \            \"type\": \"string\",\n            \"description\": \"A suitable\
          \ name for the file where the chat history will be saved.\"\n        },\n\
          \        {\n            \"name\": \"fileDescription\",\n            \"type\"\
          : \"string\",\n            \"description\": \"A brief description of the\
          \ chat history\"\n        },\n        {\n            \"name\": \"fileContent\"\
          ,\n            \"type\": \"string\",\n            \"description\": \"Content\
          \ to save. Format the content in markdown. Do not include system messages\"\
          \n        }\n    ]\n}\n                \n\nTo call a function, respond -\
          \ immediately and only - with a JSON object of the following format:\n{\n\
          \    \"function\": \"function_name\",\n    \"arguments\": {\n        \"\
          argument1\": value1,\n        \"argument2\": value2\n    }\n}\n<</SYS>>\n\
          \nIrish stew is typically made using beef, onions and potatoes. Start by\
          \ frying some onions in some butter in a pan. Add in and brown some cubes\
          \ of beer - very lightly. Add salt, pepper and some herbs. Then add the\
          \ sliced potatoes on top. Simmer over a medium heat for 15 minutes and then\
          \ leave to simmer on low heat for another hour. \n\n Save this Irish stew\
          \ recipe as irish_stew_recipes. [/INST]\n```\n\nis not working in `Trelis/Llama-2-7b-chat-hf-function-calling`\
          \ model. But if I paste it for `meta-llama/Llama-2-70b-chat-hf` model, the\
          \ result looks like this:\n\n```\n{\n\"function\": \"save_chat\",\n\"arguments\"\
          : {\n\"fileName\": \"irish_stew_recipes\",\n\"fileDescription\": \"A simple\
          \ recipe for traditional Irish stew\",\n\"fileContent\": \"Irish stew is\
          \ typically made using beef, onions, and potatoes.\\nStart by frying some\
          \ onions in some butter in a pan. Add in and brown some cubes of beef -\
          \ very lightly. Add salt, pepper, and some herbs. Then add the sliced potatoes\
          \ on top. Simmer over a medium heat for 15 minutes and then leave to simmer\
          \ on low heat for another hour.\"\n}\n}\n```\n\nSooo fancy..."
        updatedAt: '2023-08-11T03:32:22.730Z'
      numEdits: 0
      reactions: []
    id: 64d5abc6c2eedf9af808f132
    type: comment
  author: aisensiy
  content: "BTW, just paste prompt:\n\n```\n[INST] <<SYS>>\nYou are a helpful research\
    \ assistant. The following functions are available for you to fetch further data\
    \ to answer user questions, if relevant:\n\n{\n    \"function\": \"delete_file\"\
    ,\n    \"description\": \"This function provides the ability to delete files in\
    \ a user's folder. It doesn't delete 'files.json'. It updates 'files.json' to\
    \ remove entries of deleted files.\",\n    \"arguments\": [\n        {\n     \
    \       \"name\": \"fileNames\",\n            \"type\": \"array\",\n         \
    \   \"items\": {\n                \"type\": \"string\"\n            },\n     \
    \       \"description\": \"An array of file names to be deleted\"\n        }\n\
    \    ]\n}\n\n{\n    \"function\": \"save_chat\",\n    \"description\": \"This\
    \ function saves content from current chat to a user's files. Use this function\
    \ if the user asks to save this chat or content from this chat. If the user doesn't\
    \ specify a name or description, use the content of the chat to determine a suitable\
    \ chat name and description.\",\n    \"arguments\": [\n        {\n           \
    \ \"name\": \"fileName\",\n            \"type\": \"string\",\n            \"description\"\
    : \"A suitable name for the file where the chat history will be saved.\"\n   \
    \     },\n        {\n            \"name\": \"fileDescription\",\n            \"\
    type\": \"string\",\n            \"description\": \"A brief description of the\
    \ chat history\"\n        },\n        {\n            \"name\": \"fileContent\"\
    ,\n            \"type\": \"string\",\n            \"description\": \"Content to\
    \ save. Format the content in markdown. Do not include system messages\"\n   \
    \     }\n    ]\n}\n                \n\nTo call a function, respond - immediately\
    \ and only - with a JSON object of the following format:\n{\n    \"function\"\
    : \"function_name\",\n    \"arguments\": {\n        \"argument1\": value1,\n \
    \       \"argument2\": value2\n    }\n}\n<</SYS>>\n\nIrish stew is typically made\
    \ using beef, onions and potatoes. Start by frying some onions in some butter\
    \ in a pan. Add in and brown some cubes of beer - very lightly. Add salt, pepper\
    \ and some herbs. Then add the sliced potatoes on top. Simmer over a medium heat\
    \ for 15 minutes and then leave to simmer on low heat for another hour. \n\n Save\
    \ this Irish stew recipe as irish_stew_recipes. [/INST]\n```\n\nis not working\
    \ in `Trelis/Llama-2-7b-chat-hf-function-calling` model. But if I paste it for\
    \ `meta-llama/Llama-2-70b-chat-hf` model, the result looks like this:\n\n```\n\
    {\n\"function\": \"save_chat\",\n\"arguments\": {\n\"fileName\": \"irish_stew_recipes\"\
    ,\n\"fileDescription\": \"A simple recipe for traditional Irish stew\",\n\"fileContent\"\
    : \"Irish stew is typically made using beef, onions, and potatoes.\\nStart by\
    \ frying some onions in some butter in a pan. Add in and brown some cubes of beef\
    \ - very lightly. Add salt, pepper, and some herbs. Then add the sliced potatoes\
    \ on top. Simmer over a medium heat for 15 minutes and then leave to simmer on\
    \ low heat for another hour.\"\n}\n}\n```\n\nSooo fancy..."
  created_at: 2023-08-11 02:32:22+00:00
  edited: false
  hidden: false
  id: 64d5abc6c2eedf9af808f132
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-11T12:53:35.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210379719734192
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy, thanks - really interesting how Llama-70B can just zero shot
          this with good prompting.</p>

          <p>A few things:</p>

          <ol>

          <li>Google Colab:</li>

          </ol>

          <ul>

          <li>I''ve got Bing search working now, you can try it out <a rel="nofollow"
          href="https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing">here</a>.
          Let me know any feedback.</li>

          </ul>

          <ol start="2">

          <li>tgi + chatui:</li>

          </ol>

          <ul>

          <li>I don''t have a GPU myself and tgi doesn''t work with ggml - so I can''t
          run on my Mac.</li>

          <li>I''ve requested GPU access from Azure and AWS so that I can spin up
          a server to replicate what you''re seeing.</li>

          </ul>

          <ol start="3">

          <li>Prompting - I reviewed your code above. Some slight suggestions....</li>

          </ol>

          <ul>

          <li>There''s a quirk whereby Llama-2-chat expects that the first user input
          be wrapped together within the system message. This is hard to do with the
          model syntax provided. I need to think more about whether there is a hack
          to get around this limitation.</li>

          <li>The system message should be in the preprompt. Try putting the system
          prompt in the preprompt rather than the prompt. Then, put the short user
          message asking for the file in the prompt.</li>

          </ul>

          <p>I''m not sure if that is the issue with 7B but it may help.</p>

          '
        raw: 'Howdy, thanks - really interesting how Llama-70B can just zero shot
          this with good prompting.


          A few things:

          1. Google Colab:

          - I''ve got Bing search working now, you can try it out [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing).
          Let me know any feedback.

          2. tgi + chatui:

          - I don''t have a GPU myself and tgi doesn''t work with ggml - so I can''t
          run on my Mac.

          - I''ve requested GPU access from Azure and AWS so that I can spin up a
          server to replicate what you''re seeing.

          3. Prompting - I reviewed your code above. Some slight suggestions....


          - There''s a quirk whereby Llama-2-chat expects that the first user input
          be wrapped together within the system message. This is hard to do with the
          model syntax provided. I need to think more about whether there is a hack
          to get around this limitation.

          - The system message should be in the preprompt. Try putting the system
          prompt in the preprompt rather than the prompt. Then, put the short user
          message asking for the file in the prompt.


          I''m not sure if that is the issue with 7B but it may help.


          '
        updatedAt: '2023-08-11T12:53:35.559Z'
      numEdits: 0
      reactions: []
    id: 64d62f4f8f84a7738d7df18b
    type: comment
  author: RonanMcGovern
  content: 'Howdy, thanks - really interesting how Llama-70B can just zero shot this
    with good prompting.


    A few things:

    1. Google Colab:

    - I''ve got Bing search working now, you can try it out [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing).
    Let me know any feedback.

    2. tgi + chatui:

    - I don''t have a GPU myself and tgi doesn''t work with ggml - so I can''t run
    on my Mac.

    - I''ve requested GPU access from Azure and AWS so that I can spin up a server
    to replicate what you''re seeing.

    3. Prompting - I reviewed your code above. Some slight suggestions....


    - There''s a quirk whereby Llama-2-chat expects that the first user input be wrapped
    together within the system message. This is hard to do with the model syntax provided.
    I need to think more about whether there is a hack to get around this limitation.

    - The system message should be in the preprompt. Try putting the system prompt
    in the preprompt rather than the prompt. Then, put the short user message asking
    for the file in the prompt.


    I''m not sure if that is the issue with 7B but it may help.


    '
  created_at: 2023-08-11 11:53:35+00:00
  edited: false
  hidden: false
  id: 64d62f4f8f84a7738d7df18b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-08-14T06:15:24.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9285051822662354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: '<blockquote>

          <p>Howdy, thanks - really interesting how Llama-70B can just zero shot this
          with good prompting.</p>

          <p>A few things:</p>

          <ol>

          <li>Google Colab:</li>

          </ol>

          <ul>

          <li>I''ve got Bing search working now, you can try it out <a rel="nofollow"
          href="https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing">here</a>.
          Let me know any feedback.</li>

          </ul>

          <ol start="2">

          <li>tgi + chatui:</li>

          </ol>

          <ul>

          <li>I don''t have a GPU myself and tgi doesn''t work with ggml - so I can''t
          run on my Mac.</li>

          <li>I''ve requested GPU access from Azure and AWS so that I can spin up
          a server to replicate what you''re seeing.</li>

          </ul>

          <ol start="3">

          <li>Prompting - I reviewed your code above. Some slight suggestions....</li>

          </ol>

          <ul>

          <li>There''s a quirk whereby Llama-2-chat expects that the first user input
          be wrapped together within the system message. This is hard to do with the
          model syntax provided. I need to think more about whether there is a hack
          to get around this limitation.</li>

          <li>The system message should be in the preprompt. Try putting the system
          prompt in the preprompt rather than the prompt. Then, put the short user
          message asking for the file in the prompt.</li>

          </ul>

          <p>I''m not sure if that is the issue with 7B but it may help.</p>

          </blockquote>

          <p>What kind of gpu would you like to use, I hava a A6000(48GB memory) from
          a public cloud with a jupyterlab as the entrypoint (no private data in it)
          and I can give a temp access for you...But just a temp access for about
          1 week...I am not sure if it is useful for you.</p>

          '
        raw: "> Howdy, thanks - really interesting how Llama-70B can just zero shot\
          \ this with good prompting.\n> \n> A few things:\n> 1. Google Colab:\n>\
          \ - I've got Bing search working now, you can try it out [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing).\
          \ Let me know any feedback.\n> 2. tgi + chatui:\n> - I don't have a GPU\
          \ myself and tgi doesn't work with ggml - so I can't run on my Mac.\n> -\
          \ I've requested GPU access from Azure and AWS so that I can spin up a server\
          \ to replicate what you're seeing.\n> 3. Prompting - I reviewed your code\
          \ above. Some slight suggestions....\n> \n> - There's a quirk whereby Llama-2-chat\
          \ expects that the first user input be wrapped together within the system\
          \ message. This is hard to do with the model syntax provided. I need to\
          \ think more about whether there is a hack to get around this limitation.\n\
          > - The system message should be in the preprompt. Try putting the system\
          \ prompt in the preprompt rather than the prompt. Then, put the short user\
          \ message asking for the file in the prompt.\n> \n> I'm not sure if that\
          \ is the issue with 7B but it may help.\n\nWhat kind of gpu would you like\
          \ to use, I hava a A6000(48GB memory) from a public cloud with a jupyterlab\
          \ as the entrypoint (no private data in it) and I can give a temp access\
          \ for you...But just a temp access for about 1 week...I am not sure if it\
          \ is useful for you."
        updatedAt: '2023-08-14T06:15:24.914Z'
      numEdits: 0
      reactions: []
    id: 64d9c67cabf475a8080903ed
    type: comment
  author: aisensiy
  content: "> Howdy, thanks - really interesting how Llama-70B can just zero shot\
    \ this with good prompting.\n> \n> A few things:\n> 1. Google Colab:\n> - I've\
    \ got Bing search working now, you can try it out [here](https://colab.research.google.com/drive/1u8x41Jx8WWtI-nzHOgqTxkS3Q_lcjaSX?usp=sharing).\
    \ Let me know any feedback.\n> 2. tgi + chatui:\n> - I don't have a GPU myself\
    \ and tgi doesn't work with ggml - so I can't run on my Mac.\n> - I've requested\
    \ GPU access from Azure and AWS so that I can spin up a server to replicate what\
    \ you're seeing.\n> 3. Prompting - I reviewed your code above. Some slight suggestions....\n\
    > \n> - There's a quirk whereby Llama-2-chat expects that the first user input\
    \ be wrapped together within the system message. This is hard to do with the model\
    \ syntax provided. I need to think more about whether there is a hack to get around\
    \ this limitation.\n> - The system message should be in the preprompt. Try putting\
    \ the system prompt in the preprompt rather than the prompt. Then, put the short\
    \ user message asking for the file in the prompt.\n> \n> I'm not sure if that\
    \ is the issue with 7B but it may help.\n\nWhat kind of gpu would you like to\
    \ use, I hava a A6000(48GB memory) from a public cloud with a jupyterlab as the\
    \ entrypoint (no private data in it) and I can give a temp access for you...But\
    \ just a temp access for about 1 week...I am not sure if it is useful for you."
  created_at: 2023-08-14 05:15:24+00:00
  edited: false
  hidden: false
  id: 64d9c67cabf475a8080903ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-14T20:48:58.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9683915972709656
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Thanks! I appreciate that. I should get amazon or azure access soon
          to a gpu, so I''ll write back here once I''ve got tgi and chat-ui running.</p>

          '
        raw: Thanks! I appreciate that. I should get amazon or azure access soon to
          a gpu, so I'll write back here once I've got tgi and chat-ui running.
        updatedAt: '2023-08-14T20:48:58.592Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aisensiy
    id: 64da933ac307ee5369c731a2
    type: comment
  author: RonanMcGovern
  content: Thanks! I appreciate that. I should get amazon or azure access soon to
    a gpu, so I'll write back here once I've got tgi and chat-ui running.
  created_at: 2023-08-14 19:48:58+00:00
  edited: false
  hidden: false
  id: 64da933ac307ee5369c731a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-22T13:48:43.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6466667652130127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;aisensiy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aisensiy\"\
          >@<span class=\"underline\">aisensiy</span></a></span>\n\n\t</span></span>\
          \ , I just got up and running with Chat-ui.</p>\n<p>Here is my .env.local\
          \ MODELS config:</p>\n<pre><code># 'name', 'userMessageToken', 'assistantMessageToken'\
          \ are required\nMODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
          ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n    \
          \    \"description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\"\
          : \"https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\"\
          ,\n        \"assistantMessageToken\": \"[/INST]\",\n        \"messageEndToken\"\
          : \"&lt;/s&gt;\",\n        \"preprompt\": \"[INST]&lt;&lt;SYS&gt;&gt;\\\
          nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe. Your answers should not include any harmful,\
          \ unethical, &gt;\n        \"parameters\": {\n                \"temperature\"\
          : 0.01,\n                \"top_p\": 0.95,\n                \"repetition_penalty\"\
          : 1.2,\n                \"top_k\": 50,\n                \"truncate\": 1000,\n\
          \                \"max_new_tokens\": 1024\n        },\n        \"endpoints\"\
          : [{\n                \"url\": \"http://127.0.0.1:8080\"\n        }]\n}\n\
          ]`\n</code></pre>\n<p>and here's what that gave:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/cmZUeagIkHLAfOaE5kswk.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/cmZUeagIkHLAfOaE5kswk.png\"\
          ></a></p>\n<p>I then tried providing no preprompt (which actually would\
          \ allow for a syntactically correct way of prompting Llama because both\
          \ the system message and first user message will then together be wrapped\
          \ in INST tokens):</p>\n<pre><code># 'name', 'userMessageToken', 'assistantMessageToken'\
          \ are required\nMODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
          ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n    \
          \    \"description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\"\
          : \"https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\"\
          ,\n        \"assistantMessageToken\": \"[/INST]\",\n        \"parameters\"\
          : {\n                \"temperature\": 0.01,\n                \"top_p\":\
          \ 0.95,\n                \"repetition_penalty\": 1.2,\n                \"\
          top_k\": 50,\n                \"truncate\": 1000,\n                \"max_new_tokens\"\
          : 1024\n        },\n        \"endpoints\": [{\n                \"url\":\
          \ \"http://127.0.0.1:8080\"\n        }]\n}\n]`\n</code></pre>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/zLIGB-zufsJ6sSL5KxG_m.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/zLIGB-zufsJ6sSL5KxG_m.png\"\
          ></a></p>\n<p>Oddly, it's as though there is no preprompt being fed in.\
          \ I tested out various preprompts and I can't seem to affect the model.\
          \ I'm probably missing something...</p>\n<p>BTW, the raw model (not via\
          \ chat ui) responds correctly:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/kvXbYW3XozF1Hd6rz4xsY.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/kvXbYW3XozF1Hd6rz4xsY.png\"\
          ></a></p>\n"
        raw: "Howdy @aisensiy , I just got up and running with Chat-ui.\n\nHere is\
          \ my .env.local MODELS config:\n```\n# 'name', 'userMessageToken', 'assistantMessageToken'\
          \ are required\nMODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
          ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n    \
          \    \"description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\"\
          : \"https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\"\
          ,\n        \"assistantMessageToken\": \"[/INST]\",\n        \"messageEndToken\"\
          : \"</s>\",\n        \"preprompt\": \"[INST]<<SYS>>\\nYou are a helpful,\
          \ respectful and honest assistant. Always answer as helpfully as possible,\
          \ while being safe. Your answers should not include any harmful, unethical,\
          \ >\n        \"parameters\": {\n                \"temperature\": 0.01,\n\
          \                \"top_p\": 0.95,\n                \"repetition_penalty\"\
          : 1.2,\n                \"top_k\": 50,\n                \"truncate\": 1000,\n\
          \                \"max_new_tokens\": 1024\n        },\n        \"endpoints\"\
          : [{\n                \"url\": \"http://127.0.0.1:8080\"\n        }]\n}\n\
          ]`\n```\nand here's what that gave:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/cmZUeagIkHLAfOaE5kswk.png)\n\
          \nI then tried providing no preprompt (which actually would allow for a\
          \ syntactically correct way of prompting Llama because both the system message\
          \ and first user message will then together be wrapped in INST tokens):\n\
          ```\n# 'name', 'userMessageToken', 'assistantMessageToken' are required\n\
          MODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
          ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n    \
          \    \"description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\"\
          : \"https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\"\
          ,\n        \"assistantMessageToken\": \"[/INST]\",\n        \"parameters\"\
          : {\n                \"temperature\": 0.01,\n                \"top_p\":\
          \ 0.95,\n                \"repetition_penalty\": 1.2,\n                \"\
          top_k\": 50,\n                \"truncate\": 1000,\n                \"max_new_tokens\"\
          : 1024\n        },\n        \"endpoints\": [{\n                \"url\":\
          \ \"http://127.0.0.1:8080\"\n        }]\n}\n]`\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/zLIGB-zufsJ6sSL5KxG_m.png)\n\
          \nOddly, it's as though there is no preprompt being fed in. I tested out\
          \ various preprompts and I can't seem to affect the model. I'm probably\
          \ missing something...\n\nBTW, the raw model (not via chat ui) responds\
          \ correctly:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/kvXbYW3XozF1Hd6rz4xsY.png)\n\
          \n"
        updatedAt: '2023-08-22T13:48:43.174Z'
      numEdits: 0
      reactions: []
    id: 64e4bcbbb879a1ef9c9b559e
    type: comment
  author: RonanMcGovern
  content: "Howdy @aisensiy , I just got up and running with Chat-ui.\n\nHere is my\
    \ .env.local MODELS config:\n```\n# 'name', 'userMessageToken', 'assistantMessageToken'\
    \ are required\nMODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
    ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n        \"\
    description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\": \"\
    https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\",\n  \
    \      \"assistantMessageToken\": \"[/INST]\",\n        \"messageEndToken\": \"\
    </s>\",\n        \"preprompt\": \"[INST]<<SYS>>\\nYou are a helpful, respectful\
    \ and honest assistant. Always answer as helpfully as possible, while being safe.\
    \ Your answers should not include any harmful, unethical, >\n        \"parameters\"\
    : {\n                \"temperature\": 0.01,\n                \"top_p\": 0.95,\n\
    \                \"repetition_penalty\": 1.2,\n                \"top_k\": 50,\n\
    \                \"truncate\": 1000,\n                \"max_new_tokens\": 1024\n\
    \        },\n        \"endpoints\": [{\n                \"url\": \"http://127.0.0.1:8080\"\
    \n        }]\n}\n]`\n```\nand here's what that gave:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/cmZUeagIkHLAfOaE5kswk.png)\n\
    \nI then tried providing no preprompt (which actually would allow for a syntactically\
    \ correct way of prompting Llama because both the system message and first user\
    \ message will then together be wrapped in INST tokens):\n```\n# 'name', 'userMessageToken',\
    \ 'assistantMessageToken' are required\nMODELS=`[\n{\n        \"name\": \"Trelis/Llama-2-7b-chat-hf-function-calling\"\
    ,\n        \"datasetName\": \"Trelis/function_calling_extended\",\n        \"\
    description\": \"function calling Llama-7B-chat\",\n        \"websiteUrl\": \"\
    https://research.Trelis.com\",\n        \"userMessageToken\": \"[INST]\",\n  \
    \      \"assistantMessageToken\": \"[/INST]\",\n        \"parameters\": {\n  \
    \              \"temperature\": 0.01,\n                \"top_p\": 0.95,\n    \
    \            \"repetition_penalty\": 1.2,\n                \"top_k\": 50,\n  \
    \              \"truncate\": 1000,\n                \"max_new_tokens\": 1024\n\
    \        },\n        \"endpoints\": [{\n                \"url\": \"http://127.0.0.1:8080\"\
    \n        }]\n}\n]`\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/zLIGB-zufsJ6sSL5KxG_m.png)\n\
    \nOddly, it's as though there is no preprompt being fed in. I tested out various\
    \ preprompts and I can't seem to affect the model. I'm probably missing something...\n\
    \nBTW, the raw model (not via chat ui) responds correctly:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647479a17d131daf633c108d/kvXbYW3XozF1Hd6rz4xsY.png)\n\
    \n"
  created_at: 2023-08-22 12:48:43+00:00
  edited: false
  hidden: false
  id: 64e4bcbbb879a1ef9c9b559e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-08-23T04:16:09.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9753445386886597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: '<p>I have the same issue...</p>

          '
        raw: I have the same issue...
        updatedAt: '2023-08-23T04:16:09.572Z'
      numEdits: 0
      reactions: []
    id: 64e58809240e70531c00c18e
    type: comment
  author: aisensiy
  content: I have the same issue...
  created_at: 2023-08-23 03:16:09+00:00
  edited: false
  hidden: false
  id: 64e58809240e70531c00c18e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-23T09:15:45.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8398891687393188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Ok, thanks, I''ve posted an issue here: <a rel="nofollow" href="https://github.com/huggingface/chat-ui/issues/412">https://github.com/huggingface/chat-ui/issues/412</a></p>

          '
        raw: 'Ok, thanks, I''ve posted an issue here: https://github.com/huggingface/chat-ui/issues/412'
        updatedAt: '2023-08-23T09:15:45.708Z'
      numEdits: 0
      reactions: []
    id: 64e5ce4165cb558c1fbb49d5
    type: comment
  author: RonanMcGovern
  content: 'Ok, thanks, I''ve posted an issue here: https://github.com/huggingface/chat-ui/issues/412'
  created_at: 2023-08-23 08:15:45+00:00
  edited: false
  hidden: false
  id: 64e5ce4165cb558c1fbb49d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-25T09:11:14.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441392421722412
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>BTW, the above issue has been cleared <span data-props=\"{&quot;user&quot;:&quot;aisensiy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aisensiy\"\
          >@<span class=\"underline\">aisensiy</span></a></span>\n\n\t</span></span>\
          \ . The way to prompt with chat-ui is to set up a customPromptTemplate.\
          \ See above.</p>\n<p>However, there remains an issue with how input is being\
          \ fed to tgi and/or how tgi is tokenizing. New issue <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/text-generation-inference/issues/915\"\
          >here</a></p>\n"
        raw: 'BTW, the above issue has been cleared @aisensiy . The way to prompt
          with chat-ui is to set up a customPromptTemplate. See above.


          However, there remains an issue with how input is being fed to tgi and/or
          how tgi is tokenizing. New issue [here](https://github.com/huggingface/text-generation-inference/issues/915)'
        updatedAt: '2023-08-25T09:11:32.584Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aisensiy
    id: 64e870322292cd13a0b0a3c8
    type: comment
  author: RonanMcGovern
  content: 'BTW, the above issue has been cleared @aisensiy . The way to prompt with
    chat-ui is to set up a customPromptTemplate. See above.


    However, there remains an issue with how input is being fed to tgi and/or how
    tgi is tokenizing. New issue [here](https://github.com/huggingface/text-generation-inference/issues/915)'
  created_at: 2023-08-25 08:11:14+00:00
  edited: true
  hidden: false
  id: 64e870322292cd13a0b0a3c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-29T16:58:40.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8301148414611816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Ok <span data-props=\"{&quot;user&quot;:&quot;aisensiy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/aisensiy\"\
          >@<span class=\"underline\">aisensiy</span></a></span>\n\n\t</span></span>\
          \ I've now resolved this issue.</p>\n<p>Here is the <a rel=\"nofollow\"\
          \ href=\"https://github.com/TrelisResearch/tgi-chat-ui-function-calling\"\
          >github for setup</a></p>\n<p>And here is a video showing it working with\
          \ <a href=\"https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2\"\
          >llama-2-7b-chat-hf-function-calling-v2</a> (note that we've now moved to\
          \ v2)</p>\n"
        raw: 'Ok @aisensiy I''ve now resolved this issue.


          Here is the [github for setup](https://github.com/TrelisResearch/tgi-chat-ui-function-calling)


          And here is a video showing it working with [llama-2-7b-chat-hf-function-calling-v2](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2)
          (note that we''ve now moved to v2)'
        updatedAt: '2023-08-29T16:58:52.798Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aisensiy
      relatedEventId: 64ee23c014ac14ba622cd8be
    id: 64ee23c014ac14ba622cd8bc
    type: comment
  author: RonanMcGovern
  content: 'Ok @aisensiy I''ve now resolved this issue.


    Here is the [github for setup](https://github.com/TrelisResearch/tgi-chat-ui-function-calling)


    And here is a video showing it working with [llama-2-7b-chat-hf-function-calling-v2](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2)
    (note that we''ve now moved to v2)'
  created_at: 2023-08-29 15:58:40+00:00
  edited: true
  hidden: false
  id: 64ee23c014ac14ba622cd8bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-29T16:58:40.000Z'
    data:
      status: closed
    id: 64ee23c014ac14ba622cd8be
    type: status-change
  author: RonanMcGovern
  created_at: 2023-08-29 15:58:40+00:00
  id: 64ee23c014ac14ba622cd8be
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
      fullname: aisensiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aisensiy
      type: user
    createdAt: '2023-09-01T06:26:57.000Z'
    data:
      edited: false
      editors:
      - aisensiy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9670191407203674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9996cfcbfdc5bc1d57fe42ade6ca7978.svg
          fullname: aisensiy
          isHf: false
          isPro: false
          name: aisensiy
          type: user
        html: '<p>That is so great. I will try it again. Thanks !</p>

          '
        raw: That is so great. I will try it again. Thanks !
        updatedAt: '2023-09-01T06:26:57.026Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
    id: 64f18431a77d20f455e137d7
    type: comment
  author: aisensiy
  content: That is so great. I will try it again. Thanks !
  created_at: 2023-09-01 05:26:57+00:00
  edited: false
  hidden: false
  id: 64f18431a77d20f455e137d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Llama-2-7b-chat-hf-function-calling
repo_type: model
status: closed
target_branch: null
title: Notes on fLlama model performance
