!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nlpdev3
conflicting_files: null
created_at: 2023-05-15 07:18:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-15T08:18:07.000Z'
    data:
      edited: true
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: '<p>I saw the source code provided instruction for training the model.
          What''s the size of VRAM needed for finetuning with customized data?</p>

          '
        raw: I saw the source code provided instruction for training the model. What's
          the size of VRAM needed for finetuning with customized data?
        updatedAt: '2023-05-15T09:58:19.773Z'
      numEdits: 1
      reactions: []
    id: 6461eabfb922c14ef1b06f51
    type: comment
  author: nlpdev3
  content: I saw the source code provided instruction for training the model. What's
    the size of VRAM needed for finetuning with customized data?
  created_at: 2023-05-15 07:18:07+00:00
  edited: true
  hidden: false
  id: 6461eabfb922c14ef1b06f51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-16T10:02:07.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your interest in the INSTRUCTOR model!</p>

          <p>We perform all the training on GPUs with 48GB memory. It is possible
          for the smaller models to require less memory.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your interest in the INSTRUCTOR model!


          We perform all the training on GPUs with 48GB memory. It is possible for
          the smaller models to require less memory.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-05-16T10:02:07.990Z'
      numEdits: 0
      reactions: []
    id: 6463549f589d58dbc79a5263
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your interest in the INSTRUCTOR model!


    We perform all the training on GPUs with 48GB memory. It is possible for the smaller
    models to require less memory.


    Feel free to add any further questions or comments!'
  created_at: 2023-05-16 09:02:07+00:00
  edited: false
  hidden: false
  id: 6463549f589d58dbc79a5263
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-17T08:17:54.000Z'
    data:
      edited: true
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: "<p>Thanks for reply. <span data-props=\"{&quot;user&quot;:&quot;multi-train&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/multi-train\"\
          >@<span class=\"underline\">multi-train</span></a></span>\n\n\t</span></span>\
          \  is it necessary to have one GPU with at least 48GB or fine with multi-devices(total\
          \ VRAM &gt; 48GB).</p>\n"
        raw: Thanks for reply. @multi-train  is it necessary to have one GPU with
          at least 48GB or fine with multi-devices(total VRAM > 48GB).
        updatedAt: '2023-05-17T08:18:36.874Z'
      numEdits: 1
      reactions: []
    id: 64648db227d27c1fe7413e02
    type: comment
  author: nlpdev3
  content: Thanks for reply. @multi-train  is it necessary to have one GPU with at
    least 48GB or fine with multi-devices(total VRAM > 48GB).
  created_at: 2023-05-17 07:17:54+00:00
  edited: true
  hidden: false
  id: 64648db227d27c1fe7413e02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-17T14:16:11.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, it is possible to finetune a smaller model on GPU with less
          memory, e.g., 42GB. In addition, you may adjust the batch size, maximum
          sequence length to save the memory. With multiple devices, you may try to
          parallelize the training process.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, it is possible to finetune a smaller model on GPU with less memory,
          e.g., 42GB. In addition, you may adjust the batch size, maximum sequence
          length to save the memory. With multiple devices, you may try to parallelize
          the training process.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-05-17T14:16:11.775Z'
      numEdits: 0
      reactions: []
    id: 6464e1abe8e31202cb4bed67
    type: comment
  author: multi-train
  content: 'Hi, it is possible to finetune a smaller model on GPU with less memory,
    e.g., 42GB. In addition, you may adjust the batch size, maximum sequence length
    to save the memory. With multiple devices, you may try to parallelize the training
    process.


    Feel free to add any further questions or comments!'
  created_at: 2023-05-17 13:16:11+00:00
  edited: false
  hidden: false
  id: 6464e1abe8e31202cb4bed67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-22T03:31:10.000Z'
    data:
      edited: true
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: '<p>Got the training working by fintuning instructor-large. When I load
          the local trained model I got this:</p>

          <ul>

          <li>This IS expected if you are initializing T5EncoderModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).</li>

          <li>This IS NOT expected if you are initializing T5EncoderModel from the
          checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).<br>Some weights of T5EncoderModel were not initialized from the
          model checkpoint at /home/ssm-user/.cache/torch/sentence_transformers/hkunlp_instructor-large/
          and are newly initialized: [''encoder.block.18.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wo.weight'', ''encoder.block.18.layer.0.SelfAttention.v.weight'',
          ''encoder.block.7.layer.0.SelfAttention.k.weight'', ''encoder.block.9.layer.0.layer_norm.weight'',
          ''encoder.block.12.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.2.layer.0.SelfAttention.k.weight'', ''encoder.block.6.layer.0.SelfAttention.k.weight'',
          ''encoder.block.9.layer.0.SelfAttention.o.weight'', ''encoder.block.5.layer.0.layer_norm.weight'',
          ''encoder.block.15.layer.0.SelfAttention.k.weight'', ''encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight'',
          ''encoder.block.20.layer.1.DenseReluDense.wi.weight'', ''encoder.block.19.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.10.layer.1.DenseReluDense.wo.weight'', ''encoder.block.16.layer.0.SelfAttention.q.weight'',
          ''encoder.block.23.layer.0.SelfAttention.o.weight'', ''encoder.block.10.layer.0.SelfAttention.q.weight'',
          ''encoder.block.1.layer.0.SelfAttention.o.weight'', ''encoder.block.11.layer.0.SelfAttention.v.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wo.weight'', ''encoder.block.20.layer.0.SelfAttention.k.weight'',
          ''encoder.block.5.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.1.layer_norm.weight'',
          ''encoder.block.12.layer.0.SelfAttention.q.weight'', ''encoder.block.20.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.0.layer_norm.weight'', ''encoder.block.11.layer.0.SelfAttention.k.weight'',
          ''encoder.block.2.layer.0.layer_norm.weight'', ''encoder.block.11.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.1.layer.0.SelfAttention.q.weight'', ''encoder.block.16.layer.0.SelfAttention.k.weight'',
          ''encoder.block.17.layer.0.SelfAttention.k.weight'', ''encoder.block.21.layer.0.SelfAttention.v.weight'',
          ''encoder.block.12.layer.0.SelfAttention.o.weight'', ''encoder.block.16.layer.0.SelfAttention.v.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.k.weight'',
          ''encoder.block.6.layer.1.layer_norm.weight'', ''encoder.block.6.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.2.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.0.SelfAttention.k.weight'',
          ''encoder.block.20.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.q.weight'',
          ''encoder.block.19.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.layer_norm.weight'',
          ''encoder.block.8.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wi.weight'', ''encoder.block.4.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.13.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.0.SelfAttention.q.weight'',
          ''encoder.block.2.layer.0.SelfAttention.o.weight'', ''encoder.final_layer_norm.weight'',
          ''encoder.block.6.layer.0.SelfAttention.o.weight'', ''encoder.block.3.layer.0.layer_norm.weight'',
          ''encoder.block.20.layer.1.DenseReluDense.wo.weight'', ''encoder.block.0.layer.0.layer_norm.weight'',
          ''encoder.block.0.layer.1.DenseReluDense.wo.weight'', ''encoder.block.6.layer.0.layer_norm.weight'',
          ''encoder.block.7.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.SelfAttention.o.weight'',
          ''encoder.block.4.layer.1.DenseReluDense.wi.weight'', ''encoder.block.13.layer.0.layer_norm.weight'',
          ''encoder.block.13.layer.1.DenseReluDense.wi.weight'', ''encoder.block.23.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.17.layer.0.SelfAttention.v.weight'', ''encoder.block.17.layer.1.layer_norm.weight'',
          ''encoder.block.8.layer.1.layer_norm.weight'', ''encoder.block.10.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.13.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.1.layer_norm.weight'',
          ''encoder.block.9.layer.0.SelfAttention.v.weight'', ''encoder.block.15.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.19.layer.0.layer_norm.weight'', ''encoder.block.23.layer.0.SelfAttention.v.weight'',
          ''encoder.block.14.layer.1.layer_norm.weight'', ''encoder.block.22.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.22.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.0.SelfAttention.k.weight'',
          ''encoder.block.3.layer.0.SelfAttention.k.weight'', ''encoder.block.7.layer.1.layer_norm.weight'',
          ''encoder.block.19.layer.0.SelfAttention.v.weight'', ''encoder.block.13.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.8.layer.1.DenseReluDense.wi.weight'', ''encoder.block.12.layer.0.layer_norm.weight'',
          ''encoder.block.14.layer.1.DenseReluDense.wi.weight'', ''encoder.block.5.layer.0.SelfAttention.k.weight'',
          ''encoder.block.4.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.o.weight'',
          ''encoder.block.14.layer.0.layer_norm.weight'', ''encoder.block.20.layer.1.layer_norm.weight'',
          ''encoder.block.11.layer.0.layer_norm.weight'', ''encoder.block.2.layer.1.layer_norm.weight'',
          ''encoder.block.10.layer.0.SelfAttention.v.weight'', ''encoder.block.16.layer.0.SelfAttention.o.weight'',
          ''encoder.block.0.layer.0.SelfAttention.k.weight'', ''encoder.block.12.layer.0.SelfAttention.v.weight'',
          ''encoder.block.21.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.23.layer.1.layer_norm.weight'', ''encoder.block.0.layer.0.SelfAttention.q.weight'',
          ''encoder.block.11.layer.0.SelfAttention.o.weight'', ''encoder.block.1.layer.0.layer_norm.weight'',
          ''encoder.block.6.layer.1.DenseReluDense.wo.weight'', ''encoder.block.15.layer.0.layer_norm.weight'',
          ''encoder.block.8.layer.0.layer_norm.weight'', ''encoder.block.10.layer.0.SelfAttention.k.weight'',
          ''encoder.block.14.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.0.SelfAttention.v.weight'',
          ''encoder.block.16.layer.0.layer_norm.weight'', ''encoder.block.16.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.18.layer.0.SelfAttention.k.weight'', ''encoder.block.19.layer.0.SelfAttention.o.weight'',
          ''encoder.block.19.layer.1.DenseReluDense.wi.weight'', ''encoder.block.2.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.16.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.v.weight'',
          ''encoder.block.11.layer.1.layer_norm.weight'', ''encoder.block.14.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.15.layer.0.SelfAttention.v.weight'', ''encoder.block.7.layer.0.layer_norm.weight'',
          ''encoder.block.14.layer.0.SelfAttention.o.weight'', ''encoder.block.22.layer.0.SelfAttention.k.weight'',
          ''encoder.block.15.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.8.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.1.layer_norm.weight'',
          ''encoder.block.18.layer.0.SelfAttention.o.weight'', ''encoder.block.23.layer.0.SelfAttention.q.weight'',
          ''encoder.block.16.layer.1.layer_norm.weight'', ''encoder.block.15.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.23.layer.0.layer_norm.weight'', ''encoder.block.15.layer.1.layer_norm.weight'',
          ''encoder.block.20.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.v.weight'',
          ''encoder.block.4.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.layer_norm.weight'',
          ''encoder.block.9.layer.1.DenseReluDense.wo.weight'', ''encoder.block.7.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.4.layer.0.SelfAttention.o.weight'', ''encoder.block.14.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.1.DenseReluDense.wi.weight'', ''encoder.block.10.layer.0.layer_norm.weight'',
          ''encoder.block.11.layer.0.SelfAttention.q.weight'', ''encoder.block.15.layer.0.SelfAttention.o.weight'',
          ''encoder.block.9.layer.0.SelfAttention.q.weight'', ''encoder.block.1.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.5.layer.1.DenseReluDense.wi.weight'', ''encoder.block.7.layer.0.SelfAttention.o.weight'',
          ''encoder.block.3.layer.1.layer_norm.weight'', ''encoder.block.12.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.17.layer.0.layer_norm.weight'', ''encoder.embed_tokens.weight'',
          ''encoder.block.6.layer.0.SelfAttention.v.weight'', ''encoder.block.5.layer.0.SelfAttention.q.weight'',
          ''encoder.block.2.layer.0.SelfAttention.v.weight'', ''encoder.block.1.layer.1.layer_norm.weight'',
          ''encoder.block.4.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.layer_norm.weight'',
          ''encoder.block.22.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.0.SelfAttention.q.weight'',
          ''encoder.block.3.layer.0.SelfAttention.q.weight'', ''encoder.block.5.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.11.layer.1.DenseReluDense.wi.weight'', ''encoder.block.14.layer.0.SelfAttention.k.weight'',
          ''encoder.block.17.layer.0.SelfAttention.q.weight'', ''encoder.block.7.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.10.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.v.weight'',
          ''encoder.block.17.layer.0.SelfAttention.o.weight'', ''encoder.block.9.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.22.layer.0.layer_norm.weight'', ''encoder.block.7.layer.0.SelfAttention.v.weight'',
          ''encoder.block.19.layer.1.layer_norm.weight'', ''encoder.block.1.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.10.layer.1.layer_norm.weight'', ''encoder.block.3.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.23.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.0.SelfAttention.q.weight'',
          ''encoder.block.5.layer.1.layer_norm.weight'', ''encoder.block.23.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.8.layer.0.SelfAttention.o.weight'', ''encoder.block.6.layer.0.SelfAttention.q.weight'',
          ''encoder.block.19.layer.0.SelfAttention.k.weight'', ''encoder.block.20.layer.0.SelfAttention.q.weight'',
          ''encoder.block.0.layer.0.SelfAttention.v.weight'', ''encoder.block.3.layer.1.DenseReluDense.wo.weight'',
          ''shared.weight'', ''encoder.block.2.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.18.layer.0.SelfAttention.q.weight'', ''encoder.block.13.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.1.layer_norm.weight'', ''encoder.block.5.layer.0.SelfAttention.o.weight'',
          ''encoder.block.0.layer.0.SelfAttention.o.weight'']<br>You should probably
          TRAIN this model on a down-stream task to be able to use it for predictions
          and inference.</li>

          </ul>

          '
        raw: 'Got the training working by fintuning instructor-large. When I load
          the local trained model I got this:

          - This IS expected if you are initializing T5EncoderModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).

          - This IS NOT expected if you are initializing T5EncoderModel from the checkpoint
          of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
          model from a BertForSequenceClassification model).

          Some weights of T5EncoderModel were not initialized from the model checkpoint
          at /home/ssm-user/.cache/torch/sentence_transformers/hkunlp_instructor-large/
          and are newly initialized: [''encoder.block.18.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wo.weight'', ''encoder.block.18.layer.0.SelfAttention.v.weight'',
          ''encoder.block.7.layer.0.SelfAttention.k.weight'', ''encoder.block.9.layer.0.layer_norm.weight'',
          ''encoder.block.12.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.2.layer.0.SelfAttention.k.weight'', ''encoder.block.6.layer.0.SelfAttention.k.weight'',
          ''encoder.block.9.layer.0.SelfAttention.o.weight'', ''encoder.block.5.layer.0.layer_norm.weight'',
          ''encoder.block.15.layer.0.SelfAttention.k.weight'', ''encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight'',
          ''encoder.block.20.layer.1.DenseReluDense.wi.weight'', ''encoder.block.19.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.10.layer.1.DenseReluDense.wo.weight'', ''encoder.block.16.layer.0.SelfAttention.q.weight'',
          ''encoder.block.23.layer.0.SelfAttention.o.weight'', ''encoder.block.10.layer.0.SelfAttention.q.weight'',
          ''encoder.block.1.layer.0.SelfAttention.o.weight'', ''encoder.block.11.layer.0.SelfAttention.v.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wo.weight'', ''encoder.block.20.layer.0.SelfAttention.k.weight'',
          ''encoder.block.5.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.1.layer_norm.weight'',
          ''encoder.block.12.layer.0.SelfAttention.q.weight'', ''encoder.block.20.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.0.layer_norm.weight'', ''encoder.block.11.layer.0.SelfAttention.k.weight'',
          ''encoder.block.2.layer.0.layer_norm.weight'', ''encoder.block.11.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.1.layer.0.SelfAttention.q.weight'', ''encoder.block.16.layer.0.SelfAttention.k.weight'',
          ''encoder.block.17.layer.0.SelfAttention.k.weight'', ''encoder.block.21.layer.0.SelfAttention.v.weight'',
          ''encoder.block.12.layer.0.SelfAttention.o.weight'', ''encoder.block.16.layer.0.SelfAttention.v.weight'',
          ''encoder.block.21.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.k.weight'',
          ''encoder.block.6.layer.1.layer_norm.weight'', ''encoder.block.6.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.2.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.0.SelfAttention.k.weight'',
          ''encoder.block.20.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.q.weight'',
          ''encoder.block.19.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.layer_norm.weight'',
          ''encoder.block.8.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.17.layer.1.DenseReluDense.wi.weight'', ''encoder.block.4.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.13.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.0.SelfAttention.q.weight'',
          ''encoder.block.2.layer.0.SelfAttention.o.weight'', ''encoder.final_layer_norm.weight'',
          ''encoder.block.6.layer.0.SelfAttention.o.weight'', ''encoder.block.3.layer.0.layer_norm.weight'',
          ''encoder.block.20.layer.1.DenseReluDense.wo.weight'', ''encoder.block.0.layer.0.layer_norm.weight'',
          ''encoder.block.0.layer.1.DenseReluDense.wo.weight'', ''encoder.block.6.layer.0.layer_norm.weight'',
          ''encoder.block.7.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.SelfAttention.o.weight'',
          ''encoder.block.4.layer.1.DenseReluDense.wi.weight'', ''encoder.block.13.layer.0.layer_norm.weight'',
          ''encoder.block.13.layer.1.DenseReluDense.wi.weight'', ''encoder.block.23.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.17.layer.0.SelfAttention.v.weight'', ''encoder.block.17.layer.1.layer_norm.weight'',
          ''encoder.block.8.layer.1.layer_norm.weight'', ''encoder.block.10.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.13.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.1.layer_norm.weight'',
          ''encoder.block.9.layer.0.SelfAttention.v.weight'', ''encoder.block.15.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.19.layer.0.layer_norm.weight'', ''encoder.block.23.layer.0.SelfAttention.v.weight'',
          ''encoder.block.14.layer.1.layer_norm.weight'', ''encoder.block.22.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.22.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.0.SelfAttention.k.weight'',
          ''encoder.block.3.layer.0.SelfAttention.k.weight'', ''encoder.block.7.layer.1.layer_norm.weight'',
          ''encoder.block.19.layer.0.SelfAttention.v.weight'', ''encoder.block.13.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.8.layer.1.DenseReluDense.wi.weight'', ''encoder.block.12.layer.0.layer_norm.weight'',
          ''encoder.block.14.layer.1.DenseReluDense.wi.weight'', ''encoder.block.5.layer.0.SelfAttention.k.weight'',
          ''encoder.block.4.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.o.weight'',
          ''encoder.block.14.layer.0.layer_norm.weight'', ''encoder.block.20.layer.1.layer_norm.weight'',
          ''encoder.block.11.layer.0.layer_norm.weight'', ''encoder.block.2.layer.1.layer_norm.weight'',
          ''encoder.block.10.layer.0.SelfAttention.v.weight'', ''encoder.block.16.layer.0.SelfAttention.o.weight'',
          ''encoder.block.0.layer.0.SelfAttention.k.weight'', ''encoder.block.12.layer.0.SelfAttention.v.weight'',
          ''encoder.block.21.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.23.layer.1.layer_norm.weight'', ''encoder.block.0.layer.0.SelfAttention.q.weight'',
          ''encoder.block.11.layer.0.SelfAttention.o.weight'', ''encoder.block.1.layer.0.layer_norm.weight'',
          ''encoder.block.6.layer.1.DenseReluDense.wo.weight'', ''encoder.block.15.layer.0.layer_norm.weight'',
          ''encoder.block.8.layer.0.layer_norm.weight'', ''encoder.block.10.layer.0.SelfAttention.k.weight'',
          ''encoder.block.14.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.0.SelfAttention.v.weight'',
          ''encoder.block.16.layer.0.layer_norm.weight'', ''encoder.block.16.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.18.layer.0.SelfAttention.k.weight'', ''encoder.block.19.layer.0.SelfAttention.o.weight'',
          ''encoder.block.19.layer.1.DenseReluDense.wi.weight'', ''encoder.block.2.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.16.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.v.weight'',
          ''encoder.block.11.layer.1.layer_norm.weight'', ''encoder.block.14.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.15.layer.0.SelfAttention.v.weight'', ''encoder.block.7.layer.0.layer_norm.weight'',
          ''encoder.block.14.layer.0.SelfAttention.o.weight'', ''encoder.block.22.layer.0.SelfAttention.k.weight'',
          ''encoder.block.15.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.8.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.1.layer_norm.weight'',
          ''encoder.block.18.layer.0.SelfAttention.o.weight'', ''encoder.block.23.layer.0.SelfAttention.q.weight'',
          ''encoder.block.16.layer.1.layer_norm.weight'', ''encoder.block.15.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.23.layer.0.layer_norm.weight'', ''encoder.block.15.layer.1.layer_norm.weight'',
          ''encoder.block.20.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.v.weight'',
          ''encoder.block.4.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.layer_norm.weight'',
          ''encoder.block.9.layer.1.DenseReluDense.wo.weight'', ''encoder.block.7.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.4.layer.0.SelfAttention.o.weight'', ''encoder.block.14.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.1.DenseReluDense.wi.weight'', ''encoder.block.10.layer.0.layer_norm.weight'',
          ''encoder.block.11.layer.0.SelfAttention.q.weight'', ''encoder.block.15.layer.0.SelfAttention.o.weight'',
          ''encoder.block.9.layer.0.SelfAttention.q.weight'', ''encoder.block.1.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.5.layer.1.DenseReluDense.wi.weight'', ''encoder.block.7.layer.0.SelfAttention.o.weight'',
          ''encoder.block.3.layer.1.layer_norm.weight'', ''encoder.block.12.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.17.layer.0.layer_norm.weight'', ''encoder.embed_tokens.weight'',
          ''encoder.block.6.layer.0.SelfAttention.v.weight'', ''encoder.block.5.layer.0.SelfAttention.q.weight'',
          ''encoder.block.2.layer.0.SelfAttention.v.weight'', ''encoder.block.1.layer.1.layer_norm.weight'',
          ''encoder.block.4.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.layer_norm.weight'',
          ''encoder.block.22.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.0.SelfAttention.q.weight'',
          ''encoder.block.3.layer.0.SelfAttention.q.weight'', ''encoder.block.5.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.11.layer.1.DenseReluDense.wi.weight'', ''encoder.block.14.layer.0.SelfAttention.k.weight'',
          ''encoder.block.17.layer.0.SelfAttention.q.weight'', ''encoder.block.7.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.10.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.v.weight'',
          ''encoder.block.17.layer.0.SelfAttention.o.weight'', ''encoder.block.9.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.22.layer.0.layer_norm.weight'', ''encoder.block.7.layer.0.SelfAttention.v.weight'',
          ''encoder.block.19.layer.1.layer_norm.weight'', ''encoder.block.1.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.10.layer.1.layer_norm.weight'', ''encoder.block.3.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.23.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.0.SelfAttention.q.weight'',
          ''encoder.block.5.layer.1.layer_norm.weight'', ''encoder.block.23.layer.1.DenseReluDense.wi.weight'',
          ''encoder.block.8.layer.0.SelfAttention.o.weight'', ''encoder.block.6.layer.0.SelfAttention.q.weight'',
          ''encoder.block.19.layer.0.SelfAttention.k.weight'', ''encoder.block.20.layer.0.SelfAttention.q.weight'',
          ''encoder.block.0.layer.0.SelfAttention.v.weight'', ''encoder.block.3.layer.1.DenseReluDense.wo.weight'',
          ''shared.weight'', ''encoder.block.2.layer.1.DenseReluDense.wo.weight'',
          ''encoder.block.18.layer.0.SelfAttention.q.weight'', ''encoder.block.13.layer.0.SelfAttention.v.weight'',
          ''encoder.block.18.layer.1.layer_norm.weight'', ''encoder.block.5.layer.0.SelfAttention.o.weight'',
          ''encoder.block.0.layer.0.SelfAttention.o.weight'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.'
        updatedAt: '2023-05-22T03:33:00.647Z'
      numEdits: 1
      reactions: []
    id: 646ae1fe3721aab2edfeffab
    type: comment
  author: nlpdev3
  content: 'Got the training working by fintuning instructor-large. When I load the
    local trained model I got this:

    - This IS expected if you are initializing T5EncoderModel from the checkpoint
    of a model trained on another task or with another architecture (e.g. initializing
    a BertForSequenceClassification model from a BertForPreTraining model).

    - This IS NOT expected if you are initializing T5EncoderModel from the checkpoint
    of a model that you expect to be exactly identical (initializing a BertForSequenceClassification
    model from a BertForSequenceClassification model).

    Some weights of T5EncoderModel were not initialized from the model checkpoint
    at /home/ssm-user/.cache/torch/sentence_transformers/hkunlp_instructor-large/
    and are newly initialized: [''encoder.block.18.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.21.layer.1.DenseReluDense.wo.weight'', ''encoder.block.18.layer.0.SelfAttention.v.weight'',
    ''encoder.block.7.layer.0.SelfAttention.k.weight'', ''encoder.block.9.layer.0.layer_norm.weight'',
    ''encoder.block.12.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.2.layer.0.SelfAttention.k.weight'', ''encoder.block.6.layer.0.SelfAttention.k.weight'',
    ''encoder.block.9.layer.0.SelfAttention.o.weight'', ''encoder.block.5.layer.0.layer_norm.weight'',
    ''encoder.block.15.layer.0.SelfAttention.k.weight'', ''encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight'',
    ''encoder.block.20.layer.1.DenseReluDense.wi.weight'', ''encoder.block.19.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.10.layer.1.DenseReluDense.wo.weight'', ''encoder.block.16.layer.0.SelfAttention.q.weight'',
    ''encoder.block.23.layer.0.SelfAttention.o.weight'', ''encoder.block.10.layer.0.SelfAttention.q.weight'',
    ''encoder.block.1.layer.0.SelfAttention.o.weight'', ''encoder.block.11.layer.0.SelfAttention.v.weight'',
    ''encoder.block.17.layer.1.DenseReluDense.wo.weight'', ''encoder.block.20.layer.0.SelfAttention.k.weight'',
    ''encoder.block.5.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.1.layer_norm.weight'',
    ''encoder.block.12.layer.0.SelfAttention.q.weight'', ''encoder.block.20.layer.0.SelfAttention.v.weight'',
    ''encoder.block.18.layer.0.layer_norm.weight'', ''encoder.block.11.layer.0.SelfAttention.k.weight'',
    ''encoder.block.2.layer.0.layer_norm.weight'', ''encoder.block.11.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.1.layer.0.SelfAttention.q.weight'', ''encoder.block.16.layer.0.SelfAttention.k.weight'',
    ''encoder.block.17.layer.0.SelfAttention.k.weight'', ''encoder.block.21.layer.0.SelfAttention.v.weight'',
    ''encoder.block.12.layer.0.SelfAttention.o.weight'', ''encoder.block.16.layer.0.SelfAttention.v.weight'',
    ''encoder.block.21.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.k.weight'',
    ''encoder.block.6.layer.1.layer_norm.weight'', ''encoder.block.6.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.2.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.0.SelfAttention.k.weight'',
    ''encoder.block.20.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.q.weight'',
    ''encoder.block.19.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.layer_norm.weight'',
    ''encoder.block.8.layer.0.SelfAttention.q.weight'', ''encoder.block.12.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.17.layer.1.DenseReluDense.wi.weight'', ''encoder.block.4.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.13.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.0.SelfAttention.q.weight'',
    ''encoder.block.2.layer.0.SelfAttention.o.weight'', ''encoder.final_layer_norm.weight'',
    ''encoder.block.6.layer.0.SelfAttention.o.weight'', ''encoder.block.3.layer.0.layer_norm.weight'',
    ''encoder.block.20.layer.1.DenseReluDense.wo.weight'', ''encoder.block.0.layer.0.layer_norm.weight'',
    ''encoder.block.0.layer.1.DenseReluDense.wo.weight'', ''encoder.block.6.layer.0.layer_norm.weight'',
    ''encoder.block.7.layer.0.SelfAttention.q.weight'', ''encoder.block.21.layer.0.SelfAttention.o.weight'',
    ''encoder.block.4.layer.1.DenseReluDense.wi.weight'', ''encoder.block.13.layer.0.layer_norm.weight'',
    ''encoder.block.13.layer.1.DenseReluDense.wi.weight'', ''encoder.block.23.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.17.layer.0.SelfAttention.v.weight'', ''encoder.block.17.layer.1.layer_norm.weight'',
    ''encoder.block.8.layer.1.layer_norm.weight'', ''encoder.block.10.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.13.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.1.layer_norm.weight'',
    ''encoder.block.9.layer.0.SelfAttention.v.weight'', ''encoder.block.15.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.19.layer.0.layer_norm.weight'', ''encoder.block.23.layer.0.SelfAttention.v.weight'',
    ''encoder.block.14.layer.1.layer_norm.weight'', ''encoder.block.22.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.22.layer.0.SelfAttention.v.weight'', ''encoder.block.9.layer.0.SelfAttention.k.weight'',
    ''encoder.block.3.layer.0.SelfAttention.k.weight'', ''encoder.block.7.layer.1.layer_norm.weight'',
    ''encoder.block.19.layer.0.SelfAttention.v.weight'', ''encoder.block.13.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.8.layer.1.DenseReluDense.wi.weight'', ''encoder.block.12.layer.0.layer_norm.weight'',
    ''encoder.block.14.layer.1.DenseReluDense.wi.weight'', ''encoder.block.5.layer.0.SelfAttention.k.weight'',
    ''encoder.block.4.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.o.weight'',
    ''encoder.block.14.layer.0.layer_norm.weight'', ''encoder.block.20.layer.1.layer_norm.weight'',
    ''encoder.block.11.layer.0.layer_norm.weight'', ''encoder.block.2.layer.1.layer_norm.weight'',
    ''encoder.block.10.layer.0.SelfAttention.v.weight'', ''encoder.block.16.layer.0.SelfAttention.o.weight'',
    ''encoder.block.0.layer.0.SelfAttention.k.weight'', ''encoder.block.12.layer.0.SelfAttention.v.weight'',
    ''encoder.block.21.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.23.layer.1.layer_norm.weight'', ''encoder.block.0.layer.0.SelfAttention.q.weight'',
    ''encoder.block.11.layer.0.SelfAttention.o.weight'', ''encoder.block.1.layer.0.layer_norm.weight'',
    ''encoder.block.6.layer.1.DenseReluDense.wo.weight'', ''encoder.block.15.layer.0.layer_norm.weight'',
    ''encoder.block.8.layer.0.layer_norm.weight'', ''encoder.block.10.layer.0.SelfAttention.k.weight'',
    ''encoder.block.14.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.0.SelfAttention.v.weight'',
    ''encoder.block.16.layer.0.layer_norm.weight'', ''encoder.block.16.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.18.layer.0.SelfAttention.k.weight'', ''encoder.block.19.layer.0.SelfAttention.o.weight'',
    ''encoder.block.19.layer.1.DenseReluDense.wi.weight'', ''encoder.block.2.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.16.layer.1.DenseReluDense.wi.weight'', ''encoder.block.1.layer.0.SelfAttention.v.weight'',
    ''encoder.block.11.layer.1.layer_norm.weight'', ''encoder.block.14.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.15.layer.0.SelfAttention.v.weight'', ''encoder.block.7.layer.0.layer_norm.weight'',
    ''encoder.block.14.layer.0.SelfAttention.o.weight'', ''encoder.block.22.layer.0.SelfAttention.k.weight'',
    ''encoder.block.15.layer.0.SelfAttention.q.weight'', ''encoder.block.8.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.8.layer.0.SelfAttention.k.weight'', ''encoder.block.13.layer.1.layer_norm.weight'',
    ''encoder.block.18.layer.0.SelfAttention.o.weight'', ''encoder.block.23.layer.0.SelfAttention.q.weight'',
    ''encoder.block.16.layer.1.layer_norm.weight'', ''encoder.block.15.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.23.layer.0.layer_norm.weight'', ''encoder.block.15.layer.1.layer_norm.weight'',
    ''encoder.block.20.layer.0.layer_norm.weight'', ''encoder.block.3.layer.0.SelfAttention.v.weight'',
    ''encoder.block.4.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.1.layer_norm.weight'',
    ''encoder.block.9.layer.1.DenseReluDense.wo.weight'', ''encoder.block.7.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.4.layer.0.SelfAttention.o.weight'', ''encoder.block.14.layer.0.SelfAttention.v.weight'',
    ''encoder.block.18.layer.1.DenseReluDense.wi.weight'', ''encoder.block.10.layer.0.layer_norm.weight'',
    ''encoder.block.11.layer.0.SelfAttention.q.weight'', ''encoder.block.15.layer.0.SelfAttention.o.weight'',
    ''encoder.block.9.layer.0.SelfAttention.q.weight'', ''encoder.block.1.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.5.layer.1.DenseReluDense.wi.weight'', ''encoder.block.7.layer.0.SelfAttention.o.weight'',
    ''encoder.block.3.layer.1.layer_norm.weight'', ''encoder.block.12.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.17.layer.0.layer_norm.weight'', ''encoder.embed_tokens.weight'',
    ''encoder.block.6.layer.0.SelfAttention.v.weight'', ''encoder.block.5.layer.0.SelfAttention.q.weight'',
    ''encoder.block.2.layer.0.SelfAttention.v.weight'', ''encoder.block.1.layer.1.layer_norm.weight'',
    ''encoder.block.4.layer.1.layer_norm.weight'', ''encoder.block.0.layer.1.layer_norm.weight'',
    ''encoder.block.22.layer.0.SelfAttention.o.weight'', ''encoder.block.21.layer.0.SelfAttention.q.weight'',
    ''encoder.block.3.layer.0.SelfAttention.q.weight'', ''encoder.block.5.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.11.layer.1.DenseReluDense.wi.weight'', ''encoder.block.14.layer.0.SelfAttention.k.weight'',
    ''encoder.block.17.layer.0.SelfAttention.q.weight'', ''encoder.block.7.layer.1.DenseReluDense.wo.weight'',
    ''encoder.block.10.layer.0.SelfAttention.o.weight'', ''encoder.block.4.layer.0.SelfAttention.v.weight'',
    ''encoder.block.17.layer.0.SelfAttention.o.weight'', ''encoder.block.9.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.22.layer.0.layer_norm.weight'', ''encoder.block.7.layer.0.SelfAttention.v.weight'',
    ''encoder.block.19.layer.1.layer_norm.weight'', ''encoder.block.1.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.10.layer.1.layer_norm.weight'', ''encoder.block.3.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.23.layer.0.SelfAttention.k.weight'', ''encoder.block.22.layer.0.SelfAttention.q.weight'',
    ''encoder.block.5.layer.1.layer_norm.weight'', ''encoder.block.23.layer.1.DenseReluDense.wi.weight'',
    ''encoder.block.8.layer.0.SelfAttention.o.weight'', ''encoder.block.6.layer.0.SelfAttention.q.weight'',
    ''encoder.block.19.layer.0.SelfAttention.k.weight'', ''encoder.block.20.layer.0.SelfAttention.q.weight'',
    ''encoder.block.0.layer.0.SelfAttention.v.weight'', ''encoder.block.3.layer.1.DenseReluDense.wo.weight'',
    ''shared.weight'', ''encoder.block.2.layer.1.DenseReluDense.wo.weight'', ''encoder.block.18.layer.0.SelfAttention.q.weight'',
    ''encoder.block.13.layer.0.SelfAttention.v.weight'', ''encoder.block.18.layer.1.layer_norm.weight'',
    ''encoder.block.5.layer.0.SelfAttention.o.weight'', ''encoder.block.0.layer.0.SelfAttention.o.weight'']

    You should probably TRAIN this model on a down-stream task to be able to use it
    for predictions and inference.'
  created_at: 2023-05-22 02:31:10+00:00
  edited: true
  hidden: false
  id: 646ae1fe3721aab2edfeffab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-22T06:27:09.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Could I see your code to load the model?</p>

          '
        raw: Could I see your code to load the model?
        updatedAt: '2023-05-22T06:27:09.420Z'
      numEdits: 0
      reactions: []
    id: 646b0b3d5d68f5c15a10c8e6
    type: comment
  author: multi-train
  content: Could I see your code to load the model?
  created_at: 2023-05-22 05:27:09+00:00
  edited: false
  hidden: false
  id: 646b0b3d5d68f5c15a10c8e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-22T08:44:19.000Z'
    data:
      edited: false
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: '<p>I just simply replace the pytorch_mode.bin file in cache directory
          and use sample code to load.</p>

          '
        raw: I just simply replace the pytorch_mode.bin file in cache directory and
          use sample code to load.
        updatedAt: '2023-05-22T08:44:19.302Z'
      numEdits: 0
      reactions: []
    id: 646b2b635d68f5c15a1638bf
    type: comment
  author: nlpdev3
  content: I just simply replace the pytorch_mode.bin file in cache directory and
    use sample code to load.
  created_at: 2023-05-22 07:44:19+00:00
  edited: false
  hidden: false
  id: 646b2b635d68f5c15a1638bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-22T16:42:47.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Do you refer to the code:</p>

          <pre><code>from InstructorEmbedding import INSTRUCTOR

          model = INSTRUCTOR(''hkunlp/instructor-large'')

          </code></pre>

          '
        raw: 'Do you refer to the code:

          ```

          from InstructorEmbedding import INSTRUCTOR

          model = INSTRUCTOR(''hkunlp/instructor-large'')

          ```'
        updatedAt: '2023-05-22T16:42:47.514Z'
      numEdits: 0
      reactions: []
    id: 646b9b87f85ebf65c534f406
    type: comment
  author: multi-train
  content: 'Do you refer to the code:

    ```

    from InstructorEmbedding import INSTRUCTOR

    model = INSTRUCTOR(''hkunlp/instructor-large'')

    ```'
  created_at: 2023-05-22 15:42:47+00:00
  edited: false
  hidden: false
  id: 646b9b87f85ebf65c534f406
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-23T01:42:54.000Z'
    data:
      edited: false
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: '<p>Right, just use these two lines to verify.</p>

          '
        raw: Right, just use these two lines to verify.
        updatedAt: '2023-05-23T01:42:54.012Z'
      numEdits: 0
      reactions: []
    id: 646c1a1ef85ebf65c545db00
    type: comment
  author: nlpdev3
  content: Right, just use these two lines to verify.
  created_at: 2023-05-23 00:42:54+00:00
  edited: false
  hidden: false
  id: 646c1a1ef85ebf65c545db00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-23T03:49:59.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>It seems that I can run these two lines without encountering the
          errors above. Could you re-download the checkpoints?</p>

          '
        raw: It seems that I can run these two lines without encountering the errors
          above. Could you re-download the checkpoints?
        updatedAt: '2023-05-23T03:49:59.065Z'
      numEdits: 0
      reactions: []
    id: 646c37e7f85ebf65c54a6214
    type: comment
  author: multi-train
  content: It seems that I can run these two lines without encountering the errors
    above. Could you re-download the checkpoints?
  created_at: 2023-05-23 02:49:59+00:00
  edited: false
  hidden: false
  id: 646c37e7f85ebf65c54a6214
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-23T06:18:53.000Z'
    data:
      edited: true
      editors:
      - nlpdev3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: '<p>This is what I did:<br>1 - Grab top 100 samples from medi-data.json<br>2
          - Train with <code>python train.py --model_name_or_path hkunlp/instructor-large
          --output_dir ./models --cache_dir ./medi-data --max_source_length 512 --num_train_epochs
          10 --save_steps 500 --cl_temperature 0.01 --warmup_ratio 0.1 --learning_rate
          2e-5 --overwrite_output_dir</code><br>3 - Copy the ./models/pytorch_model.bin
          to the cache directory to replace the same name file with my new weights.<br>4
          -  Use this code to load data </p>

          <pre><code>from InstructorEmbedding import INSTRUCTOR

          model = INSTRUCTOR(''hkunlp/instructor-large'')

          </code></pre>

          <p>Weird that I evaluate billboard with finetuned model, the pearson result
          is 0.16 which is worse  than original hkunlp/instructor-large.</p>

          '
        raw: "This is what I did:\n1 - Grab top 100 samples from medi-data.json\n\
          2 - Train with ```python train.py --model_name_or_path hkunlp/instructor-large\
          \ --output_dir ./models --cache_dir ./medi-data --max_source_length 512\
          \ --num_train_epochs 10 --save_steps 500 --cl_temperature 0.01 --warmup_ratio\
          \ 0.1 --learning_rate 2e-5 --overwrite_output_dir```\n3 - Copy the ./models/pytorch_model.bin\
          \ to the cache directory to replace the same name file with my new weights.\n\
          4 -  Use this code to load data \n```\nfrom InstructorEmbedding import INSTRUCTOR\n\
          model = INSTRUCTOR('hkunlp/instructor-large')\n```\n\nWeird that I evaluate\
          \ billboard with finetuned model, the pearson result is 0.16 which is worse\
          \  than original hkunlp/instructor-large."
        updatedAt: '2023-05-23T09:19:38.784Z'
      numEdits: 1
      reactions: []
    id: 646c5acd10f66cc3c7533378
    type: comment
  author: nlpdev3
  content: "This is what I did:\n1 - Grab top 100 samples from medi-data.json\n2 -\
    \ Train with ```python train.py --model_name_or_path hkunlp/instructor-large --output_dir\
    \ ./models --cache_dir ./medi-data --max_source_length 512 --num_train_epochs\
    \ 10 --save_steps 500 --cl_temperature 0.01 --warmup_ratio 0.1 --learning_rate\
    \ 2e-5 --overwrite_output_dir```\n3 - Copy the ./models/pytorch_model.bin to the\
    \ cache directory to replace the same name file with my new weights.\n4 -  Use\
    \ this code to load data \n```\nfrom InstructorEmbedding import INSTRUCTOR\nmodel\
    \ = INSTRUCTOR('hkunlp/instructor-large')\n```\n\nWeird that I evaluate billboard\
    \ with finetuned model, the pearson result is 0.16 which is worse  than original\
    \ hkunlp/instructor-large."
  created_at: 2023-05-23 05:18:53+00:00
  edited: true
  hidden: false
  id: 646c5acd10f66cc3c7533378
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-05-26T14:12:15.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your comment!</p>

          <p>We have updated both the training file and the InstructorEmbedding package.
          You may update the package by:</p>

          <pre><code>pip install -U InstructorEmbedding

          </code></pre>

          <p>and use the latest training file to finetune the model.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your comment!


          We have updated both the training file and the InstructorEmbedding package.
          You may update the package by:

          ```

          pip install -U InstructorEmbedding

          ```

          and use the latest training file to finetune the model.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-05-26T14:12:15.753Z'
      numEdits: 0
      reactions: []
    id: 6470be3f3df93fddece3953f
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your comment!


    We have updated both the training file and the InstructorEmbedding package. You
    may update the package by:

    ```

    pip install -U InstructorEmbedding

    ```

    and use the latest training file to finetune the model.


    Feel free to add any further questions or comments!'
  created_at: 2023-05-26 13:12:15+00:00
  edited: false
  hidden: false
  id: 6470be3f3df93fddece3953f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-05-27T07:22:05.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
          fullname: TT
          isHf: false
          isPro: false
          name: nlpdev3
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-05T04:15:17.569Z'
      numEdits: 1
      reactions: []
    id: 6471af9d97a75cc77aa4b978
    type: comment
  author: nlpdev3
  content: This comment has been hidden
  created_at: 2023-05-27 06:22:05+00:00
  edited: true
  hidden: true
  id: 6471af9d97a75cc77aa4b978
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4d043030a81fe1b6e0b7092030f0725a.svg
      fullname: TT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpdev3
      type: user
    createdAt: '2023-06-05T04:15:11.000Z'
    data:
      status: closed
    id: 647d614fc788767ab5e6fd4c
    type: status-change
  author: nlpdev3
  created_at: 2023-06-05 03:15:11+00:00
  id: 647d614fc788767ab5e6fd4c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: hkunlp/instructor-large
repo_type: model
status: closed
target_branch: null
title: How much GPU VRAM needed for finetuning?
