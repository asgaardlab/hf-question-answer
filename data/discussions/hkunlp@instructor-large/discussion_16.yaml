!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iAFuisMe1234
conflicting_files: null
created_at: 2023-08-15 02:36:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0ffffc2b4b306027952cc4a7a3f6e9.svg
      fullname: AIFuture
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iAFuisMe1234
      type: user
    createdAt: '2023-08-15T03:36:34.000Z'
    data:
      edited: false
      editors:
      - iAFuisMe1234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9114117622375488
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0ffffc2b4b306027952cc4a7a3f6e9.svg
          fullname: AIFuture
          isHf: false
          isPro: false
          name: iAFuisMe1234
          type: user
        html: '<p>what are the LLM compatible with INSTRUCTOR Embeddings are there
          any git links with sample code.</p>

          '
        raw: what are the LLM compatible with INSTRUCTOR Embeddings are there any
          git links with sample code.
        updatedAt: '2023-08-15T03:36:34.429Z'
      numEdits: 0
      reactions: []
    id: 64daf2c2858f8a41c1034346
    type: comment
  author: iAFuisMe1234
  content: what are the LLM compatible with INSTRUCTOR Embeddings are there any git
    links with sample code.
  created_at: 2023-08-15 02:36:34+00:00
  edited: false
  hidden: false
  id: 64daf2c2858f8a41c1034346
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8be2837c1bd1e80547b874ab67a04042.svg
      fullname: kPaul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kpi
      type: user
    createdAt: '2024-01-14T15:58:26.000Z'
    data:
      edited: false
      editors:
      - kpi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7449662089347839
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8be2837c1bd1e80547b874ab67a04042.svg
          fullname: kPaul
          isHf: false
          isPro: false
          name: kpi
          type: user
        html: "<p>Hello,<br><em>Better late than never</em><br> If you want to use\
          \ this in RAG workflow you can! Using this model and compute the embedding\
          \ of all your documents. Calculate the distances between the question and\
          \ the documents' embeddings. Retrieve the documents with the smallest distance.\
          \ Pass it as context within your prompt into the llm.<br>Example using faiss\
          \ </p>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"\
          ># pip install sentence_transformers InstructorEmbedding faiss-cpu</span>\n\
          \n<span class=\"hljs-keyword\">from</span> InstructorEmbedding <span class=\"\
          hljs-keyword\">import</span> INSTRUCTOR\nmodel = INSTRUCTOR(<span class=\"\
          hljs-string\">'hkunlp/instructor-large'</span>)\n\n<span class=\"hljs-comment\"\
          ># Get your documents </span>\nsentences = [\n<span class=\"hljs-string\"\
          >\"(A) Call Mom \\@Phone +Family\"</span>,\n<span class=\"hljs-string\"\
          >\"(A) Schedule annual checkup +Health\"</span>,\n<span class=\"hljs-string\"\
          >\"(B) Outline chapter 5 +Novel \\@Computer\"</span>,\n<span class=\"hljs-string\"\
          >\"(C) Add cover sheets \\@Office +TPSReports\"</span>,\n<span class=\"\
          hljs-string\">\"Plan backyard herb garden \\@Home\"</span>,\n<span class=\"\
          hljs-string\">\"Pick up milk \\@GroceryStore\"</span>,\n<span class=\"hljs-string\"\
          >\"Research self-publishing services +Novel \\@Computer\"</span>,\n<span\
          \ class=\"hljs-string\">\"x Download Todo.txt mobile app \\@Phone\"</span>\n\
          ]\ninstruction = <span class=\"hljs-string\">\"Represent the todo.txt item\
          \ for retrieving it\"</span>\n<span class=\"hljs-comment\"># create the\
          \ embeddings for the documents</span>\nembeddings = model.encode([[instruction,sentence]\
          \ <span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\"\
          >in</span> sentences])\n\n<span class=\"hljs-comment\"># create the embeddings\
          \ for the prompt</span>\nquestion = <span class=\"hljs-string\">\"I'm at\
          \ the store what do i have to buy\"</span>\ninstruction_question = <span\
          \ class=\"hljs-string\">\"Represent the question for retrieving a todo.txt\
          \ item\"</span>\nembeddings_question = model.encode([[instruction_question,\
          \ question]])\n\n<span class=\"hljs-comment\"># using faiss to store and\
          \ compute the distances (should work with any vector database</span>\n<span\
          \ class=\"hljs-keyword\">import</span> faiss                   <span class=\"\
          hljs-comment\"># make faiss available</span>\nindex = faiss.IndexFlatL2(<span\
          \ class=\"hljs-built_in\">len</span>(embeddings[<span class=\"hljs-number\"\
          >0</span>]))   <span class=\"hljs-comment\"># build the index</span>\nindex.add(embeddings)\
          \                  <span class=\"hljs-comment\"># add vectors to the index</span>\n\
          k = <span class=\"hljs-number\">1</span>                          <span\
          \ class=\"hljs-comment\"># we want to see nearest neighbor</span>\n\n_,\
          \ I = index.search(embeddings_question, k)     <span class=\"hljs-comment\"\
          ># actual search</span>\ncontext = sentences[<span class=\"hljs-built_in\"\
          >int</span>(I[<span class=\"hljs-number\">0</span>])] \n\n<span class=\"\
          hljs-comment\"># build your prompt</span>\nprompt = <span class=\"hljs-string\"\
          >f\"using this context \\\"<span class=\"hljs-subst\">{context}</span>\\\
          \" answer the user question: <span class=\"hljs-subst\">{question}</span>\"\
          </span>\n<span class=\"hljs-comment\"># creating a dummy llm for example</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >dummy_llm</span>(<span class=\"hljs-params\">prompt</span>):\n    <span\
          \ class=\"hljs-string\">\"\"\"just return the prompt\"\"\"</span>\n    <span\
          \ class=\"hljs-keyword\">return</span> prompt\n\n<span class=\"hljs-built_in\"\
          >print</span>(dummy_llm(prompt))\n</code></pre>\n"
        raw: "Hello,\n_Better late than never_\n If you want to use this in RAG workflow\
          \ you can! Using this model and compute the embedding of all your documents.\
          \ Calculate the distances between the question and the documents' embeddings.\
          \ Retrieve the documents with the smallest distance. Pass it as context\
          \ within your prompt into the llm.\nExample using faiss \n```python\n# pip\
          \ install sentence_transformers InstructorEmbedding faiss-cpu\n\nfrom InstructorEmbedding\
          \ import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-large')\n\n#\
          \ Get your documents \nsentences = [\n\"(A) Call Mom \\@Phone +Family\"\
          ,\n\"(A) Schedule annual checkup +Health\",\n\"(B) Outline chapter 5 +Novel\
          \ \\@Computer\",\n\"(C) Add cover sheets \\@Office +TPSReports\",\n\"Plan\
          \ backyard herb garden \\@Home\",\n\"Pick up milk \\@GroceryStore\",\n\"\
          Research self-publishing services +Novel \\@Computer\",\n\"x Download Todo.txt\
          \ mobile app \\@Phone\"\n]\ninstruction = \"Represent the todo.txt item\
          \ for retrieving it\"\n# create the embeddings for the documents\nembeddings\
          \ = model.encode([[instruction,sentence] for sentence in sentences])\n\n\
          # create the embeddings for the prompt\nquestion = \"I'm at the store what\
          \ do i have to buy\"\ninstruction_question = \"Represent the question for\
          \ retrieving a todo.txt item\"\nembeddings_question = model.encode([[instruction_question,\
          \ question]])\n\n# using faiss to store and compute the distances (should\
          \ work with any vector database\nimport faiss                   # make faiss\
          \ available\nindex = faiss.IndexFlatL2(len(embeddings[0]))   # build the\
          \ index\nindex.add(embeddings)                  # add vectors to the index\n\
          k = 1                          # we want to see nearest neighbor\n\n_, I\
          \ = index.search(embeddings_question, k)     # actual search\ncontext =\
          \ sentences[int(I[0])] \n\n# build your prompt\nprompt = f\"using this context\
          \ \\\"{context}\\\" answer the user question: {question}\"\n# creating a\
          \ dummy llm for example\ndef dummy_llm(prompt):\n    \"\"\"just return the\
          \ prompt\"\"\"\n    return prompt\n\nprint(dummy_llm(prompt))\n```"
        updatedAt: '2024-01-14T15:58:26.601Z'
      numEdits: 0
      reactions: []
    id: 65a404a29f0a34c17300a600
    type: comment
  author: kpi
  content: "Hello,\n_Better late than never_\n If you want to use this in RAG workflow\
    \ you can! Using this model and compute the embedding of all your documents. Calculate\
    \ the distances between the question and the documents' embeddings. Retrieve the\
    \ documents with the smallest distance. Pass it as context within your prompt\
    \ into the llm.\nExample using faiss \n```python\n# pip install sentence_transformers\
    \ InstructorEmbedding faiss-cpu\n\nfrom InstructorEmbedding import INSTRUCTOR\n\
    model = INSTRUCTOR('hkunlp/instructor-large')\n\n# Get your documents \nsentences\
    \ = [\n\"(A) Call Mom \\@Phone +Family\",\n\"(A) Schedule annual checkup +Health\"\
    ,\n\"(B) Outline chapter 5 +Novel \\@Computer\",\n\"(C) Add cover sheets \\@Office\
    \ +TPSReports\",\n\"Plan backyard herb garden \\@Home\",\n\"Pick up milk \\@GroceryStore\"\
    ,\n\"Research self-publishing services +Novel \\@Computer\",\n\"x Download Todo.txt\
    \ mobile app \\@Phone\"\n]\ninstruction = \"Represent the todo.txt item for retrieving\
    \ it\"\n# create the embeddings for the documents\nembeddings = model.encode([[instruction,sentence]\
    \ for sentence in sentences])\n\n# create the embeddings for the prompt\nquestion\
    \ = \"I'm at the store what do i have to buy\"\ninstruction_question = \"Represent\
    \ the question for retrieving a todo.txt item\"\nembeddings_question = model.encode([[instruction_question,\
    \ question]])\n\n# using faiss to store and compute the distances (should work\
    \ with any vector database\nimport faiss                   # make faiss available\n\
    index = faiss.IndexFlatL2(len(embeddings[0]))   # build the index\nindex.add(embeddings)\
    \                  # add vectors to the index\nk = 1                         \
    \ # we want to see nearest neighbor\n\n_, I = index.search(embeddings_question,\
    \ k)     # actual search\ncontext = sentences[int(I[0])] \n\n# build your prompt\n\
    prompt = f\"using this context \\\"{context}\\\" answer the user question: {question}\"\
    \n# creating a dummy llm for example\ndef dummy_llm(prompt):\n    \"\"\"just return\
    \ the prompt\"\"\"\n    return prompt\n\nprint(dummy_llm(prompt))\n```"
  created_at: 2024-01-14 15:58:26+00:00
  edited: false
  hidden: false
  id: 65a404a29f0a34c17300a600
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: hkunlp/instructor-large
repo_type: model
status: open
target_branch: null
title: is INSTRUCTOR embeddings compatible with LLAMA2?
