!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zguo0525
conflicting_files: null
created_at: 2023-07-30 15:36:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg?w=200&h=200&f=face
      fullname: Zhen Guo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zguo0525
      type: user
    createdAt: '2023-07-30T16:36:44.000Z'
    data:
      edited: false
      editors:
      - zguo0525
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.859576940536499
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg?w=200&h=200&f=face
          fullname: Zhen Guo
          isHf: false
          isPro: false
          name: zguo0525
          type: user
        html: '<p>Hey I wonder if anyone has experienced this issue before when loading
          a fine-tuned llama 2 model</p>

          <p>Some weights of the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean
          were not used when initializing LlamaForCausalLM: [''_flat_param'']</p>

          <ul>

          <li>This IS expected if you are initializing LlamaForCausalLM from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).</li>

          <li>This IS NOT expected if you are initializing LlamaForCausalLM from the
          checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).<br>Some weights of LlamaForCausalLM were not initialized from the
          model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean
          and are newly initialized: [''layers.15.self_attn.rotary_emb.inv_freq'',
          ''layers.2.self_attn.k_proj.weight'',<br>You should probably TRAIN this
          model on a down-stream task to be able to use it for predictions and inference.</li>

          </ul>

          '
        raw: "Hey I wonder if anyone has experienced this issue before when loading\
          \ a fine-tuned llama 2 model\r\n\r\nSome weights of the model checkpoint\
          \ at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean were not\
          \ used when initializing LlamaForCausalLM: ['_flat_param']\r\n- This IS\
          \ expected if you are initializing LlamaForCausalLM from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\r\n- This IS NOT expected if you are initializing LlamaForCausalLM\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\r\nSome weights of LlamaForCausalLM were not initialized from\
          \ the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
          \ and are newly initialized: ['layers.15.self_attn.rotary_emb.inv_freq',\
          \ 'layers.2.self_attn.k_proj.weight',\r\nYou should probably TRAIN this\
          \ model on a down-stream task to be able to use it for predictions and inference.\r\
          \n\r\n"
        updatedAt: '2023-07-30T16:36:44.472Z'
      numEdits: 0
      reactions: []
    id: 64c6919c5d0ab485fb1fde79
    type: comment
  author: zguo0525
  content: "Hey I wonder if anyone has experienced this issue before when loading\
    \ a fine-tuned llama 2 model\r\n\r\nSome weights of the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
    \ were not used when initializing LlamaForCausalLM: ['_flat_param']\r\n- This\
    \ IS expected if you are initializing LlamaForCausalLM from the checkpoint of\
    \ a model trained on another task or with another architecture (e.g. initializing\
    \ a BertForSequenceClassification model from a BertForPreTraining model).\r\n\
    - This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint\
    \ of a model that you expect to be exactly identical (initializing a BertForSequenceClassification\
    \ model from a BertForSequenceClassification model).\r\nSome weights of LlamaForCausalLM\
    \ were not initialized from the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
    \ and are newly initialized: ['layers.15.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.k_proj.weight',\r\
    \nYou should probably TRAIN this model on a down-stream task to be able to use\
    \ it for predictions and inference.\r\n\r\n"
  created_at: 2023-07-30 15:36:44+00:00
  edited: false
  hidden: false
  id: 64c6919c5d0ab485fb1fde79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-08-01T06:49:58.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8600323796272278
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>Hey I wonder if anyone has experienced this issue before when loading
          a fine-tuned llama 2 model</p>

          <p>Some weights of the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean
          were not used when initializing LlamaForCausalLM: [''_flat_param'']</p>

          <ul>

          <li>This IS expected if you are initializing LlamaForCausalLM from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).</li>

          <li>This IS NOT expected if you are initializing LlamaForCausalLM from the
          checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).<br>Some weights of LlamaForCausalLM were not initialized from the
          model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean
          and are newly initialized: [''layers.15.self_attn.rotary_emb.inv_freq'',
          ''layers.2.self_attn.k_proj.weight'',<br>You should probably TRAIN this
          model on a down-stream task to be able to use it for predictions and inference.</li>

          </ul>

          </blockquote>

          <p>I have no idea</p>

          '
        raw: "> Hey I wonder if anyone has experienced this issue before when loading\
          \ a fine-tuned llama 2 model\n> \n> Some weights of the model checkpoint\
          \ at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean were not\
          \ used when initializing LlamaForCausalLM: ['_flat_param']\n> - This IS\
          \ expected if you are initializing LlamaForCausalLM from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\n> - This IS NOT expected if you are initializing LlamaForCausalLM\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\n> Some weights of LlamaForCausalLM were not initialized from\
          \ the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
          \ and are newly initialized: ['layers.15.self_attn.rotary_emb.inv_freq',\
          \ 'layers.2.self_attn.k_proj.weight',\n> You should probably TRAIN this\
          \ model on a down-stream task to be able to use it for predictions and inference.\n\
          \nI have no idea"
        updatedAt: '2023-08-01T06:49:58.009Z'
      numEdits: 0
      reactions: []
    id: 64c8ab16ca915ffd302e5ac8
    type: comment
  author: teknium
  content: "> Hey I wonder if anyone has experienced this issue before when loading\
    \ a fine-tuned llama 2 model\n> \n> Some weights of the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
    \ were not used when initializing LlamaForCausalLM: ['_flat_param']\n> - This\
    \ IS expected if you are initializing LlamaForCausalLM from the checkpoint of\
    \ a model trained on another task or with another architecture (e.g. initializing\
    \ a BertForSequenceClassification model from a BertForPreTraining model).\n> -\
    \ This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint\
    \ of a model that you expect to be exactly identical (initializing a BertForSequenceClassification\
    \ model from a BertForSequenceClassification model).\n> Some weights of LlamaForCausalLM\
    \ were not initialized from the model checkpoint at ../../vicuna4tools/output/llama-2-7b-hf/total_IC_train_clean\
    \ and are newly initialized: ['layers.15.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.k_proj.weight',\n\
    > You should probably TRAIN this model on a down-stream task to be able to use\
    \ it for predictions and inference.\n\nI have no idea"
  created_at: 2023-08-01 05:49:58+00:00
  edited: false
  hidden: false
  id: 64c8ab16ca915ffd302e5ac8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Nous-Hermes-llama-2-7b
repo_type: model
status: open
target_branch: null
title: model could not load after finetuning
