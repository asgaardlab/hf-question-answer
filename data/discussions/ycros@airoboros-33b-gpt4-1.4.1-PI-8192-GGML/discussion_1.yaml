!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-07-07 19:25:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-07-07T20:25:46.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9536646008491516
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Brandon''s model customization that you converted in GGML doesn''t
          have the success it deserves. It''s performing better than SuperHot 8k,
          and it''s now my main model, in GPTQ and GGML, for Silly Tavern RP scenarios.<br>Could
          you make a GGML quantization in Q3_KS and share it with us? I don''t have
          an iGPU, and thus, I lack half a gigabyte of VRAM to use the full context
          in Q3_KM when the 63 layers are active in KoboldCPP and without having to
          use "lowvram" parameter.<br>Thanks for your work in any case !</p>

          '
        raw: 'Brandon''s model customization that you converted in GGML doesn''t have
          the success it deserves. It''s performing better than SuperHot 8k, and it''s
          now my main model, in GPTQ and GGML, for Silly Tavern RP scenarios.

          Could you make a GGML quantization in Q3_KS and share it with us? I don''t
          have an iGPU, and thus, I lack half a gigabyte of VRAM to use the full context
          in Q3_KM when the 63 layers are active in KoboldCPP and without having to
          use "lowvram" parameter.

          Thanks for your work in any case !'
        updatedAt: '2023-07-07T20:29:53.753Z'
      numEdits: 4
      reactions: []
    id: 64a874caa347b957197a0240
    type: comment
  author: Nexesenex
  content: 'Brandon''s model customization that you converted in GGML doesn''t have
    the success it deserves. It''s performing better than SuperHot 8k, and it''s now
    my main model, in GPTQ and GGML, for Silly Tavern RP scenarios.

    Could you make a GGML quantization in Q3_KS and share it with us? I don''t have
    an iGPU, and thus, I lack half a gigabyte of VRAM to use the full context in Q3_KM
    when the 63 layers are active in KoboldCPP and without having to use "lowvram"
    parameter.

    Thanks for your work in any case !'
  created_at: 2023-07-07 19:25:46+00:00
  edited: true
  hidden: false
  id: 64a874caa347b957197a0240
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ycros/airoboros-33b-gpt4-1.4.1-PI-8192-GGML
repo_type: model
status: open
target_branch: null
title: Excellent work.
