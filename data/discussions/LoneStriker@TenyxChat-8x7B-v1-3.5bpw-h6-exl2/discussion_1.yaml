!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AUTOMATIC
conflicting_files: null
created_at: 2024-01-20 23:19:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
      fullname: a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AUTOMATIC
      type: user
    createdAt: '2024-01-20T23:19:08.000Z'
    data:
      edited: true
      editors:
      - AUTOMATIC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.907203733921051
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
          fullname: a
          isHf: false
          isPro: false
          name: AUTOMATIC
          type: user
        html: "<p>Hi. There are discussions on our forum about how your quants do\
          \ not perform too well on long contexts. The stated reason is that you use\
          \ a default context length of 2048 when quantizing. Looking at the docs\
          \ in <a rel=\"nofollow\" href=\"https://github.com/turboderp/exllamav2/blob/master/doc/convert.md\"\
          >https://github.com/turboderp/exllamav2/blob/master/doc/convert.md</a>,\
          \ I guess it is a <code>--length</code> parameter. Is that the case? Do\
          \ you have any info about whether this setting has noticeable effect during\
          \ inference?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Hi. There are discussions on our forum about how your quants do not
          perform too well on long contexts. The stated reason is that you use a default
          context length of 2048 when quantizing. Looking at the docs in https://github.com/turboderp/exllamav2/blob/master/doc/convert.md,
          I guess it is a `--length` parameter. Is that the case? Do you have any
          info about whether this setting has noticeable effect during inference?


          @LoneStriker '
        updatedAt: '2024-01-20T23:22:54.334Z'
      numEdits: 1
      reactions: []
    id: 65ac54eccb5b4fb08e6ecc27
    type: comment
  author: AUTOMATIC
  content: 'Hi. There are discussions on our forum about how your quants do not perform
    too well on long contexts. The stated reason is that you use a default context
    length of 2048 when quantizing. Looking at the docs in https://github.com/turboderp/exllamav2/blob/master/doc/convert.md,
    I guess it is a `--length` parameter. Is that the case? Do you have any info about
    whether this setting has noticeable effect during inference?


    @LoneStriker '
  created_at: 2024-01-20 23:19:08+00:00
  edited: true
  hidden: false
  id: 65ac54eccb5b4fb08e6ecc27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-21T09:23:18.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9490650296211243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>In previous discussions with Turboderp on TheBloke''s Discord, the
          length of the calibration did not make a difference in the quality (measured
          by perplexity) of the resulting models.  The lower quant models tend to
          do a bit worse in terms of going off the rails (not staying coherent).  But,
          if there''s a certain context length when the models become incoherent,
          it''s usually down to the model itself and not the quant.  What context
          does this model start breaking down for you?  I''ll load this model at a
          higher quant and test; at 6.0+ quant levels, the quantization itself should
          have little to no impact.</p>

          '
        raw: In previous discussions with Turboderp on TheBloke's Discord, the length
          of the calibration did not make a difference in the quality (measured by
          perplexity) of the resulting models.  The lower quant models tend to do
          a bit worse in terms of going off the rails (not staying coherent).  But,
          if there's a certain context length when the models become incoherent, it's
          usually down to the model itself and not the quant.  What context does this
          model start breaking down for you?  I'll load this model at a higher quant
          and test; at 6.0+ quant levels, the quantization itself should have little
          to no impact.
        updatedAt: '2024-01-21T09:23:18.823Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PapersAnon
    id: 65ace286e0ee7990a6058360
    type: comment
  author: LoneStriker
  content: In previous discussions with Turboderp on TheBloke's Discord, the length
    of the calibration did not make a difference in the quality (measured by perplexity)
    of the resulting models.  The lower quant models tend to do a bit worse in terms
    of going off the rails (not staying coherent).  But, if there's a certain context
    length when the models become incoherent, it's usually down to the model itself
    and not the quant.  What context does this model start breaking down for you?  I'll
    load this model at a higher quant and test; at 6.0+ quant levels, the quantization
    itself should have little to no impact.
  created_at: 2024-01-21 09:23:18+00:00
  edited: false
  hidden: false
  id: 65ace286e0ee7990a6058360
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
      fullname: a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AUTOMATIC
      type: user
    createdAt: '2024-01-21T09:45:57.000Z'
    data:
      edited: false
      editors:
      - AUTOMATIC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9650517702102661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
          fullname: a
          isHf: false
          isPro: false
          name: AUTOMATIC
          type: user
        html: '<p>I was largely fearmongered into using interventis, so I can''t say
          with certainty whether it''s true or not. I didn''t have any visible anomalies
          with this checkpoint when I tested it. Generally, this is about low bpp
          quants of mixtral. 3.5bpp is what fits into my RTX3090. To use any larger
          for mixtral you''d need overpriced enterprise cards.</p>

          '
        raw: I was largely fearmongered into using interventis, so I can't say with
          certainty whether it's true or not. I didn't have any visible anomalies
          with this checkpoint when I tested it. Generally, this is about low bpp
          quants of mixtral. 3.5bpp is what fits into my RTX3090. To use any larger
          for mixtral you'd need overpriced enterprise cards.
        updatedAt: '2024-01-21T09:45:57.503Z'
      numEdits: 0
      reactions: []
    id: 65ace7d527f7d6995a2eaeb6
    type: comment
  author: AUTOMATIC
  content: I was largely fearmongered into using interventis, so I can't say with
    certainty whether it's true or not. I didn't have any visible anomalies with this
    checkpoint when I tested it. Generally, this is about low bpp quants of mixtral.
    3.5bpp is what fits into my RTX3090. To use any larger for mixtral you'd need
    overpriced enterprise cards.
  created_at: 2024-01-21 09:45:57+00:00
  edited: false
  hidden: false
  id: 65ace7d527f7d6995a2eaeb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-21T09:59:13.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9498369097709656
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>There is not much science to the exl2 quants at this stage; if you
          largely stay with the defaults, one quant is going to be similar to the
          next other than the bpw. In general, you do not want to stray from the defaults
          unless you have very good reason to do so; the gains would be minimal or
          possibly downgrade the quantized model.  From the measurements that many
          have done, 3.5bpw seems to be an inflection point where you get a nice jump
          in quality from the lower bpw models.</p>

          <p>If you want to run just slightly higher bpw quants, you can add a second
          GPU into your system (if you have the space, PCIe slot, and power plugs).  Adding
          an 8 GB or 12 GB NVidia GPU will get you to the 4.0bpw models.  2x 3090s
          will get you to 6.0 and 7.0 bpw Mixtral models or 4.0+ bpw 70B models.</p>

          '
        raw: 'There is not much science to the exl2 quants at this stage; if you largely
          stay with the defaults, one quant is going to be similar to the next other
          than the bpw. In general, you do not want to stray from the defaults unless
          you have very good reason to do so; the gains would be minimal or possibly
          downgrade the quantized model.  From the measurements that many have done,
          3.5bpw seems to be an inflection point where you get a nice jump in quality
          from the lower bpw models.


          If you want to run just slightly higher bpw quants, you can add a second
          GPU into your system (if you have the space, PCIe slot, and power plugs).  Adding
          an 8 GB or 12 GB NVidia GPU will get you to the 4.0bpw models.  2x 3090s
          will get you to 6.0 and 7.0 bpw Mixtral models or 4.0+ bpw 70B models.'
        updatedAt: '2024-01-21T09:59:13.969Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ZoneOverreach
    id: 65aceaf1ac588f2a1ccc59e5
    type: comment
  author: LoneStriker
  content: 'There is not much science to the exl2 quants at this stage; if you largely
    stay with the defaults, one quant is going to be similar to the next other than
    the bpw. In general, you do not want to stray from the defaults unless you have
    very good reason to do so; the gains would be minimal or possibly downgrade the
    quantized model.  From the measurements that many have done, 3.5bpw seems to be
    an inflection point where you get a nice jump in quality from the lower bpw models.


    If you want to run just slightly higher bpw quants, you can add a second GPU into
    your system (if you have the space, PCIe slot, and power plugs).  Adding an 8
    GB or 12 GB NVidia GPU will get you to the 4.0bpw models.  2x 3090s will get you
    to 6.0 and 7.0 bpw Mixtral models or 4.0+ bpw 70B models.'
  created_at: 2024-01-21 09:59:13+00:00
  edited: false
  hidden: false
  id: 65aceaf1ac588f2a1ccc59e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-21T10:32:55.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9890079498291016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>The 6.0bpw model seems to be very good. Tested out past 10K context
          without issues (other than one reply that stopped early that I had to tell
          it to continue at around 5K context.)  The 3.5bpw model also seems to be
          coherent.  I continued my conversation past 10K context and it responded
          as well as the 6.0bpw model.</p>

          '
        raw: The 6.0bpw model seems to be very good. Tested out past 10K context without
          issues (other than one reply that stopped early that I had to tell it to
          continue at around 5K context.)  The 3.5bpw model also seems to be coherent.  I
          continued my conversation past 10K context and it responded as well as the
          6.0bpw model.
        updatedAt: '2024-01-21T10:32:55.452Z'
      numEdits: 0
      reactions: []
    id: 65acf2d7e2a2c8635625095f
    type: comment
  author: LoneStriker
  content: The 6.0bpw model seems to be very good. Tested out past 10K context without
    issues (other than one reply that stopped early that I had to tell it to continue
    at around 5K context.)  The 3.5bpw model also seems to be coherent.  I continued
    my conversation past 10K context and it responded as well as the 6.0bpw model.
  created_at: 2024-01-21 10:32:55+00:00
  edited: false
  hidden: false
  id: 65acf2d7e2a2c8635625095f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
      fullname: a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AUTOMATIC
      type: user
    createdAt: '2024-01-21T11:13:59.000Z'
    data:
      edited: false
      editors:
      - AUTOMATIC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244354963302612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478eefcbdcf53e52e5f562bac1b883e9.svg
          fullname: a
          isHf: false
          isPro: false
          name: AUTOMATIC
          type: user
        html: '<p>I loaded the longest RP I had, 21k tokens, and asked the character
          questions about events from the beginning and the middle. Tested on <code>LoneStriker_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2</code>
          and <code>intervitens_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2-rpcal</code>.
          Both checkpoints mostly answered correctly (sometimes making mistakes in
          small details), and both were able to continue the chat without degradation.
          It looks to me like the fears are unfounded then.</p>

          <p>Thanks for your work.</p>

          '
        raw: 'I loaded the longest RP I had, 21k tokens, and asked the character questions
          about events from the beginning and the middle. Tested on `LoneStriker_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2`
          and `intervitens_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2-rpcal`. Both
          checkpoints mostly answered correctly (sometimes making mistakes in small
          details), and both were able to continue the chat without degradation. It
          looks to me like the fears are unfounded then.


          Thanks for your work.'
        updatedAt: '2024-01-21T11:13:59.760Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ZoneOverreach
    id: 65acfc77c2eef2ba11352026
    type: comment
  author: AUTOMATIC
  content: 'I loaded the longest RP I had, 21k tokens, and asked the character questions
    about events from the beginning and the middle. Tested on `LoneStriker_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2`
    and `intervitens_Mixtral-8x7B-Instruct-v0.1-3.5bpw-h6-exl2-rpcal`. Both checkpoints
    mostly answered correctly (sometimes making mistakes in small details), and both
    were able to continue the chat without degradation. It looks to me like the fears
    are unfounded then.


    Thanks for your work.'
  created_at: 2024-01-21 11:13:59+00:00
  edited: false
  hidden: false
  id: 65acfc77c2eef2ba11352026
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/TenyxChat-8x7B-v1-3.5bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Question about quantization settings
