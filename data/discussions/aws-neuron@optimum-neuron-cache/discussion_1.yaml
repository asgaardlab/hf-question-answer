!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jburtoft
conflicting_files: null
created_at: 2023-12-29 03:32:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2023-12-29T03:32:00.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9140443205833435
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: '<p>I know this feature is designed for use with training, but it seems
          like the same process could be used for inference.  </p>

          <p>As it is, if I want to use a pre-compiled model, I need to create a "model"
          on Hugging Face for every compilation option and core count that I could
          want.  </p>

          <p>For example, <em>meta-llama/Llama-2-7b-hf</em> is the main model, but
          we have compiled versions <em>aws-neuron/Llama-2-7b-hf-neuron-budget</em>
          and <em>aws-neuron/Llama-2-7b-hf-neuron-throughput</em>, all the same model,
          just with a different compiled batch size and number of cores.</p>

          <p>It sure would be sweet if we could just reference the original model
          with the arguments we want and it would grab it precompiled if it was already
          there.  Otherwise, we are going to end up with a lot of different "models"
          for compilation options on Llama-2 versions, CodeLama versions, Mistral
          versions...</p>

          '
        raw: "I know this feature is designed for use with training, but it seems\
          \ like the same process could be used for inference.  \r\n\r\nAs it is,\
          \ if I want to use a pre-compiled model, I need to create a \"model\" on\
          \ Hugging Face for every compilation option and core count that I could\
          \ want.  \r\n\r\nFor example, *meta-llama/Llama-2-7b-hf* is the main model,\
          \ but we have compiled versions *aws-neuron/Llama-2-7b-hf-neuron-budget*\
          \ and *aws-neuron/Llama-2-7b-hf-neuron-throughput*, all the same model,\
          \ just with a different compiled batch size and number of cores.\r\n\r\n\
          It sure would be sweet if we could just reference the original model with\
          \ the arguments we want and it would grab it precompiled if it was already\
          \ there.  Otherwise, we are going to end up with a lot of different \"models\"\
          \ for compilation options on Llama-2 versions, CodeLama versions, Mistral\
          \ versions..."
        updatedAt: '2023-12-29T03:32:00.492Z'
      numEdits: 0
      reactions: []
    id: 658e3db0674349122cd80692
    type: comment
  author: jburtoft
  content: "I know this feature is designed for use with training, but it seems like\
    \ the same process could be used for inference.  \r\n\r\nAs it is, if I want to\
    \ use a pre-compiled model, I need to create a \"model\" on Hugging Face for every\
    \ compilation option and core count that I could want.  \r\n\r\nFor example, *meta-llama/Llama-2-7b-hf*\
    \ is the main model, but we have compiled versions *aws-neuron/Llama-2-7b-hf-neuron-budget*\
    \ and *aws-neuron/Llama-2-7b-hf-neuron-throughput*, all the same model, just with\
    \ a different compiled batch size and number of cores.\r\n\r\nIt sure would be\
    \ sweet if we could just reference the original model with the arguments we want\
    \ and it would grab it precompiled if it was already there.  Otherwise, we are\
    \ going to end up with a lot of different \"models\" for compilation options on\
    \ Llama-2 versions, CodeLama versions, Mistral versions..."
  created_at: 2023-12-29 03:32:00+00:00
  edited: false
  hidden: false
  id: 658e3db0674349122cd80692
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-20T05:02:42.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.802050769329071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: '<p>Apparently, life is sweet.  Thank you.</p>

          <p><a rel="nofollow" href="https://moon-ci-docs.huggingface.co/docs/optimum-neuron/pr_429/en/guides/cache_system">https://moon-ci-docs.huggingface.co/docs/optimum-neuron/pr_429/en/guides/cache_system</a></p>

          '
        raw: 'Apparently, life is sweet.  Thank you.


          https://moon-ci-docs.huggingface.co/docs/optimum-neuron/pr_429/en/guides/cache_system'
        updatedAt: '2024-01-20T05:02:42.383Z'
      numEdits: 0
      reactions: []
    id: 65ab53f2356bf23b4ab8cc7f
    type: comment
  author: jburtoft
  content: 'Apparently, life is sweet.  Thank you.


    https://moon-ci-docs.huggingface.co/docs/optimum-neuron/pr_429/en/guides/cache_system'
  created_at: 2024-01-20 05:02:42+00:00
  edited: false
  hidden: false
  id: 65ab53f2356bf23b4ab8cc7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-22T15:52:41.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9764689207077026
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>You''re welcome. This was indeed a much needed feature for inference
          also.</p>

          '
        raw: You're welcome. This was indeed a much needed feature for inference also.
        updatedAt: '2024-01-22T15:52:41.130Z'
      numEdits: 0
      reactions: []
    id: 65ae8f4977ce425d18e39794
    type: comment
  author: dacorvo
  content: You're welcome. This was indeed a much needed feature for inference also.
  created_at: 2024-01-22 15:52:41+00:00
  edited: false
  hidden: false
  id: 65ae8f4977ce425d18e39794
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: aws-neuron/optimum-neuron-cache
repo_type: model
status: open
target_branch: null
title: Optimum-neuron-cache for inference?
