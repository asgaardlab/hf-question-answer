!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jini1114
conflicting_files: null
created_at: 2023-04-07 04:35:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
      fullname: Jineui Kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jini1114
      type: user
    createdAt: '2023-04-07T05:35:55.000Z'
    data:
      edited: true
      editors:
      - jini1114
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
          fullname: Jineui Kim
          isHf: false
          isPro: false
          name: jini1114
          type: user
        html: "<p>i've clone this repository and run with llama_inference.py in GPTQ-for-LLaMa.</p>\n\
          <pre><code>CUDA_VISIBLE_DEVICES=1 python llama_inference.py /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g\
          \ --wbits 4 --groupsize 128 --load /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\
          \ --text \"this is llama\"\n</code></pre>\n<p>i tried both of triton branch\
          \ and cuda branch.<br>but i got same error below.</p>\n<pre><code>Loading\
          \ model ...\nDone.\nTraceback (most recent call last):\n  File \"/home/user/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 125, in &lt;module&gt;\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n\
          \  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 700, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 74, in __init__\n    super().__init__(\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 120, in __init__\n    raise ValueError(\nValueError: Couldn't instantiate\
          \ the backend tokenizer from one of: \n(1) a `tokenizers` library serialization\
          \ file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
          \ installed to convert a slow tokenizer to a fast one.\n</code></pre>\n\
          <p>i try to find the solution.<br>They said \"pip install sentencepiece\"\
          \ but i already install sentencepiece.<br>so i don't know what is the reason\
          \ of this error.<br>could you give the hint for me?</p>\n"
        raw: "i've clone this repository and run with llama_inference.py in GPTQ-for-LLaMa.\n\
          ```\nCUDA_VISIBLE_DEVICES=1 python llama_inference.py /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g\
          \ --wbits 4 --groupsize 128 --load /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\
          \ --text \"this is llama\"\n```\n\ni tried both of triton branch and cuda\
          \ branch.\nbut i got same error below.\n```\nLoading model ...\nDone.\n\
          Traceback (most recent call last):\n  File \"/home/user/GPTQ-for-LLaMa/llama_inference.py\"\
          , line 125, in <module>\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n\
          \  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 700, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 74, in __init__\n    super().__init__(\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 120, in __init__\n    raise ValueError(\nValueError: Couldn't instantiate\
          \ the backend tokenizer from one of: \n(1) a `tokenizers` library serialization\
          \ file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
          \ installed to convert a slow tokenizer to a fast one.\n```\n\ni try to\
          \ find the solution.\nThey said \"pip install sentencepiece\" but i already\
          \ install sentencepiece.\nso i don't know what is the reason of this error.\n\
          could you give the hint for me?"
        updatedAt: '2023-04-07T06:21:48.658Z'
      numEdits: 1
      reactions: []
    id: 642fabbb4e7b53c5addd4a77
    type: comment
  author: jini1114
  content: "i've clone this repository and run with llama_inference.py in GPTQ-for-LLaMa.\n\
    ```\nCUDA_VISIBLE_DEVICES=1 python llama_inference.py /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g\
    \ --wbits 4 --groupsize 128 --load /home/user/GPTQ-for-LLaMa/model/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\
    \ --text \"this is llama\"\n```\n\ni tried both of triton branch and cuda branch.\n\
    but i got same error below.\n```\nLoading model ...\nDone.\nTraceback (most recent\
    \ call last):\n  File \"/home/user/GPTQ-for-LLaMa/llama_inference.py\", line 125,\
    \ in <module>\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n  File\
    \ \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 700, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\n    return cls._from_pretrained(\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1965, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
    , line 74, in __init__\n    super().__init__(\n  File \"/home/user/anaconda3/envs/gptq/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 120, in __init__\n    raise ValueError(\nValueError: Couldn't instantiate\
    \ the backend tokenizer from one of: \n(1) a `tokenizers` library serialization\
    \ file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow\
    \ tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
    \ installed to convert a slow tokenizer to a fast one.\n```\n\ni try to find the\
    \ solution.\nThey said \"pip install sentencepiece\" but i already install sentencepiece.\n\
    so i don't know what is the reason of this error.\ncould you give the hint for\
    \ me?"
  created_at: 2023-04-07 04:35:55+00:00
  edited: true
  hidden: false
  id: 642fabbb4e7b53c5addd4a77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04d3ec5963c7d6d4628dd5c42d5ee112.svg
      fullname: novemberChopin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: novemberChopin
      type: user
    createdAt: '2023-04-07T11:10:20.000Z'
    data:
      edited: false
      editors:
      - novemberChopin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04d3ec5963c7d6d4628dd5c42d5ee112.svg
          fullname: novemberChopin
          isHf: false
          isPro: false
          name: novemberChopin
          type: user
        html: '<p>You can check this issue<br><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/829">https://github.com/oobabooga/text-generation-webui/issues/829</a></p>

          '
        raw: 'You can check this issue

          https://github.com/oobabooga/text-generation-webui/issues/829'
        updatedAt: '2023-04-07T11:10:20.650Z'
      numEdits: 0
      reactions: []
    id: 642ffa1ca98189940080761b
    type: comment
  author: novemberChopin
  content: 'You can check this issue

    https://github.com/oobabooga/text-generation-webui/issues/829'
  created_at: 2023-04-07 10:10:20+00:00
  edited: false
  hidden: false
  id: 642ffa1ca98189940080761b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: How can i use this model with GPTQ-for-LLaMa?
