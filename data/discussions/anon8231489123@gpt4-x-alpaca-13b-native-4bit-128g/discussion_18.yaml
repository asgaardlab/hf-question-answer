!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Said2k
conflicting_files: null
created_at: 2023-04-10 05:29:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-04-10T06:29:37.000Z'
    data:
      edited: false
      editors:
      - Said2k
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: Said2k
          type: user
        html: '<p>OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
          or flax_model.msgpack found in directory models\llama-13b-4bit-128g.</p>

          '
        raw: 'OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
          or flax_model.msgpack found in directory models\llama-13b-4bit-128g.'
        updatedAt: '2023-04-10T06:29:37.555Z'
      numEdits: 0
      reactions: []
    id: 6433acd1546e16f17a0f2373
    type: comment
  author: Said2k
  content: 'OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
    or flax_model.msgpack found in directory models\llama-13b-4bit-128g.'
  created_at: 2023-04-10 05:29:37+00:00
  edited: false
  hidden: false
  id: 6433acd1546e16f17a0f2373
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
      fullname: Ben Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benfy
      type: user
    createdAt: '2023-04-10T16:49:35.000Z'
    data:
      edited: false
      editors:
      - benfy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
          fullname: Ben Ong
          isHf: false
          isPro: false
          name: benfy
          type: user
        html: '<p>Modify your start-webui with this line:<br>call python server.py
          --auto-devices --chat --wbits 4 --groupsize 128</p>

          '
        raw: 'Modify your start-webui with this line:

          call python server.py --auto-devices --chat --wbits 4 --groupsize 128'
        updatedAt: '2023-04-10T16:49:35.813Z'
      numEdits: 0
      reactions: []
    id: 64343e1f2a81c2a4847e893e
    type: comment
  author: benfy
  content: 'Modify your start-webui with this line:

    call python server.py --auto-devices --chat --wbits 4 --groupsize 128'
  created_at: 2023-04-10 15:49:35+00:00
  edited: false
  hidden: false
  id: 64343e1f2a81c2a4847e893e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-04-10T18:34:45.000Z'
    data:
      edited: true
      editors:
      - Said2k
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: Said2k
          type: user
        html: '<p>I got this Error: </p>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00
          MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes
          free; 7.32 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>Output
          generated in 13.07 seconds (0.00 tokens/s, 0 tokens, context 43)</p>

          <p>I have 64GB of RAM and 8GB of VRAM.</p>

          '
        raw: "I got this Error: \n\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already\
          \ allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch) If reserved\
          \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          Output generated in 13.07 seconds (0.00 tokens/s, 0 tokens, context 43)\n\
          \nI have 64GB of RAM and 8GB of VRAM."
        updatedAt: '2023-04-10T18:35:22.305Z'
      numEdits: 1
      reactions: []
    id: 643456c5d72427b82480ac7f
    type: comment
  author: Said2k
  content: "I got this Error: \n\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
    \ Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already\
    \ allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch) If reserved\
    \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nOutput\
    \ generated in 13.07 seconds (0.00 tokens/s, 0 tokens, context 43)\n\nI have 64GB\
    \ of RAM and 8GB of VRAM."
  created_at: 2023-04-10 17:34:45+00:00
  edited: true
  hidden: false
  id: 643456c5d72427b82480ac7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-04-10T18:38:51.000Z'
    data:
      from: I'm trying to run this using oobabooga but it's not recognizing it.
      to: I'm trying to run this using oobabooga but I'm getting an OOM Error.
    id: 643457bb8d68561d7050383f
    type: title-change
  author: Said2k
  created_at: 2023-04-10 17:38:51+00:00
  id: 643457bb8d68561d7050383f
  new_title: I'm trying to run this using oobabooga but I'm getting an OOM Error.
  old_title: I'm trying to run this using oobabooga but it's not recognizing it.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d663aa45b72d3d68c5e9fedee71bf3d.svg
      fullname: mrshadowccg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrshadowccg
      type: user
    createdAt: '2023-04-10T20:45:11.000Z'
    data:
      edited: true
      editors:
      - mrshadowccg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d663aa45b72d3d68c5e9fedee71bf3d.svg
          fullname: mrshadowccg
          isHf: false
          isPro: false
          name: mrshadowccg
          type: user
        html: '<p>Someone mentioned on oobabooga''s repository issues that you need
          to also use the "pre_layer" flags in order to not completely allocate your
          GPU with the model and allow part of its VRAM to be used for text generation.
          The higher the "pre_layer" number, the faster the model will respond but
          also the more likely it''ll run out of VRAM. I used my "pre_layer" parameter
          on value 26 so it''s a bit slow but still manageable. Depending on how big
          the text history is, VRAM may still run out, I tried messing with parameters
          but still no success so far. Anyways the line should look like this:</p>

          <p>call python server.py --auto-devices --chat --wbits 4 --groupsize 128
          --pre_layer 26</p>

          <p>Note: If anyone reading this is getting CPU out of memory (not GPU),
          try increasing the virtual memory on your OS to over 100GB.</p>

          '
        raw: 'Someone mentioned on oobabooga''s repository issues that you need to
          also use the "pre_layer" flags in order to not completely allocate your
          GPU with the model and allow part of its VRAM to be used for text generation.
          The higher the "pre_layer" number, the faster the model will respond but
          also the more likely it''ll run out of VRAM. I used my "pre_layer" parameter
          on value 26 so it''s a bit slow but still manageable. Depending on how big
          the text history is, VRAM may still run out, I tried messing with parameters
          but still no success so far. Anyways the line should look like this:


          call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --pre_layer
          26


          Note: If anyone reading this is getting CPU out of memory (not GPU), try
          increasing the virtual memory on your OS to over 100GB.'
        updatedAt: '2023-04-10T20:48:02.177Z'
      numEdits: 1
      reactions: []
    id: 643475571ceac17d773b1641
    type: comment
  author: mrshadowccg
  content: 'Someone mentioned on oobabooga''s repository issues that you need to also
    use the "pre_layer" flags in order to not completely allocate your GPU with the
    model and allow part of its VRAM to be used for text generation. The higher the
    "pre_layer" number, the faster the model will respond but also the more likely
    it''ll run out of VRAM. I used my "pre_layer" parameter on value 26 so it''s a
    bit slow but still manageable. Depending on how big the text history is, VRAM
    may still run out, I tried messing with parameters but still no success so far.
    Anyways the line should look like this:


    call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --pre_layer
    26


    Note: If anyone reading this is getting CPU out of memory (not GPU), try increasing
    the virtual memory on your OS to over 100GB.'
  created_at: 2023-04-10 19:45:11+00:00
  edited: true
  hidden: false
  id: 643475571ceac17d773b1641
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
      fullname: Ben Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benfy
      type: user
    createdAt: '2023-04-11T02:06:20.000Z'
    data:
      edited: false
      editors:
      - benfy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
          fullname: Ben Ong
          isHf: false
          isPro: false
          name: benfy
          type: user
        html: '<p>Try again with this line:<br>call python server.py --auto-devices
          --chat --wbits 4 --groupsize 128 --gpu-memory 7 --pre_layer 19</p>

          <p>This will limit the amount of VRAM usage, works on my RTX 3070 w/8GB
          VRAM.</p>

          '
        raw: 'Try again with this line:

          call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory
          7 --pre_layer 19


          This will limit the amount of VRAM usage, works on my RTX 3070 w/8GB VRAM.'
        updatedAt: '2023-04-11T02:06:20.503Z'
      numEdits: 0
      reactions: []
    id: 6434c09cb1d842fdbe8090f5
    type: comment
  author: benfy
  content: 'Try again with this line:

    call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory
    7 --pre_layer 19


    This will limit the amount of VRAM usage, works on my RTX 3070 w/8GB VRAM.'
  created_at: 2023-04-11 01:06:20+00:00
  edited: false
  hidden: false
  id: 6434c09cb1d842fdbe8090f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-04-11T07:44:01.000Z'
    data:
      edited: false
      editors:
      - Said2k
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: Said2k
          type: user
        html: '<p>I''m getting an average of 0.17 tokens/second, is this normal?</p>

          '
        raw: I'm getting an average of 0.17 tokens/second, is this normal?
        updatedAt: '2023-04-11T07:44:01.535Z'
      numEdits: 0
      reactions: []
    id: 64350fc1e60b21004ec98ad7
    type: comment
  author: Said2k
  content: I'm getting an average of 0.17 tokens/second, is this normal?
  created_at: 2023-04-11 06:44:01+00:00
  edited: false
  hidden: false
  id: 64350fc1e60b21004ec98ad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-04-11T07:47:10.000Z'
    data:
      from: I'm trying to run this using oobabooga but I'm getting an OOM Error.
      to: I'm trying to run this using oobabooga but I'm getting 0.17 tokens/second.
    id: 6435107ee60b21004ec992e8
    type: title-change
  author: Said2k
  created_at: 2023-04-11 06:47:10+00:00
  id: 6435107ee60b21004ec992e8
  new_title: I'm trying to run this using oobabooga but I'm getting 0.17 tokens/second.
  old_title: I'm trying to run this using oobabooga but I'm getting an OOM Error.
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: I'm trying to run this using oobabooga but I'm getting 0.17 tokens/second.
