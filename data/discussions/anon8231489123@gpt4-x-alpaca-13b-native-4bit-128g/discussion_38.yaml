!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kralos-R
conflicting_files: null
created_at: 2023-04-20 05:43:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75c6d83c7faa08e56f3dbc29805725d2.svg
      fullname: Carlos Rodriguez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kralos-R
      type: user
    createdAt: '2023-04-20T06:43:01.000Z'
    data:
      edited: false
      editors:
      - Kralos-R
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75c6d83c7faa08e56f3dbc29805725d2.svg
          fullname: Carlos Rodriguez
          isHf: false
          isPro: false
          name: Kralos-R
          type: user
        html: '<h1 id="hello-anon-i-came-across-your-repository-not-so-long-ago-and-i-wanted-to-use-it-in-a-python-application-however-i-havent-got-luck-on-my-side-and-keep-getting-this-error">Hello
          anon! I came across your repository not so long ago and I wanted to use
          it in a Python application, however I haven''t got luck on my side and keep
          getting this error:</h1>

          <p>OSError: Could not locate pytorch_model-00001-of-00006.bin inside anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g</p>

          <h1 id="this-is-the-code-im-currently-using-if-by-any-chance-you-would-need-it-when-resolving-this-issue">This
          is the code I''m currently using if by any chance you would need it when
          resolving this issue:</h1>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>rep= "anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g"<br>tokenizer
          = AutoTokenizer.from_pretrained(rep)<br>model = AutoModelForCausalLM.from_pretrained(rep)</p>

          <p>inputs = tokenizer(["Today is"], return_tensors="pt")</p>

          <p>reply_ids = model.generate(**inputs, max_new_tokens=590)  # return_dict_in_generate=True,
          output_scores=True<br>outputs = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]<br>print(outputs)</p>

          '
        raw: "# Hello anon! I came across your repository not so long ago and I wanted\
          \ to use it in a Python application, however I haven't got luck on my side\
          \ and keep getting this error:\r\nOSError: Could not locate pytorch_model-00001-of-00006.bin\
          \ inside anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\r\n\r\n\r\n#\
          \ This is the code I'm currently using if by any chance you would need it\
          \ when resolving this issue:\r\n\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\n\r\nrep= \"anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(rep)\r\nmodel = AutoModelForCausalLM.from_pretrained(rep)\r\
          \n\r\ninputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\r\n\r\n\
          reply_ids = model.generate(**inputs, max_new_tokens=590)  # return_dict_in_generate=True,\
          \ output_scores=True\r\noutputs = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\r\
          \nprint(outputs)\r\n\r\n\r\n"
        updatedAt: '2023-04-20T06:43:01.987Z'
      numEdits: 0
      reactions: []
    id: 6440def55d600fb09518e1d6
    type: comment
  author: Kralos-R
  content: "# Hello anon! I came across your repository not so long ago and I wanted\
    \ to use it in a Python application, however I haven't got luck on my side and\
    \ keep getting this error:\r\nOSError: Could not locate pytorch_model-00001-of-00006.bin\
    \ inside anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\r\n\r\n\r\n# This is\
    \ the code I'm currently using if by any chance you would need it when resolving\
    \ this issue:\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \n\r\nrep= \"anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\"\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(rep)\r\nmodel = AutoModelForCausalLM.from_pretrained(rep)\r\
    \n\r\ninputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\r\n\r\nreply_ids\
    \ = model.generate(**inputs, max_new_tokens=590)  # return_dict_in_generate=True,\
    \ output_scores=True\r\noutputs = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\r\
    \nprint(outputs)\r\n\r\n\r\n"
  created_at: 2023-04-20 05:43:01+00:00
  edited: false
  hidden: false
  id: 6440def55d600fb09518e1d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/370edbd950f19db47140926db0850994.svg
      fullname: Seter Teeri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faaaaaaaaaaaa
      type: user
    createdAt: '2023-05-01T20:47:57.000Z'
    data:
      edited: false
      editors:
      - faaaaaaaaaaaa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/370edbd950f19db47140926db0850994.svg
          fullname: Seter Teeri
          isHf: false
          isPro: false
          name: faaaaaaaaaaaa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Kralos-R&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Kralos-R\">@<span class=\"\
          underline\">Kralos-R</span></a></span>\n\n\t</span></span> for the from_pretrained\
          \ try passing in the full path to the .pt file</p>\n"
        raw: '@Kralos-R for the from_pretrained try passing in the full path to the
          .pt file'
        updatedAt: '2023-05-01T20:47:57.189Z'
      numEdits: 0
      reactions: []
    id: 6450257d28774bd665df6c71
    type: comment
  author: faaaaaaaaaaaa
  content: '@Kralos-R for the from_pretrained try passing in the full path to the
    .pt file'
  created_at: 2023-05-01 19:47:57+00:00
  edited: false
  hidden: false
  id: 6450257d28774bd665df6c71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MBa4fzzoLU5t9qla-RrRE.jpeg?w=200&h=200&f=face
      fullname: Robert E Billings
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ditchdigger
      type: user
    createdAt: '2023-05-21T02:41:54.000Z'
    data:
      edited: false
      editors:
      - Ditchdigger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/MBa4fzzoLU5t9qla-RrRE.jpeg?w=200&h=200&f=face
          fullname: Robert E Billings
          isHf: false
          isPro: false
          name: Ditchdigger
          type: user
        html: "<p>I'm having the same problem as <span data-props=\"{&quot;user&quot;:&quot;Kralos-R&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Kralos-R\"\
          >@<span class=\"underline\">Kralos-R</span></a></span>\n\n\t</span></span>.\
          \ I tried the solution from <span data-props=\"{&quot;user&quot;:&quot;faaaaaaaaaaaa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/faaaaaaaaaaaa\"\
          >@<span class=\"underline\">faaaaaaaaaaaa</span></a></span>\n\n\t</span></span>\
          \ by giving from_pretrained the full path to the .pt file but transformers\
          \ seems to still think it's supposed to be a Repo ID. Here's the error I\
          \ get:</p>\n<p>huggingface_hub.utils.<em>validators.HFValidationError: Repo\
          \ id must use alphanumeric chars or '-', '</em>', '.', '--' and '..' are\
          \ forbidden, '-' and '.' cannot start or end the name, max length is 96:\
          \ '\\programming\\openai\\models\\gpt4-x-alpaca-13b-native-4bit-128g\\gpt-x-alpaca-13b-native-4bit-128g.pt'.</p>\n\
          <p>My code up to the point where it spits that error message looks like\
          \ this.  (The print line about the file path outputs the correct path and\
          \ says the file exists.)</p>\n<p>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM<br>from pathlib import Path</p>\n<p>modelpath = Path(\"\
          /programming/openai/models/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\"\
          )<br>print(\"Model path: \", modelpath.absolute(), \"Exists: \", modelpath.exists())</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(modelpath)</p>\n<p>I even tried\
          \ moving the file to the same directory as my .py file and renaming it to\
          \ just \"alpaca\" to get rid of the \".\" and \"/\" and stuff, but then\
          \ it tried to download it from Huggingface Hub:</p>\n<p>requests.exceptions.HTTPError:\
          \ 401 Client Error: Unauthorized for url: <a href=\"https://huggingface.co/alpaca/resolve/main/tokenizer_config.json\"\
          >https://huggingface.co/alpaca/resolve/main/tokenizer_config.json</a></p>\n\
          <p>Any suggestions are appreciated!</p>\n"
        raw: 'I''m having the same problem as @Kralos-R. I tried the solution from
          @faaaaaaaaaaaa by giving from_pretrained the full path to the .pt file but
          transformers seems to still think it''s supposed to be a Repo ID. Here''s
          the error I get:


          huggingface_hub.utils._validators.HFValidationError: Repo id must use alphanumeric
          chars or ''-'', ''_'', ''.'', ''--'' and ''..'' are forbidden, ''-'' and
          ''.'' cannot start or end the name, max length is 96: ''\programming\openai\models\gpt4-x-alpaca-13b-native-4bit-128g\gpt-x-alpaca-13b-native-4bit-128g.pt''.


          My code up to the point where it spits that error message looks like this.  (The
          print line about the file path outputs the correct path and says the file
          exists.)


          from transformers import AutoTokenizer, AutoModelForCausalLM

          from pathlib import Path


          modelpath = Path("/programming/openai/models/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt")

          print("Model path: ", modelpath.absolute(), "Exists: ", modelpath.exists())


          tokenizer = AutoTokenizer.from_pretrained(modelpath)


          I even tried moving the file to the same directory as my .py file and renaming
          it to just "alpaca" to get rid of the "." and "/" and stuff, but then it
          tried to download it from Huggingface Hub:


          requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/alpaca/resolve/main/tokenizer_config.json


          Any suggestions are appreciated!'
        updatedAt: '2023-05-21T02:41:54.873Z'
      numEdits: 0
      reactions: []
    id: 646984f297ffc33d43d1974f
    type: comment
  author: Ditchdigger
  content: 'I''m having the same problem as @Kralos-R. I tried the solution from @faaaaaaaaaaaa
    by giving from_pretrained the full path to the .pt file but transformers seems
    to still think it''s supposed to be a Repo ID. Here''s the error I get:


    huggingface_hub.utils._validators.HFValidationError: Repo id must use alphanumeric
    chars or ''-'', ''_'', ''.'', ''--'' and ''..'' are forbidden, ''-'' and ''.''
    cannot start or end the name, max length is 96: ''\programming\openai\models\gpt4-x-alpaca-13b-native-4bit-128g\gpt-x-alpaca-13b-native-4bit-128g.pt''.


    My code up to the point where it spits that error message looks like this.  (The
    print line about the file path outputs the correct path and says the file exists.)


    from transformers import AutoTokenizer, AutoModelForCausalLM

    from pathlib import Path


    modelpath = Path("/programming/openai/models/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt")

    print("Model path: ", modelpath.absolute(), "Exists: ", modelpath.exists())


    tokenizer = AutoTokenizer.from_pretrained(modelpath)


    I even tried moving the file to the same directory as my .py file and renaming
    it to just "alpaca" to get rid of the "." and "/" and stuff, but then it tried
    to download it from Huggingface Hub:


    requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/alpaca/resolve/main/tokenizer_config.json


    Any suggestions are appreciated!'
  created_at: 2023-05-21 01:41:54+00:00
  edited: false
  hidden: false
  id: 646984f297ffc33d43d1974f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: Issues running the model in python
