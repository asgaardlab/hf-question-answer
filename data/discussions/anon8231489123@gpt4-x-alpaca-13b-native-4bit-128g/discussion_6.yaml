!!python/object:huggingface_hub.community.DiscussionWithDetails
author: blueisbest
conflicting_files: null
created_at: 2023-04-08 21:44:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a74e60cf0467900b822b23ddcfb3e01.svg
      fullname: "Petr Kejkl\xED\u010Dek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blueisbest
      type: user
    createdAt: '2023-04-08T22:44:52.000Z'
    data:
      edited: false
      editors:
      - blueisbest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a74e60cf0467900b822b23ddcfb3e01.svg
          fullname: "Petr Kejkl\xED\u010Dek"
          isHf: false
          isPro: false
          name: blueisbest
          type: user
        html: '<p>Hey! I dont really know what I did, Imma just post the error I got.
          I also followed this tutorial: <a rel="nofollow" href="https://youtu.be/nVC9D9fRyNU">https://youtu.be/nVC9D9fRyNU</a></p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\admin\Desktop\oobabooga-windows\text-generation-webui\server.py",
          line 302, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\Users\admin\Desktop\oobabooga-windows\text-generation-webui\modules\models.py",
          line 100, in load_model<br>    from modules.GPTQ_loader import load_quantized<br>  File
          "C:\Users\admin\Desktop\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 14, in <br>    import llama_inference_offload<br>ModuleNotFoundError:
          No module named ''llama_inference_offload''</p>

          '
        raw: "Hey! I dont really know what I did, Imma just post the error I got.\
          \ I also followed this tutorial: https://youtu.be/nVC9D9fRyNU\r\n\r\n\r\n\
          Traceback (most recent call last):\r\n  File \"C:\\Users\\admin\\Desktop\\\
          oobabooga-windows\\text-generation-webui\\server.py\", line 302, in <module>\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\n\
          \  File \"C:\\Users\\admin\\Desktop\\oobabooga-windows\\text-generation-webui\\\
          modules\\models.py\", line 100, in load_model\r\n    from modules.GPTQ_loader\
          \ import load_quantized\r\n  File \"C:\\Users\\admin\\Desktop\\oobabooga-windows\\\
          text-generation-webui\\modules\\GPTQ_loader.py\", line 14, in <module>\r\
          \n    import llama_inference_offload\r\nModuleNotFoundError: No module named\
          \ 'llama_inference_offload'"
        updatedAt: '2023-04-08T22:44:52.654Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - danielgangl
    id: 6431ee64dec2a70d8134bcaa
    type: comment
  author: blueisbest
  content: "Hey! I dont really know what I did, Imma just post the error I got. I\
    \ also followed this tutorial: https://youtu.be/nVC9D9fRyNU\r\n\r\n\r\nTraceback\
    \ (most recent call last):\r\n  File \"C:\\Users\\admin\\Desktop\\oobabooga-windows\\\
    text-generation-webui\\server.py\", line 302, in <module>\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\Users\\admin\\\
    Desktop\\oobabooga-windows\\text-generation-webui\\modules\\models.py\", line\
    \ 100, in load_model\r\n    from modules.GPTQ_loader import load_quantized\r\n\
    \  File \"C:\\Users\\admin\\Desktop\\oobabooga-windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 14, in <module>\r\n    import llama_inference_offload\r\
    \nModuleNotFoundError: No module named 'llama_inference_offload'"
  created_at: 2023-04-08 21:44:52+00:00
  edited: false
  hidden: false
  id: 6431ee64dec2a70d8134bcaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7846955723c105caaa7d2cd14106ce4.svg
      fullname: Don Beckham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dabeckham
      type: user
    createdAt: '2023-04-08T23:46:54.000Z'
    data:
      edited: false
      editors:
      - dabeckham
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7846955723c105caaa7d2cd14106ce4.svg
          fullname: Don Beckham
          isHf: false
          isPro: false
          name: dabeckham
          type: user
        html: '<p>Watch the video again and follow the directions more carefully.
          </p>

          <p>Considering  your title, "Error using ooba-gooba", your key clue should
          be:<br>import llama_inference_offload<br>ModuleNotFoundError: No module
          named ''llama_inference_offload''</p>

          '
        raw: "Watch the video again and follow the directions more carefully. \n\n\
          Considering  your title, \"Error using ooba-gooba\", your key clue should\
          \ be:\nimport llama_inference_offload\nModuleNotFoundError: No module named\
          \ 'llama_inference_offload'"
        updatedAt: '2023-04-08T23:46:54.898Z'
      numEdits: 0
      reactions: []
    id: 6431fcee46d9d4f9d89b5908
    type: comment
  author: dabeckham
  content: "Watch the video again and follow the directions more carefully. \n\nConsidering\
    \  your title, \"Error using ooba-gooba\", your key clue should be:\nimport llama_inference_offload\n\
    ModuleNotFoundError: No module named 'llama_inference_offload'"
  created_at: 2023-04-08 22:46:54+00:00
  edited: false
  hidden: false
  id: 6431fcee46d9d4f9d89b5908
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4864005b1ee07f34f90cc2b291453033.svg
      fullname: Alydex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alydex
      type: user
    createdAt: '2023-04-09T02:37:52.000Z'
    data:
      edited: false
      editors:
      - Alydex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4864005b1ee07f34f90cc2b291453033.svg
          fullname: Alydex
          isHf: false
          isPro: false
          name: Alydex
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;blueisbest&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/blueisbest\">@<span class=\"\
          underline\">blueisbest</span></a></span>\n\n\t</span></span> did you manage\
          \ to fix the issue, im getting the same error</p>\n"
        raw: '@blueisbest did you manage to fix the issue, im getting the same error'
        updatedAt: '2023-04-09T02:37:52.746Z'
      numEdits: 0
      reactions: []
    id: 64322500f2355217ea48a070
    type: comment
  author: Alydex
  content: '@blueisbest did you manage to fix the issue, im getting the same error'
  created_at: 2023-04-09 01:37:52+00:00
  edited: false
  hidden: false
  id: 64322500f2355217ea48a070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/547aadaef26772dbacfe1fe97ce9d06d.svg
      fullname: Patrick Oliver Bustamante
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Psychopatz
      type: user
    createdAt: '2023-04-09T02:49:52.000Z'
    data:
      edited: false
      editors:
      - Psychopatz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/547aadaef26772dbacfe1fe97ce9d06d.svg
          fullname: Patrick Oliver Bustamante
          isHf: false
          isPro: false
          name: Psychopatz
          type: user
        html: '<p>It seems you''re using an AMD CPU which has limited support, yep,
          we got the same problem. Sadly your only way of using this is the LlamaCPP
          method. </p>

          <p>If someone here got any other methods, consider sharing one. Thanks</p>

          '
        raw: "It seems you're using an AMD CPU which has limited support, yep, we\
          \ got the same problem. Sadly your only way of using this is the LlamaCPP\
          \ method. \n\nIf someone here got any other methods, consider sharing one.\
          \ Thanks"
        updatedAt: '2023-04-09T02:49:52.367Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - qleoz12
    id: 643227d07d795d8e7034d487
    type: comment
  author: Psychopatz
  content: "It seems you're using an AMD CPU which has limited support, yep, we got\
    \ the same problem. Sadly your only way of using this is the LlamaCPP method.\
    \ \n\nIf someone here got any other methods, consider sharing one. Thanks"
  created_at: 2023-04-09 01:49:52+00:00
  edited: false
  hidden: false
  id: 643227d07d795d8e7034d487
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
      fullname: Hoshino Imi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Enferlain
      type: user
    createdAt: '2023-04-09T03:20:41.000Z'
    data:
      edited: false
      editors:
      - Enferlain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
          fullname: Hoshino Imi
          isHf: false
          isPro: false
          name: Enferlain
          type: user
        html: '<p>Why would having an amd cpu be an issue? Haven''t heard about that
          before.</p>

          '
        raw: Why would having an amd cpu be an issue? Haven't heard about that before.
        updatedAt: '2023-04-09T03:20:41.098Z'
      numEdits: 0
      reactions: []
    id: 64322f0946d9d4f9d89c48d1
    type: comment
  author: Enferlain
  content: Why would having an amd cpu be an issue? Haven't heard about that before.
  created_at: 2023-04-09 02:20:41+00:00
  edited: false
  hidden: false
  id: 64322f0946d9d4f9d89c48d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38581579e54aa2ff726c7e0702a54fa9.svg
      fullname: Skippy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skippyssk
      type: user
    createdAt: '2023-04-09T05:49:54.000Z'
    data:
      edited: false
      editors:
      - skippyssk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38581579e54aa2ff726c7e0702a54fa9.svg
          fullname: Skippy
          isHf: false
          isPro: false
          name: skippyssk
          type: user
        html: '<p>You need to update your start-webui bat file so the call python
          server.py has the arguments --chat --wbits 4 --groupsize 128.<br> That fixed
          it for me.</p>

          '
        raw: "You need to update your start-webui bat file so the call python server.py\
          \ has the arguments --chat --wbits 4 --groupsize 128.\n That fixed it for\
          \ me."
        updatedAt: '2023-04-09T05:49:54.139Z'
      numEdits: 0
      reactions: []
    id: 6432520246d9d4f9d89cfaad
    type: comment
  author: skippyssk
  content: "You need to update your start-webui bat file so the call python server.py\
    \ has the arguments --chat --wbits 4 --groupsize 128.\n That fixed it for me."
  created_at: 2023-04-09 04:49:54+00:00
  edited: false
  hidden: false
  id: 6432520246d9d4f9d89cfaad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/419371669ebcd0e47ad044b1756c7ab1.svg
      fullname: doctord98
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doctord98
      type: user
    createdAt: '2023-04-09T07:27:27.000Z'
    data:
      edited: false
      editors:
      - doctord98
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/419371669ebcd0e47ad044b1756c7ab1.svg
          fullname: doctord98
          isHf: false
          isPro: false
          name: doctord98
          type: user
        html: '<p>I am on the amd cpu too, and getting this error,<br>import llama_inference_offload<br>ModuleNotFoundError:
          No module named ''llama_inference_offload''</p>

          '
        raw: 'I am on the amd cpu too, and getting this error,

          import llama_inference_offload

          ModuleNotFoundError: No module named ''llama_inference_offload'''
        updatedAt: '2023-04-09T07:27:27.813Z'
      numEdits: 0
      reactions: []
    id: 643268df21fb4c094579de11
    type: comment
  author: doctord98
  content: 'I am on the amd cpu too, and getting this error,

    import llama_inference_offload

    ModuleNotFoundError: No module named ''llama_inference_offload'''
  created_at: 2023-04-09 06:27:27+00:00
  edited: false
  hidden: false
  id: 643268df21fb4c094579de11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2023-04-09T08:23:57.000Z'
    data:
      edited: true
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<p>I get the same error on Intel Xeon Cascade Lake P-8259L / NVIDIA
          T4 (Ubuntu 20.04)</p>

          '
        raw: I get the same error on Intel Xeon Cascade Lake P-8259L / NVIDIA T4 (Ubuntu
          20.04)
        updatedAt: '2023-04-09T08:24:14.747Z'
      numEdits: 1
      reactions: []
    id: 6432761dcca1de06ec0e945f
    type: comment
  author: synthetisoft
  content: I get the same error on Intel Xeon Cascade Lake P-8259L / NVIDIA T4 (Ubuntu
    20.04)
  created_at: 2023-04-09 07:23:57+00:00
  edited: true
  hidden: false
  id: 6432761dcca1de06ec0e945f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-04-09T09:57:17.000Z'
    data:
      edited: true
      editors:
      - perelmanych
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<p>I have the same issue. It looks like something is wrong. If during
          installation you select CPU, then it skips installation of GPTQ, but once
          you run start-webui it tries to load GPTQ.</p>

          '
        raw: I have the same issue. It looks like something is wrong. If during installation
          you select CPU, then it skips installation of GPTQ, but once you run start-webui
          it tries to load GPTQ.
        updatedAt: '2023-04-09T10:36:53.570Z'
      numEdits: 1
      reactions: []
    id: 64328bfdc8e2047bd9f6ca51
    type: comment
  author: perelmanych
  content: I have the same issue. It looks like something is wrong. If during installation
    you select CPU, then it skips installation of GPTQ, but once you run start-webui
    it tries to load GPTQ.
  created_at: 2023-04-09 08:57:17+00:00
  edited: true
  hidden: false
  id: 64328bfdc8e2047bd9f6ca51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d8ed169d072dafa06a07f65ce69f447.svg
      fullname: ELEPOT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ELEPOT
      type: user
    createdAt: '2023-04-09T10:28:04.000Z'
    data:
      edited: false
      editors:
      - ELEPOT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d8ed169d072dafa06a07f65ce69f447.svg
          fullname: ELEPOT
          isHf: false
          isPro: false
          name: ELEPOT
          type: user
        html: '<p>I fixed the issue with changing "!python server.py --chat --wbits
          4 --groupsize 128 --auto-devices" to "!python server.py --chat --auto-devices",
          but I got another error "FileNotFoundError: [Errno 2] No such file or directory:
          ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
          though. Not sure if the situation is better or worse.</p>

          '
        raw: 'I fixed the issue with changing "!python server.py --chat --wbits 4
          --groupsize 128 --auto-devices" to "!python server.py --chat --auto-devices",
          but I got another error "FileNotFoundError: [Errno 2] No such file or directory:
          ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
          though. Not sure if the situation is better or worse.'
        updatedAt: '2023-04-09T10:28:04.245Z'
      numEdits: 0
      reactions: []
    id: 64329334028e0ea13acf34c5
    type: comment
  author: ELEPOT
  content: 'I fixed the issue with changing "!python server.py --chat --wbits 4 --groupsize
    128 --auto-devices" to "!python server.py --chat --auto-devices", but I got another
    error "FileNotFoundError: [Errno 2] No such file or directory: ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
    though. Not sure if the situation is better or worse.'
  created_at: 2023-04-09 09:28:04+00:00
  edited: false
  hidden: false
  id: 64329334028e0ea13acf34c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ad9976f177898a99bb46e918f64bf5.svg
      fullname: Tamal Das
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tamal777
      type: user
    createdAt: '2023-04-09T10:34:51.000Z'
    data:
      edited: false
      editors:
      - tamal777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ad9976f177898a99bb46e918f64bf5.svg
          fullname: Tamal Das
          isHf: false
          isPro: false
          name: tamal777
          type: user
        html: '<p>same problem here.</p>

          '
        raw: same problem here.
        updatedAt: '2023-04-09T10:34:51.143Z'
      numEdits: 0
      reactions: []
    id: 643294cb21fb4c09457aeae3
    type: comment
  author: tamal777
  content: same problem here.
  created_at: 2023-04-09 09:34:51+00:00
  edited: false
  hidden: false
  id: 643294cb21fb4c09457aeae3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2023-04-09T10:38:16.000Z'
    data:
      edited: false
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<p>Fixed it by following step 1 from here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a></p>

          '
        raw: 'Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model'
        updatedAt: '2023-04-09T10:38:16.705Z'
      numEdits: 0
      reactions: []
    id: 64329598701755347c45de11
    type: comment
  author: synthetisoft
  content: 'Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model'
  created_at: 2023-04-09 09:38:16+00:00
  edited: false
  hidden: false
  id: 64329598701755347c45de11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2023-04-09T10:39:59.000Z'
    data:
      edited: false
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<p>Also passed <code>--wbits 4 --groupsize 128</code> to server.py</p>

          '
        raw: Also passed `--wbits 4 --groupsize 128` to server.py
        updatedAt: '2023-04-09T10:39:59.382Z'
      numEdits: 0
      reactions: []
    id: 643295ffc8e2047bd9f706e4
    type: comment
  author: synthetisoft
  content: Also passed `--wbits 4 --groupsize 128` to server.py
  created_at: 2023-04-09 09:39:59+00:00
  edited: false
  hidden: false
  id: 643295ffc8e2047bd9f706e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d8ed169d072dafa06a07f65ce69f447.svg
      fullname: ELEPOT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ELEPOT
      type: user
    createdAt: '2023-04-09T11:02:34.000Z'
    data:
      edited: false
      editors:
      - ELEPOT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d8ed169d072dafa06a07f65ce69f447.svg
          fullname: ELEPOT
          isHf: false
          isPro: false
          name: ELEPOT
          type: user
        html: '<p>I get ther error after following <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a>:
          "Could not find the quantized model in .pt or .safetensors format, exiting...",
          Any ideas?</p>

          '
        raw: 'I get ther error after following https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model:
          "Could not find the quantized model in .pt or .safetensors format, exiting...",
          Any ideas?'
        updatedAt: '2023-04-09T11:02:34.518Z'
      numEdits: 0
      reactions: []
    id: 64329b4ac8e2047bd9f722ed
    type: comment
  author: ELEPOT
  content: 'I get ther error after following https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model:
    "Could not find the quantized model in .pt or .safetensors format, exiting...",
    Any ideas?'
  created_at: 2023-04-09 10:02:34+00:00
  edited: false
  hidden: false
  id: 64329b4ac8e2047bd9f722ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-04-09T11:06:38.000Z'
    data:
      edited: true
      editors:
      - perelmanych
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<blockquote>

          <p>I get ther error after following <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a>:
          "Could not find the quantized model in .pt or .safetensors format, exiting...",
          Any ideas?</p>

          </blockquote>

          <p>Either you are trying to load not quantized model if so remove --wbits
          4 --groupsize 128, or you didn''t download quantized model into models folder.</p>

          '
        raw: '> I get ther error after following https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model:
          "Could not find the quantized model in .pt or .safetensors format, exiting...",
          Any ideas?


          Either you are trying to load not quantized model if so remove --wbits 4
          --groupsize 128, or you didn''t download quantized model into models folder.'
        updatedAt: '2023-04-09T11:07:15.956Z'
      numEdits: 1
      reactions: []
    id: 64329c3e21c6b87a1af1ece3
    type: comment
  author: perelmanych
  content: '> I get ther error after following https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model:
    "Could not find the quantized model in .pt or .safetensors format, exiting...",
    Any ideas?


    Either you are trying to load not quantized model if so remove --wbits 4 --groupsize
    128, or you didn''t download quantized model into models folder.'
  created_at: 2023-04-09 10:06:38+00:00
  edited: true
  hidden: false
  id: 64329c3e21c6b87a1af1ece3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-04-09T11:08:42.000Z'
    data:
      edited: false
      editors:
      - perelmanych
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<blockquote>

          <p>Fixed it by following step 1 from here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a></p>

          </blockquote>

          <p>I am getting "OSError: CUDA_HOME environment variable is not set. Please
          set it to your CUDA install root."</p>

          '
        raw: '> Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model


          I am getting "OSError: CUDA_HOME environment variable is not set. Please
          set it to your CUDA install root."'
        updatedAt: '2023-04-09T11:08:42.957Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - NickRimmer
        - camaudio
        - Uninspired-Otter
    id: 64329cba3f2096b2c9428f1a
    type: comment
  author: perelmanych
  content: '> Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model


    I am getting "OSError: CUDA_HOME environment variable is not set. Please set it
    to your CUDA install root."'
  created_at: 2023-04-09 10:08:42+00:00
  edited: false
  hidden: false
  id: 64329cba3f2096b2c9428f1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ad9976f177898a99bb46e918f64bf5.svg
      fullname: Tamal Das
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tamal777
      type: user
    createdAt: '2023-04-09T11:09:03.000Z'
    data:
      edited: false
      editors:
      - tamal777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ad9976f177898a99bb46e918f64bf5.svg
          fullname: Tamal Das
          isHf: false
          isPro: false
          name: tamal777
          type: user
        html: '<blockquote>

          <p>Fixed it by following step 1 from here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a></p>

          </blockquote>

          <p>didn''t you need a cuda gpu for this to work?</p>

          '
        raw: '> Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model


          didn''t you need a cuda gpu for this to work?'
        updatedAt: '2023-04-09T11:09:03.229Z'
      numEdits: 0
      reactions: []
    id: 64329ccf95b82ac2908c0525
    type: comment
  author: tamal777
  content: '> Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model


    didn''t you need a cuda gpu for this to work?'
  created_at: 2023-04-09 10:09:03+00:00
  edited: false
  hidden: false
  id: 64329ccf95b82ac2908c0525
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2023-04-09T11:13:49.000Z'
    data:
      edited: false
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Fixed it by following step 1 from here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model</a></p>

          </blockquote>

          <p>didn''t you need a cuda gpu for this to work?</p>

          </blockquote>

          <p>I am using a cuda GPU.</p>

          '
        raw: "> > Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model\n\
          > \n> didn't you need a cuda gpu for this to work?\n\nI am using a cuda\
          \ GPU."
        updatedAt: '2023-04-09T11:13:49.857Z'
      numEdits: 0
      reactions: []
    id: 64329dedcca1de06ec0f8113
    type: comment
  author: synthetisoft
  content: "> > Fixed it by following step 1 from here: https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model\n\
    > \n> didn't you need a cuda gpu for this to work?\n\nI am using a cuda GPU."
  created_at: 2023-04-09 10:13:49+00:00
  edited: false
  hidden: false
  id: 64329dedcca1de06ec0f8113
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-04-09T11:31:20.000Z'
    data:
      edited: false
      editors:
      - perelmanych
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<p>With GPU it works fine, you start to have problems when you try
          to use it with CPU.</p>

          '
        raw: With GPU it works fine, you start to have problems when you try to use
          it with CPU.
        updatedAt: '2023-04-09T11:31:20.132Z'
      numEdits: 0
      reactions: []
    id: 6432a2080cdd0c3686efbed8
    type: comment
  author: perelmanych
  content: With GPU it works fine, you start to have problems when you try to use
    it with CPU.
  created_at: 2023-04-09 10:31:20+00:00
  edited: false
  hidden: false
  id: 6432a2080cdd0c3686efbed8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2023-04-09T12:06:49.000Z'
    data:
      edited: false
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<blockquote>

          <p>With GPU it works fine, you start to have problems when you try to use
          it with CPU.</p>

          </blockquote>

          <p>oh sry. haven''t tried CPU yet. trying to get it working good on AWS
          gpu before I try to reduce cost.</p>

          '
        raw: '> With GPU it works fine, you start to have problems when you try to
          use it with CPU.


          oh sry. haven''t tried CPU yet. trying to get it working good on AWS gpu
          before I try to reduce cost.'
        updatedAt: '2023-04-09T12:06:49.940Z'
      numEdits: 0
      reactions: []
    id: 6432aa59028e0ea13acfba1c
    type: comment
  author: synthetisoft
  content: '> With GPU it works fine, you start to have problems when you try to use
    it with CPU.


    oh sry. haven''t tried CPU yet. trying to get it working good on AWS gpu before
    I try to reduce cost.'
  created_at: 2023-04-09 11:06:49+00:00
  edited: false
  hidden: false
  id: 6432aa59028e0ea13acfba1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd80470a9df1d23eb1be93864be50d1d.svg
      fullname: Daniel Gangl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielgangl
      type: user
    createdAt: '2023-04-09T14:45:59.000Z'
    data:
      edited: true
      editors:
      - danielgangl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd80470a9df1d23eb1be93864be50d1d.svg
          fullname: Daniel Gangl
          isHf: false
          isPro: false
          name: danielgangl
          type: user
        html: '<blockquote>

          <pre><code>import llama_inference_offload

          </code></pre>

          <p>ModuleNotFoundError: No module named ''llama_inference_offload''</p>

          </blockquote>

          <p>Having the exact same issue</p>

          '
        raw: '>     import llama_inference_offload

          > ModuleNotFoundError: No module named ''llama_inference_offload''


          Having the exact same issue'
        updatedAt: '2023-04-09T14:47:53.221Z'
      numEdits: 1
      reactions: []
    id: 6432cfa737d643c2690eb7bb
    type: comment
  author: danielgangl
  content: '>     import llama_inference_offload

    > ModuleNotFoundError: No module named ''llama_inference_offload''


    Having the exact same issue'
  created_at: 2023-04-09 13:45:59+00:00
  edited: true
  hidden: false
  id: 6432cfa737d643c2690eb7bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
      fullname: Hideyoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maruno
      type: user
    createdAt: '2023-04-09T19:06:10.000Z'
    data:
      edited: false
      editors:
      - Maruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
          fullname: Hideyoshi
          isHf: false
          isPro: false
          name: Maruno
          type: user
        html: '<p>I have the same exact problem and I''m using an AMD CPU, therefore
          my concern is whether it is fixable. Should I wait for the fix or delete
          everything I''ve downloaded?</p>

          '
        raw: I have the same exact problem and I'm using an AMD CPU, therefore my
          concern is whether it is fixable. Should I wait for the fix or delete everything
          I've downloaded?
        updatedAt: '2023-04-09T19:06:10.325Z'
      numEdits: 0
      reactions: []
    id: 64330ca205e626d3a33e8c2c
    type: comment
  author: Maruno
  content: I have the same exact problem and I'm using an AMD CPU, therefore my concern
    is whether it is fixable. Should I wait for the fix or delete everything I've
    downloaded?
  created_at: 2023-04-09 18:06:10+00:00
  edited: false
  hidden: false
  id: 64330ca205e626d3a33e8c2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58087de82205f5e879435ed57b800c24.svg
      fullname: khanfar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhammad
      type: user
    createdAt: '2023-04-10T02:14:21.000Z'
    data:
      edited: false
      editors:
      - mhammad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58087de82205f5e879435ed57b800c24.svg
          fullname: khanfar
          isHf: false
          isPro: false
          name: mhammad
          type: user
        html: '<p>I GOT THIS ERROR TOO . PLEASE HELP</p>

          <p>CUDA SETUP: Loading binary E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cpu.dll...<br>E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\cextension.py:31:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers and GPU quantization are unavailable.<br>warn("The
          installed version of bitsandbytes was compiled without GPU support. "<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Traceback (most recent call
          last):<br>File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\server.py",
          line 302, in<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File
          "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\models.py",
          line 100, in load_model<br>from modules.GPTQ_loader import load_quantized<br>File
          "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 14, in<br>import llama_inference_offload<br>ModuleNotFoundError: No
          module named ''llama_inference_offload''<br>Press any key to continue .
          . .</p>

          '
        raw: 'I GOT THIS ERROR TOO . PLEASE HELP


          CUDA SETUP: Loading binary E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cpu.dll...

          E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\cextension.py:31:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers and GPU quantization are unavailable.

          warn("The installed version of bitsandbytes was compiled without GPU support.
          "

          Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...

          Traceback (most recent call last):

          File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\server.py",
          line 302, in

          shared.model, shared.tokenizer = load_model(shared.model_name)

          File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\models.py",
          line 100, in load_model

          from modules.GPTQ_loader import load_quantized

          File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 14, in

          import llama_inference_offload

          ModuleNotFoundError: No module named ''llama_inference_offload''

          Press any key to continue . . .'
        updatedAt: '2023-04-10T02:14:21.976Z'
      numEdits: 0
      reactions: []
    id: 643370fd05e626d3a340ec36
    type: comment
  author: mhammad
  content: 'I GOT THIS ERROR TOO . PLEASE HELP


    CUDA SETUP: Loading binary E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cpu.dll...

    E:\vicuna-chatgpt4\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\cextension.py:31:
    UserWarning: The installed version of bitsandbytes was compiled without GPU support.
    8-bit optimizers and GPU quantization are unavailable.

    warn("The installed version of bitsandbytes was compiled without GPU support.
    "

    Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...

    Traceback (most recent call last):

    File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\server.py", line
    302, in

    shared.model, shared.tokenizer = load_model(shared.model_name)

    File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\models.py",
    line 100, in load_model

    from modules.GPTQ_loader import load_quantized

    File "E:\vicuna-chatgpt4\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
    line 14, in

    import llama_inference_offload

    ModuleNotFoundError: No module named ''llama_inference_offload''

    Press any key to continue . . .'
  created_at: 2023-04-10 01:14:21+00:00
  edited: false
  hidden: false
  id: 643370fd05e626d3a340ec36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4rXslgSVitm7p4Q8-isK0.png?w=200&h=200&f=face
      fullname: Arya Sarukkai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aryasarukkai
      type: user
    createdAt: '2023-04-10T02:29:41.000Z'
    data:
      edited: false
      editors:
      - aryasarukkai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4rXslgSVitm7p4Q8-isK0.png?w=200&h=200&f=face
          fullname: Arya Sarukkai
          isHf: false
          isPro: false
          name: aryasarukkai
          type: user
        html: '<p>Trying on M1:<br>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: Required library version not found: libsbitsandbytes_cpu.so. Maybe
          you need to compile it from source?<br>CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...<br>dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,
          0x0006): tried: ''/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so''
          (not a mach-o file)<br>CUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so.
          Maybe you need to compile it from source?<br>CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...<br>dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,
          0x0006): tried: ''/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so''
          (not a mach-o file)<br>/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:31:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers and GPU quantization are unavailable.<br>  warn("The
          installed version of bitsandbytes was compiled without GPU support. "<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Traceback (most recent call
          last):<br>  File "/Users/aryasarukkai/text-generation-webui/server.py",
          line 302, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/Users/aryasarukkai/text-generation-webui/modules/models.py", line 100,
          in load_model<br>    from modules.GPTQ_loader import load_quantized<br>  File
          "/Users/aryasarukkai/text-generation-webui/modules/GPTQ_loader.py", line
          14, in <br>    import llama_inference_offload<br>ModuleNotFoundError: No
          module named ''llama_inference_offload''</p>

          <p>I''ve tried with all solutions in this thread so far- no luck unfortunately.</p>

          '
        raw: "Trying on M1:\n===================================BUG REPORT===================================\n\
          Welcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\
          CUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so.\
          \ Maybe you need to compile it from source?\nCUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n\
          dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,\
          \ 0x0006): tried: '/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so'\
          \ (not a mach-o file)\nCUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so.\
          \ Maybe you need to compile it from source?\nCUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n\
          dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,\
          \ 0x0006): tried: '/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so'\
          \ (not a mach-o file)\n/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:31:\
          \ UserWarning: The installed version of bitsandbytes was compiled without\
          \ GPU support. 8-bit optimizers and GPU quantization are unavailable.\n\
          \  warn(\"The installed version of bitsandbytes was compiled without GPU\
          \ support. \"\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nTraceback\
          \ (most recent call last):\n  File \"/Users/aryasarukkai/text-generation-webui/server.py\"\
          , line 302, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/Users/aryasarukkai/text-generation-webui/modules/models.py\"\
          , line 100, in load_model\n    from modules.GPTQ_loader import load_quantized\n\
          \  File \"/Users/aryasarukkai/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 14, in <module>\n    import llama_inference_offload\nModuleNotFoundError:\
          \ No module named 'llama_inference_offload'\n\nI've tried with all solutions\
          \ in this thread so far- no luck unfortunately."
        updatedAt: '2023-04-10T02:29:41.823Z'
      numEdits: 0
      reactions: []
    id: 6433749537d643c26912ca8e
    type: comment
  author: aryasarukkai
  content: "Trying on M1:\n===================================BUG REPORT===================================\n\
    Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    CUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so. Maybe\
    \ you need to compile it from source?\nCUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n\
    dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,\
    \ 0x0006): tried: '/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so'\
    \ (not a mach-o file)\nCUDA SETUP: Required library version not found: libsbitsandbytes_cpu.so.\
    \ Maybe you need to compile it from source?\nCUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n\
    dlopen(/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so,\
    \ 0x0006): tried: '/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so'\
    \ (not a mach-o file)\n/Users/aryasarukkai/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:31:\
    \ UserWarning: The installed version of bitsandbytes was compiled without GPU\
    \ support. 8-bit optimizers and GPU quantization are unavailable.\n  warn(\"The\
    \ installed version of bitsandbytes was compiled without GPU support. \"\nLoading\
    \ anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nTraceback (most recent call last):\n\
    \  File \"/Users/aryasarukkai/text-generation-webui/server.py\", line 302, in\
    \ <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/Users/aryasarukkai/text-generation-webui/modules/models.py\", line\
    \ 100, in load_model\n    from modules.GPTQ_loader import load_quantized\n  File\
    \ \"/Users/aryasarukkai/text-generation-webui/modules/GPTQ_loader.py\", line 14,\
    \ in <module>\n    import llama_inference_offload\nModuleNotFoundError: No module\
    \ named 'llama_inference_offload'\n\nI've tried with all solutions in this thread\
    \ so far- no luck unfortunately."
  created_at: 2023-04-10 01:29:41+00:00
  edited: false
  hidden: false
  id: 6433749537d643c26912ca8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-10T07:04:36.000Z'
    data:
      edited: false
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: "<p>To solve the \"ModuleNotFoundError: No module named 'llama_inference_offload'\"\
          \ problem I followed the advice of  <span data-props=\"{&quot;user&quot;:&quot;synthetisoft&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/synthetisoft\"\
          >@<span class=\"underline\">synthetisoft</span></a></span>\n\n\t</span></span>\
          \ and did:</p>\n<p>mkdir \\oobabooga-windows\\text-generation-webui\\repositories<br>cd\
          \ repositories<br>git clone <a rel=\"nofollow\" href=\"https://github.com/oobabooga/GPTQ-for-LLaMa.git\"\
          >https://github.com/oobabooga/GPTQ-for-LLaMa.git</a> -b cuda</p>\n<p>It\
          \ worked, I got passed that stage.</p>\n<hr>\n<p>Now I get stuck at this\
          \ line: \"storage = cls(wrap_storage=untyped_storage)\"</p>\n<p>I think\
          \ I don't have enough memory, or free memory. I have roughly 7GB of free\
          \ memory.<br>Memory consumption spikes to 100% and the script crashes.</p>\n\
          <p>Here's my info up to that point:</p>\n<hr>\n<p>UserWarning: The installed\
          \ version of bitsandbytes was compiled without GPU support. 8-bit optimizers\
          \ and GPU quantization are unavailable.<br>  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"<br>Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>CUDA\
          \ extension not installed.<br>Found the following quantized model: models\\\
          anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors<br>Loading\
          \ model ...</p>\n<p>[<br>3 times the same warning for files storage.py:899,\
          \ _utils.py:776 and torch.py:99:<br>UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()<br>]<br>  storage = cls(wrap_storage=untyped_storage)<br>Press\
          \ any key to continue . . . [That's where the script crashes]</p>\n<hr>\n\
          <p>To the line that starts the web ui in start-webui.bat:<br>call python\
          \ server.py --auto-devices --chat --wbits 4 --groupsize 128</p>\n<p>I tried\
          \ adding  \"--pre_layer 20\" (tried --pre_layer 5, 50, 100, 1000 ) with\
          \ no success. Same result.</p>\n<hr>\n<p>My specs:</p>\n<p>Windows 11 16GB\
          \ memory, no GPU (to speak of).</p>\n<p>CPU</p>\n<pre><code>Intel(R) Core(TM)\
          \ i5-10400 CPU @ 2.90GHz\n\nBase speed:\t2.90 GHz\nSockets:\t1\nCores:\t\
          6\nLogical processors:\t12\nVirtualization:\tEnabled\nL1 cache:\t384 KB\n\
          L2 cache:\t1.5 MB\nL3 cache:\t12.0 MB\n</code></pre>\n<p>Memory</p>\n<pre><code>16.0\
          \ GB\n\nSpeed:\t2666 MHz\nSlots used:\t2 of 4\nForm factor:\tDIMM\nHardware\
          \ reserved:\t151 MB\n</code></pre>\n<p>GPU 0 (on-board)</p>\n<pre><code>Intel(R)\
          \ UHD Graphics 630\n\nDriver version:\t30.0.101.1273\nDriver date:\t14/01/2022\n\
          DirectX version:\t12 (FL 12.1)\nPhysical location:\tPCI bus 0, device 2,\
          \ function 0\n\nUtilization\t1%\nDedicated GPU memory\t\nShared GPU memory\t\
          0.4/7.9 GB\nGPU Memory\t0.4/7.9 GB\n</code></pre>\n"
        raw: "To solve the \"ModuleNotFoundError: No module named 'llama_inference_offload'\"\
          \ problem I followed the advice of  @synthetisoft and did:\n\nmkdir \\oobabooga-windows\\\
          text-generation-webui\\repositories\ncd repositories\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
          \ -b cuda\n\nIt worked, I got passed that stage.\n\n----\n\n\nNow I get\
          \ stuck at this line: \"storage = cls(wrap_storage=untyped_storage)\"\n\n\
          I think I don't have enough memory, or free memory. I have roughly 7GB of\
          \ free memory.\nMemory consumption spikes to 100% and the script crashes.\n\
          \nHere's my info up to that point:\n\n----\n\nUserWarning: The installed\
          \ version of bitsandbytes was compiled without GPU support. 8-bit optimizers\
          \ and GPU quantization are unavailable.\n  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
          CUDA extension not installed.\nFound the following quantized model: models\\\
          anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
          Loading model ...\n\n[\n3 times the same warning for files storage.py:899,\
          \ _utils.py:776 and torch.py:99:\nUserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()\n]\n  storage = cls(wrap_storage=untyped_storage)\n\
          Press any key to continue . . . [That's where the script crashes]\n\n----\n\
          \nTo the line that starts the web ui in start-webui.bat:\ncall python server.py\
          \ --auto-devices --chat --wbits 4 --groupsize 128\n\nI tried adding  \"\
          --pre_layer 20\" (tried --pre_layer 5, 50, 100, 1000 ) with no success.\
          \ Same result.\n\n---\n\nMy specs:\n\nWindows 11 16GB memory, no GPU (to\
          \ speak of).\n\nCPU\n\n\tIntel(R) Core(TM) i5-10400 CPU @ 2.90GHz\n\n\t\
          Base speed:\t2.90 GHz\n\tSockets:\t1\n\tCores:\t6\n\tLogical processors:\t\
          12\n\tVirtualization:\tEnabled\n\tL1 cache:\t384 KB\n\tL2 cache:\t1.5 MB\n\
          \tL3 cache:\t12.0 MB\n\nMemory\n\n\t16.0 GB\n\n\tSpeed:\t2666 MHz\n\tSlots\
          \ used:\t2 of 4\n\tForm factor:\tDIMM\n\tHardware reserved:\t151 MB\n\n\
          GPU 0 (on-board)\n\n\tIntel(R) UHD Graphics 630\n\n\tDriver version:\t30.0.101.1273\n\
          \tDriver date:\t14/01/2022\n\tDirectX version:\t12 (FL 12.1)\n\tPhysical\
          \ location:\tPCI bus 0, device 2, function 0\n\n\tUtilization\t1%\n\tDedicated\
          \ GPU memory\t\n\tShared GPU memory\t0.4/7.9 GB\n\tGPU Memory\t0.4/7.9 GB"
        updatedAt: '2023-04-10T07:04:36.274Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vgearen
    id: 6433b5041a1ba6b55b1464e3
    type: comment
  author: AI-Boss
  content: "To solve the \"ModuleNotFoundError: No module named 'llama_inference_offload'\"\
    \ problem I followed the advice of  @synthetisoft and did:\n\nmkdir \\oobabooga-windows\\\
    text-generation-webui\\repositories\ncd repositories\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
    \ -b cuda\n\nIt worked, I got passed that stage.\n\n----\n\n\nNow I get stuck\
    \ at this line: \"storage = cls(wrap_storage=untyped_storage)\"\n\nI think I don't\
    \ have enough memory, or free memory. I have roughly 7GB of free memory.\nMemory\
    \ consumption spikes to 100% and the script crashes.\n\nHere's my info up to that\
    \ point:\n\n----\n\nUserWarning: The installed version of bitsandbytes was compiled\
    \ without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n\
    \  warn(\"The installed version of bitsandbytes was compiled without GPU support.\
    \ \"\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nCUDA extension not\
    \ installed.\nFound the following quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\
    vicuna-13b-4bit-128g.safetensors\nLoading model ...\n\n[\n3 times the same warning\
    \ for files storage.py:899, _utils.py:776 and torch.py:99:\nUserWarning: TypedStorage\
    \ is deprecated. It will be removed in the future and UntypedStorage will be the\
    \ only storage class. This should only matter to you if you are using storages\
    \ directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead\
    \ of tensor.storage()\n]\n  storage = cls(wrap_storage=untyped_storage)\nPress\
    \ any key to continue . . . [That's where the script crashes]\n\n----\n\nTo the\
    \ line that starts the web ui in start-webui.bat:\ncall python server.py --auto-devices\
    \ --chat --wbits 4 --groupsize 128\n\nI tried adding  \"--pre_layer 20\" (tried\
    \ --pre_layer 5, 50, 100, 1000 ) with no success. Same result.\n\n---\n\nMy specs:\n\
    \nWindows 11 16GB memory, no GPU (to speak of).\n\nCPU\n\n\tIntel(R) Core(TM)\
    \ i5-10400 CPU @ 2.90GHz\n\n\tBase speed:\t2.90 GHz\n\tSockets:\t1\n\tCores:\t\
    6\n\tLogical processors:\t12\n\tVirtualization:\tEnabled\n\tL1 cache:\t384 KB\n\
    \tL2 cache:\t1.5 MB\n\tL3 cache:\t12.0 MB\n\nMemory\n\n\t16.0 GB\n\n\tSpeed:\t\
    2666 MHz\n\tSlots used:\t2 of 4\n\tForm factor:\tDIMM\n\tHardware reserved:\t\
    151 MB\n\nGPU 0 (on-board)\n\n\tIntel(R) UHD Graphics 630\n\n\tDriver version:\t\
    30.0.101.1273\n\tDriver date:\t14/01/2022\n\tDirectX version:\t12 (FL 12.1)\n\t\
    Physical location:\tPCI bus 0, device 2, function 0\n\n\tUtilization\t1%\n\tDedicated\
    \ GPU memory\t\n\tShared GPU memory\t0.4/7.9 GB\n\tGPU Memory\t0.4/7.9 GB"
  created_at: 2023-04-10 06:04:36+00:00
  edited: false
  hidden: false
  id: 6433b5041a1ba6b55b1464e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-10T07:10:33.000Z'
    data:
      edited: true
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: '<p>Tried changing the flag back to --cai-chat , now I get a different
          error, saying I have not enough memory:</p>

          <hr>

          <p>UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers and GPU quantization are unavailable.<br>  warn("The
          installed version of bitsandbytes was compiled without GPU support. "<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>CUDA extension not installed.<br>Found
          the following quantized model: models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\vicuna-13b-4bit-128g.safetensors<br>Traceback
          (most recent call last):<br>[... python files with their code lines]<br>RuntimeError:
          [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 35389440
          bytes.<br>Press any key to continue . . .</p>

          '
        raw: "Tried changing the flag back to --cai-chat , now I get a different error,\
          \ saying I have not enough memory:\n\n----\nUserWarning: The installed version\
          \ of bitsandbytes was compiled without GPU support. 8-bit optimizers and\
          \ GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes\
          \ was compiled without GPU support. \"\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
          CUDA extension not installed.\nFound the following quantized model: models\\\
          anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
          Traceback (most recent call last):\n[... python files with their code lines]\n\
          RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 35389440 bytes.\nPress any key to continue . . ."
        updatedAt: '2023-04-10T07:12:31.334Z'
      numEdits: 1
      reactions: []
    id: 6433b669d12a239d72e0b5cf
    type: comment
  author: AI-Boss
  content: "Tried changing the flag back to --cai-chat , now I get a different error,\
    \ saying I have not enough memory:\n\n----\nUserWarning: The installed version\
    \ of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization\
    \ are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled\
    \ without GPU support. \"\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
    CUDA extension not installed.\nFound the following quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\
    vicuna-13b-4bit-128g.safetensors\nTraceback (most recent call last):\n[... python\
    \ files with their code lines]\nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
    work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
    \ memory: you tried to allocate 35389440 bytes.\nPress any key to continue . .\
    \ ."
  created_at: 2023-04-10 06:10:33+00:00
  edited: true
  hidden: false
  id: 6433b669d12a239d72e0b5cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-10T08:20:49.000Z'
    data:
      edited: false
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: '<p>OK, so I tried bumping up my virtual memory and it seems to have
          helped. I now get past THIS memory issue, but I get an error saying that
          pytourch was installed wihtout conda support. or the other way around...
          This is as far as I''ve got for now. Going back to my real life :-)</p>

          '
        raw: OK, so I tried bumping up my virtual memory and it seems to have helped.
          I now get past THIS memory issue, but I get an error saying that pytourch
          was installed wihtout conda support. or the other way around... This is
          as far as I've got for now. Going back to my real life :-)
        updatedAt: '2023-04-10T08:20:49.299Z'
      numEdits: 0
      reactions: []
    id: 6433c6e1b1d842fdbe7984e6
    type: comment
  author: AI-Boss
  content: OK, so I tried bumping up my virtual memory and it seems to have helped.
    I now get past THIS memory issue, but I get an error saying that pytourch was
    installed wihtout conda support. or the other way around... This is as far as
    I've got for now. Going back to my real life :-)
  created_at: 2023-04-10 07:20:49+00:00
  edited: false
  hidden: false
  id: 6433c6e1b1d842fdbe7984e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
      fullname: Hideyoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maruno
      type: user
    createdAt: '2023-04-11T04:58:58.000Z'
    data:
      edited: false
      editors:
      - Maruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
          fullname: Hideyoshi
          isHf: false
          isPro: false
          name: Maruno
          type: user
        html: "<p>I've been trying to fix this problem for days now and here what\
          \ i learned so far: </p>\n<ol>\n<li><p>You got this error \" import llama_inference_offload\
          \ ModuleNotFoundError: No module named 'llama_inference_offload' \" Because\
          \ you're missing the repositories folder and the<br>GPTQ-for-LLaMa folder\
          \ you can follow <span data-props=\"{&quot;user&quot;:&quot;synthetisoft&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/synthetisoft\"\
          >@<span class=\"underline\">synthetisoft</span></a></span>\n\n\t</span></span>\
          \ by follow step 1 or try this fix if it's still doesn't work <a rel=\"\
          nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571\"\
          >https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571</a></p>\n\
          </li>\n<li><p>You get this error because you install it with CPU in mind\
          \ if you did that the repositories folder will be missing because it's meant\
          \ for the GPU CUDA models. Another reason is that you're trying to load\
          \ a<br>GPU models without the GPTQ-for-LLaMa and CUDA package install you\
          \ can regconize the CPU model by it prefix \" ggml- \".</p>\n</li>\n<li><p>I\
          \ tried to fix everything by following all the step above but i can't make\
          \ it work with CPU. so i came to conclusion that it's only work for NVIDIA\
          \ GPU.</p>\n<pre><code>   + Solution:\n</code></pre>\n</li>\n</ol>\n<ul>\n\
          <li><p>For those of you who're trying to use it with CPU i got good new\
          \ for you guys there's an alternative and it's very simple. it call \" Koboldcpp\
          \ \" it's like llamacpp but with Kobold Webui<br>you can have all the feature\
          \ that oobabooga have to offer if you don't mind learning how to use the\
          \ Kobold webui. </p>\n</li>\n<li><p>For those of you who're trying to use\
          \ it with GPU sorry it's only work with CPU for now. </p>\n<pre><code> \
          \  + Installation :\n</code></pre>\n</li>\n</ul>\n<ol>\n<li>Go to \" <a\
          \ rel=\"nofollow\" href=\"https://github.com/LostRuins/koboldcpp\">https://github.com/LostRuins/koboldcpp</a>\
          \ \" you can read the description if you want. </li>\n<li>Scroll down to\
          \ Usage you will see the blue Download link click on it. </li>\n<li>You\
          \ can read the description of how to use it and click download the koboldcpp.exe</li>\n\
          <li>After that you can download the CPU model of the GPT x ALPACA model\
          \ here: <a href=\"https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g\"\
          >https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g</a>\
          \   .It will take a while since it's about 10GB. </li>\n<li>Just drag and\
          \ drop the model or manually search for the ggml model yourself this work\
          \ for every CPU model. Next wait until it finished loading the model and\
          \ copy the <a rel=\"nofollow\" href=\"http://localhost:5001\">http://localhost:5001</a><br>and\
          \ paste it on your browser. </li>\n<li>you can find out more about koboldcpp\
          \ and how to use it here:  <a rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/12cfnqk/koboldcpp_combining_all_the_various_ggmlcpp_cpu/\"\
          >https://www.reddit.com/r/LocalLLaMA/comments/12cfnqk/koboldcpp_combining_all_the_various_ggmlcpp_cpu/</a></li>\n\
          </ol>\n<p>That's all for now hope this help.</p>\n"
        raw: "I've been trying to fix this problem for days now and here what i learned\
          \ so far: \n\n1. You got this error \" import llama_inference_offload ModuleNotFoundError:\
          \ No module named 'llama_inference_offload' \" Because you're missing the\
          \ repositories folder and the \nGPTQ-for-LLaMa folder you can follow @synthetisoft\
          \ by follow step 1 or try this fix if it's still doesn't work https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571\n\
          \n2. You get this error because you install it with CPU in mind if you did\
          \ that the repositories folder will be missing because it's meant for the\
          \ GPU CUDA models. Another reason is that you're trying to load a \nGPU\
          \ models without the GPTQ-for-LLaMa and CUDA package install you can regconize\
          \ the CPU model by it prefix \" ggml- \".\n\n3. I tried to fix everything\
          \ by following all the step above but i can't make it work with CPU. so\
          \ i came to conclusion that it's only work for NVIDIA GPU.\n \n        \
          \  + Solution: \n- For those of you who're trying to use it with CPU i got\
          \ good new for you guys there's an alternative and it's very simple. it\
          \ call \" Koboldcpp \" it's like llamacpp but with Kobold Webui \nyou can\
          \ have all the feature that oobabooga have to offer if you don't mind learning\
          \ how to use the Kobold webui. \n\n- For those of you who're trying to use\
          \ it with GPU sorry it's only work with CPU for now. \n\n         + Installation\
          \ : \n1. Go to \" https://github.com/LostRuins/koboldcpp \" you can read\
          \ the description if you want. \n2. Scroll down to Usage you will see the\
          \ blue Download link click on it. \n3. You can read the description of how\
          \ to use it and click download the koboldcpp.exe\n4. After that you can\
          \ download the CPU model of the GPT x ALPACA model here: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g\
          \   .It will take a while since it's about 10GB. \n5. Just drag and drop\
          \ the model or manually search for the ggml model yourself this work for\
          \ every CPU model. Next wait until it finished loading the model and copy\
          \ the http://localhost:5001\nand paste it on your browser. \n6. you can\
          \ find out more about koboldcpp and how to use it here:  https://www.reddit.com/r/LocalLLaMA/comments/12cfnqk/koboldcpp_combining_all_the_various_ggmlcpp_cpu/\n\
          \nThat's all for now hope this help."
        updatedAt: '2023-04-11T04:58:58.913Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - Psychopatz
        - Umasuki
        - AI-Boss
    id: 6434e912ef6d5abefe460a36
    type: comment
  author: Maruno
  content: "I've been trying to fix this problem for days now and here what i learned\
    \ so far: \n\n1. You got this error \" import llama_inference_offload ModuleNotFoundError:\
    \ No module named 'llama_inference_offload' \" Because you're missing the repositories\
    \ folder and the \nGPTQ-for-LLaMa folder you can follow @synthetisoft by follow\
    \ step 1 or try this fix if it's still doesn't work https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571\n\
    \n2. You get this error because you install it with CPU in mind if you did that\
    \ the repositories folder will be missing because it's meant for the GPU CUDA\
    \ models. Another reason is that you're trying to load a \nGPU models without\
    \ the GPTQ-for-LLaMa and CUDA package install you can regconize the CPU model\
    \ by it prefix \" ggml- \".\n\n3. I tried to fix everything by following all the\
    \ step above but i can't make it work with CPU. so i came to conclusion that it's\
    \ only work for NVIDIA GPU.\n \n          + Solution: \n- For those of you who're\
    \ trying to use it with CPU i got good new for you guys there's an alternative\
    \ and it's very simple. it call \" Koboldcpp \" it's like llamacpp but with Kobold\
    \ Webui \nyou can have all the feature that oobabooga have to offer if you don't\
    \ mind learning how to use the Kobold webui. \n\n- For those of you who're trying\
    \ to use it with GPU sorry it's only work with CPU for now. \n\n         + Installation\
    \ : \n1. Go to \" https://github.com/LostRuins/koboldcpp \" you can read the description\
    \ if you want. \n2. Scroll down to Usage you will see the blue Download link click\
    \ on it. \n3. You can read the description of how to use it and click download\
    \ the koboldcpp.exe\n4. After that you can download the CPU model of the GPT x\
    \ ALPACA model here: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g\
    \   .It will take a while since it's about 10GB. \n5. Just drag and drop the model\
    \ or manually search for the ggml model yourself this work for every CPU model.\
    \ Next wait until it finished loading the model and copy the http://localhost:5001\n\
    and paste it on your browser. \n6. you can find out more about koboldcpp and how\
    \ to use it here:  https://www.reddit.com/r/LocalLLaMA/comments/12cfnqk/koboldcpp_combining_all_the_various_ggmlcpp_cpu/\n\
    \nThat's all for now hope this help."
  created_at: 2023-04-11 03:58:58+00:00
  edited: false
  hidden: false
  id: 6434e912ef6d5abefe460a36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-11T13:27:46.000Z'
    data:
      edited: true
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: '<p>The script has been updated for CPU usage. Run ''iex (irm vicuna.tb.ag)''
          in powershell (in the directory you want to install the ui in) and the first
          question will allow you to install just the updated CPU mode of the model.
          I first deleted the ooga-booga directory and ran the script. It loaded a
          lot to memory and page file, but it works pretty well on my i5 16GB with
          lots of chrome tabs open... Great work.</p>

          <p>see this vieo:<br><a rel="nofollow" href="https://youtu.be/d4dk_7FptXk">https://youtu.be/d4dk_7FptXk</a></p>

          '
        raw: 'The script has been updated for CPU usage. Run ''iex (irm vicuna.tb.ag)''
          in powershell (in the directory you want to install the ui in) and the first
          question will allow you to install just the updated CPU mode of the model.
          I first deleted the ooga-booga directory and ran the script. It loaded a
          lot to memory and page file, but it works pretty well on my i5 16GB with
          lots of chrome tabs open... Great work.


          see this vieo:

          https://youtu.be/d4dk_7FptXk'
        updatedAt: '2023-04-11T13:39:14.573Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Maruno
    id: 643560523cec2a2f6598b791
    type: comment
  author: AI-Boss
  content: 'The script has been updated for CPU usage. Run ''iex (irm vicuna.tb.ag)''
    in powershell (in the directory you want to install the ui in) and the first question
    will allow you to install just the updated CPU mode of the model. I first deleted
    the ooga-booga directory and ran the script. It loaded a lot to memory and page
    file, but it works pretty well on my i5 16GB with lots of chrome tabs open...
    Great work.


    see this vieo:

    https://youtu.be/d4dk_7FptXk'
  created_at: 2023-04-11 12:27:46+00:00
  edited: true
  hidden: false
  id: 643560523cec2a2f6598b791
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
      fullname: Hideyoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maruno
      type: user
    createdAt: '2023-04-11T16:19:36.000Z'
    data:
      edited: true
      editors:
      - Maruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
          fullname: Hideyoshi
          isHf: false
          isPro: false
          name: Maruno
          type: user
        html: '<p>I tried it, and it also seems to work with the GPT4 x Alpaca CPU
          model. But it uses 20 GB of my 32GB rams and only manages to generate 60
          tokens in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and
          it only uses 12 GB of my RAM. I don''t know how it manages to use 20 GB
          of my ram and still only generate 0.17token/s I guess I''ll stick koboldcpp.
          But what about you did you get a faster generation when you use the Vicuna
          model?</p>

          '
        raw: I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
          But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens
          in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and it only
          uses 12 GB of my RAM. I don't know how it manages to use 20 GB of my ram
          and still only generate 0.17token/s I guess I'll stick koboldcpp. But what
          about you did you get a faster generation when you use the Vicuna model?
        updatedAt: '2023-04-11T16:23:54.463Z'
      numEdits: 1
      reactions: []
    id: 643588988495a63eaa18bf5d
    type: comment
  author: Maruno
  content: I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
    But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens in 5mins.
    In koboldcpp i can generate 500 tokens in only 8 mins and it only uses 12 GB of
    my RAM. I don't know how it manages to use 20 GB of my ram and still only generate
    0.17token/s I guess I'll stick koboldcpp. But what about you did you get a faster
    generation when you use the Vicuna model?
  created_at: 2023-04-11 15:19:36+00:00
  edited: true
  hidden: false
  id: 643588988495a63eaa18bf5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-11T19:40:08.000Z'
    data:
      edited: false
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: '<blockquote>

          <p>I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
          But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens
          in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and it only
          uses 12 GB of my RAM. I don''t know how it manages to use 20 GB of my ram
          and still only generate 0.17token/s I guess I''ll stick koboldcpp. But what
          about you did you get a faster generation when you use the Vicuna model?</p>

          </blockquote>

          <p>I''m not sure how to measure the token generation rate. It takes a bit
          of time for the model to start responding the first time it is loaded. But
          subsequent interactions are faster. Slower than GPT4, but faster than I
          can manage to read and understand... meaning it types faster than my in-depth
          reading speed. I guess if I try I can read faster than it types, or perhaps
          I''m just a slow reader :P English isn''t my 1st language.  Again, my specs
          are listed above, but a simple i5 16GB ram 240GB SSD, windows11 auto-managed
          page file. It works.</p>

          '
        raw: '> I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
          But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens
          in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and it only
          uses 12 GB of my RAM. I don''t know how it manages to use 20 GB of my ram
          and still only generate 0.17token/s I guess I''ll stick koboldcpp. But what
          about you did you get a faster generation when you use the Vicuna model?


          I''m not sure how to measure the token generation rate. It takes a bit of
          time for the model to start responding the first time it is loaded. But
          subsequent interactions are faster. Slower than GPT4, but faster than I
          can manage to read and understand... meaning it types faster than my in-depth
          reading speed. I guess if I try I can read faster than it types, or perhaps
          I''m just a slow reader :P English isn''t my 1st language.  Again, my specs
          are listed above, but a simple i5 16GB ram 240GB SSD, windows11 auto-managed
          page file. It works.'
        updatedAt: '2023-04-11T19:40:08.249Z'
      numEdits: 0
      reactions: []
    id: 6435b798dd2653057647d972
    type: comment
  author: AI-Boss
  content: '> I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
    But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens in 5mins.
    In koboldcpp i can generate 500 tokens in only 8 mins and it only uses 12 GB of
    my RAM. I don''t know how it manages to use 20 GB of my ram and still only generate
    0.17token/s I guess I''ll stick koboldcpp. But what about you did you get a faster
    generation when you use the Vicuna model?


    I''m not sure how to measure the token generation rate. It takes a bit of time
    for the model to start responding the first time it is loaded. But subsequent
    interactions are faster. Slower than GPT4, but faster than I can manage to read
    and understand... meaning it types faster than my in-depth reading speed. I guess
    if I try I can read faster than it types, or perhaps I''m just a slow reader :P
    English isn''t my 1st language.  Again, my specs are listed above, but a simple
    i5 16GB ram 240GB SSD, windows11 auto-managed page file. It works.'
  created_at: 2023-04-11 18:40:08+00:00
  edited: false
  hidden: false
  id: 6435b798dd2653057647d972
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
      fullname: Hoshino Imi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Enferlain
      type: user
    createdAt: '2023-04-12T02:59:35.000Z'
    data:
      edited: false
      editors:
      - Enferlain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634575c6d54fb141ded9c931/rqQTf4ewWbUR8NfuuUyS4.png?w=200&h=200&f=face
          fullname: Hoshino Imi
          isHf: false
          isPro: false
          name: Enferlain
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.
          But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens
          in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and it only
          uses 12 GB of my RAM. I don''t know how it manages to use 20 GB of my ram
          and still only generate 0.17token/s I guess I''ll stick koboldcpp. But what
          about you did you get a faster generation when you use the Vicuna model?</p>

          </blockquote>

          <p>I''m not sure how to measure the token generation rate. It takes a bit
          of time for the model to start responding the first time it is loaded. But
          subsequent interactions are faster. Slower than GPT4, but faster than I
          can manage to read and understand... meaning it types faster than my in-depth
          reading speed. I guess if I try I can read faster than it types, or perhaps
          I''m just a slow reader :P English isn''t my 1st language.  Again, my specs
          are listed above, but a simple i5 16GB ram 240GB SSD, windows11 auto-managed
          page file. It works.</p>

          </blockquote>

          <p>Can you test with max context size? Character or chat history that uses
          the whole 2048 context</p>

          '
        raw: "> > I tried it, and it also seems to work with the GPT4 x Alpaca CPU\
          \ model. But it uses 20 GB of my 32GB rams and only manages to generate\
          \ 60 tokens in 5mins. In koboldcpp i can generate 500 tokens in only 8 mins\
          \ and it only uses 12 GB of my RAM. I don't know how it manages to use 20\
          \ GB of my ram and still only generate 0.17token/s I guess I'll stick koboldcpp.\
          \ But what about you did you get a faster generation when you use the Vicuna\
          \ model?\n> \n> I'm not sure how to measure the token generation rate. It\
          \ takes a bit of time for the model to start responding the first time it\
          \ is loaded. But subsequent interactions are faster. Slower than GPT4, but\
          \ faster than I can manage to read and understand... meaning it types faster\
          \ than my in-depth reading speed. I guess if I try I can read faster than\
          \ it types, or perhaps I'm just a slow reader :P English isn't my 1st language.\
          \  Again, my specs are listed above, but a simple i5 16GB ram 240GB SSD,\
          \ windows11 auto-managed page file. It works.\n\nCan you test with max context\
          \ size? Character or chat history that uses the whole 2048 context"
        updatedAt: '2023-04-12T02:59:35.758Z'
      numEdits: 0
      reactions: []
    id: 64361e973cec2a2f659e55b2
    type: comment
  author: Enferlain
  content: "> > I tried it, and it also seems to work with the GPT4 x Alpaca CPU model.\
    \ But it uses 20 GB of my 32GB rams and only manages to generate 60 tokens in\
    \ 5mins. In koboldcpp i can generate 500 tokens in only 8 mins and it only uses\
    \ 12 GB of my RAM. I don't know how it manages to use 20 GB of my ram and still\
    \ only generate 0.17token/s I guess I'll stick koboldcpp. But what about you did\
    \ you get a faster generation when you use the Vicuna model?\n> \n> I'm not sure\
    \ how to measure the token generation rate. It takes a bit of time for the model\
    \ to start responding the first time it is loaded. But subsequent interactions\
    \ are faster. Slower than GPT4, but faster than I can manage to read and understand...\
    \ meaning it types faster than my in-depth reading speed. I guess if I try I can\
    \ read faster than it types, or perhaps I'm just a slow reader :P English isn't\
    \ my 1st language.  Again, my specs are listed above, but a simple i5 16GB ram\
    \ 240GB SSD, windows11 auto-managed page file. It works.\n\nCan you test with\
    \ max context size? Character or chat history that uses the whole 2048 context"
  created_at: 2023-04-12 01:59:35+00:00
  edited: false
  hidden: false
  id: 64361e973cec2a2f659e55b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
      fullname: Hideyoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maruno
      type: user
    createdAt: '2023-04-12T05:32:37.000Z'
    data:
      edited: true
      editors:
      - Maruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
          fullname: Hideyoshi
          isHf: false
          isPro: false
          name: Maruno
          type: user
        html: '<p>I''ve done some test on oobabooga with and without the character
          context on 2 models. </p>

          <ul>

          <li><p>Vicuna 13B without character context.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/Zj2Wfhlh6RyRJ60PQuQO3.png"><img
          alt="vicuna test1.png" src="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/Zj2Wfhlh6RyRJ60PQuQO3.png"></a><br>with
          character context<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/SOayvma7FFrOGHswZrDBI.png"><img
          alt="vicuna test2 .png" src="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/SOayvma7FFrOGHswZrDBI.png"></a></p>

          </li>

          <li><p>GPT4xAlpaca 13B without character context.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/lMPUZw1FKhzaPvuoYB5QS.png"><img
          alt="Gpt4xalpaca test1.png" src="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/lMPUZw1FKhzaPvuoYB5QS.png"></a><br>with
          character context<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/wcgFZXf8rTqi2I2-2ghe3.png"><img
          alt="Gpt4xalpaca test2.png" src="https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/wcgFZXf8rTqi2I2-2ghe3.png"></a></p>

          </li>

          <li><p>My cpu spec is Ryzen 5 4600h :</p>

          </li>

          </ul>

          <ul>

          <li>6 cores </li>

          <li>12 threads</li>

          <li>Base Clock 3.0GHz<br>And i have a 32GB rams.</li>

          </ul>

          <p>PS: I don''t have any character context that have 2048 context but you
          can imagine it take longer the more context you have. In my experiment Koboldcpp
          seem to process context and generate faster than oobabooga but oobabooga
          seem to give slightly better respond and doesn''t cut out the output of
          the character like Koboldcpp does.</p>

          '
        raw: "I've done some test on oobabooga with and without the character context\
          \ on 2 models. \n+ Vicuna 13B without character context. \n![vicuna test1.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/Zj2Wfhlh6RyRJ60PQuQO3.png\
          \ )\nwith character context \n![vicuna test2 .png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/SOayvma7FFrOGHswZrDBI.png)\n\
          + GPT4xAlpaca 13B without character context.\n![Gpt4xalpaca test1.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/lMPUZw1FKhzaPvuoYB5QS.png)\n\
          with character context \n![Gpt4xalpaca test2.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/wcgFZXf8rTqi2I2-2ghe3.png)\n\
          \n+ My cpu spec is Ryzen 5 4600h : \n- 6 cores \n- 12 threads\n- Base Clock\
          \ 3.0GHz\nAnd i have a 32GB rams. \n\nPS: I don't have any character context\
          \ that have 2048 context but you can imagine it take longer the more context\
          \ you have. In my experiment Koboldcpp seem to process context and generate\
          \ faster than oobabooga but oobabooga seem to give slightly better respond\
          \ and doesn't cut out the output of the character like Koboldcpp does."
        updatedAt: '2023-04-12T05:33:38.813Z'
      numEdits: 1
      reactions: []
    id: 64364275569e1a4b0cc8f59f
    type: comment
  author: Maruno
  content: "I've done some test on oobabooga with and without the character context\
    \ on 2 models. \n+ Vicuna 13B without character context. \n![vicuna test1.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/Zj2Wfhlh6RyRJ60PQuQO3.png\
    \ )\nwith character context \n![vicuna test2 .png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/SOayvma7FFrOGHswZrDBI.png)\n\
    + GPT4xAlpaca 13B without character context.\n![Gpt4xalpaca test1.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/lMPUZw1FKhzaPvuoYB5QS.png)\n\
    with character context \n![Gpt4xalpaca test2.png](https://cdn-uploads.huggingface.co/production/uploads/6363c7a25a8b39563ac7b6ff/wcgFZXf8rTqi2I2-2ghe3.png)\n\
    \n+ My cpu spec is Ryzen 5 4600h : \n- 6 cores \n- 12 threads\n- Base Clock 3.0GHz\n\
    And i have a 32GB rams. \n\nPS: I don't have any character context that have 2048\
    \ context but you can imagine it take longer the more context you have. In my\
    \ experiment Koboldcpp seem to process context and generate faster than oobabooga\
    \ but oobabooga seem to give slightly better respond and doesn't cut out the output\
    \ of the character like Koboldcpp does."
  created_at: 2023-04-12 04:32:37+00:00
  edited: true
  hidden: false
  id: 64364275569e1a4b0cc8f59f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
      fullname: Adi Ahiron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AI-Boss
      type: user
    createdAt: '2023-04-12T10:49:26.000Z'
    data:
      edited: false
      editors:
      - AI-Boss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff4728bc78e2e2b106e0e877c8e901c9.svg
          fullname: Adi Ahiron
          isHf: false
          isPro: false
          name: AI-Boss
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Maruno&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Maruno\">@<span class=\"\
          underline\">Maruno</span></a></span>\n\n\t</span></span> </p>\n<p>For ggml-vicuna-13b-4bit-rev1.bin:</p>\n\
          <p>Output generated in 429.89 seconds (0.47 tokens/s, 200 tokens, context\
          \ 1317, seed 409617372)<br>Output generated in 170.12 seconds (1.18 tokens/s,\
          \ 200 tokens, context 435, seed 22146058)<br>Output generated in 247.94\
          \ seconds (0.81 tokens/s, 200 tokens, context 633, seed 1962886833)<br>Output\
          \ generated in 632.09 seconds (1.58 tokens/s, 1000 tokens, context 647,\
          \ seed 2070444208)<br>Output generated in 572.93 seconds (0.79 tokens/s,\
          \ 450 tokens, context 1274, seed 926636926)</p>\n<p>I tried going max context\
          \ but it crashed on me for now. I was also doing other things and my PC\
          \ has many running programs while tokenization takes place, so it's not\
          \ accurate.<br>llama_tokenize: too many tokens<br> raise RuntimeError(f'Failed\
          \ to tokenize: text=\"{text}\" n_tokens={n_tokens}')</p>\n"
        raw: "@Maruno \n\nFor ggml-vicuna-13b-4bit-rev1.bin:\n\nOutput generated in\
          \ 429.89 seconds (0.47 tokens/s, 200 tokens, context 1317, seed 409617372)\n\
          Output generated in 170.12 seconds (1.18 tokens/s, 200 tokens, context 435,\
          \ seed 22146058)\nOutput generated in 247.94 seconds (0.81 tokens/s, 200\
          \ tokens, context 633, seed 1962886833)\nOutput generated in 632.09 seconds\
          \ (1.58 tokens/s, 1000 tokens, context 647, seed 2070444208)\nOutput generated\
          \ in 572.93 seconds (0.79 tokens/s, 450 tokens, context 1274, seed 926636926)\n\
          \nI tried going max context but it crashed on me for now. I was also doing\
          \ other things and my PC has many running programs while tokenization takes\
          \ place, so it's not accurate.\nllama_tokenize: too many tokens\n raise\
          \ RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')"
        updatedAt: '2023-04-12T10:49:26.827Z'
      numEdits: 0
      reactions: []
    id: 64368cb6adb1d6b4f1f7eb5a
    type: comment
  author: AI-Boss
  content: "@Maruno \n\nFor ggml-vicuna-13b-4bit-rev1.bin:\n\nOutput generated in\
    \ 429.89 seconds (0.47 tokens/s, 200 tokens, context 1317, seed 409617372)\nOutput\
    \ generated in 170.12 seconds (1.18 tokens/s, 200 tokens, context 435, seed 22146058)\n\
    Output generated in 247.94 seconds (0.81 tokens/s, 200 tokens, context 633, seed\
    \ 1962886833)\nOutput generated in 632.09 seconds (1.58 tokens/s, 1000 tokens,\
    \ context 647, seed 2070444208)\nOutput generated in 572.93 seconds (0.79 tokens/s,\
    \ 450 tokens, context 1274, seed 926636926)\n\nI tried going max context but it\
    \ crashed on me for now. I was also doing other things and my PC has many running\
    \ programs while tokenization takes place, so it's not accurate.\nllama_tokenize:\
    \ too many tokens\n raise RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')"
  created_at: 2023-04-12 09:49:26+00:00
  edited: false
  hidden: false
  id: 64368cb6adb1d6b4f1f7eb5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
      fullname: Hideyoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maruno
      type: user
    createdAt: '2023-04-12T11:01:14.000Z'
    data:
      edited: true
      editors:
      - Maruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667483520317-noauth.jpeg?w=200&h=200&f=face
          fullname: Hideyoshi
          isHf: false
          isPro: false
          name: Maruno
          type: user
        html: '<p>Well if you want to use oobabooga and have only a CPU it will get
          slower the more context it have so you can only chat with it for a certain
          amount of time so in short it has a short memory. if you want to keep chatting
          with it you have to clear the old chat and start with 0 context. </p>

          <ul>

          <li>For now the only solution is to use the colab someone have created:
          <a rel="nofollow" href="https://colab.research.google.com/drive/1VSHMCbBFtOGNbJzoSmEXr-OZF91TpWap">https://colab.research.google.com/drive/1VSHMCbBFtOGNbJzoSmEXr-OZF91TpWap</a></li>

          <li>just run everything until the last one you have to change the code first
          by clicking show code and add the --cai-chat . If not the interface will
          be different from the one you''re using. you can try adding other command
          but for now i only experiment with the --cai-chat command. </li>

          <li>And when you lunch don''t copy and paste the localhost website but instead
          click on the first link.</li>

          </ul>

          <p>With this i can Generate about 4token/s and it process the context very
          quickly as well.</p>

          '
        raw: "Well if you want to use oobabooga and have only a CPU it will get slower\
          \ the more context it have so you can only chat with it for a certain amount\
          \ of time so in short it has a short memory. if you want to keep chatting\
          \ with it you have to clear the old chat and start with 0 context. \n\n\
          - For now the only solution is to use the colab someone have created: https://colab.research.google.com/drive/1VSHMCbBFtOGNbJzoSmEXr-OZF91TpWap\n\
          - just run everything until the last one you have to change the code first\
          \ by clicking show code and add the --cai-chat . If not the interface will\
          \ be different from the one you're using. you can try adding other command\
          \ but for now i only experiment with the --cai-chat command. \n- And when\
          \ you lunch don't copy and paste the localhost website but instead click\
          \ on the first link. \n\nWith this i can Generate about 4token/s and it\
          \ process the context very quickly as well."
        updatedAt: '2023-04-12T11:01:47.322Z'
      numEdits: 1
      reactions: []
    id: 64368f7a62104ec8c43ddd9f
    type: comment
  author: Maruno
  content: "Well if you want to use oobabooga and have only a CPU it will get slower\
    \ the more context it have so you can only chat with it for a certain amount of\
    \ time so in short it has a short memory. if you want to keep chatting with it\
    \ you have to clear the old chat and start with 0 context. \n\n- For now the only\
    \ solution is to use the colab someone have created: https://colab.research.google.com/drive/1VSHMCbBFtOGNbJzoSmEXr-OZF91TpWap\n\
    - just run everything until the last one you have to change the code first by\
    \ clicking show code and add the --cai-chat . If not the interface will be different\
    \ from the one you're using. you can try adding other command but for now i only\
    \ experiment with the --cai-chat command. \n- And when you lunch don't copy and\
    \ paste the localhost website but instead click on the first link. \n\nWith this\
    \ i can Generate about 4token/s and it process the context very quickly as well."
  created_at: 2023-04-12 10:01:14+00:00
  edited: true
  hidden: false
  id: 64368f7a62104ec8c43ddd9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9409031a183fe5ffaa4a911526a9f927.svg
      fullname: MyName
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: UserNew
      type: user
    createdAt: '2023-04-19T08:14:07.000Z'
    data:
      edited: false
      editors:
      - UserNew
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9409031a183fe5ffaa4a911526a9f927.svg
          fullname: MyName
          isHf: false
          isPro: false
          name: UserNew
          type: user
        html: '<p>Is anyone here using a Linux VM to run this on a host system with
          an AMD CPU and a NVIDIA graphs card?</p>

          <p>I''ve been trying absolutely everything under the Sun to make it work,
          but I keep getting errors after errors and the NVIDIA card doesn''t seem
          to be recognized by the VM...<br>On that note, if I want to run this based
          on the GPU, should I still download and follow the install process for the
          "AMD" part of the guide due to my CPU being AMD or is that only relevant
          if I''m doing exclusively CPU access? None of that is made clear in the
          readmes in github, and I wish it was.</p>

          <p>Thanks!</p>

          '
        raw: 'Is anyone here using a Linux VM to run this on a host system with an
          AMD CPU and a NVIDIA graphs card?


          I''ve been trying absolutely everything under the Sun to make it work, but
          I keep getting errors after errors and the NVIDIA card doesn''t seem to
          be recognized by the VM...

          On that note, if I want to run this based on the GPU, should I still download
          and follow the install process for the "AMD" part of the guide due to my
          CPU being AMD or is that only relevant if I''m doing exclusively CPU access?
          None of that is made clear in the readmes in github, and I wish it was.


          Thanks!'
        updatedAt: '2023-04-19T08:14:07.694Z'
      numEdits: 0
      reactions: []
    id: 643fa2cf4ef21cdaa2ed2e1b
    type: comment
  author: UserNew
  content: 'Is anyone here using a Linux VM to run this on a host system with an AMD
    CPU and a NVIDIA graphs card?


    I''ve been trying absolutely everything under the Sun to make it work, but I keep
    getting errors after errors and the NVIDIA card doesn''t seem to be recognized
    by the VM...

    On that note, if I want to run this based on the GPU, should I still download
    and follow the install process for the "AMD" part of the guide due to my CPU being
    AMD or is that only relevant if I''m doing exclusively CPU access? None of that
    is made clear in the readmes in github, and I wish it was.


    Thanks!'
  created_at: 2023-04-19 07:14:07+00:00
  edited: false
  hidden: false
  id: 643fa2cf4ef21cdaa2ed2e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0078e214dc3e373afa1ba72cdca674db.svg
      fullname: Nathan Hickey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JediDwag
      type: user
    createdAt: '2023-04-25T08:34:16.000Z'
    data:
      edited: false
      editors:
      - JediDwag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0078e214dc3e373afa1ba72cdca674db.svg
          fullname: Nathan Hickey
          isHf: false
          isPro: false
          name: JediDwag
          type: user
        html: '<p>Just leaving my solution here in case anyone else is getting the  ''llama_inference_offload''
          issue running ROCm on an AMD card.</p>

          <p>Turns out you need to go into /text-generation-webui/modules/ and edit
          GPTQ_loader.py and change this line:<br>    sys.path.insert(0, str(Path("repositories/GPTQ-for-LLaMa")))<br>to<br>    sys.path.insert(0,
          str(Path("repositories/GPTQ-for-LLaMa-ROCm")))</p>

          <p>With that you should be able to load the gpt4-x-alpaca-13b-native-4bit-128g
          model with the options --wbits 4 --groupsize 128.</p>

          <p>I was also have a ton of crashes once I had it running, but it turns
          out that was transient loads on my crappy power supply that I''m running
          too close to the limit on. I fixed that by running a game in the background
          to keep the load up. haha</p>

          '
        raw: "Just leaving my solution here in case anyone else is getting the  'llama_inference_offload'\
          \ issue running ROCm on an AMD card.\n\nTurns out you need to go into /text-generation-webui/modules/\
          \ and edit GPTQ_loader.py and change this line:\n    sys.path.insert(0,\
          \ str(Path(\"repositories/GPTQ-for-LLaMa\")))\nto\n    sys.path.insert(0,\
          \ str(Path(\"repositories/GPTQ-for-LLaMa-ROCm\")))\n\nWith that you should\
          \ be able to load the gpt4-x-alpaca-13b-native-4bit-128g model with the\
          \ options --wbits 4 --groupsize 128.\n\nI was also have a ton of crashes\
          \ once I had it running, but it turns out that was transient loads on my\
          \ crappy power supply that I'm running too close to the limit on. I fixed\
          \ that by running a game in the background to keep the load up. haha"
        updatedAt: '2023-04-25T08:34:16.145Z'
      numEdits: 0
      reactions: []
    id: 64479088058f3572dd067d9b
    type: comment
  author: JediDwag
  content: "Just leaving my solution here in case anyone else is getting the  'llama_inference_offload'\
    \ issue running ROCm on an AMD card.\n\nTurns out you need to go into /text-generation-webui/modules/\
    \ and edit GPTQ_loader.py and change this line:\n    sys.path.insert(0, str(Path(\"\
    repositories/GPTQ-for-LLaMa\")))\nto\n    sys.path.insert(0, str(Path(\"repositories/GPTQ-for-LLaMa-ROCm\"\
    )))\n\nWith that you should be able to load the gpt4-x-alpaca-13b-native-4bit-128g\
    \ model with the options --wbits 4 --groupsize 128.\n\nI was also have a ton of\
    \ crashes once I had it running, but it turns out that was transient loads on\
    \ my crappy power supply that I'm running too close to the limit on. I fixed that\
    \ by running a game in the background to keep the load up. haha"
  created_at: 2023-04-25 07:34:16+00:00
  edited: false
  hidden: false
  id: 64479088058f3572dd067d9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b775b8b65b50cfe4f075bf8ac2e748.svg
      fullname: Pop Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PopGa
      type: user
    createdAt: '2023-05-16T23:40:30.000Z'
    data:
      edited: true
      editors:
      - PopGa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b775b8b65b50cfe4f075bf8ac2e748.svg
          fullname: Pop Gabriel
          isHf: false
          isPro: false
          name: PopGa
          type: user
        html: "<p>Hey guys, I get the same problem when trying to load the model \"\
          TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\" on ooba's colab version of\
          \ the UI. Any hopes to get this solved?<br>More precisely:<br>Traceback\
          \ (most recent call last):<br>File \u201C/content/text-generation-webui/server.py\u201D\
          , line 59, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201C/content/text-generation-webui/modules/models.py\u201D, line 157,\
          \ in load_model<br>from modules.GPTQ_loader import load_quantized<br>File\
          \ \u201C/content/text-generation-webui/modules/GPTQ_loader.py\u201D, line\
          \ 15, in<br>import llama_inference_offload<br>ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019</p>\n<p>EDIT: Added the quote\
          \ of the error</p>\n"
        raw: "Hey guys, I get the same problem when trying to load the model \"TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\"\
          \ on ooba's colab version of the UI. Any hopes to get this solved?\nMore\
          \ precisely:\nTraceback (most recent call last):\nFile \u201C/content/text-generation-webui/server.py\u201D\
          , line 59, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201C/content/text-generation-webui/modules/models.py\u201D, line\
          \ 157, in load_model\nfrom modules.GPTQ_loader import load_quantized\nFile\
          \ \u201C/content/text-generation-webui/modules/GPTQ_loader.py\u201D, line\
          \ 15, in\nimport llama_inference_offload\nModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019\n\nEDIT: Added the quote of\
          \ the error"
        updatedAt: '2023-05-16T23:42:15.288Z'
      numEdits: 1
      reactions: []
    id: 6464146e65d811c4962bf6cd
    type: comment
  author: PopGa
  content: "Hey guys, I get the same problem when trying to load the model \"TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\"\
    \ on ooba's colab version of the UI. Any hopes to get this solved?\nMore precisely:\n\
    Traceback (most recent call last):\nFile \u201C/content/text-generation-webui/server.py\u201D\
    , line 59, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
    File \u201C/content/text-generation-webui/modules/models.py\u201D, line 157, in\
    \ load_model\nfrom modules.GPTQ_loader import load_quantized\nFile \u201C/content/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 15, in\nimport llama_inference_offload\nModuleNotFoundError: No module\
    \ named \u2018llama_inference_offload\u2019\n\nEDIT: Added the quote of the error"
  created_at: 2023-05-16 22:40:30+00:00
  edited: true
  hidden: false
  id: 6464146e65d811c4962bf6cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c1e313e6d192790c57c15688d4058e0.svg
      fullname: Jonathan P Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joesnuffy
      type: user
    createdAt: '2023-10-25T05:16:31.000Z'
    data:
      edited: false
      editors:
      - joesnuffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6998211741447449
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c1e313e6d192790c57c15688d4058e0.svg
          fullname: Jonathan P Scott
          isHf: false
          isPro: false
          name: joesnuffy
          type: user
        html: '<blockquote>

          <p>I fixed the issue with changing "!python server.py --chat --wbits 4 --groupsize
          128 --auto-devices" to "!python server.py --chat --auto-devices", but I
          got another error "FileNotFoundError: [Errno 2] No such file or directory:
          ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
          though. Not sure if the situation is better or worse.</p>

          </blockquote>

          <p>I received the same exact error, anyone have any ideas?  No idea what
          I''m doing.</p>

          '
        raw: '> I fixed the issue with changing "!python server.py --chat --wbits
          4 --groupsize 128 --auto-devices" to "!python server.py --chat --auto-devices",
          but I got another error "FileNotFoundError: [Errno 2] No such file or directory:
          ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
          though. Not sure if the situation is better or worse.


          I received the same exact error, anyone have any ideas?  No idea what I''m
          doing.

          '
        updatedAt: '2023-10-25T05:16:31.321Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fakelifesucks
    id: 6538a4afbe39573b3b2f6119
    type: comment
  author: joesnuffy
  content: '> I fixed the issue with changing "!python server.py --chat --wbits 4
    --groupsize 128 --auto-devices" to "!python server.py --chat --auto-devices",
    but I got another error "FileNotFoundError: [Errno 2] No such file or directory:
    ''models/gpt-x-alpaca-13b-native-4bit-128g/pytorch_model-00001-of-00006.bin''"
    though. Not sure if the situation is better or worse.


    I received the same exact error, anyone have any ideas?  No idea what I''m doing.

    '
  created_at: 2023-10-25 04:16:31+00:00
  edited: false
  hidden: false
  id: 6538a4afbe39573b3b2f6119
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31d09a09868f6178cce450d18d2516da.svg
      fullname: life
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fakelifesucks
      type: user
    createdAt: '2023-11-08T20:23:24.000Z'
    data:
      edited: false
      editors:
      - fakelifesucks
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3974837064743042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31d09a09868f6178cce450d18d2516da.svg
          fullname: life
          isHf: false
          isPro: false
          name: fakelifesucks
          type: user
        html: '<p>me too same error </p>

          '
        raw: 'me too same error '
        updatedAt: '2023-11-08T20:23:24.982Z'
      numEdits: 0
      reactions: []
    id: 654bee3cca834398bf4353bf
    type: comment
  author: fakelifesucks
  content: 'me too same error '
  created_at: 2023-11-08 20:23:24+00:00
  edited: false
  hidden: false
  id: 654bee3cca834398bf4353bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: Error using ooba-gooba
