!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bhaveshNOm
conflicting_files: null
created_at: 2023-04-11 13:32:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
      fullname: solanki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bhaveshNOm
      type: user
    createdAt: '2023-04-11T14:32:20.000Z'
    data:
      edited: false
      editors:
      - bhaveshNOm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
          fullname: solanki
          isHf: false
          isPro: false
          name: bhaveshNOm
          type: user
        html: "<ol>\n<li><p>your download stopped at 40% (2/5) just ignore that and\
          \ close the tab and note to delete the model files or you'll get error like\
          \ file not found or something like that after that put your model that you\
          \ downloaded in their</p>\n</li>\n<li><p>CpudefaultAllocator out of memory\
          \ you have to use swap memory you can find tuts online (if system managed\
          \ dosent work use custom size option and click on set) it will start working\
          \ now</p>\n</li>\n<li><p>if it still doesn't work edit the start bat file\
          \ and edit this line as \"call python server.py --auto-devices --chat --wbits\
          \ 4 --groupsize 128 --load-in-8bit --pre_layer 25 --gpu-memory 5\" note\
          \ that  most useful is pre_layer  you can start from pre_layer 35 and then\
          \ go lower and lower like pre_layer 25 it note  that honestly it is use\
          \ less cause if you lower it a lot it will start giving useless response\
          \ also you can increase pre_layer to get better response</p>\n</li>\n<li><p>now\
          \ if you mp be able to run it but it may still not answer you it cause ur\
          \ now ur gpu memory is low (you cant do anything other than lowering your\
          \ pre_layer)</p>\n</li>\n<li><p>if still you cant run it you have no option\
          \ other than run it on your cpu using llmac++ it will be hell of slow and\
          \ it is hard to change parameters and dont have may features like oogabooga\
          \ (or what ever that is) also it runs on cmd so there will be other problem\
          \ like you cant just paste multiple lines etc (it will take a lot of time\
          \ to generate response)</p>\n</li>\n<li><p>if you want to run it on your\
          \ cpu and want gui you can use alpaca electron here is a tutorial <a rel=\"\
          nofollow\" href=\"https://www.youtube.com/watch?v=KopKQDmGk_o\">https://www.youtube.com/watch?v=KopKQDmGk_o</a>\
          \ it dont have proper memory and will not be able to hold conversation but\
          \ is little faster also has gui so its nice and you can other cpu models\
          \ on it as well </p>\n</li>\n<li><p>this is the best you can have use koala\
          \ it also similar if not better to alpcaXgpt 4  it is also uncensored here\
          \ is the tut <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=AZUTsp9Et-o\"\
          >https://www.youtube.com/watch?v=AZUTsp9Et-o</a> you can also run it on\
          \ browser including other models like vicunia etc  got to this web site\
          \ <a rel=\"nofollow\" href=\"https://chat.lmsys.org/?model=koala-13b\">https://chat.lmsys.org/?model=koala-13b</a>\
          \ but its not that private you can also run it locally and on google cloud\
          \ instruction in the tutorial</p>\n</li>\n</ol>\n<p>I hope this helps if\
          \ i missed anything or you have something better please tell me\U0001F642\
          </p>\n"
        raw: "1) your download stopped at 40% (2/5) just ignore that and close the\
          \ tab and note to delete the model files or you'll get error like file not\
          \ found or something like that after that put your model that you downloaded\
          \ in their\r\n\r\n 2) CpudefaultAllocator out of memory you have to use\
          \ swap memory you can find tuts online (if system managed dosent work use\
          \ custom size option and click on set) it will start working now\r\n\r\n\
          3) if it still doesn't work edit the start bat file and edit this line as\
          \ \"call python server.py --auto-devices --chat --wbits 4 --groupsize 128\
          \ --load-in-8bit --pre_layer 25 --gpu-memory 5\" note that  most useful\
          \ is pre_layer  you can start from pre_layer 35 and then go lower and lower\
          \ like pre_layer 25 it note  that honestly it is use less cause if you lower\
          \ it a lot it will start giving useless response also you can increase pre_layer\
          \ to get better response\r\n\r\n4) now if you mp be able to run it but it\
          \ may still not answer you it cause ur now ur gpu memory is low (you cant\
          \ do anything other than lowering your pre_layer)\r\n\r\n5) if still you\
          \ cant run it you have no option other than run it on your cpu using llmac++\
          \ it will be hell of slow and it is hard to change parameters and dont have\
          \ may features like oogabooga (or what ever that is) also it runs on cmd\
          \ so there will be other problem like you cant just paste multiple lines\
          \ etc (it will take a lot of time to generate response)\r\n\r\n6) if you\
          \ want to run it on your cpu and want gui you can use alpaca electron here\
          \ is a tutorial https://www.youtube.com/watch?v=KopKQDmGk_o it dont have\
          \ proper memory and will not be able to hold conversation but is little\
          \ faster also has gui so its nice and you can other cpu models on it as\
          \ well \r\n\r\n7) this is the best you can have use koala it also similar\
          \ if not better to alpcaXgpt 4  it is also uncensored here is the tut https://www.youtube.com/watch?v=AZUTsp9Et-o\
          \ you can also run it on browser including other models like vicunia etc\
          \  got to this web site https://chat.lmsys.org/?model=koala-13b but its\
          \ not that private you can also run it locally and on google cloud instruction\
          \ in the tutorial\r\n\r\nI hope this helps if i missed anything or you have\
          \ something better please tell me\U0001F642"
        updatedAt: '2023-04-11T14:32:20.757Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Vladimir-E
        - NachoVill
        - wangjiang
    id: 64356f742d0ed796668b8a68
    type: comment
  author: bhaveshNOm
  content: "1) your download stopped at 40% (2/5) just ignore that and close the tab\
    \ and note to delete the model files or you'll get error like file not found or\
    \ something like that after that put your model that you downloaded in their\r\
    \n\r\n 2) CpudefaultAllocator out of memory you have to use swap memory you can\
    \ find tuts online (if system managed dosent work use custom size option and click\
    \ on set) it will start working now\r\n\r\n3) if it still doesn't work edit the\
    \ start bat file and edit this line as \"call python server.py --auto-devices\
    \ --chat --wbits 4 --groupsize 128 --load-in-8bit --pre_layer 25 --gpu-memory\
    \ 5\" note that  most useful is pre_layer  you can start from pre_layer 35 and\
    \ then go lower and lower like pre_layer 25 it note  that honestly it is use less\
    \ cause if you lower it a lot it will start giving useless response also you can\
    \ increase pre_layer to get better response\r\n\r\n4) now if you mp be able to\
    \ run it but it may still not answer you it cause ur now ur gpu memory is low\
    \ (you cant do anything other than lowering your pre_layer)\r\n\r\n5) if still\
    \ you cant run it you have no option other than run it on your cpu using llmac++\
    \ it will be hell of slow and it is hard to change parameters and dont have may\
    \ features like oogabooga (or what ever that is) also it runs on cmd so there\
    \ will be other problem like you cant just paste multiple lines etc (it will take\
    \ a lot of time to generate response)\r\n\r\n6) if you want to run it on your\
    \ cpu and want gui you can use alpaca electron here is a tutorial https://www.youtube.com/watch?v=KopKQDmGk_o\
    \ it dont have proper memory and will not be able to hold conversation but is\
    \ little faster also has gui so its nice and you can other cpu models on it as\
    \ well \r\n\r\n7) this is the best you can have use koala it also similar if not\
    \ better to alpcaXgpt 4  it is also uncensored here is the tut https://www.youtube.com/watch?v=AZUTsp9Et-o\
    \ you can also run it on browser including other models like vicunia etc  got\
    \ to this web site https://chat.lmsys.org/?model=koala-13b but its not that private\
    \ you can also run it locally and on google cloud instruction in the tutorial\r\
    \n\r\nI hope this helps if i missed anything or you have something better please\
    \ tell me\U0001F642"
  created_at: 2023-04-11 13:32:20+00:00
  edited: false
  hidden: false
  id: 64356f742d0ed796668b8a68
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'for people with low Mid pc specs '
