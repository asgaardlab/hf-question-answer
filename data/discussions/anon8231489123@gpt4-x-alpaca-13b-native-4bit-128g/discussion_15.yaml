!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bhaveshNOm
conflicting_files: null
created_at: 2023-04-09 12:22:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
      fullname: solanki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bhaveshNOm
      type: user
    createdAt: '2023-04-09T13:22:55.000Z'
    data:
      edited: false
      editors:
      - bhaveshNOm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
          fullname: solanki
          isHf: false
          isPro: false
          name: bhaveshNOm
          type: user
        html: "<p>hey i just downloaded this model (from aitreprenuer tutorial) but\
          \  when ever i start it throws this error RuntimeError: [enforce fail at\
          \ C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp\
          \ : 72] data. DefaultCPUAIIocator: not enough memory: you tried to allocate\
          \ 13107200 bytes.<br>i have 3060(6gb) and 16 gb ram<br>please help\U0001F972\
          </p>\n"
        raw: "hey i just downloaded this model (from aitreprenuer tutorial) but  when\
          \ ever i start it throws this error RuntimeError: [enforce fail at C : \\\
          cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data.\
          \ DefaultCPUAIIocator: not enough memory: you tried to allocate 13107200\
          \ bytes.\r\ni have 3060(6gb) and 16 gb ram\r\nplease help\U0001F972"
        updatedAt: '2023-04-09T13:22:55.109Z'
      numEdits: 0
      reactions: []
    id: 6432bc2f4521083b9d26283e
    type: comment
  author: bhaveshNOm
  content: "hey i just downloaded this model (from aitreprenuer tutorial) but  when\
    \ ever i start it throws this error RuntimeError: [enforce fail at C : \\cb\\\
    pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data. DefaultCPUAIIocator:\
    \ not enough memory: you tried to allocate 13107200 bytes.\r\ni have 3060(6gb)\
    \ and 16 gb ram\r\nplease help\U0001F972"
  created_at: 2023-04-09 12:22:55+00:00
  edited: false
  hidden: false
  id: 6432bc2f4521083b9d26283e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/iAR69cmgf2NP0AuCRW7NE.jpeg?w=200&h=200&f=face
      fullname: Ethan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ferret1701
      type: user
    createdAt: '2023-04-09T14:57:47.000Z'
    data:
      edited: false
      editors:
      - ferret1701
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/iAR69cmgf2NP0AuCRW7NE.jpeg?w=200&h=200&f=face
          fullname: Ethan
          isHf: false
          isPro: false
          name: ferret1701
          type: user
        html: '<p>Having the same problem here, I see many others are having the same
          problem. More people are having problems with Oobabooga and GPT x Alpaca
          than people who are actually using it</p>

          '
        raw: Having the same problem here, I see many others are having the same problem.
          More people are having problems with Oobabooga and GPT x Alpaca than people
          who are actually using it
        updatedAt: '2023-04-09T14:57:47.232Z'
      numEdits: 0
      reactions: []
    id: 6432d26b0cdd0c3686f102a6
    type: comment
  author: ferret1701
  content: Having the same problem here, I see many others are having the same problem.
    More people are having problems with Oobabooga and GPT x Alpaca than people who
    are actually using it
  created_at: 2023-04-09 13:57:47+00:00
  edited: false
  hidden: false
  id: 6432d26b0cdd0c3686f102a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cafef6caadbcf9ece6c0d8285b109892.svg
      fullname: Rodrigo Banno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlossomSpring
      type: user
    createdAt: '2023-04-09T15:11:08.000Z'
    data:
      edited: false
      editors:
      - BlossomSpring
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cafef6caadbcf9ece6c0d8285b109892.svg
          fullname: Rodrigo Banno
          isHf: false
          isPro: false
          name: BlossomSpring
          type: user
        html: '<p>Hi guys! I got the same error and was able to move past it. What
          is happening to you is that the program is trying to allocate more memory
          than your GPU has available. To solve this, you can edit the start-webui.bat
          file and add this next parameter: --gpu-memory 5<br>That "5" is the maximum
          memory the program will allocate in the GPU. Since I have a 6GB GPU, I input
          5. For a more precise number you can do something like this: --gpu-memory
          5300MiB which would be arround 5.3GB of memory.</p>

          <p>This will make the WebUI launch correctly. However, I failed to chat
          with the bot :( I got this error now "KeyError: ''model.layers.28.self_attn.q_proj.wf1''"</p>

          <p>Guys report back with your results please</p>

          '
        raw: 'Hi guys! I got the same error and was able to move past it. What is
          happening to you is that the program is trying to allocate more memory than
          your GPU has available. To solve this, you can edit the start-webui.bat
          file and add this next parameter: --gpu-memory 5

          That "5" is the maximum memory the program will allocate in the GPU. Since
          I have a 6GB GPU, I input 5. For a more precise number you can do something
          like this: --gpu-memory 5300MiB which would be arround 5.3GB of memory.


          This will make the WebUI launch correctly. However, I failed to chat with
          the bot :( I got this error now "KeyError: ''model.layers.28.self_attn.q_proj.wf1''"


          Guys report back with your results please'
        updatedAt: '2023-04-09T15:11:08.697Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\U0001F614"
        users:
        - hollowstrawberry
        - itrswssf
        - upstream99
        - sonapollowukong
        - Pedroca
        - DarkCoverUnleashed
        - gioduarte
        - IcarusMachineGun
    id: 6432d58c701755347c477f27
    type: comment
  author: BlossomSpring
  content: 'Hi guys! I got the same error and was able to move past it. What is happening
    to you is that the program is trying to allocate more memory than your GPU has
    available. To solve this, you can edit the start-webui.bat file and add this next
    parameter: --gpu-memory 5

    That "5" is the maximum memory the program will allocate in the GPU. Since I have
    a 6GB GPU, I input 5. For a more precise number you can do something like this:
    --gpu-memory 5300MiB which would be arround 5.3GB of memory.


    This will make the WebUI launch correctly. However, I failed to chat with the
    bot :( I got this error now "KeyError: ''model.layers.28.self_attn.q_proj.wf1''"


    Guys report back with your results please'
  created_at: 2023-04-09 14:11:08+00:00
  edited: false
  hidden: false
  id: 6432d58c701755347c477f27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
      fullname: ML
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Detpircsni
      type: user
    createdAt: '2023-04-09T16:37:32.000Z'
    data:
      edited: true
      editors:
      - Detpircsni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
          fullname: ML
          isHf: false
          isPro: false
          name: Detpircsni
          type: user
        html: '<p>I used the flag "--gpu-memory 7800MiB", which is 7.8 GB of GPU memory.
          I had also tried with less, and the error code is similar.</p>

          <p>I''m reporting back with this: </p>

          <p>"RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 655380480
          bytes."</p>

          '
        raw: "I used the flag \"--gpu-memory 7800MiB\", which is 7.8 GB of GPU memory.\
          \ I had also tried with less, and the error code is similar.\n\nI'm reporting\
          \ back with this: \n\n\"RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
          work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 655380480 bytes.\""
        updatedAt: '2023-04-09T16:38:14.638Z'
      numEdits: 1
      reactions: []
    id: 6432e9cccca1de06ec11877f
    type: comment
  author: Detpircsni
  content: "I used the flag \"--gpu-memory 7800MiB\", which is 7.8 GB of GPU memory.\
    \ I had also tried with less, and the error code is similar.\n\nI'm reporting\
    \ back with this: \n\n\"RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
    work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
    \ memory: you tried to allocate 655380480 bytes.\""
  created_at: 2023-04-09 15:37:32+00:00
  edited: true
  hidden: false
  id: 6432e9cccca1de06ec11877f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6dfc49dbd0eccf92fdc9fd4a64f685e0.svg
      fullname: Rodrigo Banno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YouKnowWhoItIs2
      type: user
    createdAt: '2023-04-09T18:07:04.000Z'
    data:
      edited: false
      editors:
      - YouKnowWhoItIs2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6dfc49dbd0eccf92fdc9fd4a64f685e0.svg
          fullname: Rodrigo Banno
          isHf: false
          isPro: false
          name: YouKnowWhoItIs2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Detpircsni&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Detpircsni\">@<span class=\"\
          underline\">Detpircsni</span></a></span>\n\n\t</span></span> reading your\
          \ error message I believe you are running out of CPU memory, not GPU memory.\
          \ You can use<br> --cpu-memory<br>as a parameter isntead (Remember to include\
          \ the number after it) </p>\n<p>Guy's I'll leave you this low VRAM guide\
          \ from oobabooga, it has some usefull tips and parameters you can try:<br><a\
          \ rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\"\
          >https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide</a></p>\n\
          <p>Also if you get this error after limiting the memory: \"KeyError: 'model.layers.28.self_attn.q_proj.wf1'\"\
          , you can use:<br>--pre_layer 35<br>parameter instead. If it get's you \"\
          Out of memory\" after a few texts, you can try:<br>--pre_layer 25<br>or\
          \ you may go even lower if you need to. This parameter number of layers\
          \ that will be sent to the GPU.</p>\n<p>The awful thing about all this is\
          \ that the performance is SUPER slow :(. It generates text so so slow. If\
          \ any one else finds a way to improve performance, please let us know.</p>\n"
        raw: "@Detpircsni reading your error message I believe you are running out\
          \ of CPU memory, not GPU memory. You can use\n --cpu-memory\nas a parameter\
          \ isntead (Remember to include the number after it) \n\nGuy's I'll leave\
          \ you this low VRAM guide from oobabooga, it has some usefull tips and parameters\
          \ you can try:\nhttps://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\n\
          \nAlso if you get this error after limiting the memory: \"KeyError: 'model.layers.28.self_attn.q_proj.wf1'\"\
          , you can use:\n--pre_layer 35 \nparameter instead. If it get's you \"Out\
          \ of memory\" after a few texts, you can try:\n--pre_layer 25\nor you may\
          \ go even lower if you need to. This parameter number of layers that will\
          \ be sent to the GPU.\n\nThe awful thing about all this is that the performance\
          \ is SUPER slow :(. It generates text so so slow. If any one else finds\
          \ a way to improve performance, please let us know."
        updatedAt: '2023-04-09T18:07:04.600Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - hollowstrawberry
        - GrimmDemise
        - itrswssf
        - sanbaldo
        - dimaklt
        - hwnwhd4
      - count: 1
        reaction: "\U0001F614"
        users:
        - sonapollowukong
    id: 6432fec821fb4c09457dab68
    type: comment
  author: YouKnowWhoItIs2
  content: "@Detpircsni reading your error message I believe you are running out of\
    \ CPU memory, not GPU memory. You can use\n --cpu-memory\nas a parameter isntead\
    \ (Remember to include the number after it) \n\nGuy's I'll leave you this low\
    \ VRAM guide from oobabooga, it has some usefull tips and parameters you can try:\n\
    https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\n\nAlso\
    \ if you get this error after limiting the memory: \"KeyError: 'model.layers.28.self_attn.q_proj.wf1'\"\
    , you can use:\n--pre_layer 35 \nparameter instead. If it get's you \"Out of memory\"\
    \ after a few texts, you can try:\n--pre_layer 25\nor you may go even lower if\
    \ you need to. This parameter number of layers that will be sent to the GPU.\n\
    \nThe awful thing about all this is that the performance is SUPER slow :(. It\
    \ generates text so so slow. If any one else finds a way to improve performance,\
    \ please let us know."
  created_at: 2023-04-09 17:07:04+00:00
  edited: false
  hidden: false
  id: 6432fec821fb4c09457dab68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29e8823a31d95ca32d062c9446df5ac9.svg
      fullname: Shuaib
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shuaibtkd720
      type: user
    createdAt: '2023-04-09T19:26:11.000Z'
    data:
      edited: false
      editors:
      - shuaibtkd720
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29e8823a31d95ca32d062c9446df5ac9.svg
          fullname: Shuaib
          isHf: false
          isPro: false
          name: shuaibtkd720
          type: user
        html: '<p>I tried the pre_layer 25, i get stupid responses that don''t make
          any sense</p>

          '
        raw: I tried the pre_layer 25, i get stupid responses that don't make any
          sense
        updatedAt: '2023-04-09T19:26:11.427Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - hollowstrawberry
    id: 64331153c8e2047bd9fa2e16
    type: comment
  author: shuaibtkd720
  content: I tried the pre_layer 25, i get stupid responses that don't make any sense
  created_at: 2023-04-09 18:26:11+00:00
  edited: false
  hidden: false
  id: 64331153c8e2047bd9fa2e16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/iAR69cmgf2NP0AuCRW7NE.jpeg?w=200&h=200&f=face
      fullname: Ethan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ferret1701
      type: user
    createdAt: '2023-04-09T19:46:41.000Z'
    data:
      edited: false
      editors:
      - ferret1701
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/iAR69cmgf2NP0AuCRW7NE.jpeg?w=200&h=200&f=face
          fullname: Ethan
          isHf: false
          isPro: false
          name: ferret1701
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YouKnowWhoItIs2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YouKnowWhoItIs2\"\
          >@<span class=\"underline\">YouKnowWhoItIs2</span></a></span>\n\n\t</span></span>\
          \ yeah, it seems to be a cpu memory problem for me as well. Problem is I\
          \ am allocating 10 gb and more to the UI (which I have available) and it's\
          \ still not even launching the web ui.</p>\n"
        raw: '@YouKnowWhoItIs2 yeah, it seems to be a cpu memory problem for me as
          well. Problem is I am allocating 10 gb and more to the UI (which I have
          available) and it''s still not even launching the web ui.'
        updatedAt: '2023-04-09T19:46:41.932Z'
      numEdits: 0
      reactions: []
    id: 64331621028e0ea13ad2873b
    type: comment
  author: ferret1701
  content: '@YouKnowWhoItIs2 yeah, it seems to be a cpu memory problem for me as well.
    Problem is I am allocating 10 gb and more to the UI (which I have available) and
    it''s still not even launching the web ui.'
  created_at: 2023-04-09 18:46:41+00:00
  edited: false
  hidden: false
  id: 64331621028e0ea13ad2873b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edf085f472fac4069e4d1d7425b41d9c.svg
      fullname: Rodrigo Banno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YouKnowWhoItIs3
      type: user
    createdAt: '2023-04-09T20:26:46.000Z'
    data:
      edited: false
      editors:
      - YouKnowWhoItIs3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edf085f472fac4069e4d1d7425b41d9c.svg
          fullname: Rodrigo Banno
          isHf: false
          isPro: false
          name: YouKnowWhoItIs3
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;shuaibtkd720&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/shuaibtkd720\"\
          >@<span class=\"underline\">shuaibtkd720</span></a></span>\n\n\t</span></span>\
          \ go to the \"Parameters\" panel in the WebUI and increase the \"max_new_tokens\"\
          \ to the maximum. That seems to help alot with the quality of responses.</p>\n"
        raw: '@shuaibtkd720 go to the "Parameters" panel in the WebUI and increase
          the "max_new_tokens" to the maximum. That seems to help alot with the quality
          of responses.'
        updatedAt: '2023-04-09T20:26:46.998Z'
      numEdits: 0
      reactions: []
    id: 64331f8621c6b87a1af54053
    type: comment
  author: YouKnowWhoItIs3
  content: '@shuaibtkd720 go to the "Parameters" panel in the WebUI and increase the
    "max_new_tokens" to the maximum. That seems to help alot with the quality of responses.'
  created_at: 2023-04-09 19:26:46+00:00
  edited: false
  hidden: false
  id: 64331f8621c6b87a1af54053
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
      fullname: jj maval
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luminecr
      type: user
    createdAt: '2023-04-09T22:31:32.000Z'
    data:
      edited: false
      editors:
      - luminecr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
          fullname: jj maval
          isHf: false
          isPro: false
          name: luminecr
          type: user
        html: '<p>Someone got to resolve this problem.</p>

          <p>RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.</p>

          <p>It happen to me too. Cant fix with any hack propose here or in other
          forum. I got a 3070.</p>

          '
        raw: 'Someone got to resolve this problem.



          RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.


          It happen to me too. Cant fix with any hack propose here or in other forum.
          I got a 3070.'
        updatedAt: '2023-04-09T22:31:32.438Z'
      numEdits: 0
      reactions: []
    id: 64333cc43f2096b2c9467188
    type: comment
  author: luminecr
  content: 'Someone got to resolve this problem.



    RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
    data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
    bytes.


    It happen to me too. Cant fix with any hack propose here or in other forum. I
    got a 3070.'
  created_at: 2023-04-09 21:31:32+00:00
  edited: false
  hidden: false
  id: 64333cc43f2096b2c9467188
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SprocketGames
      type: user
    createdAt: '2023-04-09T22:58:27.000Z'
    data:
      edited: true
      editors:
      - SprocketGames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
          fullname: James
          isHf: false
          isPro: false
          name: SprocketGames
          type: user
        html: '<p>I have the same issue on launch, with a 12GB 3080, and 32GB of RAM.
          I have tried the following - </p>

          <p> --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory 10 --cpu-memory
          28</p>

          <p> DefaultCPUAllocator: not enough memory: you tried to allocate 2211840
          bytes.</p>

          '
        raw: "I have the same issue on launch, with a 12GB 3080, and 32GB of RAM.\
          \ I have tried the following - \n\n --auto-devices --chat --wbits 4 --groupsize\
          \ 128 --gpu-memory 10 --cpu-memory 28\n\n DefaultCPUAllocator: not enough\
          \ memory: you tried to allocate 2211840 bytes."
        updatedAt: '2023-04-09T22:59:09.110Z'
      numEdits: 1
      reactions: []
    id: 64334313c8e2047bd9fb35fa
    type: comment
  author: SprocketGames
  content: "I have the same issue on launch, with a 12GB 3080, and 32GB of RAM. I\
    \ have tried the following - \n\n --auto-devices --chat --wbits 4 --groupsize\
    \ 128 --gpu-memory 10 --cpu-memory 28\n\n DefaultCPUAllocator: not enough memory:\
    \ you tried to allocate 2211840 bytes."
  created_at: 2023-04-09 21:58:27+00:00
  edited: true
  hidden: false
  id: 64334313c8e2047bd9fb35fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
      fullname: jj maval
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luminecr
      type: user
    createdAt: '2023-04-09T23:05:54.000Z'
    data:
      edited: false
      editors:
      - luminecr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
          fullname: jj maval
          isHf: false
          isPro: false
          name: luminecr
          type: user
        html: '<p>Which cpu do you have? I own a 3700x... AMD.</p>

          '
        raw: Which cpu do you have? I own a 3700x... AMD.
        updatedAt: '2023-04-09T23:05:54.032Z'
      numEdits: 0
      reactions: []
    id: 643344d2028e0ea13ad37da2
    type: comment
  author: luminecr
  content: Which cpu do you have? I own a 3700x... AMD.
  created_at: 2023-04-09 22:05:54+00:00
  edited: false
  hidden: false
  id: 643344d2028e0ea13ad37da2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SprocketGames
      type: user
    createdAt: '2023-04-09T23:30:29.000Z'
    data:
      edited: false
      editors:
      - SprocketGames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
          fullname: James
          isHf: false
          isPro: false
          name: SprocketGames
          type: user
        html: '<p>I also have a AMD - Ryzen 9, 5900x</p>

          '
        raw: I also have a AMD - Ryzen 9, 5900x
        updatedAt: '2023-04-09T23:30:29.679Z'
      numEdits: 0
      reactions: []
    id: 64334a953f2096b2c946bddb
    type: comment
  author: SprocketGames
  content: I also have a AMD - Ryzen 9, 5900x
  created_at: 2023-04-09 22:30:29+00:00
  edited: false
  hidden: false
  id: 64334a953f2096b2c946bddb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53c7dfc6c5a58994d3d6284966b80ac8.svg
      fullname: scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scott-hugging
      type: user
    createdAt: '2023-04-10T07:01:24.000Z'
    data:
      edited: false
      editors:
      - scott-hugging
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53c7dfc6c5a58994d3d6284966b80ac8.svg
          fullname: scott
          isHf: false
          isPro: false
          name: scott-hugging
          type: user
        html: '<p>The issue (for me) is the amount of swap memory (especially if you''re
          using Vicuna etc - the models are big.<br>I''m testing on a little desktop
          system, 16Gb ram - so another 64Gb of swap got it functional (but slow...
          that''s the price you pay!) :) Hope that helps...</p>

          '
        raw: 'The issue (for me) is the amount of swap memory (especially if you''re
          using Vicuna etc - the models are big.

          I''m testing on a little desktop system, 16Gb ram - so another 64Gb of swap
          got it functional (but slow... that''s the price you pay!) :) Hope that
          helps...'
        updatedAt: '2023-04-10T07:01:24.634Z'
      numEdits: 0
      reactions: []
    id: 6433b4441ceac17d773573cd
    type: comment
  author: scott-hugging
  content: 'The issue (for me) is the amount of swap memory (especially if you''re
    using Vicuna etc - the models are big.

    I''m testing on a little desktop system, 16Gb ram - so another 64Gb of swap got
    it functional (but slow... that''s the price you pay!) :) Hope that helps...'
  created_at: 2023-04-10 06:01:24+00:00
  edited: false
  hidden: false
  id: 6433b4441ceac17d773573cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
      fullname: solanki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bhaveshNOm
      type: user
    createdAt: '2023-04-10T14:49:18.000Z'
    data:
      edited: false
      editors:
      - bhaveshNOm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad73e7703bd3d3863e1db1a542fd8437.svg
          fullname: solanki
          isHf: false
          isPro: false
          name: bhaveshNOm
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;scott-hugging&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/scott-hugging\"\
          >@<span class=\"underline\">scott-hugging</span></a></span>\n\n\t</span></span>\
          \ didn't work for me \U0001F972 i dont know what memory it is running out\
          \ of there is peek in task manager memory.<br>i tried everything mentioned\
          \ above nothing worked.<br>i can run it in llmac++ but is really really\
          \ slow and i dont even know how to change parameter and it uses all my ram<br>is\
          \ it also happening to someone with more than 16 gb memory ?</p>\n"
        raw: "@scott-hugging didn't work for me \U0001F972 i dont know what memory\
          \ it is running out of there is peek in task manager memory.\ni tried everything\
          \ mentioned above nothing worked.\ni can run it in llmac++ but is really\
          \ really slow and i dont even know how to change parameter and it uses all\
          \ my ram\nis it also happening to someone with more than 16 gb memory ?"
        updatedAt: '2023-04-10T14:49:18.411Z'
      numEdits: 0
      reactions: []
    id: 643421eea5ee15b2ed36c02a
    type: comment
  author: bhaveshNOm
  content: "@scott-hugging didn't work for me \U0001F972 i dont know what memory it\
    \ is running out of there is peek in task manager memory.\ni tried everything\
    \ mentioned above nothing worked.\ni can run it in llmac++ but is really really\
    \ slow and i dont even know how to change parameter and it uses all my ram\nis\
    \ it also happening to someone with more than 16 gb memory ?"
  created_at: 2023-04-10 13:49:18+00:00
  edited: false
  hidden: false
  id: 643421eea5ee15b2ed36c02a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8e2d422ef596fa435047e3b4554e815.svg
      fullname: A N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Snowej
      type: user
    createdAt: '2023-04-10T17:23:18.000Z'
    data:
      edited: false
      editors:
      - Snowej
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8e2d422ef596fa435047e3b4554e815.svg
          fullname: A N
          isHf: false
          isPro: false
          name: Snowej
          type: user
        html: '<p>I had this error. I closed a few of my browser tabs and tried again,
          and it worked. 10GB VRAM and 16GB RAM.</p>

          '
        raw: I had this error. I closed a few of my browser tabs and tried again,
          and it worked. 10GB VRAM and 16GB RAM.
        updatedAt: '2023-04-10T17:23:18.512Z'
      numEdits: 0
      reactions: []
    id: 643446068d68561d704fb7b2
    type: comment
  author: Snowej
  content: I had this error. I closed a few of my browser tabs and tried again, and
    it worked. 10GB VRAM and 16GB RAM.
  created_at: 2023-04-10 16:23:18+00:00
  edited: false
  hidden: false
  id: 643446068d68561d704fb7b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
      fullname: Aiden Black
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiiredrum
      type: user
    createdAt: '2023-04-10T17:42:41.000Z'
    data:
      edited: false
      editors:
      - xiiredrum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
          fullname: Aiden Black
          isHf: false
          isPro: false
          name: xiiredrum
          type: user
        html: "<blockquote>\n<p>hey i just downloaded this model (from aitreprenuer\
          \ tutorial) but  when ever i start it throws this error RuntimeError: [enforce\
          \ fail at C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp\
          \ : 72] data. DefaultCPUAIIocator: not enough memory: you tried to allocate\
          \ 13107200 bytes.<br>i have 3060(6gb) and 16 gb ram<br>please help\U0001F972\
          </p>\n</blockquote>\n<p>It happened to me yesterday, I have a 3060 12gb\
          \ and a Ryzen 9 12 cores + 16 ram. I managed to run it just setting the\
          \ virtual memory for the hard drive my AI stuff is in manually. It works\
          \ now, tho sometimes the AI answers stuff for me, and also its a bit rude\
          \ for some unknown reason</p>\n"
        raw: "> hey i just downloaded this model (from aitreprenuer tutorial) but\
          \  when ever i start it throws this error RuntimeError: [enforce fail at\
          \ C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp\
          \ : 72] data. DefaultCPUAIIocator: not enough memory: you tried to allocate\
          \ 13107200 bytes.\n> i have 3060(6gb) and 16 gb ram\n> please help\U0001F972\
          \n\nIt happened to me yesterday, I have a 3060 12gb and a Ryzen 9 12 cores\
          \ + 16 ram. I managed to run it just setting the virtual memory for the\
          \ hard drive my AI stuff is in manually. It works now, tho sometimes the\
          \ AI answers stuff for me, and also its a bit rude for some unknown reason"
        updatedAt: '2023-04-10T17:42:41.013Z'
      numEdits: 0
      reactions: []
    id: 64344a91d12a239d72e52516
    type: comment
  author: xiiredrum
  content: "> hey i just downloaded this model (from aitreprenuer tutorial) but  when\
    \ ever i start it throws this error RuntimeError: [enforce fail at C : \\cb\\\
    pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data. DefaultCPUAIIocator:\
    \ not enough memory: you tried to allocate 13107200 bytes.\n> i have 3060(6gb)\
    \ and 16 gb ram\n> please help\U0001F972\n\nIt happened to me yesterday, I have\
    \ a 3060 12gb and a Ryzen 9 12 cores + 16 ram. I managed to run it just setting\
    \ the virtual memory for the hard drive my AI stuff is in manually. It works now,\
    \ tho sometimes the AI answers stuff for me, and also its a bit rude for some\
    \ unknown reason"
  created_at: 2023-04-10 16:42:41+00:00
  edited: false
  hidden: false
  id: 64344a91d12a239d72e52516
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
      fullname: jj maval
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luminecr
      type: user
    createdAt: '2023-04-10T19:33:29.000Z'
    data:
      edited: false
      editors:
      - luminecr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
          fullname: jj maval
          isHf: false
          isPro: false
          name: luminecr
          type: user
        html: "<blockquote>\n<blockquote>\n<p>hey i just downloaded this model (from\
          \ aitreprenuer tutorial) but  when ever i start it throws this error RuntimeError:\
          \ [enforce fail at C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\\
          alloc_cpu.cpp : 72] data. DefaultCPUAIIocator: not enough memory: you tried\
          \ to allocate 13107200 bytes.<br>i have 3060(6gb) and 16 gb ram<br>please\
          \ help\U0001F972</p>\n</blockquote>\n<p>It happened to me yesterday, I have\
          \ a 3060 12gb and a Ryzen 9 12 cores + 16 ram. I managed to run it just\
          \ setting the virtual memory for the hard drive my AI stuff is in manually.\
          \ It works now, tho sometimes the AI answers stuff for me, and also its\
          \ a bit rude for some unknown reason</p>\n</blockquote>\n<p>Which code do\
          \ you use? What do change in your settings</p>\n"
        raw: "> > hey i just downloaded this model (from aitreprenuer tutorial) but\
          \  when ever i start it throws this error RuntimeError: [enforce fail at\
          \ C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp\
          \ : 72] data. DefaultCPUAIIocator: not enough memory: you tried to allocate\
          \ 13107200 bytes.\n> > i have 3060(6gb) and 16 gb ram\n> > please help\U0001F972\
          \n> \n> It happened to me yesterday, I have a 3060 12gb and a Ryzen 9 12\
          \ cores + 16 ram. I managed to run it just setting the virtual memory for\
          \ the hard drive my AI stuff is in manually. It works now, tho sometimes\
          \ the AI answers stuff for me, and also its a bit rude for some unknown\
          \ reason\n\nWhich code do you use? What do change in your settings"
        updatedAt: '2023-04-10T19:33:29.663Z'
      numEdits: 0
      reactions: []
    id: 64346489938d07505bb8cdbc
    type: comment
  author: luminecr
  content: "> > hey i just downloaded this model (from aitreprenuer tutorial) but\
    \  when ever i start it throws this error RuntimeError: [enforce fail at C : \\\
    cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data. DefaultCPUAIIocator:\
    \ not enough memory: you tried to allocate 13107200 bytes.\n> > i have 3060(6gb)\
    \ and 16 gb ram\n> > please help\U0001F972\n> \n> It happened to me yesterday,\
    \ I have a 3060 12gb and a Ryzen 9 12 cores + 16 ram. I managed to run it just\
    \ setting the virtual memory for the hard drive my AI stuff is in manually. It\
    \ works now, tho sometimes the AI answers stuff for me, and also its a bit rude\
    \ for some unknown reason\n\nWhich code do you use? What do change in your settings"
  created_at: 2023-04-10 18:33:29+00:00
  edited: false
  hidden: false
  id: 64346489938d07505bb8cdbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
      fullname: Aiden Black
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiiredrum
      type: user
    createdAt: '2023-04-10T21:28:37.000Z'
    data:
      edited: false
      editors:
      - xiiredrum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
          fullname: Aiden Black
          isHf: false
          isPro: false
          name: xiiredrum
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>hey i just downloaded\
          \ this model (from aitreprenuer tutorial) but  when ever i start it throws\
          \ this error RuntimeError: [enforce fail at C : \\cb\\pytorch_l000000000000\\\
          work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data. DefaultCPUAIIocator: not\
          \ enough memory: you tried to allocate 13107200 bytes.<br>i have 3060(6gb)\
          \ and 16 gb ram<br>please help\U0001F972</p>\n</blockquote>\n<p>It happened\
          \ to me yesterday, I have a 3060 12gb and a Ryzen 9 12 cores + 16 ram. I\
          \ managed to run it just setting the virtual memory for the hard drive my\
          \ AI stuff is in manually. It works now, tho sometimes the AI answers stuff\
          \ for me, and also its a bit rude for some unknown reason</p>\n</blockquote>\n\
          <p>Which code do you use? What do change in your settings</p>\n</blockquote>\n\
          <p>Its not really about code, just search how to set virtual memory for\
          \ a hard drive. In my case, I have oobabooga on my D drive, and when I checked\
          \ the virtual memory on it, it was disabled. All you need to do is to select\
          \ your hard drive where oobagooba is and check the \"System managed size\"\
          \ box and that is<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630f5adf236215d0b7096776/ToYBbNvdGIEXV7eFCGBy1.png\"\
          ><img alt=\"imagen_2023-04-10_232828302.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630f5adf236215d0b7096776/ToYBbNvdGIEXV7eFCGBy1.png\"\
          ></a></p>\n"
        raw: "> > > hey i just downloaded this model (from aitreprenuer tutorial)\
          \ but  when ever i start it throws this error RuntimeError: [enforce fail\
          \ at C : \\cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp\
          \ : 72] data. DefaultCPUAIIocator: not enough memory: you tried to allocate\
          \ 13107200 bytes.\n> > > i have 3060(6gb) and 16 gb ram\n> > > please help\U0001F972\
          \n> > \n> > It happened to me yesterday, I have a 3060 12gb and a Ryzen\
          \ 9 12 cores + 16 ram. I managed to run it just setting the virtual memory\
          \ for the hard drive my AI stuff is in manually. It works now, tho sometimes\
          \ the AI answers stuff for me, and also its a bit rude for some unknown\
          \ reason\n> \n> Which code do you use? What do change in your settings\n\
          \nIts not really about code, just search how to set virtual memory for a\
          \ hard drive. In my case, I have oobabooga on my D drive, and when I checked\
          \ the virtual memory on it, it was disabled. All you need to do is to select\
          \ your hard drive where oobagooba is and check the \"System managed size\"\
          \ box and that is\n![imagen_2023-04-10_232828302.png](https://cdn-uploads.huggingface.co/production/uploads/630f5adf236215d0b7096776/ToYBbNvdGIEXV7eFCGBy1.png)"
        updatedAt: '2023-04-10T21:28:37.022Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - bhaveshNOm
        - metalskin
    id: 64347f85ef6d5abefe43427e
    type: comment
  author: xiiredrum
  content: "> > > hey i just downloaded this model (from aitreprenuer tutorial) but\
    \  when ever i start it throws this error RuntimeError: [enforce fail at C : \\\
    cb\\pytorch_l000000000000\\work\\cl0\\core\\impl\\alloc_cpu.cpp : 72] data. DefaultCPUAIIocator:\
    \ not enough memory: you tried to allocate 13107200 bytes.\n> > > i have 3060(6gb)\
    \ and 16 gb ram\n> > > please help\U0001F972\n> > \n> > It happened to me yesterday,\
    \ I have a 3060 12gb and a Ryzen 9 12 cores + 16 ram. I managed to run it just\
    \ setting the virtual memory for the hard drive my AI stuff is in manually. It\
    \ works now, tho sometimes the AI answers stuff for me, and also its a bit rude\
    \ for some unknown reason\n> \n> Which code do you use? What do change in your\
    \ settings\n\nIts not really about code, just search how to set virtual memory\
    \ for a hard drive. In my case, I have oobabooga on my D drive, and when I checked\
    \ the virtual memory on it, it was disabled. All you need to do is to select your\
    \ hard drive where oobagooba is and check the \"System managed size\" box and\
    \ that is\n![imagen_2023-04-10_232828302.png](https://cdn-uploads.huggingface.co/production/uploads/630f5adf236215d0b7096776/ToYBbNvdGIEXV7eFCGBy1.png)"
  created_at: 2023-04-10 20:28:37+00:00
  edited: false
  hidden: false
  id: 64347f85ef6d5abefe43427e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fb3854d1296d6b2564820deee26225.svg
      fullname: Ivan Dobranovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ivan531
      type: user
    createdAt: '2023-04-10T21:33:52.000Z'
    data:
      edited: false
      editors:
      - Ivan531
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fb3854d1296d6b2564820deee26225.svg
          fullname: Ivan Dobranovic
          isHf: false
          isPro: false
          name: Ivan531
          type: user
        html: '<p>Greetings,<br>When I run web UI I got the following error:</p>

          <p>Starting the web UI...<br>Warning: --cai-chat is deprecated. Use --chat
          instead.</p>

          <p>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: CUDA runtime path found: C:\ai\LLM\oobabooga-windows\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Found the following quantized
          model: models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\vicuna-13b-4bit-128g.safetensors<br>Traceback
          (most recent call last):<br>  File "C:\ai\LLM\oobabooga-windows\text-generation-webui\server.py",
          line 346, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\models.py", line
          103, in load_model<br>    model = load_quantized(model_name)<br>  File "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 136, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 32, in _load_quant<br>    model = AutoModelForCausalLM.from_config(config)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 411, in from_config<br>    return model_class._from_config(config,
          **kwargs)<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 1138, in _from_config<br>    model = cls(config, **kwargs)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 614, in <em>init</em><br>    self.model = LlamaModel(config)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 445, in <em>init</em><br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 445, in <br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 256, in <em>init</em><br>    self.mlp = LlamaMLP(<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 152, in <em>init</em><br>    self.down_proj = nn.Linear(intermediate_size,
          hidden_size, bias=False)<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\linear.py",
          line 96, in <em>init</em><br>    self.weight = Parameter(torch.empty((out_features,
          in_features), **factory_kwargs))<br>RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.</p>

          <p>1)It says I do not have enough memory. It allocated 141557760 bytes (0.14
          GB). I have 16 GB of RAM and an RTX 3060. Which is approximately 0.875%
          of RAM usage. Something dose not add up.</p>

          <ol start="2">

          <li><p>I used a few parameters in the  WEB UI bat file like: --gpu-memory
          3500MiB --cpu-memory 3000MiB( which constrains the CPU and GPU usage), --load-in-8bit,
          --auto-devices --cai-chat --wbits 4 --groupsize 128. None of them fixed
          the issue. BTW I found these in the: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide">https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide</a>.</p>

          </li>

          <li><p>I selected option a)NVIDIA, however, based on the following line
          RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes. I think it is running on CPU not GPU. I am 100% certain that I selected
          option a)NVIDIA. Which dose not add up.</p>

          </li>

          </ol>

          <p>I have been working on this the whole day. At this point I have no clue
          what to do. Keep in mind I am pretty new to all this. I have no idea if
          I am just stupid. Any help would be highly appreciated.</p>

          '
        raw: "Greetings,\nWhen I run web UI I got the following error:\n\nStarting\
          \ the web UI...\nWarning: --cai-chat is deprecated. Use --chat instead.\n\
          \n===================================BUG REPORT===================================\n\
          Welcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\
          CUDA SETUP: CUDA runtime path found: C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\bin\\cudart64_110.dll\nCUDA SETUP: Highest compute capability among\
          \ GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP:\
          \ Loading binary C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\nLoading\
          \ anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nFound the following quantized\
          \ model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
          Traceback (most recent call last):\n  File \"C:\\ai\\LLM\\oobabooga-windows\\\
          text-generation-webui\\server.py\", line 346, in <module>\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\ai\\LLM\\\
          oobabooga-windows\\text-generation-webui\\modules\\models.py\", line 103,\
          \ in load_model\n    model = load_quantized(model_name)\n  File \"C:\\ai\\\
          LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 136, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\", line 32, in _load_quant\n    model = AutoModelForCausalLM.from_config(config)\n\
          \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 411, in from_config\n\
          \    return model_class._from_config(config, **kwargs)\n  File \"C:\\ai\\\
          LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 1138, in _from_config\n    model = cls(config,\
          \ **kwargs)\n  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 614, in _init_\n    self.model = LlamaModel(config)\n  File \"C:\\\
          ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 445, in _init_\n    self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 445, in <listcomp>\n\
          \    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 256, in _init_\n\
          \    self.mlp = LlamaMLP(\n  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 152, in _init_\n    self.down_proj = nn.Linear(intermediate_size,\
          \ hidden_size, bias=False)\n  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in _init_\n\
          \    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n\
          RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 141557760 bytes.\n\n1)It says I do not have enough\
          \ memory. It allocated 141557760 bytes (0.14 GB). I have 16 GB of RAM and\
          \ an RTX 3060. Which is approximately 0.875% of RAM usage. Something dose\
          \ not add up.\n\n2) I used a few parameters in the  WEB UI bat file like:\
          \ --gpu-memory 3500MiB --cpu-memory 3000MiB( which constrains the CPU and\
          \ GPU usage), --load-in-8bit, --auto-devices --cai-chat --wbits 4 --groupsize\
          \ 128. None of them fixed the issue. BTW I found these in the: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide.\n\
          \n3) I selected option a)NVIDIA, however, based on the following line RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 141557760 bytes. I think it is running on CPU not GPU. I am\
          \ 100% certain that I selected option a)NVIDIA. Which dose not add up.\n\
          \nI have been working on this the whole day. At this point I have no clue\
          \ what to do. Keep in mind I am pretty new to all this. I have no idea if\
          \ I am just stupid. Any help would be highly appreciated."
        updatedAt: '2023-04-10T21:33:52.808Z'
      numEdits: 0
      reactions: []
    id: 643480c0a5ee15b2ed397e76
    type: comment
  author: Ivan531
  content: "Greetings,\nWhen I run web UI I got the following error:\n\nStarting the\
    \ web UI...\nWarning: --cai-chat is deprecated. Use --chat instead.\n\n===================================BUG\
    \ REPORT===================================\nWelcome to bitsandbytes. For bug\
    \ reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    CUDA SETUP: CUDA runtime path found: C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
    env\\bin\\cudart64_110.dll\nCUDA SETUP: Highest compute capability among GPUs\
    \ detected: 8.6\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary\
    \ C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\\
    libbitsandbytes_cuda117.dll...\nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
    Found the following quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\
    vicuna-13b-4bit-128g.safetensors\nTraceback (most recent call last):\n  File \"\
    C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\server.py\", line 346,\
    \ in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\models.py\"\
    , line 103, in load_model\n    model = load_quantized(model_name)\n  File \"C:\\\
    ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\",\
    \ line 136, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 32, in _load_quant\n    model = AutoModelForCausalLM.from_config(config)\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\", line 411, in from_config\n    return\
    \ model_class._from_config(config, **kwargs)\n  File \"C:\\ai\\LLM\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\", line\
    \ 1138, in _from_config\n    model = cls(config, **kwargs)\n  File \"C:\\ai\\\
    LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 614, in _init_\n    self.model = LlamaModel(config)\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 445, in _init_\n    self.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 445, in <listcomp>\n  \
    \  self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 256, in _init_\n    self.mlp\
    \ = LlamaMLP(\n  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 152,\
    \ in _init_\n    self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n\
    \  File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\linear.py\", line 96, in _init_\n    self.weight = Parameter(torch.empty((out_features,\
    \ in_features), **factory_kwargs))\nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
    work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
    \ memory: you tried to allocate 141557760 bytes.\n\n1)It says I do not have enough\
    \ memory. It allocated 141557760 bytes (0.14 GB). I have 16 GB of RAM and an RTX\
    \ 3060. Which is approximately 0.875% of RAM usage. Something dose not add up.\n\
    \n2) I used a few parameters in the  WEB UI bat file like: --gpu-memory 3500MiB\
    \ --cpu-memory 3000MiB( which constrains the CPU and GPU usage), --load-in-8bit,\
    \ --auto-devices --cai-chat --wbits 4 --groupsize 128. None of them fixed the\
    \ issue. BTW I found these in the: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide.\n\
    \n3) I selected option a)NVIDIA, however, based on the following line RuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
    \ bytes. I think it is running on CPU not GPU. I am 100% certain that I selected\
    \ option a)NVIDIA. Which dose not add up.\n\nI have been working on this the whole\
    \ day. At this point I have no clue what to do. Keep in mind I am pretty new to\
    \ all this. I have no idea if I am just stupid. Any help would be highly appreciated."
  created_at: 2023-04-10 20:33:52+00:00
  edited: false
  hidden: false
  id: 643480c0a5ee15b2ed397e76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac077e716fa8a46cbd4f3ab5c223a2db.svg
      fullname: shane freeman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sfreeman88
      type: user
    createdAt: '2023-04-10T21:43:19.000Z'
    data:
      edited: true
      editors:
      - sfreeman88
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac077e716fa8a46cbd4f3ab5c223a2db.svg
          fullname: shane freeman
          isHf: false
          isPro: false
          name: sfreeman88
          type: user
        html: '<p>Hey guys I have a solution. I was having the same "Cuda out of memory"
          issue with a 3080. I tried to flag --gpu memory 10 and I got this error:</p>

          <p>Traceback (most recent call last):<br>  File "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\text-generation-webui\server.py",
          line 346, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\text-generation-webui\modules\models.py",
          line 103, in load_model<br>    model = load_quantized(model_name)<br>  File
          "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 147, in load_quantized<br>    device_map = accelerate.infer_auto_device_map(model,
          max_memory=max_memory, no_split_module_classes=["LlamaDecoderLayer"])<br>  File
          "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\installer_files\env\lib\site-packages\accelerate\utils\modeling.py",
          line 567, in infer_auto_device_map<br>    max_memory = get_max_memory(max_memory)<br>  File
          "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\installer_files\env\lib\site-packages\accelerate\utils\modeling.py",
          line 380, in get_max_memory<br>    max_memory[key] = convert_file_size_to_int(max_memory[key])<br>  File
          "C:\AI Programs\GPT Programs\oobabooga-windows\oobabooga-windows\installer_files\env\lib\site-packages\accelerate\utils\modeling.py",
          line 59, in convert_file_size_to_int<br>    return int(size[:-3]) * (2**30)<br>ValueError:
          invalid literal for int() with base 10: ''10GB''</p>

          <p>the value error there is basically saying the integer expects the input
          to be in bytes not GB. This means every time you write --gpu memory 10 it
          takes that as "10 bytes". If you specify "GB" it wont read it at all. So
          I changed it to --gpu Memory 10737418240 and that seems to have solved at
          least one of the issues. I am still working towards following the breadcrumbs
          to the ultimate issue here.</p>

          '
        raw: "Hey guys I have a solution. I was having the same \"Cuda out of memory\"\
          \ issue with a 3080. I tried to flag --gpu memory 10 and I got this error:\n\
          \nTraceback (most recent call last):\n  File \"C:\\AI Programs\\GPT Programs\\\
          oobabooga-windows\\oobabooga-windows\\text-generation-webui\\server.py\"\
          , line 346, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\\
          text-generation-webui\\modules\\models.py\", line 103, in load_model\n \
          \   model = load_quantized(model_name)\n  File \"C:\\AI Programs\\GPT Programs\\\
          oobabooga-windows\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 147, in load_quantized\n    device_map = accelerate.infer_auto_device_map(model,\
          \ max_memory=max_memory, no_split_module_classes=[\"LlamaDecoderLayer\"\
          ])\n  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py\"\
          , line 567, in infer_auto_device_map\n    max_memory = get_max_memory(max_memory)\n\
          \  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py\"\
          , line 380, in get_max_memory\n    max_memory[key] = convert_file_size_to_int(max_memory[key])\n\
          \  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py\"\
          , line 59, in convert_file_size_to_int\n    return int(size[:-3]) * (2**30)\n\
          ValueError: invalid literal for int() with base 10: '10GB'\n\nthe value\
          \ error there is basically saying the integer expects the input to be in\
          \ bytes not GB. This means every time you write --gpu memory 10 it takes\
          \ that as \"10 bytes\". If you specify \"GB\" it wont read it at all. So\
          \ I changed it to --gpu Memory 10737418240 and that seems to have solved\
          \ at least one of the issues. I am still working towards following the breadcrumbs\
          \ to the ultimate issue here."
        updatedAt: '2023-04-10T21:47:15.390Z'
      numEdits: 1
      reactions: []
    id: 643482f7d12a239d72e6a997
    type: comment
  author: sfreeman88
  content: "Hey guys I have a solution. I was having the same \"Cuda out of memory\"\
    \ issue with a 3080. I tried to flag --gpu memory 10 and I got this error:\n\n\
    Traceback (most recent call last):\n  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\\
    oobabooga-windows\\text-generation-webui\\server.py\", line 346, in <module>\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name)\n  File \"\
    C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\text-generation-webui\\\
    modules\\models.py\", line 103, in load_model\n    model = load_quantized(model_name)\n\
    \  File \"C:\\AI Programs\\GPT Programs\\oobabooga-windows\\oobabooga-windows\\\
    text-generation-webui\\modules\\GPTQ_loader.py\", line 147, in load_quantized\n\
    \    device_map = accelerate.infer_auto_device_map(model, max_memory=max_memory,\
    \ no_split_module_classes=[\"LlamaDecoderLayer\"])\n  File \"C:\\AI Programs\\\
    GPT Programs\\oobabooga-windows\\oobabooga-windows\\installer_files\\env\\lib\\\
    site-packages\\accelerate\\utils\\modeling.py\", line 567, in infer_auto_device_map\n\
    \    max_memory = get_max_memory(max_memory)\n  File \"C:\\AI Programs\\GPT Programs\\\
    oobabooga-windows\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    accelerate\\utils\\modeling.py\", line 380, in get_max_memory\n    max_memory[key]\
    \ = convert_file_size_to_int(max_memory[key])\n  File \"C:\\AI Programs\\GPT Programs\\\
    oobabooga-windows\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    accelerate\\utils\\modeling.py\", line 59, in convert_file_size_to_int\n    return\
    \ int(size[:-3]) * (2**30)\nValueError: invalid literal for int() with base 10:\
    \ '10GB'\n\nthe value error there is basically saying the integer expects the\
    \ input to be in bytes not GB. This means every time you write --gpu memory 10\
    \ it takes that as \"10 bytes\". If you specify \"GB\" it wont read it at all.\
    \ So I changed it to --gpu Memory 10737418240 and that seems to have solved at\
    \ least one of the issues. I am still working towards following the breadcrumbs\
    \ to the ultimate issue here."
  created_at: 2023-04-10 20:43:19+00:00
  edited: true
  hidden: false
  id: 643482f7d12a239d72e6a997
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
      fullname: Aiden Black
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiiredrum
      type: user
    createdAt: '2023-04-10T21:51:47.000Z'
    data:
      edited: false
      editors:
      - xiiredrum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
          fullname: Aiden Black
          isHf: false
          isPro: false
          name: xiiredrum
          type: user
        html: '<blockquote>

          <p>Greetings,<br>When I run web UI I got the following error:</p>

          <p>Starting the web UI...<br>Warning: --cai-chat is deprecated. Use --chat
          instead.</p>

          <p>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: CUDA runtime path found: C:\ai\LLM\oobabooga-windows\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Found the following quantized
          model: models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\vicuna-13b-4bit-128g.safetensors<br>Traceback
          (most recent call last):<br>  File "C:\ai\LLM\oobabooga-windows\text-generation-webui\server.py",
          line 346, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\models.py", line
          103, in load_model<br>    model = load_quantized(model_name)<br>  File "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 136, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\ai\LLM\oobabooga-windows\text-generation-webui\modules\GPTQ_loader.py",
          line 32, in _load_quant<br>    model = AutoModelForCausalLM.from_config(config)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 411, in from_config<br>    return model_class._from_config(config,
          **kwargs)<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 1138, in _from_config<br>    model = cls(config, **kwargs)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 614, in <em>init</em><br>    self.model = LlamaModel(config)<br>  File
          "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 445, in <em>init</em><br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 445, in <br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 256, in <em>init</em><br>    self.mlp = LlamaMLP(<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 152, in <em>init</em><br>    self.down_proj = nn.Linear(intermediate_size,
          hidden_size, bias=False)<br>  File "C:\ai\LLM\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\linear.py",
          line 96, in <em>init</em><br>    self.weight = Parameter(torch.empty((out_features,
          in_features), **factory_kwargs))<br>RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.</p>

          <p>1)It says I do not have enough memory. It allocated 141557760 bytes (0.14
          GB). I have 16 GB of RAM and an RTX 3060. Which is approximately 0.875%
          of RAM usage. Something dose not add up.</p>

          <ol start="2">

          <li><p>I used a few parameters in the  WEB UI bat file like: --gpu-memory
          3500MiB --cpu-memory 3000MiB( which constrains the CPU and GPU usage), --load-in-8bit,
          --auto-devices --cai-chat --wbits 4 --groupsize 128. None of them fixed
          the issue. BTW I found these in the: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide">https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide</a>.</p>

          </li>

          <li><p>I selected option a)NVIDIA, however, based on the following line
          RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes. I think it is running on CPU not GPU. I am 100% certain that I selected
          option a)NVIDIA. Which dose not add up.</p>

          </li>

          </ol>

          <p>I have been working on this the whole day. At this point I have no clue
          what to do. Keep in mind I am pretty new to all this. I have no idea if
          I am just stupid. Any help would be highly appreciated.</p>

          </blockquote>

          <p>It may sound weird but just change the virtual memory of the HDD where
          you have oobabooga, I answered earlier in this topic with the same problem
          and managed to fix it by just doing that</p>

          '
        raw: "> Greetings,\n> When I run web UI I got the following error:\n> \n>\
          \ Starting the web UI...\n> Warning: --cai-chat is deprecated. Use --chat\
          \ instead.\n> \n> ===================================BUG REPORT===================================\n\
          > Welcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\n> ================================================================================\n\
          > CUDA SETUP: CUDA runtime path found: C:\\ai\\LLM\\oobabooga-windows\\\
          installer_files\\env\\bin\\cudart64_110.dll\n> CUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.6\n> CUDA SETUP: Detected CUDA version\
          \ 117\n> CUDA SETUP: Loading binary C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n\
          > Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n> Found the following\
          \ quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
          > Traceback (most recent call last):\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\\
          text-generation-webui\\server.py\", line 346, in <module>\n>     shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n>   File \"C:\\ai\\\
          LLM\\oobabooga-windows\\text-generation-webui\\modules\\models.py\", line\
          \ 103, in load_model\n>     model = load_quantized(model_name)\n>   File\
          \ \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 136, in load_quantized\n>     model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          >   File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\", line 32, in _load_quant\n>     model = AutoModelForCausalLM.from_config(config)\n\
          >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 411, in from_config\n\
          >     return model_class._from_config(config, **kwargs)\n>   File \"C:\\\
          ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 1138, in _from_config\n>     model = cls(config,\
          \ **kwargs)\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 614, in _init_\n>     self.model = LlamaModel(config)\n>   File \"\
          C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 445, in _init_\n\
          >     self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 445, in <listcomp>\n\
          >     self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 256, in _init_\n\
          >     self.mlp = LlamaMLP(\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 152, in _init_\n>     self.down_proj = nn.Linear(intermediate_size,\
          \ hidden_size, bias=False)\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\"\
          , line 96, in _init_\n>     self.weight = Parameter(torch.empty((out_features,\
          \ in_features), **factory_kwargs))\n> RuntimeError: [enforce fail at C:\\\
          cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data.\
          \ DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
          \ bytes.\n> \n> 1)It says I do not have enough memory. It allocated 141557760\
          \ bytes (0.14 GB). I have 16 GB of RAM and an RTX 3060. Which is approximately\
          \ 0.875% of RAM usage. Something dose not add up.\n> \n> 2) I used a few\
          \ parameters in the  WEB UI bat file like: --gpu-memory 3500MiB --cpu-memory\
          \ 3000MiB( which constrains the CPU and GPU usage), --load-in-8bit, --auto-devices\
          \ --cai-chat --wbits 4 --groupsize 128. None of them fixed the issue. BTW\
          \ I found these in the: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide.\n\
          > \n> 3) I selected option a)NVIDIA, however, based on the following line\
          \ RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 141557760 bytes. I think it is running on CPU not\
          \ GPU. I am 100% certain that I selected option a)NVIDIA. Which dose not\
          \ add up.\n> \n> I have been working on this the whole day. At this point\
          \ I have no clue what to do. Keep in mind I am pretty new to all this. I\
          \ have no idea if I am just stupid. Any help would be highly appreciated.\n\
          \nIt may sound weird but just change the virtual memory of the HDD where\
          \ you have oobabooga, I answered earlier in this topic with the same problem\
          \ and managed to fix it by just doing that"
        updatedAt: '2023-04-10T21:51:47.419Z'
      numEdits: 0
      reactions: []
    id: 643484f3a5ee15b2ed399b50
    type: comment
  author: xiiredrum
  content: "> Greetings,\n> When I run web UI I got the following error:\n> \n> Starting\
    \ the web UI...\n> Warning: --cai-chat is deprecated. Use --chat instead.\n> \n\
    > ===================================BUG REPORT===================================\n\
    > Welcome to bitsandbytes. For bug reports, please submit your error trace to:\
    \ https://github.com/TimDettmers/bitsandbytes/issues\n> ================================================================================\n\
    > CUDA SETUP: CUDA runtime path found: C:\\ai\\LLM\\oobabooga-windows\\installer_files\\\
    env\\bin\\cudart64_110.dll\n> CUDA SETUP: Highest compute capability among GPUs\
    \ detected: 8.6\n> CUDA SETUP: Detected CUDA version 117\n> CUDA SETUP: Loading\
    \ binary C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    bitsandbytes\\libbitsandbytes_cuda117.dll...\n> Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
    > Found the following quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\
    vicuna-13b-4bit-128g.safetensors\n> Traceback (most recent call last):\n>   File\
    \ \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\server.py\", line 346,\
    \ in <module>\n>     shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\models.py\"\
    , line 103, in load_model\n>     model = load_quantized(model_name)\n>   File\
    \ \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 136, in load_quantized\n>     model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 32, in _load_quant\n>     model = AutoModelForCausalLM.from_config(config)\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\", line 411, in from_config\n>   \
    \  return model_class._from_config(config, **kwargs)\n>   File \"C:\\ai\\LLM\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
    , line 1138, in _from_config\n>     model = cls(config, **kwargs)\n>   File \"\
    C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 614, in _init_\n>     self.model = LlamaModel(config)\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 445, in _init_\n>     self.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 445, in <listcomp>\n> \
    \    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    >   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 256, in _init_\n>     self.mlp\
    \ = LlamaMLP(\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 152,\
    \ in _init_\n>     self.down_proj = nn.Linear(intermediate_size, hidden_size,\
    \ bias=False)\n>   File \"C:\\ai\\LLM\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in _init_\n>   \
    \  self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n\
    > RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\\
    impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
    \ to allocate 141557760 bytes.\n> \n> 1)It says I do not have enough memory. It\
    \ allocated 141557760 bytes (0.14 GB). I have 16 GB of RAM and an RTX 3060. Which\
    \ is approximately 0.875% of RAM usage. Something dose not add up.\n> \n> 2) I\
    \ used a few parameters in the  WEB UI bat file like: --gpu-memory 3500MiB --cpu-memory\
    \ 3000MiB( which constrains the CPU and GPU usage), --load-in-8bit, --auto-devices\
    \ --cai-chat --wbits 4 --groupsize 128. None of them fixed the issue. BTW I found\
    \ these in the: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide.\n\
    > \n> 3) I selected option a)NVIDIA, however, based on the following line RuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
    \ bytes. I think it is running on CPU not GPU. I am 100% certain that I selected\
    \ option a)NVIDIA. Which dose not add up.\n> \n> I have been working on this the\
    \ whole day. At this point I have no clue what to do. Keep in mind I am pretty\
    \ new to all this. I have no idea if I am just stupid. Any help would be highly\
    \ appreciated.\n\nIt may sound weird but just change the virtual memory of the\
    \ HDD where you have oobabooga, I answered earlier in this topic with the same\
    \ problem and managed to fix it by just doing that"
  created_at: 2023-04-10 20:51:47+00:00
  edited: false
  hidden: false
  id: 643484f3a5ee15b2ed399b50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SprocketGames
      type: user
    createdAt: '2023-04-11T08:11:26.000Z'
    data:
      edited: false
      editors:
      - SprocketGames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2aae4c558f4a186e41104912bc909fca.svg
          fullname: James
          isHf: false
          isPro: false
          name: SprocketGames
          type: user
        html: '<p>you shouldn''t haver to do this with 32gb of ram though (which I
          have) - this still seems bugged to me but its likely an oobabooga thing,
          not a model thing</p>

          '
        raw: you shouldn't haver to do this with 32gb of ram though (which I have)
          - this still seems bugged to me but its likely an oobabooga thing, not a
          model thing
        updatedAt: '2023-04-11T08:11:26.790Z'
      numEdits: 0
      reactions: []
    id: 6435162e9f2ac2e2135ea338
    type: comment
  author: SprocketGames
  content: you shouldn't haver to do this with 32gb of ram though (which I have) -
    this still seems bugged to me but its likely an oobabooga thing, not a model thing
  created_at: 2023-04-11 07:11:26+00:00
  edited: false
  hidden: false
  id: 6435162e9f2ac2e2135ea338
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
      fullname: Aiden Black
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiiredrum
      type: user
    createdAt: '2023-04-11T19:25:11.000Z'
    data:
      edited: false
      editors:
      - xiiredrum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661950635611-noauth.jpeg?w=200&h=200&f=face
          fullname: Aiden Black
          isHf: false
          isPro: false
          name: xiiredrum
          type: user
        html: '<blockquote>

          <p>you shouldn''t haver to do this with 32gb of ram though (which I have)
          - this still seems bugged to me but its likely an oobabooga thing, not a
          model thing</p>

          </blockquote>

          <p>If you watch the video made by Aitrepreneur on this whole thing, he can
          run it just fine and everything works perfectly well on oobabooga</p>

          '
        raw: '> you shouldn''t haver to do this with 32gb of ram though (which I have)
          - this still seems bugged to me but its likely an oobabooga thing, not a
          model thing


          If you watch the video made by Aitrepreneur on this whole thing, he can
          run it just fine and everything works perfectly well on oobabooga'
        updatedAt: '2023-04-11T19:25:11.312Z'
      numEdits: 0
      reactions: []
    id: 6435b4177e07c5aee23783de
    type: comment
  author: xiiredrum
  content: '> you shouldn''t haver to do this with 32gb of ram though (which I have)
    - this still seems bugged to me but its likely an oobabooga thing, not a model
    thing


    If you watch the video made by Aitrepreneur on this whole thing, he can run it
    just fine and everything works perfectly well on oobabooga'
  created_at: 2023-04-11 18:25:11+00:00
  edited: false
  hidden: false
  id: 6435b4177e07c5aee23783de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
      fullname: ML
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Detpircsni
      type: user
    createdAt: '2023-04-11T22:39:07.000Z'
    data:
      edited: false
      editors:
      - Detpircsni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
          fullname: ML
          isHf: false
          isPro: false
          name: Detpircsni
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Detpircsni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Detpircsni\"\
          >@<span class=\"underline\">Detpircsni</span></a></span>\n\n\t</span></span>\
          \ reading your error message I believe you are running out of CPU memory,\
          \ not GPU memory. You can use<br> --cpu-memory<br>as a parameter isntead\
          \ (Remember to include the number after it) </p>\n<p>Guy's I'll leave you\
          \ this low VRAM guide from oobabooga, it has some usefull tips and parameters\
          \ you can try:<br><a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\"\
          >https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide</a></p>\n\
          <p>Also if you get this error after limiting the memory: \"KeyError: 'model.layers.28.self_attn.q_proj.wf1'\"\
          , you can use:<br>--pre_layer 35<br>parameter instead. If it get's you \"\
          Out of memory\" after a few texts, you can try:<br>--pre_layer 25<br>or\
          \ you may go even lower if you need to. This parameter number of layers\
          \ that will be sent to the GPU.</p>\n<p>The awful thing about all this is\
          \ that the performance is SUPER slow :(. It generates text so so slow. If\
          \ any one else finds a way to improve performance, please let us know.</p>\n\
          </blockquote>\n<p>(To you, and to anyone else who is reading)I have a 25+\
          \ GB system, and have actually read the guide and tried all of those suggestions.\
          \ At the time of the post, my prelayers were set to 50, and went as far\
          \ as to use the --disk flag to allocate memory there. Despite this, it had\
          \ no effect on the error code.</p>\n<p>I tried <span data-props=\"{&quot;user&quot;:&quot;sfreeman88&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sfreeman88\"\
          >@<span class=\"underline\">sfreeman88</span></a></span>\n\n\t</span></span>\
          \ 's info about the --gpu-memory flag. My gpu-memory was 7, as many guidelines\
          \ would advise. I adjusted it to --gpu-memory 7073741824, which should be\
          \ roughly 7 gigabytes in bytes.</p>\n<p>The same error persisted, but the\
          \ attempted allocation was 141,557,760 bytes, which doesn't even seem to\
          \ amount to 1 GB. Something is clearly wrong.</p>\n<p>I tried the virtual\
          \ memory option suggested by <span data-props=\"{&quot;user&quot;:&quot;xiiredrum&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xiiredrum\"\
          >@<span class=\"underline\">xiiredrum</span></a></span>\n\n\t</span></span>.\
          \ I typed \"advanced system settings\" in the search, clicked on \"settings\"\
          \ under performance, went to the \"advanced\" tab, clicked on the \"change\"\
          \ button, and unchecked the \"automatically manage paging file size for\
          \ all drives\" box. I noticed only my OS drive had a paging file size enabled,\
          \ which was where oogabooga was located. It was still worth a try. Checking\
          \ the \"system managed size\" for each drive didn't work, as it disabled\
          \ my changes after leaving that window. Setting a custom size resolves this\
          \ problem(5024-50024). After a restart, I checked my results.</p>\n<p>The\
          \ error code changed. I interpreted this as a positive, at least: </p>\n\
          <p>=========</p>\n<p>Loading vicuna-13b-GPTQ-4bit-128g...<br>Loading model\
          \ ...<br>C:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()<br>  with safe_open(filename, framework=\"pt\", device=device)\
          \ as f:<br>C:\\Program Files\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\torch_utils.py:776: UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()<br>  return self.fget.<strong>get</strong>(instance,\
          \ owner)()<br>C:\\Program Files\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is\
          \ deprecated. It will be removed in the future and UntypedStorage will be\
          \ the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()<br>  storage = cls(wrap_storage=untyped_storage)</p>\n\
          <p>==========</p>\n<p>After this error code, it hangs and nothing happens.\
          \ But I ran it a second time:</p>\n<p>======</p>\n<p>Loading vicuna-13b-GPTQ-4bit-128g...<br>Loading\
          \ model ...<br>C:\\Program Files\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()<br>  with safe_open(filename, framework=\"\
          pt\", device=device) as f:<br>C:\\Program Files\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch_utils.py:776: UserWarning: TypedStorage is\
          \ deprecated. It will be removed in the future and UntypedStorage will be\
          \ the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()<br>  return self.fget.<strong>get</strong>(instance,\
          \ owner)()<br>C:\\Program Files\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is\
          \ deprecated. It will be removed in the future and UntypedStorage will be\
          \ the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()<br>  storage = cls(wrap_storage=untyped_storage)<br>Done.<br>Using\
          \ the following device map for the 4-bit model: {'': 0}<br>Loaded the model\
          \ in 15.61 seconds.<br>Running on local URL:  <a rel=\"nofollow\" href=\"\
          http://127.0.0.1:7860\">http://127.0.0.1:7860</a><br>3<br>To create a public\
          \ link, set <code>share=True</code> in <code>launch()</code>.</p>\n<p>=======</p>\n\
          <p>My flags were --wbits 4 --groupsize 128 --gpu-memory 77073741824. All\
          \ my internet browser windows were also closed. It is possible all solutions\
          \ lead to this outcome, which no longer seems related to memory hell, at\
          \ least. I also tried it with GPT4 Alpaca, and the results for this whole\
          \ post are the same.</p>\n<p>Additional Observations: </p>\n<p>I can confirm\
          \ that it is functional, but leaving your browser open is a mistake for\
          \ not-so-powerful GPUs. It, and everything else, becomes very bogged down.\
          \ It loads quickly, and then begins to draw resources upon generation. Setting\
          \ the auto-devices flag on in this scenario is advised, and still seems\
          \ to allow operation with the above troubleshooting, so does the pre-layer\
          \ flag, and the threads flag. In my case, my 8gb of vram couldn't handle\
          \ usage at 7, and so I reduced it to 5, which I would also advise. Upon\
          \ removing byte format in --gpu-memory, and adding, say, a \"7\", or presenting\
          \ it in MiBs, the UI interprets every input as a successfully generated\
          \ output from the terminal(I never tried to reproduce the error, take that\
          \ with a grain of salt), error below: </p>\n<p>KeyError: 'model.layers.27.self_attn.q_proj.wf1'</p>\n\
          <p>So, I reversed that change in the .bat file. I also learned anything\
          \ lower than pre-layer 25 is not that functional without a decently up to\
          \ date GPU, I wouldn't advise. In an attempt to make it faster, via the\
          \ UI I tested out things like deepspeed, bf16, flexgen, cuda features, etc.</p>\n\
          <p>Conclusion:</p>\n<p>After everything worked and all was said and done,\
          \ if you don't have a very high-end PC, this won't be a good experience\
          \ for you, and I would advise getting a colab version together or trying\
          \ another alternative. At 0.89 tokens per second, it will feel like watching\
          \ paint dry. This is also a very delicate program, just about anything breaks\
          \ it or sets it off in the wrong direction. If you lack patience, try koboldAI\
          \ or NAI.</p>\n<p>Despite this, thank you everyone for the help.<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/0HVBcqSaX3Ya7eOIANHEo.png\"\
          ><img alt=\"Capture.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/0HVBcqSaX3Ya7eOIANHEo.png\"\
          ></a></p>\n"
        raw: "> @Detpircsni reading your error message I believe you are running out\
          \ of CPU memory, not GPU memory. You can use\n>  --cpu-memory\n> as a parameter\
          \ isntead (Remember to include the number after it) \n> \n> Guy's I'll leave\
          \ you this low VRAM guide from oobabooga, it has some usefull tips and parameters\
          \ you can try:\n> https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\n\
          > \n> Also if you get this error after limiting the memory: \"KeyError:\
          \ 'model.layers.28.self_attn.q_proj.wf1'\", you can use:\n> --pre_layer\
          \ 35 \n> parameter instead. If it get's you \"Out of memory\" after a few\
          \ texts, you can try:\n> --pre_layer 25\n> or you may go even lower if you\
          \ need to. This parameter number of layers that will be sent to the GPU.\n\
          > \n> The awful thing about all this is that the performance is SUPER slow\
          \ :(. It generates text so so slow. If any one else finds a way to improve\
          \ performance, please let us know.\n\n(To you, and to anyone else who is\
          \ reading)I have a 25+ GB system, and have actually read the guide and tried\
          \ all of those suggestions. At the time of the post, my prelayers were set\
          \ to 50, and went as far as to use the --disk flag to allocate memory there.\
          \ Despite this, it had no effect on the error code.\n\nI tried @sfreeman88\
          \ 's info about the --gpu-memory flag. My gpu-memory was 7, as many guidelines\
          \ would advise. I adjusted it to --gpu-memory 7073741824, which should be\
          \ roughly 7 gigabytes in bytes.\n\nThe same error persisted, but the attempted\
          \ allocation was 141,557,760 bytes, which doesn't even seem to amount to\
          \ 1 GB. Something is clearly wrong.\n\nI tried the virtual memory option\
          \ suggested by @xiiredrum. I typed \"advanced system settings\" in the search,\
          \ clicked on \"settings\" under performance, went to the \"advanced\" tab,\
          \ clicked on the \"change\" button, and unchecked the \"automatically manage\
          \ paging file size for all drives\" box. I noticed only my OS drive had\
          \ a paging file size enabled, which was where oogabooga was located. It\
          \ was still worth a try. Checking the \"system managed size\" for each drive\
          \ didn't work, as it disabled my changes after leaving that window. Setting\
          \ a custom size resolves this problem(5024-50024). After a restart, I checked\
          \ my results.\n\nThe error code changed. I interpreted this as a positive,\
          \ at least: \n\n=========\n\nLoading vicuna-13b-GPTQ-4bit-128g...\nLoading\
          \ model ...\nC:\\Program Files\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  with safe_open(filename, framework=\"pt\"\
          , device=device) as f:\nC:\\Program Files\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\
          C:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\storage.py:899: UserWarning: TypedStorage is deprecated. It will\
          \ be removed in the future and UntypedStorage will be the only storage class.\
          \ This should only matter to you if you are using storages directly.  To\
          \ access UntypedStorage directly, use tensor.untyped_storage() instead of\
          \ tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\n\n==========\n\
          \nAfter this error code, it hangs and nothing happens. But I ran it a second\
          \ time:\n\n======\n\nLoading vicuna-13b-GPTQ-4bit-128g...\nLoading model\
          \ ...\nC:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()\n  with safe_open(filename, framework=\"pt\", device=device)\
          \ as f:\nC:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nC:\\\
          Program Files\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\storage.py:899: UserWarning: TypedStorage is deprecated. It will\
          \ be removed in the future and UntypedStorage will be the only storage class.\
          \ This should only matter to you if you are using storages directly.  To\
          \ access UntypedStorage directly, use tensor.untyped_storage() instead of\
          \ tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\nDone.\n\
          Using the following device map for the 4-bit model: {'': 0}\nLoaded the\
          \ model in 15.61 seconds.\nRunning on local URL:  http://127.0.0.1:7860\n\
          3\nTo create a public link, set `share=True` in `launch()`.\n\n=======\n\
          \nMy flags were --wbits 4 --groupsize 128 --gpu-memory 77073741824. All\
          \ my internet browser windows were also closed. It is possible all solutions\
          \ lead to this outcome, which no longer seems related to memory hell, at\
          \ least. I also tried it with GPT4 Alpaca, and the results for this whole\
          \ post are the same.\n\nAdditional Observations: \n\nI can confirm that\
          \ it is functional, but leaving your browser open is a mistake for not-so-powerful\
          \ GPUs. It, and everything else, becomes very bogged down. It loads quickly,\
          \ and then begins to draw resources upon generation. Setting the auto-devices\
          \ flag on in this scenario is advised, and still seems to allow operation\
          \ with the above troubleshooting, so does the pre-layer flag, and the threads\
          \ flag. In my case, my 8gb of vram couldn't handle usage at 7, and so I\
          \ reduced it to 5, which I would also advise. Upon removing byte format\
          \ in --gpu-memory, and adding, say, a \"7\", or presenting it in MiBs, the\
          \ UI interprets every input as a successfully generated output from the\
          \ terminal(I never tried to reproduce the error, take that with a grain\
          \ of salt), error below: \n\nKeyError: 'model.layers.27.self_attn.q_proj.wf1'\n\
          \nSo, I reversed that change in the .bat file. I also learned anything lower\
          \ than pre-layer 25 is not that functional without a decently up to date\
          \ GPU, I wouldn't advise. In an attempt to make it faster, via the UI I\
          \ tested out things like deepspeed, bf16, flexgen, cuda features, etc.\n\
          \n\nConclusion:\n\nAfter everything worked and all was said and done, if\
          \ you don't have a very high-end PC, this won't be a good experience for\
          \ you, and I would advise getting a colab version together or trying another\
          \ alternative. At 0.89 tokens per second, it will feel like watching paint\
          \ dry. This is also a very delicate program, just about anything breaks\
          \ it or sets it off in the wrong direction. If you lack patience, try koboldAI\
          \ or NAI.\n\n\nDespite this, thank you everyone for the help.\n![Capture.PNG](https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/0HVBcqSaX3Ya7eOIANHEo.png)"
        updatedAt: '2023-04-11T22:39:07.895Z'
      numEdits: 0
      reactions: []
    id: 6435e18ba9265d9aaf919fd6
    type: comment
  author: Detpircsni
  content: "> @Detpircsni reading your error message I believe you are running out\
    \ of CPU memory, not GPU memory. You can use\n>  --cpu-memory\n> as a parameter\
    \ isntead (Remember to include the number after it) \n> \n> Guy's I'll leave you\
    \ this low VRAM guide from oobabooga, it has some usefull tips and parameters\
    \ you can try:\n> https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\n\
    > \n> Also if you get this error after limiting the memory: \"KeyError: 'model.layers.28.self_attn.q_proj.wf1'\"\
    , you can use:\n> --pre_layer 35 \n> parameter instead. If it get's you \"Out\
    \ of memory\" after a few texts, you can try:\n> --pre_layer 25\n> or you may\
    \ go even lower if you need to. This parameter number of layers that will be sent\
    \ to the GPU.\n> \n> The awful thing about all this is that the performance is\
    \ SUPER slow :(. It generates text so so slow. If any one else finds a way to\
    \ improve performance, please let us know.\n\n(To you, and to anyone else who\
    \ is reading)I have a 25+ GB system, and have actually read the guide and tried\
    \ all of those suggestions. At the time of the post, my prelayers were set to\
    \ 50, and went as far as to use the --disk flag to allocate memory there. Despite\
    \ this, it had no effect on the error code.\n\nI tried @sfreeman88 's info about\
    \ the --gpu-memory flag. My gpu-memory was 7, as many guidelines would advise.\
    \ I adjusted it to --gpu-memory 7073741824, which should be roughly 7 gigabytes\
    \ in bytes.\n\nThe same error persisted, but the attempted allocation was 141,557,760\
    \ bytes, which doesn't even seem to amount to 1 GB. Something is clearly wrong.\n\
    \nI tried the virtual memory option suggested by @xiiredrum. I typed \"advanced\
    \ system settings\" in the search, clicked on \"settings\" under performance,\
    \ went to the \"advanced\" tab, clicked on the \"change\" button, and unchecked\
    \ the \"automatically manage paging file size for all drives\" box. I noticed\
    \ only my OS drive had a paging file size enabled, which was where oogabooga was\
    \ located. It was still worth a try. Checking the \"system managed size\" for\
    \ each drive didn't work, as it disabled my changes after leaving that window.\
    \ Setting a custom size resolves this problem(5024-50024). After a restart, I\
    \ checked my results.\n\nThe error code changed. I interpreted this as a positive,\
    \ at least: \n\n=========\n\nLoading vicuna-13b-GPTQ-4bit-128g...\nLoading model\
    \ ...\nC:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated. It will be\
    \ removed in the future and UntypedStorage will be the only storage class. This\
    \ should only matter to you if you are using storages directly.  To access UntypedStorage\
    \ directly, use tensor.untyped_storage() instead of tensor.storage()\n  with safe_open(filename,\
    \ framework=\"pt\", device=device) as f:\nC:\\Program Files\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage\
    \ is deprecated. It will be removed in the future and UntypedStorage will be the\
    \ only storage class. This should only matter to you if you are using storages\
    \ directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead\
    \ of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nC:\\Program\
    \ Files\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in\
    \ the future and UntypedStorage will be the only storage class. This should only\
    \ matter to you if you are using storages directly.  To access UntypedStorage\
    \ directly, use tensor.untyped_storage() instead of tensor.storage()\n  storage\
    \ = cls(wrap_storage=untyped_storage)\n\n==========\n\nAfter this error code,\
    \ it hangs and nothing happens. But I ran it a second time:\n\n======\n\nLoading\
    \ vicuna-13b-GPTQ-4bit-128g...\nLoading model ...\nC:\\Program Files\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\safetensors\\torch.py:99: UserWarning:\
    \ TypedStorage is deprecated. It will be removed in the future and UntypedStorage\
    \ will be the only storage class. This should only matter to you if you are using\
    \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
    \ instead of tensor.storage()\n  with safe_open(filename, framework=\"pt\", device=device)\
    \ as f:\nC:\\Program Files\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed\
    \ in the future and UntypedStorage will be the only storage class. This should\
    \ only matter to you if you are using storages directly.  To access UntypedStorage\
    \ directly, use tensor.untyped_storage() instead of tensor.storage()\n  return\
    \ self.fget.__get__(instance, owner)()\nC:\\Program Files\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\storage.py:899: UserWarning:\
    \ TypedStorage is deprecated. It will be removed in the future and UntypedStorage\
    \ will be the only storage class. This should only matter to you if you are using\
    \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
    \ instead of tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\n\
    Done.\nUsing the following device map for the 4-bit model: {'': 0}\nLoaded the\
    \ model in 15.61 seconds.\nRunning on local URL:  http://127.0.0.1:7860\n3\nTo\
    \ create a public link, set `share=True` in `launch()`.\n\n=======\n\nMy flags\
    \ were --wbits 4 --groupsize 128 --gpu-memory 77073741824. All my internet browser\
    \ windows were also closed. It is possible all solutions lead to this outcome,\
    \ which no longer seems related to memory hell, at least. I also tried it with\
    \ GPT4 Alpaca, and the results for this whole post are the same.\n\nAdditional\
    \ Observations: \n\nI can confirm that it is functional, but leaving your browser\
    \ open is a mistake for not-so-powerful GPUs. It, and everything else, becomes\
    \ very bogged down. It loads quickly, and then begins to draw resources upon generation.\
    \ Setting the auto-devices flag on in this scenario is advised, and still seems\
    \ to allow operation with the above troubleshooting, so does the pre-layer flag,\
    \ and the threads flag. In my case, my 8gb of vram couldn't handle usage at 7,\
    \ and so I reduced it to 5, which I would also advise. Upon removing byte format\
    \ in --gpu-memory, and adding, say, a \"7\", or presenting it in MiBs, the UI\
    \ interprets every input as a successfully generated output from the terminal(I\
    \ never tried to reproduce the error, take that with a grain of salt), error below:\
    \ \n\nKeyError: 'model.layers.27.self_attn.q_proj.wf1'\n\nSo, I reversed that\
    \ change in the .bat file. I also learned anything lower than pre-layer 25 is\
    \ not that functional without a decently up to date GPU, I wouldn't advise. In\
    \ an attempt to make it faster, via the UI I tested out things like deepspeed,\
    \ bf16, flexgen, cuda features, etc.\n\n\nConclusion:\n\nAfter everything worked\
    \ and all was said and done, if you don't have a very high-end PC, this won't\
    \ be a good experience for you, and I would advise getting a colab version together\
    \ or trying another alternative. At 0.89 tokens per second, it will feel like\
    \ watching paint dry. This is also a very delicate program, just about anything\
    \ breaks it or sets it off in the wrong direction. If you lack patience, try koboldAI\
    \ or NAI.\n\n\nDespite this, thank you everyone for the help.\n![Capture.PNG](https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/0HVBcqSaX3Ya7eOIANHEo.png)"
  created_at: 2023-04-11 21:39:07+00:00
  edited: false
  hidden: false
  id: 6435e18ba9265d9aaf919fd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b205e2e9aed56d3974a84630f11a3f1.svg
      fullname: E Jenner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Void2258
      type: user
    createdAt: '2023-04-13T13:14:50.000Z'
    data:
      edited: false
      editors:
      - Void2258
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b205e2e9aed56d3974a84630f11a3f1.svg
          fullname: E Jenner
          isHf: false
          isPro: false
          name: Void2258
          type: user
        html: '<p>I am also unable to load the model in oobabooga. Always out of memory.</p>

          '
        raw: I am also unable to load the model in oobabooga. Always out of memory.
        updatedAt: '2023-04-13T13:14:50.006Z'
      numEdits: 0
      reactions: []
    id: 6438004a94faafc1a2dfa0fe
    type: comment
  author: Void2258
  content: I am also unable to load the model in oobabooga. Always out of memory.
  created_at: 2023-04-13 12:14:50+00:00
  edited: false
  hidden: false
  id: 6438004a94faafc1a2dfa0fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5cf7b7b491d48f9080f7e4ef12169109.svg
      fullname: Alexandre Santos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fkz0000001
      type: user
    createdAt: '2023-04-13T18:06:02.000Z'
    data:
      edited: false
      editors:
      - fkz0000001
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5cf7b7b491d48f9080f7e4ef12169109.svg
          fullname: Alexandre Santos
          isHf: false
          isPro: false
          name: fkz0000001
          type: user
        html: '<p>I''m having the same problem.</p>

          <p>My web UI starts. But if I type something. This error occurs:</p>

          <p>Starting the web UI...<br>Loading gpt4-x-alpaca-13b-native-4bit-128g...<br>Found
          the following quantized model: models\gpt4-x-alpaca-13b-native-4bit-128g\gpt-x-alpaca-13b-native-4bit-128g-cuda.pt<br>Loading
          model ...<br>Done.<br>Loaded the model in 5.21 seconds.<br>Loading the extension
          "gallery"... Ok.<br>Running on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>Traceback
          (most recent call last):<br>  File "J:\oobabooga\text-generation-webui\modules\callbacks.py",
          line 66, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "J:\oobabooga\text-generation-webui\modules\text_generation.py", line 251,
          in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 305, in forward<br>    hidden_states = self.mlp(hidden_states)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 157, in forward<br>    return self.down_proj(self.act_fn(self.gate_proj(x))
          * self.up_proj(x))<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga\installer_files\env\lib\site-packages\transformers\activations.py",
          line 150, in forward<br>    return nn.functional.silu(input)<br>  File "J:\oobabooga\installer_files\env\lib\site-packages\torch\nn\functional.py",
          line 2059, in silu<br>    return torch._C._nn.silu(input)<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity;
          7.07 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by
          PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>Output
          generated in 2.33 seconds (0.00 tokens/s, 0 tokens, context 37, seed 2091766985)</p>

          <hr>

          <p>That is my param</p>

          <p>call python server.py --chat --wbits 4 --groupsize 128 --bf16 --gpu memory
          7</p>

          '
        raw: "I'm having the same problem.\n\nMy web UI starts. But if I type something.\
          \ This error occurs:\n\n\nStarting the web UI...\nLoading gpt4-x-alpaca-13b-native-4bit-128g...\n\
          Found the following quantized model: models\\gpt4-x-alpaca-13b-native-4bit-128g\\\
          gpt-x-alpaca-13b-native-4bit-128g-cuda.pt\nLoading model ...\nDone.\nLoaded\
          \ the model in 5.21 seconds.\nLoading the extension \"gallery\"... Ok.\n\
          Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link,\
          \ set `share=True` in `launch()`.\nTraceback (most recent call last):\n\
          \  File \"J:\\oobabooga\\text-generation-webui\\modules\\callbacks.py\"\
          , line 66, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"J:\\oobabooga\\text-generation-webui\\modules\\text_generation.py\"\
          , line 251, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
          \ **kwargs)\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 1485, in generate\n    return\
          \ self.sample(\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2524, in sample\n    outputs\
          \ = self(\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\n\
          \    outputs = self.model(\n  File \"J:\\oobabooga\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"J:\\oobabooga\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 577, in forward\n    layer_outputs = decoder_layer(\n  File \"J:\\\
          oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
          \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 305, in forward\n    hidden_states\
          \ = self.mlp(hidden_states)\n  File \"J:\\oobabooga\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"J:\\oobabooga\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 157, in forward\n    return self.down_proj(self.act_fn(self.gate_proj(x))\
          \ * self.up_proj(x))\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"J:\\oobabooga\\installer_files\\\
          env\\lib\\site-packages\\transformers\\activations.py\", line 150, in forward\n\
          \    return nn.functional.silu(input)\n  File \"J:\\oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\functional.py\", line 2059, in silu\n\
          \    return torch._C._nn.silu(input)\ntorch.cuda.OutOfMemoryError: CUDA\
          \ out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity;\
          \ 7.07 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by\
          \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          Output generated in 2.33 seconds (0.00 tokens/s, 0 tokens, context 37, seed\
          \ 2091766985)\n\n----------------------\n\nThat is my param\n\ncall python\
          \ server.py --chat --wbits 4 --groupsize 128 --bf16 --gpu memory 7"
        updatedAt: '2023-04-13T18:06:02.462Z'
      numEdits: 0
      reactions: []
    id: 6438448a90fe3ba16ace2575
    type: comment
  author: fkz0000001
  content: "I'm having the same problem.\n\nMy web UI starts. But if I type something.\
    \ This error occurs:\n\n\nStarting the web UI...\nLoading gpt4-x-alpaca-13b-native-4bit-128g...\n\
    Found the following quantized model: models\\gpt4-x-alpaca-13b-native-4bit-128g\\\
    gpt-x-alpaca-13b-native-4bit-128g-cuda.pt\nLoading model ...\nDone.\nLoaded the\
    \ model in 5.21 seconds.\nLoading the extension \"gallery\"... Ok.\nRunning on\
    \ local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True`\
    \ in `launch()`.\nTraceback (most recent call last):\n  File \"J:\\oobabooga\\\
    text-generation-webui\\modules\\callbacks.py\", line 66, in gentask\n    ret =\
    \ self.mfunc(callback=_callback, **self.kwargs)\n  File \"J:\\oobabooga\\text-generation-webui\\\
    modules\\text_generation.py\", line 251, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
    \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\utils\\\
    _contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n\
    \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 1485, in generate\n    return self.sample(\n  File\
    \ \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\\
    utils.py\", line 2524, in sample\n    outputs = self(\n  File \"J:\\oobabooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"J:\\\
    oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
    modeling_llama.py\", line 687, in forward\n    outputs = self.model(\n  File \"\
    J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 577, in forward\n    layer_outputs =\
    \ decoder_layer(\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 305, in forward\n    hidden_states\
    \ = self.mlp(hidden_states)\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n   \
    \ return forward_call(*args, **kwargs)\n  File \"J:\\oobabooga\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
    \ 157, in forward\n    return self.down_proj(self.act_fn(self.gate_proj(x)) *\
    \ self.up_proj(x))\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
    transformers\\activations.py\", line 150, in forward\n    return nn.functional.silu(input)\n\
    \  File \"J:\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
    functional.py\", line 2059, in silu\n    return torch._C._nn.silu(input)\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity;\
    \ 7.07 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    Output generated in 2.33 seconds (0.00 tokens/s, 0 tokens, context 37, seed 2091766985)\n\
    \n----------------------\n\nThat is my param\n\ncall python server.py --chat --wbits\
    \ 4 --groupsize 128 --bf16 --gpu memory 7"
  created_at: 2023-04-13 17:06:02+00:00
  edited: false
  hidden: false
  id: 6438448a90fe3ba16ace2575
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cf2543c44232c01956d44357348395.svg
      fullname: unknown ID
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: U-ID
      type: user
    createdAt: '2023-04-26T07:13:29.000Z'
    data:
      edited: true
      editors:
      - U-ID
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cf2543c44232c01956d44357348395.svg
          fullname: unknown ID
          isHf: false
          isPro: false
          name: U-ID
          type: user
        html: "<p>Ok i got it running finally. Seriously, it is as simple as <span\
          \ data-props=\"{&quot;user&quot;:&quot;xiiredrum&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xiiredrum\">@<span class=\"\
          underline\">xiiredrum</span></a></span>\n\n\t</span></span> described above,\
          \ setting the virtual memory for the drive the files for the chat bot are\
          \ located sufficed. At this point i was through all the other troubles and\
          \ it turns out setting the args is not necessary at all. lol<br>Set virtual\
          \ memory to system managed size. I tried a fixed value which did not work,\
          \ i don't know why, i have plenty.<br>Important, you don't need to set wbits\
          \ and groupsize, there is a config in the models folder that has all the\
          \ settings for these kinds of cases and picks the right profile automatically.\
          \ That also means you can start with different models without having to\
          \ set the args each time.<br>It runs just fine without all these parameters\
          \ for me. But if you specifically run out of GPU memory (cuda error) for\
          \ me it works if i write it like this:<br>--gpu-memory 10GiB (for a 12GB\
          \ card and leave a bit legroom the speed is just fine like that)<br>Same\
          \ as for the CPU memory (RAM). But when set to use a GPU it does not use\
          \ much RAM at all. </p>\n<p>As a side note, once you have your bot with\
          \ another model running, ask it for advice. It can be a surprisingly good\
          \ helper to set itself up... lol<br>But don't expect a detailed manual,\
          \ it is able to answer questions pretty well but not able to know about\
          \ the specifics of your environment and problems if you dont provide that\
          \ information</p>\n<p>/edit, just after i wrote this, i restarted UI and\
          \ it was running in the same memory problem. Rebooted the pc and it works\
          \ again.</p>\n"
        raw: "Ok i got it running finally. Seriously, it is as simple as @xiiredrum\
          \ described above, setting the virtual memory for the drive the files for\
          \ the chat bot are located sufficed. At this point i was through all the\
          \ other troubles and it turns out setting the args is not necessary at all.\
          \ lol \nSet virtual memory to system managed size. I tried a fixed value\
          \ which did not work, i don't know why, i have plenty.\nImportant, you don't\
          \ need to set wbits and groupsize, there is a config in the models folder\
          \ that has all the settings for these kinds of cases and picks the right\
          \ profile automatically. That also means you can start with different models\
          \ without having to set the args each time.\nIt runs just fine without all\
          \ these parameters for me. But if you specifically run out of GPU memory\
          \ (cuda error) for me it works if i write it like this:\n--gpu-memory 10GiB\
          \ (for a 12GB card and leave a bit legroom the speed is just fine like that)\n\
          Same as for the CPU memory (RAM). But when set to use a GPU it does not\
          \ use much RAM at all. \n\nAs a side note, once you have your bot with another\
          \ model running, ask it for advice. It can be a surprisingly good helper\
          \ to set itself up... lol\nBut don't expect a detailed manual, it is able\
          \ to answer questions pretty well but not able to know about the specifics\
          \ of your environment and problems if you dont provide that information\n\
          \n/edit, just after i wrote this, i restarted UI and it was running in the\
          \ same memory problem. Rebooted the pc and it works again."
        updatedAt: '2023-04-26T07:26:12.664Z'
      numEdits: 1
      reactions: []
    id: 6448cf19d5d86def91c6defe
    type: comment
  author: U-ID
  content: "Ok i got it running finally. Seriously, it is as simple as @xiiredrum\
    \ described above, setting the virtual memory for the drive the files for the\
    \ chat bot are located sufficed. At this point i was through all the other troubles\
    \ and it turns out setting the args is not necessary at all. lol \nSet virtual\
    \ memory to system managed size. I tried a fixed value which did not work, i don't\
    \ know why, i have plenty.\nImportant, you don't need to set wbits and groupsize,\
    \ there is a config in the models folder that has all the settings for these kinds\
    \ of cases and picks the right profile automatically. That also means you can\
    \ start with different models without having to set the args each time.\nIt runs\
    \ just fine without all these parameters for me. But if you specifically run out\
    \ of GPU memory (cuda error) for me it works if i write it like this:\n--gpu-memory\
    \ 10GiB (for a 12GB card and leave a bit legroom the speed is just fine like that)\n\
    Same as for the CPU memory (RAM). But when set to use a GPU it does not use much\
    \ RAM at all. \n\nAs a side note, once you have your bot with another model running,\
    \ ask it for advice. It can be a surprisingly good helper to set itself up...\
    \ lol\nBut don't expect a detailed manual, it is able to answer questions pretty\
    \ well but not able to know about the specifics of your environment and problems\
    \ if you dont provide that information\n\n/edit, just after i wrote this, i restarted\
    \ UI and it was running in the same memory problem. Rebooted the pc and it works\
    \ again."
  created_at: 2023-04-26 06:13:29+00:00
  edited: true
  hidden: false
  id: 6448cf19d5d86def91c6defe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4de905b78b10bd05d03b74f2eef9e14d.svg
      fullname: Carlos Frederico Paroli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cparoli
      type: user
    createdAt: '2023-04-30T12:36:38.000Z'
    data:
      edited: false
      editors:
      - cparoli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4de905b78b10bd05d03b74f2eef9e14d.svg
          fullname: Carlos Frederico Paroli
          isHf: false
          isPro: false
          name: cparoli
          type: user
        html: '<p>The problem with low performance is because oobabooga is using just
          few CPU threads. If someone knows how to increase... With alpaca.cpp CPU
          only runs very well CPU only with 20 threads.</p>

          '
        raw: The problem with low performance is because oobabooga is using just few
          CPU threads. If someone knows how to increase... With alpaca.cpp CPU only
          runs very well CPU only with 20 threads.
        updatedAt: '2023-04-30T12:36:38.076Z'
      numEdits: 0
      reactions: []
    id: 644e60d6cf72e60a5b77b6c8
    type: comment
  author: cparoli
  content: The problem with low performance is because oobabooga is using just few
    CPU threads. If someone knows how to increase... With alpaca.cpp CPU only runs
    very well CPU only with 20 threads.
  created_at: 2023-04-30 11:36:38+00:00
  edited: false
  hidden: false
  id: 644e60d6cf72e60a5b77b6c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28b603091b51e42dc16da78b591d0949.svg
      fullname: Unknown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HanshaAI
      type: user
    createdAt: '2023-05-02T20:02:28.000Z'
    data:
      edited: true
      editors:
      - HanshaAI
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28b603091b51e42dc16da78b591d0949.svg
          fullname: Unknown
          isHf: false
          isPro: false
          name: HanshaAI
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Detpircsni&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Detpircsni\">@<span class=\"\
          underline\">Detpircsni</span></a></span>\n\n\t</span></span><br>Sorry for\
          \ my English, Seems like you overcome the 'KeyError: 'model.layers.27.self_attn.q_proj.wf1''<br>I\
          \ can run the model perfectly,<br>but I can't seem to understand what's\
          \ the problem, looks like the \"--pre_layer\" flag culprit for me, no matter\
          \ what number I use it seems like I can't generate text or use anything.</p>\n\
          <p>My current param are: --chat --model-menu --wbits 4 --groupsize 128 --gpu-memory\
          \ 5<br>If probably you can help me (or anyone), it would be appreciated.</p>\n"
        raw: "@Detpircsni \nSorry for my English, Seems like you overcome the 'KeyError:\
          \ 'model.layers.27.self_attn.q_proj.wf1''\nI can run the model perfectly,\
          \ \nbut I can't seem to understand what's the problem, looks like the \"\
          --pre_layer\" flag culprit for me, no matter what number I use it seems\
          \ like I can't generate text or use anything.\n\nMy current param are: --chat\
          \ --model-menu --wbits 4 --groupsize 128 --gpu-memory 5\nIf probably you\
          \ can help me (or anyone), it would be appreciated."
        updatedAt: '2023-05-02T20:03:31.970Z'
      numEdits: 1
      reactions: []
    id: 64516c5441f3c769b9149539
    type: comment
  author: HanshaAI
  content: "@Detpircsni \nSorry for my English, Seems like you overcome the 'KeyError:\
    \ 'model.layers.27.self_attn.q_proj.wf1''\nI can run the model perfectly, \nbut\
    \ I can't seem to understand what's the problem, looks like the \"--pre_layer\"\
    \ flag culprit for me, no matter what number I use it seems like I can't generate\
    \ text or use anything.\n\nMy current param are: --chat --model-menu --wbits 4\
    \ --groupsize 128 --gpu-memory 5\nIf probably you can help me (or anyone), it\
    \ would be appreciated."
  created_at: 2023-05-02 19:02:28+00:00
  edited: true
  hidden: false
  id: 64516c5441f3c769b9149539
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4de905b78b10bd05d03b74f2eef9e14d.svg
      fullname: Carlos Frederico Paroli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cparoli
      type: user
    createdAt: '2023-05-04T12:31:57.000Z'
    data:
      edited: true
      editors:
      - cparoli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4de905b78b10bd05d03b74f2eef9e14d.svg
          fullname: Carlos Frederico Paroli
          isHf: false
          isPro: false
          name: cparoli
          type: user
        html: '<ul>

          <li></li>

          </ul>

          '
        raw: '-'
        updatedAt: '2023-05-26T15:26:54.235Z'
      numEdits: 1
      reactions: []
    id: 6453a5bddc897edfd15a5ab3
    type: comment
  author: cparoli
  content: '-'
  created_at: 2023-05-04 11:31:57+00:00
  edited: true
  hidden: false
  id: 6453a5bddc897edfd15a5ab3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8d4bbfea28398e999566005d169805f.svg
      fullname: Allston Shone
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashone935
      type: user
    createdAt: '2023-06-05T08:11:17.000Z'
    data:
      edited: false
      editors:
      - Ashone935
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9653571844100952
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8d4bbfea28398e999566005d169805f.svg
          fullname: Allston Shone
          isHf: false
          isPro: false
          name: Ashone935
          type: user
        html: '<p>Having the same issue; I have a very high-spec computer, followed
          all the steps like 4 times, starting from scratch every time, did the virtual
          data thing, set gpu memory to 10 gig, and every time I submit anything I
          get the same "CUDA out of memory" issue. Pls, someone needs to figure this
          out really bad.</p>

          '
        raw: Having the same issue; I have a very high-spec computer, followed all
          the steps like 4 times, starting from scratch every time, did the virtual
          data thing, set gpu memory to 10 gig, and every time I submit anything I
          get the same "CUDA out of memory" issue. Pls, someone needs to figure this
          out really bad.
        updatedAt: '2023-06-05T08:11:17.552Z'
      numEdits: 0
      reactions: []
    id: 647d98a5f14eafc3b4460fb3
    type: comment
  author: Ashone935
  content: Having the same issue; I have a very high-spec computer, followed all the
    steps like 4 times, starting from scratch every time, did the virtual data thing,
    set gpu memory to 10 gig, and every time I submit anything I get the same "CUDA
    out of memory" issue. Pls, someone needs to figure this out really bad.
  created_at: 2023-06-05 07:11:17+00:00
  edited: false
  hidden: false
  id: 647d98a5f14eafc3b4460fb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674202895159-noauth.png?w=200&h=200&f=face
      fullname: Eddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaltyEddy
      type: user
    createdAt: '2023-06-21T10:19:38.000Z'
    data:
      edited: true
      editors:
      - SaltyEddy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422204494476318
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674202895159-noauth.png?w=200&h=200&f=face
          fullname: Eddy
          isHf: false
          isPro: false
          name: SaltyEddy
          type: user
        html: '<p>I just saw that it is not enough to limit the GPU Memory to 7 when
          there is 8 GB in the GPU.<br>I did limit it to 5 and my GPUs memory is full
          to 6,9 GB. Means you have to give it really much headroom!<br>Same goes
          for the cpu-memory. I have 24 GBs installed and set it to 16.<br>While starting
          it took more than 16 GB, now it sits at 12 GB.</p>

          <p>i have 24 GB RAM and a 3070 with 8 GB and I got it working with following
          params:</p>

          <p>--auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory 5 --cpu-memory
          16</p>

          <p>But it is soooooo slow. Is this normal ?</p>

          '
        raw: "I just saw that it is not enough to limit the GPU Memory to 7 when there\
          \ is 8 GB in the GPU.\nI did limit it to 5 and my GPUs memory is full to\
          \ 6,9 GB. Means you have to give it really much headroom!\nSame goes for\
          \ the cpu-memory. I have 24 GBs installed and set it to 16. \nWhile starting\
          \ it took more than 16 GB, now it sits at 12 GB.\n\ni have 24 GB RAM and\
          \ a 3070 with 8 GB and I got it working with following params:\n\n--auto-devices\
          \ --chat --wbits 4 --groupsize 128 --gpu-memory 5 --cpu-memory 16\n\n\n\
          But it is soooooo slow. Is this normal ?"
        updatedAt: '2023-06-21T10:27:06.471Z'
      numEdits: 1
      reactions: []
    id: 6492ceba0b0bbf6df99b1136
    type: comment
  author: SaltyEddy
  content: "I just saw that it is not enough to limit the GPU Memory to 7 when there\
    \ is 8 GB in the GPU.\nI did limit it to 5 and my GPUs memory is full to 6,9 GB.\
    \ Means you have to give it really much headroom!\nSame goes for the cpu-memory.\
    \ I have 24 GBs installed and set it to 16. \nWhile starting it took more than\
    \ 16 GB, now it sits at 12 GB.\n\ni have 24 GB RAM and a 3070 with 8 GB and I\
    \ got it working with following params:\n\n--auto-devices --chat --wbits 4 --groupsize\
    \ 128 --gpu-memory 5 --cpu-memory 16\n\n\nBut it is soooooo slow. Is this normal\
    \ ?"
  created_at: 2023-06-21 09:19:38+00:00
  edited: true
  hidden: false
  id: 6492ceba0b0bbf6df99b1136
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d57c16353e0e1f87f5f7c184a8cc6525.svg
      fullname: drizz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: drizzcat
      type: user
    createdAt: '2023-07-01T15:17:34.000Z'
    data:
      edited: false
      editors:
      - drizzcat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9070169925689697
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d57c16353e0e1f87f5f7c184a8cc6525.svg
          fullname: drizz
          isHf: false
          isPro: false
          name: drizzcat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xiiredrum&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xiiredrum\">@<span class=\"\
          underline\">xiiredrum</span></a></span>\n\n\t</span></span><br>I have the\
          \ same build as yours - 3060 and 16 gigs of ram, and manually specifying\
          \ the pagefile memory max and min fixed it for me (set it from 64gb to 100gb),\
          \ not just setting it to system managed size.</p>\n"
        raw: "@xiiredrum \nI have the same build as yours - 3060 and 16 gigs of ram,\
          \ and manually specifying the pagefile memory max and min fixed it for me\
          \ (set it from 64gb to 100gb), not just setting it to system managed size."
        updatedAt: '2023-07-01T15:17:34.249Z'
      numEdits: 0
      reactions: []
    id: 64a0438ebfd4d3ffab96bbe3
    type: comment
  author: drizzcat
  content: "@xiiredrum \nI have the same build as yours - 3060 and 16 gigs of ram,\
    \ and manually specifying the pagefile memory max and min fixed it for me (set\
    \ it from 64gb to 100gb), not just setting it to system managed size."
  created_at: 2023-07-01 14:17:34+00:00
  edited: false
  hidden: false
  id: 64a0438ebfd4d3ffab96bbe3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e88093f22449a85b43b373e39533396b.svg
      fullname: Austin Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kieee
      type: user
    createdAt: '2023-07-05T21:47:04.000Z'
    data:
      edited: true
      editors:
      - Kieee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854879379272461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e88093f22449a85b43b373e39533396b.svg
          fullname: Austin Smith
          isHf: false
          isPro: false
          name: Kieee
          type: user
        html: '<p>In case anyone is still having an issue after after trying the above
          things, check your drive storage space. I had ~5 Gb free and was getting
          the error <code>DefaultCPUAllocator: not enough memory: you tried to allocate...</code>
          After I uninstalled some things and had ~30GB free, everything started working.
          I''m not sure at what point it would have started working, I only tested
          it at 5gb and 30gb. But I hope this helps someone!</p>

          <p>Edit: I had to do this on my C drive, even though I had ~250gb free on
          the D drive where oobabooga was installed.</p>

          '
        raw: 'In case anyone is still having an issue after after trying the above
          things, check your drive storage space. I had ~5 Gb free and was getting
          the error `DefaultCPUAllocator: not enough memory: you tried to allocate...`
          After I uninstalled some things and had ~30GB free, everything started working.
          I''m not sure at what point it would have started working, I only tested
          it at 5gb and 30gb. But I hope this helps someone!


          Edit: I had to do this on my C drive, even though I had ~250gb free on the
          D drive where oobabooga was installed.'
        updatedAt: '2023-07-05T21:48:28.611Z'
      numEdits: 2
      reactions: []
    id: 64a5e4d814dd4f0dc619c037
    type: comment
  author: Kieee
  content: 'In case anyone is still having an issue after after trying the above things,
    check your drive storage space. I had ~5 Gb free and was getting the error `DefaultCPUAllocator:
    not enough memory: you tried to allocate...` After I uninstalled some things and
    had ~30GB free, everything started working. I''m not sure at what point it would
    have started working, I only tested it at 5gb and 30gb. But I hope this helps
    someone!


    Edit: I had to do this on my C drive, even though I had ~250gb free on the D drive
    where oobabooga was installed.'
  created_at: 2023-07-05 20:47:04+00:00
  edited: true
  hidden: false
  id: 64a5e4d814dd4f0dc619c037
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ea2f07e4323d6124afa6e3a7871cc78.svg
      fullname: caleb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: calvin1527
      type: user
    createdAt: '2023-07-25T18:57:38.000Z'
    data:
      edited: false
      editors:
      - calvin1527
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9450058341026306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ea2f07e4323d6124afa6e3a7871cc78.svg
          fullname: caleb
          isHf: false
          isPro: false
          name: calvin1527
          type: user
        html: "<p>SOLUTION:<br>Error: DefaultCPUAllocator: not enough memory: you\
          \ tried to allocate 141557760 bytes.<br>Change system page file on disk\
          \ where the model is located to custom 5024-50024 as noted by <span data-props=\"\
          {&quot;user&quot;:&quot;Detpircsni&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Detpircsni\">@<span class=\"underline\"\
          >Detpircsni</span></a></span>\n\n\t</span></span></p>\n"
        raw: "SOLUTION: \nError: DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 141557760 bytes.\nChange system page file on disk where the\
          \ model is located to custom 5024-50024 as noted by @Detpircsni"
        updatedAt: '2023-07-25T18:57:38.774Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - maxmodebr
        - nocs32
    id: 64c01b22d76592ba89796c35
    type: comment
  author: calvin1527
  content: "SOLUTION: \nError: DefaultCPUAllocator: not enough memory: you tried to\
    \ allocate 141557760 bytes.\nChange system page file on disk where the model is\
    \ located to custom 5024-50024 as noted by @Detpircsni"
  created_at: 2023-07-25 17:57:38+00:00
  edited: false
  hidden: false
  id: 64c01b22d76592ba89796c35
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: out of memory error when launching from oobabooga web ui
