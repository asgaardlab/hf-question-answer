!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Damgaardian
conflicting_files: null
created_at: 2023-04-11 16:16:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f34859d3b173e28f3c1513480f699b10.svg
      fullname: Peter Damgaard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Damgaardian
      type: user
    createdAt: '2023-04-11T17:16:05.000Z'
    data:
      edited: false
      editors:
      - Damgaardian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f34859d3b173e28f3c1513480f699b10.svg
          fullname: Peter Damgaard
          isHf: false
          isPro: false
          name: Damgaardian
          type: user
        html: '<p>Hi everyone, I''m really excited to try and install this uncensored
          ChatGPT alternative on my computer, being tired of the creative limitations
          the extreme censorship seems to enforce (couldn''t even create a hate-poem
          with clever puns for the Skaven species from Warhammer without triggering
          some content policy violation...). Unfortunately, I seem to be hitting a
          wall trying to install the AI.</p>

          <p>I seem to be getting a different Runtime Error than the other people
          here in the comments, when trying to run the start-webui (following the
          recent video guide from Aitrepeneur): </p>

          <p>RuntimeError: Internal: D:\a\sentencepiece\sentencepiece\src\sentencepiece_processor.cc(1102)
          [model_proto-&gt;ParseFromArray(serialized.data(), serialized.size())]</p>

          <p>No idea what it means or what I''m supposed to do about it. Tried messing
          around with the start-webui parameters as suggested various places (e.g.
          --gpu-memory and --pre_layer etc.), but it has no effect at all, always
          the same error and traceback files. Wondering if the install itself is messed
          up somehow, so maybe I''ll have to download and install everything from
          scratch again? </p>

          <p>I did notice some deviations from Aitrepeneur''s guide in my installation
          files, such as a lack of a git folder (to delete according to his instructions),
          and instead of --chat I had --cai chat in the parameters, which the command
          window itself told me to edit to --chat upon trying to launch - though ultimately
          it still led to the same runtime error.</p>

          <p>My GPU trying to run it is a 10GB RTX 3080 (desktop) if that matters
          - I know it should ideally be 16+GB, but apparently people with even less
          powerful cards than mine are having some success?</p>

          <p>Any help or tips is welcome, and thank you in advance.</p>

          '
        raw: "Hi everyone, I'm really excited to try and install this uncensored ChatGPT\
          \ alternative on my computer, being tired of the creative limitations the\
          \ extreme censorship seems to enforce (couldn't even create a hate-poem\
          \ with clever puns for the Skaven species from Warhammer without triggering\
          \ some content policy violation...). Unfortunately, I seem to be hitting\
          \ a wall trying to install the AI.\r\n\r\nI seem to be getting a different\
          \ Runtime Error than the other people here in the comments, when trying\
          \ to run the start-webui (following the recent video guide from Aitrepeneur):\
          \ \r\n\r\nRuntimeError: Internal: D:\\a\\sentencepiece\\sentencepiece\\\
          src\\sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())]\r\n\r\nNo idea what it means or what I'm supposed\
          \ to do about it. Tried messing around with the start-webui parameters as\
          \ suggested various places (e.g. --gpu-memory and --pre_layer etc.), but\
          \ it has no effect at all, always the same error and traceback files. Wondering\
          \ if the install itself is messed up somehow, so maybe I'll have to download\
          \ and install everything from scratch again? \r\n\r\nI did notice some deviations\
          \ from Aitrepeneur's guide in my installation files, such as a lack of a\
          \ git folder (to delete according to his instructions), and instead of --chat\
          \ I had --cai chat in the parameters, which the command window itself told\
          \ me to edit to --chat upon trying to launch - though ultimately it still\
          \ led to the same runtime error.\r\n\r\nMy GPU trying to run it is a 10GB\
          \ RTX 3080 (desktop) if that matters - I know it should ideally be 16+GB,\
          \ but apparently people with even less powerful cards than mine are having\
          \ some success?\r\n\r\nAny help or tips is welcome, and thank you in advance."
        updatedAt: '2023-04-11T17:16:05.743Z'
      numEdits: 0
      reactions: []
    id: 643595d5f81a16e743639dc7
    type: comment
  author: Damgaardian
  content: "Hi everyone, I'm really excited to try and install this uncensored ChatGPT\
    \ alternative on my computer, being tired of the creative limitations the extreme\
    \ censorship seems to enforce (couldn't even create a hate-poem with clever puns\
    \ for the Skaven species from Warhammer without triggering some content policy\
    \ violation...). Unfortunately, I seem to be hitting a wall trying to install\
    \ the AI.\r\n\r\nI seem to be getting a different Runtime Error than the other\
    \ people here in the comments, when trying to run the start-webui (following the\
    \ recent video guide from Aitrepeneur): \r\n\r\nRuntimeError: Internal: D:\\a\\\
    sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())]\r\n\r\nNo idea what it means or what I'm supposed to do\
    \ about it. Tried messing around with the start-webui parameters as suggested\
    \ various places (e.g. --gpu-memory and --pre_layer etc.), but it has no effect\
    \ at all, always the same error and traceback files. Wondering if the install\
    \ itself is messed up somehow, so maybe I'll have to download and install everything\
    \ from scratch again? \r\n\r\nI did notice some deviations from Aitrepeneur's\
    \ guide in my installation files, such as a lack of a git folder (to delete according\
    \ to his instructions), and instead of --chat I had --cai chat in the parameters,\
    \ which the command window itself told me to edit to --chat upon trying to launch\
    \ - though ultimately it still led to the same runtime error.\r\n\r\nMy GPU trying\
    \ to run it is a 10GB RTX 3080 (desktop) if that matters - I know it should ideally\
    \ be 16+GB, but apparently people with even less powerful cards than mine are\
    \ having some success?\r\n\r\nAny help or tips is welcome, and thank you in advance."
  created_at: 2023-04-11 16:16:05+00:00
  edited: false
  hidden: false
  id: 643595d5f81a16e743639dc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
      fullname: Michael Moran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mm04926412
      type: user
    createdAt: '2023-04-11T17:49:36.000Z'
    data:
      edited: false
      editors:
      - mm04926412
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
          fullname: Michael Moran
          isHf: false
          isPro: false
          name: mm04926412
          type: user
        html: '<p>I solved this problem on my machine, for some reason the tokenizor
          is stored using github LFS despite being less than a megabyte, you likely
          have a 1kb file pointer instead of the real tokenizor</p>

          '
        raw: I solved this problem on my machine, for some reason the tokenizor is
          stored using github LFS despite being less than a megabyte, you likely have
          a 1kb file pointer instead of the real tokenizor
        updatedAt: '2023-04-11T17:49:36.094Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Kirboz
        - taddwhite
      - count: 2
        reaction: "\U0001F91D"
        users:
        - Kirboz
        - taddwhite
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Kirboz
      - count: 1
        reaction: "\U0001F917"
        users:
        - Kirboz
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kirboz
    id: 64359db0adb7b87154e94cce
    type: comment
  author: mm04926412
  content: I solved this problem on my machine, for some reason the tokenizor is stored
    using github LFS despite being less than a megabyte, you likely have a 1kb file
    pointer instead of the real tokenizor
  created_at: 2023-04-11 16:49:36+00:00
  edited: false
  hidden: false
  id: 64359db0adb7b87154e94cce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68a6ee35cdfb5f36b57593e2f540d8aa.svg
      fullname: anthony fourcault
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: docstark
      type: user
    createdAt: '2023-04-11T19:02:28.000Z'
    data:
      edited: true
      editors:
      - docstark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68a6ee35cdfb5f36b57593e2f540d8aa.svg
          fullname: anthony fourcault
          isHf: false
          isPro: false
          name: docstark
          type: user
        html: '<blockquote>

          <p>I solved this problem on my machine, for some reason the tokenizer is
          stored using GitHub LFS despite being less than a megabyte, you likely have
          a 1kb file pointer instead of the real tokenizer</p>

          </blockquote>

          <p>How can we fix this, because I''m running into the same issues, and can''t
          seem to find the tokenizer.</p>

          '
        raw: '> I solved this problem on my machine, for some reason the tokenizer
          is stored using GitHub LFS despite being less than a megabyte, you likely
          have a 1kb file pointer instead of the real tokenizer



          How can we fix this, because I''m running into the same issues, and can''t
          seem to find the tokenizer.'
        updatedAt: '2023-04-11T19:02:41.624Z'
      numEdits: 1
      reactions: []
    id: 6435aec4a9265d9aaf904313
    type: comment
  author: docstark
  content: '> I solved this problem on my machine, for some reason the tokenizer is
    stored using GitHub LFS despite being less than a megabyte, you likely have a
    1kb file pointer instead of the real tokenizer



    How can we fix this, because I''m running into the same issues, and can''t seem
    to find the tokenizer.'
  created_at: 2023-04-11 18:02:28+00:00
  edited: true
  hidden: false
  id: 6435aec4a9265d9aaf904313
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb4956096022e4d1838f0ac30ff3a833.svg
      fullname: erin toko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: erintoko
      type: user
    createdAt: '2023-04-12T20:00:10.000Z'
    data:
      edited: false
      editors:
      - erintoko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb4956096022e4d1838f0ac30ff3a833.svg
          fullname: erin toko
          isHf: false
          isPro: false
          name: erintoko
          type: user
        html: '<p>Try to download tokenizer.model from "Files and versions" and put
          it into the folder of the model. If there is already a file with only 1KB
          replace it with the correct tokenizer.model.</p>

          '
        raw: Try to download tokenizer.model from "Files and versions" and put it
          into the folder of the model. If there is already a file with only 1KB replace
          it with the correct tokenizer.model.
        updatedAt: '2023-04-12T20:00:10.923Z'
      numEdits: 0
      reactions: []
    id: 64370dca5f36a7ed0f4878ff
    type: comment
  author: erintoko
  content: Try to download tokenizer.model from "Files and versions" and put it into
    the folder of the model. If there is already a file with only 1KB replace it with
    the correct tokenizer.model.
  created_at: 2023-04-12 19:00:10+00:00
  edited: false
  hidden: false
  id: 64370dca5f36a7ed0f4878ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f34859d3b173e28f3c1513480f699b10.svg
      fullname: Peter Damgaard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Damgaardian
      type: user
    createdAt: '2023-04-12T20:45:14.000Z'
    data:
      edited: false
      editors:
      - Damgaardian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f34859d3b173e28f3c1513480f699b10.svg
          fullname: Peter Damgaard
          isHf: false
          isPro: false
          name: Damgaardian
          type: user
        html: "<blockquote>\n<p>I solved this problem on my machine, for some reason\
          \ the tokenizor is stored using github LFS despite being less than a megabyte,\
          \ you likely have a 1kb file pointer instead of the real tokenizor</p>\n\
          </blockquote>\n<p>Thanks mm04926412, you're a godsend! This was the fix\
          \ for me as well. Would've thanked you last night already if my newly made\
          \ account wasn't restricted \U0001F64F</p>\n"
        raw: "> I solved this problem on my machine, for some reason the tokenizor\
          \ is stored using github LFS despite being less than a megabyte, you likely\
          \ have a 1kb file pointer instead of the real tokenizor\n\nThanks mm04926412,\
          \ you're a godsend! This was the fix for me as well. Would've thanked you\
          \ last night already if my newly made account wasn't restricted \U0001F64F"
        updatedAt: '2023-04-12T20:45:14.998Z'
      numEdits: 0
      reactions: []
    id: 6437185ac06cb2064ae4bbab
    type: comment
  author: Damgaardian
  content: "> I solved this problem on my machine, for some reason the tokenizor is\
    \ stored using github LFS despite being less than a megabyte, you likely have\
    \ a 1kb file pointer instead of the real tokenizor\n\nThanks mm04926412, you're\
    \ a godsend! This was the fix for me as well. Would've thanked you last night\
    \ already if my newly made account wasn't restricted \U0001F64F"
  created_at: 2023-04-12 19:45:14+00:00
  edited: false
  hidden: false
  id: 6437185ac06cb2064ae4bbab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87c1b99f17d1e592d59df21ae7c62f3e.svg
      fullname: SES
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrcrypto
      type: user
    createdAt: '2023-05-30T17:07:55.000Z'
    data:
      edited: false
      editors:
      - mrcrypto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87c1b99f17d1e592d59df21ae7c62f3e.svg
          fullname: SES
          isHf: false
          isPro: false
          name: mrcrypto
          type: user
        html: '<p>godsend! indeed</p>

          '
        raw: godsend! indeed
        updatedAt: '2023-05-30T17:07:55.148Z'
      numEdits: 0
      reactions: []
    id: 64762d6b9dbcc90b55091116
    type: comment
  author: mrcrypto
  content: godsend! indeed
  created_at: 2023-05-30 16:07:55+00:00
  edited: false
  hidden: false
  id: 64762d6b9dbcc90b55091116
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: Internal: D:\a\sentencepiece\sentencepiece\src\sentencepiece_processor.cc(1102)
  [model_proto->ParseFromArray(serialized.data(), serialized.size())]'
