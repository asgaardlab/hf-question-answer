!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TusharRay
conflicting_files: null
created_at: 2023-06-03 14:09:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d403ccccad3c8c25a8f387174cf5d1a5.svg
      fullname: Agey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TusharRay
      type: user
    createdAt: '2023-06-03T15:09:30.000Z'
    data:
      edited: false
      editors:
      - TusharRay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034325480461121
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d403ccccad3c8c25a8f387174cf5d1a5.svg
          fullname: Agey
          isHf: false
          isPro: false
          name: TusharRay
          type: user
        html: '<p>As per <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1508">https://github.com/ggerganov/llama.cpp/pull/1508</a>,
          the current ggml q4_1 file in this model is out of support. Need to re-quantize
          with supported format for people to consume.</p>

          '
        raw: As per https://github.com/ggerganov/llama.cpp/pull/1508, the current
          ggml q4_1 file in this model is out of support. Need to re-quantize with
          supported format for people to consume.
        updatedAt: '2023-06-03T15:09:30.879Z'
      numEdits: 0
      reactions: []
    id: 647b57aac3c809c69d303d02
    type: comment
  author: TusharRay
  content: As per https://github.com/ggerganov/llama.cpp/pull/1508, the current ggml
    q4_1 file in this model is out of support. Need to re-quantize with supported
    format for people to consume.
  created_at: 2023-06-03 14:09:30+00:00
  edited: false
  hidden: false
  id: 647b57aac3c809c69d303d02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eca3f8112fd66829958e7d1c2d6a0c2c.svg
      fullname: Flavio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fjcaetano
      type: user
    createdAt: '2023-06-08T21:04:09.000Z'
    data:
      edited: false
      editors:
      - fjcaetano
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.448350191116333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eca3f8112fd66829958e7d1c2d6a0c2c.svg
          fullname: Flavio
          isHf: false
          isPro: false
          name: fjcaetano
          type: user
        html: "<p>Same issue here. Conversion using the current llama.cpp script does\
          \ not work. This is the output I'm getting when using the bundled ggml model:</p>\n\
          <pre><code>(llama.cpp) \u279C  llama.cpp git:(master) \u2717 ./main -n 256\
          \ -ngl 1 -c 2048 -p \"the truth is\" -m ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
          main: warning: model does not support context sizes greater than 2048 tokens\
          \ (4096 specified);expect poor results\nmain: build = 635 (5c64a09)\nmain:\
          \ seed  = 1686257750\nllama.cpp: loading model from ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_init_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model '../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin'\n\
          main: error: unable to load model\n</code></pre>\n"
        raw: "Same issue here. Conversion using the current llama.cpp script does\
          \ not work. This is the output I'm getting when using the bundled ggml model:\n\
          \n```\n(llama.cpp) \u279C  llama.cpp git:(master) \u2717 ./main -n 256 -ngl\
          \ 1 -c 2048 -p \"the truth is\" -m ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
          main: warning: model does not support context sizes greater than 2048 tokens\
          \ (4096 specified);expect poor results\nmain: build = 635 (5c64a09)\nmain:\
          \ seed  = 1686257750\nllama.cpp: loading model from ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_init_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model '../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin'\n\
          main: error: unable to load model\n```"
        updatedAt: '2023-06-08T21:04:09.086Z'
      numEdits: 0
      reactions: []
    id: 6482424910cd9ffea8a2dc42
    type: comment
  author: fjcaetano
  content: "Same issue here. Conversion using the current llama.cpp script does not\
    \ work. This is the output I'm getting when using the bundled ggml model:\n\n\
    ```\n(llama.cpp) \u279C  llama.cpp git:(master) \u2717 ./main -n 256 -ngl 1 -c\
    \ 2048 -p \"the truth is\" -m ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
    main: warning: model does not support context sizes greater than 2048 tokens (4096\
    \ specified);expect poor results\nmain: build = 635 (5c64a09)\nmain: seed  = 1686257750\n\
    llama.cpp: loading model from ../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n\
    error loading model: unexpectedly reached end of file\nllama_init_from_file: failed\
    \ to load model\nllama_init_from_gpt_params: error: failed to load model '../gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin'\n\
    main: error: unable to load model\n```"
  created_at: 2023-06-08 20:04:09+00:00
  edited: false
  hidden: false
  id: 6482424910cd9ffea8a2dc42
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: Requantize to support latest code on llama.cpp
