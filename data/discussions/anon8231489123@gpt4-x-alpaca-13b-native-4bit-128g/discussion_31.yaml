!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZeroH3art
conflicting_files: null
created_at: 2023-04-13 00:23:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e6180eb2d3d785087f80a6f050d2e86.svg
      fullname: ZeroHeart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeroH3art
      type: user
    createdAt: '2023-04-13T01:23:02.000Z'
    data:
      edited: false
      editors:
      - ZeroH3art
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e6180eb2d3d785087f80a6f050d2e86.svg
          fullname: ZeroHeart
          isHf: false
          isPro: false
          name: ZeroH3art
          type: user
        html: '<p>Hello am getting this bellow error when launching the version Oobabooga
          Webui</p>

          <p>CUDA SETUP: Loading binary D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cpu.dll...<br>D:\New
          folder\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\cextension.py:31:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers and GPU quantization are unavailable.<br>warn("The
          installed version of bitsandbytes was compiled without GPU support. "<br>Loading
          the extension "gallery"... Ok.<br>Running on local URL: <a rel="nofollow"
          href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set share=True in launch().<br>Traceback (most
          recent call last):<br>File "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\gradio\routes.py",
          line 393, in run_predict<br>output = await app.get_blocks().process_api(<br>File
          "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\gradio\blocks.py",
          line 1108, in process_api<br>result = await self.call_function(<br>File
          "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\gradio\blocks.py",
          line 929, in call_function<br>prediction = await anyio.to_thread.run_sync(<br>File
          "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\anyio\to_thread.py",
          line 31, in run_sync<br>return await get_asynclib().run_sync_in_worker_thread(<br>File
          "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\anyio_backends_asyncio.py",
          line 937, in run_sync_in_worker_thread<br>return await future<br>File "D:\New
          folder\oobabooga-windows\installer_files\env\lib\site-packages\anyio_backends_asyncio.py",
          line 867, in run<br>result = context.run(func, *args)<br>File "D:\New folder\oobabooga-windows\installer_files\env\lib\site-packages\gradio\utils.py",
          line 490, in async_iteration<br>return next(iterator)<br>File "D:\New folder\oobabooga-windows\text-generation-webui\modules\chat.py",
          line 218, in cai_chatbot_wrapper<br>for history in chatbot_wrapper(text,
          state):<br>File "D:\New folder\oobabooga-windows\text-generation-webui\modules\chat.py",
          line 155, in chatbot_wrapper<br>for reply in generate_reply(f"{prompt}{''
          '' if len(cumulative_reply) &gt; 0 else ''''}{cumulative_reply}", state,
          eos_token=eos_token, stopping_strings=stopping_strings):<br>File "D:\New
          folder\oobabooga-windows\text-generation-webui\modules\text_generation.py",
          line 175, in generate_reply<br>input_ids = encode(question, add_bos_token=state[''add_bos_token''],
          truncation_length=get_max_prompt_length(state))<br>File "D:\New folder\oobabooga-windows\text-generation-webui\modules\text_generation.py",
          line 31, in encode<br>input_ids = shared.tokenizer.encode(str(prompt), return_tensors=''pt'',
          add_special_tokens=add_special_tokens)<br>AttributeError: ''NoneType'' object
          has no attribute ''encode''</p>

          '
        raw: "Hello am getting this bellow error when launching the version Oobabooga\
          \ Webui\r\n\r\nCUDA SETUP: Loading binary D:\\New folder\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...\r\
          \nD:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes\
          \ was compiled without GPU support. 8-bit optimizers and GPU quantization\
          \ are unavailable.\r\nwarn(\"The installed version of bitsandbytes was compiled\
          \ without GPU support. \"\r\nLoading the extension \"gallery\"... Ok.\r\n\
          Running on local URL: http://127.0.0.1:7860\r\n\r\nTo create a public link,\
          \ set share=True in launch().\r\nTraceback (most recent call last):\r\n\
          File \"D:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          gradio\\routes.py\", line 393, in run_predict\r\noutput = await app.get_blocks().process_api(\r\
          \nFile \"D:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\gradio\\blocks.py\", line 1108, in process_api\r\nresult\
          \ = await self.call_function(\r\nFile \"D:\\New folder\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\gradio\\blocks.py\", line 929,\
          \ in call_function\r\nprediction = await anyio.to_thread.run_sync(\r\nFile\
          \ \"D:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          anyio\\to_thread.py\", line 31, in run_sync\r\nreturn await get_asynclib().run_sync_in_worker_thread(\r\
          \nFile \"D:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\anyio_backends_asyncio.py\", line 937, in run_sync_in_worker_thread\r\
          \nreturn await future\r\nFile \"D:\\New folder\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\anyio_backends_asyncio.py\", line 867, in run\r\
          \nresult = context.run(func, *args)\r\nFile \"D:\\New folder\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\gradio\\utils.py\", line 490,\
          \ in async_iteration\r\nreturn next(iterator)\r\nFile \"D:\\New folder\\\
          oobabooga-windows\\text-generation-webui\\modules\\chat.py\", line 218,\
          \ in cai_chatbot_wrapper\r\nfor history in chatbot_wrapper(text, state):\r\
          \nFile \"D:\\New folder\\oobabooga-windows\\text-generation-webui\\modules\\\
          chat.py\", line 155, in chatbot_wrapper\r\nfor reply in generate_reply(f\"\
          {prompt}{' ' if len(cumulative_reply) > 0 else ''}{cumulative_reply}\",\
          \ state, eos_token=eos_token, stopping_strings=stopping_strings):\r\nFile\
          \ \"D:\\New folder\\oobabooga-windows\\text-generation-webui\\modules\\\
          text_generation.py\", line 175, in generate_reply\r\ninput_ids = encode(question,\
          \ add_bos_token=state['add_bos_token'], truncation_length=get_max_prompt_length(state))\r\
          \nFile \"D:\\New folder\\oobabooga-windows\\text-generation-webui\\modules\\\
          text_generation.py\", line 31, in encode\r\ninput_ids = shared.tokenizer.encode(str(prompt),\
          \ return_tensors='pt', add_special_tokens=add_special_tokens)\r\nAttributeError:\
          \ 'NoneType' object has no attribute 'encode'"
        updatedAt: '2023-04-13T01:23:02.451Z'
      numEdits: 0
      reactions: []
    id: 643759769dec089097c4b27c
    type: comment
  author: ZeroH3art
  content: "Hello am getting this bellow error when launching the version Oobabooga\
    \ Webui\r\n\r\nCUDA SETUP: Loading binary D:\\New folder\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...\r\
    \nD:\\New folder\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes\
    \ was compiled without GPU support. 8-bit optimizers and GPU quantization are\
    \ unavailable.\r\nwarn(\"The installed version of bitsandbytes was compiled without\
    \ GPU support. \"\r\nLoading the extension \"gallery\"... Ok.\r\nRunning on local\
    \ URL: http://127.0.0.1:7860\r\n\r\nTo create a public link, set share=True in\
    \ launch().\r\nTraceback (most recent call last):\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\r\
    \noutput = await app.get_blocks().process_api(\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\gradio\\blocks.py\", line 1108, in process_api\r\
    \nresult = await self.call_function(\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\gradio\\blocks.py\", line 929, in call_function\r\
    \nprediction = await anyio.to_thread.run_sync(\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\
    \nreturn await get_asynclib().run_sync_in_worker_thread(\r\nFile \"D:\\New folder\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\anyio_backends_asyncio.py\"\
    , line 937, in run_sync_in_worker_thread\r\nreturn await future\r\nFile \"D:\\\
    New folder\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\anyio_backends_asyncio.py\"\
    , line 867, in run\r\nresult = context.run(func, *args)\r\nFile \"D:\\New folder\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\gradio\\utils.py\"\
    , line 490, in async_iteration\r\nreturn next(iterator)\r\nFile \"D:\\New folder\\\
    oobabooga-windows\\text-generation-webui\\modules\\chat.py\", line 218, in cai_chatbot_wrapper\r\
    \nfor history in chatbot_wrapper(text, state):\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    text-generation-webui\\modules\\chat.py\", line 155, in chatbot_wrapper\r\nfor\
    \ reply in generate_reply(f\"{prompt}{' ' if len(cumulative_reply) > 0 else ''}{cumulative_reply}\"\
    , state, eos_token=eos_token, stopping_strings=stopping_strings):\r\nFile \"D:\\\
    New folder\\oobabooga-windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 175, in generate_reply\r\ninput_ids = encode(question, add_bos_token=state['add_bos_token'],\
    \ truncation_length=get_max_prompt_length(state))\r\nFile \"D:\\New folder\\oobabooga-windows\\\
    text-generation-webui\\modules\\text_generation.py\", line 31, in encode\r\ninput_ids\
    \ = shared.tokenizer.encode(str(prompt), return_tensors='pt', add_special_tokens=add_special_tokens)\r\
    \nAttributeError: 'NoneType' object has no attribute 'encode'"
  created_at: 2023-04-13 00:23:02+00:00
  edited: false
  hidden: false
  id: 643759769dec089097c4b27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb4e250b62087e3ddd665de89b3bc4e6.svg
      fullname: Jordi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jordicor
      type: user
    createdAt: '2023-04-13T12:06:48.000Z'
    data:
      edited: false
      editors:
      - jordicor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb4e250b62087e3ddd665de89b3bc4e6.svg
          fullname: Jordi
          isHf: false
          isPro: false
          name: jordicor
          type: user
        html: '<p>same issue here</p>

          '
        raw: same issue here
        updatedAt: '2023-04-13T12:06:48.399Z'
      numEdits: 0
      reactions: []
    id: 6437f058c212a363c3aa5cd9
    type: comment
  author: jordicor
  content: same issue here
  created_at: 2023-04-13 11:06:48+00:00
  edited: false
  hidden: false
  id: 6437f058c212a363c3aa5cd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb4e250b62087e3ddd665de89b3bc4e6.svg
      fullname: Jordi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jordicor
      type: user
    createdAt: '2023-04-13T13:42:03.000Z'
    data:
      edited: false
      editors:
      - jordicor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb4e250b62087e3ddd665de89b3bc4e6.svg
          fullname: Jordi
          isHf: false
          isPro: false
          name: jordicor
          type: user
        html: '<p>ok.. going to "model" tab and selecting one fixes the problem ^^u</p>

          '
        raw: ok.. going to "model" tab and selecting one fixes the problem ^^u
        updatedAt: '2023-04-13T13:42:03.801Z'
      numEdits: 0
      reactions: []
    id: 643806ab7217743e7ca2f3ef
    type: comment
  author: jordicor
  content: ok.. going to "model" tab and selecting one fixes the problem ^^u
  created_at: 2023-04-13 12:42:03+00:00
  edited: false
  hidden: false
  id: 643806ab7217743e7ca2f3ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: No response from Abaca when using GPU version
