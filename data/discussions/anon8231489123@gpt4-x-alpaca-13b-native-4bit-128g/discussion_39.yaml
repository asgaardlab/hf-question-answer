!!python/object:huggingface_hub.community.DiscussionWithDetails
author: faaaaaaaaaaaa
conflicting_files: null
created_at: 2023-05-01 19:46:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/370edbd950f19db47140926db0850994.svg
      fullname: Seter Teeri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faaaaaaaaaaaa
      type: user
    createdAt: '2023-05-01T20:46:29.000Z'
    data:
      edited: false
      editors:
      - faaaaaaaaaaaa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/370edbd950f19db47140926db0850994.svg
          fullname: Seter Teeri
          isHf: false
          isPro: false
          name: faaaaaaaaaaaa
          type: user
        html: '<p>I can run the model fine with small context, but I''m finding that
          if I provide too much context, I run out of memory very quickly.</p>

          <p>Anyone have any tricks to reduce the vram by 1gb or more? </p>

          '
        raw: "I can run the model fine with small context, but I'm finding that if\
          \ I provide too much context, I run out of memory very quickly.\r\n\r\n\
          Anyone have any tricks to reduce the vram by 1gb or more? "
        updatedAt: '2023-05-01T20:46:29.216Z'
      numEdits: 0
      reactions: []
    id: 6450252533ac8f46fa10129a
    type: comment
  author: faaaaaaaaaaaa
  content: "I can run the model fine with small context, but I'm finding that if I\
    \ provide too much context, I run out of memory very quickly.\r\n\r\nAnyone have\
    \ any tricks to reduce the vram by 1gb or more? "
  created_at: 2023-05-01 19:46:29+00:00
  edited: false
  hidden: false
  id: 6450252533ac8f46fa10129a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-05-07T23:54:18.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>You can try to add another gpu to you pc  for video because windows
          is using about 1gb for itself if monitor is attached to your GPU</p>

          '
        raw: You can try to add another gpu to you pc  for video because windows is
          using about 1gb for itself if monitor is attached to your GPU
        updatedAt: '2023-05-07T23:54:18.070Z'
      numEdits: 0
      reactions: []
    id: 64583a2a5fc3b8a21ea8b6b1
    type: comment
  author: Onix22
  content: You can try to add another gpu to you pc  for video because windows is
    using about 1gb for itself if monitor is attached to your GPU
  created_at: 2023-05-07 22:54:18+00:00
  edited: false
  hidden: false
  id: 64583a2a5fc3b8a21ea8b6b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627a7917c7f48ed9dc4eafb2/lOEVhScKlCFIZxGfY4MOt.png?w=200&h=200&f=face
      fullname: Janice Pino
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: janicepino
      type: user
    createdAt: '2023-05-08T14:28:12.000Z'
    data:
      edited: false
      editors:
      - janicepino
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/627a7917c7f48ed9dc4eafb2/lOEVhScKlCFIZxGfY4MOt.png?w=200&h=200&f=face
          fullname: Janice Pino
          isHf: false
          isPro: false
          name: janicepino
          type: user
        html: '<p>If your context is large, it can goes to 15.5GB, I believe 13B model
          max out there.  I observed also that the memory will later drop back to
          10GB. Maybe they periodically do garbage collection? If such exists, I like
          to know how to trigger it.  I think the significant of this is that, 15.5GB
          will break even 3060 12GB configuration, leaving 3090/4090 the only option
          to run these things.</p>

          '
        raw: If your context is large, it can goes to 15.5GB, I believe 13B model
          max out there.  I observed also that the memory will later drop back to
          10GB. Maybe they periodically do garbage collection? If such exists, I like
          to know how to trigger it.  I think the significant of this is that, 15.5GB
          will break even 3060 12GB configuration, leaving 3090/4090 the only option
          to run these things.
        updatedAt: '2023-05-08T14:28:12.989Z'
      numEdits: 0
      reactions: []
    id: 645906fcf92601affa3152d7
    type: comment
  author: janicepino
  content: If your context is large, it can goes to 15.5GB, I believe 13B model max
    out there.  I observed also that the memory will later drop back to 10GB. Maybe
    they periodically do garbage collection? If such exists, I like to know how to
    trigger it.  I think the significant of this is that, 15.5GB will break even 3060
    12GB configuration, leaving 3090/4090 the only option to run these things.
  created_at: 2023-05-08 13:28:12+00:00
  edited: false
  hidden: false
  id: 645906fcf92601affa3152d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IgC0Hjj97_L9-a3dLWvwW.jpeg?w=200&h=200&f=face
      fullname: Lucy Beebe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CaptLucy
      type: user
    createdAt: '2023-05-09T16:50:47.000Z'
    data:
      edited: false
      editors:
      - CaptLucy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/IgC0Hjj97_L9-a3dLWvwW.jpeg?w=200&h=200&f=face
          fullname: Lucy Beebe
          isHf: false
          isPro: false
          name: CaptLucy
          type: user
        html: '<p>Not sure what UI you''re using. I''m using Oobabooga, and for me
          there is a couple settings that help. The first is to set the pre layer
          to 32. I do this with Oobabooga by adding "--pre_layer 32" to the "call
          python server.py" line of code in the bat file that launches it. The second
          is to set the maximum prompt size in tokens to a lower amount. I only have
          8gb of vram, so I have to limit it to 600 tokens. I decided on this amount
          because the Oobabooga terminal shows the number of tokens being used for
          context which means I could see that when I was getting the out of memory
          error, the most recent line of output in the terminal typically said it
          was using about 570-590 tokens. This tells me it was fine at that amount
          until I tried to add more. I attached a pic of what the output you would
          be looking at would look like and circled the number you would care about
          if you got the out of memory error. You could try to find the most recent
          amount of tokens that worked and use that value to cap it around that point.
          I ended up just creating a settings file to changes this stuff automatically
          at launch but you can usually find these setting somewhere in your UI. I''m
          a bit of an amateur with this stuff, so I''m sorry if I''m giving bad advice.
          It works for me though.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/645a7473d8ba048d02ab4015/49w940KYR2GJQ4Ihynd7d.png"><img
          alt="Screenshot 2023-05-09 113905.png" src="https://cdn-uploads.huggingface.co/production/uploads/645a7473d8ba048d02ab4015/49w940KYR2GJQ4Ihynd7d.png"></a></p>

          '
        raw: 'Not sure what UI you''re using. I''m using Oobabooga, and for me there
          is a couple settings that help. The first is to set the pre layer to 32.
          I do this with Oobabooga by adding "--pre_layer 32" to the "call python
          server.py" line of code in the bat file that launches it. The second is
          to set the maximum prompt size in tokens to a lower amount. I only have
          8gb of vram, so I have to limit it to 600 tokens. I decided on this amount
          because the Oobabooga terminal shows the number of tokens being used for
          context which means I could see that when I was getting the out of memory
          error, the most recent line of output in the terminal typically said it
          was using about 570-590 tokens. This tells me it was fine at that amount
          until I tried to add more. I attached a pic of what the output you would
          be looking at would look like and circled the number you would care about
          if you got the out of memory error. You could try to find the most recent
          amount of tokens that worked and use that value to cap it around that point.
          I ended up just creating a settings file to changes this stuff automatically
          at launch but you can usually find these setting somewhere in your UI. I''m
          a bit of an amateur with this stuff, so I''m sorry if I''m giving bad advice.
          It works for me though.

          ![Screenshot 2023-05-09 113905.png](https://cdn-uploads.huggingface.co/production/uploads/645a7473d8ba048d02ab4015/49w940KYR2GJQ4Ihynd7d.png)'
        updatedAt: '2023-05-09T16:50:47.293Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ricku
        - MokusEde
        - neojutsu123
        - MalMisc
    id: 645a79e716dd9c07825e682f
    type: comment
  author: CaptLucy
  content: 'Not sure what UI you''re using. I''m using Oobabooga, and for me there
    is a couple settings that help. The first is to set the pre layer to 32. I do
    this with Oobabooga by adding "--pre_layer 32" to the "call python server.py"
    line of code in the bat file that launches it. The second is to set the maximum
    prompt size in tokens to a lower amount. I only have 8gb of vram, so I have to
    limit it to 600 tokens. I decided on this amount because the Oobabooga terminal
    shows the number of tokens being used for context which means I could see that
    when I was getting the out of memory error, the most recent line of output in
    the terminal typically said it was using about 570-590 tokens. This tells me it
    was fine at that amount until I tried to add more. I attached a pic of what the
    output you would be looking at would look like and circled the number you would
    care about if you got the out of memory error. You could try to find the most
    recent amount of tokens that worked and use that value to cap it around that point.
    I ended up just creating a settings file to changes this stuff automatically at
    launch but you can usually find these setting somewhere in your UI. I''m a bit
    of an amateur with this stuff, so I''m sorry if I''m giving bad advice. It works
    for me though.

    ![Screenshot 2023-05-09 113905.png](https://cdn-uploads.huggingface.co/production/uploads/645a7473d8ba048d02ab4015/49w940KYR2GJQ4Ihynd7d.png)'
  created_at: 2023-05-09 15:50:47+00:00
  edited: false
  hidden: false
  id: 645a79e716dd9c07825e682f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'Running out of memory with 12GB of VRAM on 3080TI '
