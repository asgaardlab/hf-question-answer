!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jojokingxp45
conflicting_files: null
created_at: 2023-04-14 19:24:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2ff6290eef4d748b7c7c9a8ad3ff3ed.svg
      fullname: Johannes Schuster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jojokingxp45
      type: user
    createdAt: '2023-04-14T20:24:33.000Z'
    data:
      edited: false
      editors:
      - jojokingxp45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2ff6290eef4d748b7c7c9a8ad3ff3ed.svg
          fullname: Johannes Schuster
          isHf: false
          isPro: false
          name: jojokingxp45
          type: user
        html: '<p>I''ve played with the parameters a bit, but even with using: </p>

          <p>call python server.py --auto-devices --chat --wbits 4 --groupsize 128
          --gpu-memory 7 --pre_layer 19</p>

          <p>It still is really slow. Now I know my Card only has 8 Gigs of VRAM,
          and I''ve fixed the running out of VRAM problem, but it still seems a bit
          slow, no matter what I do.</p>

          <p>I don''t know if this is relevant, but my general specs are:<br>Ryzen
          9 3900X<br>16GB DDR4 RAM<br>RTX 3070 8GB</p>

          '
        raw: "I've played with the parameters a bit, but even with using: \r\n\r\n\
          call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory\
          \ 7 --pre_layer 19\r\n\r\nIt still is really slow. Now I know my Card only\
          \ has 8 Gigs of VRAM, and I've fixed the running out of VRAM problem, but\
          \ it still seems a bit slow, no matter what I do.\r\n\r\nI don't know if\
          \ this is relevant, but my general specs are:\r\nRyzen 9 3900X\r\n16GB DDR4\
          \ RAM\r\nRTX 3070 8GB\r\n\r\n"
        updatedAt: '2023-04-14T20:24:33.833Z'
      numEdits: 0
      reactions: []
    id: 6439b681bd5096d434778f59
    type: comment
  author: jojokingxp45
  content: "I've played with the parameters a bit, but even with using: \r\n\r\ncall\
    \ python server.py --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory\
    \ 7 --pre_layer 19\r\n\r\nIt still is really slow. Now I know my Card only has\
    \ 8 Gigs of VRAM, and I've fixed the running out of VRAM problem, but it still\
    \ seems a bit slow, no matter what I do.\r\n\r\nI don't know if this is relevant,\
    \ but my general specs are:\r\nRyzen 9 3900X\r\n16GB DDR4 RAM\r\nRTX 3070 8GB\r\
    \n\r\n"
  created_at: 2023-04-14 19:24:33+00:00
  edited: false
  hidden: false
  id: 6439b681bd5096d434778f59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3db91ddae463eb283d731afa5c03d888.svg
      fullname: alkhanzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alkhanzi
      type: user
    createdAt: '2023-04-15T19:38:42.000Z'
    data:
      edited: false
      editors:
      - alkhanzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3db91ddae463eb283d731afa5c03d888.svg
          fullname: alkhanzi
          isHf: false
          isPro: false
          name: alkhanzi
          type: user
        html: '<p>Whoa I get around 8 tokens/s with a 3060 12GB.<br>So you have set
          the --pre_layer to 19 which basically puts parts of your model in GPU VRAM
          and the rest in CPU RAM. The communication between VRAM and CPU-RAM would
          be extremely slower. Not sure if this is the only reason, check if you installed
          with the latest / fastest CUDA and pytorch versions.</p>

          '
        raw: "Whoa I get around 8 tokens/s with a 3060 12GB. \nSo you have set the\
          \ --pre_layer to 19 which basically puts parts of your model in GPU VRAM\
          \ and the rest in CPU RAM. The communication between VRAM and CPU-RAM would\
          \ be extremely slower. Not sure if this is the only reason, check if you\
          \ installed with the latest / fastest CUDA and pytorch versions."
        updatedAt: '2023-04-15T19:38:42.006Z'
      numEdits: 0
      reactions: []
    id: 643afd425a3b913ad46d1f9f
    type: comment
  author: alkhanzi
  content: "Whoa I get around 8 tokens/s with a 3060 12GB. \nSo you have set the --pre_layer\
    \ to 19 which basically puts parts of your model in GPU VRAM and the rest in CPU\
    \ RAM. The communication between VRAM and CPU-RAM would be extremely slower. Not\
    \ sure if this is the only reason, check if you installed with the latest / fastest\
    \ CUDA and pytorch versions."
  created_at: 2023-04-15 18:38:42+00:00
  edited: false
  hidden: false
  id: 643afd425a3b913ad46d1f9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fd5bd55d8a3a6490e5874c0ba4842c8.svg
      fullname: Glenn Wehmeyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djstraylight
      type: user
    createdAt: '2023-04-15T20:32:26.000Z'
    data:
      edited: false
      editors:
      - djstraylight
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fd5bd55d8a3a6490e5874c0ba4842c8.svg
          fullname: Glenn Wehmeyer
          isHf: false
          isPro: false
          name: djstraylight
          type: user
        html: '<p>I also have a video card that only has 8GB VRam and the model fits
          in the card''s memory but there isn''t enough room for inference. Once you
          put some layers of the model on the CPU it became super slow. I''m very
          disappointed that they only put 8GB on a 3070.</p>

          <p>This model runs pretty well on a 3080 and fits into the 10GB of VRAM.  If
          you have another nvidia card, you might be able to use the vram on both
          cards.</p>

          '
        raw: 'I also have a video card that only has 8GB VRam and the model fits in
          the card''s memory but there isn''t enough room for inference. Once you
          put some layers of the model on the CPU it became super slow. I''m very
          disappointed that they only put 8GB on a 3070.


          This model runs pretty well on a 3080 and fits into the 10GB of VRAM.  If
          you have another nvidia card, you might be able to use the vram on both
          cards.'
        updatedAt: '2023-04-15T20:32:26.878Z'
      numEdits: 0
      reactions: []
    id: 643b09da101fbcb9402f9b16
    type: comment
  author: djstraylight
  content: 'I also have a video card that only has 8GB VRam and the model fits in
    the card''s memory but there isn''t enough room for inference. Once you put some
    layers of the model on the CPU it became super slow. I''m very disappointed that
    they only put 8GB on a 3070.


    This model runs pretty well on a 3080 and fits into the 10GB of VRAM.  If you
    have another nvidia card, you might be able to use the vram on both cards.'
  created_at: 2023-04-15 19:32:26+00:00
  edited: false
  hidden: false
  id: 643b09da101fbcb9402f9b16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a164106a5737e21fc6e31224cfe6c33.svg
      fullname: Dmitry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dezmo
      type: user
    createdAt: '2023-04-18T02:52:41.000Z'
    data:
      edited: false
      editors:
      - Dezmo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a164106a5737e21fc6e31224cfe6c33.svg
          fullname: Dmitry
          isHf: false
          isPro: false
          name: Dezmo
          type: user
        html: '<blockquote>

          <p>I''ve played with the parameters a bit, but even with using: </p>

          <p>call python server.py --auto-devices --chat --wbits 4 --groupsize 128
          --gpu-memory 7 --pre_layer 19</p>

          <p>It still is really slow. Now I know my Card only has 8 Gigs of VRAM,
          and I''ve fixed the running out of VRAM problem, but it still seems a bit
          slow, no matter what I do.</p>

          <p>I don''t know if this is relevant, but my general specs are:<br>Ryzen
          9 3900X<br>16GB DDR4 RAM<br>RTX 3070 8GB</p>

          </blockquote>

          <p>Yes, I have the same problem. The GPU is used at only 2%</p>

          '
        raw: "> I've played with the parameters a bit, but even with using: \n> \n\
          > call python server.py --auto-devices --chat --wbits 4 --groupsize 128\
          \ --gpu-memory 7 --pre_layer 19\n> \n> It still is really slow. Now I know\
          \ my Card only has 8 Gigs of VRAM, and I've fixed the running out of VRAM\
          \ problem, but it still seems a bit slow, no matter what I do.\n> \n> I\
          \ don't know if this is relevant, but my general specs are:\n> Ryzen 9 3900X\n\
          > 16GB DDR4 RAM\n> RTX 3070 8GB\n\nYes, I have the same problem. The GPU\
          \ is used at only 2%"
        updatedAt: '2023-04-18T02:52:41.420Z'
      numEdits: 0
      reactions: []
    id: 643e05f97665793135bca6a8
    type: comment
  author: Dezmo
  content: "> I've played with the parameters a bit, but even with using: \n> \n>\
    \ call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --gpu-memory\
    \ 7 --pre_layer 19\n> \n> It still is really slow. Now I know my Card only has\
    \ 8 Gigs of VRAM, and I've fixed the running out of VRAM problem, but it still\
    \ seems a bit slow, no matter what I do.\n> \n> I don't know if this is relevant,\
    \ but my general specs are:\n> Ryzen 9 3900X\n> 16GB DDR4 RAM\n> RTX 3070 8GB\n\
    \nYes, I have the same problem. The GPU is used at only 2%"
  created_at: 2023-04-18 01:52:41+00:00
  edited: false
  hidden: false
  id: 643e05f97665793135bca6a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: RTX 3070, only getting about 0,38 tokens/minute
