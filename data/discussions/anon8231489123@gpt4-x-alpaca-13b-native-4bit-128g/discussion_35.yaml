!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FastRide2
conflicting_files: null
created_at: 2023-04-16 08:28:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/GXVhfeWb3K5sIENRzIEZj.png?w=200&h=200&f=face
      fullname: Christopher Schaaf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FastRide2
      type: user
    createdAt: '2023-04-16T09:28:07.000Z'
    data:
      edited: false
      editors:
      - FastRide2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/GXVhfeWb3K5sIENRzIEZj.png?w=200&h=200&f=face
          fullname: Christopher Schaaf
          isHf: false
          isPro: false
          name: FastRide2
          type: user
        html: "<p>I am getting this error. Setting CPU and GPU to 1MiB doesn't change\
          \ this error at all.</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
          , line 85, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 100, in load_model<br>model = load_quantized(model_name)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 151, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 32, in _load_quant<br>model = AutoModelForCausalLM.from_config(config)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 411, in from_config<br>return\
          \ model_class._from_config(config, **kwargs)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 1146, in _from_config<br>model = cls(config, **kwargs)<br>File \u201C\
          D:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 614, in init<br>self.model\
          \ = LlamaModel(config)<br>File \u201CD:\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 445, in init<br>self.layers = nn.ModuleList([LlamaDecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])<br>File \u201CD:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 445, in<br>self.layers = nn.ModuleList([LlamaDecoderLayer(config)\
          \ for _ in range(config.num_hidden_layers)])<br>File \u201CD:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 255, in init<br>self.self_attn = LlamaAttention(config=config)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 180, in init<br>self.rotary_emb\
          \ = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 106, in init<br>self.register_buffer(\u201C\
          cos_cached\u201D, emb.cos()[None, None, :, :], persistent=False)<br>RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 1048576 bytes.</p>\n"
        raw: "I am getting this error. Setting CPU and GPU to 1MiB doesn't change\
          \ this error at all.\r\n\r\nTraceback (most recent call last):\r\nFile \u201C\
          D:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line\
          \ 85, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 100, in load_model\r\nmodel = load_quantized(model_name)\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 151, in load_quantized\r\nmodel = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 32, in _load_quant\r\nmodel = AutoModelForCausalLM.from_config(config)\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 411, in from_config\r\
          \nreturn model_class._from_config(config, **kwargs)\r\nFile \u201CD:\\AI\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 1146, in _from_config\r\nmodel = cls(config,\
          \ **kwargs)\r\nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 614, in init\r\nself.model = LlamaModel(config)\r\nFile \u201CD:\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 445, in init\r\nself.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in\r\nself.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 255, in init\r\
          \nself.self_attn = LlamaAttention(config=config)\r\nFile \u201CD:\\AI\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 180, in init\r\nself.rotary_emb\
          \ = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\r\
          \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 106, in init\r\
          \nself.register_buffer(\u201Ccos_cached\u201D, emb.cos()[None, None, :,\
          \ :], persistent=False)\r\nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
          work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 1048576 bytes."
        updatedAt: '2023-04-16T09:28:07.141Z'
      numEdits: 0
      reactions: []
    id: 643bbfa7ae8d93dc394be6e5
    type: comment
  author: FastRide2
  content: "I am getting this error. Setting CPU and GPU to 1MiB doesn't change this\
    \ error at all.\r\n\r\nTraceback (most recent call last):\r\nFile \u201CD:\\AI\\\
    oobabooga-windows\\text-generation-webui\\server.py\u201D, line 85, in load_model_wrapper\r\
    \nshared.model, shared.tokenizer = load_model(shared.model_name)\r\nFile \u201C\
    D:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 100, in load_model\r\nmodel = load_quantized(model_name)\r\nFile \u201CD:\\\
    AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D, line\
    \ 151, in load_quantized\r\nmodel = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 32, in _load_quant\r\nmodel = AutoModelForCausalLM.from_config(config)\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 411, in from_config\r\n\
    return model_class._from_config(config, **kwargs)\r\nFile \u201CD:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 1146, in _from_config\r\nmodel = cls(config, **kwargs)\r\nFile \u201CD:\\\
    AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\u201D, line 614, in init\r\nself.model = LlamaModel(config)\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in init\r\nself.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in\r\nself.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 255, in init\r\nself.self_attn\
    \ = LlamaAttention(config=config)\r\nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 180, in init\r\nself.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\r\
    \nFile \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 106, in init\r\nself.register_buffer(\u201C\
    cos_cached\u201D, emb.cos()[None, None, :, :], persistent=False)\r\nRuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 1048576\
    \ bytes."
  created_at: 2023-04-16 08:28:07+00:00
  edited: false
  hidden: false
  id: 643bbfa7ae8d93dc394be6e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84aaef90eb455dbded02b7d2c50552b7.svg
      fullname: Yaroslav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yar132
      type: user
    createdAt: '2023-04-16T11:07:10.000Z'
    data:
      edited: false
      editors:
      - Yar132
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84aaef90eb455dbded02b7d2c50552b7.svg
          fullname: Yaroslav
          isHf: false
          isPro: false
          name: Yar132
          type: user
        html: '<p>The same issue. that''s good I''m not alone....</p>

          '
        raw: The same issue. that's good I'm not alone....
        updatedAt: '2023-04-16T11:07:10.981Z'
      numEdits: 0
      reactions: []
    id: 643bd6de25c7610a1cd440d5
    type: comment
  author: Yar132
  content: The same issue. that's good I'm not alone....
  created_at: 2023-04-16 10:07:10+00:00
  edited: false
  hidden: false
  id: 643bd6de25c7610a1cd440d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
      fullname: ML
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Detpircsni
      type: user
    createdAt: '2023-04-19T19:05:53.000Z'
    data:
      edited: true
      editors:
      - Detpircsni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63572790d11d85992dff43b5/JcYZJcTKjVNRrbh-42Drz.png?w=200&h=200&f=face
          fullname: ML
          isHf: false
          isPro: false
          name: Detpircsni
          type: user
        html: "<blockquote>\n<p>I am getting this error. Setting CPU and GPU to 1MiB\
          \ doesn't change this error at all.</p>\n<p>Traceback (most recent call\
          \ last):<br>File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\\
          server.py\u201D, line 85, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 100, in load_model<br>model\
          \ = load_quantized(model_name)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          text-generation-webui\\modules\\GPTQ_loader.py\u201D, line 151, in load_quantized<br>model\
          \ = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize,\
          \ kernel_switch_threshold=threshold)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          text-generation-webui\\modules\\GPTQ_loader.py\u201D, line 32, in _load_quant<br>model\
          \ = AutoModelForCausalLM.from_config(config)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 411, in from_config<br>return model_class._from_config(config, **kwargs)<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 1146, in _from_config<br>model\
          \ = cls(config, **kwargs)<br>File \u201CD:\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 614, in init<br>self.model = LlamaModel(config)<br>File \u201CD:\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 445, in init<br>self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in<br>self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])<br>File\
          \ \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 255, in init<br>self.self_attn\
          \ = LlamaAttention(config=config)<br>File \u201CD:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 180, in init<br>self.rotary_emb = LlamaRotaryEmbedding(self.head_dim,\
          \ max_position_embeddings=self.max_position_embeddings)<br>File \u201CD:\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 106, in init<br>self.register_buffer(\u201C\
          cos_cached\u201D, emb.cos()[None, None, :, :], persistent=False)<br>RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 1048576 bytes.</p>\n</blockquote>\n<p>Please review this thread.\
          \ Lots of troubleshooting, patterns, and solutions were found. Feel free\
          \ to spread the news to anyone who has the same issue. </p>\n<p><a href=\"\
          https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15\"\
          >https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15</a></p>\n"
        raw: "> I am getting this error. Setting CPU and GPU to 1MiB doesn't change\
          \ this error at all.\n> \n> Traceback (most recent call last):\n> File \u201C\
          D:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line\
          \ 85, in load_model_wrapper\n> shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          > File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 100, in load_model\n> model = load_quantized(model_name)\n\
          > File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 151, in load_quantized\n> model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          > File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 32, in _load_quant\n> model = AutoModelForCausalLM.from_config(config)\n\
          > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 411, in from_config\n\
          > return model_class._from_config(config, **kwargs)\n> File \u201CD:\\AI\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 1146, in _from_config\n> model = cls(config,\
          \ **kwargs)\n> File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 614, in init\n> self.model = LlamaModel(config)\n> File \u201CD:\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 445, in init\n> self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in\n> self.layers\
          \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 255, in init\n\
          > self.self_attn = LlamaAttention(config=config)\n> File \u201CD:\\AI\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 180, in init\n> self.rotary_emb\
          \ = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\
          > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 106, in init\n\
          > self.register_buffer(\u201Ccos_cached\u201D, emb.cos()[None, None, :,\
          \ :], persistent=False)\n> RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
          work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 1048576 bytes.\n\nPlease review this\
          \ thread. Lots of troubleshooting, patterns, and solutions were found. Feel\
          \ free to spread the news to anyone who has the same issue. \n\nhttps://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15"
        updatedAt: '2023-04-19T19:06:15.749Z'
      numEdits: 1
      reactions: []
    id: 64403b91dbd88206a83d1aeb
    type: comment
  author: Detpircsni
  content: "> I am getting this error. Setting CPU and GPU to 1MiB doesn't change\
    \ this error at all.\n> \n> Traceback (most recent call last):\n> File \u201C\
    D:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line 85, in\
    \ load_model_wrapper\n> shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    > File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 100, in load_model\n> model = load_quantized(model_name)\n> File \u201C\
    D:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 151, in load_quantized\n> model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    > File \u201CD:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 32, in _load_quant\n> model = AutoModelForCausalLM.from_config(config)\n\
    > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 411, in from_config\n\
    > return model_class._from_config(config, **kwargs)\n> File \u201CD:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 1146, in _from_config\n> model = cls(config, **kwargs)\n> File \u201CD:\\\
    AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\u201D, line 614, in init\n> self.model = LlamaModel(config)\n\
    > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in init\n> self.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 445, in\n> self.layers\
    \ = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
    > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 255, in init\n> self.self_attn\
    \ = LlamaAttention(config=config)\n> File \u201CD:\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 180, in init\n> self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\
    > File \u201CD:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\u201D, line 106, in init\n> self.register_buffer(\u201C\
    cos_cached\u201D, emb.cos()[None, None, :, :], persistent=False)\n> RuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 1048576\
    \ bytes.\n\nPlease review this thread. Lots of troubleshooting, patterns, and\
    \ solutions were found. Feel free to spread the news to anyone who has the same\
    \ issue. \n\nhttps://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15"
  created_at: 2023-04-19 18:05:53+00:00
  edited: true
  hidden: false
  id: 64403b91dbd88206a83d1aeb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
  data. DefaultCPUAllocator: not enough memory: you tried to allocate 1048576 bytes.'
