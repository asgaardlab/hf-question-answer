!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Forceee
conflicting_files: null
created_at: 2023-04-09 10:56:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d4c8e5c64428093e86951efb3a61b4f.svg
      fullname: Mirsaid Abbasov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Forceee
      type: user
    createdAt: '2023-04-09T11:56:58.000Z'
    data:
      edited: false
      editors:
      - Forceee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d4c8e5c64428093e86951efb3a61b4f.svg
          fullname: Mirsaid Abbasov
          isHf: false
          isPro: false
          name: Forceee
          type: user
        html: '<p>Hello guys, When I want to run model, when enter something on chat,
          I don''t any any reply and get this error in cmd:<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 8.00 GiB total
          capacity; 7.06 GiB already allocated; 0 bytes free; 7.29 GiB reserved in
          total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF<br>Output generated in 3.35 seconds
          (0.00 tokens/s, 0 tokens, context 376)<br>How I can fix that?<br>Note: I
          am using RTX3070.</p>

          '
        raw: "Hello guys, When I want to run model, when enter something on chat,\
          \ I don't any any reply and get this error in cmd:\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 8.00 GiB total\
          \ capacity; 7.06 GiB already allocated; 0 bytes free; 7.29 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\nOutput generated in 3.35 seconds\
          \ (0.00 tokens/s, 0 tokens, context 376)\r\nHow I can fix that? \r\nNote:\
          \ I am using RTX3070.\r\n"
        updatedAt: '2023-04-09T11:56:58.535Z'
      numEdits: 0
      reactions: []
    id: 6432a80a6c2a26ae66d09689
    type: comment
  author: Forceee
  content: "Hello guys, When I want to run model, when enter something on chat, I\
    \ don't any any reply and get this error in cmd:\r\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 8.00 GiB total capacity;\
    \ 7.06 GiB already allocated; 0 bytes free; 7.29 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \nOutput generated in 3.35 seconds (0.00 tokens/s, 0 tokens, context 376)\r\n\
    How I can fix that? \r\nNote: I am using RTX3070.\r\n"
  created_at: 2023-04-09 10:56:58+00:00
  edited: false
  hidden: false
  id: 6432a80a6c2a26ae66d09689
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cafef6caadbcf9ece6c0d8285b109892.svg
      fullname: Rodrigo Banno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlossomSpring
      type: user
    createdAt: '2023-04-09T15:13:14.000Z'
    data:
      edited: false
      editors:
      - BlossomSpring
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cafef6caadbcf9ece6c0d8285b109892.svg
          fullname: Rodrigo Banno
          isHf: false
          isPro: false
          name: BlossomSpring
          type: user
        html: '<p>Watch my response in this post: <a href="https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15">https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15</a></p>

          '
        raw: 'Watch my response in this post: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15'
        updatedAt: '2023-04-09T15:13:14.656Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - GrimmDemise
    id: 6432d60a05e626d3a33d24db
    type: comment
  author: BlossomSpring
  content: 'Watch my response in this post: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/15'
  created_at: 2023-04-09 14:13:14+00:00
  edited: false
  hidden: false
  id: 6432d60a05e626d3a33d24db
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: torch.cuda.OutOfMemoryError
