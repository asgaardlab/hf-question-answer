!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sneet
conflicting_files: null
created_at: 2023-07-31 21:38:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cc59a70550c3de6598a734d3bf97753.svg
      fullname: Justin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sneet
      type: user
    createdAt: '2023-07-31T22:38:55.000Z'
    data:
      edited: false
      editors:
      - sneet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4158179759979248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cc59a70550c3de6598a734d3bf97753.svg
          fullname: Justin
          isHf: false
          isPro: false
          name: sneet
          type: user
        html: '<p>Good evening, </p>

          <p>I am attempting to host Alpaca through the Text Generation Web UI (oobabooga).
          I have gotten my Alpaca model to load successfully using the AutoGPTQ model
          loader. When I enter a prompt into the text generation window and await
          a response from the AI, I am met with an "Is typing..." status from the
          AI for about two minutes, then I receive this error in the console window.</p>

          <p>My knowledge of AI spans only a small amount, I would appreciate any
          insight as to what might be causing this. My machine specs are as follows:<br>NVIDIA
          GeForce GTX 1050ti<br>24GB Installed RAM</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 55, in gentask<br>    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 293, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 438, in generate<br>    return self.model.generate(**kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1588, in generate<br>    return self.sample(<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2642, in sample<br>    outputs = self(<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 806, in forward<br>    outputs = self.model(<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 693, in forward<br>    layer_outputs = decoder_layer(<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\accelerate\hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 408, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\nn_modules\fused_llama_attn.py",
          line 53, in forward<br>    qkv_states = self.qkv_proj(hidden_states)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\Justin\Desktop\GPT4\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\nn_modules\qlinear\qlinear_cuda_old.py",
          line 264, in forward<br>    out = out + self.bias if self.bias is not None
          else out<br>RuntimeError: Expected all tensors to be on the same device,
          but found at least two devices, cuda:0 and cpu!<br>Output generated in 34.38
          seconds (0.00 tokens/s, 0 tokens, context 37, seed 219873787)</p>

          '
        raw: "Good evening, \r\n\r\nI am attempting to host Alpaca through the Text\
          \ Generation Web UI (oobabooga). I have gotten my Alpaca model to load successfully\
          \ using the AutoGPTQ model loader. When I enter a prompt into the text generation\
          \ window and await a response from the AI, I am met with an \"Is typing...\"\
          \ status from the AI for about two minutes, then I receive this error in\
          \ the console window.\r\n\r\nMy knowledge of AI spans only a small amount,\
          \ I would appreciate any insight as to what might be causing this. My machine\
          \ specs are as follows:\r\nNVIDIA GeForce GTX 1050ti\r\n24GB Installed RAM\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Justin\\\
          Desktop\\GPT4\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
          , line 55, in gentask\r\n    ret = self.mfunc(callback=_callback, *args,\
          \ **self.kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
          text-generation-webui\\modules\\text_generation.py\", line 293, in generate_with_callback\r\
          \n    shared.model.generate(**kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\\
          GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling\\_base.py\", line 438, in generate\r\n    return self.model.generate(**kwargs)\r\
          \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
          \n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\\
          GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1588, in generate\r\n    return self.sample(\r\
          \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2642,\
          \ in sample\r\n    outputs = self(\r\n  File \"C:\\Users\\Justin\\Desktop\\\
          GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 806, in forward\r\n    outputs = self.model(\r\
          \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\\
          Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
          \ 693, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"C:\\\
          Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
          \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\Justin\\\
          Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 408, in forward\r\n    hidden_states, self_attn_weights,\
          \ present_key_value = self.self_attn(\r\n  File \"C:\\Users\\Justin\\Desktop\\\
          GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\fused_llama_attn.py\"\
          , line 53, in forward\r\n    qkv_states = self.qkv_proj(hidden_states)\r\
          \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\\
          Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\"\
          , line 264, in forward\r\n    out = out + self.bias if self.bias is not\
          \ None else out\r\nRuntimeError: Expected all tensors to be on the same\
          \ device, but found at least two devices, cuda:0 and cpu!\r\nOutput generated\
          \ in 34.38 seconds (0.00 tokens/s, 0 tokens, context 37, seed 219873787)"
        updatedAt: '2023-07-31T22:38:55.488Z'
      numEdits: 0
      reactions: []
    id: 64c837ff1c25d2c581982624
    type: comment
  author: sneet
  content: "Good evening, \r\n\r\nI am attempting to host Alpaca through the Text\
    \ Generation Web UI (oobabooga). I have gotten my Alpaca model to load successfully\
    \ using the AutoGPTQ model loader. When I enter a prompt into the text generation\
    \ window and await a response from the AI, I am met with an \"Is typing...\" status\
    \ from the AI for about two minutes, then I receive this error in the console\
    \ window.\r\n\r\nMy knowledge of AI spans only a small amount, I would appreciate\
    \ any insight as to what might be causing this. My machine specs are as follows:\r\
    \nNVIDIA GeForce GTX 1050ti\r\n24GB Installed RAM\r\n\r\nTraceback (most recent\
    \ call last):\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
    text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\r\n    ret\
    \ = self.mfunc(callback=_callback, *args, **self.kwargs)\r\n  File \"C:\\Users\\\
    Justin\\Desktop\\GPT4\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 293, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 438, in generate\r\
    \n    return self.model.generate(**kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\\
    GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\\
    _contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\
    \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1588, in generate\r\
    \n    return self.sample(\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
    , line 2642, in sample\r\n    outputs = self(\r\n  File \"C:\\Users\\Justin\\\
    Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 806, in forward\r\n    outputs = self.model(\r\n  File \"C:\\Users\\Justin\\\
    Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 693, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"C:\\\
    Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"C:\\Users\\\
    Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 408, in forward\r\n   \
    \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File\
    \ \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\\
    GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
    nn_modules\\fused_llama_attn.py\", line 53, in forward\r\n    qkv_states = self.qkv_proj(hidden_states)\r\
    \n  File \"C:\\Users\\Justin\\Desktop\\GPT4\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\Justin\\Desktop\\\
    GPT4\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
    nn_modules\\qlinear\\qlinear_cuda_old.py\", line 264, in forward\r\n    out =\
    \ out + self.bias if self.bias is not None else out\r\nRuntimeError: Expected\
    \ all tensors to be on the same device, but found at least two devices, cuda:0\
    \ and cpu!\r\nOutput generated in 34.38 seconds (0.00 tokens/s, 0 tokens, context\
    \ 37, seed 219873787)"
  created_at: 2023-07-31 21:38:55+00:00
  edited: false
  hidden: false
  id: 64c837ff1c25d2c581982624
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 53
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: '"RuntimeError: Expected all tensors to be on the same device, but found at
  least two devices, cuda:0 and cpu!"'
