!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muzammil-eds
conflicting_files: null
created_at: 2023-07-24 12:53:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62de34290765cdfd9909dac6/f3f8v0EGBr3HK2kFdOdzl.png?w=200&h=200&f=face
      fullname: Muhammad Muzammil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muzammil-eds
      type: user
    createdAt: '2023-07-24T13:53:21.000Z'
    data:
      edited: false
      editors:
      - muzammil-eds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7979363203048706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62de34290765cdfd9909dac6/f3f8v0EGBr3HK2kFdOdzl.png?w=200&h=200&f=face
          fullname: Muhammad Muzammil
          isHf: false
          isPro: false
          name: muzammil-eds
          type: user
        html: '<p>Hi,<br>Can we fine-tune this model or not? I have tried loading
          it using:</p>

          <p>model = LlamaForCausalLM.from_pretrained(<br>        ''anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g'',<br>        load_in_4bit=True,<br>        torch_dtype=torch.float16,<br>        device_map=device_map,<br>    )</p>

          <p>but it gave me  error, the file formats are very different from original
          alpaca-13b, in this repo the files are in .pt format.<br>can anyone tell,
          how to load and fine tune this model?</p>

          '
        raw: "Hi, \r\nCan we fine-tune this model or not? I have tried loading it\
          \ using:\r\n\r\nmodel = LlamaForCausalLM.from_pretrained(\r\n        'anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g',\r\
          \n        load_in_4bit=True,\r\n        torch_dtype=torch.float16,\r\n \
          \       device_map=device_map,\r\n    )\r\n\r\nbut it gave me  error, the\
          \ file formats are very different from original alpaca-13b, in this repo\
          \ the files are in .pt format.\r\ncan anyone tell, how to load and fine\
          \ tune this model?\r\n\r\n\r\n"
        updatedAt: '2023-07-24T13:53:21.301Z'
      numEdits: 0
      reactions: []
    id: 64be8251e38420aabaf25964
    type: comment
  author: muzammil-eds
  content: "Hi, \r\nCan we fine-tune this model or not? I have tried loading it using:\r\
    \n\r\nmodel = LlamaForCausalLM.from_pretrained(\r\n        'anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g',\r\
    \n        load_in_4bit=True,\r\n        torch_dtype=torch.float16,\r\n       \
    \ device_map=device_map,\r\n    )\r\n\r\nbut it gave me  error, the file formats\
    \ are very different from original alpaca-13b, in this repo the files are in .pt\
    \ format.\r\ncan anyone tell, how to load and fine tune this model?\r\n\r\n\r\n"
  created_at: 2023-07-24 12:53:21+00:00
  edited: false
  hidden: false
  id: 64be8251e38420aabaf25964
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 52
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: Fine tuning "gpt4-x-alpaca-13b-native-4bit-128g".
