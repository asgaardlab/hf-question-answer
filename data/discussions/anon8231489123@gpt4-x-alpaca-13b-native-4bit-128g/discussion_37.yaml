!!python/object:huggingface_hub.community.DiscussionWithDetails
author: socter
conflicting_files: null
created_at: 2023-04-20 05:16:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da00648031e53846b06fce61733c3670.svg
      fullname: bhav ashok
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: socter
      type: user
    createdAt: '2023-04-20T06:16:42.000Z'
    data:
      edited: false
      editors:
      - socter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da00648031e53846b06fce61733c3670.svg
          fullname: bhav ashok
          isHf: false
          isPro: false
          name: socter
          type: user
        html: '<p>So there have been a few threads on this and none of them had a
          conclusive answer so I  thought I would mention how I got this running.</p>

          <ol>

          <li>Download the model from the text-generation-webui root:</li>

          </ol>

          <pre><code> python download-model.py anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
          --output /path/to/your/output/folder

          </code></pre>

          <ol start="2">

          <li>Download cuda branch of GPTQ-for-LLaMa</li>

          </ol>

          <pre><code>git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git -b
          cuda

          pip install -r requirements.txt

          python setup_cuda.py install

          export PYTHON_PATH=$PYTHON_PATH:/path/to/GPTQ-for-LLaMa

          </code></pre>

          <ol start="3">

          <li>Start web ui</li>

          </ol>

          <pre><code>python server.py --wbits 4 --model-dir /path/to/your/output/folder
          --model_type LLaMa

          </code></pre>

          <ol start="4">

          <li>Rename the non-cuda model so that it picks the right one. You can remove
          it or rename it to anything else as long as when you load the model it selects
          the one with *-cuda.pt</li>

          </ol>

          <pre><code>mv /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt
          /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/0gpt-x-alpaca-13b-native-4bit-128g.pt

          </code></pre>

          <p>Now go to the models tab in the web UI and load the anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g</p>

          '
        raw: "So there have been a few threads on this and none of them had a conclusive\
          \ answer so I  thought I would mention how I got this running.\r\n\r\n1.\
          \ Download the model from the text-generation-webui root:\r\n```\r\n python\
          \ download-model.py anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g --output\
          \ /path/to/your/output/folder\r\n```\r\n2. Download cuda branch of GPTQ-for-LLaMa\r\
          \n```\r\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git -b\
          \ cuda\r\npip install -r requirements.txt\r\npython setup_cuda.py install\r\
          \nexport PYTHON_PATH=$PYTHON_PATH:/path/to/GPTQ-for-LLaMa\r\n```\r\n3. Start\
          \ web ui\r\n```\r\npython server.py --wbits 4 --model-dir /path/to/your/output/folder\
          \ --model_type LLaMa\r\n```\r\n4. Rename the non-cuda model so that it picks\
          \ the right one. You can remove it or rename it to anything else as long\
          \ as when you load the model it selects the one with *-cuda.pt\r\n```\r\n\
          mv /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\
          \ /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/0gpt-x-alpaca-13b-native-4bit-128g.pt\r\
          \n```\r\nNow go to the models tab in the web UI and load the anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\r\
          \n\r\n"
        updatedAt: '2023-04-20T06:16:42.999Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
        - marcasmed
    id: 6440d8ca8bf0d7756ef8c8f6
    type: comment
  author: socter
  content: "So there have been a few threads on this and none of them had a conclusive\
    \ answer so I  thought I would mention how I got this running.\r\n\r\n1. Download\
    \ the model from the text-generation-webui root:\r\n```\r\n python download-model.py\
    \ anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g --output /path/to/your/output/folder\r\
    \n```\r\n2. Download cuda branch of GPTQ-for-LLaMa\r\n```\r\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\
    \ -b cuda\r\npip install -r requirements.txt\r\npython setup_cuda.py install\r\
    \nexport PYTHON_PATH=$PYTHON_PATH:/path/to/GPTQ-for-LLaMa\r\n```\r\n3. Start web\
    \ ui\r\n```\r\npython server.py --wbits 4 --model-dir /path/to/your/output/folder\
    \ --model_type LLaMa\r\n```\r\n4. Rename the non-cuda model so that it picks the\
    \ right one. You can remove it or rename it to anything else as long as when you\
    \ load the model it selects the one with *-cuda.pt\r\n```\r\nmv /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/gpt-x-alpaca-13b-native-4bit-128g.pt\
    \ /path/to/your/output/folder/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/0gpt-x-alpaca-13b-native-4bit-128g.pt\r\
    \n```\r\nNow go to the models tab in the web UI and load the anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\r\
    \n\r\n"
  created_at: 2023-04-20 05:16:42+00:00
  edited: false
  hidden: false
  id: 6440d8ca8bf0d7756ef8c8f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77c5c3e30d0e77e32dca7812ab373a41.svg
      fullname: Open
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: opensimworld
      type: user
    createdAt: '2023-04-21T16:34:20.000Z'
    data:
      edited: false
      editors:
      - opensimworld
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77c5c3e30d0e77e32dca7812ab373a41.svg
          fullname: Open
          isHf: false
          isPro: false
          name: opensimworld
          type: user
        html: '<p>Thanks! that works. Note that in latest oobabooga seems to have
          the server.py call is  in webui.py as run_cmd("python server.py ...</p>

          '
        raw: Thanks! that works. Note that in latest oobabooga seems to have the server.py
          call is  in webui.py as run_cmd("python server.py ...
        updatedAt: '2023-04-21T16:34:20.105Z'
      numEdits: 0
      reactions: []
    id: 6442bb0cf8b647fa4f529f60
    type: comment
  author: opensimworld
  content: Thanks! that works. Note that in latest oobabooga seems to have the server.py
    call is  in webui.py as run_cmd("python server.py ...
  created_at: 2023-04-21 15:34:20+00:00
  edited: false
  hidden: false
  id: 6442bb0cf8b647fa4f529f60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-22T20:24:23.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>is it better then vicuna?</p>

          '
        raw: is it better then vicuna?
        updatedAt: '2023-04-22T20:24:23.673Z'
      numEdits: 0
      reactions: []
    id: 64444277c63001ae6355a54e
    type: comment
  author: Ukro
  content: is it better then vicuna?
  created_at: 2023-04-22 19:24:23+00:00
  edited: false
  hidden: false
  id: 64444277c63001ae6355a54e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52e38a6ecde34148ec3a19767cf83571.svg
      fullname: Keren Elience
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shaliy
      type: user
    createdAt: '2023-04-25T02:08:41.000Z'
    data:
      edited: false
      editors:
      - Shaliy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52e38a6ecde34148ec3a19767cf83571.svg
          fullname: Keren Elience
          isHf: false
          isPro: false
          name: Shaliy
          type: user
        html: "<p>I flowed your command, but I still meet some problem, like:</p>\n\
          <pre><code class=\"language-plain\">Traceback (most recent call last):\n\
          File \u201C/home/ecs-user/Chatgpt/server.py\u201D, line 101, in load_model_wrapper\n\
          shared.model, shared.tokenizer = load_model(shared.model_name)\nFile \u201C\
          /home/ecs-user/Chatgpt/modules/models.py\u201D, line 229, in load_model\n\
          tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
          ), clean_up_tokenization_spaces=True)\nFile \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
          , line 1812, in from_pretrained\nreturn cls._from_pretrained(\nFile \u201C\
          /home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
          , line 1975, in _from_pretrained\ntokenizer = cls(*init_inputs, **init_kwargs)\n\
          File \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/models/llama/tokenization_llama.py\u201D\
          , line 96, in init\nself.sp_model.Load(vocab_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
          , line 905, in Load\nreturn self.LoadFromFile(model_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
          , line 310, in LoadFromFile\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())]\n</code></pre>\n<p>Firstly, I simply think it just\
          \ about my <code>transformers</code> version problem, so I changed the version\
          \ <code>4.28.0--&gt;4.29.0</code>, but the problem still exist.<br>T_T</p>\n"
        raw: "I flowed your command, but I still meet some problem, like:\n```plain\
          \ txt\nTraceback (most recent call last):\nFile \u201C/home/ecs-user/Chatgpt/server.py\u201D\
          , line 101, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201C/home/ecs-user/Chatgpt/modules/models.py\u201D, line 229, in\
          \ load_model\ntokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
          ), clean_up_tokenization_spaces=True)\nFile \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
          , line 1812, in from_pretrained\nreturn cls._from_pretrained(\nFile \u201C\
          /home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
          , line 1975, in _from_pretrained\ntokenizer = cls(*init_inputs, **init_kwargs)\n\
          File \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/models/llama/tokenization_llama.py\u201D\
          , line 96, in init\nself.sp_model.Load(vocab_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
          , line 905, in Load\nreturn self.LoadFromFile(model_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
          , line 310, in LoadFromFile\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())]\n```\nFirstly, I simply think it just about my `transformers`\
          \ version problem, so I changed the version `4.28.0-->4.29.0`, but the problem\
          \ still exist. \nT_T"
        updatedAt: '2023-04-25T02:08:41.586Z'
      numEdits: 0
      reactions: []
    id: 64473629177a44e335ea6976
    type: comment
  author: Shaliy
  content: "I flowed your command, but I still meet some problem, like:\n```plain\
    \ txt\nTraceback (most recent call last):\nFile \u201C/home/ecs-user/Chatgpt/server.py\u201D\
    , line 101, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
    File \u201C/home/ecs-user/Chatgpt/modules/models.py\u201D, line 229, in load_model\n\
    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
    ), clean_up_tokenization_spaces=True)\nFile \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
    , line 1812, in from_pretrained\nreturn cls._from_pretrained(\nFile \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/tokenization_utils_base.py\u201D\
    , line 1975, in _from_pretrained\ntokenizer = cls(*init_inputs, **init_kwargs)\n\
    File \u201C/home/ecs-user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers-4.29.0.dev0-py3.10.egg/transformers/models/llama/tokenization_llama.py\u201D\
    , line 96, in init\nself.sp_model.Load(vocab_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
    , line 905, in Load\nreturn self.LoadFromFile(model_file)\nFile \u201C/home/ecs-user/.local/lib/python3.10/site-packages/sentencepiece/init.py\u201D\
    , line 310, in LoadFromFile\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())]\n```\nFirstly, I simply think it just about my `transformers`\
    \ version problem, so I changed the version `4.28.0-->4.29.0`, but the problem\
    \ still exist. \nT_T"
  created_at: 2023-04-25 01:08:41+00:00
  edited: false
  hidden: false
  id: 64473629177a44e335ea6976
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7279c62b09fb1e7cd4296bac71d0618c.svg
      fullname: John-mark Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Revenant-exe
      type: user
    createdAt: '2023-04-27T20:46:57.000Z'
    data:
      edited: false
      editors:
      - Revenant-exe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7279c62b09fb1e7cd4296bac71d0618c.svg
          fullname: John-mark Johnson
          isHf: false
          isPro: false
          name: Revenant-exe
          type: user
        html: "<p>Traceback (most recent call last):<br>File \u201CC:\\Users\\johnm\\\
          AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 658, in _get_config_dict<br>config_dict = cls._dict_from_json_file(resolved_config_file)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File\
          \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 745, in _dict_from_json_file<br>text = reader.read()<br>^^^^^^^^^^^^^<br>File\
          \ \u201C\u201D, line 322, in decode<br>UnicodeDecodeError: \u2018utf-8\u2019\
          \ codec can\u2019t decode byte 0xce in position 4411: invalid continuation\
          \ byte</p>\n<p>During handling of the above exception, another exception\
          \ occurred:</p>\n<p>Traceback (most recent call last):<br>File \u201CC:\\\
          Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 102, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File\
          \ \u201CC:\\Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 71, in load_model<br>shared.model_type\
          \ = find_model_type(model_name)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File \u201C\
          C:\\Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 59, in find_model_type<br>config = AutoConfig.from_pretrained(Path(f\u2019\
          {shared.args.model_dir}/{model_name}'))<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File\
          \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\\
          auto\\configuration_auto.py\u201D, line 916, in from_pretrained<br>config_dict,\
          \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File\
          \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 573, in get_config_dict<br>config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File\
          \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 661, in _get_config_dict<br>raise EnvironmentError(<br>OSError: It\
          \ looks like the config file at \u2018C:\\Users\\johnm\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\models\\anon8231489123_gpt4-x-alpaca-13b-native-4bit-128g\\\
          tokenizer.model\u2019 is not a valid JSON file.</p>\n"
        raw: "Traceback (most recent call last):\nFile \u201CC:\\Users\\johnm\\AppData\\\
          Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\\
          local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 658, in _get_config_dict\nconfig_dict = cls._dict_from_json_file(resolved_config_file)\n\
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \u201CC:\\Users\\johnm\\\
          AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 745, in _dict_from_json_file\ntext = reader.read()\n^^^^^^^^^^^^^\n\
          File \u201C\u201D, line 322, in decode\nUnicodeDecodeError: \u2018utf-8\u2019\
          \ codec can\u2019t decode byte 0xce in position 4411: invalid continuation\
          \ byte\n\nDuring handling of the above exception, another exception occurred:\n\
          \nTraceback (most recent call last):\nFile \u201CC:\\Users\\johnm\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 102, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \u201CC:\\Users\\johnm\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 71, in load_model\nshared.model_type = find_model_type(model_name)\n\
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \u201CC:\\Users\\johnm\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 59, in find_model_type\nconfig = AutoConfig.from_pretrained(Path(f\u2019\
          {shared.args.model_dir}/{model_name}'))\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\\
          auto\\configuration_auto.py\u201D, line 916, in from_pretrained\nconfig_dict,\
          \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 573, in get_config_dict\nconfig_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
          , line 661, in _get_config_dict\nraise EnvironmentError(\nOSError: It looks\
          \ like the config file at \u2018C:\\Users\\johnm\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\models\\anon8231489123_gpt4-x-alpaca-13b-native-4bit-128g\\\
          tokenizer.model\u2019 is not a valid JSON file."
        updatedAt: '2023-04-27T20:46:57.335Z'
      numEdits: 0
      reactions: []
    id: 644adf41f9f1b0cd3d90ce8a
    type: comment
  author: Revenant-exe
  content: "Traceback (most recent call last):\nFile \u201CC:\\Users\\johnm\\AppData\\\
    Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\\
    local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
    , line 658, in _get_config_dict\nconfig_dict = cls._dict_from_json_file(resolved_config_file)\n\
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \u201CC:\\Users\\johnm\\\
    AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
    , line 745, in _dict_from_json_file\ntext = reader.read()\n^^^^^^^^^^^^^\nFile\
    \ \u201C\u201D, line 322, in decode\nUnicodeDecodeError: \u2018utf-8\u2019 codec\
    \ can\u2019t decode byte 0xce in position 4411: invalid continuation byte\n\n\
    During handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\nFile \u201CC:\\Users\\johnm\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\server.py\u201D, line 102, in load_model_wrapper\n\
    shared.model, shared.tokenizer = load_model(shared.model_name)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \u201CC:\\Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 71, in load_model\nshared.model_type\
    \ = find_model_type(model_name)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \u201CC:\\\
    Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 59, in find_model_type\nconfig = AutoConfig.from_pretrained(Path(f\u2019\
    {shared.args.model_dir}/{model_name}'))\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\\
    configuration_auto.py\u201D, line 916, in from_pretrained\nconfig_dict, unused_kwargs\
    \ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile\
    \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
    , line 573, in get_config_dict\nconfig_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile\
    \ \u201CC:\\Users\\johnm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\u201D\
    , line 661, in _get_config_dict\nraise EnvironmentError(\nOSError: It looks like\
    \ the config file at \u2018C:\\Users\\johnm\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\models\\anon8231489123_gpt4-x-alpaca-13b-native-4bit-128g\\\
    tokenizer.model\u2019 is not a valid JSON file."
  created_at: 2023-04-27 19:46:57+00:00
  edited: false
  hidden: false
  id: 644adf41f9f1b0cd3d90ce8a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: How I got this to run with oobabooga/ text-generation-webui
