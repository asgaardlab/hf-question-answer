!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mm04926412
conflicting_files: null
created_at: 2023-04-10 15:46:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
      fullname: Michael Moran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mm04926412
      type: user
    createdAt: '2023-04-10T16:46:29.000Z'
    data:
      edited: false
      editors:
      - mm04926412
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
          fullname: Michael Moran
          isHf: false
          isPro: false
          name: mm04926412
          type: user
        html: '<p>I''ve installed Ooga Booga and I''m trying to run the model from
          the cuda weights but encounter the following error when starting the web
          ui</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\mm049\Desktop\ooga
          booga\text-generation-webui\server.py", line 302, in <br>    shared.model,
          shared.tokenizer = load_model(shared.model_name)<br>  File "C:\Users\mm049\Desktop\ooga
          booga\text-generation-webui\modules\models.py", line 181, in load_model<br>    tokenizer
          = LlamaTokenizer.from_pretrained(Path(f"{shared.args.model_dir}/{shared.model_name}/"),
          clean_up_tokenization_spaces=True)<br>  File "C:\Users\mm049\Desktop\ooga
          booga\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1811, in from_pretrained<br>    return cls.<em>from_pretrained(<br>  File
          "C:\Users\mm049\Desktop\ooga booga\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1965, in <em>from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>  File
          "C:\Users\mm049\Desktop\ooga booga\installer_files\env\lib\site-packages\transformers\models\llama\tokenization_llama.py",
          line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>  File
          "C:\Users\mm049\Desktop\ooga booga\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 905, in Load<br>    return self.LoadFromFile(model_file)<br>  File
          "C:\Users\mm049\Desktop\ooga booga\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)<br>RuntimeError: Internal: D:\a\sentencepiece\sentencepiece\src\sentencepiece_processor.cc(1102)
          [model_proto-&gt;ParseFromArray(serialized.data(), serialized.size())]</p>

          <p>Does anyone have any insight on what might be causing this. I have confirmed
          that the checksum of my model weights matches the repository.</p>

          '
        raw: "I've installed Ooga Booga and I'm trying to run the model from the cuda\
          \ weights but encounter the following error when starting the web ui\r\n\
          \r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mm049\\Desktop\\\
          ooga booga\\text-generation-webui\\server.py\", line 302, in <module>\r\n\
          \    shared.model, shared.tokenizer = load_model(shared.model_name)\r\n\
          \  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\text-generation-webui\\\
          modules\\models.py\", line 181, in load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{shared.model_name}/\"), clean_up_tokenization_spaces=True)\r\
          \n  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\\
          lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1811,\
          \ in from_pretrained\r\n    return cls._from_pretrained(\r\n  File \"C:\\\
          Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\", line 1965, in _from_pretrained\r\
          \n    tokenizer = cls(*init_inputs, **init_kwargs)\r\n  File \"C:\\Users\\\
          mm049\\Desktop\\ooga booga\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\tokenization_llama.py\", line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\
          \n  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\\
          lib\\site-packages\\sentencepiece\\__init__.py\", line 905, in Load\r\n\
          \    return self.LoadFromFile(model_file)\r\n  File \"C:\\Users\\mm049\\\
          Desktop\\ooga booga\\installer_files\\env\\lib\\site-packages\\sentencepiece\\\
          __init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: D:\\a\\sentencepiece\\sentencepiece\\\
          src\\sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())]\r\n\r\nDoes anyone have any insight on what might\
          \ be causing this. I have confirmed that the checksum of my model weights\
          \ matches the repository."
        updatedAt: '2023-04-10T16:46:29.828Z'
      numEdits: 0
      reactions: []
    id: 64343d654b34368fdb0346e2
    type: comment
  author: mm04926412
  content: "I've installed Ooga Booga and I'm trying to run the model from the cuda\
    \ weights but encounter the following error when starting the web ui\r\n\r\nTraceback\
    \ (most recent call last):\r\n  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\\
    text-generation-webui\\server.py\", line 302, in <module>\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\Users\\mm049\\\
    Desktop\\ooga booga\\text-generation-webui\\modules\\models.py\", line 181, in\
    \ load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{shared.model_name}/\"\
    ), clean_up_tokenization_spaces=True)\r\n  File \"C:\\Users\\mm049\\Desktop\\\
    ooga booga\\installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"C:\\Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py\", line 1965, in _from_pretrained\r\n\
    \    tokenizer = cls(*init_inputs, **init_kwargs)\r\n  File \"C:\\Users\\mm049\\\
    Desktop\\ooga booga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\tokenization_llama.py\", line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\
    \n  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\lib\\\
    site-packages\\sentencepiece\\__init__.py\", line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\
    \n  File \"C:\\Users\\mm049\\Desktop\\ooga booga\\installer_files\\env\\lib\\\
    site-packages\\sentencepiece\\__init__.py\", line 310, in LoadFromFile\r\n   \
    \ return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\r\nRuntimeError:\
    \ Internal: D:\\a\\sentencepiece\\sentencepiece\\src\\sentencepiece_processor.cc(1102)\
    \ [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n\r\n\
    Does anyone have any insight on what might be causing this. I have confirmed that\
    \ the checksum of my model weights matches the repository."
  created_at: 2023-04-10 15:46:29+00:00
  edited: false
  hidden: false
  id: 64343d654b34368fdb0346e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jXu3FXeFEVqVPOheY2EBw.png?w=200&h=200&f=face
      fullname: Dustin Raydon Farmer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Radon088
      type: user
    createdAt: '2023-04-10T23:52:45.000Z'
    data:
      edited: false
      editors:
      - Radon088
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jXu3FXeFEVqVPOheY2EBw.png?w=200&h=200&f=face
          fullname: Dustin Raydon Farmer
          isHf: false
          isPro: false
          name: Radon088
          type: user
        html: '<p>I have the same problem. Hopefully the creators will figure out
          the incompatibility with Ooga Booga soon.</p>

          '
        raw: I have the same problem. Hopefully the creators will figure out the incompatibility
          with Ooga Booga soon.
        updatedAt: '2023-04-10T23:52:45.820Z'
      numEdits: 0
      reactions: []
    id: 6434a14d5408e9c12aff1465
    type: comment
  author: Radon088
  content: I have the same problem. Hopefully the creators will figure out the incompatibility
    with Ooga Booga soon.
  created_at: 2023-04-10 22:52:45+00:00
  edited: false
  hidden: false
  id: 6434a14d5408e9c12aff1465
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd4a5347fbc36e3b7a8c15a3363d24a0.svg
      fullname: M Eltahir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShifraSec
      type: user
    createdAt: '2023-04-11T01:35:44.000Z'
    data:
      edited: false
      editors:
      - ShifraSec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd4a5347fbc36e3b7a8c15a3363d24a0.svg
          fullname: M Eltahir
          isHf: false
          isPro: false
          name: ShifraSec
          type: user
        html: '<p>I''m also having the same issuing while using transformers straight
          in python REPL or in Code, this is my <a href="https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/23#6434b911546e16f17a17f9b3">issue</a></p>

          '
        raw: I'm also having the same issuing while using transformers straight in
          python REPL or in Code, this is my [issue](https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/23#6434b911546e16f17a17f9b3)
        updatedAt: '2023-04-11T01:35:44.503Z'
      numEdits: 0
      reactions: []
    id: 6434b970938d07505bbb020d
    type: comment
  author: ShifraSec
  content: I'm also having the same issuing while using transformers straight in python
    REPL or in Code, this is my [issue](https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/23#6434b911546e16f17a17f9b3)
  created_at: 2023-04-11 00:35:44+00:00
  edited: false
  hidden: false
  id: 6434b970938d07505bbb020d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
      fullname: Michael Moran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mm04926412
      type: user
    createdAt: '2023-04-11T17:49:44.000Z'
    data:
      edited: false
      editors:
      - mm04926412
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/195f19da7c65e3a6cb2acbe3b3bd38e3.svg
          fullname: Michael Moran
          isHf: false
          isPro: false
          name: mm04926412
          type: user
        html: '<p>I solved this problem on my machine, for some reason the tokenizor
          is stored using github LFS despite being less than a megabyte, you likely
          have a 1kb file pointer instead of the real tokenizor</p>

          '
        raw: I solved this problem on my machine, for some reason the tokenizor is
          stored using github LFS despite being less than a megabyte, you likely have
          a 1kb file pointer instead of the real tokenizor
        updatedAt: '2023-04-11T17:49:44.939Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ShifraSec
    id: 64359db857c3f4b161f5b9c6
    type: comment
  author: mm04926412
  content: I solved this problem on my machine, for some reason the tokenizor is stored
    using github LFS despite being less than a megabyte, you likely have a 1kb file
    pointer instead of the real tokenizor
  created_at: 2023-04-11 16:49:44+00:00
  edited: false
  hidden: false
  id: 64359db857c3f4b161f5b9c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0874c4e856ded87ae7ee1327cf1e269a.svg
      fullname: 'Shane '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neoszhane
      type: user
    createdAt: '2023-04-16T04:03:03.000Z'
    data:
      edited: false
      editors:
      - neoszhane
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0874c4e856ded87ae7ee1327cf1e269a.svg
          fullname: 'Shane '
          isHf: false
          isPro: false
          name: neoszhane
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mm04926412&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mm04926412\">@<span class=\"\
          underline\">mm04926412</span></a></span>\n\n\t</span></span> How do you\
          \ get the real tokenizor?</p>\n"
        raw: '@mm04926412 How do you get the real tokenizor?'
        updatedAt: '2023-04-16T04:03:03.980Z'
      numEdits: 0
      reactions: []
    id: 643b73775a3b913ad46fb4cc
    type: comment
  author: neoszhane
  content: '@mm04926412 How do you get the real tokenizor?'
  created_at: 2023-04-16 03:03:03+00:00
  edited: false
  hidden: false
  id: 643b73775a3b913ad46fb4cc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
repo_type: model
status: open
target_branch: null
title: Issue with tokenizor using Ooga Booga?
