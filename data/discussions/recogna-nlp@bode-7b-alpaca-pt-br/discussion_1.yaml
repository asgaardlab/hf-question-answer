!!python/object:huggingface_hub.community.DiscussionWithDetails
author: celsowm
conflicting_files: null
created_at: 2023-12-12 00:55:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
      fullname: Celso F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: celsowm
      type: user
    createdAt: '2023-12-12T00:55:15.000Z'
    data:
      edited: false
      editors:
      - celsowm
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.9917694330215454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
          fullname: Celso F
          isHf: false
          isPro: false
          name: celsowm
          type: user
        html: "<p>Ol\xE1 !<br>Primeiramente, parab\xE9ns pelo trabalho de voc\xEA\
          s !</p>\n<p>Estou tentando fazer fine tuning com um data set de perguntas\
          \ e respostas, por\xE9m, n\xE3o sei qual seria o formato correto para usar\
          \ no SFTTrainer, se seria:</p>\n<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n\
          {{ system_prompt }}\n&lt;&lt;/SYS&gt;&gt;\n\n{{ user_message }} [/INST]\n\
          </code></pre>\n<p>ou que est\xE1 no card:</p>\n<p>Abaixo est\xE1 uma instru\xE7\
          \xE3o que descreve uma tarefa. Escreva uma resposta que complete adequadamente\
          \ o pedido.</p>\n<pre><code>### Instru\xE7\xE3o:\n{instruction}\n\n### Resposta:\"\
          \"\"\n</code></pre>\n<p>Qual devo usar?</p>\n"
        raw: "Ol\xE1 !\r\nPrimeiramente, parab\xE9ns pelo trabalho de voc\xEAs !\r\
          \n\r\nEstou tentando fazer fine tuning com um data set de perguntas e respostas,\
          \ por\xE9m, n\xE3o sei qual seria o formato correto para usar no SFTTrainer,\
          \ se seria:\r\n\r\n```\r\n<s>[INST] <<SYS>>\r\n{{ system_prompt }}\r\n<</SYS>>\r\
          \n\r\n{{ user_message }} [/INST]\r\n\r\n```\r\nou que est\xE1 no card:\r\
          \n\r\nAbaixo est\xE1 uma instru\xE7\xE3o que descreve uma tarefa. Escreva\
          \ uma resposta que complete adequadamente o pedido.\r\n\r\n```\r\n### Instru\xE7\
          \xE3o:\r\n{instruction}\r\n\r\n### Resposta:\"\"\"\r\n\r\n```\r\n\r\nQual\
          \ devo usar?"
        updatedAt: '2023-12-12T00:55:15.131Z'
      numEdits: 0
      reactions: []
    id: 6577af7317166d821efb372a
    type: comment
  author: celsowm
  content: "Ol\xE1 !\r\nPrimeiramente, parab\xE9ns pelo trabalho de voc\xEAs !\r\n\
    \r\nEstou tentando fazer fine tuning com um data set de perguntas e respostas,\
    \ por\xE9m, n\xE3o sei qual seria o formato correto para usar no SFTTrainer, se\
    \ seria:\r\n\r\n```\r\n<s>[INST] <<SYS>>\r\n{{ system_prompt }}\r\n<</SYS>>\r\n\
    \r\n{{ user_message }} [/INST]\r\n\r\n```\r\nou que est\xE1 no card:\r\n\r\nAbaixo\
    \ est\xE1 uma instru\xE7\xE3o que descreve uma tarefa. Escreva uma resposta que\
    \ complete adequadamente o pedido.\r\n\r\n```\r\n### Instru\xE7\xE3o:\r\n{instruction}\r\
    \n\r\n### Resposta:\"\"\"\r\n\r\n```\r\n\r\nQual devo usar?"
  created_at: 2023-12-12 00:55:15+00:00
  edited: false
  hidden: false
  id: 6577af7317166d821efb372a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
      fullname: Celso F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: celsowm
      type: user
    createdAt: '2023-12-12T00:55:33.000Z'
    data:
      from: "Qual \xE9 o formato para fazer fine tuning deste modelo?"
      to: "Qual \xE9 o formato para fazer fine tuning neste modelo?"
    id: 6577af856014c0727c490ddb
    type: title-change
  author: celsowm
  created_at: 2023-12-12 00:55:33+00:00
  id: 6577af856014c0727c490ddb
  new_title: "Qual \xE9 o formato para fazer fine tuning neste modelo?"
  old_title: "Qual \xE9 o formato para fazer fine tuning deste modelo?"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/g71ytVgieuFuK0aq7GfvI.jpeg?w=200&h=200&f=face
      fullname: Recogna Laboratory
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: recogna
      type: user
    createdAt: '2023-12-13T16:11:06.000Z'
    data:
      edited: false
      editors:
      - recogna
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.9958497285842896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/g71ytVgieuFuK0aq7GfvI.jpeg?w=200&h=200&f=face
          fullname: Recogna Laboratory
          isHf: false
          isPro: false
          name: recogna
          type: user
        html: "<p>Ol\xE1 Celso, tudo bem?</p>\n<p>Muito obrigado pela participa\xE7\
          \xE3o! </p>\n<p>Em rela\xE7\xE3o ao fine tuning que utilizamos para desenvolver\
          \ o Bode, utilizamos a Adapta\xE7\xE3o de Baixo Rank (LoRA) que \xE9 uma\
          \ t\xE9cnica  para facilitar a adapta\xE7\xE3o de LLMs  pr\xE9-treinadas\
          \ para novas tarefas. A LoRA prop\xF5e congelar os pesos do modelo pr\xE9\
          -treinado e injetar camadas trein\xE1veis  (matrizes de decomposi\xE7\xE3\
          o de classifica\xE7\xE3o) em cada bloco de transformadores. Isso reduz muito\
          \ o n\xFAmero de par\xE2metros trein\xE1veis e os requisitos de mem\xF3\
          ria da GPU, uma vez que os gradientes n\xE3o precisam ser computados para\
          \ a maioria dos pesos do modelo. </p>\n<p>No que diz respeito ao uso do\
          \ SFTTrainer, seria necess\xE1rio realizar uma an\xE1lise para verificar\
          \ se h\xE1 alguma normatiza\xE7\xE3o espec\xEDfica de treinamento ou a presen\xE7\
          a de tokens especiais. No entanto, caso n\xE3o haja uma padroniza\xE7\xE3\
          o definida, \xE9 poss\xEDvel adotar qualquer formato desejado, seja aquele\
          \ estabelecido na LLM Bode ou qualquer outro padr\xE3o de prefer\xEAncia.</p>\n\
          <p>Agradecemos imensamente pela sua participa\xE7\xE3o e colocamo-nos \xE0\
          \ disposi\xE7\xE3o para quaisquer esclarecimentos adicionais.</p>\n"
        raw: "Ol\xE1 Celso, tudo bem?\n\nMuito obrigado pela participa\xE7\xE3o! \n\
          \nEm rela\xE7\xE3o ao fine tuning que utilizamos para desenvolver o Bode,\
          \ utilizamos a Adapta\xE7\xE3o de Baixo Rank (LoRA) que \xE9 uma t\xE9cnica\
          \  para facilitar a adapta\xE7\xE3o de LLMs  pr\xE9-treinadas para novas\
          \ tarefas. A LoRA prop\xF5e congelar os pesos do modelo pr\xE9-treinado\
          \ e injetar camadas trein\xE1veis  (matrizes de decomposi\xE7\xE3o de classifica\xE7\
          \xE3o) em cada bloco de transformadores. Isso reduz muito o n\xFAmero de\
          \ par\xE2metros trein\xE1veis e os requisitos de mem\xF3ria da GPU, uma\
          \ vez que os gradientes n\xE3o precisam ser computados para a maioria dos\
          \ pesos do modelo. \n\nNo que diz respeito ao uso do SFTTrainer, seria necess\xE1\
          rio realizar uma an\xE1lise para verificar se h\xE1 alguma normatiza\xE7\
          \xE3o espec\xEDfica de treinamento ou a presen\xE7a de tokens especiais.\
          \ No entanto, caso n\xE3o haja uma padroniza\xE7\xE3o definida, \xE9 poss\xED\
          vel adotar qualquer formato desejado, seja aquele estabelecido na LLM Bode\
          \ ou qualquer outro padr\xE3o de prefer\xEAncia.\n\nAgradecemos imensamente\
          \ pela sua participa\xE7\xE3o e colocamo-nos \xE0 disposi\xE7\xE3o para\
          \ quaisquer esclarecimentos adicionais."
        updatedAt: '2023-12-13T16:11:06.526Z'
      numEdits: 0
      reactions: []
    id: 6579d79a1e0e383c6f9bd1a9
    type: comment
  author: recogna
  content: "Ol\xE1 Celso, tudo bem?\n\nMuito obrigado pela participa\xE7\xE3o! \n\n\
    Em rela\xE7\xE3o ao fine tuning que utilizamos para desenvolver o Bode, utilizamos\
    \ a Adapta\xE7\xE3o de Baixo Rank (LoRA) que \xE9 uma t\xE9cnica  para facilitar\
    \ a adapta\xE7\xE3o de LLMs  pr\xE9-treinadas para novas tarefas. A LoRA prop\xF5\
    e congelar os pesos do modelo pr\xE9-treinado e injetar camadas trein\xE1veis\
    \  (matrizes de decomposi\xE7\xE3o de classifica\xE7\xE3o) em cada bloco de transformadores.\
    \ Isso reduz muito o n\xFAmero de par\xE2metros trein\xE1veis e os requisitos\
    \ de mem\xF3ria da GPU, uma vez que os gradientes n\xE3o precisam ser computados\
    \ para a maioria dos pesos do modelo. \n\nNo que diz respeito ao uso do SFTTrainer,\
    \ seria necess\xE1rio realizar uma an\xE1lise para verificar se h\xE1 alguma normatiza\xE7\
    \xE3o espec\xEDfica de treinamento ou a presen\xE7a de tokens especiais. No entanto,\
    \ caso n\xE3o haja uma padroniza\xE7\xE3o definida, \xE9 poss\xEDvel adotar qualquer\
    \ formato desejado, seja aquele estabelecido na LLM Bode ou qualquer outro padr\xE3\
    o de prefer\xEAncia.\n\nAgradecemos imensamente pela sua participa\xE7\xE3o e\
    \ colocamo-nos \xE0 disposi\xE7\xE3o para quaisquer esclarecimentos adicionais."
  created_at: 2023-12-13 16:11:06+00:00
  edited: false
  hidden: false
  id: 6579d79a1e0e383c6f9bd1a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
      fullname: Celso F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: celsowm
      type: user
    createdAt: '2023-12-15T01:08:11.000Z'
    data:
      edited: false
      editors:
      - celsowm
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.29843971133232117
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
          fullname: Celso F
          isHf: false
          isPro: false
          name: celsowm
          type: user
        html: "<p>Ol\xE1 <span data-props=\"{&quot;user&quot;:&quot;recogna&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/recogna\"\
          >@<span class=\"underline\">recogna</span></a></span>\n\n\t</span></span>\
          \ , boa noite !<br>Obrigado pela resposta ! Primeira coisa, descobri que\
          \ o exemplo do card de voc\xEAs pode funcionar de maneira mais simples usando\
          \ o pipeline, olha:</p>\n<p>from transformers import pipeline, AutoTokenizer,\
          \ AutoModelForCausalLM<br>from peft import PeftModel, PeftConfig</p>\n<pre><code>llm_model\
          \ = 'recogna-nlp/bode-7b-alpaca-pt-br'\n\nconfig = PeftConfig.from_pretrained(llm_model)\n\
          model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\
          \ trust_remote_code=True, return_dict=True, load_in_8bit=True, device_map='auto')\n\
          tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
          model = PeftModel.from_pretrained(model, llm_model)\n\npipe = pipeline(\"\
          conversational\", model=model, tokenizer=tokenizer)\n\nmessages = [\n  \
          \  {\n        \"role\": \"system\",\n        \"content\": \"Voc\xEA \xE9\
          \ um chatbot amig\xE1vel que sempre responde em portugu\xEAs\",\n    },\n\
          \    {\"role\": \"user\", \"content\": \"O que \xE9 um bode?\"},\n]\n\n\
          result = pipe(messages)\nprint(result.messages[-1]['content'])\n</code></pre>\n\
          <p>E sobre o template, se a gente rodar:</p>\n<pre><code>print(tokenizer.default_chat_template)\n\
          </code></pre>\n<p>volta isso:</p>\n<pre><code>{% if messages[0]['role']\
          \ == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message\
          \ = messages[0]['content'] %}{% elif false == true and not '&lt;&lt;SYS&gt;&gt;'\
          \ in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message\
          \ = 'You are a helpful, respectful and honest assistant. Always answer as\
          \ helpfully as possible, while being safe. Your answers should not include\
          \ any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\
          \ Please ensure that your responses are socially unbiased and positive in\
          \ nature.\\n\\nIf a question does not make any sense, or is not factually\
          \ coherent, explain why instead of answering something not correct. If you\
          \ don\\'t know the answer to a question, please don\\'t share false information.'\
          \ %}{% else %}{% set loop_messages = messages %}{% set system_message =\
          \ false %}{% endif %}{% for message in loop_messages %}{% if (message['role']\
          \ == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation\
          \ roles must alternate user/assistant/user/assistant/...') }}{% endif %}{%\
          \ if loop.index0 == 0 and system_message != false %}{% set content = '&lt;&lt;SYS&gt;&gt;\\\
          n' + system_message + '\\n&lt;&lt;/SYS&gt;&gt;\\n\\n' + message['content']\
          \ %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role']\
          \ == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{%\
          \ elif message['role'] == 'system' %}{{ '&lt;&lt;SYS&gt;&gt;\\n' + content.strip()\
          \ + '\\n&lt;&lt;/SYS&gt;&gt;\\n\\n' }}{% elif message['role'] == 'assistant'\
          \ %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n\
          </code></pre>\n<p>Fico pensando que seria at\xE9 uma boa, se poss\xEDvel\
          \ \xE9 claro, atualizar esse chat template no tokenize de voc\xEAs na pr\xF3\
          xima atualiza\xE7\xE3o, traduzindo essas partes em ingl\xEAs</p>\n"
        raw: "Ol\xE1 @recogna , boa noite !\nObrigado pela resposta ! Primeira coisa,\
          \ descobri que o exemplo do card de voc\xEAs pode funcionar de maneira mais\
          \ simples usando o pipeline, olha:\n\nfrom transformers import pipeline,\
          \ AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n\
          \n```\nllm_model = 'recogna-nlp/bode-7b-alpaca-pt-br'\n\nconfig = PeftConfig.from_pretrained(llm_model)\n\
          model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\
          \ trust_remote_code=True, return_dict=True, load_in_8bit=True, device_map='auto')\n\
          tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
          model = PeftModel.from_pretrained(model, llm_model)\n\npipe = pipeline(\"\
          conversational\", model=model, tokenizer=tokenizer)\n\nmessages = [\n  \
          \  {\n        \"role\": \"system\",\n        \"content\": \"Voc\xEA \xE9\
          \ um chatbot amig\xE1vel que sempre responde em portugu\xEAs\",\n    },\n\
          \    {\"role\": \"user\", \"content\": \"O que \xE9 um bode?\"},\n]\n\n\
          result = pipe(messages)\nprint(result.messages[-1]['content'])\n\n```\n\n\
          E sobre o template, se a gente rodar:\n\n```\nprint(tokenizer.default_chat_template)\n\
          ```\n\nvolta isso:\n\n```\n{% if messages[0]['role'] == 'system' %}{% set\
          \ loop_messages = messages[1:] %}{% set system_message = messages[0]['content']\
          \ %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{%\
          \ set loop_messages = messages %}{% set system_message = 'You are a helpful,\
          \ respectful and honest assistant. Always answer as helpfully as possible,\
          \ while being safe. Your answers should not include any harmful, unethical,\
          \ racist, sexist, toxic, dangerous, or illegal content. Please ensure that\
          \ your responses are socially unbiased and positive in nature.\\n\\nIf a\
          \ question does not make any sense, or is not factually coherent, explain\
          \ why instead of answering something not correct. If you don\\'t know the\
          \ answer to a question, please don\\'t share false information.' %}{% else\
          \ %}{% set loop_messages = messages %}{% set system_message = false %}{%\
          \ endif %}{% for message in loop_messages %}{% if (message['role'] == 'user')\
          \ != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must\
          \ alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0\
          \ == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message\
          \ + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content\
          \ = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{\
          \ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role']\
          \ == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n'\
          \ }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() +\
          \ ' ' + eos_token }}{% endif %}{% endfor %}\n```\n\nFico pensando que seria\
          \ at\xE9 uma boa, se poss\xEDvel \xE9 claro, atualizar esse chat template\
          \ no tokenize de voc\xEAs na pr\xF3xima atualiza\xE7\xE3o, traduzindo essas\
          \ partes em ingl\xEAs\n\n"
        updatedAt: '2023-12-15T01:08:11.753Z'
      numEdits: 0
      reactions: []
    id: 657ba6fb9f62ed61a20de0db
    type: comment
  author: celsowm
  content: "Ol\xE1 @recogna , boa noite !\nObrigado pela resposta ! Primeira coisa,\
    \ descobri que o exemplo do card de voc\xEAs pode funcionar de maneira mais simples\
    \ usando o pipeline, olha:\n\nfrom transformers import pipeline, AutoTokenizer,\
    \ AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n\n```\nllm_model\
    \ = 'recogna-nlp/bode-7b-alpaca-pt-br'\n\nconfig = PeftConfig.from_pretrained(llm_model)\n\
    model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True,\
    \ return_dict=True, load_in_8bit=True, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
    model = PeftModel.from_pretrained(model, llm_model)\n\npipe = pipeline(\"conversational\"\
    , model=model, tokenizer=tokenizer)\n\nmessages = [\n    {\n        \"role\":\
    \ \"system\",\n        \"content\": \"Voc\xEA \xE9 um chatbot amig\xE1vel que\
    \ sempre responde em portugu\xEAs\",\n    },\n    {\"role\": \"user\", \"content\"\
    : \"O que \xE9 um bode?\"},\n]\n\nresult = pipe(messages)\nprint(result.messages[-1]['content'])\n\
    \n```\n\nE sobre o template, se a gente rodar:\n\n```\nprint(tokenizer.default_chat_template)\n\
    ```\n\nvolta isso:\n\n```\n{% if messages[0]['role'] == 'system' %}{% set loop_messages\
    \ = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false\
    \ == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages =\
    \ messages %}{% set system_message = 'You are a helpful, respectful and honest\
    \ assistant. Always answer as helpfully as possible, while being safe. Your answers\
    \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
    \ or illegal content. Please ensure that your responses are socially unbiased\
    \ and positive in nature.\\n\\nIf a question does not make any sense, or is not\
    \ factually coherent, explain why instead of answering something not correct.\
    \ If you don\\'t know the answer to a question, please don\\'t share false information.'\
    \ %}{% else %}{% set loop_messages = messages %}{% set system_message = false\
    \ %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user')\
    \ != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate\
    \ user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and\
    \ system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\\
    n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content']\
    \ %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip()\
    \ + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip()\
    \ + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip()\
    \ + ' ' + eos_token }}{% endif %}{% endfor %}\n```\n\nFico pensando que seria\
    \ at\xE9 uma boa, se poss\xEDvel \xE9 claro, atualizar esse chat template no tokenize\
    \ de voc\xEAs na pr\xF3xima atualiza\xE7\xE3o, traduzindo essas partes em ingl\xEA\
    s\n\n"
  created_at: 2023-12-15 01:08:11+00:00
  edited: false
  hidden: false
  id: 657ba6fb9f62ed61a20de0db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/g71ytVgieuFuK0aq7GfvI.jpeg?w=200&h=200&f=face
      fullname: Recogna Laboratory
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: recogna
      type: user
    createdAt: '2023-12-27T17:35:29.000Z'
    data:
      edited: false
      editors:
      - recogna
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.9966973066329956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/g71ytVgieuFuK0aq7GfvI.jpeg?w=200&h=200&f=face
          fullname: Recogna Laboratory
          isHf: false
          isPro: false
          name: recogna
          type: user
        html: "<p>Ol\xE1 <span data-props=\"{&quot;user&quot;:&quot;celsowm&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/celsowm\"\
          >@<span class=\"underline\">celsowm</span></a></span>\n\n\t</span></span>,\
          \ tudo bem? </p>\n<p>Agradecemos pela sua participa\xE7\xE3o e ficamos contentes\
          \ com o seu envolvimento. Optamos por incorporar o c\xF3digo de exemplo\
          \ com base no Alpaca, acreditando que isso facilitaria a compreens\xE3o\
          \ para usu\xE1rios familiarizados com o Alpaca/LLM. Contudo, sua sugest\xE3\
          o \xE9 extremamente valiosa, e apreciamos seu feedback. Na vers\xE3o inicial,\
          \ nosso foco foi disponibilizar o modelo para a comunidade de Processamento\
          \ de Linguagem Natural (NLP), uma vez que encontrar um modelo puramente\
          \ em portugu\xEAs e gratuito n\xE3o era uma tarefa simples. No entanto,\
          \ estamos empenhados em aprimorar o Bode em futuras vers\xF5es, levando\
          \ em considera\xE7\xE3o os detalhes que voc\xEA destacou.</p>\n<p>Agradecemos\
          \ novamente pela sua colabora\xE7\xE3o. Esperamos que o Bode seja \xFAtil\
          \ em suas pesquisas ou trabalhos. Fique atento, pois em breve lan\xE7aremos\
          \ novas vers\xF5es do Bode!</p>\n<p>Atenciosamente,</p>\n<p>Recogna NLP.</p>\n"
        raw: "Ol\xE1 @celsowm, tudo bem? \n\nAgradecemos pela sua participa\xE7\xE3\
          o e ficamos contentes com o seu envolvimento. Optamos por incorporar o c\xF3\
          digo de exemplo com base no Alpaca, acreditando que isso facilitaria a compreens\xE3\
          o para usu\xE1rios familiarizados com o Alpaca/LLM. Contudo, sua sugest\xE3\
          o \xE9 extremamente valiosa, e apreciamos seu feedback. Na vers\xE3o inicial,\
          \ nosso foco foi disponibilizar o modelo para a comunidade de Processamento\
          \ de Linguagem Natural (NLP), uma vez que encontrar um modelo puramente\
          \ em portugu\xEAs e gratuito n\xE3o era uma tarefa simples. No entanto,\
          \ estamos empenhados em aprimorar o Bode em futuras vers\xF5es, levando\
          \ em considera\xE7\xE3o os detalhes que voc\xEA destacou.\n\nAgradecemos\
          \ novamente pela sua colabora\xE7\xE3o. Esperamos que o Bode seja \xFAtil\
          \ em suas pesquisas ou trabalhos. Fique atento, pois em breve lan\xE7aremos\
          \ novas vers\xF5es do Bode!\n\nAtenciosamente,\n\nRecogna NLP."
        updatedAt: '2023-12-27T17:35:29.071Z'
      numEdits: 0
      reactions: []
    id: 658c60619440d69c8b586bf7
    type: comment
  author: recogna
  content: "Ol\xE1 @celsowm, tudo bem? \n\nAgradecemos pela sua participa\xE7\xE3\
    o e ficamos contentes com o seu envolvimento. Optamos por incorporar o c\xF3digo\
    \ de exemplo com base no Alpaca, acreditando que isso facilitaria a compreens\xE3\
    o para usu\xE1rios familiarizados com o Alpaca/LLM. Contudo, sua sugest\xE3o \xE9\
    \ extremamente valiosa, e apreciamos seu feedback. Na vers\xE3o inicial, nosso\
    \ foco foi disponibilizar o modelo para a comunidade de Processamento de Linguagem\
    \ Natural (NLP), uma vez que encontrar um modelo puramente em portugu\xEAs e gratuito\
    \ n\xE3o era uma tarefa simples. No entanto, estamos empenhados em aprimorar o\
    \ Bode em futuras vers\xF5es, levando em considera\xE7\xE3o os detalhes que voc\xEA\
    \ destacou.\n\nAgradecemos novamente pela sua colabora\xE7\xE3o. Esperamos que\
    \ o Bode seja \xFAtil em suas pesquisas ou trabalhos. Fique atento, pois em breve\
    \ lan\xE7aremos novas vers\xF5es do Bode!\n\nAtenciosamente,\n\nRecogna NLP."
  created_at: 2023-12-27 17:35:29+00:00
  edited: false
  hidden: false
  id: 658c60619440d69c8b586bf7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: recogna-nlp/bode-7b-alpaca-pt-br
repo_type: model
status: open
target_branch: null
title: "Qual \xE9 o formato para fazer fine tuning neste modelo?"
