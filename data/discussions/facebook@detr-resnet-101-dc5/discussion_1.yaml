!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhyatt000
conflicting_files: null
created_at: 2022-09-01 13:51:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
      fullname: Matt Hyatt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhyatt000
      type: user
    createdAt: '2022-09-01T14:51:38.000Z'
    data:
      edited: false
      editors:
      - mhyatt000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
          fullname: Matt Hyatt
          isHf: false
          isPro: false
          name: mhyatt000
          type: user
        html: "<p>I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?</p>\n\
          <ul>\n<li>Claimed mAP: 44.9</li>\n<li>Recieved mAP: 42.2</li>\n</ul>\n<p>Here\
          \ are the details for my validation:</p>\n<ul>\n<li>I instantiate pre-trained\
          \ model with <code>transformers.pipeline()</code> and use COCO API to calculate\
          \ AP from detection bboxes. </li>\n<li>Evaluation was performed on macOS\
          \ CPU.</li>\n<li>Dataset was downloaded from cocodataset.org</li>\n</ul>\n\
          <hr>\n<pre><code> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.422\n Average Precision  (AP) @[ IoU=0.50      | area=\
          \   all | maxDets=100 ] = 0.613\n Average Precision  (AP) @[ IoU=0.75  \
          \    | area=   all | maxDets=100 ] = 0.449\n Average Precision  (AP) @[\
          \ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.212\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.595\n\
          \ Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ]\
          \ = 0.332\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
          \ 10 ] = 0.496\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.508\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \ small | maxDets=100 ] = 0.270\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.561\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.694\n</code></pre>\n"
        raw: "I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?\r\
          \n\r\n- Claimed mAP: 44.9\r\n- Recieved mAP: 42.2\r\n\r\nHere are the details\
          \ for my validation:\r\n\r\n- I instantiate pre-trained model with `transformers.pipeline()`\
          \ and use COCO API to calculate AP from detection bboxes. \r\n- Evaluation\
          \ was performed on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\
          \n---\r\n\r\n```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=  \
          \ all | maxDets=100 ] = 0.422\r\n Average Precision  (AP) @[ IoU=0.50  \
          \    | area=   all | maxDets=100 ] = 0.613\r\n Average Precision  (AP) @[\
          \ IoU=0.75      | area=   all | maxDets=100 ] = 0.449\r\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.212\r\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465\r\
          \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
          \ ] = 0.595\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all |\
          \ maxDets=  1 ] = 0.332\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \   all | maxDets= 10 ] = 0.496\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=   all | maxDets=100 ] = 0.508\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= small | maxDets=100 ] = 0.270\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.561\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.694\r\n```"
        updatedAt: '2022-09-01T14:51:38.076Z'
      numEdits: 0
      reactions: []
    id: 6310c6fa61cab0446e4aa4e7
    type: comment
  author: mhyatt000
  content: "I tried to reproduce the results mentioned on this model card. Seems like\
    \ my results do not match the claimed mAP in the model card. I cannot figure out\
    \ how to get the correct numbers, can you help me find my mistake?\r\n\r\n- Claimed\
    \ mAP: 44.9\r\n- Recieved mAP: 42.2\r\n\r\nHere are the details for my validation:\r\
    \n\r\n- I instantiate pre-trained model with `transformers.pipeline()` and use\
    \ COCO API to calculate AP from detection bboxes. \r\n- Evaluation was performed\
    \ on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\n---\r\n\r\n\
    ```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]\
    \ = 0.422\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100\
    \ ] = 0.613\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100\
    \ ] = 0.449\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.212\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.465\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.595\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \  1 ] = 0.332\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \ 10 ] = 0.496\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100\
    \ ] = 0.508\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.270\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.561\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.694\r\n```"
  created_at: 2022-09-01 13:51:38+00:00
  edited: false
  hidden: false
  id: 6310c6fa61cab0446e4aa4e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: facebook/detr-resnet-101-dc5
repo_type: model
status: open
target_branch: null
title: mAP Drop
