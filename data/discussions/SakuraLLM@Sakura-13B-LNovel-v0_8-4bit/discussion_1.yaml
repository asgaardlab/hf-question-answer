!!python/object:huggingface_hub.community.DiscussionWithDetails
author: l01020304
conflicting_files: null
created_at: 2023-12-24 11:09:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0321237e21d0d8381b52105ceeb322f0.svg
      fullname: luo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: l01020304
      type: user
    createdAt: '2023-12-24T11:09:33.000Z'
    data:
      edited: false
      editors:
      - l01020304
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3569909632205963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0321237e21d0d8381b52105ceeb322f0.svg
          fullname: luo
          isHf: false
          isPro: false
          name: l01020304
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "E:\FFOutput\App\Sakura-13B-Galgame\translate_novel.py",
          line 248, in <br>    main()<br>  File "E:\FFOutput\App\Sakura-13B-Galgame\translate_novel.py",
          line 195, in main<br>    sakura_model = M.SakuraModel(cfg=cfg)<br>  File
          "E:\FFOutput\App\Sakura-13B-Galgame\utils\model.py", line 135, in <strong>init</strong><br>    (tokenizer,
          model) = load_model(cfg)<br>  File "E:\FFOutput\App\Sakura-13B-Galgame\utils\model.py",
          line 84, in load_model<br>    model = AutoGPTQForCausalLM.from_quantized(<br>  File
          "E:\FFOutput\App\Sakura-13B-Galgame\sakura\lib\site-packages\auto_gptq\modeling\auto.py",
          line 108, in from_quantized<br>    return quant_func(<br>  File "E:\FFOutput\App\Sakura-13B-Galgame\sakura\lib\site-packages\auto_gptq\modeling_base.py",
          line 923, in from_quantized<br>    model = autogptq_post_init(model, use_act_order=quantize_config.desc_act)<br>  File
          "E:\FFOutput\App\Sakura-13B-Galgame\sakura\lib\site-packages\auto_gptq\modeling_utils.py",
          line 258, in autogptq_post_init<br>    prepare_buffers(device, buffers["temp_state"],
          buffers["temp_dq"])<br>RuntimeError: no device index</p>

          <p>E:\FFOutput\App\Sakura-13B-Galgame\sakura\python.exe translate_novel.py
          --model_name_or_path E:\FFOutput\App\Sakura-13B-Galgame\models\SakuraLLM\Sakura-13B-LNovel-v0_8-4bit
          --trust_remote_code --model_version 0.8 --use_gptq_model --text_length 512
          --data_path novel.txt --output_path novel_translated.txt</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\\
          translate_novel.py\", line 248, in <module>\r\n    main()\r\n  File \"E:\\\
          FFOutput\\App\\Sakura-13B-Galgame\\translate_novel.py\", line 195, in main\r\
          \n    sakura_model = M.SakuraModel(cfg=cfg)\r\n  File \"E:\\FFOutput\\App\\\
          Sakura-13B-Galgame\\utils\\model.py\", line 135, in __init__\r\n    (tokenizer,\
          \ model) = load_model(cfg)\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\\
          utils\\model.py\", line 84, in load_model\r\n    model = AutoGPTQForCausalLM.from_quantized(\r\
          \n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\sakura\\lib\\site-packages\\\
          auto_gptq\\modeling\\auto.py\", line 108, in from_quantized\r\n    return\
          \ quant_func(\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\sakura\\\
          lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 923, in from_quantized\r\
          \n    model = autogptq_post_init(model, use_act_order=quantize_config.desc_act)\r\
          \n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\sakura\\lib\\site-packages\\\
          auto_gptq\\modeling\\_utils.py\", line 258, in autogptq_post_init\r\n  \
          \  prepare_buffers(device, buffers[\"temp_state\"], buffers[\"temp_dq\"\
          ])\r\nRuntimeError: no device index\r\n\r\nE:\\FFOutput\\App\\Sakura-13B-Galgame\\\
          sakura\\python.exe translate_novel.py --model_name_or_path E:\\FFOutput\\\
          App\\Sakura-13B-Galgame\\models\\SakuraLLM\\Sakura-13B-LNovel-v0_8-4bit\
          \ --trust_remote_code --model_version 0.8 --use_gptq_model --text_length\
          \ 512 --data_path novel.txt --output_path novel_translated.txt"
        updatedAt: '2023-12-24T11:09:33.672Z'
      numEdits: 0
      reactions: []
    id: 6588116d35f23c0f1c097d67
    type: comment
  author: l01020304
  content: "Traceback (most recent call last):\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\\
    translate_novel.py\", line 248, in <module>\r\n    main()\r\n  File \"E:\\FFOutput\\\
    App\\Sakura-13B-Galgame\\translate_novel.py\", line 195, in main\r\n    sakura_model\
    \ = M.SakuraModel(cfg=cfg)\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\\
    utils\\model.py\", line 135, in __init__\r\n    (tokenizer, model) = load_model(cfg)\r\
    \n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\utils\\model.py\", line 84,\
    \ in load_model\r\n    model = AutoGPTQForCausalLM.from_quantized(\r\n  File \"\
    E:\\FFOutput\\App\\Sakura-13B-Galgame\\sakura\\lib\\site-packages\\auto_gptq\\\
    modeling\\auto.py\", line 108, in from_quantized\r\n    return quant_func(\r\n\
    \  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\sakura\\lib\\site-packages\\\
    auto_gptq\\modeling\\_base.py\", line 923, in from_quantized\r\n    model = autogptq_post_init(model,\
    \ use_act_order=quantize_config.desc_act)\r\n  File \"E:\\FFOutput\\App\\Sakura-13B-Galgame\\\
    sakura\\lib\\site-packages\\auto_gptq\\modeling\\_utils.py\", line 258, in autogptq_post_init\r\
    \n    prepare_buffers(device, buffers[\"temp_state\"], buffers[\"temp_dq\"])\r\
    \nRuntimeError: no device index\r\n\r\nE:\\FFOutput\\App\\Sakura-13B-Galgame\\\
    sakura\\python.exe translate_novel.py --model_name_or_path E:\\FFOutput\\App\\\
    Sakura-13B-Galgame\\models\\SakuraLLM\\Sakura-13B-LNovel-v0_8-4bit --trust_remote_code\
    \ --model_version 0.8 --use_gptq_model --text_length 512 --data_path novel.txt\
    \ --output_path novel_translated.txt"
  created_at: 2023-12-24 11:09:33+00:00
  edited: false
  hidden: false
  id: 6588116d35f23c0f1c097d67
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: SakuraLLM/Sakura-13B-LNovel-v0_8-4bit
repo_type: model
status: open
target_branch: null
title: "\u52A0\u8F7D\u62A5\u9519 GPTQ"
