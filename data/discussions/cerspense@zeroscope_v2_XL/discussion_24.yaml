!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NinetailsKurama
conflicting_files: null
created_at: 2023-07-07 14:25:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fad8462a9d55c5a2051c71a15b47a93.svg
      fullname: NinetailsKurama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NinetailsKurama
      type: user
    createdAt: '2023-07-07T15:25:55.000Z'
    data:
      edited: false
      editors:
      - NinetailsKurama
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.727306067943573
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fad8462a9d55c5a2051c71a15b47a93.svg
          fullname: NinetailsKurama
          isHf: false
          isPro: false
          name: NinetailsKurama
          type: user
        html: "<p>Error: lib\\site-packages\\diffusers\\models\\attention_processor.py\"\
          , line 743, in <strong>call</strong><br>    hidden_states = F.scaled_dot_product_attention(<br>RuntimeError:\
          \ Expected is_sm8x || is_sm75 to be true, but got false.  (Could this error\
          \ message be improved?  If so, please report an enhancement request to PyTorch.)</p>\n\
          <p>I have a work around for this but I wanted to share it just in case others\
          \ are experiencing this too. Im running a local LLM on a 3080 10GB and the\
          \ video processing project on my secondary GPU the 1080 ti. It seems something\
          \ change with pytorch that is causing this issue. After looking around I\
          \ found that this line of code fixes it:<br><code>  torch.backends.cuda.enable_flash_sdp(False)</code><br>SInce\
          \ this works on my 3080 and not my 1080 ti, i'm assuming that my 1080 ti\
          \ just doesn't support it (I honestly don't know and have a workaround so\
          \ im happy for now)</p>\n<p>Here is my code snippet for running the program\
          \ and the expected results:</p>\n<p>` is_device_cuda = 'cuda' in device<br>\
          \   try:<br>     print(f'running model with {str(pipe.scheduler.config)}')<br>\
          \            if request.method == 'GET':<br>                prompt = request.args.get('text')<br>\
          \            else:<br>                prompt = request.get_json()[\"text\"\
          ]</p>\n<pre><code>        supports_flash_sdp = True   \n        pipe.scheduler\
          \ = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n   \
          \     if is_device_cuda:\n            pipe.enable_model_cpu_offload(int(device.split(':')[1]))\
          \ \n           \n        try:\n           video_frames = pipe(prompt, num_inference_steps=40,\
          \ height=320, width=576, num_frames=24).frames\n        except:\n      \
          \      supports_flash_sdp = False\n            print(f'GPU may not support\
          \ flash_sdp, trying again with it disabled...')\n\n        if not supports_flash_sdp:\n\
          \            torch.backends.cuda.enable_flash_sdp(False)\n            video_frames\
          \ = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\n\
          \            torch.backends.cuda.enable_flash_sdp(True) # I reenable it\
          \ just in case...\n          \n\n        video_path = export_to_video(video_frames)\n\
          \        print(f'[[ video path is {video_path} ]]')\n</code></pre>\n<p>`</p>\n\
          <p>Hope this helps someone.</p>\n"
        raw: "Error: lib\\site-packages\\diffusers\\models\\attention_processor.py\"\
          , line 743, in __call__\r\n    hidden_states = F.scaled_dot_product_attention(\r\
          \nRuntimeError: Expected is_sm8x || is_sm75 to be true, but got false. \
          \ (Could this error message be improved?  If so, please report an enhancement\
          \ request to PyTorch.)\r\n\r\nI have a work around for this but I wanted\
          \ to share it just in case others are experiencing this too. Im running\
          \ a local LLM on a 3080 10GB and the video processing project on my secondary\
          \ GPU the 1080 ti. It seems something change with pytorch that is causing\
          \ this issue. After looking around I found that this line of code fixes\
          \ it:\r\n`  torch.backends.cuda.enable_flash_sdp(False)`\r\nSInce this works\
          \ on my 3080 and not my 1080 ti, i'm assuming that my 1080 ti just doesn't\
          \ support it (I honestly don't know and have a workaround so im happy for\
          \ now)\r\n\r\nHere is my code snippet for running the program and the expected\
          \ results:\r\n\r\n` is_device_cuda = 'cuda' in device\r\n   try:\r\n   \
          \  print(f'running model with {str(pipe.scheduler.config)}')\r\n       \
          \     if request.method == 'GET':\r\n                prompt = request.args.get('text')\r\
          \n            else:\r\n                prompt = request.get_json()[\"text\"\
          ]\r\n            \r\n            supports_flash_sdp = True   \r\n      \
          \      pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\r\
          \n            if is_device_cuda:\r\n                pipe.enable_model_cpu_offload(int(device.split(':')[1]))\
          \ \r\n               \r\n            try:\r\n               video_frames\
          \ = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\r\
          \n            except:\r\n                supports_flash_sdp = False\r\n\
          \                print(f'GPU may not support flash_sdp, trying again with\
          \ it disabled...')\r\n\r\n            if not supports_flash_sdp:\r\n   \
          \             torch.backends.cuda.enable_flash_sdp(False)\r\n          \
          \      video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576,\
          \ num_frames=24).frames\r\n                torch.backends.cuda.enable_flash_sdp(True)\
          \ # I reenable it just in case...\r\n              \r\n\r\n            video_path\
          \ = export_to_video(video_frames)\r\n            print(f'[[ video path is\
          \ {video_path} ]]')\r\n`\r\n\r\nHope this helps someone.\r\n\r\n"
        updatedAt: '2023-07-07T15:25:55.951Z'
      numEdits: 0
      reactions: []
    id: 64a82e83ee28dc964d0db3be
    type: comment
  author: NinetailsKurama
  content: "Error: lib\\site-packages\\diffusers\\models\\attention_processor.py\"\
    , line 743, in __call__\r\n    hidden_states = F.scaled_dot_product_attention(\r\
    \nRuntimeError: Expected is_sm8x || is_sm75 to be true, but got false.  (Could\
    \ this error message be improved?  If so, please report an enhancement request\
    \ to PyTorch.)\r\n\r\nI have a work around for this but I wanted to share it just\
    \ in case others are experiencing this too. Im running a local LLM on a 3080 10GB\
    \ and the video processing project on my secondary GPU the 1080 ti. It seems something\
    \ change with pytorch that is causing this issue. After looking around I found\
    \ that this line of code fixes it:\r\n`  torch.backends.cuda.enable_flash_sdp(False)`\r\
    \nSInce this works on my 3080 and not my 1080 ti, i'm assuming that my 1080 ti\
    \ just doesn't support it (I honestly don't know and have a workaround so im happy\
    \ for now)\r\n\r\nHere is my code snippet for running the program and the expected\
    \ results:\r\n\r\n` is_device_cuda = 'cuda' in device\r\n   try:\r\n     print(f'running\
    \ model with {str(pipe.scheduler.config)}')\r\n            if request.method ==\
    \ 'GET':\r\n                prompt = request.args.get('text')\r\n            else:\r\
    \n                prompt = request.get_json()[\"text\"]\r\n            \r\n  \
    \          supports_flash_sdp = True   \r\n            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\r\
    \n            if is_device_cuda:\r\n                pipe.enable_model_cpu_offload(int(device.split(':')[1]))\
    \ \r\n               \r\n            try:\r\n               video_frames = pipe(prompt,\
    \ num_inference_steps=40, height=320, width=576, num_frames=24).frames\r\n   \
    \         except:\r\n                supports_flash_sdp = False\r\n          \
    \      print(f'GPU may not support flash_sdp, trying again with it disabled...')\r\
    \n\r\n            if not supports_flash_sdp:\r\n                torch.backends.cuda.enable_flash_sdp(False)\r\
    \n                video_frames = pipe(prompt, num_inference_steps=40, height=320,\
    \ width=576, num_frames=24).frames\r\n                torch.backends.cuda.enable_flash_sdp(True)\
    \ # I reenable it just in case...\r\n              \r\n\r\n            video_path\
    \ = export_to_video(video_frames)\r\n            print(f'[[ video path is {video_path}\
    \ ]]')\r\n`\r\n\r\nHope this helps someone.\r\n\r\n"
  created_at: 2023-07-07 14:25:55+00:00
  edited: false
  hidden: false
  id: 64a82e83ee28dc964d0db3be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476835ad96758c729e3a530/g8rEoWX_5MQl2w-J5DDry.jpeg?w=200&h=200&f=face
      fullname: fanghu168
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fang168
      type: user
    createdAt: '2023-07-08T12:21:36.000Z'
    data:
      edited: false
      editors:
      - fang168
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.812576413154602
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476835ad96758c729e3a530/g8rEoWX_5MQl2w-J5DDry.jpeg?w=200&h=200&f=face
          fullname: fanghu168
          isHf: false
          isPro: false
          name: fang168
          type: user
        html: '<p>A white horse is here.Galloping on the grassland</p>

          '
        raw: A white horse is here.Galloping on the grassland
        updatedAt: '2023-07-08T12:21:36.840Z'
      numEdits: 0
      reactions: []
    id: 64a954d08a1a9187c2e5b291
    type: comment
  author: fang168
  content: A white horse is here.Galloping on the grassland
  created_at: 2023-07-08 11:21:36+00:00
  edited: false
  hidden: false
  id: 64a954d08a1a9187c2e5b291
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476835ad96758c729e3a530/g8rEoWX_5MQl2w-J5DDry.jpeg?w=200&h=200&f=face
      fullname: fanghu168
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fang168
      type: user
    createdAt: '2023-07-08T12:22:27.000Z'
    data:
      edited: false
      editors:
      - fang168
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6579779982566833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476835ad96758c729e3a530/g8rEoWX_5MQl2w-J5DDry.jpeg?w=200&h=200&f=face
          fullname: fanghu168
          isHf: false
          isPro: false
          name: fang168
          type: user
        html: '<p>A white horse gallops on the green grassland.run quickly</p>

          '
        raw: A white horse gallops on the green grassland.run quickly
        updatedAt: '2023-07-08T12:22:27.068Z'
      numEdits: 0
      reactions: []
    id: 64a95503c5a0593b309090bc
    type: comment
  author: fang168
  content: A white horse gallops on the green grassland.run quickly
  created_at: 2023-07-08 11:22:27+00:00
  edited: false
  hidden: false
  id: 64a95503c5a0593b309090bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: cerspense/zeroscope_v2_XL
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: Expected is_sm8x || is_sm75 to be true, but got false. '
