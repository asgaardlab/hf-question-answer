!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fabien-tarrade-axa-ch
conflicting_files: null
created_at: 2023-09-06 04:22:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edcb4feb41ade3673fc486ecc8ef4fa5.svg
      fullname: Fabien Tarrade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fabien-tarrade-axa-ch
      type: user
    createdAt: '2023-09-06T05:22:58.000Z'
    data:
      edited: false
      editors:
      - fabien-tarrade-axa-ch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8657186031341553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edcb4feb41ade3673fc486ecc8ef4fa5.svg
          fullname: Fabien Tarrade
          isHf: false
          isPro: false
          name: fabien-tarrade-axa-ch
          type: user
        html: "<p>Hi there,</p>\n<p>congrat for the nice work. we wanted to use it\
          \ but we are unable to clone the repo. The issue seems to be the single\
          \ 37 Gb file model.safetensors. I have no issue to clone Llama 2 70B models\
          \ (total size &gt;128 GB but maximum file size is 10 Gb)</p>\n<p>git clone\
          \ <a href=\"https://huggingface.co/TheBloke/Genz-70b-GPTQ\">https://huggingface.co/TheBloke/Genz-70b-GPTQ</a><br>Cloning\
          \ into \u2018Genz-70b-GPTQ\u2019...<br>remote: Enumerating objects: 109,\
          \ done.<br>remote: Counting objects: 100% (106/106), done.<br>remote: Compressing\
          \ objects: 100% (106/106), done.<br>remote: Total 109 (delta 43), reused\
          \ 0 (delta 0), pack-reused 3<br>Receiving objects: 100% (109/109), 503.54\
          \ KiB | 2.75 MiB/s, done.<br>Resolving deltas: 100% (43/43), done.<br>Downloading\
          \ model.safetensors (37 GB)<br>[1]    3116 killed     git clone <a href=\"\
          https://huggingface.co/TheBloke/Genz-70b-GPTQ\">https://huggingface.co/TheBloke/Genz-70b-GPTQ</a></p>\n\
          <p>Is this a know issue ? Is there a trick to have it working ? Is there\
          \ some reason to have one single file and not few shard ?</p>\n<p>Thanks</p>\n"
        raw: "Hi there,\r\n\r\ncongrat for the nice work. we wanted to use it but\
          \ we are unable to clone the repo. The issue seems to be the single 37 Gb\
          \ file model.safetensors. I have no issue to clone Llama 2 70B models (total\
          \ size >128 GB but maximum file size is 10 Gb)\r\n\r\ngit clone https://huggingface.co/TheBloke/Genz-70b-GPTQ\r\
          \nCloning into \u2018Genz-70b-GPTQ\u2019...\r\nremote: Enumerating objects:\
          \ 109, done.\r\nremote: Counting objects: 100% (106/106), done.\r\nremote:\
          \ Compressing objects: 100% (106/106), done.\r\nremote: Total 109 (delta\
          \ 43), reused 0 (delta 0), pack-reused 3\r\nReceiving objects: 100% (109/109),\
          \ 503.54 KiB | 2.75 MiB/s, done.\r\nResolving deltas: 100% (43/43), done.\r\
          \nDownloading model.safetensors (37 GB)\r\n[1]    3116 killed     git clone\
          \ https://huggingface.co/TheBloke/Genz-70b-GPTQ\r\n\r\nIs this a know issue\
          \ ? Is there a trick to have it working ? Is there some reason to have one\
          \ single file and not few shard ?\r\n\r\nThanks"
        updatedAt: '2023-09-06T05:22:58.814Z'
      numEdits: 0
      reactions: []
    id: 64f80cb28a4c5c401d78c343
    type: comment
  author: fabien-tarrade-axa-ch
  content: "Hi there,\r\n\r\ncongrat for the nice work. we wanted to use it but we\
    \ are unable to clone the repo. The issue seems to be the single 37 Gb file model.safetensors.\
    \ I have no issue to clone Llama 2 70B models (total size >128 GB but maximum\
    \ file size is 10 Gb)\r\n\r\ngit clone https://huggingface.co/TheBloke/Genz-70b-GPTQ\r\
    \nCloning into \u2018Genz-70b-GPTQ\u2019...\r\nremote: Enumerating objects: 109,\
    \ done.\r\nremote: Counting objects: 100% (106/106), done.\r\nremote: Compressing\
    \ objects: 100% (106/106), done.\r\nremote: Total 109 (delta 43), reused 0 (delta\
    \ 0), pack-reused 3\r\nReceiving objects: 100% (109/109), 503.54 KiB | 2.75 MiB/s,\
    \ done.\r\nResolving deltas: 100% (43/43), done.\r\nDownloading model.safetensors\
    \ (37 GB)\r\n[1]    3116 killed     git clone https://huggingface.co/TheBloke/Genz-70b-GPTQ\r\
    \n\r\nIs this a know issue ? Is there a trick to have it working ? Is there some\
    \ reason to have one single file and not few shard ?\r\n\r\nThanks"
  created_at: 2023-09-06 04:22:58+00:00
  edited: false
  hidden: false
  id: 64f80cb28a4c5c401d78c343
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edcb4feb41ade3673fc486ecc8ef4fa5.svg
      fullname: Fabien Tarrade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fabien-tarrade-axa-ch
      type: user
    createdAt: '2023-09-06T09:55:04.000Z'
    data:
      edited: false
      editors:
      - fabien-tarrade-axa-ch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4321174621582031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edcb4feb41ade3673fc486ecc8ef4fa5.svg
          fullname: Fabien Tarrade
          isHf: false
          isPro: false
          name: fabien-tarrade-axa-ch
          type: user
        html: '<p>Same issue using python transformer library:</p>

          <p>ValueError: Could not load model TheBloke/Genz-70b-GPTQ with any of the
          following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.llama.modeling_llama.LlamaForCausalLM''&gt;</p>

          '
        raw: 'Same issue using python transformer library:


          ValueError: Could not load model TheBloke/Genz-70b-GPTQ with any of the
          following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,
          <class ''transformers.models.llama.modeling_llama.LlamaForCausalLM''>'
        updatedAt: '2023-09-06T09:55:04.612Z'
      numEdits: 0
      reactions: []
    id: 64f84c7835a0a9fc543cd247
    type: comment
  author: fabien-tarrade-axa-ch
  content: 'Same issue using python transformer library:


    ValueError: Could not load model TheBloke/Genz-70b-GPTQ with any of the following
    classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,
    <class ''transformers.models.llama.modeling_llama.LlamaForCausalLM''>'
  created_at: 2023-09-06 08:55:04+00:00
  edited: false
  hidden: false
  id: 64f84c7835a0a9fc543cd247
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Genz-70b-GPTQ
repo_type: model
status: open
target_branch: null
title: cannot clone https://huggingface.co/TheBloke/Genz-70b-GPTQ, got killed
