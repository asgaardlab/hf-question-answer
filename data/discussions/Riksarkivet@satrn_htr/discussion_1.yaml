!!python/object:huggingface_hub.community.DiscussionWithDetails
author: patrikwm
conflicting_files: null
created_at: 2023-12-31 08:14:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c401f5a539d58b34b2e92ef9732dbf60.svg
      fullname: Patrik Wigur Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrikwm
      type: user
    createdAt: '2023-12-31T08:14:01.000Z'
    data:
      edited: true
      editors:
      - patrikwm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.899409830570221
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c401f5a539d58b34b2e92ef9732dbf60.svg
          fullname: Patrik Wigur Martin
          isHf: false
          isPro: false
          name: patrikwm
          type: user
        html: '<p>Hi, </p>

          <p>Could you provide an example on how to fine-tune this model?<br>Is it
          enough to have the correct text for fine-tuning, or do i need Image + text?</p>

          <p>I am trying to transcribe 1700th century court records from northern
          Sweden/Finland.<br>The hardest part for the model seems to be all the different
          names. </p>

          <p>Do you have any pre trained model that only has been trained on 1600-1800?</p>

          <p>best regards,<br>Patrik</p>

          '
        raw: "Hi, \n\nCould you provide an example on how to fine-tune this model?\
          \ \nIs it enough to have the correct text for fine-tuning, or do i need\
          \ Image + text?\n\nI am trying to transcribe 1700th century court records\
          \ from northern Sweden/Finland. \nThe hardest part for the model seems to\
          \ be all the different names. \n\nDo you have any pre trained model that\
          \ only has been trained on 1600-1800?\n\nbest regards,\nPatrik"
        updatedAt: '2024-01-06T11:11:52.801Z'
      numEdits: 2
      reactions: []
    id: 659122c9a6567cb93c808b5a
    type: comment
  author: patrikwm
  content: "Hi, \n\nCould you provide an example on how to fine-tune this model? \n\
    Is it enough to have the correct text for fine-tuning, or do i need Image + text?\n\
    \nI am trying to transcribe 1700th century court records from northern Sweden/Finland.\
    \ \nThe hardest part for the model seems to be all the different names. \n\nDo\
    \ you have any pre trained model that only has been trained on 1600-1800?\n\n\
    best regards,\nPatrik"
  created_at: 2023-12-31 08:14:01+00:00
  edited: true
  hidden: false
  id: 659122c9a6567cb93c808b5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/XppkjYbPDkE-QdL6L5U51.png?w=200&h=200&f=face
      fullname: Gabriel Borg
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Gabriel
      type: user
    createdAt: '2024-01-18T09:11:11.000Z'
    data:
      edited: false
      editors:
      - Gabriel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6827902793884277
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/XppkjYbPDkE-QdL6L5U51.png?w=200&h=200&f=face
          fullname: Gabriel Borg
          isHf: false
          isPro: false
          name: Gabriel
          type: user
        html: '<p>Hi! </p>

          <p>Yes,  here are two links you could follow on how to could train a such
          model:</p>

          <ul>

          <li>Huggingface:  <a rel="nofollow" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb">https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb</a></li>

          <li>MMOCR:  <a rel="nofollow" href="https://colab.research.google.com/github/open-mmlab/mmocr/blob/dev-1.x/demo/tutorial.ipynb#scrollTo=2AZqwCt09XqR">https://colab.research.google.com/github/open-mmlab/mmocr/blob/dev-1.x/demo/tutorial.ipynb#scrollTo=2AZqwCt09XqR</a></li>

          </ul>

          <p>You will need image+text paris:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/c__VfyRk4eGQD6W7OqsX-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/c__VfyRk4eGQD6W7OqsX-.png"></a></p>

          <p>Here is an example:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/2O-xTpfTOWkussJNpudPP.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/2O-xTpfTOWkussJNpudPP.png"></a></p>

          <p>So if you are going to use the SATRN architecture from MMOCR (which is
          very good out of domain) you can use our model as our base.</p>

          <ul>

          <li>Here is the config file: <a href="https://huggingface.co/Riksarkivet/satrn_htr/blob/main/config.py">https://huggingface.co/Riksarkivet/satrn_htr/blob/main/config.py</a></li>

          <li>And here is the pretrained weights: <a href="https://huggingface.co/Riksarkivet/satrn_htr/blob/main/model.pth">https://huggingface.co/Riksarkivet/satrn_htr/blob/main/model.pth</a></li>

          <li>And if you want to train it from scratch use this: <a rel="nofollow"
          href="https://github.com/open-mmlab/mmocr/tree/main/configs/textrecog/satrn">https://github.com/open-mmlab/mmocr/tree/main/configs/textrecog/satrn</a></li>

          </ul>

          <p>You need to modify the config file to our or the plain one from mmocr:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/v9VuekfrzPpAlzoegnebJ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/v9VuekfrzPpAlzoegnebJ.png"></a></p>

          <p>Altough, you dont need to change any of the hyperparameters if you start
          with our config file. But perhaps the learning rate can be adjusted if you
          are only going to fine tune on a smaller dataset.</p>

          <p>TrOCR is much more simplier to train just follow the link i sent :)</p>

          '
        raw: "Hi! \n\nYes,  here are two links you could follow on how to could train\
          \ a such model:\n\n- Huggingface:  https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb\n\
          - MMOCR:  https://colab.research.google.com/github/open-mmlab/mmocr/blob/dev-1.x/demo/tutorial.ipynb#scrollTo=2AZqwCt09XqR\n\
          \nYou will need image+text paris:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/c__VfyRk4eGQD6W7OqsX-.png)\n\
          \nHere is an example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/2O-xTpfTOWkussJNpudPP.png)\n\
          \nSo if you are going to use the SATRN architecture from MMOCR (which is\
          \ very good out of domain) you can use our model as our base.\n- Here is\
          \ the config file: https://huggingface.co/Riksarkivet/satrn_htr/blob/main/config.py\n\
          - And here is the pretrained weights: https://huggingface.co/Riksarkivet/satrn_htr/blob/main/model.pth\n\
          - And if you want to train it from scratch use this: https://github.com/open-mmlab/mmocr/tree/main/configs/textrecog/satrn\n\
          \nYou need to modify the config file to our or the plain one from mmocr:\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/v9VuekfrzPpAlzoegnebJ.png)\n\
          \nAltough, you dont need to change any of the hyperparameters if you start\
          \ with our config file. But perhaps the learning rate can be adjusted if\
          \ you are only going to fine tune on a smaller dataset.\n\nTrOCR is much\
          \ more simplier to train just follow the link i sent :)\n\n\n\n\n\n\n\n\n"
        updatedAt: '2024-01-18T09:11:11.448Z'
      numEdits: 0
      reactions: []
    id: 65a8eb2fc0e51d82e6e017b3
    type: comment
  author: Gabriel
  content: "Hi! \n\nYes,  here are two links you could follow on how to could train\
    \ a such model:\n\n- Huggingface:  https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb\n\
    - MMOCR:  https://colab.research.google.com/github/open-mmlab/mmocr/blob/dev-1.x/demo/tutorial.ipynb#scrollTo=2AZqwCt09XqR\n\
    \nYou will need image+text paris:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/c__VfyRk4eGQD6W7OqsX-.png)\n\
    \nHere is an example:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/2O-xTpfTOWkussJNpudPP.png)\n\
    \nSo if you are going to use the SATRN architecture from MMOCR (which is very\
    \ good out of domain) you can use our model as our base.\n- Here is the config\
    \ file: https://huggingface.co/Riksarkivet/satrn_htr/blob/main/config.py\n- And\
    \ here is the pretrained weights: https://huggingface.co/Riksarkivet/satrn_htr/blob/main/model.pth\n\
    - And if you want to train it from scratch use this: https://github.com/open-mmlab/mmocr/tree/main/configs/textrecog/satrn\n\
    \nYou need to modify the config file to our or the plain one from mmocr:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4e677917119d38f6bbff8/v9VuekfrzPpAlzoegnebJ.png)\n\
    \nAltough, you dont need to change any of the hyperparameters if you start with\
    \ our config file. But perhaps the learning rate can be adjusted if you are only\
    \ going to fine tune on a smaller dataset.\n\nTrOCR is much more simplier to train\
    \ just follow the link i sent :)\n\n\n\n\n\n\n\n\n"
  created_at: 2024-01-18 09:11:11+00:00
  edited: false
  hidden: false
  id: 65a8eb2fc0e51d82e6e017b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c401f5a539d58b34b2e92ef9732dbf60.svg
      fullname: Patrik Wigur Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrikwm
      type: user
    createdAt: '2024-01-20T12:47:29.000Z'
    data:
      edited: false
      editors:
      - patrikwm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9479572176933289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c401f5a539d58b34b2e92ef9732dbf60.svg
          fullname: Patrik Wigur Martin
          isHf: false
          isPro: false
          name: patrikwm
          type: user
        html: '<p>Thanks, great! </p>

          <p>I will try this. But it will probably take some time to get a good amount
          of image+text pairs.</p>

          '
        raw: "Thanks, great! \n\nI will try this. But it will probably take some time\
          \ to get a good amount of image+text pairs."
        updatedAt: '2024-01-20T12:47:29.105Z'
      numEdits: 0
      reactions: []
    id: 65abc0e19c81170e3132f741
    type: comment
  author: patrikwm
  content: "Thanks, great! \n\nI will try this. But it will probably take some time\
    \ to get a good amount of image+text pairs."
  created_at: 2024-01-20 12:47:29+00:00
  edited: false
  hidden: false
  id: 65abc0e19c81170e3132f741
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Riksarkivet/satrn_htr
repo_type: model
status: open
target_branch: null
title: Example finetuning?
