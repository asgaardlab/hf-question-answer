!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adamo1139
conflicting_files: null
created_at: 2023-11-06 23:23:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-06T23:23:30.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7117466330528259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p>First, thank you for your work on fine-tuning new models and also\
          \ quantizing them for use with exllama2, great stuff!</p>\n<p>I downloaded\
          \ this model and I am trying to run this with exllama2 on Windows.<br>Running\
          \ it with test_inference.py works fine, but I receive an error when trying\
          \ to run chat</p>\n<pre><code>python examples\\chat.py -m models\\yi6b-spicyboros\
          \ -mode raw \n -- Model: models\\yi6b-spicyboros\n -- Options: ['rope_scale\
          \ 1.0', 'rope_alpha 1.0']\nTraceback (most recent call last):\n  File \"\
          C:\\Users\\adamo\\maked\\exllama\\exllamav2\\examples\\chat.py\", line 81,\
          \ in &lt;module&gt;\n    model, tokenizer = model_init.init(args, allow_auto_split\
          \ = True)\n  File \"C:\\Users\\adamo\\AppData\\Local\\Programs\\Python\\\
          Python310\\lib\\site-packages\\exllamav2\\model_init.py\", line 64, in init\n\
          \    config.prepare()\n  File \"C:\\Users\\adamo\\AppData\\Local\\Programs\\\
          Python\\Python310\\lib\\site-packages\\exllamav2\\config.py\", line 133,\
          \ in prepare\n    raise ValueError(f\" ## Could not find {prefix}.* in model\"\
          )\nValueError:  ## Could not find model.layers.0.input_layernorm.* in model\n\
          </code></pre>\n<p>I printed the list of layers for this model (variable\
          \ <code>self.tensor_file_map</code> from <code>config.py</code>) and indeed\
          \ I can't find any input_layernorm layers there.</p>\n<p>I checked for the\
          \ same issue with <a href=\"https://huggingface.co/turboderp/TinyLlama-1B-ckpt503-exl2/tree/4.0bpw\"\
          >EXL2 quantized TinyLlama</a> from turboderp and this issue does not exist\
          \ there.<br>Am I missing something obvious?</p>\n"
        raw: "First, thank you for your work on fine-tuning new models and also quantizing\
          \ them for use with exllama2, great stuff!\r\n\r\nI downloaded this model\
          \ and I am trying to run this with exllama2 on Windows.\r\nRunning it with\
          \ test_inference.py works fine, but I receive an error when trying to run\
          \ chat\r\n\r\n\r\n```\r\npython examples\\chat.py -m models\\yi6b-spicyboros\
          \ -mode raw \r\n -- Model: models\\yi6b-spicyboros\r\n -- Options: ['rope_scale\
          \ 1.0', 'rope_alpha 1.0']\r\nTraceback (most recent call last):\r\n  File\
          \ \"C:\\Users\\adamo\\maked\\exllama\\exllamav2\\examples\\chat.py\", line\
          \ 81, in <module>\r\n    model, tokenizer = model_init.init(args, allow_auto_split\
          \ = True)\r\n  File \"C:\\Users\\adamo\\AppData\\Local\\Programs\\Python\\\
          Python310\\lib\\site-packages\\exllamav2\\model_init.py\", line 64, in init\r\
          \n    config.prepare()\r\n  File \"C:\\Users\\adamo\\AppData\\Local\\Programs\\\
          Python\\Python310\\lib\\site-packages\\exllamav2\\config.py\", line 133,\
          \ in prepare\r\n    raise ValueError(f\" ## Could not find {prefix}.* in\
          \ model\")\r\nValueError:  ## Could not find model.layers.0.input_layernorm.*\
          \ in model\r\n```\r\n\r\nI printed the list of layers for this model (variable\
          \ `self.tensor_file_map` from `config.py`) and indeed I can't find any input_layernorm\
          \ layers there.\r\n\r\nI checked for the same issue with [EXL2 quantized\
          \ TinyLlama](https://huggingface.co/turboderp/TinyLlama-1B-ckpt503-exl2/tree/4.0bpw)\
          \ from turboderp and this issue does not exist there.\r\nAm I missing something\
          \ obvious?\r\n\r\n"
        updatedAt: '2023-11-06T23:23:30.818Z'
      numEdits: 0
      reactions: []
    id: 65497572b8d6b1e86307550c
    type: comment
  author: adamo1139
  content: "First, thank you for your work on fine-tuning new models and also quantizing\
    \ them for use with exllama2, great stuff!\r\n\r\nI downloaded this model and\
    \ I am trying to run this with exllama2 on Windows.\r\nRunning it with test_inference.py\
    \ works fine, but I receive an error when trying to run chat\r\n\r\n\r\n```\r\n\
    python examples\\chat.py -m models\\yi6b-spicyboros -mode raw \r\n -- Model: models\\\
    yi6b-spicyboros\r\n -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']\r\nTraceback\
    \ (most recent call last):\r\n  File \"C:\\Users\\adamo\\maked\\exllama\\exllamav2\\\
    examples\\chat.py\", line 81, in <module>\r\n    model, tokenizer = model_init.init(args,\
    \ allow_auto_split = True)\r\n  File \"C:\\Users\\adamo\\AppData\\Local\\Programs\\\
    Python\\Python310\\lib\\site-packages\\exllamav2\\model_init.py\", line 64, in\
    \ init\r\n    config.prepare()\r\n  File \"C:\\Users\\adamo\\AppData\\Local\\\
    Programs\\Python\\Python310\\lib\\site-packages\\exllamav2\\config.py\", line\
    \ 133, in prepare\r\n    raise ValueError(f\" ## Could not find {prefix}.* in\
    \ model\")\r\nValueError:  ## Could not find model.layers.0.input_layernorm.*\
    \ in model\r\n```\r\n\r\nI printed the list of layers for this model (variable\
    \ `self.tensor_file_map` from `config.py`) and indeed I can't find any input_layernorm\
    \ layers there.\r\n\r\nI checked for the same issue with [EXL2 quantized TinyLlama](https://huggingface.co/turboderp/TinyLlama-1B-ckpt503-exl2/tree/4.0bpw)\
    \ from turboderp and this issue does not exist there.\r\nAm I missing something\
    \ obvious?\r\n\r\n"
  created_at: 2023-11-06 23:23:30+00:00
  edited: false
  hidden: false
  id: 65497572b8d6b1e86307550c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-07T04:23:40.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8151312470436096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I quantized using the latest exl2 git repo and am able to load it
          using ooba text gen and generate responses (though it still has issues stopping
          when generation is complete):</p>

          <pre><code>2023-11-06 22:23:14 INFO:Loading Yi-6B-Spicyboros-3.1-4.0bpw-h6-exl2...

          2023-11-06 22:23:16 INFO:Loaded the model in 1.38 seconds.

          Output generated in 15.38 seconds (99.69 tokens/s, 1533 tokens, context
          95, seed 482399913)

          </code></pre>

          <p>I''ll test with the command line interface when I get a chance later
          today.</p>

          '
        raw: 'I quantized using the latest exl2 git repo and am able to load it using
          ooba text gen and generate responses (though it still has issues stopping
          when generation is complete):

          ```

          2023-11-06 22:23:14 INFO:Loading Yi-6B-Spicyboros-3.1-4.0bpw-h6-exl2...

          2023-11-06 22:23:16 INFO:Loaded the model in 1.38 seconds.

          Output generated in 15.38 seconds (99.69 tokens/s, 1533 tokens, context
          95, seed 482399913)

          ```


          I''ll test with the command line interface when I get a chance later today.'
        updatedAt: '2023-11-07T04:23:40.410Z'
      numEdits: 0
      reactions: []
    id: 6549bbcc7ab20d807df0a0f8
    type: comment
  author: LoneStriker
  content: 'I quantized using the latest exl2 git repo and am able to load it using
    ooba text gen and generate responses (though it still has issues stopping when
    generation is complete):

    ```

    2023-11-06 22:23:14 INFO:Loading Yi-6B-Spicyboros-3.1-4.0bpw-h6-exl2...

    2023-11-06 22:23:16 INFO:Loaded the model in 1.38 seconds.

    Output generated in 15.38 seconds (99.69 tokens/s, 1533 tokens, context 95, seed
    482399913)

    ```


    I''ll test with the command line interface when I get a chance later today.'
  created_at: 2023-11-07 04:23:40+00:00
  edited: false
  hidden: false
  id: 6549bbcc7ab20d807df0a0f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-07T11:17:55.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9085221886634827
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I managed to solve the issue. I was running  chat.py from up-to-date
          repo, but I was using pre-compiled 0.0.7 wheel and not self-complied version.
          The pre-compiled version didn''t have commit <a rel="nofollow" href="https://github.com/turboderp/exllamav2/commit/6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2">6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2</a>
          that adds Yi compatibility.<br>I compiled the package with <code>python
          setup.py install --user</code> and now it chat.py works.</p>

          '
        raw: 'I managed to solve the issue. I was running  chat.py from up-to-date
          repo, but I was using pre-compiled 0.0.7 wheel and not self-complied version.
          The pre-compiled version didn''t have commit [6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2](https://github.com/turboderp/exllamav2/commit/6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2)
          that adds Yi compatibility.

          I compiled the package with `python setup.py install --user` and now it
          chat.py works.'
        updatedAt: '2023-11-07T11:17:55.559Z'
      numEdits: 0
      reactions: []
    id: 654a1ce3980ac7a3d56df55b
    type: comment
  author: adamo1139
  content: 'I managed to solve the issue. I was running  chat.py from up-to-date repo,
    but I was using pre-compiled 0.0.7 wheel and not self-complied version. The pre-compiled
    version didn''t have commit [6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2](https://github.com/turboderp/exllamav2/commit/6d24e1ad40d89f64b1bd3ae36e639c74c9f730b2)
    that adds Yi compatibility.

    I compiled the package with `python setup.py install --user` and now it chat.py
    works.'
  created_at: 2023-11-07 11:17:55+00:00
  edited: false
  hidden: false
  id: 654a1ce3980ac7a3d56df55b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-07T11:18:32.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9105144739151001
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>closing the issue</p>

          '
        raw: closing the issue
        updatedAt: '2023-11-07T11:18:32.996Z'
      numEdits: 0
      reactions: []
      relatedEventId: 654a1d0983e7bfc43137876f
    id: 654a1d0883e7bfc43137876c
    type: comment
  author: adamo1139
  content: closing the issue
  created_at: 2023-11-07 11:18:32+00:00
  edited: false
  hidden: false
  id: 654a1d0883e7bfc43137876c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-07T11:18:33.000Z'
    data:
      status: closed
    id: 654a1d0983e7bfc43137876f
    type: status-change
  author: adamo1139
  created_at: 2023-11-07 11:18:33+00:00
  id: 654a1d0983e7bfc43137876f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Yi-6B-Spicyboros-3.1-4.0bpw-h6-exl2
repo_type: model
status: closed
target_branch: null
title: Lack of input_layernorm layers?
