!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lucy3
conflicting_files: null
created_at: 2022-06-03 17:19:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46045b57e6095110f4476b20776b32d8.svg
      fullname: Lucy Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucy3
      type: user
    createdAt: '2022-06-03T18:19:57.000Z'
    data:
      edited: false
      editors:
      - lucy3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46045b57e6095110f4476b20776b32d8.svg
          fullname: Lucy Li
          isHf: false
          isPro: false
          name: lucy3
          type: user
        html: "<p>Might be worthwhile to look further into the vocabulary of this\
          \ model. Unsure if this is a WordPiece issue or unusual artifacts in the\
          \ training data: </p>\n<pre><code>##therehereatiteleavebesenotelinesstyandoacyndeegreeboneansenstrokemorehighe\xAF\
          veryterinetainpositivestitisementicludequityerallleveleandicludingamelylesseccedancedelmentibeingasiauddeplineungaltanurchylateworttimeontentwhichsexaintevariatepartiteagueeritiarytheseondonkageamytanolroleunateonfeongeephenreachonsequenceonvoaicijunctiondatoryestheriouslyliagealisationaliseeliegestionfullereforecrollebasedwisehateoinolexiamouhatieldethepartatehalllaihereaonceduoquantitativealiteghum\n\
          </code></pre>\n<pre><code>and : -. of ;,andstrokeverylinessgreellongandithusmentibonee\xAF\
          thety\xB0waterlatandoongthat\xA2topgineslevel _telacyhightermhashatreachtttingquityandenglyfulllexiandeeucuthere\
          \ thatriouslyonuourse buthattyearadh #cityhguo grand particularsnonwhatlamptheaycemicuntithiajingcatayllus\
          \ orlaihardcht\xBCnodes /e!cativeoundetoolsbeam jalisationof hingrlylsyhisthema\
          \ glutamatenumbersberta\n</code></pre>\n"
        raw: "Might be worthwhile to look further into the vocabulary of this model.\
          \ Unsure if this is a WordPiece issue or unusual artifacts in the training\
          \ data: \r\n\r\n```\r\n##therehereatiteleavebesenotelinesstyandoacyndeegreeboneansenstrokemorehighe\xAF\
          veryterinetainpositivestitisementicludequityerallleveleandicludingamelylesseccedancedelmentibeingasiauddeplineungaltanurchylateworttimeontentwhichsexaintevariatepartiteagueeritiarytheseondonkageamytanolroleunateonfeongeephenreachonsequenceonvoaicijunctiondatoryestheriouslyliagealisationaliseeliegestionfullereforecrollebasedwisehateoinolexiamouhatieldethepartatehalllaihereaonceduoquantitativealiteghum\r\
          \n```\r\n\r\n```\r\nand : -. of ;,andstrokeverylinessgreellongandithusmentibonee\xAF\
          thety\xB0waterlatandoongthat\xA2topgineslevel _telacyhightermhashatreachtttingquityandenglyfulllexiandeeucuthere\
          \ thatriouslyonuourse buthattyearadh #cityhguo grand particularsnonwhatlamptheaycemicuntithiajingcatayllus\
          \ orlaihardcht\xBCnodes /e!cativeoundetoolsbeam jalisationof hingrlylsyhisthema\
          \ glutamatenumbersberta\r\n```"
        updatedAt: '2022-06-03T18:19:57.470Z'
      numEdits: 0
      reactions: []
    id: 629a50cd5ab4232a3fddedc7
    type: comment
  author: lucy3
  content: "Might be worthwhile to look further into the vocabulary of this model.\
    \ Unsure if this is a WordPiece issue or unusual artifacts in the training data:\
    \ \r\n\r\n```\r\n##therehereatiteleavebesenotelinesstyandoacyndeegreeboneansenstrokemorehighe\xAF\
    veryterinetainpositivestitisementicludequityerallleveleandicludingamelylesseccedancedelmentibeingasiauddeplineungaltanurchylateworttimeontentwhichsexaintevariatepartiteagueeritiarytheseondonkageamytanolroleunateonfeongeephenreachonsequenceonvoaicijunctiondatoryestheriouslyliagealisationaliseeliegestionfullereforecrollebasedwisehateoinolexiamouhatieldethepartatehalllaihereaonceduoquantitativealiteghum\r\
    \n```\r\n\r\n```\r\nand : -. of ;,andstrokeverylinessgreellongandithusmentibonee\xAF\
    thety\xB0waterlatandoongthat\xA2topgineslevel _telacyhightermhashatreachtttingquityandenglyfulllexiandeeucuthere\
    \ thatriouslyonuourse buthattyearadh #cityhguo grand particularsnonwhatlamptheaycemicuntithiajingcatayllus\
    \ orlaihardcht\xBCnodes /e!cativeoundetoolsbeam jalisationof hingrlylsyhisthema\
    \ glutamatenumbersberta\r\n```"
  created_at: 2022-06-03 17:19:57+00:00
  edited: false
  hidden: false
  id: 629a50cd5ab4232a3fddedc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653510775935-628a3b8fedfa7a816db8b965.png?w=200&h=200&f=face
      fullname: Globus Labs
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: globuslabs
      type: user
    createdAt: '2022-06-03T18:32:50.000Z'
    data:
      edited: false
      editors:
      - globuslabs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653510775935-628a3b8fedfa7a816db8b965.png?w=200&h=200&f=face
          fullname: Globus Labs
          isHf: false
          isPro: false
          name: globuslabs
          type: user
        html: '<p>Thank you for your feedback. Could you provide more contexts on
          the issue? How did you get the two strings? Are these the tokenization results
          of some text? If so, could you provide the original text and code the used
          to tokenize it?</p>

          '
        raw: Thank you for your feedback. Could you provide more contexts on the issue?
          How did you get the two strings? Are these the tokenization results of some
          text? If so, could you provide the original text and code the used to tokenize
          it?
        updatedAt: '2022-06-03T18:32:50.214Z'
      numEdits: 0
      reactions: []
    id: 629a53d2e0753998f1fa88fc
    type: comment
  author: globuslabs
  content: Thank you for your feedback. Could you provide more contexts on the issue?
    How did you get the two strings? Are these the tokenization results of some text?
    If so, could you provide the original text and code the used to tokenize it?
  created_at: 2022-06-03 17:32:50+00:00
  edited: false
  hidden: false
  id: 629a53d2e0753998f1fa88fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46045b57e6095110f4476b20776b32d8.svg
      fullname: Lucy Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucy3
      type: user
    createdAt: '2022-06-03T20:36:10.000Z'
    data:
      edited: false
      editors:
      - lucy3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46045b57e6095110f4476b20776b32d8.svg
          fullname: Lucy Li
          isHf: false
          isPro: false
          name: lucy3
          type: user
        html: '<p>Sorry it was actually a problem on my end! What I was doing was:
          </p>

          <pre><code>from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(''globuslabs/ScholarBERT'', use_fast=True)

          tokenizer.decode([30, 4115, 1909, 18, 17, 18008, 2329, 16, 31, 1006])

          </code></pre>

          <p>when instead I should have been doing</p>

          <pre><code>from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(''globuslabs/ScholarBERT'', use_fast=True)

          tokenizer.convert_ids_to_tokens([30, 4115, 1909, 18, 17, 18008, 2329, 16,
          31, 1006])

          </code></pre>

          <p>Thanks for the prompt reply though :)</p>

          '
        raw: "Sorry it was actually a problem on my end! What I was doing was: \n\n\
          ```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('globuslabs/ScholarBERT',\
          \ use_fast=True)\ntokenizer.decode([30, 4115, 1909, 18, 17, 18008, 2329,\
          \ 16, 31, 1006])\n```\n\nwhen instead I should have been doing\n\n```\n\
          from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('globuslabs/ScholarBERT',\
          \ use_fast=True)\ntokenizer.convert_ids_to_tokens([30, 4115, 1909, 18, 17,\
          \ 18008, 2329, 16, 31, 1006])\n```\n\nThanks for the prompt reply though\
          \ :)"
        updatedAt: '2022-06-03T20:36:10.354Z'
      numEdits: 0
      reactions: []
    id: 629a70ba9d845f7fb2085ebe
    type: comment
  author: lucy3
  content: "Sorry it was actually a problem on my end! What I was doing was: \n\n\
    ```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('globuslabs/ScholarBERT',\
    \ use_fast=True)\ntokenizer.decode([30, 4115, 1909, 18, 17, 18008, 2329, 16, 31,\
    \ 1006])\n```\n\nwhen instead I should have been doing\n\n```\nfrom transformers\
    \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('globuslabs/ScholarBERT',\
    \ use_fast=True)\ntokenizer.convert_ids_to_tokens([30, 4115, 1909, 18, 17, 18008,\
    \ 2329, 16, 31, 1006])\n```\n\nThanks for the prompt reply though :)"
  created_at: 2022-06-03 19:36:10+00:00
  edited: false
  hidden: false
  id: 629a70ba9d845f7fb2085ebe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/46045b57e6095110f4476b20776b32d8.svg
      fullname: Lucy Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucy3
      type: user
    createdAt: '2022-06-03T20:41:04.000Z'
    data:
      status: closed
    id: 629a71e053a72d997d423fce
    type: status-change
  author: lucy3
  created_at: 2022-06-03 19:41:04+00:00
  id: 629a71e053a72d997d423fce
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: globuslabs/ScholarBERT_1
repo_type: model
status: closed
target_branch: null
title: Tokenization
