!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sslx
conflicting_files: null
created_at: 2023-05-13 10:02:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-13T11:02:49.000Z'
    data:
      edited: false
      editors:
      - sslx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
          fullname: Sean Lee
          isHf: false
          isPro: false
          name: sslx
          type: user
        html: '<p>Thanks for the conversion.<br>Just wondering if it''s mixed fp16
          or only fp16?<br>Would that make difference when fine tuning?</p>

          '
        raw: "Thanks for the conversion.\r\nJust wondering if it's mixed fp16 or only\
          \ fp16?\r\nWould that make difference when fine tuning?"
        updatedAt: '2023-05-13T11:02:49.220Z'
      numEdits: 0
      reactions: []
    id: 645f6e597c7bdadaa2d22a1b
    type: comment
  author: sslx
  content: "Thanks for the conversion.\r\nJust wondering if it's mixed fp16 or only\
    \ fp16?\r\nWould that make difference when fine tuning?"
  created_at: 2023-05-13 10:02:49+00:00
  edited: false
  hidden: false
  id: 645f6e597c7bdadaa2d22a1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T11:08:03.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>It should be only fp16.  It was converted from <span data-props=\"\
          {&quot;user&quot;:&quot;ehartford&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/ehartford\">@<span class=\"underline\">ehartford</span></a></span>\n\
          \n\t</span></span> 's original fp32 repo with this code:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> torch\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, LlamaForCausalLM\n<span class=\"hljs-keyword\"\
          >import</span> argparse\n<span class=\"hljs-keyword\">import</span> os\n\
          \n<span class=\"hljs-keyword\">import</span> os\n\nparser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Convert fp32 model to fp16'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'model_dir'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'fp32 model folder'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'output_dir'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'fp16 output folder'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--device'</span>, <span\
          \ class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>,\
          \ default=<span class=\"hljs-string\">\"cuda:0\"</span>, <span class=\"\
          hljs-built_in\">help</span>=<span class=\"hljs-string\">'device'</span>)\n\
          \nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\noutput_dir\
          \ = args.output_dir\n\nmodel = LlamaForCausalLM.from_pretrained(\n     \
          \       model_dir,\n            load_in_8bit=<span class=\"hljs-literal\"\
          >False</span>,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>,\n            device_map=<span class=\"\
          hljs-string\">'auto'</span>\n            )\n\nmodel.use_cache = <span class=\"\
          hljs-literal\">True</span>\n\nLlamaForCausalLM.save_pretrained(\n      \
          \      model, output_dir, torch_dtype=torch.float16\n            )\n</code></pre>\n"
        raw: "It should be only fp16.  It was converted from @ehartford 's original\
          \ fp32 repo with this code:\n\n```python\nimport torch\nfrom transformers\
          \ import AutoTokenizer, LlamaForCausalLM\nimport argparse\nimport os\n\n\
          import os\n\nparser = argparse.ArgumentParser(description='Convert fp32\
          \ model to fp16')\nparser.add_argument('model_dir', type=str, help='fp32\
          \ model folder')\nparser.add_argument('output_dir', type=str, help='fp16\
          \ output folder')\nparser.add_argument('--device', type=str, default=\"\
          cuda:0\", help='device')\n\nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\n\
          output_dir = args.output_dir\n\nmodel = LlamaForCausalLM.from_pretrained(\n\
          \            model_dir,\n            load_in_8bit=False,\n            torch_dtype=torch.float16,\n\
          \            low_cpu_mem_usage=True,\n            device_map='auto'\n  \
          \          )\n\nmodel.use_cache = True\n\nLlamaForCausalLM.save_pretrained(\n\
          \            model, output_dir, torch_dtype=torch.float16\n            )\n\
          ```"
        updatedAt: '2023-05-13T11:08:46.208Z'
      numEdits: 2
      reactions: []
    id: 645f6f936990e1208523ca51
    type: comment
  author: TheBloke
  content: "It should be only fp16.  It was converted from @ehartford 's original\
    \ fp32 repo with this code:\n\n```python\nimport torch\nfrom transformers import\
    \ AutoTokenizer, LlamaForCausalLM\nimport argparse\nimport os\n\nimport os\n\n\
    parser = argparse.ArgumentParser(description='Convert fp32 model to fp16')\nparser.add_argument('model_dir',\
    \ type=str, help='fp32 model folder')\nparser.add_argument('output_dir', type=str,\
    \ help='fp16 output folder')\nparser.add_argument('--device', type=str, default=\"\
    cuda:0\", help='device')\n\nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\n\
    output_dir = args.output_dir\n\nmodel = LlamaForCausalLM.from_pretrained(\n  \
    \          model_dir,\n            load_in_8bit=False,\n            torch_dtype=torch.float16,\n\
    \            low_cpu_mem_usage=True,\n            device_map='auto'\n        \
    \    )\n\nmodel.use_cache = True\n\nLlamaForCausalLM.save_pretrained(\n      \
    \      model, output_dir, torch_dtype=torch.float16\n            )\n```"
  created_at: 2023-05-13 10:08:03+00:00
  edited: true
  hidden: false
  id: 645f6f936990e1208523ca51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-13T11:26:06.000Z'
    data:
      edited: false
      editors:
      - sslx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
          fullname: Sean Lee
          isHf: false
          isPro: false
          name: sslx
          type: user
        html: '<p>Thanks so much for the info!<br>Original LLaMA 13b seems larger
          by 2.4GB, so the base  LLMA must be in mixed fp16.<br>It might make difference
          if you want to finetune this.<br>Thanks for the script. Probably I can modify
          to get mixed fp16.</p>

          '
        raw: 'Thanks so much for the info!

          Original LLaMA 13b seems larger by 2.4GB, so the base  LLMA must be in mixed
          fp16.

          It might make difference if you want to finetune this.

          Thanks for the script. Probably I can modify to get mixed fp16.'
        updatedAt: '2023-05-13T11:26:06.267Z'
      numEdits: 0
      reactions: []
    id: 645f73ce72397238b22d113a
    type: comment
  author: sslx
  content: 'Thanks so much for the info!

    Original LLaMA 13b seems larger by 2.4GB, so the base  LLMA must be in mixed fp16.

    It might make difference if you want to finetune this.

    Thanks for the script. Probably I can modify to get mixed fp16.'
  created_at: 2023-05-13 10:26:06+00:00
  edited: false
  hidden: false
  id: 645f73ce72397238b22d113a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T11:36:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, huh. I didn''t notice that.</p>

          <p>Is that going to be a problem? I can try saving it again, see if I can
          get it to save identically to how llama 13b saves.</p>

          '
        raw: 'Oh, huh. I didn''t notice that.


          Is that going to be a problem? I can try saving it again, see if I can get
          it to save identically to how llama 13b saves.'
        updatedAt: '2023-05-13T11:36:26.414Z'
      numEdits: 0
      reactions: []
    id: 645f763a72397238b22d2856
    type: comment
  author: TheBloke
  content: 'Oh, huh. I didn''t notice that.


    Is that going to be a problem? I can try saving it again, see if I can get it
    to save identically to how llama 13b saves.'
  created_at: 2023-05-13 10:36:26+00:00
  edited: false
  hidden: false
  id: 645f763a72397238b22d2856
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642babef48f67b6f21d5c917/z3nSlb-r82nECGNpkgBsv.png?w=200&h=200&f=face
      fullname: Bonanza Unthread
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neko-Institute-of-Science
      type: user
    createdAt: '2023-05-13T14:05:25.000Z'
    data:
      edited: true
      editors:
      - Neko-Institute-of-Science
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642babef48f67b6f21d5c917/z3nSlb-r82nECGNpkgBsv.png?w=200&h=200&f=face
          fullname: Bonanza Unthread
          isHf: false
          isPro: false
          name: Neko-Institute-of-Science
          type: user
        html: '<p>btw save can just be LlamaForCausalLM.save_pretrained(model, output_dir)
          and when you load the model use torch_dtype=torch.bfloat16 vs torch_dtype=torch.float16
          to save in mixed fp16.</p>

          '
        raw: btw save can just be LlamaForCausalLM.save_pretrained(model, output_dir)
          and when you load the model use torch_dtype=torch.bfloat16 vs torch_dtype=torch.float16
          to save in mixed fp16.
        updatedAt: '2023-05-13T14:09:25.284Z'
      numEdits: 3
      reactions: []
    id: 645f9925446a4fa469572a6f
    type: comment
  author: Neko-Institute-of-Science
  content: btw save can just be LlamaForCausalLM.save_pretrained(model, output_dir)
    and when you load the model use torch_dtype=torch.bfloat16 vs torch_dtype=torch.float16
    to save in mixed fp16.
  created_at: 2023-05-13 13:05:25+00:00
  edited: true
  hidden: false
  id: 645f9925446a4fa469572a6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T20:54:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Original LLaMA 13b seems larger by 2.4GB, so the base  LLMA must be in
          mixed fp16.<br>It might make difference if you want to finetune this.<br>Thanks
          for the script. Probably I can modify to get mixed fp16.</p>

          </blockquote>

          <p>Apologies! It turns out the model was truncated, which is why it was
          3GB short.  Something went wrong in the fp32 -&gt; fp16 script I used. I''m
          still debugging that so I can avoid it in future.</p>

          <p>I have just re-uploaded the model, and it is now the correct size for
          fp16.  So please re-download it.</p>

          <p>It wasn''t anything to do with mixed fp16 I believe. Just simply that
          some tensors were missing!</p>

          '
        raw: '> Original LLaMA 13b seems larger by 2.4GB, so the base  LLMA must be
          in mixed fp16.

          > It might make difference if you want to finetune this.

          > Thanks for the script. Probably I can modify to get mixed fp16.


          Apologies! It turns out the model was truncated, which is why it was 3GB
          short.  Something went wrong in the fp32 -> fp16 script I used. I''m still
          debugging that so I can avoid it in future.


          I have just re-uploaded the model, and it is now the correct size for fp16.  So
          please re-download it.


          It wasn''t anything to do with mixed fp16 I believe. Just simply that some
          tensors were missing!'
        updatedAt: '2023-05-13T20:54:17.141Z'
      numEdits: 0
      reactions: []
    id: 645ff8f925a4075bcf9dd72a
    type: comment
  author: TheBloke
  content: '> Original LLaMA 13b seems larger by 2.4GB, so the base  LLMA must be
    in mixed fp16.

    > It might make difference if you want to finetune this.

    > Thanks for the script. Probably I can modify to get mixed fp16.


    Apologies! It turns out the model was truncated, which is why it was 3GB short.  Something
    went wrong in the fp32 -> fp16 script I used. I''m still debugging that so I can
    avoid it in future.


    I have just re-uploaded the model, and it is now the correct size for fp16.  So
    please re-download it.


    It wasn''t anything to do with mixed fp16 I believe. Just simply that some tensors
    were missing!'
  created_at: 2023-05-13 19:54:17+00:00
  edited: false
  hidden: false
  id: 645ff8f925a4075bcf9dd72a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-14T05:36:36.000Z'
    data:
      edited: false
      editors:
      - sslx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
          fullname: Sean Lee
          isHf: false
          isPro: false
          name: sslx
          type: user
        html: '<p>Thanks again!<br>Is 4-bit also impacted by this?<br>Should I redownload
          4-bit as well?</p>

          '
        raw: 'Thanks again!

          Is 4-bit also impacted by this?

          Should I redownload 4-bit as well?'
        updatedAt: '2023-05-14T05:36:36.033Z'
      numEdits: 0
      reactions: []
    id: 64607364446a4fa4695ccf17
    type: comment
  author: sslx
  content: 'Thanks again!

    Is 4-bit also impacted by this?

    Should I redownload 4-bit as well?'
  created_at: 2023-05-14 04:36:36+00:00
  edited: false
  hidden: false
  id: 64607364446a4fa4695ccf17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T07:27:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, 4bit is fine. It was only the HF model that had issues</p>

          '
        raw: No, 4bit is fine. It was only the HF model that had issues
        updatedAt: '2023-05-14T07:27:46.273Z'
      numEdits: 0
      reactions: []
    id: 64608d72a3081fffa0587736
    type: comment
  author: TheBloke
  content: No, 4bit is fine. It was only the HF model that had issues
  created_at: 2023-05-14 06:27:46+00:00
  edited: false
  hidden: false
  id: 64608d72a3081fffa0587736
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-14T15:51:52.000Z'
    data:
      edited: false
      editors:
      - sslx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
          fullname: Sean Lee
          isHf: false
          isPro: false
          name: sslx
          type: user
        html: '<p>Great, thanks!</p>

          '
        raw: Great, thanks!
        updatedAt: '2023-05-14T15:51:52.958Z'
      numEdits: 0
      reactions: []
      relatedEventId: 646103987735f76a4a5054a6
    id: 646103987735f76a4a5054a5
    type: comment
  author: sslx
  content: Great, thanks!
  created_at: 2023-05-14 14:51:52+00:00
  edited: false
  hidden: false
  id: 646103987735f76a4a5054a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-14T15:51:52.000Z'
    data:
      status: closed
    id: 646103987735f76a4a5054a6
    type: status-change
  author: sslx
  created_at: 2023-05-14 14:51:52+00:00
  id: 646103987735f76a4a5054a6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-HF
repo_type: model
status: closed
target_branch: null
title: Is this fp16 or mixed fp16?
