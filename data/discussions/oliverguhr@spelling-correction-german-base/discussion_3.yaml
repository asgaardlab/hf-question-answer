!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WANGYIWEI
conflicting_files: null
created_at: 2024-01-19 15:53:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
      fullname: WYW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WANGYIWEI
      type: user
    createdAt: '2024-01-19T15:53:59.000Z'
    data:
      edited: false
      editors:
      - WANGYIWEI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.822361946105957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
          fullname: WYW
          isHf: false
          isPro: false
          name: WANGYIWEI
          type: user
        html: "<p>Servus Oliver :))</p>\n<p>Thanks very much for your great work.<br>I\
          \ checked your source code on Github and I found the CML you used to fine-tune\
          \ the German spell correction model:</p>\n<pre><code class=\"language-shell\"\
          >python run_summarization.py \\\n    --model_name_or_path facebook/mbart-large-50\
          \ \\\n    --do_train \\\n    --do_eval \\\n    --train_file de.train.csv\
          \ \\\n    --validation_file de.test.csv \\\n    --output_dir ./models/mbart-large-50-spelling-de/\
          \ \\\n    ......\n    --lang=\"de\"\n</code></pre>\n<p>I just wanted to\
          \ ask about the German training data, <code>de.train.csv</code>, as I am\
          \ interested in scaling up the base model a bit, with flan-t5-large. Would\
          \ you like to talk a little bit about the availability of the training data?</p>\n"
        raw: "Servus Oliver :))\r\n\r\nThanks very much for your great work.\r\nI\
          \ checked your source code on Github and I found the CML you used to fine-tune\
          \ the German spell correction model:\r\n\r\n```shell\r\npython run_summarization.py\
          \ \\\r\n    --model_name_or_path facebook/mbart-large-50 \\\r\n    --do_train\
          \ \\\r\n    --do_eval \\\r\n    --train_file de.train.csv \\\r\n    --validation_file\
          \ de.test.csv \\\r\n    --output_dir ./models/mbart-large-50-spelling-de/\
          \ \\\r\n    ......\r\n    --lang=\"de\"\r\n```\r\n\r\nI just wanted to ask\
          \ about the German training data, `de.train.csv`, as I am interested in\
          \ scaling up the base model a bit, with flan-t5-large. Would you like to\
          \ talk a little bit about the availability of the training data?"
        updatedAt: '2024-01-19T15:53:59.844Z'
      numEdits: 0
      reactions: []
    id: 65aa9b170844d9e0d6e7ec14
    type: comment
  author: WANGYIWEI
  content: "Servus Oliver :))\r\n\r\nThanks very much for your great work.\r\nI checked\
    \ your source code on Github and I found the CML you used to fine-tune the German\
    \ spell correction model:\r\n\r\n```shell\r\npython run_summarization.py \\\r\n\
    \    --model_name_or_path facebook/mbart-large-50 \\\r\n    --do_train \\\r\n\
    \    --do_eval \\\r\n    --train_file de.train.csv \\\r\n    --validation_file\
    \ de.test.csv \\\r\n    --output_dir ./models/mbart-large-50-spelling-de/ \\\r\
    \n    ......\r\n    --lang=\"de\"\r\n```\r\n\r\nI just wanted to ask about the\
    \ German training data, `de.train.csv`, as I am interested in scaling up the base\
    \ model a bit, with flan-t5-large. Would you like to talk a little bit about the\
    \ availability of the training data?"
  created_at: 2024-01-19 15:53:59+00:00
  edited: false
  hidden: false
  id: 65aa9b170844d9e0d6e7ec14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
      fullname: Oliver Guhr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oliverguhr
      type: user
    createdAt: '2024-01-22T12:06:01.000Z'
    data:
      edited: true
      editors:
      - oliverguhr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6852936148643494
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ea841a70d1df220780f0433/rpEmnBmuIImkp2yo_pzfO.jpeg?w=200&h=200&f=face
          fullname: Oliver Guhr
          isHf: false
          isPro: false
          name: oliverguhr
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;WANGYIWEI&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/WANGYIWEI\"\
          >@<span class=\"underline\">WANGYIWEI</span></a></span>\n\n\t</span></span>,<br>basically,\
          \ I took the training  data from <a rel=\"nofollow\" href=\"https://wortschatz.uni-leipzig.de/en/download/\"\
          >Leipzig Corpora Collection</a>. They provide clean sentences in different\
          \ languages for various languages. I placed the files that I used in <a\
          \ rel=\"nofollow\" href=\"https://github.com/oliverguhr/spelling/tree/main/data/raw\"\
          >this folder</a>. If you like to scale up the model, you can add there more\
          \ files from the Leipzig Corpora or other sources.</p>\n<p>The process to\
          \ create a data set in the format of the \"de.train.csv\" file:</p>\n<ol>\n\
          <li>Place text files for your language into \"data/raw\" folder</li>\n<li>Combine\
          \ the single files into one big file <a rel=\"nofollow\" href=\"https://github.com/oliverguhr/spelling/blob/main/convert_leipzig_data.sh\"\
          >with this script</a>.</li>\n<li>Run the <a rel=\"nofollow\" href=\"https://github.com/oliverguhr/spelling/blob/main/generate_dataset.py\"\
          >generate_dataset.py</a> to generate the training file. You have the adjust\
          \ the path and language in the <a rel=\"nofollow\" href=\"https://github.com/oliverguhr/spelling/blob/5bd15e09155ea20e0389cc6a4b6e4a95cdacfd08/generate_dataset.py#L93\"\
          >python script in this line</a>.</li>\n</ol>\n<p>Please let me know if you\
          \ have some other questions or some results to share :)</p>\n<p>Best,<br>Oliver</p>\n"
        raw: "Hey @WANGYIWEI,\nbasically, I took the training  data from [Leipzig\
          \ Corpora Collection](https://wortschatz.uni-leipzig.de/en/download/). They\
          \ provide clean sentences in different languages for various languages.\
          \ I placed the files that I used in [this folder](https://github.com/oliverguhr/spelling/tree/main/data/raw).\
          \ If you like to scale up the model, you can add there more files from the\
          \ Leipzig Corpora or other sources.\n\nThe process to create a data set\
          \ in the format of the \"de.train.csv\" file:\n\n1. Place text files for\
          \ your language into \"data/raw\" folder\n2. Combine the single files into\
          \ one big file [with this script](https://github.com/oliverguhr/spelling/blob/main/convert_leipzig_data.sh).\n\
          3. Run the [generate_dataset.py](https://github.com/oliverguhr/spelling/blob/main/generate_dataset.py)\
          \ to generate the training file. You have the adjust the path and language\
          \ in the [python script in this line](https://github.com/oliverguhr/spelling/blob/5bd15e09155ea20e0389cc6a4b6e4a95cdacfd08/generate_dataset.py#L93).\
          \ \n\nPlease let me know if you have some other questions or some results\
          \ to share :)\n\nBest,\nOliver"
        updatedAt: '2024-01-22T12:06:39.289Z'
      numEdits: 1
      reactions: []
    id: 65ae5a291f418c7449bfc78e
    type: comment
  author: oliverguhr
  content: "Hey @WANGYIWEI,\nbasically, I took the training  data from [Leipzig Corpora\
    \ Collection](https://wortschatz.uni-leipzig.de/en/download/). They provide clean\
    \ sentences in different languages for various languages. I placed the files that\
    \ I used in [this folder](https://github.com/oliverguhr/spelling/tree/main/data/raw).\
    \ If you like to scale up the model, you can add there more files from the Leipzig\
    \ Corpora or other sources.\n\nThe process to create a data set in the format\
    \ of the \"de.train.csv\" file:\n\n1. Place text files for your language into\
    \ \"data/raw\" folder\n2. Combine the single files into one big file [with this\
    \ script](https://github.com/oliverguhr/spelling/blob/main/convert_leipzig_data.sh).\n\
    3. Run the [generate_dataset.py](https://github.com/oliverguhr/spelling/blob/main/generate_dataset.py)\
    \ to generate the training file. You have the adjust the path and language in\
    \ the [python script in this line](https://github.com/oliverguhr/spelling/blob/5bd15e09155ea20e0389cc6a4b6e4a95cdacfd08/generate_dataset.py#L93).\
    \ \n\nPlease let me know if you have some other questions or some results to share\
    \ :)\n\nBest,\nOliver"
  created_at: 2024-01-22 12:06:01+00:00
  edited: true
  hidden: false
  id: 65ae5a291f418c7449bfc78e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
      fullname: WYW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WANGYIWEI
      type: user
    createdAt: '2024-01-24T15:50:00.000Z'
    data:
      edited: true
      editors:
      - WANGYIWEI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9238964915275574
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg?w=200&h=200&f=face
          fullname: WYW
          isHf: false
          isPro: false
          name: WANGYIWEI
          type: user
        html: '<p>Hi Oliver,</p>

          <p>Thanks for the feedback. I have slightly modified your script locally
          to make it more suitable for generating German data. It worked perfectly
          well and spared a lot of time to do the data engineering job. I will start
          the first round optimisation experiment by the end of this week with flan-t5-large.</p>

          <p>Also regarding the corruption methods I might have something interesting
          to share. I have worked on GEC previously, yet for English grammar error
          detection (binary classification). So, I have utilised <a rel="nofollow"
          href="https://blog.research.google/2021/08/the-c4200m-synthetic-dataset-for.html">Google''s
          C4_200M Synthetic Dataset</a> to train the classifier. They introduced some
          linguistic methods to better reflect the true distribution of error types
          from actual users. And I think it''s really logical and practical. </p>

          <p>Therefore, I might work in this small direction if it doesn''t take too
          much effort and will keep you updated.</p>

          <p>Best regards,<br>Yiwei Wang</p>

          '
        raw: "Hi Oliver,\n\nThanks for the feedback. I have slightly modified your\
          \ script locally to make it more suitable for generating German data. It\
          \ worked perfectly well and spared a lot of time to do the data engineering\
          \ job. I will start the first round optimisation experiment by the end of\
          \ this week with flan-t5-large.\n\nAlso regarding the corruption methods\
          \ I might have something interesting to share. I have worked on GEC previously,\
          \ yet for English grammar error detection (binary classification). So, I\
          \ have utilised [Google's C4_200M Synthetic Dataset](https://blog.research.google/2021/08/the-c4200m-synthetic-dataset-for.html)\
          \ to train the classifier. They introduced some linguistic methods to better\
          \ reflect the true distribution of error types from actual users. And I\
          \ think it's really logical and practical. \n\nTherefore, I might work in\
          \ this small direction if it doesn't take too much effort and will keep\
          \ you updated.\n\nBest regards,\nYiwei Wang"
        updatedAt: '2024-01-24T15:50:30.282Z'
      numEdits: 1
      reactions: []
    id: 65b131a8f7638a13a638ce52
    type: comment
  author: WANGYIWEI
  content: "Hi Oliver,\n\nThanks for the feedback. I have slightly modified your script\
    \ locally to make it more suitable for generating German data. It worked perfectly\
    \ well and spared a lot of time to do the data engineering job. I will start the\
    \ first round optimisation experiment by the end of this week with flan-t5-large.\n\
    \nAlso regarding the corruption methods I might have something interesting to\
    \ share. I have worked on GEC previously, yet for English grammar error detection\
    \ (binary classification). So, I have utilised [Google's C4_200M Synthetic Dataset](https://blog.research.google/2021/08/the-c4200m-synthetic-dataset-for.html)\
    \ to train the classifier. They introduced some linguistic methods to better reflect\
    \ the true distribution of error types from actual users. And I think it's really\
    \ logical and practical. \n\nTherefore, I might work in this small direction if\
    \ it doesn't take too much effort and will keep you updated.\n\nBest regards,\n\
    Yiwei Wang"
  created_at: 2024-01-24 15:50:00+00:00
  edited: true
  hidden: false
  id: 65b131a8f7638a13a638ce52
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: oliverguhr/spelling-correction-german-base
repo_type: model
status: open
target_branch: null
title: German Training Data
