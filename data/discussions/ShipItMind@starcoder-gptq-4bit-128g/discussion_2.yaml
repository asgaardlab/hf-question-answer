!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abhimortal6
conflicting_files: null
created_at: 2023-06-04 18:40:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd4417407bdf732a8c6ead79ba9b904.svg
      fullname: ' Abhi Tripathi'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhimortal6
      type: user
    createdAt: '2023-06-04T19:40:32.000Z'
    data:
      edited: false
      editors:
      - abhimortal6
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4953528940677643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcd4417407bdf732a8c6ead79ba9b904.svg
          fullname: ' Abhi Tripathi'
          isHf: false
          isPro: false
          name: abhimortal6
          type: user
        html: '<p>tried with AutoGPTQ</p>

          <pre><code>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,trust_remote_code=True)

          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)

          pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)

          print(pipeline("hello, who are you")[0]["generated_text"])

          </code></pre>

          <p>full log</p>

          <p><code>GPTBigCodeGPTQForCausalLM hasn''t fused attention module yet, will
          skip inject fused attention. GPTBigCodeGPTQForCausalLM hasn''t fused mlp
          module yet, will skip inject fused mlp. Xformers is not installed correctly.
          If you want to use memory_efficient_attention to accelerate training use
          the following command to install Xformers pip install xformers. The model
          ''GPTBigCodeGPTQForCausalLM'' is not supported for . Supported models are
          [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'', ''BigBirdForCausalLM'',
          ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'', ''BlenderbotForCausalLM'',
          ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'', ''CamembertForCausalLM'',
          ''CodeGenForCausalLM'', ''CpmAntForCausalLM'', ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'',
          ''ElectraForCausalLM'', ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'',
          ''GPT2LMHeadModel'', ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM'']. Setting `pad_token_id` to `eos_token_id`:0
          for open-end generation. E:\ML\Llama\installer_files\env\lib\site-packages\transformers\generation\utils.py:1349:
          UserWarning: Using `max_length`''s default (20) to control the generation
          length. This behaviour is deprecated and will be removed from the config
          in v5 of Transformers -- we recommend using `max_new_tokens` to control
          the maximum length of the generation.   warnings.warn( hello, who are you?
          # # # # #</code></p>

          '
        raw: "tried with AutoGPTQ\r\n\r\n```\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,trust_remote_code=True)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\npipeline\
          \ = TextGenerationPipeline(model=model, tokenizer=tokenizer)\r\nprint(pipeline(\"\
          hello, who are you\")[0][\"generated_text\"])\r\n```\r\n\r\n\r\nfull log\r\
          \n\r\n```GPTBigCodeGPTQForCausalLM hasn't fused attention module yet, will\
          \ skip inject fused attention.\r\nGPTBigCodeGPTQForCausalLM hasn't fused\
          \ mlp module yet, will skip inject fused mlp.\r\nXformers is not installed\
          \ correctly. If you want to use memory_efficient_attention to accelerate\
          \ training use the following command to install Xformers\r\npip install\
          \ xformers.\r\nThe model 'GPTBigCodeGPTQForCausalLM' is not supported for\
          \ . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\r\nSetting `pad_token_id` to `eos_token_id`:0 for\
          \ open-end generation.\r\nE:\\ML\\Llama\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py:1349: UserWarning: Using `max_length`'s\
          \ default (20) to control the generation length. This behaviour is deprecated\
          \ and will be removed from the config in v5 of Transformers -- we recommend\
          \ using `max_new_tokens` to control the maximum length of the generation.\r\
          \n  warnings.warn(\r\nhello, who are you?\r\n#\r\n#\r\n#\r\n#\r\n#```"
        updatedAt: '2023-06-04T19:40:32.846Z'
      numEdits: 0
      reactions: []
    id: 647ce8b01f878439e201009d
    type: comment
  author: abhimortal6
  content: "tried with AutoGPTQ\r\n\r\n```\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,trust_remote_code=True)\r\
    \ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\npipeline =\
    \ TextGenerationPipeline(model=model, tokenizer=tokenizer)\r\nprint(pipeline(\"\
    hello, who are you\")[0][\"generated_text\"])\r\n```\r\n\r\n\r\nfull log\r\n\r\
    \n```GPTBigCodeGPTQForCausalLM hasn't fused attention module yet, will skip inject\
    \ fused attention.\r\nGPTBigCodeGPTQForCausalLM hasn't fused mlp module yet, will\
    \ skip inject fused mlp.\r\nXformers is not installed correctly. If you want to\
    \ use memory_efficient_attention to accelerate training use the following command\
    \ to install Xformers\r\npip install xformers.\r\nThe model 'GPTBigCodeGPTQForCausalLM'\
    \ is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
    \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
    \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
    \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
    \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
    \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
    \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
    \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
    \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
    \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM',\
    \ 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM',\
    \ 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM',\
    \ 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM',\
    \ 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\nSetting\
    \ `pad_token_id` to `eos_token_id`:0 for open-end generation.\r\nE:\\ML\\Llama\\\
    installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py:1349:\
    \ UserWarning: Using `max_length`'s default (20) to control the generation length.\
    \ This behaviour is deprecated and will be removed from the config in v5 of Transformers\
    \ -- we recommend using `max_new_tokens` to control the maximum length of the\
    \ generation.\r\n  warnings.warn(\r\nhello, who are you?\r\n#\r\n#\r\n#\r\n#\r\
    \n#```"
  created_at: 2023-06-04 18:40:32+00:00
  edited: false
  hidden: false
  id: 647ce8b01f878439e201009d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ShipItMind/starcoder-gptq-4bit-128g
repo_type: model
status: open
target_branch: null
title: Add Readme for loading model with test usage
