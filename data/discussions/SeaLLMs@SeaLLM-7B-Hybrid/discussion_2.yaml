!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tanyongkeat
conflicting_files: null
created_at: 2023-12-08 10:37:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e584bfe2f36a04e8e5e69cddfab020bc.svg
      fullname: Tan Yong Keat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanyongkeat
      type: user
    createdAt: '2023-12-08T10:37:33.000Z'
    data:
      edited: true
      editors:
      - tanyongkeat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8516539931297302
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e584bfe2f36a04e8e5e69cddfab020bc.svg
          fullname: Tan Yong Keat
          isHf: false
          isPro: false
          name: tanyongkeat
          type: user
        html: "<p>Running <code>AutoModel.from_pretrained(\"SeaLLMs/SeaLLM-7B-Hybrid\"\
          )</code> gets the following error messages:</p>\n<pre><code>File /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:566,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n\
          \    565     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 566     return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n  \
          \  569 raise ValueError(\n    570     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    571     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    572 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3480,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   3471    \
          \ if dtype_orig is not None:\n   3472         torch.set_default_dtype(dtype_orig)\n\
          \   3473     (\n   3474         model,\n   3475         missing_keys,\n\
          \   3476         unexpected_keys,\n   3477         mismatched_keys,\n  \
          \ 3478         offload_index,\n   3479         error_msgs,\n-&gt; 3480 \
          \    ) = cls._load_pretrained_model(\n   3481         model,\n   3482  \
          \       state_dict,\n   3483         loaded_state_dict_keys,  # XXX: rename?\n\
          \   3484         resolved_archive_file,\n   3485         pretrained_model_name_or_path,\n\
          \   3486         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3487\
          \         sharded_metadata=sharded_metadata,\n   3488         _fast_init=_fast_init,\n\
          \   3489         low_cpu_mem_usage=low_cpu_mem_usage,\n   3490         device_map=device_map,\n\
          \   3491         offload_folder=offload_folder,\n   3492         offload_state_dict=offload_state_dict,\n\
          \   3493         dtype=torch_dtype,\n   3494         is_quantized=(getattr(model,\
          \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n\
          \   3495         keep_in_fp32_modules=keep_in_fp32_modules,\n   3496   \
          \  )\n   3498 model.is_loaded_in_4bit = load_in_4bit\n   3499 model.is_loaded_in_8bit\
          \ = load_in_8bit\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3856,\
          \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   3854\
          \ if shard_file in disk_only_shard_files:\n   3855     continue\n-&gt; 3856\
          \ state_dict = load_state_dict(shard_file)\n   3858 # Mistmatched keys contains\
          \ tuples key/shape1/shape2 of weights in the checkpoint that have a shape\
          \ not\n   3859 # matching the weights in the model.\n   3860 mismatched_keys\
          \ += _find_mismatched_keys(\n   3861     state_dict,\n   3862     model_state_dict,\n\
          \   (...)\n   3866     ignore_mismatched_sizes,\n   3867 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:469,\
          \ in load_state_dict(checkpoint_file)\n    467 with safe_open(checkpoint_file,\
          \ framework=\"pt\") as f:\n    468     metadata = f.metadata()\n--&gt; 469\
          \ if metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\"]:\n    470\
          \     raise OSError(\n    471         f\"The safetensors archive passed\
          \ at {checkpoint_file} does not contain the valid metadata. Make sure \"\
          \n    472         \"you save your model with the `save_pretrained` method.\"\
          \n    473     )\n    474 return safe_load_file(checkpoint_file)\n\nAttributeError:\
          \ 'NoneType' object has no attribute 'get'\n</code></pre>\n<p>Saw another\
          \ user with the same problem (<a href=\"https://huggingface.co/SeaLLMs/SeaLLM-7B-Hybrid/discussions/1#6571a97563aa14eb56fc6f73\"\
          >https://huggingface.co/SeaLLMs/SeaLLM-7B-Hybrid/discussions/1#6571a97563aa14eb56fc6f73</a>),\
          \ but the thread was close, so I reopened it here. </p>\n"
        raw: "Running `AutoModel.from_pretrained(\"SeaLLMs/SeaLLM-7B-Hybrid\")` gets\
          \ the following error messages:\n```\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:566,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n\
          \    565     model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 566     return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n  \
          \  569 raise ValueError(\n    570     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    571     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    572 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3480,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   3471    \
          \ if dtype_orig is not None:\n   3472         torch.set_default_dtype(dtype_orig)\n\
          \   3473     (\n   3474         model,\n   3475         missing_keys,\n\
          \   3476         unexpected_keys,\n   3477         mismatched_keys,\n  \
          \ 3478         offload_index,\n   3479         error_msgs,\n-> 3480    \
          \ ) = cls._load_pretrained_model(\n   3481         model,\n   3482     \
          \    state_dict,\n   3483         loaded_state_dict_keys,  # XXX: rename?\n\
          \   3484         resolved_archive_file,\n   3485         pretrained_model_name_or_path,\n\
          \   3486         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3487\
          \         sharded_metadata=sharded_metadata,\n   3488         _fast_init=_fast_init,\n\
          \   3489         low_cpu_mem_usage=low_cpu_mem_usage,\n   3490         device_map=device_map,\n\
          \   3491         offload_folder=offload_folder,\n   3492         offload_state_dict=offload_state_dict,\n\
          \   3493         dtype=torch_dtype,\n   3494         is_quantized=(getattr(model,\
          \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n\
          \   3495         keep_in_fp32_modules=keep_in_fp32_modules,\n   3496   \
          \  )\n   3498 model.is_loaded_in_4bit = load_in_4bit\n   3499 model.is_loaded_in_8bit\
          \ = load_in_8bit\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3856,\
          \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   3854\
          \ if shard_file in disk_only_shard_files:\n   3855     continue\n-> 3856\
          \ state_dict = load_state_dict(shard_file)\n   3858 # Mistmatched keys contains\
          \ tuples key/shape1/shape2 of weights in the checkpoint that have a shape\
          \ not\n   3859 # matching the weights in the model.\n   3860 mismatched_keys\
          \ += _find_mismatched_keys(\n   3861     state_dict,\n   3862     model_state_dict,\n\
          \   (...)\n   3866     ignore_mismatched_sizes,\n   3867 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:469,\
          \ in load_state_dict(checkpoint_file)\n    467 with safe_open(checkpoint_file,\
          \ framework=\"pt\") as f:\n    468     metadata = f.metadata()\n--> 469\
          \ if metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\"]:\n    470\
          \     raise OSError(\n    471         f\"The safetensors archive passed\
          \ at {checkpoint_file} does not contain the valid metadata. Make sure \"\
          \n    472         \"you save your model with the `save_pretrained` method.\"\
          \n    473     )\n    474 return safe_load_file(checkpoint_file)\n\nAttributeError:\
          \ 'NoneType' object has no attribute 'get'\n```\n\nSaw another user with\
          \ the same problem (https://huggingface.co/SeaLLMs/SeaLLM-7B-Hybrid/discussions/1#6571a97563aa14eb56fc6f73),\
          \ but the thread was close, so I reopened it here. "
        updatedAt: '2023-12-08T10:47:23.976Z'
      numEdits: 3
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - dreamerdeo
        - rin2401
    id: 6572f1eda5f2213c55d668a1
    type: comment
  author: tanyongkeat
  content: "Running `AutoModel.from_pretrained(\"SeaLLMs/SeaLLM-7B-Hybrid\")` gets\
    \ the following error messages:\n```\nFile /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:566,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n    565\
    \     model_class = _get_model_class(config, cls._model_mapping)\n--> 566    \
    \ return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n    569 raise\
    \ ValueError(\n    570     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    571     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \n    572 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3480,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\n   3471     if dtype_orig\
    \ is not None:\n   3472         torch.set_default_dtype(dtype_orig)\n   3473 \
    \    (\n   3474         model,\n   3475         missing_keys,\n   3476       \
    \  unexpected_keys,\n   3477         mismatched_keys,\n   3478         offload_index,\n\
    \   3479         error_msgs,\n-> 3480     ) = cls._load_pretrained_model(\n  \
    \ 3481         model,\n   3482         state_dict,\n   3483         loaded_state_dict_keys,\
    \  # XXX: rename?\n   3484         resolved_archive_file,\n   3485         pretrained_model_name_or_path,\n\
    \   3486         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3487   \
    \      sharded_metadata=sharded_metadata,\n   3488         _fast_init=_fast_init,\n\
    \   3489         low_cpu_mem_usage=low_cpu_mem_usage,\n   3490         device_map=device_map,\n\
    \   3491         offload_folder=offload_folder,\n   3492         offload_state_dict=offload_state_dict,\n\
    \   3493         dtype=torch_dtype,\n   3494         is_quantized=(getattr(model,\
    \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n   3495\
    \         keep_in_fp32_modules=keep_in_fp32_modules,\n   3496     )\n   3498 model.is_loaded_in_4bit\
    \ = load_in_4bit\n   3499 model.is_loaded_in_8bit = load_in_8bit\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3856,\
    \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
    \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
    \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
    \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   3854 if shard_file\
    \ in disk_only_shard_files:\n   3855     continue\n-> 3856 state_dict = load_state_dict(shard_file)\n\
    \   3858 # Mistmatched keys contains tuples key/shape1/shape2 of weights in the\
    \ checkpoint that have a shape not\n   3859 # matching the weights in the model.\n\
    \   3860 mismatched_keys += _find_mismatched_keys(\n   3861     state_dict,\n\
    \   3862     model_state_dict,\n   (...)\n   3866     ignore_mismatched_sizes,\n\
    \   3867 )\n\nFile /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:469,\
    \ in load_state_dict(checkpoint_file)\n    467 with safe_open(checkpoint_file,\
    \ framework=\"pt\") as f:\n    468     metadata = f.metadata()\n--> 469 if metadata.get(\"\
    format\") not in [\"pt\", \"tf\", \"flax\"]:\n    470     raise OSError(\n   \
    \ 471         f\"The safetensors archive passed at {checkpoint_file} does not\
    \ contain the valid metadata. Make sure \"\n    472         \"you save your model\
    \ with the `save_pretrained` method.\"\n    473     )\n    474 return safe_load_file(checkpoint_file)\n\
    \nAttributeError: 'NoneType' object has no attribute 'get'\n```\n\nSaw another\
    \ user with the same problem (https://huggingface.co/SeaLLMs/SeaLLM-7B-Hybrid/discussions/1#6571a97563aa14eb56fc6f73),\
    \ but the thread was close, so I reopened it here. "
  created_at: 2023-12-08 10:37:33+00:00
  edited: true
  hidden: false
  id: 6572f1eda5f2213c55d668a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acf4e9e0204a7ff7445aecc4102700cd.svg
      fullname: Phi Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nxphi47
      type: user
    createdAt: '2023-12-08T11:02:43.000Z'
    data:
      edited: false
      editors:
      - nxphi47
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9655997157096863
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acf4e9e0204a7ff7445aecc4102700cd.svg
          fullname: Phi Nguyen
          isHf: false
          isPro: false
          name: nxphi47
          type: user
        html: '<p>Thanks for reporting. I will put the metadata and upload a fix.
          In the meantime, you can overwrite metadata as torch to bypass this. </p>

          '
        raw: 'Thanks for reporting. I will put the metadata and upload a fix. In the
          meantime, you can overwrite metadata as torch to bypass this. '
        updatedAt: '2023-12-08T11:02:43.565Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - tanyongkeat
        - BlackB
    id: 6572f7d328b466f4cd297677
    type: comment
  author: nxphi47
  content: 'Thanks for reporting. I will put the metadata and upload a fix. In the
    meantime, you can overwrite metadata as torch to bypass this. '
  created_at: 2023-12-08 11:02:43+00:00
  edited: false
  hidden: false
  id: 6572f7d328b466f4cd297677
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e584bfe2f36a04e8e5e69cddfab020bc.svg
      fullname: Tan Yong Keat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanyongkeat
      type: user
    createdAt: '2023-12-10T02:24:04.000Z'
    data:
      edited: false
      editors:
      - tanyongkeat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7644376158714294
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e584bfe2f36a04e8e5e69cddfab020bc.svg
          fullname: Tan Yong Keat
          isHf: false
          isPro: false
          name: tanyongkeat
          type: user
        html: "<p>Thank you so much for the prompt reply!</p>\n<p>Meanwhile, we can\
          \ solve this by doing this on the 2 safetensors files</p>\n<pre><code>import\
          \ safetensors\nfrom safetensors.torch import save_file\n\ntensors = dict()\n\
          with safetensors.safe_open(safetensors_path, framework=\"pt\") as f:\n \
          \   for key in f.keys():\n        tensors[key] = f.get_tensor(key)\n\nsave_file(tensors,\
          \ safetensors_path, metadata={'format': 'pt'})\n</code></pre>\n"
        raw: "Thank you so much for the prompt reply!\n\nMeanwhile, we can solve this\
          \ by doing this on the 2 safetensors files\n```\nimport safetensors\nfrom\
          \ safetensors.torch import save_file\n\ntensors = dict()\nwith safetensors.safe_open(safetensors_path,\
          \ framework=\"pt\") as f:\n    for key in f.keys():\n        tensors[key]\
          \ = f.get_tensor(key)\n\nsave_file(tensors, safetensors_path, metadata={'format':\
          \ 'pt'})\n```"
        updatedAt: '2023-12-10T02:24:04.866Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - nxphi47
    id: 65752144412ee70185d49ff5
    type: comment
  author: tanyongkeat
  content: "Thank you so much for the prompt reply!\n\nMeanwhile, we can solve this\
    \ by doing this on the 2 safetensors files\n```\nimport safetensors\nfrom safetensors.torch\
    \ import save_file\n\ntensors = dict()\nwith safetensors.safe_open(safetensors_path,\
    \ framework=\"pt\") as f:\n    for key in f.keys():\n        tensors[key] = f.get_tensor(key)\n\
    \nsave_file(tensors, safetensors_path, metadata={'format': 'pt'})\n```"
  created_at: 2023-12-10 02:24:04+00:00
  edited: false
  hidden: false
  id: 65752144412ee70185d49ff5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: SeaLLMs/SeaLLM-7B-Hybrid
repo_type: model
status: open
target_branch: null
title: Seems like metadata is not in the safetensors files
