!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phi0112358
conflicting_files: null
created_at: 2023-05-04 09:29:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
      fullname: Yazan Agha-Schrader
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phi0112358
      type: user
    createdAt: '2023-05-04T10:29:45.000Z'
    data:
      edited: false
      editors:
      - phi0112358
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
          fullname: Yazan Agha-Schrader
          isHf: false
          isPro: false
          name: phi0112358
          type: user
        html: "<p>Is it possible to convert this into ggml format and run it with\
          \ llama.cpp? I am wondering if this will work, since the base should be\
          \ a llama architecture? \U0001F928 </p>\n"
        raw: "Is it possible to convert this into ggml format and run it with llama.cpp?\
          \ I am wondering if this will work, since the base should be a llama architecture?\
          \ \U0001F928 "
        updatedAt: '2023-05-04T10:29:45.232Z'
      numEdits: 0
      reactions: []
    id: 64538919d51ecce6bfc83ddc
    type: comment
  author: phi0112358
  content: "Is it possible to convert this into ggml format and run it with llama.cpp?\
    \ I am wondering if this will work, since the base should be a llama architecture?\
    \ \U0001F928 "
  created_at: 2023-05-04 09:29:45+00:00
  edited: false
  hidden: false
  id: 64538919d51ecce6bfc83ddc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-05-04T10:34:21.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: "<p>Give it some time, I think <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ will do it soon</p>\n"
        raw: Give it some time, I think @TheBloke will do it soon
        updatedAt: '2023-05-04T10:34:21.954Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - junelee
    id: 64538a2dc2ebaa255d81a00c
    type: comment
  author: CyberTimon
  content: Give it some time, I think @TheBloke will do it soon
  created_at: 2023-05-04 09:34:21+00:00
  edited: false
  hidden: false
  id: 64538a2dc2ebaa255d81a00c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-04T20:37:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve uploaded the following repos:</p>

          <ul>

          <li><a href="https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ">4bit
          GPTQ models for GPU inference</a>.</li>

          <li><a href="https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML">4bit
          and 5bit GGML models for CPU inference</a>.</li>

          <li><a href="https://huggingface.co/TheBloke/wizard-vicuna-13B-HF">float16
          HF format model for GPU inference</a>.</li>

          </ul>

          '
        raw: 'I''ve uploaded the following repos:


          * [4bit GPTQ models for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ).

          * [4bit and 5bit GGML models for CPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML).

          * [float16 HF format model for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-HF).'
        updatedAt: '2023-05-04T20:37:03.644Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - junelee
        - thefaheem
        - ehartford
        - Okki
        - froilo
        - sandorkonya
        - Ilianos
    id: 6454176fb8c58783d66b2ba9
    type: comment
  author: TheBloke
  content: 'I''ve uploaded the following repos:


    * [4bit GPTQ models for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ).

    * [4bit and 5bit GGML models for CPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML).

    * [float16 HF format model for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-HF).'
  created_at: 2023-05-04 19:37:03+00:00
  edited: false
  hidden: false
  id: 6454176fb8c58783d66b2ba9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7293a00d4da46bfa9e9f4274b3a55e0d.svg
      fullname: Deng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jiachuan94
      type: user
    createdAt: '2023-06-08T01:08:51.000Z'
    data:
      edited: false
      editors:
      - Jiachuan94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8436610102653503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7293a00d4da46bfa9e9f4274b3a55e0d.svg
          fullname: Deng
          isHf: false
          isPro: false
          name: Jiachuan94
          type: user
        html: "<blockquote>\n<p>I've uploaded the following repos:</p>\n<ul>\n<li><a\
          \ href=\"https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ\">4bit GPTQ\
          \ models for GPU inference</a>.</li>\n<li><a href=\"https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML\"\
          >4bit and 5bit GGML models for CPU inference</a>.</li>\n<li><a href=\"https://huggingface.co/TheBloke/wizard-vicuna-13B-HF\"\
          >float16 HF format model for GPU inference</a>.</li>\n</ul>\n</blockquote>\n\
          <p>hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> ,<br>thank you\
          \ so much for sharing the converted GGML versions. But may I ask if you\
          \ can also share the script how is conversion done from huggingface style\
          \ model to GGML? I have searched a lot, but didn't find anything helpful.</p>\n"
        raw: "> I've uploaded the following repos:\n> \n> * [4bit GPTQ models for\
          \ GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ).\n\
          > * [4bit and 5bit GGML models for CPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML).\n\
          > * [float16 HF format model for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-HF).\n\
          \nhello @TheBloke ,\nthank you so much for sharing the converted GGML versions.\
          \ But may I ask if you can also share the script how is conversion done\
          \ from huggingface style model to GGML? I have searched a lot, but didn't\
          \ find anything helpful."
        updatedAt: '2023-06-08T01:08:51.400Z'
      numEdits: 0
      reactions: []
    id: 64812a239aafd41918b2fa2e
    type: comment
  author: Jiachuan94
  content: "> I've uploaded the following repos:\n> \n> * [4bit GPTQ models for GPU\
    \ inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ).\n> * [4bit\
    \ and 5bit GGML models for CPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML).\n\
    > * [float16 HF format model for GPU inference](https://huggingface.co/TheBloke/wizard-vicuna-13B-HF).\n\
    \nhello @TheBloke ,\nthank you so much for sharing the converted GGML versions.\
    \ But may I ask if you can also share the script how is conversion done from huggingface\
    \ style model to GGML? I have searched a lot, but didn't find anything helpful."
  created_at: 2023-06-08 00:08:51+00:00
  edited: false
  hidden: false
  id: 64812a239aafd41918b2fa2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:27:13.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22542347013950348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Here's a Python script that I used in the past to make GGMLs</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-comment\">#!/usr/bin/env\
          \ python3</span>\n<span class=\"hljs-keyword\">import</span> argparse\n\
          <span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\"\
          >import</span> subprocess\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">main</span>(<span class=\"hljs-params\"\
          >model, outbase, outdir</span>):\n    llamabase = <span class=\"hljs-string\"\
          >\"/workspace/venv/git/llama.cpp\"</span>\n    ggml_version = <span class=\"\
          hljs-string\">\"v3\"</span>\n\n    <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-keyword\">not</span> os.path.isdir(model):\n      \
          \  <span class=\"hljs-keyword\">raise</span> Exception(<span class=\"hljs-string\"\
          >f\"Could not find model dir at <span class=\"hljs-subst\">{model}</span>\"\
          </span>)\n\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> os.path.isfile(<span class=\"hljs-string\">f\"<span class=\"\
          hljs-subst\">{model}</span>/config.json\"</span>):\n        <span class=\"\
          hljs-keyword\">raise</span> Exception(<span class=\"hljs-string\">f\"Could\
          \ not find config.json in <span class=\"hljs-subst\">{model}</span>\"</span>)\n\
          \n    os.makedirs(outdir, exist_ok=<span class=\"hljs-literal\">True</span>)\n\
          \n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Building llama.cpp\"</span>)\n    subprocess.run(<span class=\"hljs-string\"\
          >f\"cd <span class=\"hljs-subst\">{llamabase}</span> &amp;&amp; git pull\
          \ &amp;&amp; make clean &amp;&amp; LLAMA_CUBLAS=1 make\"</span>, shell=<span\
          \ class=\"hljs-literal\">True</span>, check=<span class=\"hljs-literal\"\
          >True</span>)\n\n    fp16 = <span class=\"hljs-string\">f\"<span class=\"\
          hljs-subst\">{outdir}</span>/<span class=\"hljs-subst\">{outbase}</span>.ggml<span\
          \ class=\"hljs-subst\">{ggml_version}</span>.fp16.bin\"</span>\n\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Making\
          \ unquantised GGML at <span class=\"hljs-subst\">{fp16}</span>\"</span>)\n\
          \    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> os.path.isfile(fp16):\n        subprocess.run(<span class=\"\
          hljs-string\">f\"python3 <span class=\"hljs-subst\">{llamabase}</span>/convert.py\
          \ <span class=\"hljs-subst\">{model}</span> --outtype f16 --outfile <span\
          \ class=\"hljs-subst\">{fp16}</span>\"</span>, shell=<span class=\"hljs-literal\"\
          >True</span>, check=<span class=\"hljs-literal\">True</span>)\n    <span\
          \ class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Unquantised GGML already exists\
          \ at: <span class=\"hljs-subst\">{fp16}</span>\"</span>)\n\n    <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Making quants\"\
          </span>)\n    <span class=\"hljs-keyword\">for</span> <span class=\"hljs-built_in\"\
          >type</span> <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\"\
          >\"q4_0\"</span>, <span class=\"hljs-string\">\"q4_1\"</span>, <span class=\"\
          hljs-string\">\"q5_0\"</span>, <span class=\"hljs-string\">\"q5_1\"</span>,\
          \ <span class=\"hljs-string\">\"q8_0\"</span>]:\n        outfile = <span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{outdir}</span>/<span\
          \ class=\"hljs-subst\">{outbase}</span>.ggml<span class=\"hljs-subst\">{ggml_version}</span>.<span\
          \ class=\"hljs-subst\">{<span class=\"hljs-built_in\">type</span>}</span>.bin\"\
          </span>\n        <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Making <span class=\"hljs-subst\">{<span class=\"hljs-built_in\"\
          >type</span>}</span> : <span class=\"hljs-subst\">{outfile}</span>\"</span>)\n\
          \        subprocess.run(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\"\
          >{llamabase}</span>/quantize <span class=\"hljs-subst\">{fp16}</span> <span\
          \ class=\"hljs-subst\">{outfile}</span> <span class=\"hljs-subst\">{<span\
          \ class=\"hljs-built_in\">type</span>}</span>\"</span>, shell=<span class=\"\
          hljs-literal\">True</span>, check=<span class=\"hljs-literal\">True</span>)\n\
          \n    <span class=\"hljs-comment\"># Delete FP16 GGML when done making quantisations</span>\n\
          \    os.remove(fp16)\n\n<span class=\"hljs-keyword\">if</span> __name__\
          \ == <span class=\"hljs-string\">\"__main__\"</span>:\n    parser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Convert Bash to Python.'</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">'model'</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Model directory'</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">'outbase'</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Output base name'</span>)\n  \
          \  parser.add_argument(<span class=\"hljs-string\">'outdir'</span>, <span\
          \ class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'Output\
          \ directory'</span>)\n\n    args = parser.parse_args()\n\n    main(args.model,\
          \ args.outbase, args.outdir)\n</code></pre>\n<p>Then I would run:</p>\n\
          <pre><code>python make_ggml.py /path/to/HF_model model-name /path/to/output_directory\n\
          </code></pre>\n"
        raw: "Here's a Python script that I used in the past to make GGMLs\n\n```python\n\
          #!/usr/bin/env python3\nimport argparse\nimport os\nimport subprocess\n\n\
          def main(model, outbase, outdir):\n    llamabase = \"/workspace/venv/git/llama.cpp\"\
          \n    ggml_version = \"v3\"\n\n    if not os.path.isdir(model):\n      \
          \  raise Exception(f\"Could not find model dir at {model}\")\n\n    if not\
          \ os.path.isfile(f\"{model}/config.json\"):\n        raise Exception(f\"\
          Could not find config.json in {model}\")\n\n    os.makedirs(outdir, exist_ok=True)\n\
          \n    print(\"Building llama.cpp\")\n    subprocess.run(f\"cd {llamabase}\
          \ && git pull && make clean && LLAMA_CUBLAS=1 make\", shell=True, check=True)\n\
          \n    fp16 = f\"{outdir}/{outbase}.ggml{ggml_version}.fp16.bin\"\n\n   \
          \ print(f\"Making unquantised GGML at {fp16}\")\n    if not os.path.isfile(fp16):\n\
          \        subprocess.run(f\"python3 {llamabase}/convert.py {model} --outtype\
          \ f16 --outfile {fp16}\", shell=True, check=True)\n    else:\n        print(f\"\
          Unquantised GGML already exists at: {fp16}\")\n\n    print(\"Making quants\"\
          )\n    for type in [\"q4_0\", \"q4_1\", \"q5_0\", \"q5_1\", \"q8_0\"]:\n\
          \        outfile = f\"{outdir}/{outbase}.ggml{ggml_version}.{type}.bin\"\
          \n        print(f\"Making {type} : {outfile}\")\n        subprocess.run(f\"\
          {llamabase}/quantize {fp16} {outfile} {type}\", shell=True, check=True)\n\
          \n    # Delete FP16 GGML when done making quantisations\n    os.remove(fp16)\n\
          \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Convert\
          \ Bash to Python.')\n    parser.add_argument('model', help='Model directory')\n\
          \    parser.add_argument('outbase', help='Output base name')\n    parser.add_argument('outdir',\
          \ help='Output directory')\n\n    args = parser.parse_args()\n\n    main(args.model,\
          \ args.outbase, args.outdir)\n```\n\nThen I would run:\n\n```\npython make_ggml.py\
          \ /path/to/HF_model model-name /path/to/output_directory\n```"
        updatedAt: '2023-06-08T08:28:14.686Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - dsmonk
        - westwind027
        - jacaroo
    id: 648190e16f283a7468610be3
    type: comment
  author: TheBloke
  content: "Here's a Python script that I used in the past to make GGMLs\n\n```python\n\
    #!/usr/bin/env python3\nimport argparse\nimport os\nimport subprocess\n\ndef main(model,\
    \ outbase, outdir):\n    llamabase = \"/workspace/venv/git/llama.cpp\"\n    ggml_version\
    \ = \"v3\"\n\n    if not os.path.isdir(model):\n        raise Exception(f\"Could\
    \ not find model dir at {model}\")\n\n    if not os.path.isfile(f\"{model}/config.json\"\
    ):\n        raise Exception(f\"Could not find config.json in {model}\")\n\n  \
    \  os.makedirs(outdir, exist_ok=True)\n\n    print(\"Building llama.cpp\")\n \
    \   subprocess.run(f\"cd {llamabase} && git pull && make clean && LLAMA_CUBLAS=1\
    \ make\", shell=True, check=True)\n\n    fp16 = f\"{outdir}/{outbase}.ggml{ggml_version}.fp16.bin\"\
    \n\n    print(f\"Making unquantised GGML at {fp16}\")\n    if not os.path.isfile(fp16):\n\
    \        subprocess.run(f\"python3 {llamabase}/convert.py {model} --outtype f16\
    \ --outfile {fp16}\", shell=True, check=True)\n    else:\n        print(f\"Unquantised\
    \ GGML already exists at: {fp16}\")\n\n    print(\"Making quants\")\n    for type\
    \ in [\"q4_0\", \"q4_1\", \"q5_0\", \"q5_1\", \"q8_0\"]:\n        outfile = f\"\
    {outdir}/{outbase}.ggml{ggml_version}.{type}.bin\"\n        print(f\"Making {type}\
    \ : {outfile}\")\n        subprocess.run(f\"{llamabase}/quantize {fp16} {outfile}\
    \ {type}\", shell=True, check=True)\n\n    # Delete FP16 GGML when done making\
    \ quantisations\n    os.remove(fp16)\n\nif __name__ == \"__main__\":\n    parser\
    \ = argparse.ArgumentParser(description='Convert Bash to Python.')\n    parser.add_argument('model',\
    \ help='Model directory')\n    parser.add_argument('outbase', help='Output base\
    \ name')\n    parser.add_argument('outdir', help='Output directory')\n\n    args\
    \ = parser.parse_args()\n\n    main(args.model, args.outbase, args.outdir)\n```\n\
    \nThen I would run:\n\n```\npython make_ggml.py /path/to/HF_model model-name /path/to/output_directory\n\
    ```"
  created_at: 2023-06-08 07:27:13+00:00
  edited: true
  hidden: false
  id: 648190e16f283a7468610be3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7293a00d4da46bfa9e9f4274b3a55e0d.svg
      fullname: Deng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jiachuan94
      type: user
    createdAt: '2023-06-08T09:17:01.000Z'
    data:
      edited: false
      editors:
      - Jiachuan94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24439793825149536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7293a00d4da46bfa9e9f4274b3a55e0d.svg
          fullname: Deng
          isHf: false
          isPro: false
          name: Jiachuan94
          type: user
        html: "<blockquote>\n<p>Here's a Python script that I used in the past to\
          \ make GGMLs</p>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"\
          >#!/usr/bin/env python3</span>\n<span class=\"hljs-keyword\">import</span>\
          \ argparse\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"\
          hljs-keyword\">import</span> subprocess\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">main</span>(<span class=\"\
          hljs-params\">model, outbase, outdir</span>):\n    llamabase = <span class=\"\
          hljs-string\">\"/workspace/venv/git/llama.cpp\"</span>\n    ggml_version\
          \ = <span class=\"hljs-string\">\"v3\"</span>\n\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-keyword\">not</span> os.path.isdir(model):\n\
          \        <span class=\"hljs-keyword\">raise</span> Exception(<span class=\"\
          hljs-string\">f\"Could not find model dir at <span class=\"hljs-subst\"\
          >{model}</span>\"</span>)\n\n    <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-keyword\">not</span> os.path.isfile(<span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{model}</span>/config.json\"\
          </span>):\n        <span class=\"hljs-keyword\">raise</span> Exception(<span\
          \ class=\"hljs-string\">f\"Could not find config.json in <span class=\"\
          hljs-subst\">{model}</span>\"</span>)\n\n    os.makedirs(outdir, exist_ok=<span\
          \ class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Building llama.cpp\"</span>)\n\
          \    subprocess.run(<span class=\"hljs-string\">f\"cd <span class=\"hljs-subst\"\
          >{llamabase}</span> &amp;&amp; git pull &amp;&amp; make clean &amp;&amp;\
          \ LLAMA_CUBLAS=1 make\"</span>, shell=<span class=\"hljs-literal\">True</span>,\
          \ check=<span class=\"hljs-literal\">True</span>)\n\n    fp16 = <span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{outdir}</span>/<span class=\"\
          hljs-subst\">{outbase}</span>.ggml<span class=\"hljs-subst\">{ggml_version}</span>.fp16.bin\"\
          </span>\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Making unquantised GGML at <span class=\"hljs-subst\">{fp16}</span>\"\
          </span>)\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> os.path.isfile(fp16):\n        subprocess.run(<span class=\"\
          hljs-string\">f\"python3 <span class=\"hljs-subst\">{llamabase}</span>/convert.py\
          \ <span class=\"hljs-subst\">{model}</span> --outtype f16 --outfile <span\
          \ class=\"hljs-subst\">{fp16}</span>\"</span>, shell=<span class=\"hljs-literal\"\
          >True</span>, check=<span class=\"hljs-literal\">True</span>)\n    <span\
          \ class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Unquantised GGML already exists\
          \ at: <span class=\"hljs-subst\">{fp16}</span>\"</span>)\n\n    <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Making quants\"\
          </span>)\n    <span class=\"hljs-keyword\">for</span> <span class=\"hljs-built_in\"\
          >type</span> <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\"\
          >\"q4_0\"</span>, <span class=\"hljs-string\">\"q4_1\"</span>, <span class=\"\
          hljs-string\">\"q5_0\"</span>, <span class=\"hljs-string\">\"q5_1\"</span>,\
          \ <span class=\"hljs-string\">\"q8_0\"</span>]:\n        outfile = <span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{outdir}</span>/<span\
          \ class=\"hljs-subst\">{outbase}</span>.ggml<span class=\"hljs-subst\">{ggml_version}</span>.<span\
          \ class=\"hljs-subst\">{<span class=\"hljs-built_in\">type</span>}</span>.bin\"\
          </span>\n        <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Making <span class=\"hljs-subst\">{<span class=\"hljs-built_in\"\
          >type</span>}</span> : <span class=\"hljs-subst\">{outfile}</span>\"</span>)\n\
          \        subprocess.run(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\"\
          >{llamabase}</span>/quantize <span class=\"hljs-subst\">{fp16}</span> <span\
          \ class=\"hljs-subst\">{outfile}</span> <span class=\"hljs-subst\">{<span\
          \ class=\"hljs-built_in\">type</span>}</span>\"</span>, shell=<span class=\"\
          hljs-literal\">True</span>, check=<span class=\"hljs-literal\">True</span>)\n\
          \n    <span class=\"hljs-comment\"># Delete FP16 GGML when done making quantisations</span>\n\
          \    os.remove(fp16)\n\n<span class=\"hljs-keyword\">if</span> __name__\
          \ == <span class=\"hljs-string\">\"__main__\"</span>:\n    parser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Convert Bash to Python.'</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">'model'</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Model directory'</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">'outbase'</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Output base name'</span>)\n  \
          \  parser.add_argument(<span class=\"hljs-string\">'outdir'</span>, <span\
          \ class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'Output\
          \ directory'</span>)\n\n    args = parser.parse_args()\n\n    main(args.model,\
          \ args.outbase, args.outdir)\n</code></pre>\n<p>Then I would run:</p>\n\
          <pre><code>python make_ggml.py /path/to/HF_model model-name /path/to/output_directory\n\
          </code></pre>\n</blockquote>\n<p>It works so well! Thank you <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> ! You are the hero!</p>\n"
        raw: "> Here's a Python script that I used in the past to make GGMLs\n> \n\
          > ```python\n> #!/usr/bin/env python3\n> import argparse\n> import os\n\
          > import subprocess\n> \n> def main(model, outbase, outdir):\n>     llamabase\
          \ = \"/workspace/venv/git/llama.cpp\"\n>     ggml_version = \"v3\"\n> \n\
          >     if not os.path.isdir(model):\n>         raise Exception(f\"Could not\
          \ find model dir at {model}\")\n> \n>     if not os.path.isfile(f\"{model}/config.json\"\
          ):\n>         raise Exception(f\"Could not find config.json in {model}\"\
          )\n> \n>     os.makedirs(outdir, exist_ok=True)\n> \n>     print(\"Building\
          \ llama.cpp\")\n>     subprocess.run(f\"cd {llamabase} && git pull && make\
          \ clean && LLAMA_CUBLAS=1 make\", shell=True, check=True)\n> \n>     fp16\
          \ = f\"{outdir}/{outbase}.ggml{ggml_version}.fp16.bin\"\n> \n>     print(f\"\
          Making unquantised GGML at {fp16}\")\n>     if not os.path.isfile(fp16):\n\
          >         subprocess.run(f\"python3 {llamabase}/convert.py {model} --outtype\
          \ f16 --outfile {fp16}\", shell=True, check=True)\n>     else:\n>      \
          \   print(f\"Unquantised GGML already exists at: {fp16}\")\n> \n>     print(\"\
          Making quants\")\n>     for type in [\"q4_0\", \"q4_1\", \"q5_0\", \"q5_1\"\
          , \"q8_0\"]:\n>         outfile = f\"{outdir}/{outbase}.ggml{ggml_version}.{type}.bin\"\
          \n>         print(f\"Making {type} : {outfile}\")\n>         subprocess.run(f\"\
          {llamabase}/quantize {fp16} {outfile} {type}\", shell=True, check=True)\n\
          > \n>     # Delete FP16 GGML when done making quantisations\n>     os.remove(fp16)\n\
          > \n> if __name__ == \"__main__\":\n>     parser = argparse.ArgumentParser(description='Convert\
          \ Bash to Python.')\n>     parser.add_argument('model', help='Model directory')\n\
          >     parser.add_argument('outbase', help='Output base name')\n>     parser.add_argument('outdir',\
          \ help='Output directory')\n> \n>     args = parser.parse_args()\n> \n>\
          \     main(args.model, args.outbase, args.outdir)\n> ```\n> \n> Then I would\
          \ run:\n> \n> ```\n> python make_ggml.py /path/to/HF_model model-name /path/to/output_directory\n\
          > ```\n\nIt works so well! Thank you @TheBloke ! You are the hero!"
        updatedAt: '2023-06-08T09:17:01.256Z'
      numEdits: 0
      reactions: []
    id: 64819c8d722e358ebf84fd63
    type: comment
  author: Jiachuan94
  content: "> Here's a Python script that I used in the past to make GGMLs\n> \n>\
    \ ```python\n> #!/usr/bin/env python3\n> import argparse\n> import os\n> import\
    \ subprocess\n> \n> def main(model, outbase, outdir):\n>     llamabase = \"/workspace/venv/git/llama.cpp\"\
    \n>     ggml_version = \"v3\"\n> \n>     if not os.path.isdir(model):\n>     \
    \    raise Exception(f\"Could not find model dir at {model}\")\n> \n>     if not\
    \ os.path.isfile(f\"{model}/config.json\"):\n>         raise Exception(f\"Could\
    \ not find config.json in {model}\")\n> \n>     os.makedirs(outdir, exist_ok=True)\n\
    > \n>     print(\"Building llama.cpp\")\n>     subprocess.run(f\"cd {llamabase}\
    \ && git pull && make clean && LLAMA_CUBLAS=1 make\", shell=True, check=True)\n\
    > \n>     fp16 = f\"{outdir}/{outbase}.ggml{ggml_version}.fp16.bin\"\n> \n>  \
    \   print(f\"Making unquantised GGML at {fp16}\")\n>     if not os.path.isfile(fp16):\n\
    >         subprocess.run(f\"python3 {llamabase}/convert.py {model} --outtype f16\
    \ --outfile {fp16}\", shell=True, check=True)\n>     else:\n>         print(f\"\
    Unquantised GGML already exists at: {fp16}\")\n> \n>     print(\"Making quants\"\
    )\n>     for type in [\"q4_0\", \"q4_1\", \"q5_0\", \"q5_1\", \"q8_0\"]:\n>  \
    \       outfile = f\"{outdir}/{outbase}.ggml{ggml_version}.{type}.bin\"\n>   \
    \      print(f\"Making {type} : {outfile}\")\n>         subprocess.run(f\"{llamabase}/quantize\
    \ {fp16} {outfile} {type}\", shell=True, check=True)\n> \n>     # Delete FP16\
    \ GGML when done making quantisations\n>     os.remove(fp16)\n> \n> if __name__\
    \ == \"__main__\":\n>     parser = argparse.ArgumentParser(description='Convert\
    \ Bash to Python.')\n>     parser.add_argument('model', help='Model directory')\n\
    >     parser.add_argument('outbase', help='Output base name')\n>     parser.add_argument('outdir',\
    \ help='Output directory')\n> \n>     args = parser.parse_args()\n> \n>     main(args.model,\
    \ args.outbase, args.outdir)\n> ```\n> \n> Then I would run:\n> \n> ```\n> python\
    \ make_ggml.py /path/to/HF_model model-name /path/to/output_directory\n> ```\n\
    \nIt works so well! Thank you @TheBloke ! You are the hero!"
  created_at: 2023-06-08 08:17:01+00:00
  edited: false
  hidden: false
  id: 64819c8d722e358ebf84fd63
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: junelee/wizard-vicuna-13b
repo_type: model
status: open
target_branch: null
title: Convert to ggml and run with llama.cpp?
