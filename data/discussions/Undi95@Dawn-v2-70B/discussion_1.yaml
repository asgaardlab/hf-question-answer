!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ks01
conflicting_files: null
created_at: 2023-11-08 06:01:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
      fullname: Ks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ks01
      type: user
    createdAt: '2023-11-08T06:01:53.000Z'
    data:
      edited: false
      editors:
      - Ks01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9796706438064575
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jiBJzFDGtCMsOt7U3czoG.png?w=200&h=200&f=face
          fullname: Ks
          isHf: false
          isPro: false
          name: Ks01
          type: user
        html: '<p>Dawn v2 wasn''t what I expected.<br>It didn''t follow complex prompts
          (ex. RPG status in character prompts).<br>It gave a short answer compared
          to other models, with no detailed depiction.<br>Character role-play was
          not bad, but not extraordinary either.</p>

          <p>Compared to what I experienced with your 13b and 20b models, it was an
          unexpected result.<br>I know Dawn 0.1 is a failed model, but somehow it
          worked better on complex prompts, and the output was even better than using
          Xwin alone.</p>

          <p>After a few more tests and trying what''s going on with Dawn v2 myself,
          I found it didn''t take advantage of good models.</p>

          <p>ORCA_LLaMA_70B<br>airoboros-l2-c70b<br>Nous-Hermes-Llama2-70b<br>Samantha-1.11-70b</p>

          <p>Those are the underwhelming models that I tested before. I can''t say
          much about qCammel since I didn''t test it, but I know those four models
          were not good at understanding prompts.<br>ORCA_LLAMA is not good at all
          at RP, even with its high score in the Ayumi leaderboard.<br>Airoboros 3.1.2
          was a bit underwhelming compared to 2.2.1. It gave me a short output and
          an extremely short depiction. It did great on role-playing, but 2.2.1 performed
          better on both understanding complex prompts and RP.<br>Nous-Hermes-Llama2
          is good at writing but has bad intelligence. But Dawn didn''t give enough
          output to show its strength. It feels like it took bad intelligence, not
          strength.<br>Samantha is a model that has intelligence like ORCA_LLAMA or
          FashionGPT but is not suitable for RP.</p>

          <p>I just assume that''s the reason why it''s not working as I expected.<br>Maybe
          someone found more interesting strengths in this model, so it''s just unpopular
          feedback.<br>Thanks for experimenting on 70B though. I hope you keep interested
          in 70B, not only 7b or 13b.</p>

          '
        raw: "Dawn v2 wasn't what I expected.\r\nIt didn't follow complex prompts\
          \ (ex. RPG status in character prompts).\r\nIt gave a short answer compared\
          \ to other models, with no detailed depiction.\r\nCharacter role-play was\
          \ not bad, but not extraordinary either.\r\n\r\nCompared to what I experienced\
          \ with your 13b and 20b models, it was an unexpected result.\r\nI know Dawn\
          \ 0.1 is a failed model, but somehow it worked better on complex prompts,\
          \ and the output was even better than using Xwin alone.\r\n\r\nAfter a few\
          \ more tests and trying what's going on with Dawn v2 myself, I found it\
          \ didn't take advantage of good models.\r\n\r\nORCA_LLaMA_70B\r\nairoboros-l2-c70b\r\
          \nNous-Hermes-Llama2-70b\r\nSamantha-1.11-70b\r\n\r\nThose are the underwhelming\
          \ models that I tested before. I can't say much about qCammel since I didn't\
          \ test it, but I know those four models were not good at understanding prompts.\r\
          \nORCA_LLAMA is not good at all at RP, even with its high score in the Ayumi\
          \ leaderboard.\r\nAiroboros 3.1.2 was a bit underwhelming compared to 2.2.1.\
          \ It gave me a short output and an extremely short depiction. It did great\
          \ on role-playing, but 2.2.1 performed better on both understanding complex\
          \ prompts and RP.\r\nNous-Hermes-Llama2 is good at writing but has bad intelligence.\
          \ But Dawn didn't give enough output to show its strength. It feels like\
          \ it took bad intelligence, not strength.\r\nSamantha is a model that has\
          \ intelligence like ORCA_LLAMA or FashionGPT but is not suitable for RP.\r\
          \n\r\nI just assume that's the reason why it's not working as I expected.\r\
          \nMaybe someone found more interesting strengths in this model, so it's\
          \ just unpopular feedback.\r\nThanks for experimenting on 70B though. I\
          \ hope you keep interested in 70B, not only 7b or 13b."
        updatedAt: '2023-11-08T06:01:53.707Z'
      numEdits: 0
      reactions: []
    id: 654b245132d67f12f89157b9
    type: comment
  author: Ks01
  content: "Dawn v2 wasn't what I expected.\r\nIt didn't follow complex prompts (ex.\
    \ RPG status in character prompts).\r\nIt gave a short answer compared to other\
    \ models, with no detailed depiction.\r\nCharacter role-play was not bad, but\
    \ not extraordinary either.\r\n\r\nCompared to what I experienced with your 13b\
    \ and 20b models, it was an unexpected result.\r\nI know Dawn 0.1 is a failed\
    \ model, but somehow it worked better on complex prompts, and the output was even\
    \ better than using Xwin alone.\r\n\r\nAfter a few more tests and trying what's\
    \ going on with Dawn v2 myself, I found it didn't take advantage of good models.\r\
    \n\r\nORCA_LLaMA_70B\r\nairoboros-l2-c70b\r\nNous-Hermes-Llama2-70b\r\nSamantha-1.11-70b\r\
    \n\r\nThose are the underwhelming models that I tested before. I can't say much\
    \ about qCammel since I didn't test it, but I know those four models were not\
    \ good at understanding prompts.\r\nORCA_LLAMA is not good at all at RP, even\
    \ with its high score in the Ayumi leaderboard.\r\nAiroboros 3.1.2 was a bit underwhelming\
    \ compared to 2.2.1. It gave me a short output and an extremely short depiction.\
    \ It did great on role-playing, but 2.2.1 performed better on both understanding\
    \ complex prompts and RP.\r\nNous-Hermes-Llama2 is good at writing but has bad\
    \ intelligence. But Dawn didn't give enough output to show its strength. It feels\
    \ like it took bad intelligence, not strength.\r\nSamantha is a model that has\
    \ intelligence like ORCA_LLAMA or FashionGPT but is not suitable for RP.\r\n\r\
    \nI just assume that's the reason why it's not working as I expected.\r\nMaybe\
    \ someone found more interesting strengths in this model, so it's just unpopular\
    \ feedback.\r\nThanks for experimenting on 70B though. I hope you keep interested\
    \ in 70B, not only 7b or 13b."
  created_at: 2023-11-08 06:01:53+00:00
  edited: false
  hidden: false
  id: 654b245132d67f12f89157b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-08T09:54:46.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9827965497970581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Hi!<br>It''s okay, a feedback is a feedback, and I thank you for
          the time you took to make it.<br>Dawn v1 is usable, but as you know it was
          just LimaRP on top of Xwin, and not something I wanted.<br>If this model
          don''t serve you well it''s okay, I will do better next time, I''m currently
          still working on making usable EXL2 quant, so I will be able to check the
          real limit when I will succeed, because f16 and GGUF are really too slow
          for me, but I found the output to be correct atm.<br>I keep what you said
          in mind about the choice of model if I do a v3 someday!</p>

          '
        raw: 'Hi!

          It''s okay, a feedback is a feedback, and I thank you for the time you took
          to make it.

          Dawn v1 is usable, but as you know it was just LimaRP on top of Xwin, and
          not something I wanted.

          If this model don''t serve you well it''s okay, I will do better next time,
          I''m currently still working on making usable EXL2 quant, so I will be able
          to check the real limit when I will succeed, because f16 and GGUF are really
          too slow for me, but I found the output to be correct atm.

          I keep what you said in mind about the choice of model if I do a v3 someday!'
        updatedAt: '2023-11-08T09:54:46.733Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - Ks01
        - Latent-Dreamscape
        - mishima
    id: 654b5ae6fce0d4fb01bf507f
    type: comment
  author: Undi95
  content: 'Hi!

    It''s okay, a feedback is a feedback, and I thank you for the time you took to
    make it.

    Dawn v1 is usable, but as you know it was just LimaRP on top of Xwin, and not
    something I wanted.

    If this model don''t serve you well it''s okay, I will do better next time, I''m
    currently still working on making usable EXL2 quant, so I will be able to check
    the real limit when I will succeed, because f16 and GGUF are really too slow for
    me, but I found the output to be correct atm.

    I keep what you said in mind about the choice of model if I do a v3 someday!'
  created_at: 2023-11-08 09:54:46+00:00
  edited: false
  hidden: false
  id: 654b5ae6fce0d4fb01bf507f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
      fullname: Blair Sadewitz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tachyphylaxis
      type: user
    createdAt: '2023-11-10T21:35:13.000Z'
    data:
      edited: true
      editors:
      - tachyphylaxis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9635137319564819
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
          fullname: Blair Sadewitz
          isHf: false
          isPro: false
          name: tachyphylaxis
          type: user
        html: '<p>My experience is the diametrical opposite--and I just started using
          SillyTavern, so I''m sure the prompting, etc. is suboptimal.  Nevertheless,
          its prose was orders of magnitude better than some other 70b models I''ve
          tried, especially with metaphors (could be in part that I started using
          sillytavern, tho).  Did OP specify whether they were using a quant or not?  </p>

          <p>There is AzureBlack/Dawn-v2-70B-exl2 (IIRC a full spectrum of quants
          are in there)--you don''t have to quantize <em>nothin''</em>--unless you
          do it for the thrill of it. ;-)</p>

          <p>Frankly, I suspect that part of the reason it was better had to do with
          sillytavern vs. koboldai and the flexibility in prompting it, but I doubt
          that was all of it.</p>

          <p>Oh, BTW:  IMHO you should provide specific settings for samplers, etc.
          or maybe even a preset file for people to try with the model.  That way,
          you''ll at least have SOME independent variables.</p>

          '
        raw: "My experience is the diametrical opposite--and I just started using\
          \ SillyTavern, so I'm sure the prompting, etc. is suboptimal.  Nevertheless,\
          \ its prose was orders of magnitude better than some other 70b models I've\
          \ tried, especially with metaphors (could be in part that I started using\
          \ sillytavern, tho).  Did OP specify whether they were using a quant or\
          \ not?  \n\nThere is AzureBlack/Dawn-v2-70B-exl2 (IIRC a full spectrum of\
          \ quants are in there)--you don't have to quantize *nothin'*--unless you\
          \ do it for the thrill of it. ;-)\n\nFrankly, I suspect that part of the\
          \ reason it was better had to do with sillytavern vs. koboldai and the flexibility\
          \ in prompting it, but I doubt that was all of it.\n\nOh, BTW:  IMHO you\
          \ should provide specific settings for samplers, etc. or maybe even a preset\
          \ file for people to try with the model.  That way, you'll at least have\
          \ SOME independent variables.\n\n"
        updatedAt: '2023-11-10T21:49:55.452Z'
      numEdits: 1
      reactions: []
    id: 654ea2116c46a91f12fb758d
    type: comment
  author: tachyphylaxis
  content: "My experience is the diametrical opposite--and I just started using SillyTavern,\
    \ so I'm sure the prompting, etc. is suboptimal.  Nevertheless, its prose was\
    \ orders of magnitude better than some other 70b models I've tried, especially\
    \ with metaphors (could be in part that I started using sillytavern, tho).  Did\
    \ OP specify whether they were using a quant or not?  \n\nThere is AzureBlack/Dawn-v2-70B-exl2\
    \ (IIRC a full spectrum of quants are in there)--you don't have to quantize *nothin'*--unless\
    \ you do it for the thrill of it. ;-)\n\nFrankly, I suspect that part of the reason\
    \ it was better had to do with sillytavern vs. koboldai and the flexibility in\
    \ prompting it, but I doubt that was all of it.\n\nOh, BTW:  IMHO you should provide\
    \ specific settings for samplers, etc. or maybe even a preset file for people\
    \ to try with the model.  That way, you'll at least have SOME independent variables.\n\
    \n"
  created_at: 2023-11-10 21:35:13+00:00
  edited: true
  hidden: false
  id: 654ea2116c46a91f12fb758d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
      fullname: Blair Sadewitz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tachyphylaxis
      type: user
    createdAt: '2023-11-10T21:43:01.000Z'
    data:
      edited: false
      editors:
      - tachyphylaxis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.930791974067688
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
          fullname: Blair Sadewitz
          isHf: false
          isPro: false
          name: tachyphylaxis
          type: user
        html: '<p>Oh, Also, you might want to try a merge like this one except with
          chronos-70b substituted for one or more of the more generic instruct models.  </p>

          '
        raw: 'Oh, Also, you might want to try a merge like this one except with chronos-70b
          substituted for one or more of the more generic instruct models.  '
        updatedAt: '2023-11-10T21:43:01.770Z'
      numEdits: 0
      reactions: []
    id: 654ea3e57490049d62ee9824
    type: comment
  author: tachyphylaxis
  content: 'Oh, Also, you might want to try a merge like this one except with chronos-70b
    substituted for one or more of the more generic instruct models.  '
  created_at: 2023-11-10 21:43:01+00:00
  edited: false
  hidden: false
  id: 654ea3e57490049d62ee9824
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6336aa3b2d4e8b8c34651e9c/UIcwvPa0gXlDLUGzpOjsh.png?w=200&h=200&f=face
      fullname: Alex Aphebis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ninjaman12
      type: user
    createdAt: '2023-11-21T19:43:06.000Z'
    data:
      edited: false
      editors:
      - ninjaman12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9843767881393433
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6336aa3b2d4e8b8c34651e9c/UIcwvPa0gXlDLUGzpOjsh.png?w=200&h=200&f=face
          fullname: Alex Aphebis
          isHf: false
          isPro: false
          name: ninjaman12
          type: user
        html: '<p>Its early days for me. I found it wasn''t that inspiring for chat,
          chat-instruct on ooga but when I switched to using alpaca instruct format
          with a bit of prompt tweaking it was doing really well, hardly had to regenerate
          at all so real plus there!</p>

          '
        raw: Its early days for me. I found it wasn't that inspiring for chat, chat-instruct
          on ooga but when I switched to using alpaca instruct format with a bit of
          prompt tweaking it was doing really well, hardly had to regenerate at all
          so real plus there!
        updatedAt: '2023-11-21T19:43:06.143Z'
      numEdits: 0
      reactions: []
    id: 655d084ac166f4c0a78c3a1d
    type: comment
  author: ninjaman12
  content: Its early days for me. I found it wasn't that inspiring for chat, chat-instruct
    on ooga but when I switched to using alpaca instruct format with a bit of prompt
    tweaking it was doing really well, hardly had to regenerate at all so real plus
    there!
  created_at: 2023-11-21 19:43:06+00:00
  edited: false
  hidden: false
  id: 655d084ac166f4c0a78c3a1d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Undi95/Dawn-v2-70B
repo_type: model
status: open
target_branch: null
title: unpopular feedback after some test.
