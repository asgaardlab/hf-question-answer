!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Undi95
conflicting_files: null
created_at: 2023-11-08 11:13:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-08T11:13:59.000Z'
    data:
      edited: true
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8777239918708801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Hi, it''s been 3 days I try to have an usable EXL2 quant of my 70B
          model, but I don''t succeed, tried the following :</p>

          <ul>

          <li>2048 length with wikiset dataset</li>

          <li>4096 length with wikiset dataset</li>

          <li>2048 length with WizardLM dataset</li>

          <li>4096 length with WizardLM dataset</li>

          </ul>

          <p>And those bpw :</p>

          <ul>

          <li>2.40bpw / h6</li>

          <li>2.55bpw / h6</li>

          </ul>

          <p>I do this on an A100 on collab, and THIS SPECIFIC POST tell me the issue
          could be the A100 : <a href="https://huggingface.co/AzureBlack/UtopiaXL-13B-exl2/discussions/1#6547a6f86c818bb7b5ca30f7">https://huggingface.co/AzureBlack/UtopiaXL-13B-exl2/discussions/1#6547a6f86c818bb7b5ca30f7</a><br>I''m
          clueless, an helping hand to guide me to the right path or someone that
          can do it for me (4096 length if possible for better output) would really
          be helpful.<br>Pinning this as I''m going crazy and can''t use another GPU
          (shitty internet, only sub to colab), will link you on the model page and
          credit you, thank you all!</p>

          '
        raw: 'Hi, it''s been 3 days I try to have an usable EXL2 quant of my 70B model,
          but I don''t succeed, tried the following :

          - 2048 length with wikiset dataset

          - 4096 length with wikiset dataset

          - 2048 length with WizardLM dataset

          - 4096 length with WizardLM dataset


          And those bpw :

          - 2.40bpw / h6

          - 2.55bpw / h6


          I do this on an A100 on collab, and THIS SPECIFIC POST tell me the issue
          could be the A100 : https://huggingface.co/AzureBlack/UtopiaXL-13B-exl2/discussions/1#6547a6f86c818bb7b5ca30f7

          I''m clueless, an helping hand to guide me to the right path or someone
          that can do it for me (4096 length if possible for better output) would
          really be helpful.

          Pinning this as I''m going crazy and can''t use another GPU (shitty internet,
          only sub to colab), will link you on the model page and credit you, thank
          you all!'
        updatedAt: '2023-11-08T11:15:17.855Z'
      numEdits: 3
      reactions: []
    id: 654b6d77c1b0078d6329485f
    type: comment
  author: Undi95
  content: 'Hi, it''s been 3 days I try to have an usable EXL2 quant of my 70B model,
    but I don''t succeed, tried the following :

    - 2048 length with wikiset dataset

    - 4096 length with wikiset dataset

    - 2048 length with WizardLM dataset

    - 4096 length with WizardLM dataset


    And those bpw :

    - 2.40bpw / h6

    - 2.55bpw / h6


    I do this on an A100 on collab, and THIS SPECIFIC POST tell me the issue could
    be the A100 : https://huggingface.co/AzureBlack/UtopiaXL-13B-exl2/discussions/1#6547a6f86c818bb7b5ca30f7

    I''m clueless, an helping hand to guide me to the right path or someone that can
    do it for me (4096 length if possible for better output) would really be helpful.

    Pinning this as I''m going crazy and can''t use another GPU (shitty internet,
    only sub to colab), will link you on the model page and credit you, thank you
    all!'
  created_at: 2023-11-08 11:13:59+00:00
  edited: true
  hidden: false
  id: 654b6d77c1b0078d6329485f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-08T11:14:07.000Z'
    data:
      pinned: true
    id: 654b6d7f31bda013eb53c5db
    type: pinning-change
  author: Undi95
  created_at: 2023-11-08 11:14:07+00:00
  id: 654b6d7f31bda013eb53c5db
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-08T11:54:59.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8940892219543457
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Update: Got another machine to try this</p>

          '
        raw: 'Update: Got another machine to try this'
        updatedAt: '2023-11-08T11:54:59.901Z'
      numEdits: 0
      reactions: []
    id: 654b77138ea7c084b8781064
    type: comment
  author: Undi95
  content: 'Update: Got another machine to try this'
  created_at: 2023-11-08 11:54:59+00:00
  edited: false
  hidden: false
  id: 654b77138ea7c084b8781064
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-08T18:35:02.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642941355705261
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>From the tests that folks have done on TheBloke''s discord, the
          length of the measurement dataset doesn''t make much of a difference. What
          issues are you running into with the exl2 quants? The low bpw quants are
          more likely to go off the rails and generate gibberish. They''re also more
          sensitive to the prompt format, so it helps if you can follow the prompt
          format more closely. </p>

          '
        raw: 'From the tests that folks have done on TheBloke''s discord, the length
          of the measurement dataset doesn''t make much of a difference. What issues
          are you running into with the exl2 quants? The low bpw quants are more likely
          to go off the rails and generate gibberish. They''re also more sensitive
          to the prompt format, so it helps if you can follow the prompt format more
          closely. '
        updatedAt: '2023-11-08T18:35:02.897Z'
      numEdits: 0
      reactions: []
    id: 654bd4d6386fc5525c02ddab
    type: comment
  author: LoneStriker
  content: 'From the tests that folks have done on TheBloke''s discord, the length
    of the measurement dataset doesn''t make much of a difference. What issues are
    you running into with the exl2 quants? The low bpw quants are more likely to go
    off the rails and generate gibberish. They''re also more sensitive to the prompt
    format, so it helps if you can follow the prompt format more closely. '
  created_at: 2023-11-08 18:35:02+00:00
  edited: false
  hidden: false
  id: 654bd4d6386fc5525c02ddab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-08T20:47:05.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553548097610474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<blockquote>

          <p>From the tests that folks have done on TheBloke''s discord, the length
          of the measurement dataset doesn''t make much of a difference. What issues
          are you running into with the exl2 quants? The low bpw quants are more likely
          to go off the rails and generate gibberish. They''re also more sensitive
          to the prompt format, so it helps if you can follow the prompt format more
          closely.</p>

          </blockquote>

          <p>I get some letter missing, some bad punctuation and bad reply in general...<br>I
          tested Q2_K and Q3_K_S quantization on my 3090 and they seems really better,
          it''s day and night, I dont recognize my model on the 2.4/2.55 bpw.<br>I''m
          trying one last time on a L40 to see if the A100 is at fault because I never
          saw so much damage done to a 70B before with EXL2 quant, and yes, I use
          the good prompting (Alpaca, work on the Q2_K and Q3_K_S too).<br>Will finish
          soon, but will take a little time to download.</p>

          '
        raw: '> From the tests that folks have done on TheBloke''s discord, the length
          of the measurement dataset doesn''t make much of a difference. What issues
          are you running into with the exl2 quants? The low bpw quants are more likely
          to go off the rails and generate gibberish. They''re also more sensitive
          to the prompt format, so it helps if you can follow the prompt format more
          closely.


          I get some letter missing, some bad punctuation and bad reply in general...

          I tested Q2_K and Q3_K_S quantization on my 3090 and they seems really better,
          it''s day and night, I dont recognize my model on the 2.4/2.55 bpw.

          I''m trying one last time on a L40 to see if the A100 is at fault because
          I never saw so much damage done to a 70B before with EXL2 quant, and yes,
          I use the good prompting (Alpaca, work on the Q2_K and Q3_K_S too).

          Will finish soon, but will take a little time to download.'
        updatedAt: '2023-11-08T20:47:05.349Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - waldie
    id: 654bf3c9dc772c8e80d06077
    type: comment
  author: Undi95
  content: '> From the tests that folks have done on TheBloke''s discord, the length
    of the measurement dataset doesn''t make much of a difference. What issues are
    you running into with the exl2 quants? The low bpw quants are more likely to go
    off the rails and generate gibberish. They''re also more sensitive to the prompt
    format, so it helps if you can follow the prompt format more closely.


    I get some letter missing, some bad punctuation and bad reply in general...

    I tested Q2_K and Q3_K_S quantization on my 3090 and they seems really better,
    it''s day and night, I dont recognize my model on the 2.4/2.55 bpw.

    I''m trying one last time on a L40 to see if the A100 is at fault because I never
    saw so much damage done to a 70B before with EXL2 quant, and yes, I use the good
    prompting (Alpaca, work on the Q2_K and Q3_K_S too).

    Will finish soon, but will take a little time to download.'
  created_at: 2023-11-08 20:47:05+00:00
  edited: false
  hidden: false
  id: 654bf3c9dc772c8e80d06077
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2023-11-08T22:47:43.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658563733100891
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>Don''t we expect those to do better though, given that they weigh
          an extra 30% per weight? The Q3_K_S 3.47BPW, and the Q2_K is barely any
          smaller at least in terms of filesize.</p>

          <p>It strikes me that there''s probably a reason there was never a Q3_0
          never mind a Q2_0/Q2_1. </p>

          '
        raw: 'Don''t we expect those to do better though, given that they weigh an
          extra 30% per weight? The Q3_K_S 3.47BPW, and the Q2_K is barely any smaller
          at least in terms of filesize.


          It strikes me that there''s probably a reason there was never a Q3_0 never
          mind a Q2_0/Q2_1. '
        updatedAt: '2023-11-08T22:55:33.855Z'
      numEdits: 2
      reactions: []
    id: 654c100f24d9d9a1a5941c71
    type: comment
  author: ProphetOfBostrom
  content: 'Don''t we expect those to do better though, given that they weigh an extra
    30% per weight? The Q3_K_S 3.47BPW, and the Q2_K is barely any smaller at least
    in terms of filesize.


    It strikes me that there''s probably a reason there was never a Q3_0 never mind
    a Q2_0/Q2_1. '
  created_at: 2023-11-08 22:47:43+00:00
  edited: true
  hidden: false
  id: 654c100f24d9d9a1a5941c71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-09T01:10:27.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9647486209869385
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>I done it again on L40 but no change, setting need to be changed
          for better result or use higher bpw, it''s the only way.<br>Or use GGUF...<br>I
          close the thread, thanks for all your help, even if you didn''t posted here
          but was helping on Discord or other platform!</p>

          '
        raw: 'I done it again on L40 but no change, setting need to be changed for
          better result or use higher bpw, it''s the only way.

          Or use GGUF...

          I close the thread, thanks for all your help, even if you didn''t posted
          here but was helping on Discord or other platform!'
        updatedAt: '2023-11-09T01:10:27.388Z'
      numEdits: 0
      reactions: []
    id: 654c3183e06d25def58a0971
    type: comment
  author: Undi95
  content: 'I done it again on L40 but no change, setting need to be changed for better
    result or use higher bpw, it''s the only way.

    Or use GGUF...

    I close the thread, thanks for all your help, even if you didn''t posted here
    but was helping on Discord or other platform!'
  created_at: 2023-11-09 01:10:27+00:00
  edited: false
  hidden: false
  id: 654c3183e06d25def58a0971
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-09T01:11:07.000Z'
    data:
      status: closed
    id: 654c31ab6217c5e2f8e12876
    type: status-change
  author: Undi95
  created_at: 2023-11-09 01:11:07+00:00
  id: 654c31ab6217c5e2f8e12876
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-09T01:11:15.000Z'
    data:
      pinned: false
    id: 654c31b317d83697c7525e51
    type: pinning-change
  author: Undi95
  created_at: 2023-11-09 01:11:15+00:00
  id: 654c31b317d83697c7525e51
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-11-09T01:39:11.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8159230947494507
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>I also find that this setting fuck thing up, it''s somewhat usable
          when put to 0</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/21ne4F7tvAS3Ki91syG-W.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/21ne4F7tvAS3Ki91syG-W.png"></a></p>

          '
        raw: 'I also find that this setting fuck thing up, it''s somewhat usable when
          put to 0


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/21ne4F7tvAS3Ki91syG-W.png)

          '
        updatedAt: '2023-11-09T01:39:11.032Z'
      numEdits: 0
      reactions: []
    id: 654c383f7824e2bb58e6166b
    type: comment
  author: Undi95
  content: 'I also find that this setting fuck thing up, it''s somewhat usable when
    put to 0


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/21ne4F7tvAS3Ki91syG-W.png)

    '
  created_at: 2023-11-09 01:39:11+00:00
  edited: false
  hidden: false
  id: 654c383f7824e2bb58e6166b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-09T11:25:50.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8480366468429565
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>You can also try disabling this ooba option. I think it''s helped
          me make some 2.4bpw models stop spitting gibberish:<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/NUUbHyaS4Y6-7WSg1kx9U.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/NUUbHyaS4Y6-7WSg1kx9U.png"></a></p>

          '
        raw: 'You can also try disabling this ooba option. I think it''s helped me
          make some 2.4bpw models stop spitting gibberish:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/NUUbHyaS4Y6-7WSg1kx9U.png)

          '
        updatedAt: '2023-11-09T11:25:50.035Z'
      numEdits: 0
      reactions: []
    id: 654cc1be167e23258e43c3b3
    type: comment
  author: LoneStriker
  content: 'You can also try disabling this ooba option. I think it''s helped me make
    some 2.4bpw models stop spitting gibberish:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/NUUbHyaS4Y6-7WSg1kx9U.png)

    '
  created_at: 2023-11-09 11:25:50+00:00
  edited: false
  hidden: false
  id: 654cc1be167e23258e43c3b3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Undi95/Dawn-v2-70B
repo_type: model
status: closed
target_branch: null
title: 2.40bpw and 2.55bpw 6h EXL2 request.
