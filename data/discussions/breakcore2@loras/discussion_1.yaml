!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Anglestaw
conflicting_files: null
created_at: 2023-02-06 11:29:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
      fullname: mopanzhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anglestaw
      type: user
    createdAt: '2023-02-06T11:29:58.000Z'
    data:
      edited: false
      editors:
      - Anglestaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
          fullname: mopanzhong
          isHf: false
          isPro: false
          name: Anglestaw
          type: user
        html: '<p>your work is nice. i want to know which code you used to trainning?
          is it kohya-ss/sd-scripts? if it is the code from kohya-ss/sd-scripts, can
          you tell me how should i set Trigger Word for trainning, like your "amber
          (genshin impact)". i will be grateful for your gracious help.</p>

          '
        raw: your work is nice. i want to know which code you used to trainning? is
          it kohya-ss/sd-scripts? if it is the code from kohya-ss/sd-scripts, can
          you tell me how should i set Trigger Word for trainning, like your "amber
          \(genshin impact\)". i will be grateful for your gracious help.
        updatedAt: '2023-02-06T11:29:58.168Z'
      numEdits: 0
      reactions: []
    id: 63e0e4b65c6964861ebc3fd6
    type: comment
  author: Anglestaw
  content: your work is nice. i want to know which code you used to trainning? is
    it kohya-ss/sd-scripts? if it is the code from kohya-ss/sd-scripts, can you tell
    me how should i set Trigger Word for trainning, like your "amber \(genshin impact\)".
    i will be grateful for your gracious help.
  created_at: 2023-02-06 11:29:58+00:00
  edited: false
  hidden: false
  id: 63e0e4b65c6964861ebc3fd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
      fullname: breakcore2
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: breakcore2
      type: user
    createdAt: '2023-02-07T00:49:11.000Z'
    data:
      edited: false
      editors:
      - breakcore2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
          fullname: breakcore2
          isHf: false
          isPro: false
          name: breakcore2
          type: user
        html: '<p>I use kohya-ss/sd-scripts. I use the dreambooth lora training style.
          Each of my images has a corresponding .txt file with the tags in it. The
          tags are the tag_string_general from danbooru metadata of the image. I then
          add amber (genshin impact) to the front of every .txt file and when training
          I use --keep-tokens=1 so that amber (genshin impact) is always kept in the
          training example while the other tags can get shuffled.<br>A popular alternative
          to using danbooru metadata is to use an AI based tagger, sd-scripts has
          a script to use wd tagger, I think its under the finetune folder.</p>

          '
        raw: "I use kohya-ss/sd-scripts. I use the dreambooth lora training style.\
          \ Each of my images has a corresponding .txt file with the tags in it. The\
          \ tags are the tag_string_general from danbooru metadata of the image. I\
          \ then add amber (genshin impact) to the front of every .txt file and when\
          \ training I use --keep-tokens=1 so that amber (genshin impact) is always\
          \ kept in the training example while the other tags can get shuffled. \n\
          A popular alternative to using danbooru metadata is to use an AI based tagger,\
          \ sd-scripts has a script to use wd tagger, I think its under the finetune\
          \ folder."
        updatedAt: '2023-02-07T00:49:11.908Z'
      numEdits: 0
      reactions: []
    id: 63e1a007e17c1192ecb53586
    type: comment
  author: breakcore2
  content: "I use kohya-ss/sd-scripts. I use the dreambooth lora training style. Each\
    \ of my images has a corresponding .txt file with the tags in it. The tags are\
    \ the tag_string_general from danbooru metadata of the image. I then add amber\
    \ (genshin impact) to the front of every .txt file and when training I use --keep-tokens=1\
    \ so that amber (genshin impact) is always kept in the training example while\
    \ the other tags can get shuffled. \nA popular alternative to using danbooru metadata\
    \ is to use an AI based tagger, sd-scripts has a script to use wd tagger, I think\
    \ its under the finetune folder."
  created_at: 2023-02-07 00:49:11+00:00
  edited: false
  hidden: false
  id: 63e1a007e17c1192ecb53586
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
      fullname: mopanzhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anglestaw
      type: user
    createdAt: '2023-02-07T10:00:59.000Z'
    data:
      edited: true
      editors:
      - Anglestaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
          fullname: mopanzhong
          isHf: false
          isPro: false
          name: Anglestaw
          type: user
        html: '<blockquote>

          <p>I use kohya-ss/sd-scripts. I use the dreambooth lora training style.
          Each of my images has a corresponding .txt file with the tags in it. The
          tags are the tag_string_general from danbooru metadata of the image. I then
          add amber (genshin impact) to the front of every .txt file and when training
          I use --keep-tokens=1 so that amber (genshin impact) is always kept in the
          training example while the other tags can get shuffled.<br>A popular alternative
          to using danbooru metadata is to use an AI based tagger, sd-scripts has
          a script to use wd tagger, I think its under the finetune folder.</p>

          </blockquote>

          <p>i trained some safetensors, there are no big troblems but wrong face.
          the face of charactor is odd and unnormal predicted by sd with my own lora
          safetensors sometimes. i away trained with 66 images and 100 time(repeat)
          for each image. is that too small of the repeat time of images? your safetensors
          are so greate, the quality of the output images is very nice and perfet
          both body and face. that''s impressing me so much.</p>

          '
        raw: "> I use kohya-ss/sd-scripts. I use the dreambooth lora training style.\
          \ Each of my images has a corresponding .txt file with the tags in it. The\
          \ tags are the tag_string_general from danbooru metadata of the image. I\
          \ then add amber (genshin impact) to the front of every .txt file and when\
          \ training I use --keep-tokens=1 so that amber (genshin impact) is always\
          \ kept in the training example while the other tags can get shuffled. \n\
          > A popular alternative to using danbooru metadata is to use an AI based\
          \ tagger, sd-scripts has a script to use wd tagger, I think its under the\
          \ finetune folder.\n\n\ni trained some safetensors, there are no big troblems\
          \ but wrong face. the face of charactor is odd and unnormal predicted by\
          \ sd with my own lora safetensors sometimes. i away trained with 66 images\
          \ and 100 time(repeat) for each image. is that too small of the repeat time\
          \ of images? your safetensors are so greate, the quality of the output images\
          \ is very nice and perfet both body and face. that's impressing me so much."
        updatedAt: '2023-02-07T10:01:44.288Z'
      numEdits: 1
      reactions: []
    id: 63e2215b1c0d033a0f9a9d3e
    type: comment
  author: Anglestaw
  content: "> I use kohya-ss/sd-scripts. I use the dreambooth lora training style.\
    \ Each of my images has a corresponding .txt file with the tags in it. The tags\
    \ are the tag_string_general from danbooru metadata of the image. I then add amber\
    \ (genshin impact) to the front of every .txt file and when training I use --keep-tokens=1\
    \ so that amber (genshin impact) is always kept in the training example while\
    \ the other tags can get shuffled. \n> A popular alternative to using danbooru\
    \ metadata is to use an AI based tagger, sd-scripts has a script to use wd tagger,\
    \ I think its under the finetune folder.\n\n\ni trained some safetensors, there\
    \ are no big troblems but wrong face. the face of charactor is odd and unnormal\
    \ predicted by sd with my own lora safetensors sometimes. i away trained with\
    \ 66 images and 100 time(repeat) for each image. is that too small of the repeat\
    \ time of images? your safetensors are so greate, the quality of the output images\
    \ is very nice and perfet both body and face. that's impressing me so much."
  created_at: 2023-02-07 10:00:59+00:00
  edited: true
  hidden: false
  id: 63e2215b1c0d033a0f9a9d3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
      fullname: breakcore2
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: breakcore2
      type: user
    createdAt: '2023-02-07T21:49:37.000Z'
    data:
      edited: false
      editors:
      - breakcore2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
          fullname: breakcore2
          isHf: false
          isPro: false
          name: breakcore2
          type: user
        html: '<p>66 images is a decent amount for characters. I usually try to do
          around 1000 steps. Images * epochs * repeats / batch size = 1000, thats
          roughly the equation I try to solve.<br>Try adjusting learning rates, I
          sometimes have to retrain multiple times to get it looking good and learning
          rates is pretty impactful. The most important change is usually in the dataset,
          if the model isn''t coming out good I would also double check if all the
          images are good. I rarely get the results I want one the very first try.<br>If
          faces are coming out weird even without using a LoRA then you might have
          "restore faces" checked, which isn''t useful for anime images as that is
          trained on photographs.<br>You can also try training at 768 resolution instead
          of 512 (also increase your bucket resolution to something like 480,1280).
          Larger resolution training is much slower but it helps with capturing very
          small details. That said, Amber was trained on 512 and worked out fine.</p>

          '
        raw: '66 images is a decent amount for characters. I usually try to do around
          1000 steps. Images * epochs * repeats / batch size = 1000, thats roughly
          the equation I try to solve.

          Try adjusting learning rates, I sometimes have to retrain multiple times
          to get it looking good and learning rates is pretty impactful. The most
          important change is usually in the dataset, if the model isn''t coming out
          good I would also double check if all the images are good. I rarely get
          the results I want one the very first try.

          If faces are coming out weird even without using a LoRA then you might have
          "restore faces" checked, which isn''t useful for anime images as that is
          trained on photographs.

          You can also try training at 768 resolution instead of 512 (also increase
          your bucket resolution to something like 480,1280). Larger resolution training
          is much slower but it helps with capturing very small details. That said,
          Amber was trained on 512 and worked out fine.'
        updatedAt: '2023-02-07T21:49:37.941Z'
      numEdits: 0
      reactions: []
    id: 63e2c7718104dccdae3f4c03
    type: comment
  author: breakcore2
  content: '66 images is a decent amount for characters. I usually try to do around
    1000 steps. Images * epochs * repeats / batch size = 1000, thats roughly the equation
    I try to solve.

    Try adjusting learning rates, I sometimes have to retrain multiple times to get
    it looking good and learning rates is pretty impactful. The most important change
    is usually in the dataset, if the model isn''t coming out good I would also double
    check if all the images are good. I rarely get the results I want one the very
    first try.

    If faces are coming out weird even without using a LoRA then you might have "restore
    faces" checked, which isn''t useful for anime images as that is trained on photographs.

    You can also try training at 768 resolution instead of 512 (also increase your
    bucket resolution to something like 480,1280). Larger resolution training is much
    slower but it helps with capturing very small details. That said, Amber was trained
    on 512 and worked out fine.'
  created_at: 2023-02-07 21:49:37+00:00
  edited: false
  hidden: false
  id: 63e2c7718104dccdae3f4c03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
      fullname: mopanzhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anglestaw
      type: user
    createdAt: '2023-02-10T07:35:40.000Z'
    data:
      edited: false
      editors:
      - Anglestaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
          fullname: mopanzhong
          isHf: false
          isPro: false
          name: Anglestaw
          type: user
        html: '<blockquote>

          <p>66 images is a decent amount for characters. I usually try to do around
          1000 steps. Images * epochs * repeats / batch size = 1000, thats roughly
          the equation I try to solve.<br>Try adjusting learning rates, I sometimes
          have to retrain multiple times to get it looking good and learning rates
          is pretty impactful. The most important change is usually in the dataset,
          if the model isn''t coming out good I would also double check if all the
          images are good. I rarely get the results I want one the very first try.<br>If
          faces are coming out weird even without using a LoRA then you might have
          "restore faces" checked, which isn''t useful for anime images as that is
          trained on photographs.<br>You can also try training at 768 resolution instead
          of 512 (also increase your bucket resolution to something like 480,1280).
          Larger resolution training is much slower but it helps with capturing very
          small details. That said, Amber was trained on 512 and worked out fine.</p>

          </blockquote>

          <p>i try to trained some safetensors, the only question i found is the wrong
          or unclean eyes and face but person in my dataset have a nice an clean face
          and eyes. i found your safetensors deal with face and eyes perfectly even
          just using a Trigger word "AMBER (GENSHIN IMPACT)", i wonder if your learning
          rate is a list? like [0.0001, 0.00001,0.000001,0.0000001]? not just set
          learn_rate as 0.0001. as my thought, the key of trainning wonderful small
          part like face and eyes is small learning rate after trained a nice body
          and background with a bigger learning rate, is it right? i will keep going
          to correct my trainning process. and i will be very grateful for your kindness
          and gracious suggestion. if will be wonderful if you put a trainning video
          of lora into youtube, i am sure there will be a lot of person enjoy your
          video, because there are just a few video of trainning lora in youtube.</p>

          '
        raw: '> 66 images is a decent amount for characters. I usually try to do around
          1000 steps. Images * epochs * repeats / batch size = 1000, thats roughly
          the equation I try to solve.

          > Try adjusting learning rates, I sometimes have to retrain multiple times
          to get it looking good and learning rates is pretty impactful. The most
          important change is usually in the dataset, if the model isn''t coming out
          good I would also double check if all the images are good. I rarely get
          the results I want one the very first try.

          > If faces are coming out weird even without using a LoRA then you might
          have "restore faces" checked, which isn''t useful for anime images as that
          is trained on photographs.

          > You can also try training at 768 resolution instead of 512 (also increase
          your bucket resolution to something like 480,1280). Larger resolution training
          is much slower but it helps with capturing very small details. That said,
          Amber was trained on 512 and worked out fine.




          i try to trained some safetensors, the only question i found is the wrong
          or unclean eyes and face but person in my dataset have a nice an clean face
          and eyes. i found your safetensors deal with face and eyes perfectly even
          just using a Trigger word "AMBER \(GENSHIN IMPACT\)", i wonder if your learning
          rate is a list? like [0.0001, 0.00001,0.000001,0.0000001]? not just set
          learn_rate as 0.0001. as my thought, the key of trainning wonderful small
          part like face and eyes is small learning rate after trained a nice body
          and background with a bigger learning rate, is it right? i will keep going
          to correct my trainning process. and i will be very grateful for your kindness
          and gracious suggestion. if will be wonderful if you put a trainning video
          of lora into youtube, i am sure there will be a lot of person enjoy your
          video, because there are just a few video of trainning lora in youtube.'
        updatedAt: '2023-02-10T07:35:40.469Z'
      numEdits: 0
      reactions: []
    id: 63e5f3cc26fa42e117df8b95
    type: comment
  author: Anglestaw
  content: '> 66 images is a decent amount for characters. I usually try to do around
    1000 steps. Images * epochs * repeats / batch size = 1000, thats roughly the equation
    I try to solve.

    > Try adjusting learning rates, I sometimes have to retrain multiple times to
    get it looking good and learning rates is pretty impactful. The most important
    change is usually in the dataset, if the model isn''t coming out good I would
    also double check if all the images are good. I rarely get the results I want
    one the very first try.

    > If faces are coming out weird even without using a LoRA then you might have
    "restore faces" checked, which isn''t useful for anime images as that is trained
    on photographs.

    > You can also try training at 768 resolution instead of 512 (also increase your
    bucket resolution to something like 480,1280). Larger resolution training is much
    slower but it helps with capturing very small details. That said, Amber was trained
    on 512 and worked out fine.




    i try to trained some safetensors, the only question i found is the wrong or unclean
    eyes and face but person in my dataset have a nice an clean face and eyes. i found
    your safetensors deal with face and eyes perfectly even just using a Trigger word
    "AMBER \(GENSHIN IMPACT\)", i wonder if your learning rate is a list? like [0.0001,
    0.00001,0.000001,0.0000001]? not just set learn_rate as 0.0001. as my thought,
    the key of trainning wonderful small part like face and eyes is small learning
    rate after trained a nice body and background with a bigger learning rate, is
    it right? i will keep going to correct my trainning process. and i will be very
    grateful for your kindness and gracious suggestion. if will be wonderful if you
    put a trainning video of lora into youtube, i am sure there will be a lot of person
    enjoy your video, because there are just a few video of trainning lora in youtube.'
  created_at: 2023-02-10 07:35:40+00:00
  edited: false
  hidden: false
  id: 63e5f3cc26fa42e117df8b95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
      fullname: breakcore2
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: breakcore2
      type: user
    createdAt: '2023-02-10T18:26:42.000Z'
    data:
      edited: true
      editors:
      - breakcore2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672834221773-63afaf915a416291f4049e47.png?w=200&h=200&f=face
          fullname: breakcore2
          isHf: false
          isPro: false
          name: breakcore2
          type: user
        html: '<p>I mostly use constant learning rate scheduler so the learning rate
          wouldn''t be changing (actually AdamW, which everyone using kohya_ss will
          be using, does adjust the learning rate).<br>I find learning rates between
          2e-5 to 8e-5 to be fairly effective (default is 1e-4 and I find that too
          high for large dim loras). You want to avoid using high learning rates since
          those will quickly end up with fried outputs that do not look good. Very
          low learning rates aren''t good either because it will take forever to learn
          anything.<br>When it learns, it learns everything in the image at the same
          time so there isn''t anything like using different learning rates to learn
          different parts of the image.<br>If the eyes and face aren''t coming out
          great and the base model you are using is capable of generating good faces,
          such as AnythingV3, then I would double check your training data. Maybe
          there is too much variation in the style or you don''t have enough images
          that clearly show the face in detail.</p>

          '
        raw: 'I mostly use constant learning rate scheduler so the learning rate wouldn''t
          be changing (actually AdamW, which everyone using kohya_ss will be using,
          does adjust the learning rate).

          I find learning rates between 2e-5 to 8e-5 to be fairly effective (default
          is 1e-4 and I find that too high for large dim loras). You want to avoid
          using high learning rates since those will quickly end up with fried outputs
          that do not look good. Very low learning rates aren''t good either because
          it will take forever to learn anything.

          When it learns, it learns everything in the image at the same time so there
          isn''t anything like using different learning rates to learn different parts
          of the image.

          If the eyes and face aren''t coming out great and the base model you are
          using is capable of generating good faces, such as AnythingV3, then I would
          double check your training data. Maybe there is too much variation in the
          style or you don''t have enough images that clearly show the face in detail.'
        updatedAt: '2023-02-10T18:27:56.523Z'
      numEdits: 1
      reactions: []
    id: 63e68c6226fa42e117fde70f
    type: comment
  author: breakcore2
  content: 'I mostly use constant learning rate scheduler so the learning rate wouldn''t
    be changing (actually AdamW, which everyone using kohya_ss will be using, does
    adjust the learning rate).

    I find learning rates between 2e-5 to 8e-5 to be fairly effective (default is
    1e-4 and I find that too high for large dim loras). You want to avoid using high
    learning rates since those will quickly end up with fried outputs that do not
    look good. Very low learning rates aren''t good either because it will take forever
    to learn anything.

    When it learns, it learns everything in the image at the same time so there isn''t
    anything like using different learning rates to learn different parts of the image.

    If the eyes and face aren''t coming out great and the base model you are using
    is capable of generating good faces, such as AnythingV3, then I would double check
    your training data. Maybe there is too much variation in the style or you don''t
    have enough images that clearly show the face in detail.'
  created_at: 2023-02-10 18:26:42+00:00
  edited: true
  hidden: false
  id: 63e68c6226fa42e117fde70f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
      fullname: mopanzhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anglestaw
      type: user
    createdAt: '2023-02-13T10:36:01.000Z'
    data:
      edited: false
      editors:
      - Anglestaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec16021d33b85acd045cd5a7256f556d.svg
          fullname: mopanzhong
          isHf: false
          isPro: false
          name: Anglestaw
          type: user
        html: '<blockquote>

          <p>I mostly use constant learning rate scheduler so the learning rate wouldn''t
          be changing (actually AdamW, which everyone using kohya_ss will be using,
          does adjust the learning rate).<br>I find learning rates between 2e-5 to
          8e-5 to be fairly effective (default is 1e-4 and I find that too high for
          large dim loras). You want to avoid using high learning rates since those
          will quickly end up with fried outputs that do not look good. Very low learning
          rates aren''t good either because it will take forever to learn anything.<br>When
          it learns, it learns everything in the image at the same time so there isn''t
          anything like using different learning rates to learn different parts of
          the image.<br>If the eyes and face aren''t coming out great and the base
          model you are using is capable of generating good faces, such as AnythingV3,
          then I would double check your training data. Maybe there is too much variation
          in the style or you don''t have enough images that clearly show the face
          in detail.</p>

          </blockquote>

          <p>i have found the key raising this proble. i trained 1 epoch and set repeat
          as 100 for each image, and i predict a image with wrong eyes and face. then
          i try to train 2 epochs and set repeat as 50, and i get a predicted image
          with better eyes and face. the secondtime, i train 10 epochs and set repeat
          as 10, the eyes and face are more better. but, after all, my safetensors
          still can not predict a wonderfull and stable eyes and face as your safetensors
          in txt2img and img2img, especially in img2img. i will keep trainning for
          a better safetensors.</p>

          '
        raw: '> I mostly use constant learning rate scheduler so the learning rate
          wouldn''t be changing (actually AdamW, which everyone using kohya_ss will
          be using, does adjust the learning rate).

          > I find learning rates between 2e-5 to 8e-5 to be fairly effective (default
          is 1e-4 and I find that too high for large dim loras). You want to avoid
          using high learning rates since those will quickly end up with fried outputs
          that do not look good. Very low learning rates aren''t good either because
          it will take forever to learn anything.

          > When it learns, it learns everything in the image at the same time so
          there isn''t anything like using different learning rates to learn different
          parts of the image.

          > If the eyes and face aren''t coming out great and the base model you are
          using is capable of generating good faces, such as AnythingV3, then I would
          double check your training data. Maybe there is too much variation in the
          style or you don''t have enough images that clearly show the face in detail.



          i have found the key raising this proble. i trained 1 epoch and set repeat
          as 100 for each image, and i predict a image with wrong eyes and face. then
          i try to train 2 epochs and set repeat as 50, and i get a predicted image
          with better eyes and face. the secondtime, i train 10 epochs and set repeat
          as 10, the eyes and face are more better. but, after all, my safetensors
          still can not predict a wonderfull and stable eyes and face as your safetensors
          in txt2img and img2img, especially in img2img. i will keep trainning for
          a better safetensors.'
        updatedAt: '2023-02-13T10:36:01.716Z'
      numEdits: 0
      reactions: []
    id: 63ea12916114eb22db731643
    type: comment
  author: Anglestaw
  content: '> I mostly use constant learning rate scheduler so the learning rate wouldn''t
    be changing (actually AdamW, which everyone using kohya_ss will be using, does
    adjust the learning rate).

    > I find learning rates between 2e-5 to 8e-5 to be fairly effective (default is
    1e-4 and I find that too high for large dim loras). You want to avoid using high
    learning rates since those will quickly end up with fried outputs that do not
    look good. Very low learning rates aren''t good either because it will take forever
    to learn anything.

    > When it learns, it learns everything in the image at the same time so there
    isn''t anything like using different learning rates to learn different parts of
    the image.

    > If the eyes and face aren''t coming out great and the base model you are using
    is capable of generating good faces, such as AnythingV3, then I would double check
    your training data. Maybe there is too much variation in the style or you don''t
    have enough images that clearly show the face in detail.



    i have found the key raising this proble. i trained 1 epoch and set repeat as
    100 for each image, and i predict a image with wrong eyes and face. then i try
    to train 2 epochs and set repeat as 50, and i get a predicted image with better
    eyes and face. the secondtime, i train 10 epochs and set repeat as 10, the eyes
    and face are more better. but, after all, my safetensors still can not predict
    a wonderfull and stable eyes and face as your safetensors in txt2img and img2img,
    especially in img2img. i will keep trainning for a better safetensors.'
  created_at: 2023-02-13 10:36:01+00:00
  edited: false
  hidden: false
  id: 63ea12916114eb22db731643
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: breakcore2/loras
repo_type: model
status: open
target_branch: null
title: some questions about trainning
