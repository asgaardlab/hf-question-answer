!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2023-07-01 11:35:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-01T12:35:35.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9684769511222839
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>I''m still new in using Lora and I thought that adapter model can''t
          be merged. But, I saw your script to merge adapter to base model.</p>

          <p>What I don''t understand is quantization. Do you have to quantize the
          base model first then merge it with the adapter, or you just quantize the
          merged model? Can you please explain how and kindly share your way to quantize?
          Thank you....</p>

          '
        raw: "I'm still new in using Lora and I thought that adapter model can't be\
          \ merged. But, I saw your script to merge adapter to base model.\r\n\r\n\
          What I don't understand is quantization. Do you have to quantize the base\
          \ model first then merge it with the adapter, or you just quantize the merged\
          \ model? Can you please explain how and kindly share your way to quantize?\
          \ Thank you...."
        updatedAt: '2023-07-01T12:35:35.633Z'
      numEdits: 0
      reactions: []
    id: 64a01d9737bfb5202bfed4f9
    type: comment
  author: fahadh4ilyas
  content: "I'm still new in using Lora and I thought that adapter model can't be\
    \ merged. But, I saw your script to merge adapter to base model.\r\n\r\nWhat I\
    \ don't understand is quantization. Do you have to quantize the base model first\
    \ then merge it with the adapter, or you just quantize the merged model? Can you\
    \ please explain how and kindly share your way to quantize? Thank you...."
  created_at: 2023-07-01 11:35:35+00:00
  edited: false
  hidden: false
  id: 64a01d9737bfb5202bfed4f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-01T12:53:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7651774883270264
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The latter. I merge the lora on to the unquantised base model to
          produce a merged model, then I quantise that merged model.</p>

          <p>I quantised these with the old CUDA branch of GPTQ-for-LLaMa, to maximise
          compatibility.  The command to do that is simple:</p>

          <pre><code>python llama.py source_model wikitext --wbits 4 --true-sequential
          --groupsize 128 --act-order  --save_safetensors outputfile.safetensors

          </code></pre>

          <p>However for most users I recommend quantising with AutoGPTQ instead.  There
          are example scripts shown in the AutoGPTQ repo on Github.  Or here is a
          script I made that provides command line parameters to easily choose the
          quantisation configuration:  <a rel="nofollow" href="https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682">https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682</a></p>

          '
        raw: "The latter. I merge the lora on to the unquantised base model to produce\
          \ a merged model, then I quantise that merged model.\n\nI quantised these\
          \ with the old CUDA branch of GPTQ-for-LLaMa, to maximise compatibility.\
          \  The command to do that is simple:\n ```\n python llama.py source_model\
          \ wikitext --wbits 4 --true-sequential --groupsize 128 --act-order  --save_safetensors\
          \ outputfile.safetensors\n ```\n\nHowever for most users I recommend quantising\
          \ with AutoGPTQ instead.  There are example scripts shown in the AutoGPTQ\
          \ repo on Github.  Or here is a script I made that provides command line\
          \ parameters to easily choose the quantisation configuration:  https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682"
        updatedAt: '2023-07-01T12:53:01.023Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Badgids
        - fahadh4ilyas
    id: 64a021ad1dcdd7e9e089f6d5
    type: comment
  author: TheBloke
  content: "The latter. I merge the lora on to the unquantised base model to produce\
    \ a merged model, then I quantise that merged model.\n\nI quantised these with\
    \ the old CUDA branch of GPTQ-for-LLaMa, to maximise compatibility.  The command\
    \ to do that is simple:\n ```\n python llama.py source_model wikitext --wbits\
    \ 4 --true-sequential --groupsize 128 --act-order  --save_safetensors outputfile.safetensors\n\
    \ ```\n\nHowever for most users I recommend quantising with AutoGPTQ instead.\
    \  There are example scripts shown in the AutoGPTQ repo on Github.  Or here is\
    \ a script I made that provides command line parameters to easily choose the quantisation\
    \ configuration:  https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682"
  created_at: 2023-07-01 11:53:01+00:00
  edited: false
  hidden: false
  id: 64a021ad1dcdd7e9e089f6d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-01T13:03:33.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7843223214149475
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>So, we can quantize merged model? Okay thank you for the information
          and the script...</p>

          '
        raw: So, we can quantize merged model? Okay thank you for the information
          and the script...
        updatedAt: '2023-07-01T13:03:33.377Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64a02425778e2594141ef226
    id: 64a02425778e2594141ef223
    type: comment
  author: fahadh4ilyas
  content: So, we can quantize merged model? Okay thank you for the information and
    the script...
  created_at: 2023-07-01 12:03:33+00:00
  edited: false
  hidden: false
  id: 64a02425778e2594141ef223
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-01T13:03:33.000Z'
    data:
      status: closed
    id: 64a02425778e2594141ef226
    type: status-change
  author: fahadh4ilyas
  created_at: 2023-07-01 12:03:33+00:00
  id: 64a02425778e2594141ef226
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Vicuna-13B-1-3-SuperHOT-8K-GPTQ
repo_type: model
status: closed
target_branch: null
title: How do you Quantize the model?
