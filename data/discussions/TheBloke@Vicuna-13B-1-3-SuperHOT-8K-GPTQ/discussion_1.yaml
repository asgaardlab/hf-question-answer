!!python/object:huggingface_hub.community.DiscussionWithDetails
author: underlines
conflicting_files: null
created_at: 2023-06-26 21:57:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-26T22:57:51.000Z'
    data:
      edited: false
      editors:
      - underlines
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7690720558166504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<p>I set everything exactly as mentioned, ExLlama, max_seq_len 8192,
          compress_pos_emb 4, short contexts work, but when I paste a 3000 token context
          for summarization, the generated tokens are simply 0.<br>Tried all Vicuna
          Prompt Templates, instruct, chat-instruct and chat etc.</p>

          '
        raw: "I set everything exactly as mentioned, ExLlama, max_seq_len 8192, compress_pos_emb\
          \ 4, short contexts work, but when I paste a 3000 token context for summarization,\
          \ the generated tokens are simply 0.\r\nTried all Vicuna Prompt Templates,\
          \ instruct, chat-instruct and chat etc."
        updatedAt: '2023-06-26T22:57:51.566Z'
      numEdits: 0
      reactions: []
    id: 649a17ef4e39022b26e9c947
    type: comment
  author: underlines
  content: "I set everything exactly as mentioned, ExLlama, max_seq_len 8192, compress_pos_emb\
    \ 4, short contexts work, but when I paste a 3000 token context for summarization,\
    \ the generated tokens are simply 0.\r\nTried all Vicuna Prompt Templates, instruct,\
    \ chat-instruct and chat etc."
  created_at: 2023-06-26 21:57:51+00:00
  edited: false
  hidden: false
  id: 649a17ef4e39022b26e9c947
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-28T19:34:33.000Z'
    data:
      edited: false
      editors:
      - underlines
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.701590359210968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<p><a href="https://huggingface.co/TheBloke/Vicuna-33B-1-1-preview-SuperHOT-8K-GPTQ/discussions/1#649c0950272ee9fd6b635ea3">https://huggingface.co/TheBloke/Vicuna-33B-1-1-preview-SuperHOT-8K-GPTQ/discussions/1#649c0950272ee9fd6b635ea3</a></p>

          <p>For people using TheBloke''s Runpod Template: It didn''t update ExLlama,
          but it''s now fixed. Restart your pods or update ExLlama.</p>

          '
        raw: 'https://huggingface.co/TheBloke/Vicuna-33B-1-1-preview-SuperHOT-8K-GPTQ/discussions/1#649c0950272ee9fd6b635ea3


          For people using TheBloke''s Runpod Template: It didn''t update ExLlama,
          but it''s now fixed. Restart your pods or update ExLlama.'
        updatedAt: '2023-06-28T19:34:33.737Z'
      numEdits: 0
      reactions: []
    id: 649c8b497bc1f09aee13c2fd
    type: comment
  author: underlines
  content: 'https://huggingface.co/TheBloke/Vicuna-33B-1-1-preview-SuperHOT-8K-GPTQ/discussions/1#649c0950272ee9fd6b635ea3


    For people using TheBloke''s Runpod Template: It didn''t update ExLlama, but it''s
    now fixed. Restart your pods or update ExLlama.'
  created_at: 2023-06-28 18:34:33+00:00
  edited: false
  hidden: false
  id: 649c8b497bc1f09aee13c2fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-28T19:34:42.000Z'
    data:
      status: closed
    id: 649c8b527cac47026d02c195
    type: status-change
  author: underlines
  created_at: 2023-06-28 18:34:42+00:00
  id: 649c8b527cac47026d02c195
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Vicuna-13B-1-3-SuperHOT-8K-GPTQ
repo_type: model
status: closed
target_branch: null
title: 3000+ context, no output
