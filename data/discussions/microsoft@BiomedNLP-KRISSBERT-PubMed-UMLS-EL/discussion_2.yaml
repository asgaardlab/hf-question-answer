!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhill
conflicting_files: null
created_at: 2023-02-02 23:20:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dcac86a1b7a9ff26ea54b79a5cee371.svg
      fullname: Matt Hill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhill
      type: user
    createdAt: '2023-02-02T23:20:25.000Z'
    data:
      edited: true
      editors:
      - mhill
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dcac86a1b7a9ff26ea54b79a5cee371.svg
          fullname: Matt Hill
          isHf: false
          isPro: false
          name: mhill
          type: user
        html: '<p>Does this model include the entity centric reference encoder? If
          not are you planning to release that part of the model or code to train
          the model with that encoder?</p>

          '
        raw: Does this model include the entity centric reference encoder? If not
          are you planning to release that part of the model or code to train the
          model with that encoder?
        updatedAt: '2023-02-02T23:23:38.646Z'
      numEdits: 2
      reactions: []
    id: 63dc45395d310b018be05e14
    type: comment
  author: mhill
  content: Does this model include the entity centric reference encoder? If not are
    you planning to release that part of the model or code to train the model with
    that encoder?
  created_at: 2023-02-02 23:20:25+00:00
  edited: true
  hidden: false
  id: 63dc45395d310b018be05e14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
      fullname: Ashwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayadav
      type: user
    createdAt: '2023-02-03T17:22:03.000Z'
    data:
      edited: false
      editors:
      - ayadav
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
          fullname: Ashwani
          isHf: false
          isPro: false
          name: ayadav
          type: user
        html: '<p>Same question.</p>

          <p>I''m struggling to figure how to use this model without entity embeddings.
          The <code>run_entity_linking</code> script creates FAISS index using some
          vector files, but those are not included in model files.</p>

          '
        raw: 'Same question.


          I''m struggling to figure how to use this model without entity embeddings.
          The `run_entity_linking` script creates FAISS index using some vector files,
          but those are not included in model files.'
        updatedAt: '2023-02-03T17:22:03.467Z'
      numEdits: 0
      reactions: []
    id: 63dd42bb327159311ac6edeb
    type: comment
  author: ayadav
  content: 'Same question.


    I''m struggling to figure how to use this model without entity embeddings. The
    `run_entity_linking` script creates FAISS index using some vector files, but those
    are not included in model files.'
  created_at: 2023-02-03 17:22:03+00:00
  edited: false
  hidden: false
  id: 63dd42bb327159311ac6edeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c383e8902dc9003c2013384d8b0d636a.svg
      fullname: Marius Petruc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: InformaticsSolutions
      type: user
    createdAt: '2023-11-08T14:59:26.000Z'
    data:
      edited: true
      editors:
      - InformaticsSolutions
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7327926754951477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c383e8902dc9003c2013384d8b0d636a.svg
          fullname: Marius Petruc
          isHf: false
          isPro: false
          name: InformaticsSolutions
          type: user
        html: "<p>Same question. I am unsure about how to use the model for inference.\
          \ So far i've tried using it like any other transformer model for computing\
          \ word embeddings, but am still confused whether this is the way the model\
          \ was intended to be used, and if yes, what to do next:</p>\n<pre><code>from\
          \ tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, AutoModel\
          \ \nmodel=microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL\ndef get_word_embeddings(tokenizer,\
          \ model, list): \n    all_names = list\n    bs = 128 # batch size during\
          \ inference\n    all_embs = []\n    for i in tqdm(np.arange(0, len(all_names),\
          \ bs)):\n        toks = tokenizer.batch_encode_plus(all_names[i:i+bs], \n\
          \                                        padding=\"max_length\", \n    \
          \                                    max_length=25, \n                 \
          \                       truncation=True,\n                             \
          \           return_tensors=\"pt\")\n        toks_cuda = {}\n        for\
          \ k,v in toks.items():\n            toks_cuda[k] = v.cuda()\n        cls_rep\
          \ = model(**toks_cuda)[0][:,0,:] # use CLS representation as the embedding\n\
          \n        all_embs.append(cls_rep.cpu().detach().numpy())\n    all_embs\
          \ = np.concatenate(all_embs, axis=0)\n    return(all_embs)\ntokenizer =\
          \ AutoTokenizer.from_pretrained(model_name)  \nmodel = AutoModel.from_pretrained(model_name).cuda()\n\
          ent_emb= get_word_embeddings(tokenizer, model, ent_l)\n</code></pre>\n<p>This\
          \ approach to training a model for entity linking has great potential, and\
          \ i'm extremely grateful to Microsoft for their research and for releasing\
          \ their models. It would be great if they chose to elaborate a little about\
          \ how this one can be used for entity linking, assuming a valid UMLS license.\
          \ Otherwise, I'm hoping we (the community) might be able to collaborate\
          \ to figure out how to use it.</p>\n"
        raw: "Same question. I am unsure about how to use the model for inference.\
          \ So far i've tried using it like any other transformer model for computing\
          \ word embeddings, but am still confused whether this is the way the model\
          \ was intended to be used, and if yes, what to do next:\n\n    from tqdm.auto\
          \ import tqdm\n    from transformers import AutoTokenizer, AutoModel \n\
          \    model=microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL\n    def get_word_embeddings(tokenizer,\
          \ model, list): \n        all_names = list\n        bs = 128 # batch size\
          \ during inference\n        all_embs = []\n        for i in tqdm(np.arange(0,\
          \ len(all_names), bs)):\n            toks = tokenizer.batch_encode_plus(all_names[i:i+bs],\
          \ \n                                            padding=\"max_length\",\
          \ \n                                            max_length=25, \n      \
          \                                      truncation=True,\n              \
          \                              return_tensors=\"pt\")\n            toks_cuda\
          \ = {}\n            for k,v in toks.items():\n                toks_cuda[k]\
          \ = v.cuda()\n            cls_rep = model(**toks_cuda)[0][:,0,:] # use CLS\
          \ representation as the embedding\n\n            all_embs.append(cls_rep.cpu().detach().numpy())\n\
          \        all_embs = np.concatenate(all_embs, axis=0)\n        return(all_embs)\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_name)  \n    model\
          \ = AutoModel.from_pretrained(model_name).cuda()\n    ent_emb= get_word_embeddings(tokenizer,\
          \ model, ent_l)\n\nThis approach to training a model for entity linking\
          \ has great potential, and i'm extremely grateful to Microsoft for their\
          \ research and for releasing their models. It would be great if they chose\
          \ to elaborate a little about how this one can be used for entity linking,\
          \ assuming a valid UMLS license. Otherwise, I'm hoping we (the community)\
          \ might be able to collaborate to figure out how to use it."
        updatedAt: '2023-11-08T15:06:43.652Z'
      numEdits: 3
      reactions: []
    id: 654ba24e1a9e65ef2595a8c1
    type: comment
  author: InformaticsSolutions
  content: "Same question. I am unsure about how to use the model for inference. So\
    \ far i've tried using it like any other transformer model for computing word\
    \ embeddings, but am still confused whether this is the way the model was intended\
    \ to be used, and if yes, what to do next:\n\n    from tqdm.auto import tqdm\n\
    \    from transformers import AutoTokenizer, AutoModel \n    model=microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL\n\
    \    def get_word_embeddings(tokenizer, model, list): \n        all_names = list\n\
    \        bs = 128 # batch size during inference\n        all_embs = []\n     \
    \   for i in tqdm(np.arange(0, len(all_names), bs)):\n            toks = tokenizer.batch_encode_plus(all_names[i:i+bs],\
    \ \n                                            padding=\"max_length\", \n   \
    \                                         max_length=25, \n                  \
    \                          truncation=True,\n                                \
    \            return_tensors=\"pt\")\n            toks_cuda = {}\n            for\
    \ k,v in toks.items():\n                toks_cuda[k] = v.cuda()\n            cls_rep\
    \ = model(**toks_cuda)[0][:,0,:] # use CLS representation as the embedding\n\n\
    \            all_embs.append(cls_rep.cpu().detach().numpy())\n        all_embs\
    \ = np.concatenate(all_embs, axis=0)\n        return(all_embs)\n    tokenizer\
    \ = AutoTokenizer.from_pretrained(model_name)  \n    model = AutoModel.from_pretrained(model_name).cuda()\n\
    \    ent_emb= get_word_embeddings(tokenizer, model, ent_l)\n\nThis approach to\
    \ training a model for entity linking has great potential, and i'm extremely grateful\
    \ to Microsoft for their research and for releasing their models. It would be\
    \ great if they chose to elaborate a little about how this one can be used for\
    \ entity linking, assuming a valid UMLS license. Otherwise, I'm hoping we (the\
    \ community) might be able to collaborate to figure out how to use it."
  created_at: 2023-11-08 14:59:26+00:00
  edited: true
  hidden: false
  id: 654ba24e1a9e65ef2595a8c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL
repo_type: model
status: open
target_branch: null
title: Entity centric reference encoder?
