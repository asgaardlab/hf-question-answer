!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2024-01-14 23:57:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-14T23:57:24.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9690369963645935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>I''m downloading and can''t wait to test them, starting with their
          perplexity.</p>

          <p>If you''re in mood for another 70b model to quantize in GGUF 2 bits anytime
          soon, I''d suggest LZLV ( <a href="https://huggingface.co/lizpreciatior/lzlv_70b_fp16_hf">https://huggingface.co/lizpreciatior/lzlv_70b_fp16_hf</a>
          ).<br>It is a classic which performs very well in both logical and creative
          tasks!</p>

          <p>Edit : here are the perplexities at 512ctx :<br>Aurora-Nights-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.9372,<br>Aurora-Nights-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.5800
          -&gt; best for 24GB VRAM users for long context with rope, I guess.<br>Aurora-Nights-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.5313<br>Aurora-Nights-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.2042</p>

          '
        raw: 'I''m downloading and can''t wait to test them, starting with their perplexity.


          If you''re in mood for another 70b model to quantize in GGUF 2 bits anytime
          soon, I''d suggest LZLV ( https://huggingface.co/lizpreciatior/lzlv_70b_fp16_hf
          ).

          It is a classic which performs very well in both logical and creative tasks!


          Edit : here are the perplexities at 512ctx :

          Aurora-Nights-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.9372,

          Aurora-Nights-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.5800 -> best for
          24GB VRAM users for long context with rope, I guess.

          Aurora-Nights-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.5313

          Aurora-Nights-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.2042'
        updatedAt: '2024-01-15T01:03:08.454Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65a474e480e2523eea6dd47c
    type: comment
  author: Nexesenex
  content: 'I''m downloading and can''t wait to test them, starting with their perplexity.


    If you''re in mood for another 70b model to quantize in GGUF 2 bits anytime soon,
    I''d suggest LZLV ( https://huggingface.co/lizpreciatior/lzlv_70b_fp16_hf ).

    It is a classic which performs very well in both logical and creative tasks!


    Edit : here are the perplexities at 512ctx :

    Aurora-Nights-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.9372,

    Aurora-Nights-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.5800 -> best for 24GB
    VRAM users for long context with rope, I guess.

    Aurora-Nights-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.5313

    Aurora-Nights-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.2042'
  created_at: 2024-01-14 23:57:24+00:00
  edited: true
  hidden: false
  id: 65a474e480e2523eea6dd47c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-15T18:13:50.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9174189567565918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p>Thanks for the feedback!</p>

          <p>I''ll have a look at making the new 2bit quants of lzlv-70b. Should be
          fairly quick now that the longest step (calculating the imatrix) can be
          offloaded to the GPU.</p>

          '
        raw: 'Thanks for the feedback!


          I''ll have a look at making the new 2bit quants of lzlv-70b. Should be fairly
          quick now that the longest step (calculating the imatrix) can be offloaded
          to the GPU.'
        updatedAt: '2024-01-15T18:13:50.448Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a575debcc380eddd73a318
    type: comment
  author: Artefact2
  content: 'Thanks for the feedback!


    I''ll have a look at making the new 2bit quants of lzlv-70b. Should be fairly
    quick now that the longest step (calculating the imatrix) can be offloaded to
    the GPU.'
  created_at: 2024-01-15 18:13:50+00:00
  edited: false
  hidden: false
  id: 65a575debcc380eddd73a318
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-15T21:24:47.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8722443580627441
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: "<p>You're welcome. The model is quite sensical in IQ2_XS, and that's\
          \ much better than the experience with the exllama v2 quants, even post\
          \ 0.0.11. Some regens are enough to steer the model right when it deviates\
          \ from the context, this way beyond 1,000 or even 2,000 tokens.</p>\n<p>Also,\
          \ great news for the imatrix on GPU, I will be able to toy with it soon\
          \ enough then on smaller models. Thanks for LZLV, I'm a bit short on hardware\
          \ to quantize efficiently 70b models!</p>\n<p>Here comes a relatively extensive\
          \ perplexity test of your 4 quants, because they are the first 70b quants\
          \ I test beyond Ikawakow's and I wanted to get a good look at the quality\
          \ of the new quantizations.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/LHL28X1QstV4xwnI9xBWU.png\"\
          ><img alt=\"2024-01-15 22_27_43-evaluations.csv \u2014 LibreOffice Calc.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/LHL28X1QstV4xwnI9xBWU.png\"\
          ></a></p>\n"
        raw: "You're welcome. The model is quite sensical in IQ2_XS, and that's much\
          \ better than the experience with the exllama v2 quants, even post 0.0.11.\
          \ Some regens are enough to steer the model right when it deviates from\
          \ the context, this way beyond 1,000 or even 2,000 tokens.\n\nAlso, great\
          \ news for the imatrix on GPU, I will be able to toy with it soon enough\
          \ then on smaller models. Thanks for LZLV, I'm a bit short on hardware to\
          \ quantize efficiently 70b models!\n\nHere comes a relatively extensive\
          \ perplexity test of your 4 quants, because they are the first 70b quants\
          \ I test beyond Ikawakow's and I wanted to get a good look at the quality\
          \ of the new quantizations.\n\n\n![2024-01-15 22_27_43-evaluations.csv \u2014\
          \ LibreOffice Calc.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/LHL28X1QstV4xwnI9xBWU.png)\n"
        updatedAt: '2024-01-15T21:32:06.190Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65a5a29f2823ba72ed1f8977
    type: comment
  author: Nexesenex
  content: "You're welcome. The model is quite sensical in IQ2_XS, and that's much\
    \ better than the experience with the exllama v2 quants, even post 0.0.11. Some\
    \ regens are enough to steer the model right when it deviates from the context,\
    \ this way beyond 1,000 or even 2,000 tokens.\n\nAlso, great news for the imatrix\
    \ on GPU, I will be able to toy with it soon enough then on smaller models. Thanks\
    \ for LZLV, I'm a bit short on hardware to quantize efficiently 70b models!\n\n\
    Here comes a relatively extensive perplexity test of your 4 quants, because they\
    \ are the first 70b quants I test beyond Ikawakow's and I wanted to get a good\
    \ look at the quality of the new quantizations.\n\n\n![2024-01-15 22_27_43-evaluations.csv\
    \ \u2014 LibreOffice Calc.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/LHL28X1QstV4xwnI9xBWU.png)\n"
  created_at: 2024-01-15 21:24:47+00:00
  edited: true
  hidden: false
  id: 65a5a29f2823ba72ed1f8977
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-16T06:21:00.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6196144819259644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p><a href="https://huggingface.co/Artefact2/lzlv_70b-GGUF-2bit">https://huggingface.co/Artefact2/lzlv_70b-GGUF-2bit</a></p>

          '
        raw: https://huggingface.co/Artefact2/lzlv_70b-GGUF-2bit
        updatedAt: '2024-01-16T06:21:00.726Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a6204c95a29115921c7415
    type: comment
  author: Artefact2
  content: https://huggingface.co/Artefact2/lzlv_70b-GGUF-2bit
  created_at: 2024-01-16 06:21:00+00:00
  edited: false
  hidden: false
  id: 65a6204c95a29115921c7415
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T13:50:24.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.7494369745254517
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Super, merci beaucoup!</p>

          '
        raw: Super, merci beaucoup!
        updatedAt: '2024-01-16T13:50:24.089Z'
      numEdits: 0
      reactions: []
    id: 65a689a05fe6413360d96092
    type: comment
  author: Nexesenex
  content: Super, merci beaucoup!
  created_at: 2024-01-16 13:50:24+00:00
  edited: false
  hidden: false
  id: 65a689a05fe6413360d96092
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-19T03:49:13.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5652511119842529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Once again, you nailed it with your iMatrix, because it''s quite
          tricky.<br>I played with the LZLV-70B-v1.0-IQ2_XS quant. It''s honestly
          rich and coherent for a single GPU use, I could push a few stories to 7.4k
          tokens (with rope 1 22277) without problems at mono-GPU speed.<br>If you''re
          in the mood to make more quants like this, I''d suggest you :</p>

          <ul>

          <li><a href="https://huggingface.co/sophosympatheia/Midnight-Rose-70B-v1.0">https://huggingface.co/sophosympatheia/Midnight-Rose-70B-v1.0</a>
          (no GGUF yet, and the Bloke ain''t making SOTA yes).</li>

          <li><a href="https://huggingface.co/Sao10K/WinterGoddess-1.4x-70B-L2">https://huggingface.co/Sao10K/WinterGoddess-1.4x-70B-L2</a>
          (a very good all rounder, like Aurora Nights and LZLV)</li>

          </ul>

          <p>Benchs :<br>LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,hellaswag,81.75<br>LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.4105,512</p>

          <p>LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,hellaswag,83.25<br>LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.7768,512</p>

          <p>LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,hellaswag,82.75<br>LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.1369,512</p>

          <p>LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,hellaswag,81.5<br>LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.3750,512</p>

          <p>LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,hellaswag,82.75<br>LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,wikitext,3.7827,512</p>

          '
        raw: 'Once again, you nailed it with your iMatrix, because it''s quite tricky.

          I played with the LZLV-70B-v1.0-IQ2_XS quant. It''s honestly rich and coherent
          for a single GPU use, I could push a few stories to 7.4k tokens (with rope
          1 22277) without problems at mono-GPU speed.

          If you''re in the mood to make more quants like this, I''d suggest you :

          - https://huggingface.co/sophosympatheia/Midnight-Rose-70B-v1.0 (no GGUF
          yet, and the Bloke ain''t making SOTA yes).

          - https://huggingface.co/Sao10K/WinterGoddess-1.4x-70B-L2 (a very good all
          rounder, like Aurora Nights and LZLV)


          Benchs :

          LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,hellaswag,81.75

          LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.4105,512


          LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,hellaswag,83.25

          LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.7768,512


          LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,hellaswag,82.75

          LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.1369,512


          LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,hellaswag,81.5

          LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.3750,512


          LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,hellaswag,82.75

          LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,wikitext,3.7827,512'
        updatedAt: '2024-01-19T03:49:13.836Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65a9f139043d53781a839833
    type: comment
  author: Nexesenex
  content: 'Once again, you nailed it with your iMatrix, because it''s quite tricky.

    I played with the LZLV-70B-v1.0-IQ2_XS quant. It''s honestly rich and coherent
    for a single GPU use, I could push a few stories to 7.4k tokens (with rope 1 22277)
    without problems at mono-GPU speed.

    If you''re in the mood to make more quants like this, I''d suggest you :

    - https://huggingface.co/sophosympatheia/Midnight-Rose-70B-v1.0 (no GGUF yet,
    and the Bloke ain''t making SOTA yes).

    - https://huggingface.co/Sao10K/WinterGoddess-1.4x-70B-L2 (a very good all rounder,
    like Aurora Nights and LZLV)


    Benchs :

    LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,hellaswag,81.75

    LZLV-70B-v1.0-IQ2_XS-2.36bpw.gguf,-,wikitext,4.4105,512


    LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,hellaswag,83.25

    LZLV-70B-v1.0-IQ2_XXS-2.12bpw.gguf,-,wikitext,4.7768,512


    LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,hellaswag,82.75

    LZLV-70B-v1.0-Q2_K-2.95bpw.gguf,-,wikitext,4.1369,512


    LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,hellaswag,81.5

    LZLV-70B-v1.0-Q2_K_S-2.70bpw.gguf,-,wikitext,4.3750,512


    LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,hellaswag,82.75

    LZLV-70B-v1.0-Q3_K_S-3.47bpw.gguf,-,wikitext,3.7827,512'
  created_at: 2024-01-19 03:49:13+00:00
  edited: false
  hidden: false
  id: 65a9f139043d53781a839833
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-19T12:35:48.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750930070877075
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p>Thanks, I''ll get started on these two, should be up relatively
          soon!</p>

          '
        raw: Thanks, I'll get started on these two, should be up relatively soon!
        updatedAt: '2024-01-19T12:35:48.774Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65aa6ca4e0ee7990a628ba15
    type: comment
  author: Artefact2
  content: Thanks, I'll get started on these two, should be up relatively soon!
  created_at: 2024-01-19 12:35:48+00:00
  edited: false
  hidden: false
  id: 65aa6ca4e0ee7990a628ba15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-19T22:22:26.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.402619868516922
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p><a href="https://huggingface.co/Artefact2/Midnight-Rose-70B-v1.0-GGUF">https://huggingface.co/Artefact2/Midnight-Rose-70B-v1.0-GGUF</a></p>

          '
        raw: https://huggingface.co/Artefact2/Midnight-Rose-70B-v1.0-GGUF
        updatedAt: '2024-01-19T22:22:26.604Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65aaf622d6b4c93ba6a4accc
    type: comment
  author: Artefact2
  content: https://huggingface.co/Artefact2/Midnight-Rose-70B-v1.0-GGUF
  created_at: 2024-01-19 22:22:26+00:00
  edited: false
  hidden: false
  id: 65aaf622d6b4c93ba6a4accc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-19T23:13:48.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9494354724884033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>You beat me on Midnight Rose, I started it 1h ago but I''m still
          at the Q8_0.<br>Thanks, Man, I''ll use yours instead lol!</p>

          <p>Here''s some numbers :</p>

          <p>Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,5.4897,512,512,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,<br>Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.7105,2048,2048,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,<br>Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.5765,4096,4096,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,<br>Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,hellaswag,85.75,400,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,<br>Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,Winogrande,74.0331,,1267,2024-01-19
          05:40:00,,01.3b,Llama_2,4096,,,GGUF,Sophosympatheia,Artefact2,</p>

          <p>Edit : I know I''m asking a lot. But here comes something else which
          came to interest :<br>mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF</p>

          <p>A guy left this model without the fp16 weights. I tested it, it works,
          including at long context because it has a rope 8, which scales down nicely
          to 4 and even 2, for a better perplexity and hellaswag. I chatted a bit
          with it and it''s allright.<br>Could you iMatrix it, and publish the 2 bits
          and Q3_K_S/Q3_K_M quants?<br>It''ll be a base perplexity loss of 0.05 and
          a Hellaswag loss of 1-2 compared to the fp16, but as Aurelian is still in
          fine tuning stage, it''s for now the best 70b 32k (or even 16 or 8) that
          we have).</p>

          <p>Here are my tests of this lost gem :<br>Rope 8 10000<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.2177,4096<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1324,6144<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.3923,2048<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.4945,1536<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.6700,1024<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,5.2577,512<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,84.5,,400<br>Rope
          4 10000<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.5762,2048<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1235,512<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,87.25,,400<br>Rope
          2 10000<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.3394
          *327,2048<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.8254,512<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,88,,400<br>Rope
          1 10000<br>WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,85,,400</p>

          '
        raw: 'You beat me on Midnight Rose, I started it 1h ago but I''m still at
          the Q8_0.

          Thanks, Man, I''ll use yours instead lol!


          Here''s some numbers :


          Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,5.4897,512,512,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

          Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.7105,2048,2048,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

          Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.5765,4096,4096,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

          Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,hellaswag,85.75,400,2024-01-09
          01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

          Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,Winogrande,74.0331,,1267,2024-01-19
          05:40:00,,01.3b,Llama_2,4096,,,GGUF,Sophosympatheia,Artefact2,


          Edit : I know I''m asking a lot. But here comes something else which came
          to interest :

          mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF


          A guy left this model without the fp16 weights. I tested it, it works, including
          at long context because it has a rope 8, which scales down nicely to 4 and
          even 2, for a better perplexity and hellaswag. I chatted a bit with it and
          it''s allright.

          Could you iMatrix it, and publish the 2 bits and Q3_K_S/Q3_K_M quants?

          It''ll be a base perplexity loss of 0.05 and a Hellaswag loss of 1-2 compared
          to the fp16, but as Aurelian is still in fine tuning stage, it''s for now
          the best 70b 32k (or even 16 or 8) that we have).


          Here are my tests of this lost gem :

          Rope 8 10000

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.2177,4096

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1324,6144

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.3923,2048

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.4945,1536

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.6700,1024

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,5.2577,512

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,84.5,,400

          Rope 4 10000

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.5762,2048

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1235,512

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,87.25,,400

          Rope 2 10000

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.3394 *327,2048

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.8254,512

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,88,,400

          Rope 1 10000

          WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,85,,400'
        updatedAt: '2024-01-20T04:34:03.018Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65ab022cac588f2a1c29598e
    type: comment
  author: Nexesenex
  content: 'You beat me on Midnight Rose, I started it 1h ago but I''m still at the
    Q8_0.

    Thanks, Man, I''ll use yours instead lol!


    Here''s some numbers :


    Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,5.4897,512,512,2024-01-09
    01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

    Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.7105,2048,2048,2024-01-09
    01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

    Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,wikitext,4.5765,4096,4096,2024-01-09
    01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

    Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,hellaswag,85.75,400,2024-01-09 01:40:00,,70b,Llama_2,4096,15:35,1/5.18,GGUF,Sophosympatheia,Artefact2,

    Midnight-Rose-70B-v1.0-IQ2_XS_Art_Wiki.gguf,-,Winogrande,74.0331,,1267,2024-01-19
    05:40:00,,01.3b,Llama_2,4096,,,GGUF,Sophosympatheia,Artefact2,


    Edit : I know I''m asking a lot. But here comes something else which came to interest
    :

    mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF


    A guy left this model without the fp16 weights. I tested it, it works, including
    at long context because it has a rope 8, which scales down nicely to 4 and even
    2, for a better perplexity and hellaswag. I chatted a bit with it and it''s allright.

    Could you iMatrix it, and publish the 2 bits and Q3_K_S/Q3_K_M quants?

    It''ll be a base perplexity loss of 0.05 and a Hellaswag loss of 1-2 compared
    to the fp16, but as Aurelian is still in fine tuning stage, it''s for now the
    best 70b 32k (or even 16 or 8) that we have).


    Here are my tests of this lost gem :

    Rope 8 10000

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.2177,4096

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1324,6144

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.3923,2048

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.4945,1536

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.6700,1024

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,5.2577,512

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,84.5,,400

    Rope 4 10000

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.5762,2048

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,4.1235,512

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,87.25,,400

    Rope 2 10000

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.3394 *327,2048

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,wikitext,3.8254,512

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,88,,400

    Rope 1 10000

    WinterGoddess-1.4x-limarpv3-70B-L2-32k.Q4_K_S.gguf,-,hellaswag,85,,400'
  created_at: 2024-01-19 23:13:48+00:00
  edited: true
  hidden: false
  id: 65ab022cac588f2a1c29598e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-20T14:49:23.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8728574514389038
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p><a href="https://huggingface.co/Artefact2/WinterGoddess-1.4x-70B-L2-GGUF">https://huggingface.co/Artefact2/WinterGoddess-1.4x-70B-L2-GGUF</a></p>

          <p>As for WinterGoddess-1.4x-limarpv3-70B-L2-32k, there''s not much that
          can be done without f16 weights available.</p>

          '
        raw: 'https://huggingface.co/Artefact2/WinterGoddess-1.4x-70B-L2-GGUF


          As for WinterGoddess-1.4x-limarpv3-70B-L2-32k, there''s not much that can
          be done without f16 weights available.'
        updatedAt: '2024-01-20T14:49:23.485Z'
      numEdits: 0
      reactions: []
    id: 65abdd73df466514067d6542
    type: comment
  author: Artefact2
  content: 'https://huggingface.co/Artefact2/WinterGoddess-1.4x-70B-L2-GGUF


    As for WinterGoddess-1.4x-limarpv3-70B-L2-32k, there''s not much that can be done
    without f16 weights available.'
  created_at: 2024-01-20 14:49:23+00:00
  edited: false
  hidden: false
  id: 65abdd73df466514067d6542
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-20T18:40:22.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9132029414176941
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Actually, it can be done.</p>

          <ol>

          <li>Requantize the Q4_K_S in q8_0, the best base for requant even from a
          smaller quant source (I tested that a while ago).</li>

          <li>Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to
          be set or not, I''d say yes though, and the iMatrix would logically be calibrated
          on the chosen rope).</li>

          <li>Make the quants from q8_0 with the q8_0 iMatrix.</li>

          </ol>

          <p>Thanks for WinterGoddess!</p>

          '
        raw: 'Actually, it can be done.

          1. Requantize the Q4_K_S in q8_0, the best base for requant even from a
          smaller quant source (I tested that a while ago).

          2. Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to
          be set or not, I''d say yes though, and the iMatrix would logically be calibrated
          on the chosen rope).

          3. Make the quants from q8_0 with the q8_0 iMatrix.


          Thanks for WinterGoddess!'
        updatedAt: '2024-01-20T18:40:22.431Z'
      numEdits: 0
      reactions: []
    id: 65ac139699c3bd19c7b55181
    type: comment
  author: Nexesenex
  content: 'Actually, it can be done.

    1. Requantize the Q4_K_S in q8_0, the best base for requant even from a smaller
    quant source (I tested that a while ago).

    2. Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to be set
    or not, I''d say yes though, and the iMatrix would logically be calibrated on
    the chosen rope).

    3. Make the quants from q8_0 with the q8_0 iMatrix.


    Thanks for WinterGoddess!'
  created_at: 2024-01-20 18:40:22+00:00
  edited: false
  hidden: false
  id: 65ac139699c3bd19c7b55181
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-20T19:41:26.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992795467376709
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p>I don''t think it''s worth requantizing from Q4KS, so I won''t do
          it. But feel free to try it yourself!</p>

          '
        raw: I don't think it's worth requantizing from Q4KS, so I won't do it. But
          feel free to try it yourself!
        updatedAt: '2024-01-20T19:41:26.683Z'
      numEdits: 0
      reactions: []
    id: 65ac21e60150f64adf429383
    type: comment
  author: Artefact2
  content: I don't think it's worth requantizing from Q4KS, so I won't do it. But
    feel free to try it yourself!
  created_at: 2024-01-20 19:41:26+00:00
  edited: false
  hidden: false
  id: 65ac21e60150f64adf429383
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-20T20:26:56.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824802279472351
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Okay, that one is for me then, and I hope I''ll make it right!<br>I''ll
          still come to beg for quants of more appetizing quants, though!</p>

          '
        raw: 'Okay, that one is for me then, and I hope I''ll make it right!

          I''ll still come to beg for quants of more appetizing quants, though!'
        updatedAt: '2024-01-20T20:26:56.873Z'
      numEdits: 0
      reactions: []
    id: 65ac2c90356bf23b4aff25f5
    type: comment
  author: Nexesenex
  content: 'Okay, that one is for me then, and I hope I''ll make it right!

    I''ll still come to beg for quants of more appetizing quants, though!'
  created_at: 2024-01-20 20:26:56+00:00
  edited: false
  hidden: false
  id: 65ac2c90356bf23b4aff25f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-21T03:25:23.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9132029414176941
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Actually, it can be done.</p>

          <ol>

          <li>Requantize the Q4_K_S in q8_0, the best base for requant even from a
          smaller quant source (I tested that a while ago).</li>

          <li>Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to
          be set or not, I''d say yes though, and the iMatrix would logically be calibrated
          on the chosen rope).</li>

          <li>Make the quants from q8_0 with the q8_0 iMatrix.</li>

          </ol>

          <p>Thanks for WinterGoddess!</p>

          '
        raw: 'Actually, it can be done.

          1. Requantize the Q4_K_S in q8_0, the best base for requant even from a
          smaller quant source (I tested that a while ago).

          2. Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to
          be set or not, I''d say yes though, and the iMatrix would logically be calibrated
          on the chosen rope).

          3. Make the quants from q8_0 with the q8_0 iMatrix.


          Thanks for WinterGoddess!'
        updatedAt: '2024-01-21T03:25:23.874Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65ac8ea3f8111f40c0e915e0
    id: 65ac8ea3f8111f40c0e915dd
    type: comment
  author: Nexesenex
  content: 'Actually, it can be done.

    1. Requantize the Q4_K_S in q8_0, the best base for requant even from a smaller
    quant source (I tested that a while ago).

    2. Make the iMatrix of the obtained q8_0 (I''m not sure if rope needs to be set
    or not, I''d say yes though, and the iMatrix would logically be calibrated on
    the chosen rope).

    3. Make the quants from q8_0 with the q8_0 iMatrix.


    Thanks for WinterGoddess!'
  created_at: 2024-01-21 03:25:23+00:00
  edited: false
  hidden: false
  id: 65ac8ea3f8111f40c0e915dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-21T03:25:23.000Z'
    data:
      status: closed
    id: 65ac8ea3f8111f40c0e915e0
    type: status-change
  author: Nexesenex
  created_at: 2024-01-21 03:25:23+00:00
  id: 65ac8ea3f8111f40c0e915e0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-21T03:25:48.000Z'
    data:
      status: open
    id: 65ac8ebc3e7c8dcbbe4b8551
    type: status-change
  author: Nexesenex
  created_at: 2024-01-21 03:25:48+00:00
  id: 65ac8ebc3e7c8dcbbe4b8551
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-21T20:35:14.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8965246081352234
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>So here''s what I did :</p>

          <p><a href="https://huggingface.co/Nexesenex/WinterGoddess-1.4x-limarpv3-70B-L2-32k-Requant.GGUF">https://huggingface.co/Nexesenex/WinterGoddess-1.4x-limarpv3-70B-L2-32k-Requant.GGUF</a></p>

          <p>The IQ2 quants are very slow to crunch on my hardware, I''ll do them
          a bit later, but the result in Q2_K and Q3_K_S are extremely satisfactory!</p>

          '
        raw: 'So here''s what I did :


          https://huggingface.co/Nexesenex/WinterGoddess-1.4x-limarpv3-70B-L2-32k-Requant.GGUF


          The IQ2 quants are very slow to crunch on my hardware, I''ll do them a bit
          later, but the result in Q2_K and Q3_K_S are extremely satisfactory!'
        updatedAt: '2024-01-21T20:35:39.632Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65ad8002dd55025184121abd
    type: comment
  author: Nexesenex
  content: 'So here''s what I did :


    https://huggingface.co/Nexesenex/WinterGoddess-1.4x-limarpv3-70B-L2-32k-Requant.GGUF


    The IQ2 quants are very slow to crunch on my hardware, I''ll do them a bit later,
    but the result in Q2_K and Q3_K_S are extremely satisfactory!'
  created_at: 2024-01-21 20:35:14+00:00
  edited: true
  hidden: false
  id: 65ad8002dd55025184121abd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-22T05:38:19.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8625288009643555
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Here''s another one in fp16 which might be worth a 2/3 bits quantization
          for the Llama 2 70b vanilla experience at 32k context :<br><a href="https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k">https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k</a></p>

          <p>I just don''t have the CPU power to make the IQ2 quants in a reasonable
          time ! :)</p>

          '
        raw: 'Here''s another one in fp16 which might be worth a 2/3 bits quantization
          for the Llama 2 70b vanilla experience at 32k context :

          https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k


          I just don''t have the CPU power to make the IQ2 quants in a reasonable
          time ! :)'
        updatedAt: '2024-01-22T05:39:00.577Z'
      numEdits: 1
      reactions: []
    id: 65adff4bd63812c33e71d8af
    type: comment
  author: Nexesenex
  content: 'Here''s another one in fp16 which might be worth a 2/3 bits quantization
    for the Llama 2 70b vanilla experience at 32k context :

    https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k


    I just don''t have the CPU power to make the IQ2 quants in a reasonable time !
    :)'
  created_at: 2024-01-22 05:38:19+00:00
  edited: true
  hidden: false
  id: 65adff4bd63812c33e71d8af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-23T06:28:58.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34776806831359863
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p><a href="https://huggingface.co/Artefact2/Yarn-Llama-2-70b-32k-GGUF">https://huggingface.co/Artefact2/Yarn-Llama-2-70b-32k-GGUF</a></p>

          '
        raw: https://huggingface.co/Artefact2/Yarn-Llama-2-70b-32k-GGUF
        updatedAt: '2024-01-23T06:28:58.712Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65af5caafd71cbc318ee5121
    type: comment
  author: Artefact2
  content: https://huggingface.co/Artefact2/Yarn-Llama-2-70b-32k-GGUF
  created_at: 2024-01-23 06:28:58+00:00
  edited: false
  hidden: false
  id: 65af5caafd71cbc318ee5121
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-23T11:39:02.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7652804851531982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Thanks man.<br>I tested it, and it works like a charm.<br>Also :<br>Rope
          8 :<br>Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6948,512<br>Rope 2 :<br>Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6868,512<br>Your
          quants are really neat!<br>And basically, it seems useless to lower the
          rope with Yarn. I love it!</p>

          '
        raw: 'Thanks man.

          I tested it, and it works like a charm.

          Also :

          Rope 8 :

          Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6948,512

          Rope 2 :

          Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6868,512

          Your quants are really neat!

          And basically, it seems useless to lower the rope with Yarn. I love it!'
        updatedAt: '2024-01-23T11:39:02.181Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Artefact2
    id: 65afa55630e33d1b60cb71f7
    type: comment
  author: Nexesenex
  content: 'Thanks man.

    I tested it, and it works like a charm.

    Also :

    Rope 8 :

    Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6948,512

    Rope 2 :

    Yarn-Llama-2-70b-32k-Q3_K_S,-,wikitext,3.6868,512

    Your quants are really neat!

    And basically, it seems useless to lower the rope with Yarn. I love it!'
  created_at: 2024-01-23 11:39:02+00:00
  edited: false
  hidden: false
  id: 65afa55630e33d1b60cb71f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
      fullname: Romain D.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Artefact2
      type: user
    createdAt: '2024-01-23T12:06:27.000Z'
    data:
      edited: false
      editors:
      - Artefact2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8569828867912292
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65a3e7ede82b0b8490667486/i97WAM9a13qSFbRkJ6GVm.jpeg?w=200&h=200&f=face
          fullname: Romain D.
          isHf: false
          isPro: false
          name: Artefact2
          type: user
        html: '<p>Thanks! Always open to more model suggestions (70B or under).</p>

          '
        raw: Thanks! Always open to more model suggestions (70B or under).
        updatedAt: '2024-01-23T12:06:27.425Z'
      numEdits: 0
      reactions: []
    id: 65afabc322762a684ed75ad6
    type: comment
  author: Artefact2
  content: Thanks! Always open to more model suggestions (70B or under).
  created_at: 2024-01-23 12:06:27+00:00
  edited: false
  hidden: false
  id: 65afabc322762a684ed75ad6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Artefact2/Aurora-Nights-70B-v1.0-GGUF-2bit
repo_type: model
status: open
target_branch: null
title: Merci Romain !
