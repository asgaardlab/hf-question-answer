!!python/object:huggingface_hub.community.DiscussionWithDetails
author: winglian
conflicting_files: null
created_at: 2023-05-01 15:53:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-01T16:53:08.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>Can you provide any insight on the rationale for training the MLP
          vs the Self attention modules?</p>

          '
        raw: Can you provide any insight on the rationale for training the MLP vs
          the Self attention modules?
        updatedAt: '2023-05-01T16:53:08.378Z'
      numEdits: 0
      reactions: []
    id: 644fee74f8b353c94918b661
    type: comment
  author: winglian
  content: Can you provide any insight on the rationale for training the MLP vs the
    Self attention modules?
  created_at: 2023-05-01 15:53:08+00:00
  edited: false
  hidden: false
  id: 644fee74f8b353c94918b661
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/081d99f42c5f0feceff0b5516ff51399.svg
      fullname: Chao-Hong Tan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chtan
      type: user
    createdAt: '2023-05-05T02:12:19.000Z'
    data:
      edited: false
      editors:
      - chtan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/081d99f42c5f0feceff0b5516ff51399.svg
          fullname: Chao-Hong Tan
          isHf: false
          isPro: false
          name: chtan
          type: user
        html: "<p>Followed He et al. 2022, we hypothesize that this is because the\
          \ FFN learns task-speci\uFB01c textual patterns (Geva et al., 2021), while\
          \ attention learns pairwise positional interactions which do not require\
          \ large capacity for adapting to new tasks.</p>\n<p>[1] Junxian He, Chunting\
          \ Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig: Towards a Unified\
          \ View of Parameter-Efficient Transfer Learning. ICLR 2022<br>[2] Geva M,\
          \ Schuster R, Berant J, et al. Transformer Feed-Forward Layers Are Key-Value\
          \ Memories[C]//Proceedings of the 2021 Conference on Empirical Methods in\
          \ Natural Language Processing. 2021: 5484-5495.</p>\n"
        raw: "Followed He et al. 2022, we hypothesize that this is because the FFN\
          \ learns task-speci\uFB01c textual patterns (Geva et al., 2021), while attention\
          \ learns pairwise positional interactions which do not require large capacity\
          \ for adapting to new tasks.\n\n[1] Junxian He, Chunting Zhou, Xuezhe Ma,\
          \ Taylor Berg-Kirkpatrick, Graham Neubig: Towards a Unified View of Parameter-Efficient\
          \ Transfer Learning. ICLR 2022\n[2] Geva M, Schuster R, Berant J, et al.\
          \ Transformer Feed-Forward Layers Are Key-Value Memories[C]//Proceedings\
          \ of the 2021 Conference on Empirical Methods in Natural Language Processing.\
          \ 2021: 5484-5495."
        updatedAt: '2023-05-05T02:12:19.357Z'
      numEdits: 0
      reactions: []
    id: 64546603ecdd69b813500c6b
    type: comment
  author: chtan
  content: "Followed He et al. 2022, we hypothesize that this is because the FFN learns\
    \ task-speci\uFB01c textual patterns (Geva et al., 2021), while attention learns\
    \ pairwise positional interactions which do not require large capacity for adapting\
    \ to new tasks.\n\n[1] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,\
    \ Graham Neubig: Towards a Unified View of Parameter-Efficient Transfer Learning.\
    \ ICLR 2022\n[2] Geva M, Schuster R, Berant J, et al. Transformer Feed-Forward\
    \ Layers Are Key-Value Memories[C]//Proceedings of the 2021 Conference on Empirical\
    \ Methods in Natural Language Processing. 2021: 5484-5495."
  created_at: 2023-05-05 01:12:19+00:00
  edited: false
  hidden: false
  id: 64546603ecdd69b813500c6b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chtan/gpt4-alpaca-lora_mlp-65b
repo_type: model
status: open
target_branch: null
title: MLP vs Self Attention
