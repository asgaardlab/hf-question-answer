!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brownyeyez
conflicting_files: null
created_at: 2023-09-05 07:46:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91c66891d144028d092709a85c971c6d.svg
      fullname: Thanh Dat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brownyeyez
      type: user
    createdAt: '2023-09-05T08:46:40.000Z'
    data:
      edited: false
      editors:
      - brownyeyez
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6193941235542297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91c66891d144028d092709a85c971c6d.svg
          fullname: Thanh Dat
          isHf: false
          isPro: false
          name: brownyeyez
          type: user
        html: "<p>In my experiments, i realize that the model architecture of polylm-13b\
          \ is difference from polylm-multialpaca-13b version, but in model cards,\
          \ it was said that multialpaca is a SFT version of polylm-13b. This is so\
          \ confusing that the model type is not matching that in config of base model\
          \ polylm the LMhead is PolyLMHead, and in SFT version is GPT2LMHead, can\
          \ you explain clearly about this mismatching?</p>\n<p>And if i can using\
          \ the GPT2LMHead replace for the base model PolyLMHead, and if i can, how\
          \ can i convert the tensor shape of model to this type of LMHead? (my aims\
          \ to deploy model with vLLM, supported GPT2LMHead)</p>\n<p>This is config\
          \ of two version, polylm-13b:</p>\n<pre><code class=\"language-bash\">{\n\
          \  <span class=\"hljs-string\">\"activation_function\"</span>: <span class=\"\
          hljs-string\">\"gelu_fast\"</span>,\n  <span class=\"hljs-string\">\"architectures\"\
          </span>: [\n    <span class=\"hljs-string\">\"PolyLMHeadModel\"</span>\n\
          \  ],\n  <span class=\"hljs-string\">\"auto_map\"</span>: {\n    <span class=\"\
          hljs-string\">\"AutoModelForCausalLM\"</span>: <span class=\"hljs-string\"\
          >\"modeling_polylm.PolyLMHeadModel\"</span>\n  },\n  <span class=\"hljs-string\"\
          >\"attn_pdrop\"</span>: 0.0,\n  <span class=\"hljs-string\">\"embd_pdrop\"\
          </span>: 0.0,\n  <span class=\"hljs-string\">\"eos_token_id\"</span>: 2,\n\
          \  <span class=\"hljs-string\">\"initializer_range\"</span>: 0.02,\n  <span\
          \ class=\"hljs-string\">\"layer_norm_epsilon\"</span>: 1e-05,\n  <span class=\"\
          hljs-string\">\"model_type\"</span>: <span class=\"hljs-string\">\"gpt2\"\
          </span>,\n  <span class=\"hljs-string\">\"n_ctx\"</span>: 2048,\n  <span\
          \ class=\"hljs-string\">\"n_embd\"</span>: 5120,\n  <span class=\"hljs-string\"\
          >\"n_head\"</span>: 40,\n  <span class=\"hljs-string\">\"n_inner\"</span>:\
          \ 20480,\n  <span class=\"hljs-string\">\"n_layer\"</span>: 40,\n  <span\
          \ class=\"hljs-string\">\"n_positions\"</span>: 2048,\n  <span class=\"\
          hljs-string\">\"reorder_and_upcast_attn\"</span>: <span class=\"hljs-literal\"\
          >false</span>,\n  <span class=\"hljs-string\">\"resid_pdrop\"</span>: 0.0,\n\
          \  <span class=\"hljs-string\">\"scale_attn_by_inverse_layer_idx\"</span>:\
          \ <span class=\"hljs-literal\">false</span>,\n  <span class=\"hljs-string\"\
          >\"scale_attn_weights\"</span>: <span class=\"hljs-literal\">true</span>,\n\
          \  <span class=\"hljs-string\">\"summary_activation\"</span>: null,\n  <span\
          \ class=\"hljs-string\">\"summary_first_dropout\"</span>: 0.0,\n  <span\
          \ class=\"hljs-string\">\"summary_proj_to_labels\"</span>: <span class=\"\
          hljs-literal\">true</span>,\n  <span class=\"hljs-string\">\"summary_type\"\
          </span>: <span class=\"hljs-string\">\"cls_index\"</span>,\n  <span class=\"\
          hljs-string\">\"summary_use_proj\"</span>: <span class=\"hljs-literal\"\
          >true</span>,\n  <span class=\"hljs-string\">\"tokenizer_class\"</span>:\
          \ <span class=\"hljs-string\">\"AutoTokenizer\"</span>,\n  <span class=\"\
          hljs-string\">\"transformers_version\"</span>: <span class=\"hljs-string\"\
          >\"4.31.0\"</span>,\n  <span class=\"hljs-string\">\"use_cache\"</span>:\
          \ <span class=\"hljs-literal\">true</span>,\n  <span class=\"hljs-string\"\
          >\"vocab_size\"</span>: 256000\n}\n</code></pre>\n<p>and poly-multialpaca-13b:</p>\n\
          <pre><code class=\"language-bash\">{\n  <span class=\"hljs-string\">\"activation_function\"\
          </span>: <span class=\"hljs-string\">\"gelu_fast\"</span>,\n  <span class=\"\
          hljs-string\">\"architectures\"</span>: [\n    <span class=\"hljs-string\"\
          >\"GPT2LMHeadModel\"</span>\n  ],\n  <span class=\"hljs-string\">\"attn_pdrop\"\
          </span>: 0.0,\n  <span class=\"hljs-string\">\"bos_token_id\"</span>: 255999,\n\
          \  <span class=\"hljs-string\">\"embd_pdrop\"</span>: 0.0,\n  <span class=\"\
          hljs-string\">\"eos_token_id\"</span>: 255999,\n  <span class=\"hljs-string\"\
          >\"initializer_range\"</span>: 0.02,\n  <span class=\"hljs-string\">\"layer_norm_epsilon\"\
          </span>: 1e-05,\n  <span class=\"hljs-string\">\"model_type\"</span>: <span\
          \ class=\"hljs-string\">\"gpt2\"</span>,\n  <span class=\"hljs-string\"\
          >\"n_embd\"</span>: 5120,\n  <span class=\"hljs-string\">\"n_head\"</span>:\
          \ 40,\n  <span class=\"hljs-string\">\"n_inner\"</span>: 20480,\n  <span\
          \ class=\"hljs-string\">\"n_layer\"</span>: 40,\n  <span class=\"hljs-string\"\
          >\"n_positions\"</span>: 2048,\n  <span class=\"hljs-string\">\"reorder_and_upcast_attn\"\
          </span>: <span class=\"hljs-literal\">false</span>,\n  <span class=\"hljs-string\"\
          >\"resid_pdrop\"</span>: 0.0,\n  <span class=\"hljs-string\">\"scale_attn_by_inverse_layer_idx\"\
          </span>: <span class=\"hljs-literal\">false</span>,\n  <span class=\"hljs-string\"\
          >\"scale_attn_weights\"</span>: <span class=\"hljs-literal\">true</span>,\n\
          \  <span class=\"hljs-string\">\"summary_activation\"</span>: null,\n  <span\
          \ class=\"hljs-string\">\"summary_first_dropout\"</span>: 0.0,\n  <span\
          \ class=\"hljs-string\">\"summary_proj_to_labels\"</span>: <span class=\"\
          hljs-literal\">true</span>,\n  <span class=\"hljs-string\">\"summary_type\"\
          </span>: <span class=\"hljs-string\">\"cls_index\"</span>,\n  <span class=\"\
          hljs-string\">\"summary_use_proj\"</span>: <span class=\"hljs-literal\"\
          >true</span>,\n  <span class=\"hljs-string\">\"tokenizer_class\"</span>:\
          \ <span class=\"hljs-string\">\"AutoTokenizer\"</span>,\n  <span class=\"\
          hljs-string\">\"transformers_version\"</span>: <span class=\"hljs-string\"\
          >\"4.29.2\"</span>,\n  <span class=\"hljs-string\">\"use_cache\"</span>:\
          \ <span class=\"hljs-literal\">true</span>,\n  <span class=\"hljs-string\"\
          >\"vocab_size\"</span>: 256000\n}\n \n</code></pre>\n"
        raw: "In my experiments, i realize that the model architecture of polylm-13b\
          \ is difference from polylm-multialpaca-13b version, but in model cards,\
          \ it was said that multialpaca is a SFT version of polylm-13b. This is so\
          \ confusing that the model type is not matching that in config of base model\
          \ polylm the LMhead is PolyLMHead, and in SFT version is GPT2LMHead, can\
          \ you explain clearly about this mismatching?\r\n\r\nAnd if i can using\
          \ the GPT2LMHead replace for the base model PolyLMHead, and if i can, how\
          \ can i convert the tensor shape of model to this type of LMHead? (my aims\
          \ to deploy model with vLLM, supported GPT2LMHead)\r\n\r\nThis is config\
          \ of two version, polylm-13b:\r\n```bash\r\n{\r\n  \"activation_function\"\
          : \"gelu_fast\",\r\n  \"architectures\": [\r\n    \"PolyLMHeadModel\"\r\n\
          \  ],\r\n  \"auto_map\": {\r\n    \"AutoModelForCausalLM\": \"modeling_polylm.PolyLMHeadModel\"\
          \r\n  },\r\n  \"attn_pdrop\": 0.0,\r\n  \"embd_pdrop\": 0.0,\r\n  \"eos_token_id\"\
          : 2,\r\n  \"initializer_range\": 0.02,\r\n  \"layer_norm_epsilon\": 1e-05,\r\
          \n  \"model_type\": \"gpt2\",\r\n  \"n_ctx\": 2048,\r\n  \"n_embd\": 5120,\r\
          \n  \"n_head\": 40,\r\n  \"n_inner\": 20480,\r\n  \"n_layer\": 40,\r\n \
          \ \"n_positions\": 2048,\r\n  \"reorder_and_upcast_attn\": false,\r\n  \"\
          resid_pdrop\": 0.0,\r\n  \"scale_attn_by_inverse_layer_idx\": false,\r\n\
          \  \"scale_attn_weights\": true,\r\n  \"summary_activation\": null,\r\n\
          \  \"summary_first_dropout\": 0.0,\r\n  \"summary_proj_to_labels\": true,\r\
          \n  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n\
          \  \"tokenizer_class\": \"AutoTokenizer\",\r\n  \"transformers_version\"\
          : \"4.31.0\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 256000\r\n\
          }\r\n```\r\nand poly-multialpaca-13b:\r\n```bash\r\n{\r\n  \"activation_function\"\
          : \"gelu_fast\",\r\n  \"architectures\": [\r\n    \"GPT2LMHeadModel\"\r\n\
          \  ],\r\n  \"attn_pdrop\": 0.0,\r\n  \"bos_token_id\": 255999,\r\n  \"embd_pdrop\"\
          : 0.0,\r\n  \"eos_token_id\": 255999,\r\n  \"initializer_range\": 0.02,\r\
          \n  \"layer_norm_epsilon\": 1e-05,\r\n  \"model_type\": \"gpt2\",\r\n  \"\
          n_embd\": 5120,\r\n  \"n_head\": 40,\r\n  \"n_inner\": 20480,\r\n  \"n_layer\"\
          : 40,\r\n  \"n_positions\": 2048,\r\n  \"reorder_and_upcast_attn\": false,\r\
          \n  \"resid_pdrop\": 0.0,\r\n  \"scale_attn_by_inverse_layer_idx\": false,\r\
          \n  \"scale_attn_weights\": true,\r\n  \"summary_activation\": null,\r\n\
          \  \"summary_first_dropout\": 0.0,\r\n  \"summary_proj_to_labels\": true,\r\
          \n  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n\
          \  \"tokenizer_class\": \"AutoTokenizer\",\r\n  \"transformers_version\"\
          : \"4.29.2\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 256000\r\n\
          }\r\n \r\n```"
        updatedAt: '2023-09-05T08:46:40.087Z'
      numEdits: 0
      reactions: []
    id: 64f6eaf07c9c53d083801931
    type: comment
  author: brownyeyez
  content: "In my experiments, i realize that the model architecture of polylm-13b\
    \ is difference from polylm-multialpaca-13b version, but in model cards, it was\
    \ said that multialpaca is a SFT version of polylm-13b. This is so confusing that\
    \ the model type is not matching that in config of base model polylm the LMhead\
    \ is PolyLMHead, and in SFT version is GPT2LMHead, can you explain clearly about\
    \ this mismatching?\r\n\r\nAnd if i can using the GPT2LMHead replace for the base\
    \ model PolyLMHead, and if i can, how can i convert the tensor shape of model\
    \ to this type of LMHead? (my aims to deploy model with vLLM, supported GPT2LMHead)\r\
    \n\r\nThis is config of two version, polylm-13b:\r\n```bash\r\n{\r\n  \"activation_function\"\
    : \"gelu_fast\",\r\n  \"architectures\": [\r\n    \"PolyLMHeadModel\"\r\n  ],\r\
    \n  \"auto_map\": {\r\n    \"AutoModelForCausalLM\": \"modeling_polylm.PolyLMHeadModel\"\
    \r\n  },\r\n  \"attn_pdrop\": 0.0,\r\n  \"embd_pdrop\": 0.0,\r\n  \"eos_token_id\"\
    : 2,\r\n  \"initializer_range\": 0.02,\r\n  \"layer_norm_epsilon\": 1e-05,\r\n\
    \  \"model_type\": \"gpt2\",\r\n  \"n_ctx\": 2048,\r\n  \"n_embd\": 5120,\r\n\
    \  \"n_head\": 40,\r\n  \"n_inner\": 20480,\r\n  \"n_layer\": 40,\r\n  \"n_positions\"\
    : 2048,\r\n  \"reorder_and_upcast_attn\": false,\r\n  \"resid_pdrop\": 0.0,\r\n\
    \  \"scale_attn_by_inverse_layer_idx\": false,\r\n  \"scale_attn_weights\": true,\r\
    \n  \"summary_activation\": null,\r\n  \"summary_first_dropout\": 0.0,\r\n  \"\
    summary_proj_to_labels\": true,\r\n  \"summary_type\": \"cls_index\",\r\n  \"\
    summary_use_proj\": true,\r\n  \"tokenizer_class\": \"AutoTokenizer\",\r\n  \"\
    transformers_version\": \"4.31.0\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\"\
    : 256000\r\n}\r\n```\r\nand poly-multialpaca-13b:\r\n```bash\r\n{\r\n  \"activation_function\"\
    : \"gelu_fast\",\r\n  \"architectures\": [\r\n    \"GPT2LMHeadModel\"\r\n  ],\r\
    \n  \"attn_pdrop\": 0.0,\r\n  \"bos_token_id\": 255999,\r\n  \"embd_pdrop\": 0.0,\r\
    \n  \"eos_token_id\": 255999,\r\n  \"initializer_range\": 0.02,\r\n  \"layer_norm_epsilon\"\
    : 1e-05,\r\n  \"model_type\": \"gpt2\",\r\n  \"n_embd\": 5120,\r\n  \"n_head\"\
    : 40,\r\n  \"n_inner\": 20480,\r\n  \"n_layer\": 40,\r\n  \"n_positions\": 2048,\r\
    \n  \"reorder_and_upcast_attn\": false,\r\n  \"resid_pdrop\": 0.0,\r\n  \"scale_attn_by_inverse_layer_idx\"\
    : false,\r\n  \"scale_attn_weights\": true,\r\n  \"summary_activation\": null,\r\
    \n  \"summary_first_dropout\": 0.0,\r\n  \"summary_proj_to_labels\": true,\r\n\
    \  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n  \"tokenizer_class\"\
    : \"AutoTokenizer\",\r\n  \"transformers_version\": \"4.29.2\",\r\n  \"use_cache\"\
    : true,\r\n  \"vocab_size\": 256000\r\n}\r\n \r\n```"
  created_at: 2023-09-05 07:46:40+00:00
  edited: false
  hidden: false
  id: 64f6eaf07c9c53d083801931
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: DAMO-NLP-MT/polylm-13b
repo_type: model
status: open
target_branch: null
title: Model architecture is not same in variant models
