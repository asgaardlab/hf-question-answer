!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wiccanmind
conflicting_files: null
created_at: 2023-07-20 06:36:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-07-20T07:36:34.000Z'
    data:
      edited: false
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9207926392555237
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: "<p>Firstly, I really appreciate this amazing contribution for the multilingual\
          \ LLM and would like to extend my thanks to you.</p>\n<p>As I see it, the\
          \ pretrained model is efficient in bfloat16. However, it will dramatically\
          \ decrease performance when inferring in fploat16 (and may not generate\
          \ any tokens that make sense, relevant to discussion <a href=\"/DAMO-NLP-MT/polylm-13b/discussions/1\"\
          >#1</a>. The model just prints unk tokens).<br>So, I am pondering the question\
          \ of how to finetune the downstream task with only an fp16 supported GPU,\
          \ while the pretrained model only makes sense in bf16.</p>\n<p>Another thing,\
          \ do you plan to release the quantizing versions (int8, int4) as well?\n\
          \ </p>\n"
        raw: "Firstly, I really appreciate this amazing contribution for the multilingual\
          \ LLM and would like to extend my thanks to you.\r\n\r\nAs I see it, the\
          \ pretrained model is efficient in bfloat16. However, it will dramatically\
          \ decrease performance when inferring in fploat16 (and may not generate\
          \ any tokens that make sense, relevant to discussion #1. The model just\
          \ prints unk tokens).\r\nSo, I am pondering the question of how to finetune\
          \ the downstream task with only an fp16 supported GPU, while the pretrained\
          \ model only makes sense in bf16.\r\n\r\nAnother thing, do you plan to release\
          \ the quantizing versions (int8, int4) as well?\r\n "
        updatedAt: '2023-07-20T07:36:34.189Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - thanh991102
      - count: 1
        reaction: "\U0001F917"
        users:
        - thanh991102
    id: 64b8e402ee257c3a4cf19b98
    type: comment
  author: wiccanmind
  content: "Firstly, I really appreciate this amazing contribution for the multilingual\
    \ LLM and would like to extend my thanks to you.\r\n\r\nAs I see it, the pretrained\
    \ model is efficient in bfloat16. However, it will dramatically decrease performance\
    \ when inferring in fploat16 (and may not generate any tokens that make sense,\
    \ relevant to discussion #1. The model just prints unk tokens).\r\nSo, I am pondering\
    \ the question of how to finetune the downstream task with only an fp16 supported\
    \ GPU, while the pretrained model only makes sense in bf16.\r\n\r\nAnother thing,\
    \ do you plan to release the quantizing versions (int8, int4) as well?\r\n "
  created_at: 2023-07-20 06:36:34+00:00
  edited: false
  hidden: false
  id: 64b8e402ee257c3a4cf19b98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
      fullname: Xiangpeng Wei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pemywei
      type: user
    createdAt: '2023-07-24T03:58:02.000Z'
    data:
      edited: false
      editors:
      - pemywei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553505778312683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
          fullname: Xiangpeng Wei
          isHf: false
          isPro: false
          name: pemywei
          type: user
        html: '<p>Thank you for your attention. We understand that performing fp16
          inference using the bf16 pre-trained model can be a challenge, and we regret
          to inform you that there are no plans to release an fp16 version of the
          model at this time. However, int4 and int8 quantization versions of the
          model will be released soon.</p>

          '
        raw: Thank you for your attention. We understand that performing fp16 inference
          using the bf16 pre-trained model can be a challenge, and we regret to inform
          you that there are no plans to release an fp16 version of the model at this
          time. However, int4 and int8 quantization versions of the model will be
          released soon.
        updatedAt: '2023-07-24T03:58:02.504Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - wiccanmind
    id: 64bdf6ca891751ce9b2b5b64
    type: comment
  author: pemywei
  content: Thank you for your attention. We understand that performing fp16 inference
    using the bf16 pre-trained model can be a challenge, and we regret to inform you
    that there are no plans to release an fp16 version of the model at this time.
    However, int4 and int8 quantization versions of the model will be released soon.
  created_at: 2023-07-24 02:58:02+00:00
  edited: false
  hidden: false
  id: 64bdf6ca891751ce9b2b5b64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-07-24T04:23:45.000Z'
    data:
      edited: true
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957339882850647
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: '<blockquote>

          <p>Thank you for your attention. We understand that performing fp16 inference
          using the bf16 pre-trained model can be a challenge, and we regret to inform
          you that there are no plans to release an fp16 version of the model at this
          time. However, int4 and int8 quantization versions of the model will be
          released soon.</p>

          </blockquote>

          <p>Thank you, and I''m looking forward to the new release!<br>Please notice
          me in this discusion when it is come out.</p>

          '
        raw: "> Thank you for your attention. We understand that performing fp16 inference\
          \ using the bf16 pre-trained model can be a challenge, and we regret to\
          \ inform you that there are no plans to release an fp16 version of the model\
          \ at this time. However, int4 and int8 quantization versions of the model\
          \ will be released soon.\n\nThank you, and I'm looking forward to the new\
          \ release! \nPlease notice me in this discusion when it is come out."
        updatedAt: '2023-07-24T04:25:15.596Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - pemywei
        - NoirNeko
    id: 64bdfcd1ef8c0e42bf3a18d9
    type: comment
  author: wiccanmind
  content: "> Thank you for your attention. We understand that performing fp16 inference\
    \ using the bf16 pre-trained model can be a challenge, and we regret to inform\
    \ you that there are no plans to release an fp16 version of the model at this\
    \ time. However, int4 and int8 quantization versions of the model will be released\
    \ soon.\n\nThank you, and I'm looking forward to the new release! \nPlease notice\
    \ me in this discusion when it is come out."
  created_at: 2023-07-24 03:23:45+00:00
  edited: true
  hidden: false
  id: 64bdfcd1ef8c0e42bf3a18d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4f7bb8626fd28493dca6d561c11b7109.svg
      fullname: Le
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NoirNeko
      type: user
    createdAt: '2023-10-14T09:10:14.000Z'
    data:
      edited: false
      editors:
      - NoirNeko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8770543336868286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4f7bb8626fd28493dca6d561c11b7109.svg
          fullname: Le
          isHf: false
          isPro: false
          name: NoirNeko
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pemywei&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pemywei\">@<span class=\"\
          underline\">pemywei</span></a></span>\n\n\t</span></span> Hi, thank you\
          \ for creating the model.<br>I was wondering whether there are any updates\
          \ on the quantized version yet?<br>Is it possible to quantize using the\
          \ below project?<br><a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp\"\
          >https://github.com/ggerganov/llama.cpp</a></p>\n"
        raw: "@pemywei Hi, thank you for creating the model. \nI was wondering whether\
          \ there are any updates on the quantized version yet? \nIs it possible to\
          \ quantize using the below project?\nhttps://github.com/ggerganov/llama.cpp\n"
        updatedAt: '2023-10-14T09:10:14.011Z'
      numEdits: 0
      reactions: []
    id: 652a5af64d8e7e7646da8551
    type: comment
  author: NoirNeko
  content: "@pemywei Hi, thank you for creating the model. \nI was wondering whether\
    \ there are any updates on the quantized version yet? \nIs it possible to quantize\
    \ using the below project?\nhttps://github.com/ggerganov/llama.cpp\n"
  created_at: 2023-10-14 08:10:14+00:00
  edited: false
  hidden: false
  id: 652a5af64d8e7e7646da8551
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: DAMO-NLP-MT/polylm-13b
repo_type: model
status: open
target_branch: null
title: Finetune onfp16
