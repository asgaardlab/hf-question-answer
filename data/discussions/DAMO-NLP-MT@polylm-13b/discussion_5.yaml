!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SaTaBa
conflicting_files: null
created_at: 2023-08-09 10:50:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e29c0c4b8eb0779e065189329518e721.svg
      fullname: Sarah Bachinger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaTaBa
      type: user
    createdAt: '2023-08-09T11:50:24.000Z'
    data:
      edited: true
      editors:
      - SaTaBa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520798325538635
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e29c0c4b8eb0779e065189329518e721.svg
          fullname: Sarah Bachinger
          isHf: false
          isPro: false
          name: SaTaBa
          type: user
        html: '<p>Hello, I would love to use this model for my master thesis but unfortunately
          I get this error when trying to execute the example code. I installed sentencepiece
          and it is already lowercase, so the fixes that helped others are not working
          here.<br>I am happy about any suggestions. Thank you!</p>

          '
        raw: "Hello, I would love to use this model for my master thesis but unfortunately\
          \ I get this error when trying to execute the example code. I installed\
          \ sentencepiece and it is already lowercase, so the fixes that helped others\
          \ are not working here. \nI am happy about any suggestions. Thank you!"
        updatedAt: '2023-08-09T11:51:39.843Z'
      numEdits: 1
      reactions: []
    id: 64d37d80e1830780114015d7
    type: comment
  author: SaTaBa
  content: "Hello, I would love to use this model for my master thesis but unfortunately\
    \ I get this error when trying to execute the example code. I installed sentencepiece\
    \ and it is already lowercase, so the fixes that helped others are not working\
    \ here. \nI am happy about any suggestions. Thank you!"
  created_at: 2023-08-09 10:50:24+00:00
  edited: true
  hidden: false
  id: 64d37d80e1830780114015d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
      fullname: Xiangpeng Wei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pemywei
      type: user
    createdAt: '2023-08-10T01:33:19.000Z'
    data:
      edited: false
      editors:
      - pemywei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5243805646896362
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
          fullname: Xiangpeng Wei
          isHf: false
          isPro: false
          name: pemywei
          type: user
        html: "<p>Please make sure you have cloned the latest model files. You can\
          \ try to install the latest transformers (v4.31.0) and execute the following\
          \ example code again:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\"># pip install accelerate</span>\n\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"DAMO-NLP-MT/polylm-13b\"</span>, legacy=<span class=\"hljs-literal\"\
          >False</span>, use_fast=<span class=\"hljs-literal\">False</span>)\n\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"\
          DAMO-NLP-MT/polylm-13b\"</span>, device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\
          model.<span class=\"hljs-built_in\">eval</span>()\n\ninput_doc = <span class=\"\
          hljs-string\">f\"Beijing is the capital of China.\\nTranslate this sentence\
          \ from English to Chinese.\"</span>\n\ninputs = tokenizer(input_doc, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\n\ngenerate_ids = model.generate(inputs.input_ids,\
          \ attention_mask=inputs.attention_mask, do_sample=<span class=\"hljs-literal\"\
          >False</span>, num_beams=<span class=\"hljs-number\">4</span>, max_length=<span\
          \ class=\"hljs-number\">128</span>, early_stopping=<span class=\"hljs-literal\"\
          >True</span>)\ndecoded = tokenizer.batch_decode(generate_ids, skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>, clean_up_tokenization_spaces=<span\
          \ class=\"hljs-literal\">False</span>)[<span class=\"hljs-number\">0</span>]\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"&gt;&gt;&gt; <span class=\"hljs-subst\">{decoded}</span>\"</span>)\n\
          <span class=\"hljs-comment\">### results</span>\n<span class=\"hljs-comment\"\
          >### Beijing is the capital of China.\\nTranslate this sentence from English\
          \ to Chinese.\\\\n\u5317\u4EAC\u662F\u4E2D\u534E\u4EBA\u6C11\u5171\u548C\
          \u56FD\u7684\u9996\u90FD\u3002\\n ...</span>\n</code></pre>\n"
        raw: "Please make sure you have cloned the latest model files. You can try\
          \ to install the latest transformers (v4.31.0) and execute the following\
          \ example code again:\n```python\n# pip install accelerate\n\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          DAMO-NLP-MT/polylm-13b\", legacy=False, use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          DAMO-NLP-MT/polylm-13b\", device_map=\"auto\", trust_remote_code=True)\n\
          model.eval()\n\ninput_doc = f\"Beijing is the capital of China.\\nTranslate\
          \ this sentence from English to Chinese.\"\n\ninputs = tokenizer(input_doc,\
          \ return_tensors=\"pt\")\n\ngenerate_ids = model.generate(inputs.input_ids,\
          \ attention_mask=inputs.attention_mask, do_sample=False, num_beams=4, max_length=128,\
          \ early_stopping=True)\ndecoded = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\
          \ clean_up_tokenization_spaces=False)[0]\n\nprint(f\">>> {decoded}\")\n\
          ### results\n### Beijing is the capital of China.\\nTranslate this sentence\
          \ from English to Chinese.\\\\n\u5317\u4EAC\u662F\u4E2D\u534E\u4EBA\u6C11\
          \u5171\u548C\u56FD\u7684\u9996\u90FD\u3002\\n ...\n```"
        updatedAt: '2023-08-10T01:33:19.200Z'
      numEdits: 0
      reactions: []
    id: 64d43e5f87169466f4801516
    type: comment
  author: pemywei
  content: "Please make sure you have cloned the latest model files. You can try to\
    \ install the latest transformers (v4.31.0) and execute the following example\
    \ code again:\n```python\n# pip install accelerate\n\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    DAMO-NLP-MT/polylm-13b\", legacy=False, use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    DAMO-NLP-MT/polylm-13b\", device_map=\"auto\", trust_remote_code=True)\nmodel.eval()\n\
    \ninput_doc = f\"Beijing is the capital of China.\\nTranslate this sentence from\
    \ English to Chinese.\"\n\ninputs = tokenizer(input_doc, return_tensors=\"pt\"\
    )\n\ngenerate_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask,\
    \ do_sample=False, num_beams=4, max_length=128, early_stopping=True)\ndecoded\
    \ = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\
    \nprint(f\">>> {decoded}\")\n### results\n### Beijing is the capital of China.\\\
    nTranslate this sentence from English to Chinese.\\\\n\u5317\u4EAC\u662F\u4E2D\
    \u534E\u4EBA\u6C11\u5171\u548C\u56FD\u7684\u9996\u90FD\u3002\\n ...\n```"
  created_at: 2023-08-10 00:33:19+00:00
  edited: false
  hidden: false
  id: 64d43e5f87169466f4801516
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e29c0c4b8eb0779e065189329518e721.svg
      fullname: Sarah Bachinger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaTaBa
      type: user
    createdAt: '2023-08-10T13:22:39.000Z'
    data:
      edited: false
      editors:
      - SaTaBa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973761260509491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e29c0c4b8eb0779e065189329518e721.svg
          fullname: Sarah Bachinger
          isHf: false
          isPro: false
          name: SaTaBa
          type: user
        html: '<p>Yes, that solved my problem. Turned out I did have an older version
          of transformers and now it works like a charm! Thank you so much for the
          help and training this model!</p>

          '
        raw: Yes, that solved my problem. Turned out I did have an older version of
          transformers and now it works like a charm! Thank you so much for the help
          and training this model!
        updatedAt: '2023-08-10T13:22:39.126Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pemywei
      relatedEventId: 64d4e49f5e5f05485c9c2ab5
    id: 64d4e49f5e5f05485c9c2ab3
    type: comment
  author: SaTaBa
  content: Yes, that solved my problem. Turned out I did have an older version of
    transformers and now it works like a charm! Thank you so much for the help and
    training this model!
  created_at: 2023-08-10 12:22:39+00:00
  edited: false
  hidden: false
  id: 64d4e49f5e5f05485c9c2ab3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e29c0c4b8eb0779e065189329518e721.svg
      fullname: Sarah Bachinger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaTaBa
      type: user
    createdAt: '2023-08-10T13:22:39.000Z'
    data:
      status: closed
    id: 64d4e49f5e5f05485c9c2ab5
    type: status-change
  author: SaTaBa
  created_at: 2023-08-10 12:22:39+00:00
  id: 64d4e49f5e5f05485c9c2ab5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: DAMO-NLP-MT/polylm-13b
repo_type: model
status: closed
target_branch: null
title: Tokenizer class LlamaTokenizer does not exist or is not currently imported.
