!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xnohat
conflicting_files: null
created_at: 2023-10-26 14:41:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4b820daac23cc020c20d3e08025059.svg
      fullname: NGUYEN HONG PHUC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xnohat
      type: user
    createdAt: '2023-10-26T15:41:04.000Z'
    data:
      edited: false
      editors:
      - xnohat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8091462850570679
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4b820daac23cc020c20d3e08025059.svg
          fullname: NGUYEN HONG PHUC
          isHf: false
          isPro: false
          name: xnohat
          type: user
        html: '<p>Hi,<br>8K context length are very useful for Embedding, we stuck
          to long at 768 context length of all existed BERT based Text Embedding Model<br>But
          how we can fine-tuning this model (small or base) with another languages
          (non-english or non-latin) and not harm to 8K context length ?<br>Thanks
          Jina for this great leap</p>

          '
        raw: "Hi,\r\n8K context length are very useful for Embedding, we stuck to\
          \ long at 768 context length of all existed BERT based Text Embedding Model\r\
          \nBut how we can fine-tuning this model (small or base) with another languages\
          \ (non-english or non-latin) and not harm to 8K context length ?\r\nThanks\
          \ Jina for this great leap"
        updatedAt: '2023-10-26T15:41:04.503Z'
      numEdits: 0
      reactions: []
    id: 653a8890025e5d01723d7f78
    type: comment
  author: xnohat
  content: "Hi,\r\n8K context length are very useful for Embedding, we stuck to long\
    \ at 768 context length of all existed BERT based Text Embedding Model\r\nBut\
    \ how we can fine-tuning this model (small or base) with another languages (non-english\
    \ or non-latin) and not harm to 8K context length ?\r\nThanks Jina for this great\
    \ leap"
  created_at: 2023-10-26 14:41:04+00:00
  edited: false
  hidden: false
  id: 653a8890025e5d01723d7f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-26T21:57:20.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8913141489028931
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p>We are actually in the process of training models for more languages
          so stay tuned!</p>

          <p>Anyways, for non-latin languages, you will probably want to change to
          a tokenizer that is trained for non-english languages. You can either use
          an existing multilingual tokenizer from <a href="https://huggingface.co/bert-base-multilingual-cased">mBERT</a>
          or <a href="https://huggingface.co/xlm-roberta-base">XLM-R</a> or <a href="https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt">train
          your own tokenizer</a>.</p>

          <p>You can then freeze the encoder and only train the word embeddings with
          a procedure similar to Step 2 of <a rel="nofollow" href="https://aclanthology.org/2020.acl-main.421.pdf">this
          paper</a>.</p>

          <p>At the moment, you can only do this on embedding tasks which usually
          have lesser available training data (requires pairs or triplets). If you
          had the pretrained model with the masked language modeling (MLM) head, you
          would be able to do this with the MLM task which would only require you
          to have a corpus in your target language. Unfortunately, our pretrained
          model is not currently public.</p>

          '
        raw: 'We are actually in the process of training models for more languages
          so stay tuned!


          Anyways, for non-latin languages, you will probably want to change to a
          tokenizer that is trained for non-english languages. You can either use
          an existing multilingual tokenizer from [mBERT](https://huggingface.co/bert-base-multilingual-cased)
          or [XLM-R](https://huggingface.co/xlm-roberta-base) or [train your own tokenizer](https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt).


          You can then freeze the encoder and only train the word embeddings with
          a procedure similar to Step 2 of [this paper](https://aclanthology.org/2020.acl-main.421.pdf).


          At the moment, you can only do this on embedding tasks which usually have
          lesser available training data (requires pairs or triplets). If you had
          the pretrained model with the masked language modeling (MLM) head, you would
          be able to do this with the MLM task which would only require you to have
          a corpus in your target language. Unfortunately, our pretrained model is
          not currently public.'
        updatedAt: '2023-10-26T21:57:20.563Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - xnohat
    id: 653ae0c08abd634b83e5f8ea
    type: comment
  author: Jackmin108
  content: 'We are actually in the process of training models for more languages so
    stay tuned!


    Anyways, for non-latin languages, you will probably want to change to a tokenizer
    that is trained for non-english languages. You can either use an existing multilingual
    tokenizer from [mBERT](https://huggingface.co/bert-base-multilingual-cased) or
    [XLM-R](https://huggingface.co/xlm-roberta-base) or [train your own tokenizer](https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt).


    You can then freeze the encoder and only train the word embeddings with a procedure
    similar to Step 2 of [this paper](https://aclanthology.org/2020.acl-main.421.pdf).


    At the moment, you can only do this on embedding tasks which usually have lesser
    available training data (requires pairs or triplets). If you had the pretrained
    model with the masked language modeling (MLM) head, you would be able to do this
    with the MLM task which would only require you to have a corpus in your target
    language. Unfortunately, our pretrained model is not currently public.'
  created_at: 2023-10-26 20:57:20+00:00
  edited: false
  hidden: false
  id: 653ae0c08abd634b83e5f8ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63491dc83d8dc83a55cb749c/IoqJrOIaEnYO_S7si4KGp.jpeg?w=200&h=200&f=face
      fullname: Bo Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bwang0911
      type: user
    createdAt: '2023-11-09T09:01:03.000Z'
    data:
      status: closed
    id: 654c9fcf2772771fb2935109
    type: status-change
  author: bwang0911
  created_at: 2023-11-09 09:01:03+00:00
  id: 654c9fcf2772771fb2935109
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: jinaai/jina-embeddings-v2-small-en
repo_type: model
status: closed
target_branch: null
title: How to fine-tune model for non-english language
