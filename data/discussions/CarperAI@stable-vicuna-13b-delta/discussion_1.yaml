!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-04-28 19:00:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T20:00:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thanks very much for the new model. I'm excited to try it out.</p>\n\
          <p>I just did the merge and noticed that special_tokens_map.json contains\
          \ this:</p>\n<pre><code>{\n  \"bos_token\": \"&lt;/s&gt;\",\n  \"eos_token\"\
          : \"&lt;/s&gt;\",\n  \"pad_token\": \"[PAD]\",\n  \"unk_token\": \"&lt;/s&gt;\"\
          \n}\n</code></pre>\n<p>I've not seen a model with that layout before. Is\
          \ it correct that BOS, EOS and UNK are all the same?</p>\n<p>And relatedly,\
          \ I see in your README example that you do inference with <code>skip_special_tokens=True</code>:</p>\n\
          <pre><code>print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
          </code></pre>\n<p>Is that a requirement for this model? And if so, is that\
          \ why the <code>special_tokens_map.json</code> is like that? Or maybe given\
          \ <code>skip_special_tokens=True</code> is set, <code>special_tokens_map.json</code>\
          \ isn't even used?</p>\n<p>Reason I ask is this will have implications for\
          \ people doing inference in eg text-generation-webui. It has an option to\
          \ \"skip special tokens\" but the user needs to know to set that, if it's\
          \ required.</p>\n<p>And if they're not going to set that, it does seem odd\
          \ to me that BOS and EOS would be the same.</p>\n<p>So it'd be awesome to\
          \ get some clarification on that.</p>\n<p>Thanks again!</p>\n"
        raw: "Thanks very much for the new model. I'm excited to try it out.\r\n\r\
          \nI just did the merge and noticed that special_tokens_map.json contains\
          \ this:\r\n```\r\n{\r\n  \"bos_token\": \"</s>\",\r\n  \"eos_token\": \"\
          </s>\",\r\n  \"pad_token\": \"[PAD]\",\r\n  \"unk_token\": \"</s>\"\r\n\
          }\r\n```\r\nI've not seen a model with that layout before. Is it correct\
          \ that BOS, EOS and UNK are all the same?\r\n\r\nAnd relatedly, I see in\
          \ your README example that you do inference with `skip_special_tokens=True`:\r\
          \n```\r\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\r\n\
          ```\r\n\r\nIs that a requirement for this model? And if so, is that why\
          \ the `special_tokens_map.json` is like that? Or maybe given `skip_special_tokens=True`\
          \ is set, `special_tokens_map.json` isn't even used?\r\n\r\nReason I ask\
          \ is this will have implications for people doing inference in eg text-generation-webui.\
          \ It has an option to \"skip special tokens\" but the user needs to know\
          \ to set that, if it's required.\r\n\r\nAnd if they're not going to set\
          \ that, it does seem odd to me that BOS and EOS would be the same.\r\n\r\
          \nSo it'd be awesome to get some clarification on that.\r\n\r\nThanks again!"
        updatedAt: '2023-04-28T20:00:25.372Z'
      numEdits: 0
      reactions: []
    id: 644c25d9cfebfbf8d8005bdd
    type: comment
  author: TheBloke
  content: "Thanks very much for the new model. I'm excited to try it out.\r\n\r\n\
    I just did the merge and noticed that special_tokens_map.json contains this:\r\
    \n```\r\n{\r\n  \"bos_token\": \"</s>\",\r\n  \"eos_token\": \"</s>\",\r\n  \"\
    pad_token\": \"[PAD]\",\r\n  \"unk_token\": \"</s>\"\r\n}\r\n```\r\nI've not seen\
    \ a model with that layout before. Is it correct that BOS, EOS and UNK are all\
    \ the same?\r\n\r\nAnd relatedly, I see in your README example that you do inference\
    \ with `skip_special_tokens=True`:\r\n```\r\nprint(tokenizer.decode(tokens[0],\
    \ skip_special_tokens=True))\r\n```\r\n\r\nIs that a requirement for this model?\
    \ And if so, is that why the `special_tokens_map.json` is like that? Or maybe\
    \ given `skip_special_tokens=True` is set, `special_tokens_map.json` isn't even\
    \ used?\r\n\r\nReason I ask is this will have implications for people doing inference\
    \ in eg text-generation-webui. It has an option to \"skip special tokens\" but\
    \ the user needs to know to set that, if it's required.\r\n\r\nAnd if they're\
    \ not going to set that, it does seem odd to me that BOS and EOS would be the\
    \ same.\r\n\r\nSo it'd be awesome to get some clarification on that.\r\n\r\nThanks\
    \ again!"
  created_at: 2023-04-28 19:00:25+00:00
  edited: false
  hidden: false
  id: 644c25d9cfebfbf8d8005bdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T20:00:38.000Z'
    data:
      from: Thanks for the new models! Couple of questions
      to: Thanks for the new models! Couple of questions regarding special_tokens_map
    id: 644c25e6ed08a4fdf4e0fc50
    type: title-change
  author: TheBloke
  created_at: 2023-04-28 19:00:38+00:00
  id: 644c25e6ed08a4fdf4e0fc50
  new_title: Thanks for the new models! Couple of questions regarding special_tokens_map
  old_title: Thanks for the new models! Couple of questions
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T23:03:33.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK so after posting this I realised that this is likely fine, and
          the reason for the unusual special_tokens_map is that this is trained on
          the following prompt template:</p>

          <pre><code>### Human: Write a story about llamas

          ### Assistant:

          </code></pre>

          <p>And therefore the BOS/EOS tokens don''t really come into it I suppose?</p>

          <p>It certainly seems to run fine with that prompt template anyway, so I
          guess the config is fine!</p>

          '
        raw: 'OK so after posting this I realised that this is likely fine, and the
          reason for the unusual special_tokens_map is that this is trained on the
          following prompt template:

          ```

          ### Human: Write a story about llamas

          ### Assistant:

          ```

          And therefore the BOS/EOS tokens don''t really come into it I suppose?


          It certainly seems to run fine with that prompt template anyway, so I guess
          the config is fine!'
        updatedAt: '2023-04-28T23:03:53.443Z'
      numEdits: 1
      reactions: []
      relatedEventId: 644c50c50ce4f8fb5174060b
    id: 644c50c50ce4f8fb5174060a
    type: comment
  author: TheBloke
  content: 'OK so after posting this I realised that this is likely fine, and the
    reason for the unusual special_tokens_map is that this is trained on the following
    prompt template:

    ```

    ### Human: Write a story about llamas

    ### Assistant:

    ```

    And therefore the BOS/EOS tokens don''t really come into it I suppose?


    It certainly seems to run fine with that prompt template anyway, so I guess the
    config is fine!'
  created_at: 2023-04-28 22:03:33+00:00
  edited: true
  hidden: false
  id: 644c50c50ce4f8fb5174060a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T23:03:33.000Z'
    data:
      status: closed
    id: 644c50c50ce4f8fb5174060b
    type: status-change
  author: TheBloke
  created_at: 2023-04-28 22:03:33+00:00
  id: 644c50c50ce4f8fb5174060b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: CarperAI/stable-vicuna-13b-delta
repo_type: model
status: closed
target_branch: null
title: Thanks for the new models! Couple of questions regarding special_tokens_map
