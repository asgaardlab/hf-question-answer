!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lukaemon
conflicting_files: null
created_at: 2022-11-21 10:34:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
      fullname: Lucas Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lukaemon
      type: user
    createdAt: '2022-11-21T10:34:54.000Z'
    data:
      edited: false
      editors:
      - lukaemon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
          fullname: Lucas Shen
          isHf: false
          isPro: false
          name: lukaemon
          type: user
        html: "<p>Doing basic stuff as in readme</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-comment\"># pip install accelerate</span>\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/galactica-30b\"</span>)\nmodel = OPTForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/galactica-30b\"</span>, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>)\n\ninput_text = <span class=\"\
          hljs-string\">\"The Transformer architecture [START_REF]\"</span>\ninput_ids\
          \ = tokenizer(input_text, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>).input_ids.to(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          \noutputs = model.generate(input_ids)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Then I got this error\
          \ msg:</p>\n<pre><code class=\"language-python\">File /opt/conda/lib/python3<span\
          \ class=\"hljs-number\">.8</span>/site-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">2326</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
          \ **kwargs)\n   <span class=\"hljs-number\">2323</span>     <span class=\"\
          hljs-keyword\">if</span> dtype_orig <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \   <span class=\"hljs-number\">2324</span>         torch.set_default_dtype(dtype_orig)\n\
          -&gt; <span class=\"hljs-number\">2326</span>     model, missing_keys, unexpected_keys,\
          \ mismatched_keys, error_msgs = cls._load_pretrained_model(\n   <span class=\"\
          hljs-number\">2327</span>         model,\n   <span class=\"hljs-number\"\
          >2328</span>         state_dict,\n   <span class=\"hljs-number\">2329</span>\
          \         loaded_state_dict_keys,  <span class=\"hljs-comment\"># <span\
          \ class=\"hljs-doctag\">XXX:</span> rename?</span>\n   <span class=\"hljs-number\"\
          >2330</span>         resolved_archive_file,\n   <span class=\"hljs-number\"\
          >2331</span>         pretrained_model_name_or_path,\n   <span class=\"hljs-number\"\
          >2332</span>         ignore_mismatched_sizes=ignore_mismatched_sizes,\n\
          \   <span class=\"hljs-number\">2333</span>         sharded_metadata=sharded_metadata,\n\
          \   <span class=\"hljs-number\">2334</span>         _fast_init=_fast_init,\n\
          \   <span class=\"hljs-number\">2335</span>         low_cpu_mem_usage=low_cpu_mem_usage,\n\
          \   <span class=\"hljs-number\">2336</span>         device_map=device_map,\n\
          \   <span class=\"hljs-number\">2337</span>         offload_folder=offload_folder,\n\
          \   <span class=\"hljs-number\">2338</span>         offload_state_dict=offload_state_dict,\n\
          \   <span class=\"hljs-number\">2339</span>         dtype=torch_dtype,\n\
          \   <span class=\"hljs-number\">2340</span>         load_in_8bit=load_in_8bit,\n\
          \   <span class=\"hljs-number\">2341</span>     )\n   <span class=\"hljs-number\"\
          >2343</span> <span class=\"hljs-comment\"># make sure token embedding weights\
          \ are still tied if needed</span>\n...\n-&gt; <span class=\"hljs-number\"\
          >2448</span> param = model_state_dict[key]\n   <span class=\"hljs-number\"\
          >2449</span> <span class=\"hljs-keyword\">if</span> param.device == torch.device(<span\
          \ class=\"hljs-string\">\"meta\"</span>):\n   <span class=\"hljs-number\"\
          >2450</span>     <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> load_in_8bit:\n\nKeyError: <span class=\"hljs-string\">'decoder.layers.37.self_attn.out_proj.bias'</span>\n\
          </code></pre>\n<p>Did I miss something obvious?</p>\n"
        raw: "Doing basic stuff as in readme\r\n```python\r\n# pip install accelerate\r\
          \nfrom transformers import AutoTokenizer, OPTForCausalLM\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\")\r\nmodel =\
          \ OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\", device_map=\"\
          auto\")\r\n\r\ninput_text = \"The Transformer architecture [START_REF]\"\
          \r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")\r\n\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
          \n```\r\n\r\nThen I got this error msg:\r\n```python\r\nFile /opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2326,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n   2323     if dtype_orig is not None:\r\n \
          \  2324         torch.set_default_dtype(dtype_orig)\r\n-> 2326     model,\
          \ missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(\r\
          \n   2327         model,\r\n   2328         state_dict,\r\n   2329     \
          \    loaded_state_dict_keys,  # XXX: rename?\r\n   2330         resolved_archive_file,\r\
          \n   2331         pretrained_model_name_or_path,\r\n   2332         ignore_mismatched_sizes=ignore_mismatched_sizes,\r\
          \n   2333         sharded_metadata=sharded_metadata,\r\n   2334        \
          \ _fast_init=_fast_init,\r\n   2335         low_cpu_mem_usage=low_cpu_mem_usage,\r\
          \n   2336         device_map=device_map,\r\n   2337         offload_folder=offload_folder,\r\
          \n   2338         offload_state_dict=offload_state_dict,\r\n   2339    \
          \     dtype=torch_dtype,\r\n   2340         load_in_8bit=load_in_8bit,\r\
          \n   2341     )\r\n   2343 # make sure token embedding weights are still\
          \ tied if needed\r\n...\r\n-> 2448 param = model_state_dict[key]\r\n   2449\
          \ if param.device == torch.device(\"meta\"):\r\n   2450     if not load_in_8bit:\r\
          \n\r\nKeyError: 'decoder.layers.37.self_attn.out_proj.bias'\r\n```\r\n\r\
          \nDid I miss something obvious?"
        updatedAt: '2022-11-21T10:34:54.905Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Jackmin108
        - Cryptheon
        - hwasiti
    id: 637b544eda59f5103ec3bfe1
    type: comment
  author: lukaemon
  content: "Doing basic stuff as in readme\r\n```python\r\n# pip install accelerate\r\
    \nfrom transformers import AutoTokenizer, OPTForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    facebook/galactica-30b\")\r\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\"\
    , device_map=\"auto\")\r\n\r\ninput_text = \"The Transformer architecture [START_REF]\"\
    \r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\"\
    )\r\n\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n```\r\n\r\nThen I got this error msg:\r\n```python\r\nFile /opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2326,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n   2323     if dtype_orig is not None:\r\n   2324         torch.set_default_dtype(dtype_orig)\r\
    \n-> 2326     model, missing_keys, unexpected_keys, mismatched_keys, error_msgs\
    \ = cls._load_pretrained_model(\r\n   2327         model,\r\n   2328         state_dict,\r\
    \n   2329         loaded_state_dict_keys,  # XXX: rename?\r\n   2330         resolved_archive_file,\r\
    \n   2331         pretrained_model_name_or_path,\r\n   2332         ignore_mismatched_sizes=ignore_mismatched_sizes,\r\
    \n   2333         sharded_metadata=sharded_metadata,\r\n   2334         _fast_init=_fast_init,\r\
    \n   2335         low_cpu_mem_usage=low_cpu_mem_usage,\r\n   2336         device_map=device_map,\r\
    \n   2337         offload_folder=offload_folder,\r\n   2338         offload_state_dict=offload_state_dict,\r\
    \n   2339         dtype=torch_dtype,\r\n   2340         load_in_8bit=load_in_8bit,\r\
    \n   2341     )\r\n   2343 # make sure token embedding weights are still tied\
    \ if needed\r\n...\r\n-> 2448 param = model_state_dict[key]\r\n   2449 if param.device\
    \ == torch.device(\"meta\"):\r\n   2450     if not load_in_8bit:\r\n\r\nKeyError:\
    \ 'decoder.layers.37.self_attn.out_proj.bias'\r\n```\r\n\r\nDid I miss something\
    \ obvious?"
  created_at: 2022-11-21 10:34:54+00:00
  edited: false
  hidden: false
  id: 637b544eda59f5103ec3bfe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-22T08:12:43.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>Same error but different key.</p>\n<pre><code class=\"language-console\"\
          >KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'\n</code></pre>\n\
          <p>I think this might be a bug in the way huggingface downloads blobs where\
          \ intermittent failures arent detected and can corrupt the blob.<br>It also\
          \ does not check hashsums for the blobs so it is unable to detect that the\
          \ blob was corrupted.<br>I lost my original output of which key it failed\
          \ on but I got suspicious of my 05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\
          \ shard because it is smaller than the others.<br>So I deleted it and had\
          \ it repulled and it's size changed in the new pull.</p>\n<p><strong>Old\
          \ disk usage</strong></p>\n<pre><code class=\"language-console\">\u276F\
          \ du -csh ./models--facebook--galactica-30b/blobs/*\n785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5.3G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          2.5G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          46G     total\n</code></pre>\n<p><strong>Latest disk usage</strong></p>\n\
          <pre><code class=\"language-console\">785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          9.2G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          56G     total\n</code></pre>\n<p>Im computing the md5sum for the blobs now\
          \ with </p>\n<pre><code class=\"language-console\">\u276F md5sum ./models--facebook--galactica-30b/blobs/*\n\
          </code></pre>\n<p>Let's compare?</p>\n"
        raw: "Same error but different key.\n\n```console\nKeyError: 'decoder.layers.44.self_attn_layer_norm.bias'\n\
          ```\n\nI think this might be a bug in the way huggingface downloads blobs\
          \ where intermittent failures arent detected and can corrupt the blob.\n\
          It also does not check hashsums for the blobs so it is unable to detect\
          \ that the blob was corrupted.\nI lost my original output of which key it\
          \ failed on but I got suspicious of my 05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\
          \ shard because it is smaller than the others.\nSo I deleted it and had\
          \ it repulled and it's size changed in the new pull.\n\n**Old disk usage**\n\
          ```console\n\u276F du -csh ./models--facebook--galactica-30b/blobs/*\n785M\
          \    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5.3G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          2.5G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          46G     total\n```\n\n**Latest disk usage**\n```console\n785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          9.2G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          56G     total\n```\n\nIm computing the md5sum for the blobs now with \n\
          ```console\n\u276F md5sum ./models--facebook--galactica-30b/blobs/*\n```\n\
          \nLet's compare?"
        updatedAt: '2022-11-22T09:56:54.391Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hwasiti
    id: 637c847bcaabb38c460530d8
    type: comment
  author: Jackmin108
  content: "Same error but different key.\n\n```console\nKeyError: 'decoder.layers.44.self_attn_layer_norm.bias'\n\
    ```\n\nI think this might be a bug in the way huggingface downloads blobs where\
    \ intermittent failures arent detected and can corrupt the blob.\nIt also does\
    \ not check hashsums for the blobs so it is unable to detect that the blob was\
    \ corrupted.\nI lost my original output of which key it failed on but I got suspicious\
    \ of my 05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf shard\
    \ because it is smaller than the others.\nSo I deleted it and had it repulled\
    \ and it's size changed in the new pull.\n\n**Old disk usage**\n```console\n\u276F\
    \ du -csh ./models--facebook--galactica-30b/blobs/*\n785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
    5.3G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
    4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
    2.5G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
    4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
    9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
    24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
    9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
    9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
    4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
    9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
    2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
    46G     total\n```\n\n**Latest disk usage**\n```console\n785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
    9.2G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
    4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
    9.2G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
    4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
    9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
    24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
    9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
    9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
    4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
    9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
    2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
    56G     total\n```\n\nIm computing the md5sum for the blobs now with \n```console\n\
    \u276F md5sum ./models--facebook--galactica-30b/blobs/*\n```\n\nLet's compare?"
  created_at: 2022-11-22 08:12:43+00:00
  edited: true
  hidden: false
  id: 637c847bcaabb38c460530d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-22T08:19:18.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>These are my md5sums which get <code>KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'</code>:</p>\n\
          <pre><code>\u276F md5sum ./models--facebook--galactica-30b/blobs/*\nee6deb059a899a51aa3e1c726e935aa2\
          \  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          </code></pre>\n"
        raw: "These are my md5sums which get `KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'`:\n\
          \n```\n\u276F md5sum ./models--facebook--galactica-30b/blobs/*\nee6deb059a899a51aa3e1c726e935aa2\
          \  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          ```"
        updatedAt: '2022-11-22T08:49:48.438Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - julien-c
        - hwasiti
    id: 637c8606d55081513c5679ef
    type: comment
  author: Jackmin108
  content: "These are my md5sums which get `KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'`:\n\
    \n```\n\u276F md5sum ./models--facebook--galactica-30b/blobs/*\nee6deb059a899a51aa3e1c726e935aa2\
    \  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
    5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
    8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
    3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
    f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
    4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
    2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
    1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
    ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
    fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
    4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
    a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
    ```"
  created_at: 2022-11-22 08:19:18+00:00
  edited: true
  hidden: false
  id: 637c8606d55081513c5679ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-22T08:38:35.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>Can the authors check as well? I think <strong>30b in particular\
          \ might be broken</strong> because the artifacts are much smaller than expected.</p>\n\
          <p>If you calculated bytes / parameter ratios, my install of 30B is an obvious\
          \ outlier:</p>\n<div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t\
          <thead><tr>\n<th align=\"center\">Size</th>\n<th align=\"center\">Parameters</th>\n\
          <th align=\"center\">Disk Usage</th>\n<th align=\"center\">Bytes / Parameter\
          \ ratio</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n<td align=\"center\"><code>mini</code></td>\n\
          <td align=\"center\">125 M</td>\n<td align=\"center\">480M</td>\n<td align=\"\
          center\">4.0265</td>\n</tr>\n<tr>\n<td align=\"center\"><code>base</code></td>\n\
          <td align=\"center\">1.3 B</td>\n<td align=\"center\">5.0G</td>\n<td align=\"\
          center\">4.1298</td>\n</tr>\n<tr>\n<td align=\"center\"><code>standard</code></td>\n\
          <td align=\"center\">6.7 B</td>\n<td align=\"center\">26G</td>\n<td align=\"\
          center\">4.1667</td>\n</tr>\n<tr>\n<td align=\"center\"><code>large</code></td>\n\
          <td align=\"center\">30 B</td>\n<td align=\"center\">56G</td>\n<td align=\"\
          center\">2.0043</td>\n</tr>\n<tr>\n<td align=\"center\"><code>huge</code></td>\n\
          <td align=\"center\">120 B</td>\n<td align=\"center\">453G</td>\n<td align=\"\
          center\">4.0534</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n<p>If we roughly\
          \ interpolate and say the models are ~ 4 bytes (fp32) per parameter, we\
          \ should expect the 30b model to have 120GB of blobs. However, if you sum\
          \ all the blobs in the repo. It is only ~60GB.</p>\n<h3 id=\"get-disk-usage\"\
          >Get disk usage</h3>\n<p>\u276F du -csh ~/.cache/huggingface/hub/*/blobs<br>453G\
          \    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-120b/blobs<br>480M\
          \    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-125m/blobs<br>5.0G\
          \    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-1.3b/blobs<br>56G\
          \     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-30b/blobs<br>26G\
          \     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs<br>539G\
          \    total</p>\n<h3 id=\"bytes--parameter-calculation\">Bytes / Parameter\
          \ Calculation</h3>\n<pre><code>[125m] 480 * (2 ** 20) / 125e6 = 4.0265_\n\
          [1.3b] 5 * (2 ** 30) / 1.3e9 = 4.1298_\n[6.7b] 26 * (2 ** 30) / 6.7e9 =\
          \ 4.1667_\n[30b] 56 * (2 ** 30) / 30e9 = 2.0043_\n[120b] 453 * (2 ** 30)\
          \ / 120e9 = 4.0534_\n</code></pre>\n<h3 id=\"artifacts-in-the-repo\">Artifacts\
          \ in the repo</h3>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1669106825470-634e2b60a00c472888747e4c.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1669106825470-634e2b60a00c472888747e4c.png\"\
          ></a></p>\n"
        raw: "Can the authors check as well? I think **30b in particular might be\
          \ broken** because the artifacts are much smaller than expected.\n\nIf you\
          \ calculated bytes / parameter ratios, my install of 30B is an obvious outlier:\n\
          |  Size       | Parameters  | Disk Usage | Bytes / Parameter ratio |\n|:-----------:|:-----------:|:-----------:|:-----------:|\n\
          | `mini`      |    125 M    |  480M | 4.0265 |\n| `base`      |    1.3 B\
          \    | 5.0G | 4.1298 |\n| `standard`  |    6.7 B    | 26G | 4.1667 |\n|\
          \ `large`     |     30 B    | 56G | 2.0043 |\n| `huge`      |    120 B \
          \   | 453G | 4.0534 |\n\nIf we roughly interpolate and say the models are\
          \ ~ 4 bytes (fp32) per parameter, we should expect the 30b model to have\
          \ 120GB of blobs. However, if you sum all the blobs in the repo. It is only\
          \ ~60GB.\n\n### Get disk usage\n\u276F du -csh ~/.cache/huggingface/hub/*/blobs\n\
          453G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-120b/blobs\n\
          480M    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-125m/blobs\n\
          5.0G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-1.3b/blobs\n\
          56G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-30b/blobs\n\
          26G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs\n\
          539G    total\n\n### Bytes / Parameter Calculation\n```\n[125m] 480 * (2\
          \ ** 20) / 125e6 = 4.0265_\n[1.3b] 5 * (2 ** 30) / 1.3e9 = 4.1298_\n[6.7b]\
          \ 26 * (2 ** 30) / 6.7e9 = 4.1667_\n[30b] 56 * (2 ** 30) / 30e9 = 2.0043_\n\
          [120b] 453 * (2 ** 30) / 120e9 = 4.0534_\n```\n\n### Artifacts in the repo\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1669106825470-634e2b60a00c472888747e4c.png)"
        updatedAt: '2022-11-22T10:16:51.219Z'
      numEdits: 8
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hwasiti
    id: 637c8a8bf219c71f93ed5470
    type: comment
  author: Jackmin108
  content: "Can the authors check as well? I think **30b in particular might be broken**\
    \ because the artifacts are much smaller than expected.\n\nIf you calculated bytes\
    \ / parameter ratios, my install of 30B is an obvious outlier:\n|  Size      \
    \ | Parameters  | Disk Usage | Bytes / Parameter ratio |\n|:-----------:|:-----------:|:-----------:|:-----------:|\n\
    | `mini`      |    125 M    |  480M | 4.0265 |\n| `base`      |    1.3 B    |\
    \ 5.0G | 4.1298 |\n| `standard`  |    6.7 B    | 26G | 4.1667 |\n| `large`   \
    \  |     30 B    | 56G | 2.0043 |\n| `huge`      |    120 B    | 453G | 4.0534\
    \ |\n\nIf we roughly interpolate and say the models are ~ 4 bytes (fp32) per parameter,\
    \ we should expect the 30b model to have 120GB of blobs. However, if you sum all\
    \ the blobs in the repo. It is only ~60GB.\n\n### Get disk usage\n\u276F du -csh\
    \ ~/.cache/huggingface/hub/*/blobs\n453G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-120b/blobs\n\
    480M    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-125m/blobs\n\
    5.0G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-1.3b/blobs\n\
    56G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-30b/blobs\n\
    26G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs\n\
    539G    total\n\n### Bytes / Parameter Calculation\n```\n[125m] 480 * (2 ** 20)\
    \ / 125e6 = 4.0265_\n[1.3b] 5 * (2 ** 30) / 1.3e9 = 4.1298_\n[6.7b] 26 * (2 **\
    \ 30) / 6.7e9 = 4.1667_\n[30b] 56 * (2 ** 30) / 30e9 = 2.0043_\n[120b] 453 * (2\
    \ ** 30) / 120e9 = 4.0534_\n```\n\n### Artifacts in the repo\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1669106825470-634e2b60a00c472888747e4c.png)"
  created_at: 2022-11-22 08:38:35+00:00
  edited: true
  hidden: false
  id: 637c8a8bf219c71f93ed5470
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-22T09:56:01.000Z'
    data:
      edited: true
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>I have deduced that the 30b model pickles <em>have no biases</em>.</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ tqdm <span class=\"hljs-keyword\">import</span> tqdm\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> pathlib <span\
          \ class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\"\
          >import</span> pickle\n\nblob_path = Path.home() / Path(<span class=\"hljs-string\"\
          >'.cache/huggingface/hub/models--facebook--galactica-30b/blobs'</span>)\n\
          \nkeys2blob = {}\nerrors = {}\nblobs = [blob <span class=\"hljs-keyword\"\
          >for</span> blob <span class=\"hljs-keyword\">in</span> blob_path.glob(<span\
          \ class=\"hljs-string\">'./*'</span>) <span class=\"hljs-keyword\">if</span>\
          \ blob.is_file()]\n\n<span class=\"hljs-keyword\">for</span> blob <span\
          \ class=\"hljs-keyword\">in</span> tqdm(blobs):\n    <span class=\"hljs-keyword\"\
          >try</span>:\n        keys2blob.update({k: blob <span class=\"hljs-keyword\"\
          >for</span> k <span class=\"hljs-keyword\">in</span> torch.load(blob).keys()})\n\
          \    <span class=\"hljs-keyword\">except</span> pickle.UnpicklingError <span\
          \ class=\"hljs-keyword\">as</span> e:\n        errors[blob] = e\n\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Num_weights:\
          \ <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>([i\
          \ <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> keys2blob.keys() <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-string\">'weight'</span> <span class=\"hljs-keyword\">in</span>\
          \ i])}</span>\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Num_biases: <span class=\"hljs-subst\">{<span\
          \ class=\"hljs-built_in\">len</span>([i <span class=\"hljs-keyword\">for</span>\
          \ i <span class=\"hljs-keyword\">in</span> keys2blob.keys() <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-string\">'bias'</span> <span\
          \ class=\"hljs-keyword\">in</span> i])}</span>\"</span>)\n</code></pre>\n\
          <pre><code class=\"language-console\"><span class=\"hljs-meta prompt_\"\
          >100%</span><span class=\"language-bash\">|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 12/12 [00:50&lt;00:00,  4.19s/it]</span>\n\
          Num_weights: 290\nNum_biases: 0\n</code></pre>\n<p>This is opposed to the\
          \ 6.7b model which <em>contains a lot of biases</em>.</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> tqdm <span class=\"\
          hljs-keyword\">import</span> tqdm\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"\
          hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">import</span>\
          \ pickle\n\nblob_path = Path.home() / Path(<span class=\"hljs-string\">'.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs'</span>)\n\
          \nkeys2blob = {}\nerrors = {}\nblobs = [blob <span class=\"hljs-keyword\"\
          >for</span> blob <span class=\"hljs-keyword\">in</span> blob_path.glob(<span\
          \ class=\"hljs-string\">'./*'</span>) <span class=\"hljs-keyword\">if</span>\
          \ blob.is_file()]\n\n<span class=\"hljs-keyword\">for</span> blob <span\
          \ class=\"hljs-keyword\">in</span> tqdm(blobs):\n    <span class=\"hljs-keyword\"\
          >try</span>:\n        keys2blob.update({k: blob <span class=\"hljs-keyword\"\
          >for</span> k <span class=\"hljs-keyword\">in</span> torch.load(blob).keys()})\n\
          \    <span class=\"hljs-keyword\">except</span> pickle.UnpicklingError <span\
          \ class=\"hljs-keyword\">as</span> e:\n        errors[blob] = e\n\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Num_weights:\
          \ <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>([i\
          \ <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> keys2blob.keys() <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-string\">'weight'</span> <span class=\"hljs-keyword\">in</span>\
          \ i])}</span>\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Num_biases: <span class=\"hljs-subst\">{<span\
          \ class=\"hljs-built_in\">len</span>([i <span class=\"hljs-keyword\">for</span>\
          \ i <span class=\"hljs-keyword\">in</span> keys2blob.keys() <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-string\">'bias'</span> <span\
          \ class=\"hljs-keyword\">in</span> i])}</span>\"</span>)\n</code></pre>\n\
          <pre><code class=\"language-console\"><span class=\"hljs-meta prompt_\"\
          >50%</span><span class=\"language-bash\">|\u2588\u2588\u2588\u2588\u2588\
          \     | 4/8 [00:14&lt;00:14,  3.57s/it]</span>\nNum_weights: 260\nNum_biases:\
          \ 257\n</code></pre>\n"
        raw: "I have deduced that the 30b model pickles _have no biases_.\n\n```python\n\
          from tqdm import tqdm\nimport torch\nfrom pathlib import Path\nimport pickle\n\
          \nblob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-30b/blobs')\n\
          \nkeys2blob = {}\nerrors = {}\nblobs = [blob for blob in blob_path.glob('./*')\
          \ if blob.is_file()]\n\nfor blob in tqdm(blobs):\n    try:\n        keys2blob.update({k:\
          \ blob for k in torch.load(blob).keys()})\n    except pickle.UnpicklingError\
          \ as e:\n        errors[blob] = e\n\nprint(f\"Num_weights: {len([i for i\
          \ in keys2blob.keys() if 'weight' in i])}\")\nprint(f\"Num_biases: {len([i\
          \ for i in keys2blob.keys() if 'bias' in i])}\")\n```\n```console\n100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:50<00:00,\
          \  4.19s/it]\nNum_weights: 290\nNum_biases: 0\n```\n\nThis is opposed to\
          \ the 6.7b model which _contains a lot of biases_.\n```python\nfrom tqdm\
          \ import tqdm\nimport torch\nfrom pathlib import Path\nimport pickle\n\n\
          blob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs')\n\
          \nkeys2blob = {}\nerrors = {}\nblobs = [blob for blob in blob_path.glob('./*')\
          \ if blob.is_file()]\n\nfor blob in tqdm(blobs):\n    try:\n        keys2blob.update({k:\
          \ blob for k in torch.load(blob).keys()})\n    except pickle.UnpicklingError\
          \ as e:\n        errors[blob] = e\n\nprint(f\"Num_weights: {len([i for i\
          \ in keys2blob.keys() if 'weight' in i])}\")\nprint(f\"Num_biases: {len([i\
          \ for i in keys2blob.keys() if 'bias' in i])}\")\n```\n```console\n50%|\u2588\
          \u2588\u2588\u2588\u2588     | 4/8 [00:14<00:14,  3.57s/it]\nNum_weights:\
          \ 260\nNum_biases: 257\n```"
        updatedAt: '2022-11-22T10:06:11.452Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - lukaemon
        - julien-c
        - hwasiti
    id: 637c9cb1767be4aa306dbe3f
    type: comment
  author: Jackmin108
  content: "I have deduced that the 30b model pickles _have no biases_.\n\n```python\n\
    from tqdm import tqdm\nimport torch\nfrom pathlib import Path\nimport pickle\n\
    \nblob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-30b/blobs')\n\
    \nkeys2blob = {}\nerrors = {}\nblobs = [blob for blob in blob_path.glob('./*')\
    \ if blob.is_file()]\n\nfor blob in tqdm(blobs):\n    try:\n        keys2blob.update({k:\
    \ blob for k in torch.load(blob).keys()})\n    except pickle.UnpicklingError as\
    \ e:\n        errors[blob] = e\n\nprint(f\"Num_weights: {len([i for i in keys2blob.keys()\
    \ if 'weight' in i])}\")\nprint(f\"Num_biases: {len([i for i in keys2blob.keys()\
    \ if 'bias' in i])}\")\n```\n```console\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 12/12 [00:50<00:00,  4.19s/it]\nNum_weights: 290\nNum_biases:\
    \ 0\n```\n\nThis is opposed to the 6.7b model which _contains a lot of biases_.\n\
    ```python\nfrom tqdm import tqdm\nimport torch\nfrom pathlib import Path\nimport\
    \ pickle\n\nblob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs')\n\
    \nkeys2blob = {}\nerrors = {}\nblobs = [blob for blob in blob_path.glob('./*')\
    \ if blob.is_file()]\n\nfor blob in tqdm(blobs):\n    try:\n        keys2blob.update({k:\
    \ blob for k in torch.load(blob).keys()})\n    except pickle.UnpicklingError as\
    \ e:\n        errors[blob] = e\n\nprint(f\"Num_weights: {len([i for i in keys2blob.keys()\
    \ if 'weight' in i])}\")\nprint(f\"Num_biases: {len([i for i in keys2blob.keys()\
    \ if 'bias' in i])}\")\n```\n```console\n50%|\u2588\u2588\u2588\u2588\u2588  \
    \   | 4/8 [00:14<00:14,  3.57s/it]\nNum_weights: 260\nNum_biases: 257\n```"
  created_at: 2022-11-22 09:56:01+00:00
  edited: true
  hidden: false
  id: 637c9cb1767be4aa306dbe3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-22T12:16:43.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p>Update: 30b is the only model in half precision. It also has less\
          \ tensors than expected.</p>\n<div class=\"max-w-full overflow-auto\">\n\
          \t<table>\n\t\t<thead><tr>\n<th align=\"center\">Size</th>\n<th align=\"\
          center\">Parameters</th>\n<th align=\"center\">Disk Usage</th>\n<th align=\"\
          center\">Bytes / Parameter ratio</th>\n<th align=\"center\">Sum(layer.numels)</th>\n\
          <th align=\"center\">Data type of tensors</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n\
          <td align=\"center\"><code>mini</code></td>\n<td align=\"center\">125 M</td>\n\
          <td align=\"center\">480M</td>\n<td align=\"center\">4.0265</td>\n<td align=\"\
          center\">163,430,400</td>\n<td align=\"center\">{torch.float32: 197}</td>\n\
          </tr>\n<tr>\n<td align=\"center\"><code>base</code></td>\n<td align=\"center\"\
          >1.3 B</td>\n<td align=\"center\">5.0G</td>\n<td align=\"center\">4.1298</td>\n\
          <td align=\"center\">1,417,601,024</td>\n<td align=\"center\">{torch.float32:\
          \ 389}</td>\n</tr>\n<tr>\n<td align=\"center\"><code>standard</code></td>\n\
          <td align=\"center\">6.7 B</td>\n<td align=\"center\">26G</td>\n<td align=\"\
          center\">4.1667</td>\n<td align=\"center\">6,862,159,872</td>\n<td align=\"\
          center\">{torch.float32: 517}</td>\n</tr>\n<tr>\n<td align=\"center\"><code>large</code></td>\n\
          <td align=\"center\">30 B</td>\n<td align=\"center\">56G</td>\n<td align=\"\
          center\">2.0043</td>\n<td align=\"center\">29,968,103,424</td>\n<td align=\"\
          center\">{torch.float16: 290}</td>\n</tr>\n<tr>\n<td align=\"center\"><code>huge</code></td>\n\
          <td align=\"center\">120 B</td>\n<td align=\"center\">453G</td>\n<td align=\"\
          center\">4.0534</td>\n<td align=\"center\">121,853,747,200</td>\n<td align=\"\
          center\">{torch.float32: 1541}</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: 'Update: 30b is the only model in half precision. It also has less tensors
          than expected.


          |  Size       | Parameters  | Disk Usage | Bytes / Parameter ratio | Sum(layer.numels)
          | Data type of tensors

          |:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|

          | `mini`      |    125 M    |  480M | 4.0265 | 163,430,400 | {torch.float32:
          197} |

          | `base`      |    1.3 B    | 5.0G | 4.1298 | 1,417,601,024 | {torch.float32:
          389} |

          | `standard`  |    6.7 B    | 26G | 4.1667 | 6,862,159,872 | {torch.float32:
          517} |

          | `large`     |     30 B    | 56G | 2.0043 | 29,968,103,424 | {torch.float16:
          290} |

          | `huge`      |    120 B    | 453G | 4.0534 | 121,853,747,200 | {torch.float32:
          1541} |'
        updatedAt: '2022-11-22T12:16:43.566Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hwasiti
    id: 637cbdabbb031d2afee2a89f
    type: comment
  author: Jackmin108
  content: 'Update: 30b is the only model in half precision. It also has less tensors
    than expected.


    |  Size       | Parameters  | Disk Usage | Bytes / Parameter ratio | Sum(layer.numels)
    | Data type of tensors

    |:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|

    | `mini`      |    125 M    |  480M | 4.0265 | 163,430,400 | {torch.float32: 197}
    |

    | `base`      |    1.3 B    | 5.0G | 4.1298 | 1,417,601,024 | {torch.float32:
    389} |

    | `standard`  |    6.7 B    | 26G | 4.1667 | 6,862,159,872 | {torch.float32: 517}
    |

    | `large`     |     30 B    | 56G | 2.0043 | 29,968,103,424 | {torch.float16:
    290} |

    | `huge`      |    120 B    | 453G | 4.0534 | 121,853,747,200 | {torch.float32:
    1541} |'
  created_at: 2022-11-22 12:16:43+00:00
  edited: false
  hidden: false
  id: 637cbdabbb031d2afee2a89f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-23T10:17:06.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>Getting key errors too like:<br><code>KeyError: 'decoder.layers.27.final_layer_norm.weight'</code><br><code>KeyError:\
          \ 'decoder.layers.11.fc1.bias'</code><br><code>KeyError: 'decoder.layers.6.fc1.bias'</code><br>which\
          \ are different with each run</p>\n<p>Here are the md5sum</p>\n<p><code>md5sum\
          \ ./models--facebook--galactica-30b/blobs/*</code></p>\n<pre><code>ee6deb059a899a51aa3e1c726e935aa2\
          \  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          </code></pre>\n<p>which is the same hash of <span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\"\
          >@<span class=\"underline\">Jackmin108</span></a></span>\n\n\t</span></span>\
          \  :<br> <a href=\"https://huggingface.co/facebook/galactica-30b/discussions/4#637c8606d55081513c5679ef\"\
          >https://huggingface.co/facebook/galactica-30b/discussions/4#637c8606d55081513c5679ef</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;mrm8488&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mrm8488\">@<span class=\"\
          underline\">mrm8488</span></a></span>\n\n\t</span></span>  For me too other\
          \ models worked fine. Only the galactica 30b model gives me key errors.</p>\n"
        raw: "Getting key errors too like:\n`KeyError: 'decoder.layers.27.final_layer_norm.weight'`\n\
          `KeyError: 'decoder.layers.11.fc1.bias'`\n`KeyError: 'decoder.layers.6.fc1.bias'`\n\
          which are different with each run\n\n\nHere are the md5sum\n\n`md5sum ./models--facebook--galactica-30b/blobs/*`\n\
          \n```\nee6deb059a899a51aa3e1c726e935aa2  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
          5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
          8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
          3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
          f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
          4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
          2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
          1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
          ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
          fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
          4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
          a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
          ```\n\nwhich is the same hash of @Jackmin108  :\n https://huggingface.co/facebook/galactica-30b/discussions/4#637c8606d55081513c5679ef\n\
          \n@mrm8488  For me too other models worked fine. Only the galactica 30b\
          \ model gives me key errors."
        updatedAt: '2022-11-23T10:17:06.748Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Jackmin108
    id: 637df32268653436e5332aeb
    type: comment
  author: hwasiti
  content: "Getting key errors too like:\n`KeyError: 'decoder.layers.27.final_layer_norm.weight'`\n\
    `KeyError: 'decoder.layers.11.fc1.bias'`\n`KeyError: 'decoder.layers.6.fc1.bias'`\n\
    which are different with each run\n\n\nHere are the md5sum\n\n`md5sum ./models--facebook--galactica-30b/blobs/*`\n\
    \n```\nee6deb059a899a51aa3e1c726e935aa2  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6\n\
    5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf\n\
    8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93\n\
    3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8\n\
    f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6\n\
    4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71\n\
    2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317\n\
    1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7\n\
    ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f\n\
    fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb\n\
    4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93\n\
    a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0\n\
    ```\n\nwhich is the same hash of @Jackmin108  :\n https://huggingface.co/facebook/galactica-30b/discussions/4#637c8606d55081513c5679ef\n\
    \n@mrm8488  For me too other models worked fine. Only the galactica 30b model\
    \ gives me key errors."
  created_at: 2022-11-23 10:17:06+00:00
  edited: false
  hidden: false
  id: 637df32268653436e5332aeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2022-11-23T10:23:54.000Z'
    data:
      edited: false
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ made a PR yesterday to fix it ASAP :)</p>\n"
        raw: Hi, @ybelkada made a PR yesterday to fix it ASAP :)
        updatedAt: '2022-11-23T10:23:54.319Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 637df4baa5d885bf5ffe0039
    type: comment
  author: mrm8488
  content: Hi, @ybelkada made a PR yesterday to fix it ASAP :)
  created_at: 2022-11-23 10:23:54+00:00
  edited: false
  hidden: false
  id: 637df4baa5d885bf5ffe0039
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-11-23T18:14:48.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;hwasiti&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hwasiti\"\
          >@<span class=\"underline\">hwasiti</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\">@<span class=\"\
          underline\">Jackmin108</span></a></span>\n\n\t</span></span><br>Just out\
          \ of curiosity, have you tried it with the largest model too? The 120b</p>\n"
        raw: "Hey @hwasiti @Jackmin108 \nJust out of curiosity, have you tried it\
          \ with the largest model too? The 120b"
        updatedAt: '2022-11-23T18:14:48.844Z'
      numEdits: 0
      reactions: []
    id: 637e6318ed78be2e905838cf
    type: comment
  author: ybelkada
  content: "Hey @hwasiti @Jackmin108 \nJust out of curiosity, have you tried it with\
    \ the largest model too? The 120b"
  created_at: 2022-11-23 18:14:48+00:00
  edited: false
  hidden: false
  id: 637e6318ed78be2e905838cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2022-11-23T18:41:58.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> Unfortunately\
          \ I don't have the hardware to load 120b even if I sampled down to int8.<br>However,\
          \ there doesn't seem to be anything suspicious with the 120b checkpoint.</p>\n\
          <p>The reason I believe so is because you sort the names of the layers and\
          \ cutoff at the last occurrence of layer 0 in the decoder, you get the same\
          \ output from the 120b checkpoint as the 125m checkpoint, the tensors just\
          \ have different dimensions.<br>This is not true for the 30b checkpoint,\
          \ which is missing final layers and biases.</p>\n<h3 id=\"120b\">120b</h3>\n\
          <pre><code class=\"language-python\">lm_head.weight torch.Size([<span class=\"\
          hljs-number\">50000</span>, <span class=\"hljs-number\">10240</span>])\n\
          model.decoder.embed_positions.weight torch.Size([<span class=\"hljs-number\"\
          >2050</span>, <span class=\"hljs-number\">10240</span>])\nmodel.decoder.embed_tokens.weight\
          \ torch.Size([<span class=\"hljs-number\">50000</span>, <span class=\"hljs-number\"\
          >10240</span>])\nmodel.decoder.final_layer_norm.bias torch.Size([<span class=\"\
          hljs-number\">10240</span>])\nmodel.decoder.final_layer_norm.weight torch.Size([<span\
          \ class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span class=\"\
          hljs-number\">.0</span>.fc1.bias torch.Size([<span class=\"hljs-number\"\
          >40960</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.fc1.weight\
          \ torch.Size([<span class=\"hljs-number\">40960</span>, <span class=\"hljs-number\"\
          >10240</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.fc2.bias\
          \ torch.Size([<span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.fc2.weight torch.Size([<span class=\"\
          hljs-number\">10240</span>, <span class=\"hljs-number\">40960</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.final_layer_norm.bias\
          \ torch.Size([<span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.final_layer_norm.weight torch.Size([<span\
          \ class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span class=\"\
          hljs-number\">.0</span>.self_attn.k_proj.bias torch.Size([<span class=\"\
          hljs-number\">10240</span>])\nmodel.decoder.layers<span class=\"hljs-number\"\
          >.0</span>.self_attn.k_proj.weight torch.Size([<span class=\"hljs-number\"\
          >10240</span>, <span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.out_proj.bias torch.Size([<span\
          \ class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span class=\"\
          hljs-number\">.0</span>.self_attn.out_proj.weight torch.Size([<span class=\"\
          hljs-number\">10240</span>, <span class=\"hljs-number\">10240</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.q_proj.bias\
          \ torch.Size([<span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.q_proj.weight torch.Size([<span\
          \ class=\"hljs-number\">10240</span>, <span class=\"hljs-number\">10240</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.v_proj.bias\
          \ torch.Size([<span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.v_proj.weight torch.Size([<span\
          \ class=\"hljs-number\">10240</span>, <span class=\"hljs-number\">10240</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn_layer_norm.bias\
          \ torch.Size([<span class=\"hljs-number\">10240</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn_layer_norm.weight torch.Size([<span\
          \ class=\"hljs-number\">10240</span>])\n</code></pre>\n<h3 id=\"125m\">125m</h3>\n\
          <pre><code class=\"language-python\">lm_head.weight torch.Size([<span class=\"\
          hljs-number\">50000</span>, <span class=\"hljs-number\">768</span>])\nmodel.decoder.embed_positions.weight\
          \ torch.Size([<span class=\"hljs-number\">2050</span>, <span class=\"hljs-number\"\
          >768</span>])\nmodel.decoder.embed_tokens.weight torch.Size([<span class=\"\
          hljs-number\">50000</span>, <span class=\"hljs-number\">768</span>])\nmodel.decoder.final_layer_norm.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.final_layer_norm.weight\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.fc1.bias torch.Size([<span class=\"hljs-number\"\
          >3072</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.fc1.weight\
          \ torch.Size([<span class=\"hljs-number\">3072</span>, <span class=\"hljs-number\"\
          >768</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.fc2.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.fc2.weight torch.Size([<span class=\"\
          hljs-number\">768</span>, <span class=\"hljs-number\">3072</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.final_layer_norm.bias torch.Size([<span\
          \ class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span class=\"\
          hljs-number\">.0</span>.final_layer_norm.weight torch.Size([<span class=\"\
          hljs-number\">768</span>])\nmodel.decoder.layers<span class=\"hljs-number\"\
          >.0</span>.self_attn.k_proj.bias torch.Size([<span class=\"hljs-number\"\
          >768</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.k_proj.weight\
          \ torch.Size([<span class=\"hljs-number\">768</span>, <span class=\"hljs-number\"\
          >768</span>])\nmodel.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.out_proj.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.out_proj.weight torch.Size([<span\
          \ class=\"hljs-number\">768</span>, <span class=\"hljs-number\">768</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.q_proj.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.q_proj.weight torch.Size([<span\
          \ class=\"hljs-number\">768</span>, <span class=\"hljs-number\">768</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.v_proj.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn.v_proj.weight torch.Size([<span\
          \ class=\"hljs-number\">768</span>, <span class=\"hljs-number\">768</span>])\n\
          model.decoder.layers<span class=\"hljs-number\">.0</span>.self_attn_layer_norm.bias\
          \ torch.Size([<span class=\"hljs-number\">768</span>])\nmodel.decoder.layers<span\
          \ class=\"hljs-number\">.0</span>.self_attn_layer_norm.weight torch.Size([<span\
          \ class=\"hljs-number\">768</span>])\n</code></pre>\n<h3 id=\"30b\">30b</h3>\n\
          <pre><code class=\"language-python\">decoder.embed_positions.weight torch.Size([<span\
          \ class=\"hljs-number\">2050</span>, <span class=\"hljs-number\">7168</span>])\n\
          decoder.embed_tokens.weight torch.Size([<span class=\"hljs-number\">50000</span>,\
          \ <span class=\"hljs-number\">7168</span>])\ndecoder.layers<span class=\"\
          hljs-number\">.0</span>.fc1.weight torch.Size([<span class=\"hljs-number\"\
          >28672</span>, <span class=\"hljs-number\">7168</span>])\ndecoder.layers<span\
          \ class=\"hljs-number\">.0</span>.fc2.weight torch.Size([<span class=\"\
          hljs-number\">7168</span>, <span class=\"hljs-number\">28672</span>])\n\
          decoder.layers<span class=\"hljs-number\">.0</span>.self_attn.k_proj.weight\
          \ torch.Size([<span class=\"hljs-number\">7168</span>, <span class=\"hljs-number\"\
          >7168</span>])\ndecoder.layers<span class=\"hljs-number\">.0</span>.self_attn.out_proj.weight\
          \ torch.Size([<span class=\"hljs-number\">7168</span>, <span class=\"hljs-number\"\
          >7168</span>])\ndecoder.layers<span class=\"hljs-number\">.0</span>.self_attn.q_proj.weight\
          \ torch.Size([<span class=\"hljs-number\">7168</span>, <span class=\"hljs-number\"\
          >7168</span>])\ndecoder.layers<span class=\"hljs-number\">.0</span>.self_attn.v_proj.weight\
          \ torch.Size([<span class=\"hljs-number\">7168</span>, <span class=\"hljs-number\"\
          >7168</span>])\n</code></pre>\n"
        raw: '@ybelkada Unfortunately I don''t have the hardware to load 120b even
          if I sampled down to int8.

          However, there doesn''t seem to be anything suspicious with the 120b checkpoint.


          The reason I believe so is because you sort the names of the layers and
          cutoff at the last occurrence of layer 0 in the decoder, you get the same
          output from the 120b checkpoint as the 125m checkpoint, the tensors just
          have different dimensions.

          This is not true for the 30b checkpoint, which is missing final layers and
          biases.


          ### 120b

          ```python

          lm_head.weight torch.Size([50000, 10240])

          model.decoder.embed_positions.weight torch.Size([2050, 10240])

          model.decoder.embed_tokens.weight torch.Size([50000, 10240])

          model.decoder.final_layer_norm.bias torch.Size([10240])

          model.decoder.final_layer_norm.weight torch.Size([10240])

          model.decoder.layers.0.fc1.bias torch.Size([40960])

          model.decoder.layers.0.fc1.weight torch.Size([40960, 10240])

          model.decoder.layers.0.fc2.bias torch.Size([10240])

          model.decoder.layers.0.fc2.weight torch.Size([10240, 40960])

          model.decoder.layers.0.final_layer_norm.bias torch.Size([10240])

          model.decoder.layers.0.final_layer_norm.weight torch.Size([10240])

          model.decoder.layers.0.self_attn.k_proj.bias torch.Size([10240])

          model.decoder.layers.0.self_attn.k_proj.weight torch.Size([10240, 10240])

          model.decoder.layers.0.self_attn.out_proj.bias torch.Size([10240])

          model.decoder.layers.0.self_attn.out_proj.weight torch.Size([10240, 10240])

          model.decoder.layers.0.self_attn.q_proj.bias torch.Size([10240])

          model.decoder.layers.0.self_attn.q_proj.weight torch.Size([10240, 10240])

          model.decoder.layers.0.self_attn.v_proj.bias torch.Size([10240])

          model.decoder.layers.0.self_attn.v_proj.weight torch.Size([10240, 10240])

          model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([10240])

          model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([10240])

          ```


          ### 125m

          ```python

          lm_head.weight torch.Size([50000, 768])

          model.decoder.embed_positions.weight torch.Size([2050, 768])

          model.decoder.embed_tokens.weight torch.Size([50000, 768])

          model.decoder.final_layer_norm.bias torch.Size([768])

          model.decoder.final_layer_norm.weight torch.Size([768])

          model.decoder.layers.0.fc1.bias torch.Size([3072])

          model.decoder.layers.0.fc1.weight torch.Size([3072, 768])

          model.decoder.layers.0.fc2.bias torch.Size([768])

          model.decoder.layers.0.fc2.weight torch.Size([768, 3072])

          model.decoder.layers.0.final_layer_norm.bias torch.Size([768])

          model.decoder.layers.0.final_layer_norm.weight torch.Size([768])

          model.decoder.layers.0.self_attn.k_proj.bias torch.Size([768])

          model.decoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])

          model.decoder.layers.0.self_attn.out_proj.bias torch.Size([768])

          model.decoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])

          model.decoder.layers.0.self_attn.q_proj.bias torch.Size([768])

          model.decoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])

          model.decoder.layers.0.self_attn.v_proj.bias torch.Size([768])

          model.decoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])

          model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([768])

          model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([768])

          ```


          ### 30b

          ```python

          decoder.embed_positions.weight torch.Size([2050, 7168])

          decoder.embed_tokens.weight torch.Size([50000, 7168])

          decoder.layers.0.fc1.weight torch.Size([28672, 7168])

          decoder.layers.0.fc2.weight torch.Size([7168, 28672])

          decoder.layers.0.self_attn.k_proj.weight torch.Size([7168, 7168])

          decoder.layers.0.self_attn.out_proj.weight torch.Size([7168, 7168])

          decoder.layers.0.self_attn.q_proj.weight torch.Size([7168, 7168])

          decoder.layers.0.self_attn.v_proj.weight torch.Size([7168, 7168])

          ```'
        updatedAt: '2022-11-23T18:41:58.869Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 637e6976460de456bee8139e
    type: comment
  author: Jackmin108
  content: '@ybelkada Unfortunately I don''t have the hardware to load 120b even if
    I sampled down to int8.

    However, there doesn''t seem to be anything suspicious with the 120b checkpoint.


    The reason I believe so is because you sort the names of the layers and cutoff
    at the last occurrence of layer 0 in the decoder, you get the same output from
    the 120b checkpoint as the 125m checkpoint, the tensors just have different dimensions.

    This is not true for the 30b checkpoint, which is missing final layers and biases.


    ### 120b

    ```python

    lm_head.weight torch.Size([50000, 10240])

    model.decoder.embed_positions.weight torch.Size([2050, 10240])

    model.decoder.embed_tokens.weight torch.Size([50000, 10240])

    model.decoder.final_layer_norm.bias torch.Size([10240])

    model.decoder.final_layer_norm.weight torch.Size([10240])

    model.decoder.layers.0.fc1.bias torch.Size([40960])

    model.decoder.layers.0.fc1.weight torch.Size([40960, 10240])

    model.decoder.layers.0.fc2.bias torch.Size([10240])

    model.decoder.layers.0.fc2.weight torch.Size([10240, 40960])

    model.decoder.layers.0.final_layer_norm.bias torch.Size([10240])

    model.decoder.layers.0.final_layer_norm.weight torch.Size([10240])

    model.decoder.layers.0.self_attn.k_proj.bias torch.Size([10240])

    model.decoder.layers.0.self_attn.k_proj.weight torch.Size([10240, 10240])

    model.decoder.layers.0.self_attn.out_proj.bias torch.Size([10240])

    model.decoder.layers.0.self_attn.out_proj.weight torch.Size([10240, 10240])

    model.decoder.layers.0.self_attn.q_proj.bias torch.Size([10240])

    model.decoder.layers.0.self_attn.q_proj.weight torch.Size([10240, 10240])

    model.decoder.layers.0.self_attn.v_proj.bias torch.Size([10240])

    model.decoder.layers.0.self_attn.v_proj.weight torch.Size([10240, 10240])

    model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([10240])

    model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([10240])

    ```


    ### 125m

    ```python

    lm_head.weight torch.Size([50000, 768])

    model.decoder.embed_positions.weight torch.Size([2050, 768])

    model.decoder.embed_tokens.weight torch.Size([50000, 768])

    model.decoder.final_layer_norm.bias torch.Size([768])

    model.decoder.final_layer_norm.weight torch.Size([768])

    model.decoder.layers.0.fc1.bias torch.Size([3072])

    model.decoder.layers.0.fc1.weight torch.Size([3072, 768])

    model.decoder.layers.0.fc2.bias torch.Size([768])

    model.decoder.layers.0.fc2.weight torch.Size([768, 3072])

    model.decoder.layers.0.final_layer_norm.bias torch.Size([768])

    model.decoder.layers.0.final_layer_norm.weight torch.Size([768])

    model.decoder.layers.0.self_attn.k_proj.bias torch.Size([768])

    model.decoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])

    model.decoder.layers.0.self_attn.out_proj.bias torch.Size([768])

    model.decoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])

    model.decoder.layers.0.self_attn.q_proj.bias torch.Size([768])

    model.decoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])

    model.decoder.layers.0.self_attn.v_proj.bias torch.Size([768])

    model.decoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])

    model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([768])

    model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([768])

    ```


    ### 30b

    ```python

    decoder.embed_positions.weight torch.Size([2050, 7168])

    decoder.embed_tokens.weight torch.Size([50000, 7168])

    decoder.layers.0.fc1.weight torch.Size([28672, 7168])

    decoder.layers.0.fc2.weight torch.Size([7168, 28672])

    decoder.layers.0.self_attn.k_proj.weight torch.Size([7168, 7168])

    decoder.layers.0.self_attn.out_proj.weight torch.Size([7168, 7168])

    decoder.layers.0.self_attn.q_proj.weight torch.Size([7168, 7168])

    decoder.layers.0.self_attn.v_proj.weight torch.Size([7168, 7168])

    ```'
  created_at: 2022-11-23 18:41:58+00:00
  edited: false
  hidden: false
  id: 637e6976460de456bee8139e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-11-23T21:27:51.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Great thank you very much! You really did a great job debugging
          here and helped me a lot understanding the rootcause of the issue.<br>Let''s
          wait for <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/20390">https://github.com/huggingface/transformers/pull/20390</a>
          to be addressed and keep the thread open here</p>

          '
        raw: "Great thank you very much! You really did a great job debugging here\
          \ and helped me a lot understanding the rootcause of the issue. \nLet's\
          \ wait for https://github.com/huggingface/transformers/pull/20390 to be\
          \ addressed and keep the thread open here"
        updatedAt: '2022-11-23T21:27:51.601Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - mrm8488
        - Jackmin108
        - lukaemon
        - hwasiti
    id: 637e90571dbae0919104b582
    type: comment
  author: ybelkada
  content: "Great thank you very much! You really did a great job debugging here and\
    \ helped me a lot understanding the rootcause of the issue. \nLet's wait for https://github.com/huggingface/transformers/pull/20390\
    \ to be addressed and keep the thread open here"
  created_at: 2022-11-23 21:27:51+00:00
  edited: false
  hidden: false
  id: 637e90571dbae0919104b582
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-25T22:38:04.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> The 120B did\
          \ not work eventually, but at least I think the model has been mapped to\
          \ the GPUs/RAM/SSD and gave me Cuda out-of-memory error in GPU0, which is\
          \ solvable I guess. I did not want to try again. The whole initialization\
          \ took around 10 hrs and it was not worth it. I just felt that it is not\
          \ practical to use such a slow model when I have only 64GB RAM and 2 GPUs\
          \ each with 11GB. The rest will be mapped to SSD and that is not a good\
          \ idea practically speaking. </p>\n<p>I do have the intention to spin up\
          \ an instance in AWS or GCP with huge RAM to fit all the models in RAM and\
          \ test how fast it is if using CPU only or use the CPU/640 GB RAM with 1\
          \ GPU. The cost for such an instance is around $1-2/hr (spot instance) which\
          \ is worth it in case I want a few hours to aid me in writing the intro\
          \ of a research paper or something.</p>\n"
        raw: "@ybelkada The 120B did not work eventually, but at least I think the\
          \ model has been mapped to the GPUs/RAM/SSD and gave me Cuda out-of-memory\
          \ error in GPU0, which is solvable I guess. I did not want to try again.\
          \ The whole initialization took around 10 hrs and it was not worth it. I\
          \ just felt that it is not practical to use such a slow model when I have\
          \ only 64GB RAM and 2 GPUs each with 11GB. The rest will be mapped to SSD\
          \ and that is not a good idea practically speaking. \n\nI do have the intention\
          \ to spin up an instance in AWS or GCP with huge RAM to fit all the models\
          \ in RAM and test how fast it is if using CPU only or use the CPU/640 GB\
          \ RAM with 1 GPU. The cost for such an instance is around $1-2/hr (spot\
          \ instance) which is worth it in case I want a few hours to aid me in writing\
          \ the intro of a research paper or something."
        updatedAt: '2022-11-25T22:38:04.995Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Jackmin108
        - ybelkada
    id: 638143cc4491649a3a6e0def
    type: comment
  author: hwasiti
  content: "@ybelkada The 120B did not work eventually, but at least I think the model\
    \ has been mapped to the GPUs/RAM/SSD and gave me Cuda out-of-memory error in\
    \ GPU0, which is solvable I guess. I did not want to try again. The whole initialization\
    \ took around 10 hrs and it was not worth it. I just felt that it is not practical\
    \ to use such a slow model when I have only 64GB RAM and 2 GPUs each with 11GB.\
    \ The rest will be mapped to SSD and that is not a good idea practically speaking.\
    \ \n\nI do have the intention to spin up an instance in AWS or GCP with huge RAM\
    \ to fit all the models in RAM and test how fast it is if using CPU only or use\
    \ the CPU/640 GB RAM with 1 GPU. The cost for such an instance is around $1-2/hr\
    \ (spot instance) which is worth it in case I want a few hours to aid me in writing\
    \ the intro of a research paper or something."
  created_at: 2022-11-25 22:38:04+00:00
  edited: false
  hidden: false
  id: 638143cc4491649a3a6e0def
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-11-29T09:24:15.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Hi there!<br><a rel="nofollow" href="https://github.com/huggingface/transformers/pull/20390">https://github.com/huggingface/transformers/pull/20390</a>
          is probably going to be merged, I can confirm that I can at least load the
          model with the fix, may I ask you to try the same thing on your side? The
          instructions would be:<br><code>pip install --upgrade git+https://github.com/younesbelkada/transformers.git@fix-opt-bias</code><br><code>git
          clone https://huggingface.co/facebook/galactica-30b/</code><br>Then modify
          the <code>config.json</code> file of the cloned repository by adding 2 lines:
          </p>

          <pre><code>"enable_bias": false,

          "layer_norm_elementwise_affine":false,

          </code></pre>

          <p>Looking forward to hearing from you!</p>

          '
        raw: "Hi there! \nhttps://github.com/huggingface/transformers/pull/20390 is\
          \ probably going to be merged, I can confirm that I can at least load the\
          \ model with the fix, may I ask you to try the same thing on your side?\
          \ The instructions would be:\n```pip install --upgrade git+https://github.com/younesbelkada/transformers.git@fix-opt-bias```\n\
          ```git clone https://huggingface.co/facebook/galactica-30b/```\nThen modify\
          \ the `config.json` file of the cloned repository by adding 2 lines: \n\
          ```\n\"enable_bias\": false,\n\"layer_norm_elementwise_affine\":false,\n\
          ```\nLooking forward to hearing from you!"
        updatedAt: '2022-11-29T09:24:50.684Z'
      numEdits: 2
      reactions: []
    id: 6385cfbfc12615765ca9dd72
    type: comment
  author: ybelkada
  content: "Hi there! \nhttps://github.com/huggingface/transformers/pull/20390 is\
    \ probably going to be merged, I can confirm that I can at least load the model\
    \ with the fix, may I ask you to try the same thing on your side? The instructions\
    \ would be:\n```pip install --upgrade git+https://github.com/younesbelkada/transformers.git@fix-opt-bias```\n\
    ```git clone https://huggingface.co/facebook/galactica-30b/```\nThen modify the\
    \ `config.json` file of the cloned repository by adding 2 lines: \n```\n\"enable_bias\"\
    : false,\n\"layer_norm_elementwise_affine\":false,\n```\nLooking forward to hearing\
    \ from you!"
  created_at: 2022-11-29 09:24:15+00:00
  edited: true
  hidden: false
  id: 6385cfbfc12615765ca9dd72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-30T07:28:07.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> Does this model\
          \ support half-precision (float16)?<br>Otherwise, I don't think it will\
          \ fit my 64GB RAM.</p>\n<p>See a similar issue with the 6.7b model:<br><a\
          \ href=\"https://huggingface.co/facebook/galactica-6.7b/discussions/6\"\
          >https://huggingface.co/facebook/galactica-6.7b/discussions/6</a></p>\n"
        raw: "@ybelkada Does this model support half-precision (float16)? \nOtherwise,\
          \ I don't think it will fit my 64GB RAM.\n\nSee a similar issue with the\
          \ 6.7b model:\nhttps://huggingface.co/facebook/galactica-6.7b/discussions/6"
        updatedAt: '2022-11-30T07:28:07.869Z'
      numEdits: 0
      reactions: []
    id: 6387060720244d72a73e57b7
    type: comment
  author: hwasiti
  content: "@ybelkada Does this model support half-precision (float16)? \nOtherwise,\
    \ I don't think it will fit my 64GB RAM.\n\nSee a similar issue with the 6.7b\
    \ model:\nhttps://huggingface.co/facebook/galactica-6.7b/discussions/6"
  created_at: 2022-11-30 07:28:07+00:00
  edited: false
  hidden: false
  id: 6387060720244d72a73e57b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-11-30T08:02:27.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Yes it should support float16, you just have to load it by adding
          the argument <code>torch_dtype=torch.float16</code> when calling <code>.from_pretrained</code></p>

          '
        raw: Yes it should support float16, you just have to load it by adding the
          argument `torch_dtype=torch.float16` when calling `.from_pretrained`
        updatedAt: '2022-11-30T08:02:27.974Z'
      numEdits: 0
      reactions: []
    id: 63870e13aaee5cd69dc08d57
    type: comment
  author: ybelkada
  content: Yes it should support float16, you just have to load it by adding the argument
    `torch_dtype=torch.float16` when calling `.from_pretrained`
  created_at: 2022-11-30 08:02:27+00:00
  edited: false
  hidden: false
  id: 63870e13aaee5cd69dc08d57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-30T09:38:31.000Z'
    data:
      edited: true
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: '<p>I have used that for the 6.7b model. So is the 6.7b model in particular
          not supporting the argument <code>torch_dtype=torch.float16</code> when
          calling <code>.from_pretrained</code>?</p>

          '
        raw: I have used that for the 6.7b model. So is the 6.7b model in particular
          not supporting the argument `torch_dtype=torch.float16` when calling `.from_pretrained`?
        updatedAt: '2022-11-30T09:39:46.209Z'
      numEdits: 1
      reactions: []
    id: 6387249785f406f24f532f1c
    type: comment
  author: hwasiti
  content: I have used that for the 6.7b model. So is the 6.7b model in particular
    not supporting the argument `torch_dtype=torch.float16` when calling `.from_pretrained`?
  created_at: 2022-11-30 09:38:31+00:00
  edited: true
  hidden: false
  id: 6387249785f406f24f532f1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-08-21T18:20:41.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-21T20:20:07.038Z'
      numEdits: 0
      reactions: []
    id: 64e3aaf9f2c90b2e1452a7f9
    type: comment
  author: Saptarshi7
  content: This comment has been hidden
  created_at: 2023-08-21 17:20:41+00:00
  edited: true
  hidden: true
  id: 64e3aaf9f2c90b2e1452a7f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: facebook/galactica-30b
repo_type: model
status: open
target_branch: null
title: KeyError
