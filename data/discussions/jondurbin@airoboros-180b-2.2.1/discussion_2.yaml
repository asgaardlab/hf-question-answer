!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmajor
conflicting_files: null
created_at: 2023-11-03 08:45:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-11-03T09:45:02.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9552132487297058
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Since the GGUF/llama.cpp option is broken (for now?) I want to see
          if transformers has ''advanced'' enough to allow loading models in 8bit
          when using CPUs for inference.</p>

          <p>I have a woeful amount of VRAM, but with 256 GB of RAM,  a quantized
          version of the model should fit... but since transformers have not been
          relevant to me for a while due to their VRAM focus for development, I am
          out of touch regarding transformers ability to use the CPU with quantization
          methods, so I am interested in seeing if you or someone else that haunts
          your page could chime in and offer some advice.</p>

          '
        raw: "Since the GGUF/llama.cpp option is broken (for now?) I want to see if\
          \ transformers has 'advanced' enough to allow loading models in 8bit when\
          \ using CPUs for inference.\r\n\r\nI have a woeful amount of VRAM, but with\
          \ 256 GB of RAM,  a quantized version of the model should fit... but since\
          \ transformers have not been relevant to me for a while due to their VRAM\
          \ focus for development, I am out of touch regarding transformers ability\
          \ to use the CPU with quantization methods, so I am interested in seeing\
          \ if you or someone else that haunts your page could chime in and offer\
          \ some advice."
        updatedAt: '2023-11-03T09:45:02.077Z'
      numEdits: 0
      reactions: []
    id: 6544c11e6f7fb5b3548e6fa7
    type: comment
  author: vmajor
  content: "Since the GGUF/llama.cpp option is broken (for now?) I want to see if\
    \ transformers has 'advanced' enough to allow loading models in 8bit when using\
    \ CPUs for inference.\r\n\r\nI have a woeful amount of VRAM, but with 256 GB of\
    \ RAM,  a quantized version of the model should fit... but since transformers\
    \ have not been relevant to me for a while due to their VRAM focus for development,\
    \ I am out of touch regarding transformers ability to use the CPU with quantization\
    \ methods, so I am interested in seeing if you or someone else that haunts your\
    \ page could chime in and offer some advice."
  created_at: 2023-11-03 08:45:02+00:00
  edited: false
  hidden: false
  id: 6544c11e6f7fb5b3548e6fa7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39851163dc31c98e5b2fef1b9fddf632.svg
      fullname: Malando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ransom
      type: user
    createdAt: '2023-11-05T10:02:47.000Z'
    data:
      edited: false
      editors:
      - Ransom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5756948590278625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39851163dc31c98e5b2fef1b9fddf632.svg
          fullname: Malando
          isHf: false
          isPro: false
          name: Ransom
          type: user
        html: '<p>Someone has posted a quant that works with llama.cpp here: <a href="https://huggingface.co/imi2/airoboros-180b-2.2.1-gguf">https://huggingface.co/imi2/airoboros-180b-2.2.1-gguf</a><br>Just
          make sure you''re running the latest version of llama.cpp and follow the
          instructions for merging the files.<br>Here''s the command I use to run
          it:<br>./server --model models/airoboros-180b-2.2.1-Q5_K_M.gguf --n-gpu-layers
          128 --ctx-size 4090 --port 5005 --host 0.0.0.0  --parallel 1 --cont-batching
          --threads 24</p>

          '
        raw: 'Someone has posted a quant that works with llama.cpp here: https://huggingface.co/imi2/airoboros-180b-2.2.1-gguf

          Just make sure you''re running the latest version of llama.cpp and follow
          the instructions for merging the files.

          Here''s the command I use to run it:

          ./server --model models/airoboros-180b-2.2.1-Q5_K_M.gguf --n-gpu-layers
          128 --ctx-size 4090 --port 5005 --host 0.0.0.0  --parallel 1 --cont-batching
          --threads 24

          '
        updatedAt: '2023-11-05T10:02:47.042Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vmajor
    id: 654768479295970f8790071d
    type: comment
  author: Ransom
  content: 'Someone has posted a quant that works with llama.cpp here: https://huggingface.co/imi2/airoboros-180b-2.2.1-gguf

    Just make sure you''re running the latest version of llama.cpp and follow the
    instructions for merging the files.

    Here''s the command I use to run it:

    ./server --model models/airoboros-180b-2.2.1-Q5_K_M.gguf --n-gpu-layers 128 --ctx-size
    4090 --port 5005 --host 0.0.0.0  --parallel 1 --cont-batching --threads 24

    '
  created_at: 2023-11-05 10:02:47+00:00
  edited: false
  hidden: false
  id: 654768479295970f8790071d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jondurbin/airoboros-180b-2.2.1
repo_type: model
status: open
target_branch: null
title: Question about being able to load the model
