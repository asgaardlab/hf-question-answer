!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gautamjain8
conflicting_files: null
created_at: 2023-06-09 13:14:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f952f56c929d13a470d8df8ae03403d.svg
      fullname: Gautam Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gautamjain8
      type: user
    createdAt: '2023-06-09T14:14:30.000Z'
    data:
      edited: true
      editors:
      - gautamjain8
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616470336914062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f952f56c929d13a470d8df8ae03403d.svg
          fullname: Gautam Jain
          isHf: false
          isPro: false
          name: gautamjain8
          type: user
        html: '<p>Hi, I''m fairly new to the Hugging Face hub and all the models.
          While I am fascinated by the things achieved by leveraging the power of
          this fantastic model, I am unable to try and test it out on my own IDE.
          I''m not sure on how I can use this model and I am seeking any form of help
          on how I can go about it. Any suggestions are appreciated. Thanks.</p>

          <p>Edit: I tried to copy the code on the Colab notebook but it doesn''t
          seem to work, either</p>

          '
        raw: 'Hi, I''m fairly new to the Hugging Face hub and all the models. While
          I am fascinated by the things achieved by leveraging the power of this fantastic
          model, I am unable to try and test it out on my own IDE. I''m not sure on
          how I can use this model and I am seeking any form of help on how I can
          go about it. Any suggestions are appreciated. Thanks.


          Edit: I tried to copy the code on the Colab notebook but it doesn''t seem
          to work, either'
        updatedAt: '2023-06-09T14:16:58.128Z'
      numEdits: 1
      reactions: []
    id: 648333c66e7ccc304d29c8d4
    type: comment
  author: gautamjain8
  content: 'Hi, I''m fairly new to the Hugging Face hub and all the models. While
    I am fascinated by the things achieved by leveraging the power of this fantastic
    model, I am unable to try and test it out on my own IDE. I''m not sure on how
    I can use this model and I am seeking any form of help on how I can go about it.
    Any suggestions are appreciated. Thanks.


    Edit: I tried to copy the code on the Colab notebook but it doesn''t seem to work,
    either'
  created_at: 2023-06-09 13:14:30+00:00
  edited: true
  hidden: false
  id: 648333c66e7ccc304d29c8d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-16T09:48:08.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8904697895050049
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>hi! thanks for your interest in the model. When using the colab
          in chrome as intended, it does work for you correctly or not?</p>

          <p>for the basics/installation I would recommend checking out the <a href="https://huggingface.co/docs/transformers/installation">docs</a>
          or the <a href="https://huggingface.co/learn">course</a> and then seeking
          help on the forums as needed.</p>

          '
        raw: 'hi! thanks for your interest in the model. When using the colab in chrome
          as intended, it does work for you correctly or not?


          for the basics/installation I would recommend checking out the [docs](https://huggingface.co/docs/transformers/installation)
          or the [course](https://huggingface.co/learn) and then seeking help on the
          forums as needed.'
        updatedAt: '2023-06-16T09:48:08.888Z'
      numEdits: 0
      reactions: []
    id: 648c2fd8e939f674f7aac279
    type: comment
  author: pszemraj
  content: 'hi! thanks for your interest in the model. When using the colab in chrome
    as intended, it does work for you correctly or not?


    for the basics/installation I would recommend checking out the [docs](https://huggingface.co/docs/transformers/installation)
    or the [course](https://huggingface.co/learn) and then seeking help on the forums
    as needed.'
  created_at: 2023-06-16 08:48:08+00:00
  edited: false
  hidden: false
  id: 648c2fd8e939f674f7aac279
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f952f56c929d13a470d8df8ae03403d.svg
      fullname: Gautam Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gautamjain8
      type: user
    createdAt: '2023-06-19T13:24:22.000Z'
    data:
      edited: false
      editors:
      - gautamjain8
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953447699546814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f952f56c929d13a470d8df8ae03403d.svg
          fullname: Gautam Jain
          isHf: false
          isPro: false
          name: gautamjain8
          type: user
        html: '<p>Hi. I was able to get the ball rolling and successfully ran the
          basic implementation. I am aware that the xl model is quite large and should
          ideally be used with a GPU, but since I have limited resources (16 GB RAM,
          No GPU), I am struggling to make it generate text faster.<br>If you have
          any suggestions on how I can do so, please let me know.<br>Also, using it
          with the streamlit package generates pipeline errors. Any ideas on how I
          can fix it?</p>

          '
        raw: "Hi. I was able to get the ball rolling and successfully ran the basic\
          \ implementation. I am aware that the xl model is quite large and should\
          \ ideally be used with a GPU, but since I have limited resources (16 GB\
          \ RAM, No GPU), I am struggling to make it generate text faster. \nIf you\
          \ have any suggestions on how I can do so, please let me know.\nAlso, using\
          \ it with the streamlit package generates pipeline errors. Any ideas on\
          \ how I can fix it?"
        updatedAt: '2023-06-19T13:24:22.831Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
    id: 649057061afdee3acd0413a7
    type: comment
  author: gautamjain8
  content: "Hi. I was able to get the ball rolling and successfully ran the basic\
    \ implementation. I am aware that the xl model is quite large and should ideally\
    \ be used with a GPU, but since I have limited resources (16 GB RAM, No GPU),\
    \ I am struggling to make it generate text faster. \nIf you have any suggestions\
    \ on how I can do so, please let me know.\nAlso, using it with the streamlit package\
    \ generates pipeline errors. Any ideas on how I can fix it?"
  created_at: 2023-06-19 12:24:22+00:00
  edited: false
  hidden: false
  id: 649057061afdee3acd0413a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-19T16:44:04.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9268489480018616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<blockquote>

          <p>If you have any suggestions on how I can do so, please let me know.</p>

          </blockquote>

          <p>I''d say the following as a quick list of recommendations:</p>

          <ol>

          <li>when running inference, trying playing with <code>num_beams</code> (decreasing
          it from the default of ~4 will be faster, but yield worse performance)</li>

          <li>see if using a <a href="https://huggingface.co/models?other=grammar%20synthesis">smaller
          model fine-tuned to do the same thing works for you</a>. you can also try
          "increasing" the performance via number of beams higher than default on
          the smaller models </li>

          <li>do the things on the <a href="https://huggingface.co/docs/transformers/perf_infer_cpu">inference
          with CPU</a> page - note that <em>BetterTransformer</em> is not supported
          for T5 based models but you could still try <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines">onnx
          inference</a></li>

          </ol>

          <p>be careful/monitor performance  for the model/data you are working with
          if you decide to use optimum. for use cases where there is discretely a
          "right" answer (i.e. here, bc it''s grammar). I find that  post-training
          optimization can be very sensitive </p>

          <blockquote>

          <p>Also, using it with the streamlit package generates pipeline errors.
          Any ideas on how I can fix it?</p>

          </blockquote>

          <p>Can''t help here, sorry! I have not used streamlit in a while. For the
          <code>large</code> model I do have a working <code>gradio</code>-based demo
          app that you can see <a href="https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis">here</a>.</p>

          <hr>

          <p>Hope that helps! I''m going to mark this as closed as the issue for inference
          seems resolved, but you are of course welcome to still comment here and
          also re-open should you have other issues with the base usage of the model.
          </p>

          '
        raw: "> If you have any suggestions on how I can do so, please let me know.\n\
          \nI'd say the following as a quick list of recommendations:\n\n1. when running\
          \ inference, trying playing with `num_beams` (decreasing it from the default\
          \ of ~4 will be faster, but yield worse performance)\n2. see if using a\
          \ [smaller model fine-tuned to do the same thing works for you](https://huggingface.co/models?other=grammar%20synthesis).\
          \ you can also try \"increasing\" the performance via number of beams higher\
          \ than default on the smaller models \n3. do the things on the [inference\
          \ with CPU](https://huggingface.co/docs/transformers/perf_infer_cpu) page\
          \ - note that _BetterTransformer_ is not supported for T5 based models but\
          \ you could still try [onnx inference](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines)\n\
          \nbe careful/monitor performance  for the model/data you are working with\
          \ if you decide to use optimum. for use cases where there is discretely\
          \ a \"right\" answer (i.e. here, bc it's grammar). I find that  post-training\
          \ optimization can be very sensitive \n\n\n> Also, using it with the streamlit\
          \ package generates pipeline errors. Any ideas on how I can fix it?\n\n\
          Can't help here, sorry! I have not used streamlit in a while. For the `large`\
          \ model I do have a working `gradio`-based demo app that you can see [here](https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis).\n\
          \n\n---\n\nHope that helps! I'm going to mark this as closed as the issue\
          \ for inference seems resolved, but you are of course welcome to still comment\
          \ here and also re-open should you have other issues with the base usage\
          \ of the model. \n"
        updatedAt: '2023-06-19T16:44:04.213Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649085d4175db2a6ed1f4328
    id: 649085d4175db2a6ed1f4327
    type: comment
  author: pszemraj
  content: "> If you have any suggestions on how I can do so, please let me know.\n\
    \nI'd say the following as a quick list of recommendations:\n\n1. when running\
    \ inference, trying playing with `num_beams` (decreasing it from the default of\
    \ ~4 will be faster, but yield worse performance)\n2. see if using a [smaller\
    \ model fine-tuned to do the same thing works for you](https://huggingface.co/models?other=grammar%20synthesis).\
    \ you can also try \"increasing\" the performance via number of beams higher than\
    \ default on the smaller models \n3. do the things on the [inference with CPU](https://huggingface.co/docs/transformers/perf_infer_cpu)\
    \ page - note that _BetterTransformer_ is not supported for T5 based models but\
    \ you could still try [onnx inference](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines)\n\
    \nbe careful/monitor performance  for the model/data you are working with if you\
    \ decide to use optimum. for use cases where there is discretely a \"right\" answer\
    \ (i.e. here, bc it's grammar). I find that  post-training optimization can be\
    \ very sensitive \n\n\n> Also, using it with the streamlit package generates pipeline\
    \ errors. Any ideas on how I can fix it?\n\nCan't help here, sorry! I have not\
    \ used streamlit in a while. For the `large` model I do have a working `gradio`-based\
    \ demo app that you can see [here](https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis).\n\
    \n\n---\n\nHope that helps! I'm going to mark this as closed as the issue for\
    \ inference seems resolved, but you are of course welcome to still comment here\
    \ and also re-open should you have other issues with the base usage of the model.\
    \ \n"
  created_at: 2023-06-19 15:44:04+00:00
  edited: false
  hidden: false
  id: 649085d4175db2a6ed1f4327
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-06-19T16:44:04.000Z'
    data:
      status: closed
    id: 649085d4175db2a6ed1f4328
    type: status-change
  author: pszemraj
  created_at: 2023-06-19 15:44:04+00:00
  id: 649085d4175db2a6ed1f4328
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pszemraj/flan-t5-xl-grammar-synthesis
repo_type: model
status: closed
target_branch: null
title: How to start using the model?
