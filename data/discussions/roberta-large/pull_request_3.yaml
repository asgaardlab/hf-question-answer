!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fa2345
conflicting_files: []
created_at: 2023-03-17 11:08:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce79f36f925221afe184ae/CfJenrymvFkz2vH7MaTya.png?w=200&h=200&f=face
      fullname: Fahad Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fa2345
      type: user
    createdAt: '2023-03-17T12:08:04.000Z'
    data:
      edited: false
      editors:
      - fa2345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce79f36f925221afe184ae/CfJenrymvFkz2vH7MaTya.png?w=200&h=200&f=face
          fullname: Fahad Ali
          isHf: false
          isPro: false
          name: fa2345
          type: user
        html: "<p>Beep boop I am the <a href=\"https://huggingface.co/spaces/optimum/exporters\"\
          >ONNX export bot \U0001F916\U0001F3CE\uFE0F</a>. On behalf of <a href=\"\
          https://huggingface.co/fa2345\">fa2345</a>, I would like to add to this\
          \ repository the model converted to ONNX.</p>\n<p>What is ONNX? It stands\
          \ for \"Open Neural Network Exchange\", and is the most commonly used open\
          \ standard for machine learning interoperability. You can find out more\
          \ at <a rel=\"nofollow\" href=\"https://onnx.ai/\">onnx.ai</a>!</p>\n<p>The\
          \ exported ONNX model can be then be consumed by various backends as TensorRT\
          \ or TVM, or simply be used in a few lines with \U0001F917 Optimum through\
          \ ONNX Runtime, check out how <a href=\"https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models\"\
          >here</a>!</p>\n"
        raw: "Beep boop I am the [ONNX export bot \U0001F916\U0001F3CE\uFE0F](https://huggingface.co/spaces/optimum/exporters).\
          \ On behalf of [fa2345](https://huggingface.co/fa2345), I would like to\
          \ add to this repository the model converted to ONNX.\n\nWhat is ONNX? It\
          \ stands for \"Open Neural Network Exchange\", and is the most commonly\
          \ used open standard for machine learning interoperability. You can find\
          \ out more at [onnx.ai](https://onnx.ai/)!\n\nThe exported ONNX model can\
          \ be then be consumed by various backends as TensorRT or TVM, or simply\
          \ be used in a few lines with \U0001F917 Optimum through ONNX Runtime, check\
          \ out how [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models)!"
        updatedAt: '2023-03-17T12:08:04.689Z'
      numEdits: 0
      reactions: []
    id: 64145824cf6b10e52e2f21df
    type: comment
  author: fa2345
  content: "Beep boop I am the [ONNX export bot \U0001F916\U0001F3CE\uFE0F](https://huggingface.co/spaces/optimum/exporters).\
    \ On behalf of [fa2345](https://huggingface.co/fa2345), I would like to add to\
    \ this repository the model converted to ONNX.\n\nWhat is ONNX? It stands for\
    \ \"Open Neural Network Exchange\", and is the most commonly used open standard\
    \ for machine learning interoperability. You can find out more at [onnx.ai](https://onnx.ai/)!\n\
    \nThe exported ONNX model can be then be consumed by various backends as TensorRT\
    \ or TVM, or simply be used in a few lines with \U0001F917 Optimum through ONNX\
    \ Runtime, check out how [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models)!"
  created_at: 2023-03-17 11:08:04+00:00
  edited: false
  hidden: false
  id: 64145824cf6b10e52e2f21df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce79f36f925221afe184ae/CfJenrymvFkz2vH7MaTya.png?w=200&h=200&f=face
      fullname: Fahad Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fa2345
      type: user
    createdAt: '2023-03-17T12:08:05.000Z'
    data:
      oid: ae4a20221bb77f66ff11bded5513996e1728b865
      parents:
      - 5069d8a2a32a7df4c69ef9b56348be04152a2341
      subject: Adding ONNX file of this model
    id: '641458250000000000000000'
    type: commit
  author: fa2345
  created_at: 2023-03-17 11:08:05+00:00
  id: '641458250000000000000000'
  oid: ae4a20221bb77f66ff11bded5513996e1728b865
  summary: Adding ONNX file of this model
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2023-03-21T14:58:54.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mfuntowicz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mfuntowicz\">@<span class=\"\
          underline\">mfuntowicz</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;fxmarty&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/fxmarty\">@<span class=\"underline\">fxmarty</span></a></span>\n\
          \n\t</span></span> ok for you to merge?</p>\n"
        raw: '@mfuntowicz @fxmarty ok for you to merge?'
        updatedAt: '2023-03-21T14:58:54.971Z'
      numEdits: 0
      reactions: []
    id: 6419c62e3c73fa42caf12963
    type: comment
  author: julien-c
  content: '@mfuntowicz @fxmarty ok for you to merge?'
  created_at: 2023-03-21 13:58:54+00:00
  edited: false
  hidden: false
  id: 6419c62e3c73fa42caf12963
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583858935715-5e67c47c100906368940747e.jpeg?w=200&h=200&f=face
      fullname: Morgan Funtowicz
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mfuntowicz
      type: user
    createdAt: '2023-03-22T08:05:53.000Z'
    data:
      edited: false
      editors:
      - mfuntowicz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583858935715-5e67c47c100906368940747e.jpeg?w=200&h=200&f=face
          fullname: Morgan Funtowicz
          isHf: true
          isPro: false
          name: mfuntowicz
          type: user
        html: '<p>Sounds good!</p>

          '
        raw: Sounds good!
        updatedAt: '2023-03-22T08:05:53.457Z'
      numEdits: 0
      reactions: []
    id: 641ab6e1fc01c26fcae85407
    type: comment
  author: mfuntowicz
  content: Sounds good!
  created_at: 2023-03-22 07:05:53+00:00
  edited: false
  hidden: false
  id: 641ab6e1fc01c26fcae85407
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2023-03-22T09:25:01.000Z'
    data:
      status: merged
    id: 641ac96d1b6ca4151d992d48
    type: status-change
  author: julien-c
  created_at: 2023-03-22 08:25:01+00:00
  id: 641ac96d1b6ca4151d992d48
  new_status: merged
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04e841988d44db8b2024a57f588ceafb.svg
      fullname: Hrayr Matevosyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HrayrM
      type: user
    createdAt: '2024-01-12T01:52:10.000Z'
    data:
      edited: true
      editors:
      - HrayrM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5257536768913269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04e841988d44db8b2024a57f588ceafb.svg
          fullname: Hrayr Matevosyan
          isHf: false
          isPro: false
          name: HrayrM
          type: user
        html: "<p>Hello, </p>\n<p> We tried to use the included .onnx model for roberta-large\
          \ for benchmarking purposes against PyTorch version, but the results were\
          \ unusual both in inference timings and the model outputs. A simple test\
          \ shows that somehow the last dimension of the .onnx model's last hidden\
          \ state if 50625 (the tokenizer's vocab size), while for the PyTorch model\
          \ it is the expected 1024. </p>\n<p> When I create a .onnx model using Optimum\
          \ all works as expected. A sample test script is attached.</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-comment\">#!/usr/bin/env\
          \ python</span>\n<span class=\"hljs-comment\"># coding: utf-8</span>\n\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># A\
          \ simple comparison of Roberta-Large model'inference outputs with PyTorch\
          \ vs ONNX.</span>\n<span class=\"hljs-comment\">#</span>\n\n<span class=\"\
          hljs-comment\"># Import the needed modules</span>\n<span class=\"hljs-keyword\"\
          >import</span> time\n<span class=\"hljs-keyword\">import</span> numpy <span\
          \ class=\"hljs-keyword\">as</span> np\n\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">import</span> transformers\n<span\
          \ class=\"hljs-keyword\">from</span> optimum.onnxruntime <span class=\"\
          hljs-keyword\">import</span> ORTModel\n<span class=\"hljs-keyword\">import</span>\
          \ onnxruntime <span class=\"hljs-keyword\">as</span> rt\n\n<span class=\"\
          hljs-comment\"># Specify the local path to the model checkpoint directory</span>\n\
          model_checkpoint = <span class=\"hljs-string\">\"./roberta-large\"</span>\n\
          model_name_onnx = <span class=\"hljs-string\">\"model.onnx\"</span>\n\n\
          <span class=\"hljs-comment\"># Load the Pt Model using .safetensors</span>\n\
          model_pt = transformers.AutoModel.from_pretrained(<span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{model_checkpoint}</span>\"</span>, use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>);\n\n<span class=\"hljs-comment\">#\
          \ Prepare the ONNX model</span>\nsess_options = rt.SessionOptions()\nsess_options.graph_optimization_level\
          \ = rt.GraphOptimizationLevel.ORT_DISABLE_ALL\nproviders=[<span class=\"\
          hljs-string\">\"CPUExecutionProvider\"</span>]\nrt_session =  rt.InferenceSession(\n\
          \    <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{model_checkpoint}</span>/<span\
          \ class=\"hljs-subst\">{model_name_onnx}</span>\"</span>,\n    providers=providers,\n\
          \    sess_options=sess_options,\n)\n\n<span class=\"hljs-comment\"># Create\
          \ a sample input</span>\ninput_text = <span class=\"hljs-string\">\"Once\
          \ upon a time, in a land down under, a drop bear was having a heated discussion\
          \ with a roo on why he needs to switch to using ONNX instead of PyTorch.\"\
          </span>\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n\
          encodings_pt   = tokenizer(input_text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>, max_length=<span class=\"hljs-number\">128</span>, truncation=<span\
          \ class=\"hljs-literal\">True</span>, padding = <span class=\"hljs-literal\"\
          >False</span>)\nencodings_onnx = tokenizer(input_text, return_tensors=<span\
          \ class=\"hljs-string\">\"np\"</span>, max_length=<span class=\"hljs-number\"\
          >128</span>, truncation=<span class=\"hljs-literal\">True</span>, padding\
          \ = <span class=\"hljs-literal\">False</span>)\n\n<span class=\"hljs-comment\"\
          ># Infer outputs using both frameworks</span>\noutputs_pt    = model_pt.forward(**encodings_pt).last_hidden_state.detach().cpu().numpy()\n\
          outputs_onnx  = rt_session.run(<span class=\"hljs-literal\">None</span>,\
          \ <span class=\"hljs-built_in\">dict</span>(encodings_onnx))[<span class=\"\
          hljs-number\">0</span>]\n\n<span class=\"hljs-comment\"># Observe the mismatched\
          \ output shapes</span>\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"\\n\"</span> + <span class=\"hljs-string\">\"\
          -\"</span>*<span class=\"hljs-number\">40</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Output shape PT:       <span\
          \ class=\"hljs-subst\">{outputs_pt.shape}</span>\"</span>)\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Output shape\
          \ ONNX:     <span class=\"hljs-subst\">{outputs_onnx.shape}</span>\"</span>)\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          -\"</span>*<span class=\"hljs-number\">40</span> + <span class=\"hljs-string\"\
          >\"\\n\"</span>)\n\n<span class=\"hljs-comment\"># Create an ONNX file using\
          \ Optimum</span>\nort_model = ORTModel.from_pretrained(model_checkpoint,\
          \ export=<span class=\"hljs-literal\">True</span>)\nrt_opt_model_path =\
          \ <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{model_checkpoint}</span>/opt_onnx/\"\
          </span>\nort_model.save_pretrained(rt_opt_model_path)\n\n<span class=\"\
          hljs-comment\"># Load the new ONNX model and run inference on the sample\
          \ input</span>\nrt_session_opt =  rt.InferenceSession(\n    <span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{rt_opt_model_path}</span>/model.onnx\"\
          </span>,\n    providers=providers,\n    sess_options=sess_options,\n)\n\
          outputs_onnx_opt  = rt_session_opt.run(<span class=\"hljs-literal\">None</span>,\
          \ <span class=\"hljs-built_in\">dict</span>(encodings_onnx))[<span class=\"\
          hljs-number\">0</span>]\n\n<span class=\"hljs-comment\"># Compare the output\
          \ shapes to ensure that the Optimum-created  ONNX model's result is identical\
          \ to the PyTorch one</span>\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"\\n\"</span> + <span class=\"hljs-string\">\"\
          -\"</span>*<span class=\"hljs-number\">40</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Output shape PT:       <span\
          \ class=\"hljs-subst\">{outputs_pt.shape}</span>\"</span>)\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Output shape\
          \ ONNX OPT: <span class=\"hljs-subst\">{outputs_onnx_opt.shape}</span>\"\
          </span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"All elements close: <span class=\"hljs-subst\">{np.allclose(outputs_pt[<span\
          \ class=\"hljs-number\">0</span>], outputs_onnx_opt[<span class=\"hljs-number\"\
          >0</span>], rtol=<span class=\"hljs-number\">1e-05</span>, atol=<span class=\"\
          hljs-number\">1e-5</span>)}</span>\"</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"-\"</span>*<span class=\"hljs-number\"\
          >40</span> + <span class=\"hljs-string\">\"\\n\"</span>)\n\n<span class=\"\
          hljs-comment\"># Sample elements of the hidden states</span>\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\n\"</span>\
          \ + <span class=\"hljs-string\">\"-\"</span>*<span class=\"hljs-number\"\
          >40</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{np.argmax(outputs_pt[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-number\">0</span>])=}</span>\"</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{np.argmax(outputs_onnx_opt[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-number\">0</span>])=}</span>\"</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{np.argmax(outputs_onnx[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-number\">0</span>])=}</span>\"</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"-\"\
          </span>*<span class=\"hljs-number\">40</span> + <span class=\"hljs-string\"\
          >\"\\n\"</span>)\n</code></pre>\n"
        raw: "Hello, \n\n We tried to use the included .onnx model for roberta-large\
          \ for benchmarking purposes against PyTorch version, but the results were\
          \ unusual both in inference timings and the model outputs. A simple test\
          \ shows that somehow the last dimension of the .onnx model's last hidden\
          \ state if 50625 (the tokenizer's vocab size), while for the PyTorch model\
          \ it is the expected 1024. \n\n When I create a .onnx model using Optimum\
          \ all works as expected. A sample test script is attached.\n\n```python\n\
          #!/usr/bin/env python\n# coding: utf-8\n\n#\n# A simple comparison of Roberta-Large\
          \ model'inference outputs with PyTorch vs ONNX.\n#\n\n# Import the needed\
          \ modules\nimport time\nimport numpy as np\n\nimport torch\nimport transformers\n\
          from optimum.onnxruntime import ORTModel\nimport onnxruntime as rt\n\n#\
          \ Specify the local path to the model checkpoint directory\nmodel_checkpoint\
          \ = \"./roberta-large\"\nmodel_name_onnx = \"model.onnx\"\n\n# Load the\
          \ Pt Model using .safetensors\nmodel_pt = transformers.AutoModel.from_pretrained(f\"\
          {model_checkpoint}\", use_safetensors=True);\n\n# Prepare the ONNX model\n\
          sess_options = rt.SessionOptions()\nsess_options.graph_optimization_level\
          \ = rt.GraphOptimizationLevel.ORT_DISABLE_ALL\nproviders=[\"CPUExecutionProvider\"\
          ]\nrt_session =  rt.InferenceSession(\n    f\"{model_checkpoint}/{model_name_onnx}\"\
          ,\n    providers=providers,\n    sess_options=sess_options,\n)\n\n# Create\
          \ a sample input\ninput_text = \"Once upon a time, in a land down under,\
          \ a drop bear was having a heated discussion with a roo on why he needs\
          \ to switch to using ONNX instead of PyTorch.\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n\
          encodings_pt   = tokenizer(input_text, return_tensors=\"pt\", max_length=128,\
          \ truncation=True, padding = False)\nencodings_onnx = tokenizer(input_text,\
          \ return_tensors=\"np\", max_length=128, truncation=True, padding = False)\n\
          \n# Infer outputs using both frameworks\noutputs_pt    = model_pt.forward(**encodings_pt).last_hidden_state.detach().cpu().numpy()\n\
          outputs_onnx  = rt_session.run(None, dict(encodings_onnx))[0]\n\n# Observe\
          \ the mismatched output shapes\nprint(\"\\n\" + \"-\"*40)\nprint(f\"Output\
          \ shape PT:       {outputs_pt.shape}\")\nprint(f\"Output shape ONNX:   \
          \  {outputs_onnx.shape}\")\nprint(\"-\"*40 + \"\\n\")\n\n# Create an ONNX\
          \ file using Optimum\nort_model = ORTModel.from_pretrained(model_checkpoint,\
          \ export=True)\nrt_opt_model_path = f\"{model_checkpoint}/opt_onnx/\"\n\
          ort_model.save_pretrained(rt_opt_model_path)\n\n# Load the new ONNX model\
          \ and run inference on the sample input\nrt_session_opt =  rt.InferenceSession(\n\
          \    f\"{rt_opt_model_path}/model.onnx\",\n    providers=providers,\n  \
          \  sess_options=sess_options,\n)\noutputs_onnx_opt  = rt_session_opt.run(None,\
          \ dict(encodings_onnx))[0]\n\n# Compare the output shapes to ensure that\
          \ the Optimum-created  ONNX model's result is identical to the PyTorch one\n\
          print(\"\\n\" + \"-\"*40)\nprint(f\"Output shape PT:       {outputs_pt.shape}\"\
          )\nprint(f\"Output shape ONNX OPT: {outputs_onnx_opt.shape}\")\nprint(f\"\
          All elements close: {np.allclose(outputs_pt[0], outputs_onnx_opt[0], rtol=1e-05,\
          \ atol=1e-5)}\")\nprint(\"-\"*40 + \"\\n\")\n\n# Sample elements of the\
          \ hidden states\nprint(\"\\n\" + \"-\"*40)\nprint(f\"{np.argmax(outputs_pt[0][0])=}\"\
          )\nprint(f\"{np.argmax(outputs_onnx_opt[0][0])=}\")\nprint(f\"{np.argmax(outputs_onnx[0][0])=}\"\
          )\nprint(\"-\"*40 + \"\\n\")\n```\n"
        updatedAt: '2024-01-14T23:28:23.280Z'
      numEdits: 1
      reactions: []
    id: 65a09b4a5fafc248c20fbfc3
    type: comment
  author: HrayrM
  content: "Hello, \n\n We tried to use the included .onnx model for roberta-large\
    \ for benchmarking purposes against PyTorch version, but the results were unusual\
    \ both in inference timings and the model outputs. A simple test shows that somehow\
    \ the last dimension of the .onnx model's last hidden state if 50625 (the tokenizer's\
    \ vocab size), while for the PyTorch model it is the expected 1024. \n\n When\
    \ I create a .onnx model using Optimum all works as expected. A sample test script\
    \ is attached.\n\n```python\n#!/usr/bin/env python\n# coding: utf-8\n\n#\n# A\
    \ simple comparison of Roberta-Large model'inference outputs with PyTorch vs ONNX.\n\
    #\n\n# Import the needed modules\nimport time\nimport numpy as np\n\nimport torch\n\
    import transformers\nfrom optimum.onnxruntime import ORTModel\nimport onnxruntime\
    \ as rt\n\n# Specify the local path to the model checkpoint directory\nmodel_checkpoint\
    \ = \"./roberta-large\"\nmodel_name_onnx = \"model.onnx\"\n\n# Load the Pt Model\
    \ using .safetensors\nmodel_pt = transformers.AutoModel.from_pretrained(f\"{model_checkpoint}\"\
    , use_safetensors=True);\n\n# Prepare the ONNX model\nsess_options = rt.SessionOptions()\n\
    sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_DISABLE_ALL\n\
    providers=[\"CPUExecutionProvider\"]\nrt_session =  rt.InferenceSession(\n   \
    \ f\"{model_checkpoint}/{model_name_onnx}\",\n    providers=providers,\n    sess_options=sess_options,\n\
    )\n\n# Create a sample input\ninput_text = \"Once upon a time, in a land down\
    \ under, a drop bear was having a heated discussion with a roo on why he needs\
    \ to switch to using ONNX instead of PyTorch.\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n\
    encodings_pt   = tokenizer(input_text, return_tensors=\"pt\", max_length=128,\
    \ truncation=True, padding = False)\nencodings_onnx = tokenizer(input_text, return_tensors=\"\
    np\", max_length=128, truncation=True, padding = False)\n\n# Infer outputs using\
    \ both frameworks\noutputs_pt    = model_pt.forward(**encodings_pt).last_hidden_state.detach().cpu().numpy()\n\
    outputs_onnx  = rt_session.run(None, dict(encodings_onnx))[0]\n\n# Observe the\
    \ mismatched output shapes\nprint(\"\\n\" + \"-\"*40)\nprint(f\"Output shape PT:\
    \       {outputs_pt.shape}\")\nprint(f\"Output shape ONNX:     {outputs_onnx.shape}\"\
    )\nprint(\"-\"*40 + \"\\n\")\n\n# Create an ONNX file using Optimum\nort_model\
    \ = ORTModel.from_pretrained(model_checkpoint, export=True)\nrt_opt_model_path\
    \ = f\"{model_checkpoint}/opt_onnx/\"\nort_model.save_pretrained(rt_opt_model_path)\n\
    \n# Load the new ONNX model and run inference on the sample input\nrt_session_opt\
    \ =  rt.InferenceSession(\n    f\"{rt_opt_model_path}/model.onnx\",\n    providers=providers,\n\
    \    sess_options=sess_options,\n)\noutputs_onnx_opt  = rt_session_opt.run(None,\
    \ dict(encodings_onnx))[0]\n\n# Compare the output shapes to ensure that the Optimum-created\
    \  ONNX model's result is identical to the PyTorch one\nprint(\"\\n\" + \"-\"\
    *40)\nprint(f\"Output shape PT:       {outputs_pt.shape}\")\nprint(f\"Output shape\
    \ ONNX OPT: {outputs_onnx_opt.shape}\")\nprint(f\"All elements close: {np.allclose(outputs_pt[0],\
    \ outputs_onnx_opt[0], rtol=1e-05, atol=1e-5)}\")\nprint(\"-\"*40 + \"\\n\")\n\
    \n# Sample elements of the hidden states\nprint(\"\\n\" + \"-\"*40)\nprint(f\"\
    {np.argmax(outputs_pt[0][0])=}\")\nprint(f\"{np.argmax(outputs_onnx_opt[0][0])=}\"\
    )\nprint(f\"{np.argmax(outputs_onnx[0][0])=}\")\nprint(\"-\"*40 + \"\\n\")\n```\n"
  created_at: 2024-01-12 01:52:10+00:00
  edited: true
  hidden: false
  id: 65a09b4a5fafc248c20fbfc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2024-01-12T20:20:08.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7856355905532837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: '<p>@HrayrMSint please wrap your code inside code fences in your comments</p>

          '
        raw: '@HrayrMSint please wrap your code inside code fences in your comments'
        updatedAt: '2024-01-12T20:20:08.031Z'
      numEdits: 0
      reactions: []
    id: 65a19ef83e3a00a9c184bf17
    type: comment
  author: julien-c
  content: '@HrayrMSint please wrap your code inside code fences in your comments'
  created_at: 2024-01-12 20:20:08+00:00
  edited: false
  hidden: false
  id: 65a19ef83e3a00a9c184bf17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04e841988d44db8b2024a57f588ceafb.svg
      fullname: Hrayr Matevosyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HrayrM
      type: user
    createdAt: '2024-01-14T23:29:28.000Z'
    data:
      edited: false
      editors:
      - HrayrM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8762074112892151
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04e841988d44db8b2024a57f588ceafb.svg
          fullname: Hrayr Matevosyan
          isHf: false
          isPro: false
          name: HrayrM
          type: user
        html: "<blockquote>\n<p>@HrayrMSint please wrap your code inside code fences\
          \ in your comments</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;julien-c&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/julien-c\"\
          >@<span class=\"underline\">julien-c</span></a></span>\n\n\t</span></span>\
          \ I have code-fenced and slightly reformatted the sample code in my previous\
          \ comment.</p>\n"
        raw: '> @HrayrMSint please wrap your code inside code fences in your comments


          @julien-c I have code-fenced and slightly reformatted the sample code in
          my previous comment.'
        updatedAt: '2024-01-14T23:29:28.699Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mfuntowicz
    id: 65a46e581051c2b0db440a69
    type: comment
  author: HrayrM
  content: '> @HrayrMSint please wrap your code inside code fences in your comments


    @julien-c I have code-fenced and slightly reformatted the sample code in my previous
    comment.'
  created_at: 2024-01-14 23:29:28+00:00
  edited: false
  hidden: false
  id: 65a46e581051c2b0db440a69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583858935715-5e67c47c100906368940747e.jpeg?w=200&h=200&f=face
      fullname: Morgan Funtowicz
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mfuntowicz
      type: user
    createdAt: '2024-01-15T13:55:48.000Z'
    data:
      edited: false
      editors:
      - mfuntowicz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9601349234580994
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583858935715-5e67c47c100906368940747e.jpeg?w=200&h=200&f=face
          fullname: Morgan Funtowicz
          isHf: true
          isPro: false
          name: mfuntowicz
          type: user
        html: "<p>Thanks a lot for the repro @HrayrMSint \U0001F64F -  we are going\
          \ to take a look and get back to you asap on this.</p>\n"
        raw: "Thanks a lot for the repro @HrayrMSint \U0001F64F -  we are going to\
          \ take a look and get back to you asap on this."
        updatedAt: '2024-01-15T13:55:48.279Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - julien-c
    id: 65a5396490b5e87bcd3e8cf0
    type: comment
  author: mfuntowicz
  content: "Thanks a lot for the repro @HrayrMSint \U0001F64F -  we are going to take\
    \ a look and get back to you asap on this."
  created_at: 2024-01-15 13:55:48+00:00
  edited: false
  hidden: false
  id: 65a5396490b5e87bcd3e8cf0
  type: comment
is_pull_request: true
merge_commit_oid: 716877d372b884cad6d419d828bac6c85b3b18d9
num: 3
repo_id: roberta-large
repo_type: model
status: merged
target_branch: refs/heads/main
title: Adding ONNX file of this model
