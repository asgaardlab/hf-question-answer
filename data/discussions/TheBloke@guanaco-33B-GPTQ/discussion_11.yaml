!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArbitrationCity
conflicting_files: null
created_at: 2023-06-26 12:13:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54334315bb84c613ce8a07eb64c80958.svg
      fullname: ARBI Assistant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArbitrationCity
      type: user
    createdAt: '2023-06-26T13:13:36.000Z'
    data:
      edited: false
      editors:
      - ArbitrationCity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9106549620628357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54334315bb84c613ce8a07eb64c80958.svg
          fullname: ARBI Assistant
          isHf: false
          isPro: false
          name: ArbitrationCity
          type: user
        html: "<p>Very impressive model and optimization! I am currently running it\
          \ locally (using text-generation-webui) on a single RTX 4090 and getting\
          \ well over 20 tokens per second, though it's quite variable dependent on\
          \ context length.   On par with ChatGPT as far as streaming is concerned,\
          \ and the concise responses are good (and you can get pretty decent long\
          \ ones too using the \"continue\" button)</p>\n<p>I do get CUDA out of memory\
          \ errors (on the server side) if the context is over a certain length, but\
          \ it does not crash and processes the next query processes just fine.  </p>\n\
          <p>Does anyone have suggestions on what further optimizations might be possible?\
          \  text-generation-webui uses CUDA 11.7, which I understand does not take\
          \ full advantage of the Ada Lovelace architecture, so I'd be curious what\
          \ kind of speedup might be possible with CUDA 11.8 or even 12.1 (which I\
          \ understand works with the latest Pytorch builds).\n </p>\n"
        raw: "Very impressive model and optimization! I am currently running it locally\
          \ (using text-generation-webui) on a single RTX 4090 and getting well over\
          \ 20 tokens per second, though it's quite variable dependent on context\
          \ length.   On par with ChatGPT as far as streaming is concerned, and the\
          \ concise responses are good (and you can get pretty decent long ones too\
          \ using the \"continue\" button)\r\n\r\nI do get CUDA out of memory errors\
          \ (on the server side) if the context is over a certain length, but it does\
          \ not crash and processes the next query processes just fine.  \r\n\r\n\
          Does anyone have suggestions on what further optimizations might be possible?\
          \  text-generation-webui uses CUDA 11.7, which I understand does not take\
          \ full advantage of the Ada Lovelace architecture, so I'd be curious what\
          \ kind of speedup might be possible with CUDA 11.8 or even 12.1 (which I\
          \ understand works with the latest Pytorch builds).\r\n "
        updatedAt: '2023-06-26T13:13:36.155Z'
      numEdits: 0
      reactions: []
    id: 64998f00fe6988f9397ab781
    type: comment
  author: ArbitrationCity
  content: "Very impressive model and optimization! I am currently running it locally\
    \ (using text-generation-webui) on a single RTX 4090 and getting well over 20\
    \ tokens per second, though it's quite variable dependent on context length. \
    \  On par with ChatGPT as far as streaming is concerned, and the concise responses\
    \ are good (and you can get pretty decent long ones too using the \"continue\"\
    \ button)\r\n\r\nI do get CUDA out of memory errors (on the server side) if the\
    \ context is over a certain length, but it does not crash and processes the next\
    \ query processes just fine.  \r\n\r\nDoes anyone have suggestions on what further\
    \ optimizations might be possible?  text-generation-webui uses CUDA 11.7, which\
    \ I understand does not take full advantage of the Ada Lovelace architecture,\
    \ so I'd be curious what kind of speedup might be possible with CUDA 11.8 or even\
    \ 12.1 (which I understand works with the latest Pytorch builds).\r\n "
  created_at: 2023-06-26 12:13:36+00:00
  edited: false
  hidden: false
  id: 64998f00fe6988f9397ab781
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/54334315bb84c613ce8a07eb64c80958.svg
      fullname: ARBI Assistant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArbitrationCity
      type: user
    createdAt: '2023-06-26T13:22:08.000Z'
    data:
      from: running this on a single RTX 4090
      to: running this on a single RTX 4090 - optimizations / CUDA
    id: 64999100810d4e41ebe4d878
    type: title-change
  author: ArbitrationCity
  created_at: 2023-06-26 12:22:08+00:00
  id: 64999100810d4e41ebe4d878
  new_title: running this on a single RTX 4090 - optimizations / CUDA
  old_title: running this on a single RTX 4090
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: open
target_branch: null
title: running this on a single RTX 4090 - optimizations / CUDA
