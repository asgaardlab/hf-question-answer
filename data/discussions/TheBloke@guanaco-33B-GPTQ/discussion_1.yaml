!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bodaay
conflicting_files: null
created_at: 2023-05-25 15:18:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bb7b26df47957e57e03d62c1383771f.svg
      fullname: khalefa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bodaay
      type: user
    createdAt: '2023-05-25T16:18:06.000Z'
    data:
      edited: false
      editors:
      - bodaay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bb7b26df47957e57e03d62c1383771f.svg
          fullname: khalefa
          isHf: false
          isPro: false
          name: bodaay
          type: user
        html: '<p>First</p>

          <p>You are awesome man, I keep refreshing you repo every hour to see whats
          new :)</p>

          <p>Are you going to release the 65B parameters version</p>

          <p>and I''m wondering, this new QLora, since I don''t really understand
          the difference, is it any better or more efficient? </p>

          '
        raw: "First\r\n\r\nYou are awesome man, I keep refreshing you repo every hour\
          \ to see whats new :)\r\n\r\nAre you going to release the 65B parameters\
          \ version\r\n\r\nand I'm wondering, this new QLora, since I don't really\
          \ understand the difference, is it any better or more efficient? "
        updatedAt: '2023-05-25T16:18:06.272Z'
      numEdits: 0
      reactions: []
    id: 646f8a3e5b8225e31ae522f3
    type: comment
  author: bodaay
  content: "First\r\n\r\nYou are awesome man, I keep refreshing you repo every hour\
    \ to see whats new :)\r\n\r\nAre you going to release the 65B parameters version\r\
    \n\r\nand I'm wondering, this new QLora, since I don't really understand the difference,\
    \ is it any better or more efficient? "
  created_at: 2023-05-25 15:18:06+00:00
  edited: false
  hidden: false
  id: 646f8a3e5b8225e31ae522f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-25T16:19:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks!  Glad you''re finding these models useful.</p>

          <p>Of course I''m going to release 65B :)  It''s processing now - will take
          a couple of hours at least though.  And I''ll do 7B as well when I''m back
          - going for dinner now.</p>

          '
        raw: 'Thanks!  Glad you''re finding these models useful.


          Of course I''m going to release 65B :)  It''s processing now - will take
          a couple of hours at least though.  And I''ll do 7B as well when I''m back
          - going for dinner now.'
        updatedAt: '2023-05-25T16:19:13.179Z'
      numEdits: 0
      reactions: []
    id: 646f8a816098ee820fbbffbb
    type: comment
  author: TheBloke
  content: 'Thanks!  Glad you''re finding these models useful.


    Of course I''m going to release 65B :)  It''s processing now - will take a couple
    of hours at least though.  And I''ll do 7B as well when I''m back - going for
    dinner now.'
  created_at: 2023-05-25 15:19:13+00:00
  edited: false
  hidden: false
  id: 646f8a816098ee820fbbffbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-25T16:21:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>re QLORA - the key advantage is that it makes it easier/cheaper
          to train, because it enables training on GPUs with a lot less VRAM.</p>

          <p>Basically it''s quantised training. You can now train in 4bit, which
          means instead of needing loads of big GPUs, like 4 x A100 80GB, you can
          now train on a single GPU.  Possibly the quality/accuracy will be slightly
          lower (I don''t know for sure), but it shouldn''t be that noticeable.</p>

          <p>So expect to see a lot more models coming out!</p>

          '
        raw: 're QLORA - the key advantage is that it makes it easier/cheaper to train,
          because it enables training on GPUs with a lot less VRAM.


          Basically it''s quantised training. You can now train in 4bit, which means
          instead of needing loads of big GPUs, like 4 x A100 80GB, you can now train
          on a single GPU.  Possibly the quality/accuracy will be slightly lower (I
          don''t know for sure), but it shouldn''t be that noticeable.


          So expect to see a lot more models coming out!'
        updatedAt: '2023-05-25T16:21:07.518Z'
      numEdits: 0
      reactions: []
    id: 646f8af3bc42f4b002305d46
    type: comment
  author: TheBloke
  content: 're QLORA - the key advantage is that it makes it easier/cheaper to train,
    because it enables training on GPUs with a lot less VRAM.


    Basically it''s quantised training. You can now train in 4bit, which means instead
    of needing loads of big GPUs, like 4 x A100 80GB, you can now train on a single
    GPU.  Possibly the quality/accuracy will be slightly lower (I don''t know for
    sure), but it shouldn''t be that noticeable.


    So expect to see a lot more models coming out!'
  created_at: 2023-05-25 15:21:07+00:00
  edited: false
  hidden: false
  id: 646f8af3bc42f4b002305d46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bb7b26df47957e57e03d62c1383771f.svg
      fullname: khalefa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bodaay
      type: user
    createdAt: '2023-05-25T17:06:25.000Z'
    data:
      edited: false
      editors:
      - bodaay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bb7b26df47957e57e03d62c1383771f.svg
          fullname: khalefa
          isHf: false
          isPro: false
          name: bodaay
          type: user
        html: '<p>Thanks for the clarification</p>

          '
        raw: Thanks for the clarification
        updatedAt: '2023-05-25T17:06:25.142Z'
      numEdits: 0
      reactions: []
    id: 646f9591bc42f4b0023126ae
    type: comment
  author: bodaay
  content: Thanks for the clarification
  created_at: 2023-05-25 16:06:25+00:00
  edited: false
  hidden: false
  id: 646f9591bc42f4b0023126ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-27T15:42:50.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> just a \"thank\
          \ you\" note for doing all the hard work releasing quantiized models before\
          \ I can even download the original models to try out most of the time. \
          \ From my testing so far with the original qLora version of this model,\
          \ it seems to be the best quality and most promising so far compared to\
          \ the other LLaMA and non-LLaMA-based models so far.  But the qLora inference\
          \ times were very slow.</p>\n<p>Again, \"thank you\" from an internet stranger\
          \ for your work.</p>\n<p>Edit: just loaded this model and the inference\
          \ speed is night and day compared to the qLora version!  We now have a viable\
          \ inference (GPTQ) AND fine-tuning option (qLora) for consumer-grade GPUs.</p>\n"
        raw: '@TheBloke just a "thank you" note for doing all the hard work releasing
          quantiized models before I can even download the original models to try
          out most of the time.  From my testing so far with the original qLora version
          of this model, it seems to be the best quality and most promising so far
          compared to the other LLaMA and non-LLaMA-based models so far.  But the
          qLora inference times were very slow.


          Again, "thank you" from an internet stranger for your work.


          Edit: just loaded this model and the inference speed is night and day compared
          to the qLora version!  We now have a viable inference (GPTQ) AND fine-tuning
          option (qLora) for consumer-grade GPUs.'
        updatedAt: '2023-05-27T16:12:26.080Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - hipozz
        - mikolodz
        - practical-dreamer
    id: 647224fac27f74a0eba48293
    type: comment
  author: LoneStriker
  content: '@TheBloke just a "thank you" note for doing all the hard work releasing
    quantiized models before I can even download the original models to try out most
    of the time.  From my testing so far with the original qLora version of this model,
    it seems to be the best quality and most promising so far compared to the other
    LLaMA and non-LLaMA-based models so far.  But the qLora inference times were very
    slow.


    Again, "thank you" from an internet stranger for your work.


    Edit: just loaded this model and the inference speed is night and day compared
    to the qLora version!  We now have a viable inference (GPTQ) AND fine-tuning option
    (qLora) for consumer-grade GPUs.'
  created_at: 2023-05-27 14:42:50+00:00
  edited: true
  hidden: false
  id: 647224fac27f74a0eba48293
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c94e5c5337335baf0770c10bbea3831e.svg
      fullname: hipozz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hipozz
      type: user
    createdAt: '2023-05-30T02:02:58.000Z'
    data:
      edited: false
      editors:
      - hipozz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c94e5c5337335baf0770c10bbea3831e.svg
          fullname: hipozz
          isHf: false
          isPro: false
          name: hipozz
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ just a \"thank you\" note for doing all the hard work releasing quantiized\
          \ models before I can even download the original models to try out most\
          \ of the time.  From my testing so far with the original qLora version of\
          \ this model, it seems to be the best quality and most promising so far\
          \ compared to the other LLaMA and non-LLaMA-based models so far.  But the\
          \ qLora inference times were very slow.</p>\n<p>Again, \"thank you\" from\
          \ an internet stranger for your work.</p>\n<p>Edit: just loaded this model\
          \ and the inference speed is night and day compared to the qLora version!\
          \  We now have a viable inference (GPTQ) AND fine-tuning option (qLora)\
          \ for consumer-grade GPUs.</p>\n</blockquote>\n<p>Thank you, I just wanted\
          \ to use qLora for inference, it seems that there is no need to try, continue\
          \ to use GPTQ. In addition, I use the dual card 3090 connected through nvlink\
          \ to run the guanaco-65B GPTQ model, and Its effect does not seem to be\
          \ as good as 33B.</p>\n"
        raw: "> @TheBloke just a \"thank you\" note for doing all the hard work releasing\
          \ quantiized models before I can even download the original models to try\
          \ out most of the time.  From my testing so far with the original qLora\
          \ version of this model, it seems to be the best quality and most promising\
          \ so far compared to the other LLaMA and non-LLaMA-based models so far.\
          \  But the qLora inference times were very slow.\n> \n> Again, \"thank you\"\
          \ from an internet stranger for your work.\n> \n> Edit: just loaded this\
          \ model and the inference speed is night and day compared to the qLora version!\
          \  We now have a viable inference (GPTQ) AND fine-tuning option (qLora)\
          \ for consumer-grade GPUs.\n\nThank you, I just wanted to use qLora for\
          \ inference, it seems that there is no need to try, continue to use GPTQ.\
          \ In addition, I use the dual card 3090 connected through nvlink to run\
          \ the guanaco-65B GPTQ model, and Its effect does not seem to be as good\
          \ as 33B."
        updatedAt: '2023-05-30T02:02:58.108Z'
      numEdits: 0
      reactions: []
    id: 64755952f9e3e0b312f88ff9
    type: comment
  author: hipozz
  content: "> @TheBloke just a \"thank you\" note for doing all the hard work releasing\
    \ quantiized models before I can even download the original models to try out\
    \ most of the time.  From my testing so far with the original qLora version of\
    \ this model, it seems to be the best quality and most promising so far compared\
    \ to the other LLaMA and non-LLaMA-based models so far.  But the qLora inference\
    \ times were very slow.\n> \n> Again, \"thank you\" from an internet stranger\
    \ for your work.\n> \n> Edit: just loaded this model and the inference speed is\
    \ night and day compared to the qLora version!  We now have a viable inference\
    \ (GPTQ) AND fine-tuning option (qLora) for consumer-grade GPUs.\n\nThank you,\
    \ I just wanted to use qLora for inference, it seems that there is no need to\
    \ try, continue to use GPTQ. In addition, I use the dual card 3090 connected through\
    \ nvlink to run the guanaco-65B GPTQ model, and Its effect does not seem to be\
    \ as good as 33B."
  created_at: 2023-05-30 01:02:58+00:00
  edited: false
  hidden: false
  id: 64755952f9e3e0b312f88ff9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-06-14T01:09:38.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9550734758377075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hi\">@<span class=\"\
          underline\">hi</span></a></span>\n\n\t</span></span></p>\n<blockquote>\n\
          <blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ just a \"thank you\" note for doing all the hard work releasing quantiized\
          \ models before I can even download the original models to try out most\
          \ of the time.  From my testing so far with the original qLora version of\
          \ this model, it seems to be the best quality and most promising so far\
          \ compared to the other LLaMA and non-LLaMA-based models so far.  But the\
          \ qLora inference times were very slow.</p>\n<p>Again, \"thank you\" from\
          \ an internet stranger for your work.</p>\n<p>Edit: just loaded this model\
          \ and the inference speed is night and day compared to the qLora version!\
          \  We now have a viable inference (GPTQ) AND fine-tuning option (qLora)\
          \ for consumer-grade GPUs.</p>\n</blockquote>\n<p>Thank you, I just wanted\
          \ to use qLora for inference, it seems that there is no need to try, continue\
          \ to use GPTQ. In addition, I use the dual card 3090 connected through nvlink\
          \ to run the guanaco-65B GPTQ model, and Its effect does not seem to be\
          \ as good as 33B.</p>\n</blockquote>\n<p>So 33B almost as good as 65B?</p>\n"
        raw: "@hi\n> > @TheBloke just a \"thank you\" note for doing all the hard\
          \ work releasing quantiized models before I can even download the original\
          \ models to try out most of the time.  From my testing so far with the original\
          \ qLora version of this model, it seems to be the best quality and most\
          \ promising so far compared to the other LLaMA and non-LLaMA-based models\
          \ so far.  But the qLora inference times were very slow.\n> > \n> > Again,\
          \ \"thank you\" from an internet stranger for your work.\n> > \n> > Edit:\
          \ just loaded this model and the inference speed is night and day compared\
          \ to the qLora version!  We now have a viable inference (GPTQ) AND fine-tuning\
          \ option (qLora) for consumer-grade GPUs.\n> \n> Thank you, I just wanted\
          \ to use qLora for inference, it seems that there is no need to try, continue\
          \ to use GPTQ. In addition, I use the dual card 3090 connected through nvlink\
          \ to run the guanaco-65B GPTQ model, and Its effect does not seem to be\
          \ as good as 33B.\n\nSo 33B almost as good as 65B?"
        updatedAt: '2023-06-14T01:09:38.004Z'
      numEdits: 0
      reactions: []
    id: 648913525ff1e85a60384650
    type: comment
  author: zokica
  content: "@hi\n> > @TheBloke just a \"thank you\" note for doing all the hard work\
    \ releasing quantiized models before I can even download the original models to\
    \ try out most of the time.  From my testing so far with the original qLora version\
    \ of this model, it seems to be the best quality and most promising so far compared\
    \ to the other LLaMA and non-LLaMA-based models so far.  But the qLora inference\
    \ times were very slow.\n> > \n> > Again, \"thank you\" from an internet stranger\
    \ for your work.\n> > \n> > Edit: just loaded this model and the inference speed\
    \ is night and day compared to the qLora version!  We now have a viable inference\
    \ (GPTQ) AND fine-tuning option (qLora) for consumer-grade GPUs.\n> \n> Thank\
    \ you, I just wanted to use qLora for inference, it seems that there is no need\
    \ to try, continue to use GPTQ. In addition, I use the dual card 3090 connected\
    \ through nvlink to run the guanaco-65B GPTQ model, and Its effect does not seem\
    \ to be as good as 33B.\n\nSo 33B almost as good as 65B?"
  created_at: 2023-06-14 00:09:38+00:00
  edited: false
  hidden: false
  id: 648913525ff1e85a60384650
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: open
target_branch: null
title: guanaco-65b
