!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boffy
conflicting_files: null
created_at: 2023-05-26 10:21:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-05-26T11:21:53.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>4090 so I don''t get the memory issue.. what am I missing here?</p>

          <p>INFO:Loading guanaco-33B-GPTQ...<br>WARNING:Auto-assiging --gpu-memory
          23 for your GPU to try to prevent out-of-memory errors. You can manually
          set other values.<br>Traceback (most recent call last):<br>  File "C:\Projects\AI\text-generation-webui\server.py",
          line 1087, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\Projects\AI\text-generation-webui\modules\models.py", line 95, in load_model<br>    output
          = load_func(model_name)<br>  File "C:\Projects\AI\text-generation-webui\modules\models.py",
          line 223, in huggingface_loader<br>    model = LoaderClass.from_pretrained(checkpoint,
          **params)<br>  File "C:\Projects\AI\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "C:\Projects\AI\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2405, in from_pretrained<br>    raise EnvironmentError(<br>OSError:
          Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or
          flax_model.msgpack found in directory models\guanaco-33B-GPTQ.</p>

          '
        raw: "4090 so I don't get the memory issue.. what am I missing here?\r\n\r\
          \nINFO:Loading guanaco-33B-GPTQ...\r\nWARNING:Auto-assiging --gpu-memory\
          \ 23 for your GPU to try to prevent out-of-memory errors. You can manually\
          \ set other values.\r\nTraceback (most recent call last):\r\n  File \"C:\\\
          Projects\\AI\\text-generation-webui\\server.py\", line 1087, in <module>\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\n\
          \  File \"C:\\Projects\\AI\\text-generation-webui\\modules\\models.py\"\
          , line 95, in load_model\r\n    output = load_func(model_name)\r\n  File\
          \ \"C:\\Projects\\AI\\text-generation-webui\\modules\\models.py\", line\
          \ 223, in huggingface_loader\r\n    model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\r\n  File \"C:\\Projects\\AI\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\r\
          \n    return model_class.from_pretrained(\r\n  File \"C:\\Projects\\AI\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2405, in from_pretrained\r\n    raise EnvironmentError(\r\nOSError:\
          \ Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or\
          \ flax_model.msgpack found in directory models\\guanaco-33B-GPTQ."
        updatedAt: '2023-05-26T11:21:53.045Z'
      numEdits: 0
      reactions: []
    id: 64709651cfd57849518e5d23
    type: comment
  author: Boffy
  content: "4090 so I don't get the memory issue.. what am I missing here?\r\n\r\n\
    INFO:Loading guanaco-33B-GPTQ...\r\nWARNING:Auto-assiging --gpu-memory 23 for\
    \ your GPU to try to prevent out-of-memory errors. You can manually set other\
    \ values.\r\nTraceback (most recent call last):\r\n  File \"C:\\Projects\\AI\\\
    text-generation-webui\\server.py\", line 1087, in <module>\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\Projects\\\
    AI\\text-generation-webui\\modules\\models.py\", line 95, in load_model\r\n  \
    \  output = load_func(model_name)\r\n  File \"C:\\Projects\\AI\\text-generation-webui\\\
    modules\\models.py\", line 223, in huggingface_loader\r\n    model = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\r\n  File \"C:\\Projects\\AI\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\r\n\
    \    return model_class.from_pretrained(\r\n  File \"C:\\Projects\\AI\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2405, in from_pretrained\r\
    \n    raise EnvironmentError(\r\nOSError: Error no file named pytorch_model.bin,\
    \ tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory models\\\
    guanaco-33B-GPTQ."
  created_at: 2023-05-26 10:21:53+00:00
  edited: false
  hidden: false
  id: 64709651cfd57849518e5d23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-26T11:26:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to set the GPTQ parameters - please see the instructions
          in the README</p>

          '
        raw: You need to set the GPTQ parameters - please see the instructions in
          the README
        updatedAt: '2023-05-26T11:26:19.922Z'
      numEdits: 0
      reactions: []
    id: 6470975b3df93fddece0d295
    type: comment
  author: TheBloke
  content: You need to set the GPTQ parameters - please see the instructions in the
    README
  created_at: 2023-05-26 10:26:19+00:00
  edited: false
  hidden: false
  id: 6470975b3df93fddece0d295
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-05-26T11:41:04.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: "<p>I tried that.. it doesn't like it  the error below ...the other\
          \ model '<a href=\"https://huggingface.co/MetaIX/Guanaco-33B-4bit\">Guanaco-33B-4bit'</a>\
          \  works fine so will use that.. not sure on the performance difference?</p>\n\
          <p>Traceback (most recent call last): File \u201CC:\\Projects\\AI\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\text-generation-webui\\\
          modules\\models.py\u201D, line 95, in load_model output = load_func(model_name)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\models.py\u201D\
          , line 289, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 84, in _load_quant model.load_state_dict(safe_load(checkpoint), strict=False)\
          \ File \u201CC:\\Projects\\AI\\installer_files\\env\\lib\\site-packages\\\
          safetensors\\torch.py\u201D, line 261, in load_file result[k] = f.get_tensor(k)\
          \ RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 425984000 bytes.</p>\n"
        raw: "I tried that.. it doesn't like it  the error below ...the other model\
          \ '[Guanaco-33B-4bit'](https://huggingface.co/MetaIX/Guanaco-33B-4bit) \
          \ works fine so will use that.. not sure on the performance difference?\n\
          \nTraceback (most recent call last): File \u201CC:\\Projects\\AI\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\text-generation-webui\\\
          modules\\models.py\u201D, line 95, in load_model output = load_func(model_name)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\models.py\u201D\
          , line 289, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 84, in _load_quant model.load_state_dict(safe_load(checkpoint), strict=False)\
          \ File \u201CC:\\Projects\\AI\\installer_files\\env\\lib\\site-packages\\\
          safetensors\\torch.py\u201D, line 261, in load_file result[k] = f.get_tensor(k)\
          \ RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 425984000 bytes."
        updatedAt: '2023-05-26T11:41:04.806Z'
      numEdits: 0
      reactions: []
    id: 64709ad0806c7d87fa15ad87
    type: comment
  author: Boffy
  content: "I tried that.. it doesn't like it  the error below ...the other model\
    \ '[Guanaco-33B-4bit'](https://huggingface.co/MetaIX/Guanaco-33B-4bit)  works\
    \ fine so will use that.. not sure on the performance difference?\n\nTraceback\
    \ (most recent call last): File \u201CC:\\Projects\\AI\\text-generation-webui\\\
    server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
    \ = load_model(shared.model_name) File \u201CC:\\Projects\\AI\\text-generation-webui\\\
    modules\\models.py\u201D, line 95, in load_model output = load_func(model_name)\
    \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\models.py\u201D\
    , line 289, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
    \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CC:\\Projects\\AI\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 84, in _load_quant model.load_state_dict(safe_load(checkpoint), strict=False)\
    \ File \u201CC:\\Projects\\AI\\installer_files\\env\\lib\\site-packages\\safetensors\\\
    torch.py\u201D, line 261, in load_file result[k] = f.get_tensor(k) RuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 425984000\
    \ bytes."
  created_at: 2023-05-26 10:41:04+00:00
  edited: false
  hidden: false
  id: 64709ad0806c7d87fa15ad87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-05-26T11:42:52.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>never mind.. spoke to soon.. restarted it and it loaded fine</p>

          '
        raw: never mind.. spoke to soon.. restarted it and it loaded fine
        updatedAt: '2023-05-26T11:42:52.103Z'
      numEdits: 0
      reactions: []
    id: 64709b3c1f0e7ee7fb19af14
    type: comment
  author: Boffy
  content: never mind.. spoke to soon.. restarted it and it loaded fine
  created_at: 2023-05-26 10:42:52+00:00
  edited: false
  hidden: false
  id: 64709b3c1f0e7ee7fb19af14
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: open
target_branch: null
title: Error no file named pytorch_model.bin
