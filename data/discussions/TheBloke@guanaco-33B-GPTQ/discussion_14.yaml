!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ezhuwork
conflicting_files: null
created_at: 2023-07-07 06:32:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5986bdfa05aaecf9416c7b819b7e7337.svg
      fullname: EZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ezhuwork
      type: user
    createdAt: '2023-07-07T07:32:06.000Z'
    data:
      edited: false
      editors:
      - ezhuwork
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33013293147087097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5986bdfa05aaecf9416c7b819b7e7337.svg
          fullname: EZ
          isHf: false
          isPro: false
          name: ezhuwork
          type: user
        html: "<p>Traceback (most recent call last): File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 17, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019</p>\n<p>During handling of the\
          \ above exception, another exception occurred:</p>\n<p>Traceback (most recent\
          \ call last): File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
          , line 74, in load_model output = load_func_maploader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
          , line 278, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 21, in sys.exit(-1) SystemExit: -1</p>\n"
        raw: "Traceback (most recent call last): File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 17, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019\r\n\r\nDuring handling of the\
          \ above exception, another exception occurred:\r\n\r\nTraceback (most recent\
          \ call last): File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
          , line 74, in load_model output = load_func_maploader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
          , line 278, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 21, in sys.exit(-1) SystemExit: -1"
        updatedAt: '2023-07-07T07:32:06.205Z'
      numEdits: 0
      reactions: []
    id: 64a7bf764baad3b219fe93a2
    type: comment
  author: ezhuwork
  content: "Traceback (most recent call last): File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 17, in import llama_inference_offload ModuleNotFoundError: No module named\
    \ \u2018llama_inference_offload\u2019\r\n\r\nDuring handling of the above exception,\
    \ another exception occurred:\r\n\r\nTraceback (most recent call last): File \u201C\
    /Users/ezhu/AI/oobabooga_macos/text-generation-webui/server.py\u201D, line 68,\
    \ in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
    , line 74, in load_model output = load_func_maploader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/models.py\u201D\
    , line 278, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/ezhu/AI/oobabooga_macos/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 21, in sys.exit(-1) SystemExit: -1"
  created_at: 2023-07-07 06:32:06+00:00
  edited: false
  hidden: false
  id: 64a7bf764baad3b219fe93a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/5986bdfa05aaecf9416c7b819b7e7337.svg
      fullname: EZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ezhuwork
      type: user
    createdAt: '2023-07-07T07:33:13.000Z'
    data:
      from: get this error, is it cus it needs dedicated GPU, and im running on mac
        air m2?
      to: get this error, running on mac book air m2
    id: 64a7bfb944387a55047e387b
    type: title-change
  author: ezhuwork
  created_at: 2023-07-07 06:33:13+00:00
  id: 64a7bfb944387a55047e387b
  new_title: get this error, running on mac book air m2
  old_title: get this error, is it cus it needs dedicated GPU, and im running on mac
    air m2?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5986bdfa05aaecf9416c7b819b7e7337.svg
      fullname: EZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ezhuwork
      type: user
    createdAt: '2023-07-07T07:57:27.000Z'
    data:
      status: closed
    id: 64a7c5679281998a18cd49cd
    type: status-change
  author: ezhuwork
  created_at: 2023-07-07 06:57:27+00:00
  id: 64a7c5679281998a18cd49cd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5986bdfa05aaecf9416c7b819b7e7337.svg
      fullname: EZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ezhuwork
      type: user
    createdAt: '2023-07-07T07:57:36.000Z'
    data:
      status: open
    id: 64a7c57087cbd4dc7304da2b
    type: status-change
  author: ezhuwork
  created_at: 2023-07-07 06:57:36+00:00
  id: 64a7c57087cbd4dc7304da2b
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:08:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8871122002601624
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GPTQ models aren''t properly supported on macOS.  The one-click-installer
          won''t install any GPTQ library, which is why you''re getting this error.  You
          could install it manually but there''s no GPU acceleration so it will be
          really slow.</p>

          <p>On macOS please use GGML instead.  To get GPU acceleration you''ll need
          to manually compile llama-cpp-python with Metal support <a rel="nofollow"
          href="https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal">https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal</a></p>

          <p>Or much easier is to use LM Studio instead, which has full GPU acceleration
          on macOS and supports all GGML models: <a rel="nofollow" href="https://lmstudio.ai/">https://lmstudio.ai/</a></p>

          '
        raw: 'GPTQ models aren''t properly supported on macOS.  The one-click-installer
          won''t install any GPTQ library, which is why you''re getting this error.  You
          could install it manually but there''s no GPU acceleration so it will be
          really slow.


          On macOS please use GGML instead.  To get GPU acceleration you''ll need
          to manually compile llama-cpp-python with Metal support https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal


          Or much easier is to use LM Studio instead, which has full GPU acceleration
          on macOS and supports all GGML models: https://lmstudio.ai/'
        updatedAt: '2023-07-07T09:08:12.231Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - davide221
        - eusip
    id: 64a7d5fce2ffcdc1f134d434
    type: comment
  author: TheBloke
  content: 'GPTQ models aren''t properly supported on macOS.  The one-click-installer
    won''t install any GPTQ library, which is why you''re getting this error.  You
    could install it manually but there''s no GPU acceleration so it will be really
    slow.


    On macOS please use GGML instead.  To get GPU acceleration you''ll need to manually
    compile llama-cpp-python with Metal support https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal


    Or much easier is to use LM Studio instead, which has full GPU acceleration on
    macOS and supports all GGML models: https://lmstudio.ai/'
  created_at: 2023-07-07 08:08:12+00:00
  edited: false
  hidden: false
  id: 64a7d5fce2ffcdc1f134d434
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: open
target_branch: null
title: get this error, running on mac book air m2
