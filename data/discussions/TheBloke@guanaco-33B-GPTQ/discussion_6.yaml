!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Exterminant
conflicting_files: null
created_at: 2023-05-27 21:03:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646d2d8bda8e99940b70405d/D2iVsLVm-xOzjg_kHxq9i.png?w=200&h=200&f=face
      fullname: Konstantin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Exterminant
      type: user
    createdAt: '2023-05-27T22:03:14.000Z'
    data:
      edited: false
      editors:
      - Exterminant
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646d2d8bda8e99940b70405d/D2iVsLVm-xOzjg_kHxq9i.png?w=200&h=200&f=face
          fullname: Konstantin
          isHf: false
          isPro: false
          name: Exterminant
          type: user
        html: "<p>Can't load this model into my 3090, wonder if someone managed how\
          \ to do this? 13B models work well.<br>Tried to play with pre-layers params,\
          \ none of them are working<br>When try to load the model, get this error:<br><code>Traceback\
          \ (most recent call last): File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 68, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Users\\\
          konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 85, in load_model output = load_func(model_name) File\
          \ \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 259, in GPTQ_loader model = load_quantized(model_name)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 175, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 236,\
          \ in load_quant model.load_state_dict(safe_load(checkpoint)) File \u201C\
          C:\\Users\\konstantin\\Desktop\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
          \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
          .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\
          \ size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
          \ with shape torch.Size([1, 832]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for</code></p>\n"
        raw: "Can't load this model into my 3090, wonder if someone managed how to\
          \ do this? 13B models work well. \r\nTried to play with pre-layers params,\
          \ none of them are working\r\nWhen try to load the model, get this error:\r\
          \n```Traceback (most recent call last): File \u201CC:\\Users\\konstantin\\\
          Desktop\\oobabooga_windows\\text-generation-webui\\server.py\u201D, line\
          \ 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 85, in load_model output = load_func(model_name)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 259, in GPTQ_loader model = load_quantized(model_name)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 175, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\
          \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 236,\
          \ in load_quant model.load_state_dict(safe_load(checkpoint)) File \u201C\
          C:\\Users\\konstantin\\Desktop\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
          \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
          .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\
          \ size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
          \ with shape torch.Size([1, 832]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for```"
        updatedAt: '2023-05-27T22:03:14.027Z'
      numEdits: 0
      reactions: []
    id: 64727e22a2b0a376b8ba4364
    type: comment
  author: Exterminant
  content: "Can't load this model into my 3090, wonder if someone managed how to do\
    \ this? 13B models work well. \r\nTried to play with pre-layers params, none of\
    \ them are working\r\nWhen try to load the model, get this error:\r\n```Traceback\
    \ (most recent call last): File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\\
    text-generation-webui\\server.py\u201D, line 68, in load_model_wrapper shared.model,\
    \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Users\\konstantin\\\
    Desktop\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 85, in load_model output = load_func(model_name) File \u201CC:\\Users\\konstantin\\\
    Desktop\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 259, in GPTQ_loader model = load_quantized(model_name) File \u201CC:\\Users\\\
    konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 175, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, shared.args.pre_layer) File \u201C\
    C:\\Users\\konstantin\\Desktop\\oobabooga_windows\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\llama_inference_offload.py\u201D, line 236, in load_quant model.load_state_dict(safe_load(checkpoint))\
    \ File \u201CC:\\Users\\konstantin\\Desktop\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
    \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
    .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([1, 832]) from checkpoint, the shape in current model is torch.Size([52,\
    \ 832]). size mismatch for model.layers.0.self_attn.k_proj.scales: copying a param\
    \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current model\
    \ is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for```"
  created_at: 2023-05-27 21:03:14+00:00
  edited: false
  hidden: false
  id: 64727e22a2b0a376b8ba4364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e50c17154d65d0ebb5863a3f03a4c784.svg
      fullname: Jorge Hernandez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: heyjorge
      type: user
    createdAt: '2023-05-27T22:10:17.000Z'
    data:
      edited: true
      editors:
      - heyjorge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e50c17154d65d0ebb5863a3f03a4c784.svg
          fullname: Jorge Hernandez
          isHf: false
          isPro: false
          name: heyjorge
          type: user
        html: '<p>edit: After re-reading your error, double check to make sure you
          actually fully downloaded the model by checking the file size on your disk
          vs what is in huggingface. I think I got a similar error initially because
          the model didn''t fully download in one shot.</p>

          <p>I''m not getting this error, but also having trouble loading this but
          in my 4090. I have GPTQ wbits set to 4, none for group, llama for model
          type, pre-layers doesn''t seem to do much. What seems to happen is it tries
          to load the model fully in RAM and is not loading it into the 4090 VRAM
          at all. Am I doing something wrong? It essentially runs out of memory then
          says "press any key to continue" without any other messages. This is my
          first time trying to load a GPTQ model as well, I figured using wbit 4 would
          allow plenty of memory space to load the 33B model.</p>

          '
        raw: 'edit: After re-reading your error, double check to make sure you actually
          fully downloaded the model by checking the file size on your disk vs what
          is in huggingface. I think I got a similar error initially because the model
          didn''t fully download in one shot.


          I''m not getting this error, but also having trouble loading this but in
          my 4090. I have GPTQ wbits set to 4, none for group, llama for model type,
          pre-layers doesn''t seem to do much. What seems to happen is it tries to
          load the model fully in RAM and is not loading it into the 4090 VRAM at
          all. Am I doing something wrong? It essentially runs out of memory then
          says "press any key to continue" without any other messages. This is my
          first time trying to load a GPTQ model as well, I figured using wbit 4 would
          allow plenty of memory space to load the 33B model.'
        updatedAt: '2023-05-27T22:11:59.599Z'
      numEdits: 2
      reactions: []
    id: 64727fc997a75cc77ab94265
    type: comment
  author: heyjorge
  content: 'edit: After re-reading your error, double check to make sure you actually
    fully downloaded the model by checking the file size on your disk vs what is in
    huggingface. I think I got a similar error initially because the model didn''t
    fully download in one shot.


    I''m not getting this error, but also having trouble loading this but in my 4090.
    I have GPTQ wbits set to 4, none for group, llama for model type, pre-layers doesn''t
    seem to do much. What seems to happen is it tries to load the model fully in RAM
    and is not loading it into the 4090 VRAM at all. Am I doing something wrong? It
    essentially runs out of memory then says "press any key to continue" without any
    other messages. This is my first time trying to load a GPTQ model as well, I figured
    using wbit 4 would allow plenty of memory space to load the 33B model.'
  created_at: 2023-05-27 21:10:17+00:00
  edited: true
  hidden: false
  id: 64727fc997a75cc77ab94265
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T22:10:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it can be loaded. That''s an error with GPTQ parameters</p>

          <p>Firstly, double check that the GPTQ parameters are set and saved for
          this model:</p>

          <ul>

          <li>bits = 4</li>

          <li>group_size = None</li>

          <li>model_type = Llama</li>

          </ul>

          <p>If they are, then you might be hitting a text-generation-webui bug.  In
          that case please edit <code>models/config-user.yaml</code> and find the
          entry for <code>TheBloke_guanaco-33B-GPTQ</code> and see if <code>groupsize</code>
          is set to 128. If it is, set it to <code>-1</code> then save the file and
          re-load text-generation-webui</p>

          <p>Try that and report back</p>

          '
        raw: 'Yeah it can be loaded. That''s an error with GPTQ parameters


          Firstly, double check that the GPTQ parameters are set and saved for this
          model:

          - bits = 4

          - group_size = None

          - model_type = Llama


          If they are, then you might be hitting a text-generation-webui bug.  In
          that case please edit `models/config-user.yaml` and find the entry for `TheBloke_guanaco-33B-GPTQ`
          and see if `groupsize` is set to 128. If it is, set it to `-1` then save
          the file and re-load text-generation-webui


          Try that and report back'
        updatedAt: '2023-05-27T22:10:24.667Z'
      numEdits: 0
      reactions: []
    id: 64727fd097a75cc77ab942c0
    type: comment
  author: TheBloke
  content: 'Yeah it can be loaded. That''s an error with GPTQ parameters


    Firstly, double check that the GPTQ parameters are set and saved for this model:

    - bits = 4

    - group_size = None

    - model_type = Llama


    If they are, then you might be hitting a text-generation-webui bug.  In that case
    please edit `models/config-user.yaml` and find the entry for `TheBloke_guanaco-33B-GPTQ`
    and see if `groupsize` is set to 128. If it is, set it to `-1` then save the file
    and re-load text-generation-webui


    Try that and report back'
  created_at: 2023-05-27 21:10:24+00:00
  edited: false
  hidden: false
  id: 64727fd097a75cc77ab942c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T22:19:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>edit: After re-reading your error, double check to make sure you actually
          fully downloaded the model by checking the file size on your disk vs what
          is in huggingface. I think I got a similar error initially because the model
          didn''t fully download in one shot.</p>

          <p>I''m not getting this error, but also having trouble loading this but
          in my 4090. I have GPTQ wbits set to 4, none for group, llama for model
          type, pre-layers doesn''t seem to do much. What seems to happen is it tries
          to load the model fully in RAM and is not loading it into the 4090 VRAM
          at all. Am I doing something wrong? It essentially runs out of memory then
          says "press any key to continue" without any other messages. This is my
          first time trying to load a GPTQ model as well, I figured using wbit 4 would
          allow plenty of memory space to load the 33B model.</p>

          </blockquote>

          <p>Yeah this is a common problem. I''m not sure why it happens exactly,
          but basically you are right - it needs to load it fully into RAM first,
          then it moves to VRAM.  It seems to be exclusive to Windows, and if you
          have plenty of RAM, you may still get this issue.</p>

          <p>Fortunately the solution is simple: increase your Windows pagefile size,
          eg to around 90GB.  This has solved the problem for others who have reported
          it.</p>

          '
        raw: "> edit: After re-reading your error, double check to make sure you actually\
          \ fully downloaded the model by checking the file size on your disk vs what\
          \ is in huggingface. I think I got a similar error initially because the\
          \ model didn't fully download in one shot.\n> \n> I'm not getting this error,\
          \ but also having trouble loading this but in my 4090. I have GPTQ wbits\
          \ set to 4, none for group, llama for model type, pre-layers doesn't seem\
          \ to do much. What seems to happen is it tries to load the model fully in\
          \ RAM and is not loading it into the 4090 VRAM at all. Am I doing something\
          \ wrong? It essentially runs out of memory then says \"press any key to\
          \ continue\" without any other messages. This is my first time trying to\
          \ load a GPTQ model as well, I figured using wbit 4 would allow plenty of\
          \ memory space to load the 33B model.\n\nYeah this is a common problem.\
          \ I'm not sure why it happens exactly, but basically you are right - it\
          \ needs to load it fully into RAM first, then it moves to VRAM.  It seems\
          \ to be exclusive to Windows, and if you have plenty of RAM, you may still\
          \ get this issue.\n\nFortunately the solution is simple: increase your Windows\
          \ pagefile size, eg to around 90GB.  This has solved the problem for others\
          \ who have reported it."
        updatedAt: '2023-05-27T22:19:51.010Z'
      numEdits: 0
      reactions: []
    id: 6472820797a75cc77ab96255
    type: comment
  author: TheBloke
  content: "> edit: After re-reading your error, double check to make sure you actually\
    \ fully downloaded the model by checking the file size on your disk vs what is\
    \ in huggingface. I think I got a similar error initially because the model didn't\
    \ fully download in one shot.\n> \n> I'm not getting this error, but also having\
    \ trouble loading this but in my 4090. I have GPTQ wbits set to 4, none for group,\
    \ llama for model type, pre-layers doesn't seem to do much. What seems to happen\
    \ is it tries to load the model fully in RAM and is not loading it into the 4090\
    \ VRAM at all. Am I doing something wrong? It essentially runs out of memory then\
    \ says \"press any key to continue\" without any other messages. This is my first\
    \ time trying to load a GPTQ model as well, I figured using wbit 4 would allow\
    \ plenty of memory space to load the 33B model.\n\nYeah this is a common problem.\
    \ I'm not sure why it happens exactly, but basically you are right - it needs\
    \ to load it fully into RAM first, then it moves to VRAM.  It seems to be exclusive\
    \ to Windows, and if you have plenty of RAM, you may still get this issue.\n\n\
    Fortunately the solution is simple: increase your Windows pagefile size, eg to\
    \ around 90GB.  This has solved the problem for others who have reported it."
  created_at: 2023-05-27 21:19:51+00:00
  edited: false
  hidden: false
  id: 6472820797a75cc77ab96255
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e50c17154d65d0ebb5863a3f03a4c784.svg
      fullname: Jorge Hernandez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: heyjorge
      type: user
    createdAt: '2023-05-27T22:20:37.000Z'
    data:
      edited: false
      editors:
      - heyjorge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e50c17154d65d0ebb5863a3f03a4c784.svg
          fullname: Jorge Hernandez
          isHf: false
          isPro: false
          name: heyjorge
          type: user
        html: '<p>That did the trick for my issue - it was set to ''None'' but explicitly
          setting it to -1 allowed it to eventually offload onto the GPU VRAM. Thanks!</p>

          '
        raw: That did the trick for my issue - it was set to 'None' but explicitly
          setting it to -1 allowed it to eventually offload onto the GPU VRAM. Thanks!
        updatedAt: '2023-05-27T22:20:37.705Z'
      numEdits: 0
      reactions: []
    id: 64728235c27f74a0ebac2f48
    type: comment
  author: heyjorge
  content: That did the trick for my issue - it was set to 'None' but explicitly setting
    it to -1 allowed it to eventually offload onto the GPU VRAM. Thanks!
  created_at: 2023-05-27 21:20:37+00:00
  edited: false
  hidden: false
  id: 64728235c27f74a0ebac2f48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646d2d8bda8e99940b70405d/D2iVsLVm-xOzjg_kHxq9i.png?w=200&h=200&f=face
      fullname: Konstantin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Exterminant
      type: user
    createdAt: '2023-05-27T22:20:54.000Z'
    data:
      edited: false
      editors:
      - Exterminant
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646d2d8bda8e99940b70405d/D2iVsLVm-xOzjg_kHxq9i.png?w=200&h=200&f=face
          fullname: Konstantin
          isHf: false
          isPro: false
          name: Exterminant
          type: user
        html: '<p>Thank you so much TheBloke, I did as you said and it now works for
          me!</p>

          <p>Here''s my config-user.yaml for this model, if someone will need it:<br>TheBloke_guanaco-33B-GPTQ$:<br>  auto_devices:
          false<br>  bf16: false<br>  cpu: false<br>  cpu_memory: 0<br>  disk: false<br>  gpu_memory_0:
          0<br>  groupsize: 128<br>  load_in_8bit: false<br>  mlock: false<br>  model_type:
          llama<br>  n_batch: 512<br>  n_gpu_layers: 0<br>  no_mmap: false<br>  pre_layer:
          0<br>  threads: 0<br>  wbits: 4</p>

          '
        raw: "Thank you so much TheBloke, I did as you said and it now works for me!\n\
          \nHere's my config-user.yaml for this model, if someone will need it:\n\
          TheBloke_guanaco-33B-GPTQ$:\n  auto_devices: false\n  bf16: false\n  cpu:\
          \ false\n  cpu_memory: 0\n  disk: false\n  gpu_memory_0: 0\n  groupsize:\
          \ 128\n  load_in_8bit: false\n  mlock: false\n  model_type: llama\n  n_batch:\
          \ 512\n  n_gpu_layers: 0\n  no_mmap: false\n  pre_layer: 0\n  threads: 0\n\
          \  wbits: 4"
        updatedAt: '2023-05-27T22:20:54.788Z'
      numEdits: 0
      reactions: []
    id: 6472824697a75cc77ab96576
    type: comment
  author: Exterminant
  content: "Thank you so much TheBloke, I did as you said and it now works for me!\n\
    \nHere's my config-user.yaml for this model, if someone will need it:\nTheBloke_guanaco-33B-GPTQ$:\n\
    \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk:\
    \ false\n  gpu_memory_0: 0\n  groupsize: 128\n  load_in_8bit: false\n  mlock:\
    \ false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n  no_mmap: false\n\
    \  pre_layer: 0\n  threads: 0\n  wbits: 4"
  created_at: 2023-05-27 21:20:54+00:00
  edited: false
  hidden: false
  id: 6472824697a75cc77ab96576
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T22:55:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great!</p>

          '
        raw: Great!
        updatedAt: '2023-05-27T22:55:26.766Z'
      numEdits: 0
      reactions: []
    id: 64728a5e5afd6a69658dcf89
    type: comment
  author: TheBloke
  content: Great!
  created_at: 2023-05-27 21:55:26+00:00
  edited: false
  hidden: false
  id: 64728a5e5afd6a69658dcf89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f0bcccb7b3ab8044743635036c35ae.svg
      fullname: LMachine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MachineLURN
      type: user
    createdAt: '2023-05-28T01:06:03.000Z'
    data:
      edited: true
      editors:
      - MachineLURN
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f0bcccb7b3ab8044743635036c35ae.svg
          fullname: LMachine
          isHf: false
          isPro: false
          name: MachineLURN
          type: user
        html: "<p>I'm assuming this is obvious but i'd like to state all of these\
          \ changes do not allow it to work on a 3080.</p>\n<p>\u201Coobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 844, in _apply self._buffers[key] = fn(buf) File \u201Coobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1143, in convert return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking) torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 10.00 GiB total\
          \ capacity; 9.16 GiB already allocated; 0 bytes free; 9.27 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation. See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n"
        raw: "I'm assuming this is obvious but i'd like to state all of these changes\
          \ do not allow it to work on a 3080.\n\n\u201Coobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 844,\
          \ in _apply self._buffers[key] = fn(buf) File \u201Coobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 1143, in convert return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking) torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 10.00 GiB total\
          \ capacity; 9.16 GiB already allocated; 0 bytes free; 9.27 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation. See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-05-28T01:06:54.693Z'
      numEdits: 1
      reactions: []
    id: 6472a8fbc27f74a0ebaf3c4b
    type: comment
  author: MachineLURN
  content: "I'm assuming this is obvious but i'd like to state all of these changes\
    \ do not allow it to work on a 3080.\n\n\u201Coobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 844, in _apply\
    \ self._buffers[key] = fn(buf) File \u201Coobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 1143, in convert\
    \ return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\
    \ non_blocking) torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 58.00 MiB (GPU 0; 10.00 GiB total capacity; 9.16 GiB already allocated; 0 bytes\
    \ free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation. See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-05-28 00:06:03+00:00
  edited: true
  hidden: false
  id: 6472a8fbc27f74a0ebaf3c4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-28T01:27:49.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I find it that GPTQ is slower than GGML.</p>

          <p>I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only
          4-5 t/s.</p>

          '
        raw: 'I find it that GPTQ is slower than GGML.


          I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only 4-5
          t/s.'
        updatedAt: '2023-05-28T01:27:49.551Z'
      numEdits: 0
      reactions: []
    id: 6472ae1597a75cc77abcd2f1
    type: comment
  author: mancub
  content: 'I find it that GPTQ is slower than GGML.


    I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only 4-5 t/s.'
  created_at: 2023-05-28 00:27:49+00:00
  edited: false
  hidden: false
  id: 6472ae1597a75cc77abcd2f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T01:46:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I''m assuming this is obvious but i''d like to state all of these changes
          do not allow it to work on a 3080.<br>Yeah I''m afraid that''s correct -
          30B models need 24GB VRAM to fully load a GPTQ, so 12GB VRAM is not enough.</p>

          </blockquote>

          <p>You can use offloading to load half the model in RAM instead. But that
          will be really slow, much slower than using a GGML and offloading as many
          layers as possible to the GPU.</p>

          <blockquote>

          <p>I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only
          4-5 t/s.</p>

          </blockquote>

          <p>Wow that''s really interesting.  When I tested this a week or so ago,
          I found GPTQ was still around twice the performance of GGML, as long as
          it was possible to load the full model into VRAM.</p>

          <p>But I don''t think I tested with 30B.  And also llama.cpp has had a bunch
          of further improvements since then.</p>

          <p>One factor is CPU single core speed.  pytorch inference (ie GPTQ) is
          single-core bottlenecked.  So if you have a lot of cores but with a low
          maximum clock speed, this bottlenecks GPU inference.    Whereas llama.cpp
          is multi-threaded and might not be bottlenecked in the same way.</p>

          <p>I will have to test again!</p>

          '
        raw: '> I''m assuming this is obvious but i''d like to state all of these
          changes do not allow it to work on a 3080.

          Yeah I''m afraid that''s correct - 30B models need 24GB VRAM to fully load
          a GPTQ, so 12GB VRAM is not enough.


          You can use offloading to load half the model in RAM instead. But that will
          be really slow, much slower than using a GGML and offloading as many layers
          as possible to the GPU.


          > I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only
          4-5 t/s.


          Wow that''s really interesting.  When I tested this a week or so ago, I
          found GPTQ was still around twice the performance of GGML, as long as it
          was possible to load the full model into VRAM.


          But I don''t think I tested with 30B.  And also llama.cpp has had a bunch
          of further improvements since then.


          One factor is CPU single core speed.  pytorch inference (ie GPTQ) is single-core
          bottlenecked.  So if you have a lot of cores but with a low maximum clock
          speed, this bottlenecks GPU inference.    Whereas llama.cpp is multi-threaded
          and might not be bottlenecked in the same way.


          I will have to test again!'
        updatedAt: '2023-05-28T01:46:11.389Z'
      numEdits: 0
      reactions: []
    id: 6472b263c27f74a0ebb00e64
    type: comment
  author: TheBloke
  content: '> I''m assuming this is obvious but i''d like to state all of these changes
    do not allow it to work on a 3080.

    Yeah I''m afraid that''s correct - 30B models need 24GB VRAM to fully load a GPTQ,
    so 12GB VRAM is not enough.


    You can use offloading to load half the model in RAM instead. But that will be
    really slow, much slower than using a GGML and offloading as many layers as possible
    to the GPU.


    > I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only 4-5 t/s.


    Wow that''s really interesting.  When I tested this a week or so ago, I found
    GPTQ was still around twice the performance of GGML, as long as it was possible
    to load the full model into VRAM.


    But I don''t think I tested with 30B.  And also llama.cpp has had a bunch of further
    improvements since then.


    One factor is CPU single core speed.  pytorch inference (ie GPTQ) is single-core
    bottlenecked.  So if you have a lot of cores but with a low maximum clock speed,
    this bottlenecks GPU inference.    Whereas llama.cpp is multi-threaded and might
    not be bottlenecked in the same way.


    I will have to test again!'
  created_at: 2023-05-28 00:46:11+00:00
  edited: false
  hidden: false
  id: 6472b263c27f74a0ebb00e64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f0bcccb7b3ab8044743635036c35ae.svg
      fullname: LMachine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MachineLURN
      type: user
    createdAt: '2023-05-28T01:53:52.000Z'
    data:
      edited: false
      editors:
      - MachineLURN
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f0bcccb7b3ab8044743635036c35ae.svg
          fullname: LMachine
          isHf: false
          isPro: false
          name: MachineLURN
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I''m assuming this is obvious but i''d like to state all of these changes
          do not allow it to work on a 3080.<br>Yeah I''m afraid that''s correct -
          30B models need 24GB VRAM to fully load a GPTQ, so 12GB VRAM is not enough.</p>

          </blockquote>

          <p>You can use offloading to load half the model in RAM instead. But that
          will be really slow, much slower than using a GGML and offloading as many
          layers as possible to the GPU.</p>

          </blockquote>

          <p>This is interesting, I mean I use 128gb of ram and a 5950x CPU so it''d
          be nice one day to be able to use my hardware on a model like this.</p>

          '
        raw: "> > I'm assuming this is obvious but i'd like to state all of these\
          \ changes do not allow it to work on a 3080.\n> Yeah I'm afraid that's correct\
          \ - 30B models need 24GB VRAM to fully load a GPTQ, so 12GB VRAM is not\
          \ enough.\n> \n> You can use offloading to load half the model in RAM instead.\
          \ But that will be really slow, much slower than using a GGML and offloading\
          \ as many layers as possible to the GPU.\n\n\nThis is interesting, I mean\
          \ I use 128gb of ram and a 5950x CPU so it'd be nice one day to be able\
          \ to use my hardware on a model like this."
        updatedAt: '2023-05-28T01:53:52.685Z'
      numEdits: 0
      reactions: []
    id: 6472b4300211f852701047e5
    type: comment
  author: MachineLURN
  content: "> > I'm assuming this is obvious but i'd like to state all of these changes\
    \ do not allow it to work on a 3080.\n> Yeah I'm afraid that's correct - 30B models\
    \ need 24GB VRAM to fully load a GPTQ, so 12GB VRAM is not enough.\n> \n> You\
    \ can use offloading to load half the model in RAM instead. But that will be really\
    \ slow, much slower than using a GGML and offloading as many layers as possible\
    \ to the GPU.\n\n\nThis is interesting, I mean I use 128gb of ram and a 5950x\
    \ CPU so it'd be nice one day to be able to use my hardware on a model like this."
  created_at: 2023-05-28 00:53:52+00:00
  edited: false
  hidden: false
  id: 6472b4300211f852701047e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-28T13:58:29.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MachineLURN&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MachineLURN\"\
          >@<span class=\"underline\">MachineLURN</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I think with your 3080 and the CPU/RAM setup you should be getting\
          \ a decent performance, based on what TheBloke said about single threaded\
          \ performance of GPTQ and looking at these charts: <a rel=\"nofollow\" href=\"\
          https://www.cpubenchmark.net/singleThread.html\">https://www.cpubenchmark.net/singleThread.html</a></p>\n\
          <p>That OOM error is something I had in the past too and IIRC the issue\
          \ was with the python environment and versions of packages, not the hardware\
          \ setup.</p>\n"
        raw: "@MachineLURN \n\nI think with your 3080 and the CPU/RAM setup you should\
          \ be getting a decent performance, based on what TheBloke said about single\
          \ threaded performance of GPTQ and looking at these charts: https://www.cpubenchmark.net/singleThread.html\n\
          \nThat OOM error is something I had in the past too and IIRC the issue was\
          \ with the python environment and versions of packages, not the hardware\
          \ setup."
        updatedAt: '2023-05-28T13:58:29.215Z'
      numEdits: 0
      reactions: []
    id: 64735e0563001a0002ca757d
    type: comment
  author: mancub
  content: "@MachineLURN \n\nI think with your 3080 and the CPU/RAM setup you should\
    \ be getting a decent performance, based on what TheBloke said about single threaded\
    \ performance of GPTQ and looking at these charts: https://www.cpubenchmark.net/singleThread.html\n\
    \nThat OOM error is something I had in the past too and IIRC the issue was with\
    \ the python environment and versions of packages, not the hardware setup."
  created_at: 2023-05-28 12:58:29+00:00
  edited: false
  hidden: false
  id: 64735e0563001a0002ca757d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b074c243d82f5b110fb8b9933d474747.svg
      fullname: Harry Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawhu2023
      type: user
    createdAt: '2023-05-28T15:21:13.000Z'
    data:
      edited: false
      editors:
      - shawhu2023
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b074c243d82f5b110fb8b9933d474747.svg
          fullname: Harry Xiao
          isHf: false
          isPro: false
          name: shawhu2023
          type: user
        html: '<p>i had the same error, and by changing groupsize to -1 I''d managed
          to finally load the model.<br>what is groupsize -1?<br>I was running oobabooga
          in a wsl2 linux in windows10</p>

          '
        raw: "i had the same error, and by changing groupsize to -1 I'd managed to\
          \ finally load the model.\nwhat is groupsize -1? \nI was running oobabooga\
          \ in a wsl2 linux in windows10"
        updatedAt: '2023-05-28T15:21:13.306Z'
      numEdits: 0
      reactions: []
    id: 647371696cff2f8672054d7f
    type: comment
  author: shawhu2023
  content: "i had the same error, and by changing groupsize to -1 I'd managed to finally\
    \ load the model.\nwhat is groupsize -1? \nI was running oobabooga in a wsl2 linux\
    \ in windows10"
  created_at: 2023-05-28 14:21:13+00:00
  edited: false
  hidden: false
  id: 647371696cff2f8672054d7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/096cf8b6cd70bf11ce2c13d4e2e14d6c.svg
      fullname: Jamon Terrell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamont
      type: user
    createdAt: '2023-05-28T16:41:49.000Z'
    data:
      edited: false
      editors:
      - jamont
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/096cf8b6cd70bf11ce2c13d4e2e14d6c.svg
          fullname: Jamon Terrell
          isHf: false
          isPro: false
          name: jamont
          type: user
        html: '<p>I''m able to run it on a 4090, so it should run on a 3090 since
          they have the same size VRAM.  Performance is good on a 4090--around 9 tokens/second
          typically.</p>

          '
        raw: I'm able to run it on a 4090, so it should run on a 3090 since they have
          the same size VRAM.  Performance is good on a 4090--around 9 tokens/second
          typically.
        updatedAt: '2023-05-28T16:41:49.369Z'
      numEdits: 0
      reactions: []
    id: 6473844d63001a0002cd0e5f
    type: comment
  author: jamont
  content: I'm able to run it on a 4090, so it should run on a 3090 since they have
    the same size VRAM.  Performance is good on a 4090--around 9 tokens/second typically.
  created_at: 2023-05-28 15:41:49+00:00
  edited: false
  hidden: false
  id: 6473844d63001a0002cd0e5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-05-28T22:00:57.000Z'
    data:
      edited: true
      editors:
      - clevnumb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>I have a 4090...I''m using the same settings in my config as Exterminant''s
          above, but still won''t load for me.. also tried groupsize -1, it just fails
          to load the model at all then (no error, no anything) I get this with config
          settings above:</p>

          <p>[image redacted by choice]</p>

          '
        raw: 'I have a 4090...I''m using the same settings in my config as Exterminant''s
          above, but still won''t load for me.. also tried groupsize -1, it just fails
          to load the model at all then (no error, no anything) I get this with config
          settings above:


          [image redacted by choice]'
        updatedAt: '2023-09-12T21:20:56.229Z'
      numEdits: 2
      reactions: []
    id: 6473cf19352c94a20ddbf4b4
    type: comment
  author: clevnumb
  content: 'I have a 4090...I''m using the same settings in my config as Exterminant''s
    above, but still won''t load for me.. also tried groupsize -1, it just fails to
    load the model at all then (no error, no anything) I get this with config settings
    above:


    [image redacted by choice]'
  created_at: 2023-05-28 21:00:57+00:00
  edited: true
  hidden: false
  id: 6473cf19352c94a20ddbf4b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T22:06:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>groupsize -1 is correct</p>

          <p>When you say it won''t load with that, no error - does it just say "Done"
          and then close? </p>

          <p>If so, you need to increase your Windows pagefile size . Try setting
          it to 90GB.  A lot of people are having this issue on Windows. Even if you
          have plenty of RAM, you still need a lot of pagefile just to get the model
          loaded into VRAM.</p>

          '
        raw: "groupsize -1 is correct\n\nWhen you say it won't load with that, no\
          \ error - does it just say \"Done\" and then close? \n\nIf so, you need\
          \ to increase your Windows pagefile size . Try setting it to 90GB.  A lot\
          \ of people are having this issue on Windows. Even if you have plenty of\
          \ RAM, you still need a lot of pagefile just to get the model loaded into\
          \ VRAM."
        updatedAt: '2023-05-28T22:06:12.910Z'
      numEdits: 0
      reactions: []
    id: 6473d05463001a0002d1d42a
    type: comment
  author: TheBloke
  content: "groupsize -1 is correct\n\nWhen you say it won't load with that, no error\
    \ - does it just say \"Done\" and then close? \n\nIf so, you need to increase\
    \ your Windows pagefile size . Try setting it to 90GB.  A lot of people are having\
    \ this issue on Windows. Even if you have plenty of RAM, you still need a lot\
    \ of pagefile just to get the model loaded into VRAM."
  created_at: 2023-05-28 21:06:12+00:00
  edited: false
  hidden: false
  id: 6473d05463001a0002d1d42a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-29T00:09:37.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Ok, for those of you who run WSL2 (I am also running it) you have
          to adjust its config to give it more resources.</p>

          <p>Check if you have: C:\Users&lt;username&gt;.wslconfig </p>

          <p>This is what mine looks like (I have 256GB RAM and 24 cores as dual 12-core
          CPUs, so plenty left for Windows):</p>

          <p>[wsl2]<br>memory=128GB<br>swap=0<br>localhostForwarding=true<br>processors=16</p>

          <p>EDIT: by the way I only use 8-9 cores fror GGML models because from my
          testing (in my environment) beyond that there is no significant improvement.</p>

          '
        raw: "Ok, for those of you who run WSL2 (I am also running it) you have to\
          \ adjust its config to give it more resources.\n\nCheck if you have: C:\\\
          Users\\<username>\\.wslconfig \n\nThis is what mine looks like (I have 256GB\
          \ RAM and 24 cores as dual 12-core CPUs, so plenty left for Windows):\n\n\
          [wsl2]\nmemory=128GB\nswap=0\nlocalhostForwarding=true\nprocessors=16\n\n\
          EDIT: by the way I only use 8-9 cores fror GGML models because from my testing\
          \ (in my environment) beyond that there is no significant improvement."
        updatedAt: '2023-05-29T00:10:46.768Z'
      numEdits: 1
      reactions: []
    id: 6473ed416cff2f86720d2c8d
    type: comment
  author: mancub
  content: "Ok, for those of you who run WSL2 (I am also running it) you have to adjust\
    \ its config to give it more resources.\n\nCheck if you have: C:\\Users\\<username>\\\
    .wslconfig \n\nThis is what mine looks like (I have 256GB RAM and 24 cores as\
    \ dual 12-core CPUs, so plenty left for Windows):\n\n[wsl2]\nmemory=128GB\nswap=0\n\
    localhostForwarding=true\nprocessors=16\n\nEDIT: by the way I only use 8-9 cores\
    \ fror GGML models because from my testing (in my environment) beyond that there\
    \ is no significant improvement."
  created_at: 2023-05-28 23:09:37+00:00
  edited: true
  hidden: false
  id: 6473ed416cff2f86720d2c8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
      fullname: Patrick Shechet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kajuberdut
      type: user
    createdAt: '2023-06-02T21:40:53.000Z'
    data:
      edited: true
      editors:
      - kajuberdut
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.859296977519989
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
          fullname: Patrick Shechet
          isHf: false
          isPro: false
          name: kajuberdut
          type: user
        html: '<blockquote>

          <p>I find it that GPTQ is slower than GGML.</p>

          <p>I can load a q5_0 into my 3090 and I get 6-7 t/s, while with GPTQ only
          4-5 t/s.</p>

          </blockquote>

          <p>I''m running a 3090 and Ryzen 7 5800X all at default clock / settings
          and I''m getting nearly 9 t/s on GPTQ.<br>Main oobabooga/text-generation-web
          at commit 28198bc15cc7065b8f4a594f6799ad1be39a209c.<br>Edit: I am on Ubuntu
          so if you are in WSL 2 that might explain the gap.</p>

          '
        raw: "> I find it that GPTQ is slower than GGML.\n> \n> I can load a q5_0\
          \ into my 3090 and I get 6-7 t/s, while with GPTQ only 4-5 t/s.\n\nI'm running\
          \ a 3090 and Ryzen 7 5800X all at default clock / settings and I'm getting\
          \ nearly 9 t/s on GPTQ.\nMain oobabooga/text-generation-web at commit 28198bc15cc7065b8f4a594f6799ad1be39a209c.\n\
          Edit: I am on Ubuntu so if you are in WSL 2 that might explain the gap."
        updatedAt: '2023-06-02T21:41:36.445Z'
      numEdits: 1
      reactions: []
    id: 647a61e5822b7e8ccbd86a64
    type: comment
  author: kajuberdut
  content: "> I find it that GPTQ is slower than GGML.\n> \n> I can load a q5_0 into\
    \ my 3090 and I get 6-7 t/s, while with GPTQ only 4-5 t/s.\n\nI'm running a 3090\
    \ and Ryzen 7 5800X all at default clock / settings and I'm getting nearly 9 t/s\
    \ on GPTQ.\nMain oobabooga/text-generation-web at commit 28198bc15cc7065b8f4a594f6799ad1be39a209c.\n\
    Edit: I am on Ubuntu so if you are in WSL 2 that might explain the gap."
  created_at: 2023-06-02 20:40:53+00:00
  edited: true
  hidden: false
  id: 647a61e5822b7e8ccbd86a64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-03T15:41:39.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9701525568962097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Ryzen 7 probably has better single core performance than my Xeon
          E5v3s so that would be another reason why you get more t/s.</p>

          <p>I''m going to setup a native Linux drive soon and try it out, just need
          to source another SSD. :)</p>

          '
        raw: 'Ryzen 7 probably has better single core performance than my Xeon E5v3s
          so that would be another reason why you get more t/s.


          I''m going to setup a native Linux drive soon and try it out, just need
          to source another SSD. :)'
        updatedAt: '2023-06-03T15:41:39.355Z'
      numEdits: 0
      reactions: []
    id: 647b5f336dbad6ab057c81cd
    type: comment
  author: mancub
  content: 'Ryzen 7 probably has better single core performance than my Xeon E5v3s
    so that would be another reason why you get more t/s.


    I''m going to setup a native Linux drive soon and try it out, just need to source
    another SSD. :)'
  created_at: 2023-06-03 14:41:39+00:00
  edited: false
  hidden: false
  id: 647b5f336dbad6ab057c81cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-06-06T07:46:52.000Z'
    data:
      edited: false
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553157091140747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: '<blockquote>

          <p>Ryzen 7 probably has better single core performance than my Xeon E5v3s
          so that would be another reason why you get more t/s.</p>

          <p>I''m going to setup a native Linux drive soon and try it out, just need
          to source another SSD. :)</p>

          </blockquote>

          <p>Well, I''m getting exactly half of what I get on 13B models (wizard-vicuna
          etc.), which results (Guanaco 33B) in 12-15 t/s on a single 3090 using GPTQ
          + monkey patch, i5 13500 with some cheap b760 mobo. I''ve tested Guanaco
          couple of days ago on a runpod with 2 x 3090 and I was getting the same
          results as I''m getting locally o na single GPU, so I was quite shocked.
          It could be due to the monkey patch + triton version of GPTQ-for-llama (not
          one-shot installer of text-generation-webui!!). Otherwise, you could consider
          moving to linux with your project.</p>

          '
        raw: "> Ryzen 7 probably has better single core performance than my Xeon E5v3s\
          \ so that would be another reason why you get more t/s.\n> \n> I'm going\
          \ to setup a native Linux drive soon and try it out, just need to source\
          \ another SSD. :)\n\nWell, I'm getting exactly half of what I get on 13B\
          \ models (wizard-vicuna etc.), which results (Guanaco 33B) in 12-15 t/s\
          \ on a single 3090 using GPTQ + monkey patch, i5 13500 with some cheap b760\
          \ mobo. I've tested Guanaco couple of days ago on a runpod with 2 x 3090\
          \ and I was getting the same results as I'm getting locally o na single\
          \ GPU, so I was quite shocked. It could be due to the monkey patch + triton\
          \ version of GPTQ-for-llama (not one-shot installer of text-generation-webui!!).\
          \ Otherwise, you could consider moving to linux with your project."
        updatedAt: '2023-06-06T07:46:52.833Z'
      numEdits: 0
      reactions: []
    id: 647ee46c9c31024457965bbf
    type: comment
  author: mikolodz
  content: "> Ryzen 7 probably has better single core performance than my Xeon E5v3s\
    \ so that would be another reason why you get more t/s.\n> \n> I'm going to setup\
    \ a native Linux drive soon and try it out, just need to source another SSD. :)\n\
    \nWell, I'm getting exactly half of what I get on 13B models (wizard-vicuna etc.),\
    \ which results (Guanaco 33B) in 12-15 t/s on a single 3090 using GPTQ + monkey\
    \ patch, i5 13500 with some cheap b760 mobo. I've tested Guanaco couple of days\
    \ ago on a runpod with 2 x 3090 and I was getting the same results as I'm getting\
    \ locally o na single GPU, so I was quite shocked. It could be due to the monkey\
    \ patch + triton version of GPTQ-for-llama (not one-shot installer of text-generation-webui!!).\
    \ Otherwise, you could consider moving to linux with your project."
  created_at: 2023-06-06 06:46:52+00:00
  edited: false
  hidden: false
  id: 647ee46c9c31024457965bbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T00:43:16.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9828488826751709
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I don''t think that 2x3090 will perform faster than a single 3090,
          because inference only happens on a single GPU.</p>

          <p>Who knows, maybe there is some setting that could be enabled in the driver
          to allow full NVLink on these consumer cards. But why would nvidia do that,
          when that''s not good for business...</p>

          '
        raw: 'I don''t think that 2x3090 will perform faster than a single 3090, because
          inference only happens on a single GPU.


          Who knows, maybe there is some setting that could be enabled in the driver
          to allow full NVLink on these consumer cards. But why would nvidia do that,
          when that''s not good for business...'
        updatedAt: '2023-06-07T00:43:16.251Z'
      numEdits: 0
      reactions: []
    id: 647fd2a41637c1c0e6f23125
    type: comment
  author: mancub
  content: 'I don''t think that 2x3090 will perform faster than a single 3090, because
    inference only happens on a single GPU.


    Who knows, maybe there is some setting that could be enabled in the driver to
    allow full NVLink on these consumer cards. But why would nvidia do that, when
    that''s not good for business...'
  created_at: 2023-06-06 23:43:16+00:00
  edited: false
  hidden: false
  id: 647fd2a41637c1c0e6f23125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba75fa36ec4101c6f45905a1103d34ab.svg
      fullname: wrzaskun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wrzaskun
      type: user
    createdAt: '2023-06-08T00:52:56.000Z'
    data:
      edited: false
      editors:
      - wrzaskun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669839143753052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba75fa36ec4101c6f45905a1103d34ab.svg
          fullname: wrzaskun
          isHf: false
          isPro: false
          name: wrzaskun
          type: user
        html: '<p>I''ve read that with 2 cards, they both have the same data in memory,
          unfortunately, I don''t have a way to verify it, but it seems logical. On
          the other hand, I''ve heard that it''s possible to combine graphics cards
          with a 20% memory loss so that they have different data in memory. For example,
          is it possible to divide the 65B version into 3 parts and distribute them
          among 3 x RTX 4060 16GB VRAM with a total capacity of 48VRAM?<br>Is it even
          possible to divide the data like that or develop software that transfers
          computation results between the cards without involving RAM and works solely
          on the GPU?</p>

          <p>One can imagine that 10x RTX 4060 16GB VRAM would provide 160GB of RAM
          - with a 20% loss, it would give 128GB of VRAM for large models and consume
          only about 1200-1500 watts of power. It would be slower but cheaper than
          H100. Sorry if my question is pointless :)</p>

          '
        raw: 'I''ve read that with 2 cards, they both have the same data in memory,
          unfortunately, I don''t have a way to verify it, but it seems logical. On
          the other hand, I''ve heard that it''s possible to combine graphics cards
          with a 20% memory loss so that they have different data in memory. For example,
          is it possible to divide the 65B version into 3 parts and distribute them
          among 3 x RTX 4060 16GB VRAM with a total capacity of 48VRAM?

          Is it even possible to divide the data like that or develop software that
          transfers computation results between the cards without involving RAM and
          works solely on the GPU?


          One can imagine that 10x RTX 4060 16GB VRAM would provide 160GB of RAM -
          with a 20% loss, it would give 128GB of VRAM for large models and consume
          only about 1200-1500 watts of power. It would be slower but cheaper than
          H100. Sorry if my question is pointless :)'
        updatedAt: '2023-06-08T00:52:56.206Z'
      numEdits: 0
      reactions: []
    id: 64812668e1421e205fe27ea9
    type: comment
  author: wrzaskun
  content: 'I''ve read that with 2 cards, they both have the same data in memory,
    unfortunately, I don''t have a way to verify it, but it seems logical. On the
    other hand, I''ve heard that it''s possible to combine graphics cards with a 20%
    memory loss so that they have different data in memory. For example, is it possible
    to divide the 65B version into 3 parts and distribute them among 3 x RTX 4060
    16GB VRAM with a total capacity of 48VRAM?

    Is it even possible to divide the data like that or develop software that transfers
    computation results between the cards without involving RAM and works solely on
    the GPU?


    One can imagine that 10x RTX 4060 16GB VRAM would provide 160GB of RAM - with
    a 20% loss, it would give 128GB of VRAM for large models and consume only about
    1200-1500 watts of power. It would be slower but cheaper than H100. Sorry if my
    question is pointless :)'
  created_at: 2023-06-07 23:52:56+00:00
  edited: false
  hidden: false
  id: 64812668e1421e205fe27ea9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-08T01:53:34.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7968288064002991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I think you can split the model across multiple cards by specifying
          --auto-devices in text-generation-webui, or when using python, when loading
          the model setting device_map=''auto'', but inference will still happen on
          a single GPU.</p>

          <p>I see there is also some kind of a bug in text-gen-webui atm regarding
          multiple devices, that requres the equal amount of RAM: <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/issues/2543">https://github.com/oobabooga/text-generation-webui/issues/2543</a></p>

          '
        raw: 'I think you can split the model across multiple cards by specifying
          --auto-devices in text-generation-webui, or when using python, when loading
          the model setting device_map=''auto'', but inference will still happen on
          a single GPU.


          I see there is also some kind of a bug in text-gen-webui atm regarding multiple
          devices, that requres the equal amount of RAM: https://github.com/oobabooga/text-generation-webui/issues/2543'
        updatedAt: '2023-06-08T01:53:34.994Z'
      numEdits: 0
      reactions: []
    id: 6481349e9aafd41918b38e11
    type: comment
  author: mancub
  content: 'I think you can split the model across multiple cards by specifying --auto-devices
    in text-generation-webui, or when using python, when loading the model setting
    device_map=''auto'', but inference will still happen on a single GPU.


    I see there is also some kind of a bug in text-gen-webui atm regarding multiple
    devices, that requres the equal amount of RAM: https://github.com/oobabooga/text-generation-webui/issues/2543'
  created_at: 2023-06-08 00:53:34+00:00
  edited: false
  hidden: false
  id: 6481349e9aafd41918b38e11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/06b908c7537c82a4d6295c5e6773802b.svg
      fullname: Jonghuyn JUNGBOIX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jmew1989
      type: user
    createdAt: '2023-06-13T21:27:37.000Z'
    data:
      edited: false
      editors:
      - jmew1989
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9623966813087463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/06b908c7537c82a4d6295c5e6773802b.svg
          fullname: Jonghuyn JUNGBOIX
          isHf: false
          isPro: false
          name: jmew1989
          type: user
        html: '<p>Hey TheBloke !<br>Thanks for sharing and following up !<br>I''m
          not having any issues with 13B models you shared, I just followed instructions
          by aitrepreneur and went smooth.</p>

          <p>But now I''m trying to get models + 30b working, and everytime -(same
          for gpt4xalpaca 30b for example) I get this cuda out of memory error.</p>

          <p>Tried to allocate 58.00 MiB (GPU 0; 16.00 GiB total capacity; 15.09 GiB
          already allocated; 0 bytes free; 15.28 GiB reserved in total by PyTorch)</p>

          <p>I''m on RTX 3080 but I have 64GB ram and 32GB gpu memory .. I just don''t
          understand why it doesn''t find the ressources for that.</p>

          <p>Since I''m on windows, I also tried your "paging settings" but didn''t
          change anything. I''m not good at all this, but one thing I am thinking
          is that since paging sounds to be using "disk space", maybe I should free
          some space to make it more efficient ?...</p>

          <p>Any help would be appreciated !!</p>

          '
        raw: 'Hey TheBloke !

          Thanks for sharing and following up !

          I''m not having any issues with 13B models you shared, I just followed instructions
          by aitrepreneur and went smooth.


          But now I''m trying to get models + 30b working, and everytime -(same for
          gpt4xalpaca 30b for example) I get this cuda out of memory error.


          Tried to allocate 58.00 MiB (GPU 0; 16.00 GiB total capacity; 15.09 GiB
          already allocated; 0 bytes free; 15.28 GiB reserved in total by PyTorch)


          I''m on RTX 3080 but I have 64GB ram and 32GB gpu memory .. I just don''t
          understand why it doesn''t find the ressources for that.


          Since I''m on windows, I also tried your "paging settings" but didn''t change
          anything. I''m not good at all this, but one thing I am thinking is that
          since paging sounds to be using "disk space", maybe I should free some space
          to make it more efficient ?...


          Any help would be appreciated !!'
        updatedAt: '2023-06-13T21:27:37.325Z'
      numEdits: 0
      reactions: []
    id: 6488df4958087537cfc06b92
    type: comment
  author: jmew1989
  content: 'Hey TheBloke !

    Thanks for sharing and following up !

    I''m not having any issues with 13B models you shared, I just followed instructions
    by aitrepreneur and went smooth.


    But now I''m trying to get models + 30b working, and everytime -(same for gpt4xalpaca
    30b for example) I get this cuda out of memory error.


    Tried to allocate 58.00 MiB (GPU 0; 16.00 GiB total capacity; 15.09 GiB already
    allocated; 0 bytes free; 15.28 GiB reserved in total by PyTorch)


    I''m on RTX 3080 but I have 64GB ram and 32GB gpu memory .. I just don''t understand
    why it doesn''t find the ressources for that.


    Since I''m on windows, I also tried your "paging settings" but didn''t change
    anything. I''m not good at all this, but one thing I am thinking is that since
    paging sounds to be using "disk space", maybe I should free some space to make
    it more efficient ?...


    Any help would be appreciated !!'
  created_at: 2023-06-13 20:27:37+00:00
  edited: false
  hidden: false
  id: 6488df4958087537cfc06b92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-14T07:54:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9173627495765686
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>A 30B model is too big for a 16GB card. You need a 24GB card to
          fully load a 30B GPTQ model.</p>

          <p>It is technically possible to load it. If you''re using text-generation-ui,
          then you can do that by setting the "GPU Memory" slider to around 10GB.  That
          will load 10Gb of model, and then allow 6GB for context. The rest will go
          to RAM.  It will be slow as hell though.</p>

          <p>If you really want to use a 30B model, I recommend you use a GGML version
          instead.  GGML with CUDA acceleration performs much better when you don''t
          have enough VRAM to fully load the model.</p>

          '
        raw: 'A 30B model is too big for a 16GB card. You need a 24GB card to fully
          load a 30B GPTQ model.


          It is technically possible to load it. If you''re using text-generation-ui,
          then you can do that by setting the "GPU Memory" slider to around 10GB.  That
          will load 10Gb of model, and then allow 6GB for context. The rest will go
          to RAM.  It will be slow as hell though.


          If you really want to use a 30B model, I recommend you use a GGML version
          instead.  GGML with CUDA acceleration performs much better when you don''t
          have enough VRAM to fully load the model.'
        updatedAt: '2023-06-14T07:54:58.901Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Renegadesoffun
    id: 64897252d54072d46f15fc39
    type: comment
  author: TheBloke
  content: 'A 30B model is too big for a 16GB card. You need a 24GB card to fully
    load a 30B GPTQ model.


    It is technically possible to load it. If you''re using text-generation-ui, then
    you can do that by setting the "GPU Memory" slider to around 10GB.  That will
    load 10Gb of model, and then allow 6GB for context. The rest will go to RAM.  It
    will be slow as hell though.


    If you really want to use a 30B model, I recommend you use a GGML version instead.  GGML
    with CUDA acceleration performs much better when you don''t have enough VRAM to
    fully load the model.'
  created_at: 2023-06-14 06:54:58+00:00
  edited: false
  hidden: false
  id: 64897252d54072d46f15fc39
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: open
target_branch: null
title: Could this model be loaded in 3090 GPU?
