!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ryzzlestrizzle
conflicting_files: null
created_at: 2023-06-30 13:38:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc25c0c502ba7a0225acbd3fc2411535.svg
      fullname: "Jonathan Rystr\xF8m"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ryzzlestrizzle
      type: user
    createdAt: '2023-06-30T14:38:29.000Z'
    data:
      edited: false
      editors:
      - ryzzlestrizzle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6037483215332031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc25c0c502ba7a0225acbd3fc2411535.svg
          fullname: "Jonathan Rystr\xF8m"
          isHf: false
          isPro: false
          name: ryzzlestrizzle
          type: user
        html: "<p>Hello! Thank you for your great work. I'm running into this dumb\
          \ error when trying to load the model using auto_gptq - the code is copied\
          \ from the \"TheBloke/Nous-Hermes-13B-GPTQ\"-repo. Any ideas on how to resolve\
          \ it?</p>\n<pre><code>from transformers import AutoTokenizer\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path =\
          \ \"TheBloke/guanaco-33B-GPTQ\"\nmodel_basename = \"Guanaco-33B-GPTQ-4bit.act-order.safetensors\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n</code></pre>\n"
        raw: "Hello! Thank you for your great work. I'm running into this dumb error\
          \ when trying to load the model using auto_gptq - the code is copied from\
          \ the \"TheBloke/Nous-Hermes-13B-GPTQ\"-repo. Any ideas on how to resolve\
          \ it?\r\n\r\n```\r\nfrom transformers import AutoTokenizer\r\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path\
          \ = \"TheBloke/guanaco-33B-GPTQ\"\r\nmodel_basename = \"Guanaco-33B-GPTQ-4bit.act-order.safetensors\"\
          \r\n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n    \
          \    use_triton=use_triton,\r\n        quantize_config=None)\r\n```"
        updatedAt: '2023-06-30T14:38:29.204Z'
      numEdits: 0
      reactions: []
    id: 649ee8e5bab0c6d0e23e957c
    type: comment
  author: ryzzlestrizzle
  content: "Hello! Thank you for your great work. I'm running into this dumb error\
    \ when trying to load the model using auto_gptq - the code is copied from the\
    \ \"TheBloke/Nous-Hermes-13B-GPTQ\"-repo. Any ideas on how to resolve it?\r\n\r\
    \n```\r\nfrom transformers import AutoTokenizer\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"TheBloke/guanaco-33B-GPTQ\"\
    \r\nmodel_basename = \"Guanaco-33B-GPTQ-4bit.act-order.safetensors\"\r\n\r\nuse_triton\
    \ = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\n\
    \        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\
    \n        quantize_config=None)\r\n```"
  created_at: 2023-06-30 13:38:29+00:00
  edited: false
  hidden: false
  id: 649ee8e5bab0c6d0e23e957c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-30T14:52:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8905753493309021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s because <code>model_basename</code> shouldn''t include the
          <code>.safetensors</code> extension. It should be:</p>

          <pre><code>model_basename = "Guanaco-33B-GPTQ-4bit.act-order"

          </code></pre>

          <p>It''s confusing I know - I''ve never liked how it''s implemented. Should
          just be able to specify the whole filename.</p>

          '
        raw: 'It''s because `model_basename` shouldn''t include the `.safetensors`
          extension. It should be:

          ```

          model_basename = "Guanaco-33B-GPTQ-4bit.act-order"

          ```


          It''s confusing I know - I''ve never liked how it''s implemented. Should
          just be able to specify the whole filename.'
        updatedAt: '2023-06-30T14:52:35.666Z'
      numEdits: 0
      reactions: []
    id: 649eec3323c9440af9dd99ff
    type: comment
  author: TheBloke
  content: 'It''s because `model_basename` shouldn''t include the `.safetensors` extension.
    It should be:

    ```

    model_basename = "Guanaco-33B-GPTQ-4bit.act-order"

    ```


    It''s confusing I know - I''ve never liked how it''s implemented. Should just
    be able to specify the whole filename.'
  created_at: 2023-06-30 13:52:35+00:00
  edited: false
  hidden: false
  id: 649eec3323c9440af9dd99ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc25c0c502ba7a0225acbd3fc2411535.svg
      fullname: "Jonathan Rystr\xF8m"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ryzzlestrizzle
      type: user
    createdAt: '2023-06-30T15:31:03.000Z'
    data:
      edited: false
      editors:
      - ryzzlestrizzle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8366996049880981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc25c0c502ba7a0225acbd3fc2411535.svg
          fullname: "Jonathan Rystr\xF8m"
          isHf: false
          isPro: false
          name: ryzzlestrizzle
          type: user
        html: '<p>Works now, thank you!</p>

          '
        raw: Works now, thank you!
        updatedAt: '2023-06-30T15:31:03.869Z'
      numEdits: 0
      reactions: []
    id: 649ef537086f71dc28db8d65
    type: comment
  author: ryzzlestrizzle
  content: Works now, thank you!
  created_at: 2023-06-30 14:31:03+00:00
  edited: false
  hidden: false
  id: 649ef537086f71dc28db8d65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/cc25c0c502ba7a0225acbd3fc2411535.svg
      fullname: "Jonathan Rystr\xF8m"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ryzzlestrizzle
      type: user
    createdAt: '2023-06-30T15:31:08.000Z'
    data:
      status: closed
    id: 649ef53c25ad2294f7cc6993
    type: status-change
  author: ryzzlestrizzle
  created_at: 2023-06-30 14:31:08+00:00
  id: 649ef53c25ad2294f7cc6993
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/guanaco-33B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'FileNotFoundError: Could not find model in TheBloke/guanaco-33B-GPTQ'
