!!python/object:huggingface_hub.community.DiscussionWithDetails
author: meddebma
conflicting_files: null
created_at: 2024-01-13 09:59:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0cc495e4eb918f753399f6fd863c555a.svg
      fullname: Aymen Meddeb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: meddebma
      type: user
    createdAt: '2024-01-13T09:59:33.000Z'
    data:
      edited: false
      editors:
      - meddebma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9430814385414124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0cc495e4eb918f753399f6fd863c555a.svg
          fullname: Aymen Meddeb
          isHf: false
          isPro: false
          name: meddebma
          type: user
        html: "<p>Hi Guys,</p>\n<p>I just got a Macbook M3 Max (36 RAM, 30\u2011Core\
          \ GPU) and I'd like to run Mixtral on MLX and do some RAG. I saw there are\
          \ already some quantized mixtral models (<a href=\"https://huggingface.co/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF\"\
          >https://huggingface.co/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF</a>). Do\
          \ somedbody has experience with it? could you provide an example or few\
          \ best practices?<br>Thanks<br>Aymen</p>\n"
        raw: "Hi Guys,\r\n\r\nI just got a Macbook M3 Max (36 RAM, 30\u2011Core GPU)\
          \ and I'd like to run Mixtral on MLX and do some RAG. I saw there are already\
          \ some quantized mixtral models (https://huggingface.co/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF).\
          \ Do somedbody has experience with it? could you provide an example or few\
          \ best practices? \r\nThanks\r\nAymen"
        updatedAt: '2024-01-13T09:59:33.155Z'
      numEdits: 0
      reactions: []
    id: 65a25f053e3a00a9c1e0d839
    type: comment
  author: meddebma
  content: "Hi Guys,\r\n\r\nI just got a Macbook M3 Max (36 RAM, 30\u2011Core GPU)\
    \ and I'd like to run Mixtral on MLX and do some RAG. I saw there are already\
    \ some quantized mixtral models (https://huggingface.co/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF).\
    \ Do somedbody has experience with it? could you provide an example or few best\
    \ practices? \r\nThanks\r\nAymen"
  created_at: 2024-01-13 09:59:33+00:00
  edited: false
  hidden: false
  id: 65a25f053e3a00a9c1e0d839
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: mlx-community/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Mixtral quantization for Apple Silicon
