!!python/object:huggingface_hub.community.DiscussionWithDetails
author: stanislav-seltser
conflicting_files: null
created_at: 2023-08-28 20:51:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3047c31d1e782958a48189a298cfaefa.svg
      fullname: Stan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stanislav-seltser
      type: user
    createdAt: '2023-08-28T21:51:46.000Z'
    data:
      edited: false
      editors:
      - stanislav-seltser
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8017714023590088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3047c31d1e782958a48189a298cfaefa.svg
          fullname: Stan
          isHf: false
          isPro: false
          name: stanislav-seltser
          type: user
        html: '<p>the code should be like this<br>mport torch<br>from transformers
          import AutoTokenizer, AutoModelForCausalLM</p>

          <h1 id="load-base-llm-model-and-tokenizer">load base LLM model and tokenizer</h1>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    "philschmid/llama-2-7b-instruction-generator",<br>    low_cpu_mem_usage=True,<br>    torch_dtype=torch.float16,<br>    load_in_4bit=True,<br>)<br>tokenizer
          = AutoTokenizer.from_pretrained("philschmid/llama-2-7b-instruction-generator")</p>

          <p>prompt = f"""### Instruction:<br>Use the Input below to create an instruction,
          which could have been used to generate the input using an LLM. </p>

          <h3 id="input">Input:</h3>

          <p>Dear [boss name],</p>

          <p>I''m writing to request next week, August 1st through August 4th,<br>off
          as paid time off.</p>

          <p>I have some personal matters to attend to that week that require<br>me
          to be out of the office. I wanted to give you as much advance<br>notice
          as possible so you can plan accordingly while I am away.</p>

          <p>Please let me know if you need any additional information from me<br>or
          have any concerns with me taking next week off. I appreciate you<br>considering
          this request.</p>

          <p>Thank you, [Your name]</p>

          <h3 id="response">Response:</h3>

          <p>"""</p>

          <p>input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()<br>outputs
          = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True,
          top_p=0.9,temperature=0.9)</p>

          <p>print(f"Generated instruction:\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(),
          skip_special_tokens=True)[0][len(prompt):]}")</p>

          '
        raw: "the code should be like this\r\nmport torch\r\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\r\n\r\n# load base LLM model and tokenizer\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"philschmid/llama-2-7b-instruction-generator\"\
          ,\r\n    low_cpu_mem_usage=True,\r\n    torch_dtype=torch.float16,\r\n \
          \   load_in_4bit=True,\r\n) \r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          philschmid/llama-2-7b-instruction-generator\")\r\n\r\nprompt = f\"\"\"###\
          \ Instruction:\r\nUse the Input below to create an instruction, which could\
          \ have been used to generate the input using an LLM. \r\n\r\n### Input:\r\
          \nDear [boss name],\r\n\r\nI'm writing to request next week, August 1st\
          \ through August 4th,\r\noff as paid time off.\r\n\r\nI have some personal\
          \ matters to attend to that week that require \r\nme to be out of the office.\
          \ I wanted to give you as much advance \r\nnotice as possible so you can\
          \ plan accordingly while I am away.\r\n\r\nPlease let me know if you need\
          \ any additional information from me \r\nor have any concerns with me taking\
          \ next week off. I appreciate you \r\nconsidering this request.\r\n\r\n\
          Thank you, [Your name]\r\n\r\n### Response:\r\n\"\"\"\r\n\r\ninput_ids =\
          \ tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\r\
          \noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True,\
          \ top_p=0.9,temperature=0.9)\r\n\r\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(),\
          \ skip_special_tokens=True)[0][len(prompt):]}\")"
        updatedAt: '2023-08-28T21:51:46.628Z'
      numEdits: 0
      reactions: []
    id: 64ed16f219e092db62dcf00f
    type: comment
  author: stanislav-seltser
  content: "the code should be like this\r\nmport torch\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\n\r\n# load base LLM model and tokenizer\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"philschmid/llama-2-7b-instruction-generator\"\
    ,\r\n    low_cpu_mem_usage=True,\r\n    torch_dtype=torch.float16,\r\n    load_in_4bit=True,\r\
    \n) \r\ntokenizer = AutoTokenizer.from_pretrained(\"philschmid/llama-2-7b-instruction-generator\"\
    )\r\n\r\nprompt = f\"\"\"### Instruction:\r\nUse the Input below to create an\
    \ instruction, which could have been used to generate the input using an LLM.\
    \ \r\n\r\n### Input:\r\nDear [boss name],\r\n\r\nI'm writing to request next week,\
    \ August 1st through August 4th,\r\noff as paid time off.\r\n\r\nI have some personal\
    \ matters to attend to that week that require \r\nme to be out of the office.\
    \ I wanted to give you as much advance \r\nnotice as possible so you can plan\
    \ accordingly while I am away.\r\n\r\nPlease let me know if you need any additional\
    \ information from me \r\nor have any concerns with me taking next week off. I\
    \ appreciate you \r\nconsidering this request.\r\n\r\nThank you, [Your name]\r\
    \n\r\n### Response:\r\n\"\"\"\r\n\r\ninput_ids = tokenizer(prompt, return_tensors=\"\
    pt\", truncation=True).input_ids.cuda()\r\noutputs = model.generate(input_ids=input_ids,\
    \ max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\r\n\r\nprint(f\"\
    Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(),\
    \ skip_special_tokens=True)[0][len(prompt):]}\")"
  created_at: 2023-08-28 20:51:46+00:00
  edited: false
  hidden: false
  id: 64ed16f219e092db62dcf00f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: philschmid/llama-2-7b-instruction-generator
repo_type: model
status: open
target_branch: null
title: author mispelled code for python automodelml
