!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Labmem009
conflicting_files: null
created_at: 2024-01-08 11:44:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcaa3bb720b9bbda1de4ff0ae3b3666f.svg
      fullname: Yongchi Zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Labmem009
      type: user
    createdAt: '2024-01-08T11:44:48.000Z'
    data:
      edited: true
      editors:
      - Labmem009
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5171034336090088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcaa3bb720b9bbda1de4ff0ae3b3666f.svg
          fullname: Yongchi Zhu
          isHf: false
          isPro: false
          name: Labmem009
          type: user
        html: '<p>When I run this version, I encountered error like:<br>    with safe_open(checkpoint_file,
          framework="pt") as f:<br>FileNotFoundError: No such file or directory: "Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction/model-00001-of-00008.safetensors"<br>Did
          you upload the wrong model.safetensors.index.json? Or I use a false script?<br>My
          script is below:<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model_path = ''Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction''</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)</p>

          <h1 id="since-transformers-4350-the-gpt-qawq-model-can-be-loaded-using-automodelforcausallm">Since
          transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.</h1>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    model_path,<br>    device_map="auto",<br>    torch_dtype=''auto''<br>).eval()<br>with
          open(''text.txt'', ''r'', encoding=''utf-8-sig'') as file:<br>    user_message
          = file.read()<br>    print(len(user_message))<br>user_message=user_message+''\n''+''Please
          take note of the content in these sentences...''<br>system_message=''Below
          is a task for...''</p>

          <h1 id="prompt-content-hi">Prompt content: "hi"</h1>

          <p>messages = [<br>    {"role": "user", "content": system_message+user_message},<br>]</p>

          <p>input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True,
          add_generation_prompt=True, return_tensors=''pt'')<br>output_ids = model.generate(input_ids.to(''cuda''),max_new_tokens=10240,
          do_sample=True, temperature=0.9)<br>response = tokenizer.decode(output_ids[0][input_ids.shape[1]:],
          skip_special_tokens=True)</p>

          <h1 id="model-response-hello-how-can-i-assist-you-today">Model response:
          "Hello! How can I assist you today?"</h1>

          <p>print(response)</p>

          '
        raw: "When I run this version, I encountered error like:\n    with safe_open(checkpoint_file,\
          \ framework=\"pt\") as f:\nFileNotFoundError: No such file or directory:\
          \ \"Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction/model-00001-of-00008.safetensors\"\
          \nDid you upload the wrong model.safetensors.index.json? Or I use a false\
          \ script?\nMy script is below:\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel_path = 'Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction'\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\
          \n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"\
          auto\",\n    torch_dtype='auto'\n).eval()\nwith open('text.txt', 'r', encoding='utf-8-sig')\
          \ as file:\n    user_message = file.read()\n    print(len(user_message))\n\
          user_message=user_message+'\\n'+'Please take note of the content in these\
          \ sentences...'\nsystem_message='Below is a task for...'\n\n# Prompt content:\
          \ \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": system_message+user_message},\n\
          ]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True,\
          \ add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'),max_new_tokens=10240,\
          \ do_sample=True, temperature=0.9)\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:],\
          \ skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist\
          \ you today?\"\nprint(response)"
        updatedAt: '2024-01-08T11:52:49.788Z'
      numEdits: 1
      reactions: []
    id: 659be030e1604bf9960ab53e
    type: comment
  author: Labmem009
  content: "When I run this version, I encountered error like:\n    with safe_open(checkpoint_file,\
    \ framework=\"pt\") as f:\nFileNotFoundError: No such file or directory: \"Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction/model-00001-of-00008.safetensors\"\
    \nDid you upload the wrong model.safetensors.index.json? Or I use a false script?\n\
    My script is below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \nmodel_path = 'Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction'\n\ntokenizer =\
    \ AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers\
    \ 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"\
    auto\",\n    torch_dtype='auto'\n).eval()\nwith open('text.txt', 'r', encoding='utf-8-sig')\
    \ as file:\n    user_message = file.read()\n    print(len(user_message))\nuser_message=user_message+'\\\
    n'+'Please take note of the content in these sentences...'\nsystem_message='Below\
    \ is a task for...'\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\":\
    \ \"user\", \"content\": system_message+user_message},\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages,\
    \ tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids\
    \ = model.generate(input_ids.to('cuda'),max_new_tokens=10240, do_sample=True,\
    \ temperature=0.9)\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:],\
    \ skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you\
    \ today?\"\nprint(response)"
  created_at: 2024-01-08 11:44:48+00:00
  edited: true
  hidden: false
  id: 659be030e1604bf9960ab53e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-09T21:02:13.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8673328161239624
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p> AutoModelForCausalLM.from_pretrained</p>

          </blockquote>

          <p>This is an exl2 model, you have to load it with an exllamav2 loader.</p>

          '
        raw: '>  AutoModelForCausalLM.from_pretrained


          This is an exl2 model, you have to load it with an exllamav2 loader.'
        updatedAt: '2024-01-09T21:02:13.797Z'
      numEdits: 0
      reactions: []
    id: 659db455912b75e2c34f9767
    type: comment
  author: brucethemoose
  content: '>  AutoModelForCausalLM.from_pretrained


    This is an exl2 model, you have to load it with an exllamav2 loader.'
  created_at: 2024-01-09 21:02:13+00:00
  edited: false
  hidden: false
  id: 659db455912b75e2c34f9767
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/Yi-34B-200K-DARE-merge-v5-2.67bpw-exl2-fiction
repo_type: model
status: open
target_branch: null
title: Did you upload the wrong model.safetensors.index.json?
