!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sinchir0
conflicting_files: null
created_at: 2023-05-11 12:12:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30c69048a849bd3c56551f2c3af4035d.svg
      fullname: shinichiro saito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sinchir0
      type: user
    createdAt: '2023-05-11T13:12:32.000Z'
    data:
      edited: true
      editors:
      - sinchir0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30c69048a849bd3c56551f2c3af4035d.svg
          fullname: shinichiro saito
          isHf: false
          isPro: false
          name: sinchir0
          type: user
        html: '<p>I am trying to do fine-tuning google/deplot according to the link
          and Notebook below.</p>

          <p>link: <a href="https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning">https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning</a><br>Notebook:
          <a rel="nofollow" href="https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb">https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb</a></p>

          <p>What form of text should be given as input data table for fine-tuning
          deplot?<br>From the figure 1 of <a rel="nofollow" href="https://arxiv.org/abs/2212.10505">paper</a>,
          I think that the text format is as follows</p>

          <pre><code>text = """

          Header: models | augmented-set | human-set

          Row 1: VisionTapas |67.2 | 22.2

          Row 2: Pix2Struct |82.9 | 30.4

          """

          </code></pre>

          <p>Is this correct?</p>

          <p>On the <a rel="nofollow" href="https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb">fine-tuning
          notebook</a>, I think that the above data will be placed in <code>texts</code>
          of the following code.(in <code>collator </code> functions)</p>

          <pre><code> text_inputs = processor(text=texts, padding="max_length", return_tensors="pt",
          add_special_tokens=True, max_length=20)

          </code></pre>

          '
        raw: "I am trying to do fine-tuning google/deplot according to the link and\
          \ Notebook below.\n\nlink: https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning\n\
          Notebook: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb\n\
          \nWhat form of text should be given as input data table for fine-tuning\
          \ deplot?\nFrom the figure 1 of [paper](https://arxiv.org/abs/2212.10505),\
          \ I think that the text format is as follows\n```\ntext = \"\"\"\nHeader:\
          \ models | augmented-set | human-set\nRow 1: VisionTapas |67.2 | 22.2\n\
          Row 2: Pix2Struct |82.9 | 30.4\n\"\"\"\n```\nIs this correct?\n\nOn the\
          \ [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb),\
          \ I think that the above data will be placed in `texts` of the following\
          \ code.(in `collator ` functions)\n\n```\n text_inputs = processor(text=texts,\
          \ padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True,\
          \ max_length=20)\n```"
        updatedAt: '2023-05-11T13:14:30.102Z'
      numEdits: 3
      reactions: []
    id: 645ce9c0f5760d1530d34009
    type: comment
  author: sinchir0
  content: "I am trying to do fine-tuning google/deplot according to the link and\
    \ Notebook below.\n\nlink: https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning\n\
    Notebook: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb\n\
    \nWhat form of text should be given as input data table for fine-tuning deplot?\n\
    From the figure 1 of [paper](https://arxiv.org/abs/2212.10505), I think that the\
    \ text format is as follows\n```\ntext = \"\"\"\nHeader: models | augmented-set\
    \ | human-set\nRow 1: VisionTapas |67.2 | 22.2\nRow 2: Pix2Struct |82.9 | 30.4\n\
    \"\"\"\n```\nIs this correct?\n\nOn the [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb),\
    \ I think that the above data will be placed in `texts` of the following code.(in\
    \ `collator ` functions)\n\n```\n text_inputs = processor(text=texts, padding=\"\
    max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=20)\n\
    ```"
  created_at: 2023-05-11 12:12:32+00:00
  edited: true
  hidden: false
  id: 645ce9c0f5760d1530d34009
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f881856ee5616341bc51e67/9UCZCuhBTpJC9tGPyGmMb.jpeg?w=200&h=200&f=face
      fullname: Fangyu Liu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: fl399
      type: user
    createdAt: '2023-05-11T17:01:05.000Z'
    data:
      edited: false
      editors:
      - fl399
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f881856ee5616341bc51e67/9UCZCuhBTpJC9tGPyGmMb.jpeg?w=200&h=200&f=face
          fullname: Fangyu Liu
          isHf: false
          isPro: false
          name: fl399
          type: user
        html: "<p>Hi, thanks for the questions.</p>\n<p>We used the following format\
          \ for groudtruth:</p>\n<pre><code>models | augmented-set | human-set \n\
          VisionTapas | 67.2 | 22.2\nPix2Struct | 82.9 | 30.4\n</code></pre>\n<p>The\
          \ <code>Header:</code> and <code>Row x:</code> labels are added using a\
          \ post-processing function (see <a href=\"https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L24\"\
          >here</a>). Or you can see <a href=\"https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L39\"\
          >this</a> as an example groudtruth table.</p>\n<p>And yes the groudtruths\
          \ should be put into <code>texts</code> in the <code>collator</code> function.</p>\n\
          <p>Hope this helps!</p>\n"
        raw: "Hi, thanks for the questions.\n\nWe used the following format for groudtruth:\n\
          ```\nmodels | augmented-set | human-set \nVisionTapas | 67.2 | 22.2\nPix2Struct\
          \ | 82.9 | 30.4\n```\nThe `Header:` and `Row x:` labels are added using\
          \ a post-processing function (see [here](https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L24)).\
          \ Or you can see [this](https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L39)\
          \ as an example groudtruth table.\n\nAnd yes the groudtruths should be put\
          \ into `texts` in the `collator` function.\n\nHope this helps!"
        updatedAt: '2023-05-11T17:01:05.641Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - sinchir0
        - DoctorSlimm
    id: 645d1f514435a8ae3fc6bd22
    type: comment
  author: fl399
  content: "Hi, thanks for the questions.\n\nWe used the following format for groudtruth:\n\
    ```\nmodels | augmented-set | human-set \nVisionTapas | 67.2 | 22.2\nPix2Struct\
    \ | 82.9 | 30.4\n```\nThe `Header:` and `Row x:` labels are added using a post-processing\
    \ function (see [here](https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L24)).\
    \ Or you can see [this](https://huggingface.co/spaces/fl399/deplot_plus_llm/blob/a82c90c37bd452250319484a5e6a1208bea31972/app.py#L39)\
    \ as an example groudtruth table.\n\nAnd yes the groudtruths should be put into\
    \ `texts` in the `collator` function.\n\nHope this helps!"
  created_at: 2023-05-11 16:01:05+00:00
  edited: false
  hidden: false
  id: 645d1f514435a8ae3fc6bd22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f881856ee5616341bc51e67/9UCZCuhBTpJC9tGPyGmMb.jpeg?w=200&h=200&f=face
      fullname: Fangyu Liu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: fl399
      type: user
    createdAt: '2023-05-12T08:45:33.000Z'
    data:
      status: closed
    id: 645dfcadbc87e52e9ab48754
    type: status-change
  author: fl399
  created_at: 2023-05-12 07:45:33+00:00
  id: 645dfcadbc87e52e9ab48754
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30c69048a849bd3c56551f2c3af4035d.svg
      fullname: shinichiro saito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sinchir0
      type: user
    createdAt: '2023-05-14T07:01:55.000Z'
    data:
      edited: false
      editors:
      - sinchir0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30c69048a849bd3c56551f2c3af4035d.svg
          fullname: shinichiro saito
          isHf: false
          isPro: false
          name: sinchir0
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fl399&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/fl399\">@<span class=\"\
          underline\">fl399</span></a></span>\n\n\t</span></span><br>I understand,\
          \ thank you so much!</p>\n"
        raw: "@fl399 \nI understand, thank you so much!"
        updatedAt: '2023-05-14T07:01:55.603Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - fl399
    id: 64608763bf985b8b4eb3215d
    type: comment
  author: sinchir0
  content: "@fl399 \nI understand, thank you so much!"
  created_at: 2023-05-14 06:01:55+00:00
  edited: false
  hidden: false
  id: 64608763bf985b8b4eb3215d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6179126785e44b00ef53ed06/xjp94XueMNJCMysnCmQpo.png?w=200&h=200&f=face
      fullname: drslimm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: DoctorSlimm
      type: user
    createdAt: '2023-08-13T19:15:28.000Z'
    data:
      edited: false
      editors:
      - DoctorSlimm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8467885851860046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6179126785e44b00ef53ed06/xjp94XueMNJCMysnCmQpo.png?w=200&h=200&f=face
          fullname: drslimm
          isHf: false
          isPro: true
          name: DoctorSlimm
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fl399&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/fl399\">@<span class=\"\
          underline\">fl399</span></a></span>\n\n\t</span></span> Hi! In regards Fine\
          \ Tuning and The Text PreProcessing, Are \\n newline character by default\
          \ converted into &lt;0x0A&gt;, or do we need to do this ourselves before\
          \ passing the text into the tokenizer?</p>\n"
        raw: '@fl399 Hi! In regards Fine Tuning and The Text PreProcessing, Are \n
          newline character by default converted into <0x0A>, or do we need to do
          this ourselves before passing the text into the tokenizer?'
        updatedAt: '2023-08-13T19:15:28.566Z'
      numEdits: 0
      reactions: []
    id: 64d92bd03ca2924d6eb9cd9a
    type: comment
  author: DoctorSlimm
  content: '@fl399 Hi! In regards Fine Tuning and The Text PreProcessing, Are \n newline
    character by default converted into <0x0A>, or do we need to do this ourselves
    before passing the text into the tokenizer?'
  created_at: 2023-08-13 18:15:28+00:00
  edited: false
  hidden: false
  id: 64d92bd03ca2924d6eb9cd9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-08-18T03:21:45.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9309802055358887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fl399&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/fl399\">@<span class=\"\
          underline\">fl399</span></a></span>\n\n\t</span></span>  or <span data-props=\"\
          {&quot;user&quot;:&quot;sinchir0&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/sinchir0\">@<span class=\"underline\">sinchir0</span></a></span>\n\
          \n\t</span></span><br>Please could you share the changes needed for <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb\"\
          >image_captioning_pix2struct.ipynb</a>?  I've spent the better part of an\
          \ afternoon / evening working with Colab on solving this and allowing it\
          \ to fine tune.  I've shared the paper, the notebook, and this discussion\
          \ with Anthropic's Claude and tried with Google's Colab LLM but I can't\
          \ get it working.  Please share with us what we need to change in the cells\
          \ of the notebook, specifically the ImageCaptioningDataset class, the collator,\
          \ and anything else?  I, and many others, would be so grateful for this,\
          \ please \U0001F64F\U0001F64F</p>\n"
        raw: "@fl399  or @sinchir0 \nPlease could you share the changes needed for\
          \ [image_captioning_pix2struct.ipynb](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)?\
          \  I've spent the better part of an afternoon / evening working with Colab\
          \ on solving this and allowing it to fine tune.  I've shared the paper,\
          \ the notebook, and this discussion with Anthropic's Claude and tried with\
          \ Google's Colab LLM but I can't get it working.  Please share with us what\
          \ we need to change in the cells of the notebook, specifically the ImageCaptioningDataset\
          \ class, the collator, and anything else?  I, and many others, would be\
          \ so grateful for this, please \U0001F64F\U0001F64F"
        updatedAt: '2023-08-18T03:21:45.817Z'
      numEdits: 0
      reactions: []
    id: 64dee3c9d1885643ca3892e1
    type: comment
  author: kronosprime
  content: "@fl399  or @sinchir0 \nPlease could you share the changes needed for [image_captioning_pix2struct.ipynb](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)?\
    \  I've spent the better part of an afternoon / evening working with Colab on\
    \ solving this and allowing it to fine tune.  I've shared the paper, the notebook,\
    \ and this discussion with Anthropic's Claude and tried with Google's Colab LLM\
    \ but I can't get it working.  Please share with us what we need to change in\
    \ the cells of the notebook, specifically the ImageCaptioningDataset class, the\
    \ collator, and anything else?  I, and many others, would be so grateful for this,\
    \ please \U0001F64F\U0001F64F"
  created_at: 2023-08-18 02:21:45+00:00
  edited: false
  hidden: false
  id: 64dee3c9d1885643ca3892e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-08-21T02:23:45.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5915154814720154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: "<p>The challenge is that we're using a different processor, initialized\
          \ with:<br>processor = Pix2StructProcessor.from_pretrained(\"google/deplot\"\
          )</p>\n<p>If we reference the <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning\"\
          >model card fine-tuning section</a><br>we see the example for using the\
          \ processor as:<br><code>inputs = processor(images=images, text=\"Generate\
          \ underlying data table of the figure below:\", return_tensors=\"pt\")</code></p>\n\
          <p>And we are pointed to the <a rel=\"nofollow\" href=\"https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb\"\
          >image_captioning_pix2struct notebook</a></p>\n<p>The code below is from\
          \ the original notebook where \"text\" is essentially the text-based label/answer\
          \ for the \"image\", but can we get the remaining updates so at least the\
          \ example will work?  I have fiddled with it until it is training but left\
          \ with a lot of regret and uncertainty that it's making progress.  I'd like\
          \ to know the code is implemented correctly before dedicating an A100 for\
          \ a few hours on the job.  </p>\n<pre><code>from torch.utils.data import\
          \ Dataset, DataLoader\n\nMAX_PATCHES = 1024\n\nclass ImageCaptioningDataset(Dataset):\n\
          \    def __init__(self, dataset, processor):\n        self.dataset = dataset\n\
          \        self.processor = processor\n\n    def __len__(self):\n        return\
          \ len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n\
          \        encoding = self.processor(images=item[\"image\"], return_tensors=\"\
          pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n        \n    \
          \    encoding = {k:v.squeeze() for k,v in encoding.items()}\n        encoding[\"\
          text\"] = item[\"text\"]\n        return encoding\n</code></pre>\n<pre><code>def\
          \ collator(batch):\n  new_batch = {\"flattened_patches\":[], \"attention_mask\"\
          :[]}\n  texts = [item[\"text\"] for item in batch]\n  \n  text_inputs =\
          \ processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True,\
          \ max_length=20)\n  \n  new_batch[\"labels\"] = text_inputs.input_ids\n\
          \  \n  for item in batch:\n    new_batch[\"flattened_patches\"].append(item[\"\
          flattened_patches\"])\n    new_batch[\"attention_mask\"].append(item[\"\
          attention_mask\"])\n  \n  new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"\
          flattened_patches\"])\n  new_batch[\"attention_mask\"] = torch.stack(new_batch[\"\
          attention_mask\"])\n\n  return new_batch\n</code></pre>\n"
        raw: "The challenge is that we're using a different processor, initialized\
          \ with:\nprocessor = Pix2StructProcessor.from_pretrained(\"google/deplot\"\
          )\n\nIf we reference the [model card fine-tuning section](https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning)\n\
          we see the example for using the processor as:\n`inputs = processor(images=images,\
          \ text=\"Generate underlying data table of the figure below:\", return_tensors=\"\
          pt\")`\n\nAnd we are pointed to the [image_captioning_pix2struct notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)\n\
          \nThe code below is from the original notebook where \"text\" is essentially\
          \ the text-based label/answer for the \"image\", but can we get the remaining\
          \ updates so at least the example will work?  I have fiddled with it until\
          \ it is training but left with a lot of regret and uncertainty that it's\
          \ making progress.  I'd like to know the code is implemented correctly before\
          \ dedicating an A100 for a few hours on the job.  \n\n```\nfrom torch.utils.data\
          \ import Dataset, DataLoader\n\nMAX_PATCHES = 1024\n\nclass ImageCaptioningDataset(Dataset):\n\
          \    def __init__(self, dataset, processor):\n        self.dataset = dataset\n\
          \        self.processor = processor\n\n    def __len__(self):\n        return\
          \ len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n\
          \        encoding = self.processor(images=item[\"image\"], return_tensors=\"\
          pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n        \n    \
          \    encoding = {k:v.squeeze() for k,v in encoding.items()}\n        encoding[\"\
          text\"] = item[\"text\"]\n        return encoding\n```\n\n```\ndef collator(batch):\n\
          \  new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n  texts\
          \ = [item[\"text\"] for item in batch]\n  \n  text_inputs = processor(text=texts,\
          \ padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True,\
          \ max_length=20)\n  \n  new_batch[\"labels\"] = text_inputs.input_ids\n\
          \  \n  for item in batch:\n    new_batch[\"flattened_patches\"].append(item[\"\
          flattened_patches\"])\n    new_batch[\"attention_mask\"].append(item[\"\
          attention_mask\"])\n  \n  new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"\
          flattened_patches\"])\n  new_batch[\"attention_mask\"] = torch.stack(new_batch[\"\
          attention_mask\"])\n\n  return new_batch\n```\n\n"
        updatedAt: '2023-08-21T02:23:45.223Z'
      numEdits: 0
      reactions: []
    id: 64e2cab193103b4537cd8ca8
    type: comment
  author: kronosprime
  content: "The challenge is that we're using a different processor, initialized with:\n\
    processor = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n\nIf we reference\
    \ the [model card fine-tuning section](https://huggingface.co/docs/transformers/main/en/model_doc/deplot#finetuning)\n\
    we see the example for using the processor as:\n`inputs = processor(images=images,\
    \ text=\"Generate underlying data table of the figure below:\", return_tensors=\"\
    pt\")`\n\nAnd we are pointed to the [image_captioning_pix2struct notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)\n\
    \nThe code below is from the original notebook where \"text\" is essentially the\
    \ text-based label/answer for the \"image\", but can we get the remaining updates\
    \ so at least the example will work?  I have fiddled with it until it is training\
    \ but left with a lot of regret and uncertainty that it's making progress.  I'd\
    \ like to know the code is implemented correctly before dedicating an A100 for\
    \ a few hours on the job.  \n\n```\nfrom torch.utils.data import Dataset, DataLoader\n\
    \nMAX_PATCHES = 1024\n\nclass ImageCaptioningDataset(Dataset):\n    def __init__(self,\
    \ dataset, processor):\n        self.dataset = dataset\n        self.processor\
    \ = processor\n\n    def __len__(self):\n        return len(self.dataset)\n\n\
    \    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        encoding\
    \ = self.processor(images=item[\"image\"], return_tensors=\"pt\", add_special_tokens=True,\
    \ max_patches=MAX_PATCHES)\n        \n        encoding = {k:v.squeeze() for k,v\
    \ in encoding.items()}\n        encoding[\"text\"] = item[\"text\"]\n        return\
    \ encoding\n```\n\n```\ndef collator(batch):\n  new_batch = {\"flattened_patches\"\
    :[], \"attention_mask\":[]}\n  texts = [item[\"text\"] for item in batch]\n  \n\
    \  text_inputs = processor(text=texts, padding=\"max_length\", return_tensors=\"\
    pt\", add_special_tokens=True, max_length=20)\n  \n  new_batch[\"labels\"] = text_inputs.input_ids\n\
    \  \n  for item in batch:\n    new_batch[\"flattened_patches\"].append(item[\"\
    flattened_patches\"])\n    new_batch[\"attention_mask\"].append(item[\"attention_mask\"\
    ])\n  \n  new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"\
    ])\n  new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"\
    ])\n\n  return new_batch\n```\n\n"
  created_at: 2023-08-21 01:23:45+00:00
  edited: false
  hidden: false
  id: 64e2cab193103b4537cd8ca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-08-30T03:43:12.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396926760673523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: '<p>Anyone?</p>

          '
        raw: Anyone?
        updatedAt: '2023-08-30T03:43:12.446Z'
      numEdits: 0
      reactions: []
    id: 64eebad0c670e08b13ae8d25
    type: comment
  author: kronosprime
  content: Anyone?
  created_at: 2023-08-30 02:43:12+00:00
  edited: false
  hidden: false
  id: 64eebad0c670e08b13ae8d25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-09-17T16:55:54.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396926760673523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: '<p>Anyone?</p>

          '
        raw: Anyone?
        updatedAt: '2023-09-17T16:55:54.953Z'
      numEdits: 0
      reactions: []
    id: 65072f9a67876ea3202644d6
    type: comment
  author: kronosprime
  content: Anyone?
  created_at: 2023-09-17 15:55:54+00:00
  edited: false
  hidden: false
  id: 65072f9a67876ea3202644d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/010f514f01815a1067ff64686110a97a.svg
      fullname: SungBeomChoi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SungBeom
      type: user
    createdAt: '2023-10-11T07:51:32.000Z'
    data:
      edited: false
      editors:
      - SungBeom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2844703197479248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/010f514f01815a1067ff64686110a97a.svg
          fullname: SungBeomChoi
          isHf: false
          isPro: false
          name: SungBeom
          type: user
        html: '<p>You need to change below line<br>--------------------------- before
          ------------------------------------<br>text_inputs = processor(text=texts,
          padding="max_length", return_tensors="pt", add_special_tokens=True, max_length=20)<br>---------------------------
          after ------------------------------------<br>text_inputs = processor.tokenizer(text=texts,
          padding="max_length", return_tensors="pt", add_special_tokens=True, max_length=20)</p>

          '
        raw: 'You need to change below line

          --------------------------- before ------------------------------------

          text_inputs = processor(text=texts, padding="max_length", return_tensors="pt",
          add_special_tokens=True, max_length=20)

          --------------------------- after ------------------------------------

          text_inputs = processor.tokenizer(text=texts, padding="max_length", return_tensors="pt",
          add_special_tokens=True, max_length=20)'
        updatedAt: '2023-10-11T07:51:32.149Z'
      numEdits: 0
      reactions: []
    id: 6526540469946cd5aa3d855c
    type: comment
  author: SungBeom
  content: 'You need to change below line

    --------------------------- before ------------------------------------

    text_inputs = processor(text=texts, padding="max_length", return_tensors="pt",
    add_special_tokens=True, max_length=20)

    --------------------------- after ------------------------------------

    text_inputs = processor.tokenizer(text=texts, padding="max_length", return_tensors="pt",
    add_special_tokens=True, max_length=20)'
  created_at: 2023-10-11 06:51:32+00:00
  edited: false
  hidden: false
  id: 6526540469946cd5aa3d855c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-10-24T13:19:51.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33232954144477844
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: "<p>thank you <span data-props=\"{&quot;user&quot;:&quot;SungBeom&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SungBeom\"\
          >@<span class=\"underline\">SungBeom</span></a></span>\n\n\t</span></span>\
          \  \U0001F64F</p>\n"
        raw: "thank you @SungBeom  \U0001F64F"
        updatedAt: '2023-10-24T13:19:51.591Z'
      numEdits: 0
      reactions: []
    id: 6537c477b33b913282cdbf6e
    type: comment
  author: kronosprime
  content: "thank you @SungBeom  \U0001F64F"
  created_at: 2023-10-24 12:19:51+00:00
  edited: false
  hidden: false
  id: 6537c477b33b913282cdbf6e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: google/deplot
repo_type: model
status: closed
target_branch: null
title: For fine-tuning deplot, what form of text should be given as input data table?
