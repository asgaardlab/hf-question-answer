!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DoctorSlimm
conflicting_files: null
created_at: 2023-08-16 20:19:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6179126785e44b00ef53ed06/xjp94XueMNJCMysnCmQpo.png?w=200&h=200&f=face
      fullname: drslimm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: DoctorSlimm
      type: user
    createdAt: '2023-08-16T21:19:07.000Z'
    data:
      edited: false
      editors:
      - DoctorSlimm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416688680648804
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6179126785e44b00ef53ed06/xjp94XueMNJCMysnCmQpo.png?w=200&h=200&f=face
          fullname: drslimm
          isHf: false
          isPro: true
          name: DoctorSlimm
          type: user
        html: '<p>Noob Here.</p>

          <p>Fine Tuning and Both the Train AND Validation Loss Drop to Zero Extremely
          Quickly (within ~100 steps).  </p>

          <p>What Loss Function is the default inside this model? I have been trying
          to fine tune it , and the loss very quickly falls to zero (within one epoch
          or so) whilst the validation metrics improve and then flatline. </p>

          <p>I am concerned that the loss function may well suited to my Metric (Rouge
          L)? </p>

          <p>Fine Tuning On Synthetically generated Chart-Table Pairs of Data which
          would explain why both validation and training loss  drop to zero so quickly
          as they learn the distribution very easily, though does not explain why
          the Rouge Metric does not also Reach 1 When the Loss is pretty much zero
          for a handful of epochs. </p>

          <p>Was Using Adam Optimizer with PyTorch Lightning, as When I tried to use
          AdaFactor with cosine scheduler I got a tonne of errors.</p>

          <p>Am I getting stuck in a Local Minimum? And Should I work harder on integrating
          AdaFactor and Cosine Scheduling?</p>

          '
        raw: "Noob Here.\r\n\r\nFine Tuning and Both the Train AND Validation Loss\
          \ Drop to Zero Extremely Quickly (within ~100 steps).  \r\n\r\nWhat Loss\
          \ Function is the default inside this model? I have been trying to fine\
          \ tune it , and the loss very quickly falls to zero (within one epoch or\
          \ so) whilst the validation metrics improve and then flatline. \r\n\r\n\
          I am concerned that the loss function may well suited to my Metric (Rouge\
          \ L)? \r\n\r\nFine Tuning On Synthetically generated Chart-Table Pairs of\
          \ Data which would explain why both validation and training loss  drop to\
          \ zero so quickly as they learn the distribution very easily, though does\
          \ not explain why the Rouge Metric does not also Reach 1 When the Loss is\
          \ pretty much zero for a handful of epochs. \r\n\r\nWas Using Adam Optimizer\
          \ with PyTorch Lightning, as When I tried to use AdaFactor with cosine scheduler\
          \ I got a tonne of errors.\r\n\r\n\r\nAm I getting stuck in a Local Minimum?\
          \ And Should I work harder on integrating AdaFactor and Cosine Scheduling?"
        updatedAt: '2023-08-16T21:19:07.762Z'
      numEdits: 0
      reactions: []
    id: 64dd3d4b5f21fb7f85b479b0
    type: comment
  author: DoctorSlimm
  content: "Noob Here.\r\n\r\nFine Tuning and Both the Train AND Validation Loss Drop\
    \ to Zero Extremely Quickly (within ~100 steps).  \r\n\r\nWhat Loss Function is\
    \ the default inside this model? I have been trying to fine tune it , and the\
    \ loss very quickly falls to zero (within one epoch or so) whilst the validation\
    \ metrics improve and then flatline. \r\n\r\nI am concerned that the loss function\
    \ may well suited to my Metric (Rouge L)? \r\n\r\nFine Tuning On Synthetically\
    \ generated Chart-Table Pairs of Data which would explain why both validation\
    \ and training loss  drop to zero so quickly as they learn the distribution very\
    \ easily, though does not explain why the Rouge Metric does not also Reach 1 When\
    \ the Loss is pretty much zero for a handful of epochs. \r\n\r\nWas Using Adam\
    \ Optimizer with PyTorch Lightning, as When I tried to use AdaFactor with cosine\
    \ scheduler I got a tonne of errors.\r\n\r\n\r\nAm I getting stuck in a Local\
    \ Minimum? And Should I work harder on integrating AdaFactor and Cosine Scheduling?"
  created_at: 2023-08-16 20:19:07+00:00
  edited: false
  hidden: false
  id: 64dd3d4b5f21fb7f85b479b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: google/deplot
repo_type: model
status: open
target_branch: null
title: Loss Function Configuration
