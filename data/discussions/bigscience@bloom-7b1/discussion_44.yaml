!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TTCoding
conflicting_files: null
created_at: 2023-03-19 15:27:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6753fa275a33c7f3ad9b8c6927f3fd2.svg
      fullname: XB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TTCoding
      type: user
    createdAt: '2023-03-19T16:27:48.000Z'
    data:
      edited: false
      editors:
      - TTCoding
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6753fa275a33c7f3ad9b8c6927f3fd2.svg
          fullname: XB
          isHf: false
          isPro: false
          name: TTCoding
          type: user
        html: '<p>I have tried many configurations the FT 7b1 on four A100. But unfortunately,  I
          got OOM all the time. So I am curious about minimal demands of GPUS to FT
          this model. Could you share your experiences?</p>

          '
        raw: I have tried many configurations the FT 7b1 on four A100. But unfortunately,  I
          got OOM all the time. So I am curious about minimal demands of GPUS to FT
          this model. Could you share your experiences?
        updatedAt: '2023-03-19T16:27:48.567Z'
      numEdits: 0
      reactions: []
    id: 64173804249360df28d9d319
    type: comment
  author: TTCoding
  content: I have tried many configurations the FT 7b1 on four A100. But unfortunately,  I
    got OOM all the time. So I am curious about minimal demands of GPUS to FT this
    model. Could you share your experiences?
  created_at: 2023-03-19 15:27:48+00:00
  edited: false
  hidden: false
  id: 64173804249360df28d9d319
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56e741f42333bbb50c02b2bc6d9dd5e7.svg
      fullname: Hatim Bourfoune
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hatimbr
      type: user
    createdAt: '2023-03-20T13:29:48.000Z'
    data:
      edited: false
      editors:
      - hatimbr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56e741f42333bbb50c02b2bc6d9dd5e7.svg
          fullname: Hatim Bourfoune
          isHf: false
          isPro: false
          name: hatimbr
          type: user
        html: '<p>If you freeze some layers, even in 1 A100 it is possible.<br>Check
          this: <a rel="nofollow" href="https://gitlab.inria.fr/synalp/plm4all/-/tree/main/finetune_accelerate">https://gitlab.inria.fr/synalp/plm4all/-/tree/main/finetune_accelerate</a><br>It
          is still a draft but it''s running.</p>

          '
        raw: 'If you freeze some layers, even in 1 A100 it is possible.

          Check this: https://gitlab.inria.fr/synalp/plm4all/-/tree/main/finetune_accelerate

          It is still a draft but it''s running.'
        updatedAt: '2023-03-20T13:29:48.386Z'
      numEdits: 0
      reactions: []
    id: 64185fcc7818018e3e46be26
    type: comment
  author: hatimbr
  content: 'If you freeze some layers, even in 1 A100 it is possible.

    Check this: https://gitlab.inria.fr/synalp/plm4all/-/tree/main/finetune_accelerate

    It is still a draft but it''s running.'
  created_at: 2023-03-20 12:29:48+00:00
  edited: false
  hidden: false
  id: 64185fcc7818018e3e46be26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d8a1af9f8f22e18315bcd50f590caf7.svg
      fullname: zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: redauzhang
      type: user
    createdAt: '2023-06-12T11:20:21.000Z'
    data:
      edited: false
      editors:
      - redauzhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8168273568153381
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d8a1af9f8f22e18315bcd50f590caf7.svg
          fullname: zhang
          isHf: false
          isPro: false
          name: redauzhang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hatimbr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hatimbr\">@<span class=\"\
          underline\">hatimbr</span></a></span>\n\n\t</span></span> hi, how to transform\
          \ the quat <code>fp32</code> model to <code>fp16</code>, and then can i\
          \ ft it with 24g RTX3090ti for fp16?<br>sure, this model-fit is fp32 of\
          \ quantitated\uFF1F</p>\n"
        raw: "@hatimbr hi, how to transform the quat `fp32` model to `fp16`, and then\
          \ can i ft it with 24g RTX3090ti for fp16?\nsure, this model-fit is fp32\
          \ of quantitated\uFF1F"
        updatedAt: '2023-06-12T11:20:21.325Z'
      numEdits: 0
      reactions: []
    id: 6486ff7536feccd67914269d
    type: comment
  author: redauzhang
  content: "@hatimbr hi, how to transform the quat `fp32` model to `fp16`, and then\
    \ can i ft it with 24g RTX3090ti for fp16?\nsure, this model-fit is fp32 of quantitated\uFF1F"
  created_at: 2023-06-12 10:20:21+00:00
  edited: false
  hidden: false
  id: 6486ff7536feccd67914269d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56e741f42333bbb50c02b2bc6d9dd5e7.svg
      fullname: Hatim Bourfoune
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hatimbr
      type: user
    createdAt: '2023-06-16T08:18:36.000Z'
    data:
      edited: false
      editors:
      - hatimbr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43599629402160645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56e741f42333bbb50c02b2bc6d9dd5e7.svg
          fullname: Hatim Bourfoune
          isHf: false
          isPro: false
          name: hatimbr
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;redauzhang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/redauzhang\"\
          >@<span class=\"underline\">redauzhang</span></a></span>\n\n\t</span></span>\
          \  you can pass the parameter torch_dtype=torch.float16 (or even better\
          \ torch_dtype=torch.bfloat16) in the from_pretrained method.</p>\n"
        raw: hi @redauzhang  you can pass the parameter torch_dtype=torch.float16
          (or even better torch_dtype=torch.bfloat16) in the from_pretrained method.
        updatedAt: '2023-06-16T08:18:36.272Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FreakingPotato
    id: 648c1adc96bc03fd03679dfe
    type: comment
  author: hatimbr
  content: hi @redauzhang  you can pass the parameter torch_dtype=torch.float16 (or
    even better torch_dtype=torch.bfloat16) in the from_pretrained method.
  created_at: 2023-06-16 07:18:36+00:00
  edited: false
  hidden: false
  id: 648c1adc96bc03fd03679dfe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: bigscience/bloom-7b1
repo_type: model
status: open
target_branch: null
title: Is it possible to fine-tuning the 7b1 model on 4 A100 (80G) gpus?
