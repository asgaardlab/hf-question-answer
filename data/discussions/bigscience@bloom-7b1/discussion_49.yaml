!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Saugatkafley
conflicting_files: null
created_at: 2023-10-10 16:00:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ca45c90609f1def7e2775a/mlxL5CKq0z9obRKKG_P-o.png?w=200&h=200&f=face
      fullname: Saugat Kafley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saugatkafley
      type: user
    createdAt: '2023-10-10T17:00:45.000Z'
    data:
      edited: false
      editors:
      - Saugatkafley
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45997047424316406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ca45c90609f1def7e2775a/mlxL5CKq0z9obRKKG_P-o.png?w=200&h=200&f=face
          fullname: Saugat Kafley
          isHf: false
          isPro: false
          name: Saugatkafley
          type: user
        html: "<p>I wanted to know the correct prompt formatting for <code>bloom-7b1</code>.\
          \  I have tried some of the prompts :\\n<br>Eg: </p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> time\nstart\
          \ = time.time()\nquery=<span class=\"hljs-string\">\"\"\"Given a question\
          \ delimitted by triplebackticks. \\nUser: ```\u0915\u0947 \u0906\u092B\u0942\
          \u0932\u0947 \u092E\u0928 \u092A\u0930\u093E\u090F\u0915\u094B \u0935\u094D\
          \u092F\u0915\u094D\u0924\u093F\u0932\u0947 \u0930\u093E\u0916\u0947\u0915\
          \u094B \u092F\u094C\u0928 \u0938\u092E\u094D\u092A\u0930\u094D\u0915\u0915\
          \u094B \u092E\u093E\u0917\u0932\u093E\u0908 \u0938\u094D\u0935\u0940\u0915\
          \u093E\u0930\u094D\u0928\u0941 \u0920\u0940\u0915 \u0939\u094B ?```\\nAssistant:\"\
          \"\"</span>\n\ninputs = tokenizer.encode(query , return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>).to(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          outputs = model.generate(inputs , max_new_tokens = <span class=\"hljs-number\"\
          >200</span> , temperature = <span class=\"hljs-number\">0.5</span>, do_sample=<span\
          \ class=\"hljs-literal\">True</span>)\nans = tokenizer.batch_decode(outputs\
          \ , skip_special_tokens = <span class=\"hljs-literal\">True</span>)[<span\
          \ class=\"hljs-number\">0</span>]\n<span class=\"hljs-built_in\">print</span>(\
          \ ans[<span class=\"hljs-built_in\">len</span>(query):])\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"inferecne time\"\
          </span>,  time.time() - start)\n</code></pre>\n<p>The answer extremely hallucinates.\
          \  What can be done?</p>\n"
        raw: "I wanted to know the correct prompt formatting for `bloom-7b1`.  I have\
          \ tried some of the prompts :\\n\r\nEg: \r\n```python\r\nimport time\r\n\
          start = time.time()\r\nquery=\"\"\"Given a question delimitted by triplebackticks.\
          \ \\nUser: ```\u0915\u0947 \u0906\u092B\u0942\u0932\u0947 \u092E\u0928 \u092A\
          \u0930\u093E\u090F\u0915\u094B \u0935\u094D\u092F\u0915\u094D\u0924\u093F\
          \u0932\u0947 \u0930\u093E\u0916\u0947\u0915\u094B \u092F\u094C\u0928 \u0938\
          \u092E\u094D\u092A\u0930\u094D\u0915\u0915\u094B \u092E\u093E\u0917\u0932\
          \u093E\u0908 \u0938\u094D\u0935\u0940\u0915\u093E\u0930\u094D\u0928\u0941\
          \ \u0920\u0940\u0915 \u0939\u094B ?```\\nAssistant:\"\"\"\r\n\r\ninputs\
          \ = tokenizer.encode(query , return_tensors=\"pt\").to(\"cuda\")\r\noutputs\
          \ = model.generate(inputs , max_new_tokens = 200 , temperature = 0.5, do_sample=True)\r\
          \nans = tokenizer.batch_decode(outputs , skip_special_tokens = True)[0]\r\
          \nprint( ans[len(query):])\r\nprint(\"inferecne time\",  time.time() - start)\r\
          \n```\r\nThe answer extremely hallucinates.  What can be done?\r\n"
        updatedAt: '2023-10-10T17:00:45.046Z'
      numEdits: 0
      reactions: []
    id: 6525833d095931e1065b0528
    type: comment
  author: Saugatkafley
  content: "I wanted to know the correct prompt formatting for `bloom-7b1`.  I have\
    \ tried some of the prompts :\\n\r\nEg: \r\n```python\r\nimport time\r\nstart\
    \ = time.time()\r\nquery=\"\"\"Given a question delimitted by triplebackticks.\
    \ \\nUser: ```\u0915\u0947 \u0906\u092B\u0942\u0932\u0947 \u092E\u0928 \u092A\u0930\
    \u093E\u090F\u0915\u094B \u0935\u094D\u092F\u0915\u094D\u0924\u093F\u0932\u0947\
    \ \u0930\u093E\u0916\u0947\u0915\u094B \u092F\u094C\u0928 \u0938\u092E\u094D\u092A\
    \u0930\u094D\u0915\u0915\u094B \u092E\u093E\u0917\u0932\u093E\u0908 \u0938\u094D\
    \u0935\u0940\u0915\u093E\u0930\u094D\u0928\u0941 \u0920\u0940\u0915 \u0939\u094B\
    \ ?```\\nAssistant:\"\"\"\r\n\r\ninputs = tokenizer.encode(query , return_tensors=\"\
    pt\").to(\"cuda\")\r\noutputs = model.generate(inputs , max_new_tokens = 200 ,\
    \ temperature = 0.5, do_sample=True)\r\nans = tokenizer.batch_decode(outputs ,\
    \ skip_special_tokens = True)[0]\r\nprint( ans[len(query):])\r\nprint(\"inferecne\
    \ time\",  time.time() - start)\r\n```\r\nThe answer extremely hallucinates. \
    \ What can be done?\r\n"
  created_at: 2023-10-10 16:00:45+00:00
  edited: false
  hidden: false
  id: 6525833d095931e1065b0528
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: bigscience/bloom-7b1
repo_type: model
status: open
target_branch: null
title: Correct Prompt format for Chat Response of bloom-7b1
