!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mitu820
conflicting_files: null
created_at: 2022-08-12 23:15:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e93c42e74175418d59857b621cfeca2d.svg
      fullname: Jahid Hashan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mitu820
      type: user
    createdAt: '2022-08-13T00:15:18.000Z'
    data:
      edited: false
      editors:
      - mitu820
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e93c42e74175418d59857b621cfeca2d.svg
          fullname: Jahid Hashan
          isHf: false
          isPro: false
          name: mitu820
          type: user
        html: '<p>Hi, I have an 8 Core Ryzen 7 desktop PC with 64GB Ram, I have an
          old 4GB GPU also. Is it possible to run this model on my PC? </p>

          <p>If yes then do we have any guide for this??</p>

          '
        raw: "Hi, I have an 8 Core Ryzen 7 desktop PC with 64GB Ram, I have an old\
          \ 4GB GPU also. Is it possible to run this model on my PC? \r\n\r\nIf yes\
          \ then do we have any guide for this??"
        updatedAt: '2022-08-13T00:15:18.962Z'
      numEdits: 0
      reactions: []
    id: 62f6ed16d3bdacb7eec30100
    type: comment
  author: mitu820
  content: "Hi, I have an 8 Core Ryzen 7 desktop PC with 64GB Ram, I have an old 4GB\
    \ GPU also. Is it possible to run this model on my PC? \r\n\r\nIf yes then do\
    \ we have any guide for this??"
  created_at: 2022-08-12 23:15:18+00:00
  edited: false
  hidden: false
  id: 62f6ed16d3bdacb7eec30100
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-08-13T12:44:11.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>You can try running it on your CPUs but it will be extremely slow.<br>If
          you want to run it on a single GPU, I''d recommend at least a 40GB GPU with
          FP16 support.</p>

          <p>There''s some inference information here for the 176B model: <a rel="nofollow"
          href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference</a><br>It
          should also be applicable to this model just that you need less memory &amp;
          FP16 instead of BF16.</p>

          '
        raw: 'You can try running it on your CPUs but it will be extremely slow.

          If you want to run it on a single GPU, I''d recommend at least a 40GB GPU
          with FP16 support.


          There''s some inference information here for the 176B model: https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference

          It should also be applicable to this model just that you need less memory
          & FP16 instead of BF16.'
        updatedAt: '2022-08-13T12:44:11.080Z'
      numEdits: 0
      reactions: []
    id: 62f79c9b5ad18b9403c91703
    type: comment
  author: Muennighoff
  content: 'You can try running it on your CPUs but it will be extremely slow.

    If you want to run it on a single GPU, I''d recommend at least a 40GB GPU with
    FP16 support.


    There''s some inference information here for the 176B model: https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference

    It should also be applicable to this model just that you need less memory & FP16
    instead of BF16.'
  created_at: 2022-08-13 11:44:11+00:00
  edited: false
  hidden: false
  id: 62f79c9b5ad18b9403c91703
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e93c42e74175418d59857b621cfeca2d.svg
      fullname: Jahid Hashan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mitu820
      type: user
    createdAt: '2022-08-13T14:10:44.000Z'
    data:
      edited: false
      editors:
      - mitu820
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e93c42e74175418d59857b621cfeca2d.svg
          fullname: Jahid Hashan
          isHf: false
          isPro: false
          name: mitu820
          type: user
        html: '<blockquote>

          <p>You can try running it on your CPUs but it will be extremely slow.<br>If
          you want to run it on a single GPU, I''d recommend at least a 40GB GPU with
          FP16 support.</p>

          <p>There''s some inference information here for the 176B model: <a rel="nofollow"
          href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference</a><br>It
          should also be applicable to this model just that you need less memory &amp;
          FP16 instead of BF16.</p>

          </blockquote>

          <p>Thank you, I thought 7b model takes low resources.</p>

          '
        raw: "> You can try running it on your CPUs but it will be extremely slow.\n\
          > If you want to run it on a single GPU, I'd recommend at least a 40GB GPU\
          \ with FP16 support.\n> \n> There's some inference information here for\
          \ the 176B model: https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference\n\
          > It should also be applicable to this model just that you need less memory\
          \ & FP16 instead of BF16.\n\nThank you, I thought 7b model takes low resources."
        updatedAt: '2022-08-13T14:10:44.051Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Muennighoff
    id: 62f7b0e4b9fda55613c9f6ff
    type: comment
  author: mitu820
  content: "> You can try running it on your CPUs but it will be extremely slow.\n\
    > If you want to run it on a single GPU, I'd recommend at least a 40GB GPU with\
    \ FP16 support.\n> \n> There's some inference information here for the 176B model:\
    \ https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference\n\
    > It should also be applicable to this model just that you need less memory &\
    \ FP16 instead of BF16.\n\nThank you, I thought 7b model takes low resources."
  created_at: 2022-08-13 13:10:44+00:00
  edited: false
  hidden: false
  id: 62f7b0e4b9fda55613c9f6ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
      fullname: Tom A.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: synthetisoft
      type: user
    createdAt: '2022-09-01T01:48:50.000Z'
    data:
      edited: false
      editors:
      - synthetisoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a124dd166e249801f4a08e118de26a19.svg
          fullname: Tom A.
          isHf: false
          isPro: false
          name: synthetisoft
          type: user
        html: '<blockquote>

          <p>You can try running it on your CPUs but it will be extremely slow.<br>If
          you want to run it on a single GPU, I''d recommend at least a 40GB GPU with
          FP16 support.</p>

          <p>There''s some inference information here for the 176B model: <a rel="nofollow"
          href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference</a><br>It
          should also be applicable to this model just that you need less memory &amp;
          FP16 instead of BF16.</p>

          </blockquote>

          <p>Is this the largest version I can do inference on using say, 4 x NVIDIA
          A10s? The total GPU memory would be 96GB. It would be nice to have a breakdown
          of all system requirements for each model.</p>

          '
        raw: "> You can try running it on your CPUs but it will be extremely slow.\n\
          > If you want to run it on a single GPU, I'd recommend at least a 40GB GPU\
          \ with FP16 support.\n> \n> There's some inference information here for\
          \ the 176B model: https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference\n\
          > It should also be applicable to this model just that you need less memory\
          \ & FP16 instead of BF16.\n\nIs this the largest version I can do inference\
          \ on using say, 4 x NVIDIA A10s? The total GPU memory would be 96GB. It\
          \ would be nice to have a breakdown of all system requirements for each\
          \ model."
        updatedAt: '2022-09-01T01:48:50.650Z'
      numEdits: 0
      reactions: []
    id: 63100f82e52a259b856831ad
    type: comment
  author: synthetisoft
  content: "> You can try running it on your CPUs but it will be extremely slow.\n\
    > If you want to run it on a single GPU, I'd recommend at least a 40GB GPU with\
    \ FP16 support.\n> \n> There's some inference information here for the 176B model:\
    \ https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/inference\n\
    > It should also be applicable to this model just that you need less memory &\
    \ FP16 instead of BF16.\n\nIs this the largest version I can do inference on using\
    \ say, 4 x NVIDIA A10s? The total GPU memory would be 96GB. It would be nice to\
    \ have a breakdown of all system requirements for each model."
  created_at: 2022-09-01 00:48:50+00:00
  edited: false
  hidden: false
  id: 63100f82e52a259b856831ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-09-01T08:27:46.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>You may be able to run the 176B by sacrificing performance or time,
          see <a href="https://huggingface.co/bigscience/bloom/discussions/87">https://huggingface.co/bigscience/bloom/discussions/87</a>
          or <a href="https://huggingface.co/bigscience/bloom/discussions/88">https://huggingface.co/bigscience/bloom/discussions/88</a></p>

          <p>The thing is there are no hard system requirements. It depends on how
          fast you want it to be and how much performance you''re willing to sacrifice
          (by e.g. reducing precision).</p>

          '
        raw: 'You may be able to run the 176B by sacrificing performance or time,
          see https://huggingface.co/bigscience/bloom/discussions/87 or https://huggingface.co/bigscience/bloom/discussions/88


          The thing is there are no hard system requirements. It depends on how fast
          you want it to be and how much performance you''re willing to sacrifice
          (by e.g. reducing precision).'
        updatedAt: '2022-09-01T08:27:46.956Z'
      numEdits: 0
      reactions: []
    id: 63106d028ed909cf34d09175
    type: comment
  author: Muennighoff
  content: 'You may be able to run the 176B by sacrificing performance or time, see
    https://huggingface.co/bigscience/bloom/discussions/87 or https://huggingface.co/bigscience/bloom/discussions/88


    The thing is there are no hard system requirements. It depends on how fast you
    want it to be and how much performance you''re willing to sacrifice (by e.g. reducing
    precision).'
  created_at: 2022-09-01 07:27:46+00:00
  edited: false
  hidden: false
  id: 63106d028ed909cf34d09175
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a007d9345926827a60bcc847f4a0870.svg
      fullname: TA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tailen
      type: user
    createdAt: '2022-12-19T11:51:18.000Z'
    data:
      edited: false
      editors:
      - Tailen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a007d9345926827a60bcc847f4a0870.svg
          fullname: TA
          isHf: false
          isPro: false
          name: Tailen
          type: user
        html: '<blockquote>

          <p>If you want to run it on a single GPU, I''d recommend at least a 40GB
          GPU with FP16 support.</p>

          </blockquote>

          <p>This is not true. I was able to run the 7b1 model in fp16 on my GPU with
          24GB VRAM.</p>

          '
        raw: '> If you want to run it on a single GPU, I''d recommend at least a 40GB
          GPU with FP16 support.


          This is not true. I was able to run the 7b1 model in fp16 on my GPU with
          24GB VRAM.'
        updatedAt: '2022-12-19T11:51:18.005Z'
      numEdits: 0
      reactions: []
    id: 63a05036f3334a6553d6ca93
    type: comment
  author: Tailen
  content: '> If you want to run it on a single GPU, I''d recommend at least a 40GB
    GPU with FP16 support.


    This is not true. I was able to run the 7b1 model in fp16 on my GPU with 24GB
    VRAM.'
  created_at: 2022-12-19 11:51:18+00:00
  edited: false
  hidden: false
  id: 63a05036f3334a6553d6ca93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-19T11:57:50.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<blockquote>\n<p>If you want to run it on a single GPU,\
          \ I'd recommend at least a 40GB GPU with FP16 support.</p>\n</blockquote>\n\
          <p>This is not true. I was able to run the 7b1 model in fp16 on my GPU with\
          \ 24GB VRAM.</p>\n</blockquote>\n<p>Very nice! Didn't say it wasn't possible\
          \ \U0001F44D</p>\n"
        raw: "> > If you want to run it on a single GPU, I'd recommend at least a\
          \ 40GB GPU with FP16 support.\n> \n> This is not true. I was able to run\
          \ the 7b1 model in fp16 on my GPU with 24GB VRAM.\n\nVery nice! Didn't say\
          \ it wasn't possible \U0001F44D"
        updatedAt: '2022-12-19T11:57:50.862Z'
      numEdits: 0
      reactions: []
    id: 63a051be0c6d9efa3060e44b
    type: comment
  author: Muennighoff
  content: "> > If you want to run it on a single GPU, I'd recommend at least a 40GB\
    \ GPU with FP16 support.\n> \n> This is not true. I was able to run the 7b1 model\
    \ in fp16 on my GPU with 24GB VRAM.\n\nVery nice! Didn't say it wasn't possible\
    \ \U0001F44D"
  created_at: 2022-12-19 11:57:50+00:00
  edited: false
  hidden: false
  id: 63a051be0c6d9efa3060e44b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a007d9345926827a60bcc847f4a0870.svg
      fullname: TA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tailen
      type: user
    createdAt: '2022-12-22T12:57:09.000Z'
    data:
      edited: false
      editors:
      - Tailen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a007d9345926827a60bcc847f4a0870.svg
          fullname: TA
          isHf: false
          isPro: false
          name: Tailen
          type: user
        html: '<p>I ran into a strange issue after a few runs though. Plenty of VRAM
          is free, yet PyTorch reports OOM.</p>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00
          MiB (GPU 0; 24.00 GiB total capacity; 7.04 GiB already allocated; 15.73
          GiB free; 7.04 GiB reserved in total by PyTorch)</p>

          <p>My guess is this is some sort of memory leak, since the issue doesn''t
          occur after a system restart. I know this might not be the right place to
          ask, but is this a known issue?</p>

          '
        raw: 'I ran into a strange issue after a few runs though. Plenty of VRAM is
          free, yet PyTorch reports OOM.


          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00
          MiB (GPU 0; 24.00 GiB total capacity; 7.04 GiB already allocated; 15.73
          GiB free; 7.04 GiB reserved in total by PyTorch)


          My guess is this is some sort of memory leak, since the issue doesn''t occur
          after a system restart. I know this might not be the right place to ask,
          but is this a known issue?'
        updatedAt: '2022-12-22T12:57:09.314Z'
      numEdits: 0
      reactions: []
    id: 63a45425412fd71fb7f31a85
    type: comment
  author: Tailen
  content: 'I ran into a strange issue after a few runs though. Plenty of VRAM is
    free, yet PyTorch reports OOM.


    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB
    (GPU 0; 24.00 GiB total capacity; 7.04 GiB already allocated; 15.73 GiB free;
    7.04 GiB reserved in total by PyTorch)


    My guess is this is some sort of memory leak, since the issue doesn''t occur after
    a system restart. I know this might not be the right place to ask, but is this
    a known issue?'
  created_at: 2022-12-22 12:57:09+00:00
  edited: false
  hidden: false
  id: 63a45425412fd71fb7f31a85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fdb9438b3c5087ff8069e9/tMcdlnFKOFLh2lSifDxGk.png?w=200&h=200&f=face
      fullname: Nguyen Trieu Phong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrtrieuphong
      type: user
    createdAt: '2023-08-09T14:46:55.000Z'
    data:
      edited: false
      editors:
      - mrtrieuphong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9189174175262451
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fdb9438b3c5087ff8069e9/tMcdlnFKOFLh2lSifDxGk.png?w=200&h=200&f=face
          fullname: Nguyen Trieu Phong
          isHf: false
          isPro: false
          name: mrtrieuphong
          type: user
        html: '<p>I can run BLOOM 7B1 and LoRa on 24GB GPU, it''s take about 17GB</p>

          '
        raw: I can run BLOOM 7B1 and LoRa on 24GB GPU, it's take about 17GB
        updatedAt: '2023-08-09T14:46:55.492Z'
      numEdits: 0
      reactions: []
    id: 64d3a6df4812871c21d3526a
    type: comment
  author: mrtrieuphong
  content: I can run BLOOM 7B1 and LoRa on 24GB GPU, it's take about 17GB
  created_at: 2023-08-09 13:46:55+00:00
  edited: false
  hidden: false
  id: 64d3a6df4812871c21d3526a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: bigscience/bloom-7b1
repo_type: model
status: open
target_branch: null
title: 'Can I run model in my desktop? '
