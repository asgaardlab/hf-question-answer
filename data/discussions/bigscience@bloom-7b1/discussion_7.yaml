!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsarti
conflicting_files: null
created_at: 2022-07-07 16:50:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
      fullname: Gabriele Sarti
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gsarti
      type: user
    createdAt: '2022-07-07T17:50:13.000Z'
    data:
      edited: false
      editors:
      - gsarti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
          fullname: Gabriele Sarti
          isHf: false
          isPro: false
          name: gsarti
          type: user
        html: "<p>If I try to load the checkpoint for `bigscience/bloom-6b3' using\
          \ Pytorch, I see only the modules of the 'BloomModel', but not the 'lm_head'\
          \ that should also be there to instantiate a 'BloomForCausalLM':</p>\n<pre><code\
          \ class=\"language-python\">!git clone https;//huggingface.co/bigscience/bloom-6b3\n\
          checkpoint = torch.load(<span class=\"hljs-string\">\"bloom-6b3/pytorch_model.bin\"\
          </span>)\n<span class=\"hljs-keyword\">for</span> param_name, param <span\
          \ class=\"hljs-keyword\">in</span> checkpoint.items():\n    <span class=\"\
          hljs-built_in\">print</span>(param_name)\n</code></pre>\n<p>Output:</p>\n\
          <pre><code class=\"language-shell\">word_embeddings.weight\nword_embeddings_layernorm.weight\n\
          word_embeddings_layernorm.bias\nh.0.input_layernorm.weight\nh.0.input_layernorm.bias\n\
          h.0.self_attention.query_key_value.weight\nh.0.self_attention.query_key_value.bias\n\
          h.0.self_attention.dense.weight\nh.0.self_attention.dense.bias\nh.0.post_attention_layernorm.weight\n\
          h.0.post_attention_layernorm.bias\nh.0.mlp.dense_h_to_4h.weight\nh.0.mlp.dense_h_to_4h.bias\n\
          h.0.mlp.dense_4h_to_h.weight\nh.0.mlp.dense_4h_to_h.bias\n... # Omitted\
          \ for brevity, simply all layers between 1 and 29\nh.29.input_layernorm.weight\n\
          h.29.input_layernorm.bias\nh.29.self_attention.query_key_value.weight\n\
          h.29.self_attention.query_key_value.bias\nh.29.self_attention.dense.weight\n\
          h.29.self_attention.dense.bias\nh.29.post_attention_layernorm.weight\nh.29.post_attention_layernorm.bias\n\
          h.29.mlp.dense_h_to_4h.weight\nh.29.mlp.dense_h_to_4h.bias\nh.29.mlp.dense_4h_to_h.weight\n\
          h.29.mlp.dense_4h_to_h.bias\nln_f.weight\nln_f.bias\n</code></pre>\n<p>How\
          \ is it possible to load the model with a trained causal language modeling\
          \ head?</p>\n"
        raw: "If I try to load the checkpoint for `bigscience/bloom-6b3' using Pytorch,\
          \ I see only the modules of the 'BloomModel', but not the 'lm_head' that\
          \ should also be there to instantiate a 'BloomForCausalLM':\r\n\r\n```python\r\
          \n!git clone https;//huggingface.co/bigscience/bloom-6b3\r\ncheckpoint =\
          \ torch.load(\"bloom-6b3/pytorch_model.bin\")\r\nfor param_name, param in\
          \ checkpoint.items():\r\n    print(param_name)\r\n```\r\n\r\nOutput:\r\n\
          \r\n```shell\r\nword_embeddings.weight\r\nword_embeddings_layernorm.weight\r\
          \nword_embeddings_layernorm.bias\r\nh.0.input_layernorm.weight\r\nh.0.input_layernorm.bias\r\
          \nh.0.self_attention.query_key_value.weight\r\nh.0.self_attention.query_key_value.bias\r\
          \nh.0.self_attention.dense.weight\r\nh.0.self_attention.dense.bias\r\nh.0.post_attention_layernorm.weight\r\
          \nh.0.post_attention_layernorm.bias\r\nh.0.mlp.dense_h_to_4h.weight\r\n\
          h.0.mlp.dense_h_to_4h.bias\r\nh.0.mlp.dense_4h_to_h.weight\r\nh.0.mlp.dense_4h_to_h.bias\r\
          \n... # Omitted for brevity, simply all layers between 1 and 29\r\nh.29.input_layernorm.weight\r\
          \nh.29.input_layernorm.bias\r\nh.29.self_attention.query_key_value.weight\r\
          \nh.29.self_attention.query_key_value.bias\r\nh.29.self_attention.dense.weight\r\
          \nh.29.self_attention.dense.bias\r\nh.29.post_attention_layernorm.weight\r\
          \nh.29.post_attention_layernorm.bias\r\nh.29.mlp.dense_h_to_4h.weight\r\n\
          h.29.mlp.dense_h_to_4h.bias\r\nh.29.mlp.dense_4h_to_h.weight\r\nh.29.mlp.dense_4h_to_h.bias\r\
          \nln_f.weight\r\nln_f.bias\r\n```\r\n\r\nHow is it possible to load the\
          \ model with a trained causal language modeling head?"
        updatedAt: '2022-07-07T17:50:13.510Z'
      numEdits: 0
      reactions: []
    id: 62c71cd571d1a0742d06fc78
    type: comment
  author: gsarti
  content: "If I try to load the checkpoint for `bigscience/bloom-6b3' using Pytorch,\
    \ I see only the modules of the 'BloomModel', but not the 'lm_head' that should\
    \ also be there to instantiate a 'BloomForCausalLM':\r\n\r\n```python\r\n!git\
    \ clone https;//huggingface.co/bigscience/bloom-6b3\r\ncheckpoint = torch.load(\"\
    bloom-6b3/pytorch_model.bin\")\r\nfor param_name, param in checkpoint.items():\r\
    \n    print(param_name)\r\n```\r\n\r\nOutput:\r\n\r\n```shell\r\nword_embeddings.weight\r\
    \nword_embeddings_layernorm.weight\r\nword_embeddings_layernorm.bias\r\nh.0.input_layernorm.weight\r\
    \nh.0.input_layernorm.bias\r\nh.0.self_attention.query_key_value.weight\r\nh.0.self_attention.query_key_value.bias\r\
    \nh.0.self_attention.dense.weight\r\nh.0.self_attention.dense.bias\r\nh.0.post_attention_layernorm.weight\r\
    \nh.0.post_attention_layernorm.bias\r\nh.0.mlp.dense_h_to_4h.weight\r\nh.0.mlp.dense_h_to_4h.bias\r\
    \nh.0.mlp.dense_4h_to_h.weight\r\nh.0.mlp.dense_4h_to_h.bias\r\n... # Omitted\
    \ for brevity, simply all layers between 1 and 29\r\nh.29.input_layernorm.weight\r\
    \nh.29.input_layernorm.bias\r\nh.29.self_attention.query_key_value.weight\r\n\
    h.29.self_attention.query_key_value.bias\r\nh.29.self_attention.dense.weight\r\
    \nh.29.self_attention.dense.bias\r\nh.29.post_attention_layernorm.weight\r\nh.29.post_attention_layernorm.bias\r\
    \nh.29.mlp.dense_h_to_4h.weight\r\nh.29.mlp.dense_h_to_4h.bias\r\nh.29.mlp.dense_4h_to_h.weight\r\
    \nh.29.mlp.dense_4h_to_h.bias\r\nln_f.weight\r\nln_f.bias\r\n```\r\n\r\nHow is\
    \ it possible to load the model with a trained causal language modeling head?"
  created_at: 2022-07-07 16:50:13+00:00
  edited: false
  hidden: false
  id: 62c71cd571d1a0742d06fc78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-07T18:01:59.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Hi ! For BLOOM models I think that the weights of the LM head corresponds
          to the transpose of the embedding weights. The <code>ForCausalLM</code>
          module takes automatically care of that ;)</p>

          '
        raw: Hi ! For BLOOM models I think that the weights of the LM head corresponds
          to the transpose of the embedding weights. The `ForCausalLM` module takes
          automatically care of that ;)
        updatedAt: '2022-07-07T18:01:59.794Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gsarti
    id: 62c71f977a431f6282877075
    type: comment
  author: ybelkada
  content: Hi ! For BLOOM models I think that the weights of the LM head corresponds
    to the transpose of the embedding weights. The `ForCausalLM` module takes automatically
    care of that ;)
  created_at: 2022-07-07 17:01:59+00:00
  edited: false
  hidden: false
  id: 62c71f977a431f6282877075
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
      fullname: Gabriele Sarti
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gsarti
      type: user
    createdAt: '2022-07-07T18:06:23.000Z'
    data:
      edited: false
      editors:
      - gsarti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
          fullname: Gabriele Sarti
          isHf: false
          isPro: false
          name: gsarti
          type: user
        html: '<p>Thanks for the info! Maybe the class itself does, but it makes it
          pretty painful to load the checkpoint with Accelerate using <code>load_checkpoint_and_dispatch</code>!
          I think the only alternative at the moment is to code a custom loop to map
          checkpoint module names to the ones expected by the class, right?</p>

          '
        raw: Thanks for the info! Maybe the class itself does, but it makes it pretty
          painful to load the checkpoint with Accelerate using `load_checkpoint_and_dispatch`!
          I think the only alternative at the moment is to code a custom loop to map
          checkpoint module names to the ones expected by the class, right?
        updatedAt: '2022-07-07T18:06:23.199Z'
      numEdits: 0
      reactions: []
    id: 62c7209fe7d825deaae584e3
    type: comment
  author: gsarti
  content: Thanks for the info! Maybe the class itself does, but it makes it pretty
    painful to load the checkpoint with Accelerate using `load_checkpoint_and_dispatch`!
    I think the only alternative at the moment is to code a custom loop to map checkpoint
    module names to the ones expected by the class, right?
  created_at: 2022-07-07 17:06:23+00:00
  edited: false
  hidden: false
  id: 62c7209fe7d825deaae584e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2af41572967678a555230c987c3f3906.svg
      fullname: Amir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atamir
      type: user
    createdAt: '2023-02-20T16:33:13.000Z'
    data:
      edited: false
      editors:
      - atamir
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2af41572967678a555230c987c3f3906.svg
          fullname: Amir
          isHf: false
          isPro: false
          name: atamir
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gsarti&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gsarti\">@<span class=\"\
          underline\">gsarti</span></a></span>\n\n\t</span></span> I'm facing the\
          \ same problem, could you solve this issue using <code>load_checkpoint_and_dispatch</code>?</p>\n"
        raw: '@gsarti I''m facing the same problem, could you solve this issue using
          `load_checkpoint_and_dispatch`?'
        updatedAt: '2023-02-20T16:33:13.145Z'
      numEdits: 0
      reactions: []
    id: 63f3a0c9230a01e7d04b3bf5
    type: comment
  author: atamir
  content: '@gsarti I''m facing the same problem, could you solve this issue using
    `load_checkpoint_and_dispatch`?'
  created_at: 2023-02-20 16:33:13+00:00
  edited: false
  hidden: false
  id: 63f3a0c9230a01e7d04b3bf5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: bigscience/bloom-7b1
repo_type: model
status: open
target_branch: null
title: Missing LM Head
