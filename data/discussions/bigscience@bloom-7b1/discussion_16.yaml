!!python/object:huggingface_hub.community.DiscussionWithDetails
author: darragh
conflicting_files: null
created_at: 2022-09-02 17:15:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
      fullname: Darragh Hanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darragh
      type: user
    createdAt: '2022-09-02T18:15:36.000Z'
    data:
      edited: true
      editors:
      - darragh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
          fullname: Darragh Hanley
          isHf: false
          isPro: false
          name: darragh
          type: user
        html: "<p>Hi,<br>I am working on a docker instance with 4 X 40GB a100 cards.\
          \ On a single 40GB card I am unable to fit a single sample through the model\
          \ for finetuning, so  I am trying to finetune with sharding to split the\
          \ model layers across the cards.<br>I have my own script using the trainer,\
          \ like below and execute it with <code>python -m torch.distributed.launch\
          \ --nproc_per_node 4</code>. </p>\n<p>When running the script I can see\
          \ that the model is split across the 4 GPUs with the <code>fsdp</code> setting\
          \ below. However, the <code>per_device_train_batch_size</code> loads 4 samples\
          \ (1 per card) in each step so I get an OOM. I am wondering is it possible\
          \ to only load one sample total in each step, instead of one sample per\
          \ card. </p>\n<p>Let me know if it is better to post this in the pytorch\
          \ forums. </p>\n<p>Thanks!</p>\n<pre><code>training_args = TrainingArguments(\n\
          \    output_dir=args.outdir,\n    overwrite_output_dir=True,\n    save_total_limit=1,\n\
          \    do_train=True,\n    do_eval=False,\n    do_predict=True,\n    num_train_epochs=args.epochs,\
          \              # total number of training epochs\n    per_device_train_batch_size=1,\
          \  # batch size per device during training\n    per_device_eval_batch_size=1,\
          \   # batch size for evaluation\n    gradient_accumulation_steps = 256,\n\
          \    warmup_ratio=0.1,                # number of warmup steps for learning\
          \ rate scheduler\n    weight_decay=0.01,               # strength of weight\
          \ decay\n    logging_dir=args.logdir,            # directory for storing\
          \ logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"\
          epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n  \
          \  learning_rate=args.learning_rate,\n    seed=99,\n    local_rank=os.environ['LOCAL_RANK'],\n\
          \    dataloader_num_workers = 16,\n    gradient_checkpointing=True,\n  \
          \  lr_scheduler_type=\"cosine\",\n    fsdp='shard_grad_op', \n    fp16 =\
          \ True if platform.system()!='Darwin' else False\n)\n</code></pre>\n"
        raw: "Hi, \nI am working on a docker instance with 4 X 40GB a100 cards. On\
          \ a single 40GB card I am unable to fit a single sample through the model\
          \ for finetuning, so  I am trying to finetune with sharding to split the\
          \ model layers across the cards.\nI have my own script using the trainer,\
          \ like below and execute it with `python -m torch.distributed.launch --nproc_per_node\
          \ 4`. \n\nWhen running the script I can see that the model is split across\
          \ the 4 GPUs with the `fsdp` setting below. However, the `per_device_train_batch_size`\
          \ loads 4 samples (1 per card) in each step so I get an OOM. I am wondering\
          \ is it possible to only load one sample total in each step, instead of\
          \ one sample per card. \n\nLet me know if it is better to post this in the\
          \ pytorch forums. \n\nThanks!\n\n```\ntraining_args = TrainingArguments(\n\
          \    output_dir=args.outdir,\n    overwrite_output_dir=True,\n    save_total_limit=1,\n\
          \    do_train=True,\n    do_eval=False,\n    do_predict=True,\n    num_train_epochs=args.epochs,\
          \              # total number of training epochs\n    per_device_train_batch_size=1,\
          \  # batch size per device during training\n    per_device_eval_batch_size=1,\
          \   # batch size for evaluation\n    gradient_accumulation_steps = 256,\n\
          \    warmup_ratio=0.1,                # number of warmup steps for learning\
          \ rate scheduler\n    weight_decay=0.01,               # strength of weight\
          \ decay\n    logging_dir=args.logdir,            # directory for storing\
          \ logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"\
          epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n  \
          \  learning_rate=args.learning_rate,\n    seed=99,\n    local_rank=os.environ['LOCAL_RANK'],\n\
          \    dataloader_num_workers = 16,\n    gradient_checkpointing=True,\n  \
          \  lr_scheduler_type=\"cosine\",\n    fsdp='shard_grad_op', \n    fp16 =\
          \ True if platform.system()!='Darwin' else False\n)\n```"
        updatedAt: '2022-09-02T18:22:17.338Z'
      numEdits: 3
      reactions: []
    id: 63124848f568fb0098f5f6ee
    type: comment
  author: darragh
  content: "Hi, \nI am working on a docker instance with 4 X 40GB a100 cards. On a\
    \ single 40GB card I am unable to fit a single sample through the model for finetuning,\
    \ so  I am trying to finetune with sharding to split the model layers across the\
    \ cards.\nI have my own script using the trainer, like below and execute it with\
    \ `python -m torch.distributed.launch --nproc_per_node 4`. \n\nWhen running the\
    \ script I can see that the model is split across the 4 GPUs with the `fsdp` setting\
    \ below. However, the `per_device_train_batch_size` loads 4 samples (1 per card)\
    \ in each step so I get an OOM. I am wondering is it possible to only load one\
    \ sample total in each step, instead of one sample per card. \n\nLet me know if\
    \ it is better to post this in the pytorch forums. \n\nThanks!\n\n```\ntraining_args\
    \ = TrainingArguments(\n    output_dir=args.outdir,\n    overwrite_output_dir=True,\n\
    \    save_total_limit=1,\n    do_train=True,\n    do_eval=False,\n    do_predict=True,\n\
    \    num_train_epochs=args.epochs,              # total number of training epochs\n\
    \    per_device_train_batch_size=1,  # batch size per device during training\n\
    \    per_device_eval_batch_size=1,   # batch size for evaluation\n    gradient_accumulation_steps\
    \ = 256,\n    warmup_ratio=0.1,                # number of warmup steps for learning\
    \ rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n\
    \    logging_dir=args.logdir,            # directory for storing logs\n    logging_steps=10,\n\
    \    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n\
    \    report_to=\"none\",\n    learning_rate=args.learning_rate,\n    seed=99,\n\
    \    local_rank=os.environ['LOCAL_RANK'],\n    dataloader_num_workers = 16,\n\
    \    gradient_checkpointing=True,\n    lr_scheduler_type=\"cosine\",\n    fsdp='shard_grad_op',\
    \ \n    fp16 = True if platform.system()!='Darwin' else False\n)\n```"
  created_at: 2022-09-02 17:15:36+00:00
  edited: true
  hidden: false
  id: 63124848f568fb0098f5f6ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-09-04T14:36:22.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Yeah it''s probably more suitable to open an issue for this on <a
          rel="nofollow" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>,
          as it''s not specific to the model</p>

          '
        raw: Yeah it's probably more suitable to open an issue for this on https://github.com/huggingface/transformers,
          as it's not specific to the model
        updatedAt: '2022-09-04T14:36:22.788Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cakiki
    id: 6314b7e6b031f7b1c7bd8d3e
    type: comment
  author: Muennighoff
  content: Yeah it's probably more suitable to open an issue for this on https://github.com/huggingface/transformers,
    as it's not specific to the model
  created_at: 2022-09-04 13:36:22+00:00
  edited: false
  hidden: false
  id: 6314b7e6b031f7b1c7bd8d3e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: bigscience/bloom-7b1
repo_type: model
status: open
target_branch: null
title: Distributed Finetuning with Trainer
