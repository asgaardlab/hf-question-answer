!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wangchenpeng
conflicting_files: null
created_at: 2023-03-27 09:35:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f3627dd35d9bd38a7c74e3f3785f8c9d.svg
      fullname: wangchenpeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangchenpeng
      type: user
    createdAt: '2023-03-27T10:35:36.000Z'
    data:
      edited: false
      editors:
      - wangchenpeng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f3627dd35d9bd38a7c74e3f3785f8c9d.svg
          fullname: wangchenpeng
          isHf: false
          isPro: false
          name: wangchenpeng
          type: user
        html: '<p>WARNING:root:Reducing target length to 224, Retrying...<br>WARNING:root:OpenAIError:
          This model''s maximum context length is 2049 tokens, however you requested
          3078 tokens (2854 in your prompt; 224 for the completion). Please reduce
          your prompt; or completion length..<br>WARNING:root:Reducing target length
          to 179, Retrying...<br>WARNING:root:OpenAIError: This model''s maximum context
          length is 2049 tokens, however you requested 3033 tokens (2854 in your prompt;
          179 for the completion). Please reduce your prompt; or completion length..<br>WARNING:root:Reducing
          target length to 143, Retrying...<br>WARNING:root:OpenAIError: This model''s
          maximum context length is 2049 tokens, however you requested 2997 tokens
          (2854 in your prompt; 143 for the completion). Please reduce your prompt;
          or completion length..<br>WARNING:root:Reducing target length to 114, Retrying...<br>WARNING:root:OpenAIError:
          This model''s maximum context length is 2049 tokens, however you requested
          2968 tokens (2854 in your prompt; 114 for the completion). Please reduce
          your prompt; or completion length..<br>WARNING:root:Reducing target length
          to 91, Retrying...<br>WARNING:root:OpenAIError: This model''s maximum context
          length is 2049 tokens, however you requested 2945 tokens (2854 in your prompt;
          91 for the completion). Please reduce your prompt; or completion length..<br>WARNING:root:Reducing
          target length to 72, Retrying...<br>WARNING:root:OpenAIError: This model''s
          maximum context length is 2049 tokens, however you requested 2926 tokens
          (2854 in your prompt; 72 for the completion). Please reduce your prompt;
          or completion length..<br>WARNING:root:Reducing target length to 57, Retrying...<br>WARNING:root:OpenAIError:
          This model''s maximum context length is 2049 tokens, however you requested
          2911 tokens (2854 in your prompt; 57 for the completion). Please reduce
          your prompt; or completion length..<br>WARNING:root:Reducing target length
          to 45, Retrying...<br>WARNING:root:OpenAIError: This model''s maximum context
          length is 2049 tokens, however you requested 2899 tokens (2854 in your prompt;
          45 for the completion). Please reduce your prompt; or</p>

          '
        raw: "WARNING:root:Reducing target length to 224, Retrying...\r\nWARNING:root:OpenAIError:\
          \ This model's maximum context length is 2049 tokens, however you requested\
          \ 3078 tokens (2854 in your prompt; 224 for the completion). Please reduce\
          \ your prompt; or completion length..\r\nWARNING:root:Reducing target length\
          \ to 179, Retrying...\r\nWARNING:root:OpenAIError: This model's maximum\
          \ context length is 2049 tokens, however you requested 3033 tokens (2854\
          \ in your prompt; 179 for the completion). Please reduce your prompt; or\
          \ completion length..\r\nWARNING:root:Reducing target length to 143, Retrying...\r\
          \nWARNING:root:OpenAIError: This model's maximum context length is 2049\
          \ tokens, however you requested 2997 tokens (2854 in your prompt; 143 for\
          \ the completion). Please reduce your prompt; or completion length..\r\n\
          WARNING:root:Reducing target length to 114, Retrying...\r\nWARNING:root:OpenAIError:\
          \ This model's maximum context length is 2049 tokens, however you requested\
          \ 2968 tokens (2854 in your prompt; 114 for the completion). Please reduce\
          \ your prompt; or completion length..\r\nWARNING:root:Reducing target length\
          \ to 91, Retrying...\r\nWARNING:root:OpenAIError: This model's maximum context\
          \ length is 2049 tokens, however you requested 2945 tokens (2854 in your\
          \ prompt; 91 for the completion). Please reduce your prompt; or completion\
          \ length..\r\nWARNING:root:Reducing target length to 72, Retrying...\r\n\
          WARNING:root:OpenAIError: This model's maximum context length is 2049 tokens,\
          \ however you requested 2926 tokens (2854 in your prompt; 72 for the completion).\
          \ Please reduce your prompt; or completion length..\r\nWARNING:root:Reducing\
          \ target length to 57, Retrying...\r\nWARNING:root:OpenAIError: This model's\
          \ maximum context length is 2049 tokens, however you requested 2911 tokens\
          \ (2854 in your prompt; 57 for the completion). Please reduce your prompt;\
          \ or completion length..\r\nWARNING:root:Reducing target length to 45, Retrying...\r\
          \nWARNING:root:OpenAIError: This model's maximum context length is 2049\
          \ tokens, however you requested 2899 tokens (2854 in your prompt; 45 for\
          \ the completion). Please reduce your prompt; or"
        updatedAt: '2023-03-27T10:35:36.970Z'
      numEdits: 0
      reactions: []
    id: 642171782cc2b3c39e801a7f
    type: comment
  author: wangchenpeng
  content: "WARNING:root:Reducing target length to 224, Retrying...\r\nWARNING:root:OpenAIError:\
    \ This model's maximum context length is 2049 tokens, however you requested 3078\
    \ tokens (2854 in your prompt; 224 for the completion). Please reduce your prompt;\
    \ or completion length..\r\nWARNING:root:Reducing target length to 179, Retrying...\r\
    \nWARNING:root:OpenAIError: This model's maximum context length is 2049 tokens,\
    \ however you requested 3033 tokens (2854 in your prompt; 179 for the completion).\
    \ Please reduce your prompt; or completion length..\r\nWARNING:root:Reducing target\
    \ length to 143, Retrying...\r\nWARNING:root:OpenAIError: This model's maximum\
    \ context length is 2049 tokens, however you requested 2997 tokens (2854 in your\
    \ prompt; 143 for the completion). Please reduce your prompt; or completion length..\r\
    \nWARNING:root:Reducing target length to 114, Retrying...\r\nWARNING:root:OpenAIError:\
    \ This model's maximum context length is 2049 tokens, however you requested 2968\
    \ tokens (2854 in your prompt; 114 for the completion). Please reduce your prompt;\
    \ or completion length..\r\nWARNING:root:Reducing target length to 91, Retrying...\r\
    \nWARNING:root:OpenAIError: This model's maximum context length is 2049 tokens,\
    \ however you requested 2945 tokens (2854 in your prompt; 91 for the completion).\
    \ Please reduce your prompt; or completion length..\r\nWARNING:root:Reducing target\
    \ length to 72, Retrying...\r\nWARNING:root:OpenAIError: This model's maximum\
    \ context length is 2049 tokens, however you requested 2926 tokens (2854 in your\
    \ prompt; 72 for the completion). Please reduce your prompt; or completion length..\r\
    \nWARNING:root:Reducing target length to 57, Retrying...\r\nWARNING:root:OpenAIError:\
    \ This model's maximum context length is 2049 tokens, however you requested 2911\
    \ tokens (2854 in your prompt; 57 for the completion). Please reduce your prompt;\
    \ or completion length..\r\nWARNING:root:Reducing target length to 45, Retrying...\r\
    \nWARNING:root:OpenAIError: This model's maximum context length is 2049 tokens,\
    \ however you requested 2899 tokens (2854 in your prompt; 45 for the completion).\
    \ Please reduce your prompt; or"
  created_at: 2023-03-27 09:35:36+00:00
  edited: false
  hidden: false
  id: 642171782cc2b3c39e801a7f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: BelleGroup/BELLE-7B-2M
repo_type: model
status: open
target_branch: null
title: "\u5728\u751F\u6210\u6570\u636E\u65F6\u51FA\u73B0\u8D85\u51FAtoken\u95EE\u9898\
  \ \u8BF7\u95EE\u8BE5\u600E\u4E48\u89E3\u51B3"
