!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sibgat-Ul
conflicting_files: null
created_at: 2023-10-26 03:07:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d30cc6a24b69a830ec3488b793bc637.svg
      fullname: Sibgat Ul Islam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sibgat-Ul
      type: user
    createdAt: '2023-10-26T04:07:45.000Z'
    data:
      edited: false
      editors:
      - Sibgat-Ul
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9464516043663025
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d30cc6a24b69a830ec3488b793bc637.svg
          fullname: Sibgat Ul Islam
          isHf: false
          isPro: false
          name: Sibgat-Ul
          type: user
        html: '<h3 id="hello">Hello,</h3>

          <p><strong>First of all thank you guys</strong> for making this repo and
          all of your hard works.<br>I was using this model to train a <strong>Seq2SeqTrainer</strong>
          and <strong>hitting the memory limit (3060 12gb)</strong>, which was not
          the case for t5 and mt5. I have been using the <strong>same training args</strong>
          for all of the cases except the <strong>batch_size = 64/48/32</strong> but
          for banglat5 I had to set the <strong>batch_size = 16</strong>,</p>

          <p>Is there anyway to optimize the gpu memory usage?</p>

          <p>Thank you,</p>

          '
        raw: "### Hello,\r\n\r\n**First of all thank you guys** for making this repo\
          \ and all of your hard works.\r\nI was using this model to train a **Seq2SeqTrainer**\
          \ and **hitting the memory limit (3060 12gb)**, which was not the case for\
          \ t5 and mt5. I have been using the **same training args** for all of the\
          \ cases except the **batch_size = 64/48/32** but for banglat5 I had to set\
          \ the **batch_size = 16**,\r\n\r\nIs there anyway to optimize the gpu memory\
          \ usage?\r\n\r\nThank you,"
        updatedAt: '2023-10-26T04:07:45.517Z'
      numEdits: 0
      reactions: []
    id: 6539e611821b9e4a34b8a932
    type: comment
  author: Sibgat-Ul
  content: "### Hello,\r\n\r\n**First of all thank you guys** for making this repo\
    \ and all of your hard works.\r\nI was using this model to train a **Seq2SeqTrainer**\
    \ and **hitting the memory limit (3060 12gb)**, which was not the case for t5\
    \ and mt5. I have been using the **same training args** for all of the cases except\
    \ the **batch_size = 64/48/32** but for banglat5 I had to set the **batch_size\
    \ = 16**,\r\n\r\nIs there anyway to optimize the gpu memory usage?\r\n\r\nThank\
    \ you,"
  created_at: 2023-10-26 03:07:45+00:00
  edited: false
  hidden: false
  id: 6539e611821b9e4a34b8a932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cabf4a6696b7a599bc0886/ysN3TOCAIer_nJxOAHbiW.jpeg?w=200&h=200&f=face
      fullname: Sameen Shahgir
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Lancelot53
      type: user
    createdAt: '2023-10-26T04:50:02.000Z'
    data:
      edited: false
      editors:
      - Lancelot53
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8119065165519714
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cabf4a6696b7a599bc0886/ysN3TOCAIer_nJxOAHbiW.jpeg?w=200&h=200&f=face
          fullname: Sameen Shahgir
          isHf: false
          isPro: false
          name: Lancelot53
          type: user
        html: '<p>Possibly a tokenization issue since banglat5 has the exact same
          architecture as t5. In fact, banglat5 should have lower memory requirements
          because the banglat5 tokenizer creates a lower number of tokens than mt5
          given the same Bangla text. </p>

          <p>Check if you''re using the right tokenizer (the one in this repo) </p>

          <p>Maybe worthwhile to explicitly set max_length, truncation, and padding
          variables when calling the tokenizer.</p>

          <p>Good Luck.</p>

          '
        raw: "Possibly a tokenization issue since banglat5 has the exact same architecture\
          \ as t5. In fact, banglat5 should have lower memory requirements because\
          \ the banglat5 tokenizer creates a lower number of tokens than mt5 given\
          \ the same Bangla text. \n\nCheck if you're using the right tokenizer (the\
          \ one in this repo) \n\nMaybe worthwhile to explicitly set max_length, truncation,\
          \ and padding variables when calling the tokenizer.\n\nGood Luck."
        updatedAt: '2023-10-26T04:50:02.248Z'
      numEdits: 0
      reactions: []
    id: 6539effa6913a31c5f195933
    type: comment
  author: Lancelot53
  content: "Possibly a tokenization issue since banglat5 has the exact same architecture\
    \ as t5. In fact, banglat5 should have lower memory requirements because the banglat5\
    \ tokenizer creates a lower number of tokens than mt5 given the same Bangla text.\
    \ \n\nCheck if you're using the right tokenizer (the one in this repo) \n\nMaybe\
    \ worthwhile to explicitly set max_length, truncation, and padding variables when\
    \ calling the tokenizer.\n\nGood Luck."
  created_at: 2023-10-26 03:50:02+00:00
  edited: false
  hidden: false
  id: 6539effa6913a31c5f195933
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: csebuetnlp/banglat5
repo_type: model
status: open
target_branch: null
title: 100% gpu memory usage
