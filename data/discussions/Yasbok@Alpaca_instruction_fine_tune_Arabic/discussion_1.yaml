!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abdoelsayed
conflicting_files: null
created_at: 2023-04-11 21:49:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
      fullname: Abdelrahman Abdallah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdoelsayed
      type: user
    createdAt: '2023-04-11T22:49:12.000Z'
    data:
      edited: true
      editors:
      - abdoelsayed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
          fullname: Abdelrahman Abdallah
          isHf: false
          isPro: false
          name: abdoelsayed
          type: user
        html: '<p>Hello<br>I think there is a mistake the llama tokenizer not working
          with Arabic if you tried it<br>you will find that it tokenize the word as
          characters</p>

          '
        raw: "Hello \nI think there is a mistake the llama tokenizer not working with\
          \ Arabic if you tried it \nyou will find that it tokenize the word as characters"
        updatedAt: '2023-04-11T22:49:53.858Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - isrouush
        - moslemsamiee
    id: 6435e3e82d0ed796668f9c3a
    type: comment
  author: abdoelsayed
  content: "Hello \nI think there is a mistake the llama tokenizer not working with\
    \ Arabic if you tried it \nyou will find that it tokenize the word as characters"
  created_at: 2023-04-11 21:49:12+00:00
  edited: true
  hidden: false
  id: 6435e3e82d0ed796668f9c3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
      fullname: Israa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: isrouush
      type: user
    createdAt: '2023-05-17T17:30:49.000Z'
    data:
      edited: false
      editors:
      - isrouush
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
          fullname: Israa
          isHf: false
          isPro: false
          name: isrouush
          type: user
        html: '<p>I can confirm this. Has there been a solution for that?</p>

          '
        raw: I can confirm this. Has there been a solution for that?
        updatedAt: '2023-05-17T17:30:49.967Z'
      numEdits: 0
      reactions: []
    id: 64650f49e8e31202cb4f3530
    type: comment
  author: isrouush
  content: I can confirm this. Has there been a solution for that?
  created_at: 2023-05-17 16:30:49+00:00
  edited: false
  hidden: false
  id: 64650f49e8e31202cb4f3530
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
      fullname: Abdelrahman Abdallah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdoelsayed
      type: user
    createdAt: '2023-05-17T17:48:05.000Z'
    data:
      edited: false
      editors:
      - abdoelsayed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
          fullname: Abdelrahman Abdallah
          isHf: false
          isPro: false
          name: abdoelsayed
          type: user
        html: '<p>toy have two solution one if easy to make the model work with characters
          like what happen now.<br>or increase tokenizer with arabic tokens but in
          this case you will finetune the model to update embedding weight</p>

          '
        raw: 'toy have two solution one if easy to make the model work with characters
          like what happen now.

          or increase tokenizer with arabic tokens but in this case you will finetune
          the model to update embedding weight'
        updatedAt: '2023-05-17T17:48:05.100Z'
      numEdits: 0
      reactions: []
    id: 646513556ceebdc7fd8c4094
    type: comment
  author: abdoelsayed
  content: 'toy have two solution one if easy to make the model work with characters
    like what happen now.

    or increase tokenizer with arabic tokens but in this case you will finetune the
    model to update embedding weight'
  created_at: 2023-05-17 16:48:05+00:00
  edited: false
  hidden: false
  id: 646513556ceebdc7fd8c4094
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
      fullname: Israa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: isrouush
      type: user
    createdAt: '2023-05-17T18:15:08.000Z'
    data:
      edited: false
      editors:
      - isrouush
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
          fullname: Israa
          isHf: false
          isPro: false
          name: isrouush
          type: user
        html: '<p>You can increase the tokenizer''s vocabulary using tokenizer.add_tokens()
          right?<br>Also do you mean by your last statement that you first need a
          fine-tuning round for the model to update the embedding weight, then another
          round to fine-tune on a specific task?</p>

          '
        raw: 'You can increase the tokenizer''s vocabulary using tokenizer.add_tokens()
          right?

          Also do you mean by your last statement that you first need a fine-tuning
          round for the model to update the embedding weight, then another round to
          fine-tune on a specific task?'
        updatedAt: '2023-05-17T18:15:08.448Z'
      numEdits: 0
      reactions: []
    id: 646519aca0748f9aa4c6d970
    type: comment
  author: isrouush
  content: 'You can increase the tokenizer''s vocabulary using tokenizer.add_tokens()
    right?

    Also do you mean by your last statement that you first need a fine-tuning round
    for the model to update the embedding weight, then another round to fine-tune
    on a specific task?'
  created_at: 2023-05-17 17:15:08+00:00
  edited: false
  hidden: false
  id: 646519aca0748f9aa4c6d970
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
      fullname: Abdelrahman Abdallah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdoelsayed
      type: user
    createdAt: '2023-05-17T18:17:08.000Z'
    data:
      edited: false
      editors:
      - abdoelsayed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2fd7eb57179fe2c5e3f0f53aa07fb398.svg
          fullname: Abdelrahman Abdallah
          isHf: false
          isPro: false
          name: abdoelsayed
          type: user
        html: '<p>ya if you increased the tokens you should finetung the model using
          arabic dataset so you can update the embedding weight for arabic tokens</p>

          '
        raw: ya if you increased the tokens you should finetung the model using arabic
          dataset so you can update the embedding weight for arabic tokens
        updatedAt: '2023-05-17T18:17:08.506Z'
      numEdits: 0
      reactions: []
    id: 64651a246ceebdc7fd8cb22b
    type: comment
  author: abdoelsayed
  content: ya if you increased the tokens you should finetung the model using arabic
    dataset so you can update the embedding weight for arabic tokens
  created_at: 2023-05-17 17:17:08+00:00
  edited: false
  hidden: false
  id: 64651a246ceebdc7fd8cb22b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
      fullname: Israa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: isrouush
      type: user
    createdAt: '2023-05-17T18:44:29.000Z'
    data:
      edited: false
      editors:
      - isrouush
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72c0c112b2b13944f1c54eb306569306.svg
          fullname: Israa
          isHf: false
          isPro: false
          name: isrouush
          type: user
        html: '<p>Thanks a lot!</p>

          '
        raw: Thanks a lot!
        updatedAt: '2023-05-17T18:44:29.890Z'
      numEdits: 0
      reactions: []
    id: 6465208da0748f9aa4c73bc4
    type: comment
  author: isrouush
  content: Thanks a lot!
  created_at: 2023-05-17 17:44:29+00:00
  edited: false
  hidden: false
  id: 6465208da0748f9aa4c73bc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104c4cbcef28930bee2655f564057a20.svg
      fullname: Abdullah Abdelrhim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdullah
      type: user
    createdAt: '2024-01-05T17:51:05.000Z'
    data:
      edited: false
      editors:
      - abdullah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6242195963859558
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104c4cbcef28930bee2655f564057a20.svg
          fullname: Abdullah Abdelrhim
          isHf: false
          isPro: false
          name: abdullah
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;abdoelsayed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abdoelsayed\"\
          >@<span class=\"underline\">abdoelsayed</span></a></span>\n\n\t</span></span><br>About\
          \ <code>ya if you increased the tokens you should finetung the model using\
          \ arabic dataset so you can update the embedding weight for arabic tokens</code></p>\n\
          <p>Will finetune be sufficient or will we need to continue pretraining?</p>\n"
        raw: "@abdoelsayed \nAbout ```ya if you increased the tokens you should finetung\
          \ the model using arabic dataset so you can update the embedding weight\
          \ for arabic tokens```\n\nWill finetune be sufficient or will we need to\
          \ continue pretraining?\n\n"
        updatedAt: '2024-01-05T17:51:05.490Z'
      numEdits: 0
      reactions: []
    id: 659841893b0b56c5e06c9e5c
    type: comment
  author: abdullah
  content: "@abdoelsayed \nAbout ```ya if you increased the tokens you should finetung\
    \ the model using arabic dataset so you can update the embedding weight for arabic\
    \ tokens```\n\nWill finetune be sufficient or will we need to continue pretraining?\n\
    \n"
  created_at: 2024-01-05 17:51:05+00:00
  edited: false
  hidden: false
  id: 659841893b0b56c5e06c9e5c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Yasbok/Alpaca_instruction_fine_tune_Arabic
repo_type: model
status: open
target_branch: null
title: arabic tokenizer
