!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ucllovelab
conflicting_files: null
created_at: 2024-01-17 14:33:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd4d55ad3bf62d0cbe443b794f15a8a8.svg
      fullname: Bradley Love
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ucllovelab
      type: user
    createdAt: '2024-01-17T14:33:08.000Z'
    data:
      edited: false
      editors:
      - ucllovelab
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6724629402160645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd4d55ad3bf62d0cbe443b794f15a8a8.svg
          fullname: Bradley Love
          isHf: false
          isPro: false
          name: ucllovelab
          type: user
        html: "<p>I have a pro subscription and can query <code>meta-llama/Llama-2-7b-chat-hf</code>\
          \ directly using the InferenceClient like so - </p>\n<pre><code>client =\
          \ InferenceClient(\n    model=\"meta-llama/Llama-2-7b-chat-hf\", \n    token=API_TOKEN\n\
          )\n\noutput = client.text_generation(\"Can you please let us know more details\
          \ about your \")\n</code></pre>\n<p>But I couldn't do the same with your\
          \ finetuned model and getting - </p>\n<pre><code>    raise BadRequestError(message,\
          \ response=response) from e\nhuggingface_hub.utils._errors.BadRequestError:\
          \  (Request ID: INIZGDhU1EFXqvABh0Bzu)\n\nBad request:\nmeta-llama/Llama-2-7b-hf\
          \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
          If this is a private repository, make sure to pass a token having permission\
          \ to this repo with `use_auth_token` or log in with `huggingface-cli login`\
          \ and pass `use_auth_token=True`.\n</code></pre>\n<p>Any suggestions?</p>\n\
          <p>Thanks!</p>\n"
        raw: "I have a pro subscription and can query `meta-llama/Llama-2-7b-chat-hf`\
          \ directly using the InferenceClient like so - \r\n```\r\nclient = InferenceClient(\r\
          \n    model=\"meta-llama/Llama-2-7b-chat-hf\", \r\n    token=API_TOKEN\r\
          \n)\r\n\r\noutput = client.text_generation(\"Can you please let us know\
          \ more details about your \")\r\n```\r\n\r\nBut I couldn't do the same with\
          \ your finetuned model and getting - \r\n\r\n```\r\n    raise BadRequestError(message,\
          \ response=response) from e\r\nhuggingface_hub.utils._errors.BadRequestError:\
          \  (Request ID: INIZGDhU1EFXqvABh0Bzu)\r\n\r\nBad request:\r\nmeta-llama/Llama-2-7b-hf\
          \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\r\
          \nIf this is a private repository, make sure to pass a token having permission\
          \ to this repo with `use_auth_token` or log in with `huggingface-cli login`\
          \ and pass `use_auth_token=True`.\r\n```\r\n\r\nAny suggestions?\r\n\r\n\
          Thanks!"
        updatedAt: '2024-01-17T14:33:08.205Z'
      numEdits: 0
      reactions: []
    id: 65a7e52487dd2bd7f9abb63c
    type: comment
  author: ucllovelab
  content: "I have a pro subscription and can query `meta-llama/Llama-2-7b-chat-hf`\
    \ directly using the InferenceClient like so - \r\n```\r\nclient = InferenceClient(\r\
    \n    model=\"meta-llama/Llama-2-7b-chat-hf\", \r\n    token=API_TOKEN\r\n)\r\n\
    \r\noutput = client.text_generation(\"Can you please let us know more details\
    \ about your \")\r\n```\r\n\r\nBut I couldn't do the same with your finetuned\
    \ model and getting - \r\n\r\n```\r\n    raise BadRequestError(message, response=response)\
    \ from e\r\nhuggingface_hub.utils._errors.BadRequestError:  (Request ID: INIZGDhU1EFXqvABh0Bzu)\r\
    \n\r\nBad request:\r\nmeta-llama/Llama-2-7b-hf is not a local folder and is not\
    \ a valid model identifier listed on 'https://huggingface.co/models'\r\nIf this\
    \ is a private repository, make sure to pass a token having permission to this\
    \ repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.\r\
    \n```\r\n\r\nAny suggestions?\r\n\r\nThanks!"
  created_at: 2024-01-17 14:33:08+00:00
  edited: false
  hidden: false
  id: 65a7e52487dd2bd7f9abb63c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: juliensimon/llama2-7b-qlora-openassistant-guanaco
repo_type: model
status: open
target_branch: null
title: Can't access via InferenceClient
