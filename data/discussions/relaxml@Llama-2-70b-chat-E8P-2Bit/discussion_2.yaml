!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-12-19 01:45:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-19T01:45:40.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9167535901069641
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Found it interesting to try your approach on a few long context
          window LLMs, such as yi-34b-200k, longalpaca-70b-16k, giraffe-70b-32k and
          yi fine tunes.<br>I guess the "right" approach would be to choose a context
          length as long as the window context, but that seems to take very long to
          process and likely blows up my vram, so now I just use HQQ quantization
          (i.e. <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ">https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ</a>)<br>as
          it appears to be independent.<br>Can you consider doing long context LLMs?
          I guess your method is sota.<br>Smaller model size = we can utilize more
          context on a given amount of vram. </p>

          '
        raw: "Found it interesting to try your approach on a few long context window\
          \ LLMs, such as yi-34b-200k, longalpaca-70b-16k, giraffe-70b-32k and yi\
          \ fine tunes. \r\nI guess the \"right\" approach would be to choose a context\
          \ length as long as the window context, but that seems to take very long\
          \ to process and likely blows up my vram, so now I just use HQQ quantization\
          \ (i.e. https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ)\
          \ \r\nas it appears to be independent. \r\nCan you consider doing long context\
          \ LLMs? I guess your method is sota. \r\nSmaller model size = we can utilize\
          \ more context on a given amount of vram. "
        updatedAt: '2023-12-19T01:45:40.168Z'
      numEdits: 0
      reactions: []
    id: 6580f5c416eb2b758e86afc8
    type: comment
  author: KnutJaegersberg
  content: "Found it interesting to try your approach on a few long context window\
    \ LLMs, such as yi-34b-200k, longalpaca-70b-16k, giraffe-70b-32k and yi fine tunes.\
    \ \r\nI guess the \"right\" approach would be to choose a context length as long\
    \ as the window context, but that seems to take very long to process and likely\
    \ blows up my vram, so now I just use HQQ quantization (i.e. https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ)\
    \ \r\nas it appears to be independent. \r\nCan you consider doing long context\
    \ LLMs? I guess your method is sota. \r\nSmaller model size = we can utilize more\
    \ context on a given amount of vram. "
  created_at: 2023-12-19 01:45:40+00:00
  edited: false
  hidden: false
  id: 6580f5c416eb2b758e86afc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
      fullname: Albert
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: at676
      type: user
    createdAt: '2023-12-19T21:05:53.000Z'
    data:
      edited: false
      editors:
      - at676
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975433349609375
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
          fullname: Albert
          isHf: false
          isPro: false
          name: at676
          type: user
        html: '<p>As you noted, to get accurate "hessians" for long context models
          one needs to have long context datasets to play with. We''re looking into
          finding more datasets, but 32k should be doable with RedPajama 1. We will
          try to add longer context LLMs in the future but we don''t have that many
          people working on this and have a lot of things on our todo lists.</p>

          '
        raw: As you noted, to get accurate "hessians" for long context models one
          needs to have long context datasets to play with. We're looking into finding
          more datasets, but 32k should be doable with RedPajama 1. We will try to
          add longer context LLMs in the future but we don't have that many people
          working on this and have a lot of things on our todo lists.
        updatedAt: '2023-12-19T21:05:53.339Z'
      numEdits: 0
      reactions: []
    id: 658205b15def15befa1ef1d9
    type: comment
  author: at676
  content: As you noted, to get accurate "hessians" for long context models one needs
    to have long context datasets to play with. We're looking into finding more datasets,
    but 32k should be doable with RedPajama 1. We will try to add longer context LLMs
    in the future but we don't have that many people working on this and have a lot
    of things on our todo lists.
  created_at: 2023-12-19 21:05:53+00:00
  edited: false
  hidden: false
  id: 658205b15def15befa1ef1d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-20T07:23:17.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9286004900932312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I''m was just trying to do 8k context window with an instruction
          tuned yi-34b-200k.<br>If we do that, will the model be useless? I mean,
          it''s a different method, but the bloke for example quantizes awq versions
          of yi with ''just'' 8k context. </p>

          '
        raw: "I'm was just trying to do 8k context window with an instruction tuned\
          \ yi-34b-200k. \nIf we do that, will the model be useless? I mean, it's\
          \ a different method, but the bloke for example quantizes awq versions of\
          \ yi with 'just' 8k context. "
        updatedAt: '2023-12-20T07:23:17.296Z'
      numEdits: 0
      reactions: []
    id: 6582966521aa786b76b7e332
    type: comment
  author: KnutJaegersberg
  content: "I'm was just trying to do 8k context window with an instruction tuned\
    \ yi-34b-200k. \nIf we do that, will the model be useless? I mean, it's a different\
    \ method, but the bloke for example quantizes awq versions of yi with 'just' 8k\
    \ context. "
  created_at: 2023-12-20 07:23:17+00:00
  edited: false
  hidden: false
  id: 6582966521aa786b76b7e332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-20T07:25:42.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523097276687622
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: "<p>the memory limitations of my sytstem don't allow me to go a lot\
          \ beyond 8k, it seems. maybe <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ has access to a server with a sota gpu. </p>\n"
        raw: 'the memory limitations of my sytstem don''t allow me to go a lot beyond
          8k, it seems. maybe @TheBloke has access to a server with a sota gpu. '
        updatedAt: '2023-12-20T07:25:42.260Z'
      numEdits: 0
      reactions: []
    id: 658296f666701405d5fbb3a6
    type: comment
  author: KnutJaegersberg
  content: 'the memory limitations of my sytstem don''t allow me to go a lot beyond
    8k, it seems. maybe @TheBloke has access to a server with a sota gpu. '
  created_at: 2023-12-20 07:25:42+00:00
  edited: false
  hidden: false
  id: 658296f666701405d5fbb3a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
      fullname: Albert
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: at676
      type: user
    createdAt: '2023-12-20T07:39:47.000Z'
    data:
      edited: false
      editors:
      - at676
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729639291763306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35298884b7633c67a9bcff0a32d31211.svg
          fullname: Albert
          isHf: false
          isPro: false
          name: at676
          type: user
        html: '<p>I don''t think the quantized model will be useless if you only use
          8k context. This part of the pipeline is for Hessian estimation, so using
          8k will give you a less accurate Hessian than the full context. However,
          that less accurate Hessian may work perfectly fine and not be that far off
          a longer context Hessian. If I were you, I would try 4k or 8k and see how
          well the quantized model does on long context. This is something we haven''t
          looked into yet but are planning to in future.</p>

          '
        raw: I don't think the quantized model will be useless if you only use 8k
          context. This part of the pipeline is for Hessian estimation, so using 8k
          will give you a less accurate Hessian than the full context. However, that
          less accurate Hessian may work perfectly fine and not be that far off a
          longer context Hessian. If I were you, I would try 4k or 8k and see how
          well the quantized model does on long context. This is something we haven't
          looked into yet but are planning to in future.
        updatedAt: '2023-12-20T07:39:47.637Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - KnutJaegersberg
    id: 65829a434867f8699d411d90
    type: comment
  author: at676
  content: I don't think the quantized model will be useless if you only use 8k context.
    This part of the pipeline is for Hessian estimation, so using 8k will give you
    a less accurate Hessian than the full context. However, that less accurate Hessian
    may work perfectly fine and not be that far off a longer context Hessian. If I
    were you, I would try 4k or 8k and see how well the quantized model does on long
    context. This is something we haven't looked into yet but are planning to in future.
  created_at: 2023-12-20 07:39:47+00:00
  edited: false
  hidden: false
  id: 65829a434867f8699d411d90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-20T08:18:07.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8013384342193604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>gonna try it, thanks! </p>

          '
        raw: 'gonna try it, thanks! '
        updatedAt: '2023-12-20T08:18:07.883Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6582a33f92b5a9664df3d27f
    id: 6582a33f92b5a9664df3d27a
    type: comment
  author: KnutJaegersberg
  content: 'gonna try it, thanks! '
  created_at: 2023-12-20 08:18:07+00:00
  edited: false
  hidden: false
  id: 6582a33f92b5a9664df3d27a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-20T08:18:07.000Z'
    data:
      status: closed
    id: 6582a33f92b5a9664df3d27f
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-12-20 08:18:07+00:00
  id: 6582a33f92b5a9664df3d27f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-26T17:51:08.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8432176113128662
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Here is a yi-34b fine tune, but I had issues loading it on your
          latest library version. </p>

          <p><a href="https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit">https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit</a></p>

          '
        raw: "Here is a yi-34b fine tune, but I had issues loading it on your latest\
          \ library version. \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit"
        updatedAt: '2023-12-26T17:51:08.251Z'
      numEdits: 0
      reactions: []
    id: 658b128ce2db8a8e252fd6ea
    type: comment
  author: KnutJaegersberg
  content: "Here is a yi-34b fine tune, but I had issues loading it on your latest\
    \ library version. \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit"
  created_at: 2023-12-26 17:51:08+00:00
  edited: false
  hidden: false
  id: 658b128ce2db8a8e252fd6ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-03T09:09:56.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7110143899917603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>There is now two further instruct models that should be compatible
          with the current version of quip#: </p>

          <p><a href="https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit">https://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit</a></p>

          <p><a href="https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit">https://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit</a></p>

          '
        raw: "There is now two further instruct models that should be compatible with\
          \ the current version of quip#: \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit\n\
          \nhttps://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit"
        updatedAt: '2024-01-03T09:09:56.526Z'
      numEdits: 0
      reactions: []
    id: 659524643b794014b2ea01df
    type: comment
  author: KnutJaegersberg
  content: "There is now two further instruct models that should be compatible with\
    \ the current version of quip#: \n\nhttps://huggingface.co/KnutJaegersberg/Tess-M-34B-2bit\n\
    \nhttps://huggingface.co/KnutJaegersberg/orca-mini-70b-2bit"
  created_at: 2024-01-03 09:09:56+00:00
  edited: false
  hidden: false
  id: 659524643b794014b2ea01df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: relaxml/Llama-2-70b-chat-E8P-2Bit
repo_type: model
status: closed
target_branch: null
title: Long context window models
