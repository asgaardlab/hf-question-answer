!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Firejowl
conflicting_files: null
created_at: 2023-11-08 08:54:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6522227494f3a1a209fe0ff3/u8q5HgylvERjDH3Uc-aut.png?w=200&h=200&f=face
      fullname: Sage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Firejowl
      type: user
    createdAt: '2023-11-08T08:54:32.000Z'
    data:
      edited: false
      editors:
      - Firejowl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9395408034324646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6522227494f3a1a209fe0ff3/u8q5HgylvERjDH3Uc-aut.png?w=200&h=200&f=face
          fullname: Sage
          isHf: false
          isPro: false
          name: Firejowl
          type: user
        html: '<p>Hello NousResearch team,</p>

          <p>I hope this message finds you well. I am reaching out to inquire about
          the possibility of sharding the Yarn-Mistral-7b-64k model. The aim of this
          request is to facilitate users with lower-end PCs to run the model more
          effectively.</p>

          <p>As you''re aware, the computational demands of large models can be a
          barrier to entry for individuals without access to high-end hardware. By
          providing a sharded version of the Yarn-Mistral-7b-64k model, we can democratize
          access, enabling a broader range of users to experiment with and benefit
          from this impressive model.</p>

          <p>Moreover, sharding could also benefit users who prefer to run models
          on cloud-based platforms like Google Colab or Kaggle, where there may be
          limitations on resources or users seek to optimize their allocated compute
          time.</p>

          <p>I understand that model sharding can come with its own set of challenges,
          but I believe the benefits to the community could be significant. If there''s
          any possibility to consider this, or if there are alternative solutions
          that could accommodate the needs mentioned above, I''d love to hear your
          thoughts.</p>

          <p>Thank you for your time and for your contributions to the AI community.</p>

          '
        raw: "Hello NousResearch team,\r\n\r\nI hope this message finds you well.\
          \ I am reaching out to inquire about the possibility of sharding the Yarn-Mistral-7b-64k\
          \ model. The aim of this request is to facilitate users with lower-end PCs\
          \ to run the model more effectively.\r\n\r\nAs you're aware, the computational\
          \ demands of large models can be a barrier to entry for individuals without\
          \ access to high-end hardware. By providing a sharded version of the Yarn-Mistral-7b-64k\
          \ model, we can democratize access, enabling a broader range of users to\
          \ experiment with and benefit from this impressive model.\r\n\r\nMoreover,\
          \ sharding could also benefit users who prefer to run models on cloud-based\
          \ platforms like Google Colab or Kaggle, where there may be limitations\
          \ on resources or users seek to optimize their allocated compute time.\r\
          \n\r\nI understand that model sharding can come with its own set of challenges,\
          \ but I believe the benefits to the community could be significant. If there's\
          \ any possibility to consider this, or if there are alternative solutions\
          \ that could accommodate the needs mentioned above, I'd love to hear your\
          \ thoughts.\r\n\r\nThank you for your time and for your contributions to\
          \ the AI community."
        updatedAt: '2023-11-08T08:54:32.548Z'
      numEdits: 0
      reactions: []
    id: 654b4cc8fce0d4fb01bd2e28
    type: comment
  author: Firejowl
  content: "Hello NousResearch team,\r\n\r\nI hope this message finds you well. I\
    \ am reaching out to inquire about the possibility of sharding the Yarn-Mistral-7b-64k\
    \ model. The aim of this request is to facilitate users with lower-end PCs to\
    \ run the model more effectively.\r\n\r\nAs you're aware, the computational demands\
    \ of large models can be a barrier to entry for individuals without access to\
    \ high-end hardware. By providing a sharded version of the Yarn-Mistral-7b-64k\
    \ model, we can democratize access, enabling a broader range of users to experiment\
    \ with and benefit from this impressive model.\r\n\r\nMoreover, sharding could\
    \ also benefit users who prefer to run models on cloud-based platforms like Google\
    \ Colab or Kaggle, where there may be limitations on resources or users seek to\
    \ optimize their allocated compute time.\r\n\r\nI understand that model sharding\
    \ can come with its own set of challenges, but I believe the benefits to the community\
    \ could be significant. If there's any possibility to consider this, or if there\
    \ are alternative solutions that could accommodate the needs mentioned above,\
    \ I'd love to hear your thoughts.\r\n\r\nThank you for your time and for your\
    \ contributions to the AI community."
  created_at: 2023-11-08 08:54:32+00:00
  edited: false
  hidden: false
  id: 654b4cc8fce0d4fb01bd2e28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Yarn-Mistral-7b-64k
repo_type: model
status: open
target_branch: null
title: Request for Model Sharding to Support Lower-End PCs
