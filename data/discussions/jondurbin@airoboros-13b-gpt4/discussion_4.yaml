!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flashvenom
conflicting_files: null
created_at: 2023-06-07 15:13:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-07T16:13:56.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8515357375144958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Noticed you removed the context length piece from the README, any
          reason?</p>

          '
        raw: Noticed you removed the context length piece from the README, any reason?
        updatedAt: '2023-06-07T16:13:56.342Z'
      numEdits: 0
      reactions: []
    id: 6480acc4cb64a2b85a41f0a1
    type: comment
  author: flashvenom
  content: Noticed you removed the context length piece from the README, any reason?
  created_at: 2023-06-07 15:13:56+00:00
  edited: false
  hidden: false
  id: 6480acc4cb64a2b85a41f0a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-06-07T16:53:16.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723179936408997
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Yeah, I just added a note to the readme.  I was getting feedback
          that the larger context sizes didn''t work, but haven''t had a chance to
          test it until now, but in the meantime I removed that note until I could
          test.</p>

          <p>To train, I updated the code to use a context size of 4096, and made
          sure to include training data exceeding 2048, and event tested a few prompts
          over 2048 which produced coherent output, but unfortunately I did not do
          enough due diligence in testing the larger ranges.  My sincerest apologies
          for misleading you (and anyone else)!</p>

          <p>It seems that the whole foundation model would have to be overhauled
          to properly achieve this.  I started testing an alternative, using landmark
          attention (landmark tokens for each context section), but it''ll be a while
          before that is ready (if it even proves successful).</p>

          '
        raw: 'Yeah, I just added a note to the readme.  I was getting feedback that
          the larger context sizes didn''t work, but haven''t had a chance to test
          it until now, but in the meantime I removed that note until I could test.


          To train, I updated the code to use a context size of 4096, and made sure
          to include training data exceeding 2048, and event tested a few prompts
          over 2048 which produced coherent output, but unfortunately I did not do
          enough due diligence in testing the larger ranges.  My sincerest apologies
          for misleading you (and anyone else)!


          It seems that the whole foundation model would have to be overhauled to
          properly achieve this.  I started testing an alternative, using landmark
          attention (landmark tokens for each context section), but it''ll be a while
          before that is ready (if it even proves successful).'
        updatedAt: '2023-06-07T16:53:16.105Z'
      numEdits: 0
      reactions: []
    id: 6480b5fcbb25a636c9deb803
    type: comment
  author: jondurbin
  content: 'Yeah, I just added a note to the readme.  I was getting feedback that
    the larger context sizes didn''t work, but haven''t had a chance to test it until
    now, but in the meantime I removed that note until I could test.


    To train, I updated the code to use a context size of 4096, and made sure to include
    training data exceeding 2048, and event tested a few prompts over 2048 which produced
    coherent output, but unfortunately I did not do enough due diligence in testing
    the larger ranges.  My sincerest apologies for misleading you (and anyone else)!


    It seems that the whole foundation model would have to be overhauled to properly
    achieve this.  I started testing an alternative, using landmark attention (landmark
    tokens for each context section), but it''ll be a while before that is ready (if
    it even proves successful).'
  created_at: 2023-06-07 15:53:16+00:00
  edited: false
  hidden: false
  id: 6480b5fcbb25a636c9deb803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baf5ed9f881f58fc3ee584f5f6091b35.svg
      fullname: nanashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PapersAnon
      type: user
    createdAt: '2023-06-08T07:47:41.000Z'
    data:
      edited: false
      editors:
      - PapersAnon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.847464382648468
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baf5ed9f881f58fc3ee584f5f6091b35.svg
          fullname: nanashi
          isHf: false
          isPro: false
          name: PapersAnon
          type: user
        html: '<p>You might want to look into implementing xPos for increased context
          length.<br><a rel="nofollow" href="https://github.com/kaiokendev/cutoff-len-is-context-len">https://github.com/kaiokendev/cutoff-len-is-context-len</a></p>

          '
        raw: 'You might want to look into implementing xPos for increased context
          length.

          https://github.com/kaiokendev/cutoff-len-is-context-len'
        updatedAt: '2023-06-08T07:47:41.345Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jondurbin
    id: 6481879d15c5dc529062ec8a
    type: comment
  author: PapersAnon
  content: 'You might want to look into implementing xPos for increased context length.

    https://github.com/kaiokendev/cutoff-len-is-context-len'
  created_at: 2023-06-08 06:47:41+00:00
  edited: false
  hidden: false
  id: 6481879d15c5dc529062ec8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-06-09T10:18:59.000Z'
    data:
      status: closed
    id: 6482fc937c0e529fe8df53f1
    type: status-change
  author: jondurbin
  created_at: 2023-06-09 09:18:59+00:00
  id: 6482fc937c0e529fe8df53f1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: jondurbin/airoboros-13b-gpt4
repo_type: model
status: closed
target_branch: null
title: Context Length?
