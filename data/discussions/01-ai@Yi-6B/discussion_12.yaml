!!python/object:huggingface_hub.community.DiscussionWithDetails
author: guocuimi
conflicting_files: null
created_at: 2023-11-08 16:47:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
      fullname: Michael Mi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guocuimi
      type: user
    createdAt: '2023-11-08T16:47:49.000Z'
    data:
      edited: true
      editors:
      - guocuimi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6350941061973572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7ceb8416acf1236a86a7f8242ae48a8.svg
          fullname: Michael Mi
          isHf: false
          isPro: false
          name: guocuimi
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/vectorch-ai/ScaleLLM\"\
          >https://github.com/vectorch-ai/ScaleLLM</a></p>\n<p>ScaleLLM is a tool\
          \ that enables you to serve language models locally. You can find the project\
          \ and documentation here: <a rel=\"nofollow\" href=\"https://github.com/vectorch-ai/ScaleLLM\"\
          >ScaleLLM GitHub</a>. Here's how you can set it up:</p>\n<p>1: start the\
          \ model inference server<br>First, run the model inference server using\
          \ the following Docker command. This command will start a container with\
          \ GPU support (if available) and link it to your local model cache:</p>\n\
          <pre><code>docker run -it --gpus=all --net=host --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models\
          \ \\\n  -e HF_MODEL_ID=01-ai/Yi-6B \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest\
          \ --logtostderr\n</code></pre>\n<p>2: start REST API server<br>Next, start\
          \ the REST API server by running the following Docker command:</p>\n<pre><code>docker\
          \ run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n</code></pre>\n<p>you will get following running services:</p>\n\
          <p>ScaleLLM gRPC server on port 8888: localhost:8888<br>ScaleLLM HTTP server\
          \ for monitoring on port 9999: localhost:9999<br>ScaleLLM REST API server\
          \ on port 8080: localhost:8080</p>\n<p>You can now send requests to the\
          \ local REST API server to generate text completions using a command like\
          \ this:</p>\n<pre><code>curl http://localhost:8080/v1/completions   -H \"\
          Content-Type: application/json\"   -d '{\n    \"model\": \"01-ai/Yi-6B\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n</code></pre>\n<p>This command sends a POST request\
          \ to the local REST API server, specifying the model, prompt, and other\
          \ parameters to generate completions.</p>\n<p>Make sure you have Docker\
          \ installed and configured for GPU usage if you want to take advantage of\
          \ GPU acceleration. This setup allows you to efficiently run the language\
          \ model locally with ScaleLLM.</p>\n"
        raw: "https://github.com/vectorch-ai/ScaleLLM\n\nScaleLLM is a tool that enables\
          \ you to serve language models locally. You can find the project and documentation\
          \ here: [ScaleLLM GitHub](https://github.com/vectorch-ai/ScaleLLM). Here's\
          \ how you can set it up:\n\n1: start the model inference server\nFirst,\
          \ run the model inference server using the following Docker command. This\
          \ command will start a container with GPU support (if available) and link\
          \ it to your local model cache:\n```\ndocker run -it --gpus=all --net=host\
          \ --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models \\\n  -e HF_MODEL_ID=01-ai/Yi-6B\
          \ \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest --logtostderr\n\
          ```\n2: start REST API server\nNext, start the REST API server by running\
          \ the following Docker command:\n```\ndocker run -it --net=host \\\n  docker.io/vectorchai/scalellm-gateway:latest\
          \ --logtostderr\n```\nyou will get following running services:\n\nScaleLLM\
          \ gRPC server on port 8888: localhost:8888\nScaleLLM HTTP server for monitoring\
          \ on port 9999: localhost:9999\nScaleLLM REST API server on port 8080: localhost:8080\n\
          \nYou can now send requests to the local REST API server to generate text\
          \ completions using a command like this:\n```\ncurl http://localhost:8080/v1/completions\
          \   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"01-ai/Yi-6B\"\
          ,\n    \"prompt\": \"what is vue.js\",\n    \"max_tokens\": 32,\n    \"\
          temperature\": 0.7\n  }'\n```\nThis command sends a POST request to the\
          \ local REST API server, specifying the model, prompt, and other parameters\
          \ to generate completions.\n\nMake sure you have Docker installed and configured\
          \ for GPU usage if you want to take advantage of GPU acceleration. This\
          \ setup allows you to efficiently run the language model locally with ScaleLLM."
        updatedAt: '2023-11-11T07:13:42.707Z'
      numEdits: 2
      reactions: []
    id: 654bbbb5f8853732606e0a7e
    type: comment
  author: guocuimi
  content: "https://github.com/vectorch-ai/ScaleLLM\n\nScaleLLM is a tool that enables\
    \ you to serve language models locally. You can find the project and documentation\
    \ here: [ScaleLLM GitHub](https://github.com/vectorch-ai/ScaleLLM). Here's how\
    \ you can set it up:\n\n1: start the model inference server\nFirst, run the model\
    \ inference server using the following Docker command. This command will start\
    \ a container with GPU support (if available) and link it to your local model\
    \ cache:\n```\ndocker run -it --gpus=all --net=host --shm-size=1g \\\n  -v $HOME/.cache/huggingface/hub:/models\
    \ \\\n  -e HF_MODEL_ID=01-ai/Yi-6B \\\n  -e DEVICE=auto \\\n  docker.io/vectorchai/scalellm:latest\
    \ --logtostderr\n```\n2: start REST API server\nNext, start the REST API server\
    \ by running the following Docker command:\n```\ndocker run -it --net=host \\\n\
    \  docker.io/vectorchai/scalellm-gateway:latest --logtostderr\n```\nyou will get\
    \ following running services:\n\nScaleLLM gRPC server on port 8888: localhost:8888\n\
    ScaleLLM HTTP server for monitoring on port 9999: localhost:9999\nScaleLLM REST\
    \ API server on port 8080: localhost:8080\n\nYou can now send requests to the\
    \ local REST API server to generate text completions using a command like this:\n\
    ```\ncurl http://localhost:8080/v1/completions   -H \"Content-Type: application/json\"\
    \   -d '{\n    \"model\": \"01-ai/Yi-6B\",\n    \"prompt\": \"what is vue.js\"\
    ,\n    \"max_tokens\": 32,\n    \"temperature\": 0.7\n  }'\n```\nThis command\
    \ sends a POST request to the local REST API server, specifying the model, prompt,\
    \ and other parameters to generate completions.\n\nMake sure you have Docker installed\
    \ and configured for GPU usage if you want to take advantage of GPU acceleration.\
    \ This setup allows you to efficiently run the language model locally with ScaleLLM."
  created_at: 2023-11-08 16:47:49+00:00
  edited: true
  hidden: false
  id: 654bbbb5f8853732606e0a7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-16T03:55:57.000Z'
    data:
      edited: false
      editors:
      - FancyZhao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7449842691421509
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
          fullname: FancyZhao
          isHf: false
          isPro: false
          name: FancyZhao
          type: user
        html: '<p>I''m closing this one as explained in <a href="https://huggingface.co/01-ai/Yi-34B/discussions/16#65558f0c7446daf1ce2f4dbf">https://huggingface.co/01-ai/Yi-34B/discussions/16#65558f0c7446daf1ce2f4dbf</a></p>

          '
        raw: I'm closing this one as explained in https://huggingface.co/01-ai/Yi-34B/discussions/16#65558f0c7446daf1ce2f4dbf
        updatedAt: '2023-11-16T03:55:57.923Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655592cd48093c3f6e47def5
    id: 655592cd48093c3f6e47def1
    type: comment
  author: FancyZhao
  content: I'm closing this one as explained in https://huggingface.co/01-ai/Yi-34B/discussions/16#65558f0c7446daf1ce2f4dbf
  created_at: 2023-11-16 03:55:57+00:00
  edited: false
  hidden: false
  id: 655592cd48093c3f6e47def1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-16T03:55:57.000Z'
    data:
      status: closed
    id: 655592cd48093c3f6e47def5
    type: status-change
  author: FancyZhao
  created_at: 2023-11-16 03:55:57+00:00
  id: 655592cd48093c3f6e47def5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: 01-ai/Yi-6B
repo_type: model
status: closed
target_branch: null
title: Efficiently run the model locally using ScaleLLM
