!!python/object:huggingface_hub.community.DiscussionWithDetails
author: macadeliccc
conflicting_files: null
created_at: 2023-11-06 21:02:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-06T21:02:32.000Z'
    data:
      edited: false
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4970865845680237
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: "<p>The 6B model has run on generation and the eos_token_id is not set.</p>\n\
          <p>Here is the corrected version that produces the intended results: </p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\n<span class=\"hljs-comment\"># Load the model</span>\n\
          model = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"01-ai/Yi-6B\"</span>, device_map=<span class=\"hljs-string\">\"auto\"\
          </span>, torch_dtype=<span class=\"hljs-string\">\"auto\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\">#\
          \ Load the tokenizer</span>\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"01-ai/Yi-6B\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\">#\
          \ Encode the input text</span>\ninputs = tokenizer(<span class=\"hljs-string\"\
          >\"There's a place where time stands still. A place of breath taking wonder,\
          \ but also\"</span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          max_length = <span class=\"hljs-number\">256</span> \n\n<span class=\"hljs-comment\"\
          ># Generate output with the end-of-sequence token</span>\noutputs = model.generate(\n\
          \    inputs.input_ids.cuda(),\n    max_length=max_length,\n    eos_token_id=tokenizer.eos_token_id\
          \  <span class=\"hljs-comment\"># Use the EOS token ID which is 2</span>\n\
          )\n\n<span class=\"hljs-comment\"># Decode and print the output</span>\n\
          <span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n<p>eos_token_id=2 if anyone wants the value</p>\n\
          <p>Special Tokens: {'bos_token': '&lt;|startoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;',\
          \ 'unk_token': '', 'pad_token': ''}<br>Vocabulary Size: 64000<br>ID 0 :\
          \ Token <br>ID 1 : Token &lt;|startoftext|&gt;<br>ID 2 : Token &lt;|endoftext|&gt;</p>\n"
        raw: "The 6B model has run on generation and the eos_token_id is not set.\r\
          \n\r\nHere is the corrected version that produces the intended results:\
          \ \r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\n# Load the model\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          01-ai/Yi-6B\", device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\r\
          \n\r\n# Load the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          01-ai/Yi-6B\", trust_remote_code=True)\r\n\r\n# Encode the input text\r\n\
          inputs = tokenizer(\"There's a place where time stands still. A place of\
          \ breath taking wonder, but also\", return_tensors=\"pt\")\r\nmax_length\
          \ = 256 \r\n\r\n# Generate output with the end-of-sequence token\r\noutputs\
          \ = model.generate(\r\n    inputs.input_ids.cuda(),\r\n    max_length=max_length,\r\
          \n    eos_token_id=tokenizer.eos_token_id  # Use the EOS token ID which\
          \ is 2\r\n)\r\n\r\n# Decode and print the output\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\r\n```\r\neos_token_id=2 if anyone wants the\
          \ value\r\n\r\nSpecial Tokens: {'bos_token': '<|startoftext|>', 'eos_token':\
          \ '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\r\nVocabulary\
          \ Size: 64000\r\nID 0 : Token <unk>\r\nID 1 : Token <|startoftext|>\r\n\
          ID 2 : Token <|endoftext|>\r\n"
        updatedAt: '2023-11-06T21:02:32.643Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - FancyZhao
    id: 654954686299d81663893569
    type: comment
  author: macadeliccc
  content: "The 6B model has run on generation and the eos_token_id is not set.\r\n\
    \r\nHere is the corrected version that produces the intended results: \r\n\r\n\
    ```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\
    \n# Load the model\r\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-6B\"\
    , device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\r\n\r\n#\
    \ Load the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-6B\"\
    , trust_remote_code=True)\r\n\r\n# Encode the input text\r\ninputs = tokenizer(\"\
    There's a place where time stands still. A place of breath taking wonder, but\
    \ also\", return_tensors=\"pt\")\r\nmax_length = 256 \r\n\r\n# Generate output\
    \ with the end-of-sequence token\r\noutputs = model.generate(\r\n    inputs.input_ids.cuda(),\r\
    \n    max_length=max_length,\r\n    eos_token_id=tokenizer.eos_token_id  # Use\
    \ the EOS token ID which is 2\r\n)\r\n\r\n# Decode and print the output\r\nprint(tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True))\r\n```\r\neos_token_id=2 if anyone wants the value\r\
    \n\r\nSpecial Tokens: {'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>',\
    \ 'unk_token': '<unk>', 'pad_token': '<unk>'}\r\nVocabulary Size: 64000\r\nID\
    \ 0 : Token <unk>\r\nID 1 : Token <|startoftext|>\r\nID 2 : Token <|endoftext|>\r\
    \n"
  created_at: 2023-11-06 21:02:32+00:00
  edited: false
  hidden: false
  id: 654954686299d81663893569
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-07T07:38:05.000Z'
    data:
      edited: false
      editors:
      - FancyZhao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7266313433647156
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
          fullname: FancyZhao
          isHf: false
          isPro: false
          name: FancyZhao
          type: user
        html: '<p>Thanks, should be fixed in <a rel="nofollow" href="https://github.com/01-ai/Yi/pull/33">https://github.com/01-ai/Yi/pull/33</a>
          .</p>

          '
        raw: Thanks, should be fixed in https://github.com/01-ai/Yi/pull/33 .
        updatedAt: '2023-11-07T07:38:05.341Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - macadeliccc
      relatedEventId: 6549e95d3ee6a84ff2f97b78
    id: 6549e95d3ee6a84ff2f97b77
    type: comment
  author: FancyZhao
  content: Thanks, should be fixed in https://github.com/01-ai/Yi/pull/33 .
  created_at: 2023-11-07 07:38:05+00:00
  edited: false
  hidden: false
  id: 6549e95d3ee6a84ff2f97b77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/WTAS-SnQ_rnriMPGSMkrH.jpeg?w=200&h=200&f=face
      fullname: FancyZhao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FancyZhao
      type: user
    createdAt: '2023-11-07T07:38:05.000Z'
    data:
      status: closed
    id: 6549e95d3ee6a84ff2f97b78
    type: status-change
  author: FancyZhao
  created_at: 2023-11-07 07:38:05+00:00
  id: 6549e95d3ee6a84ff2f97b78
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: 01-ai/Yi-6B
repo_type: model
status: closed
target_branch: null
title: fix for run on generation
