!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2023-07-02 16:28:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-02T17:28:24.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518750905990601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>I know that this model has it''s own monkey patches for forward
          function and additional rotary positional embedding layer. But, because
          it''s not added here, loading model blindly using transformers might not
          work as expected. At least put the monkey patches in the repo so user can
          load the model easily using transformers. Or you could add instruction to
          load the model in the model card.</p>

          '
        raw: I know that this model has it's own monkey patches for forward function
          and additional rotary positional embedding layer. But, because it's not
          added here, loading model blindly using transformers might not work as expected.
          At least put the monkey patches in the repo so user can load the model easily
          using transformers. Or you could add instruction to load the model in the
          model card.
        updatedAt: '2023-07-02T17:28:24.702Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - gonanc13
        - viktor-ferenczi
    id: 64a1b3b8660cce8b86ca9b44
    type: comment
  author: fahadh4ilyas
  content: I know that this model has it's own monkey patches for forward function
    and additional rotary positional embedding layer. But, because it's not added
    here, loading model blindly using transformers might not work as expected. At
    least put the monkey patches in the repo so user can load the model easily using
    transformers. Or you could add instruction to load the model in the model card.
  created_at: 2023-07-02 16:28:24+00:00
  edited: false
  hidden: false
  id: 64a1b3b8660cce8b86ca9b44
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lmsys/longchat-13b-16k
repo_type: model
status: open
target_branch: null
title: Please add the monkey patch so it can be load using transformers with `trust_remote_code=True`
