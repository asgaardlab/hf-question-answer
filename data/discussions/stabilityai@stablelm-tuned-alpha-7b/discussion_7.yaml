!!python/object:huggingface_hub.community.DiscussionWithDetails
author: oeathus
conflicting_files: null
created_at: 2023-04-26 10:32:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bf2d10ed5d04d346edcdfc4a91f3e81.svg
      fullname: Timothy Haskins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oeathus
      type: user
    createdAt: '2023-04-26T11:32:46.000Z'
    data:
      edited: false
      editors:
      - oeathus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bf2d10ed5d04d346edcdfc4a91f3e81.svg
          fullname: Timothy Haskins
          isHf: false
          isPro: false
          name: oeathus
          type: user
        html: '<p>I converted and quantized these with <a rel="nofollow" href="https://github.com/ggerganov/ggml/">https://github.com/ggerganov/ggml/</a>
          on a MacBook pro M1 w/ 16GB RAM.</p>

          <p><a href="https://huggingface.co/oeathus/stablelm-tuned-alpha-7b-ggml-q4">https://huggingface.co/oeathus/stablelm-tuned-alpha-7b-ggml-q4</a></p>

          '
        raw: "I converted and quantized these with https://github.com/ggerganov/ggml/\
          \ on a MacBook pro M1 w/ 16GB RAM.\r\n\r\nhttps://huggingface.co/oeathus/stablelm-tuned-alpha-7b-ggml-q4\r\
          \n"
        updatedAt: '2023-04-26T11:32:46.934Z'
      numEdits: 0
      reactions: []
    id: 64490bde35cc8107ecbcf167
    type: comment
  author: oeathus
  content: "I converted and quantized these with https://github.com/ggerganov/ggml/\
    \ on a MacBook pro M1 w/ 16GB RAM.\r\n\r\nhttps://huggingface.co/oeathus/stablelm-tuned-alpha-7b-ggml-q4\r\
    \n"
  created_at: 2023-04-26 10:32:46+00:00
  edited: false
  hidden: false
  id: 64490bde35cc8107ecbcf167
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e906a44ef973fe5d32444a225b6846b8.svg
      fullname: Andy Miller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andy-og
      type: user
    createdAt: '2023-04-28T20:29:51.000Z'
    data:
      edited: false
      editors:
      - andy-og
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e906a44ef973fe5d32444a225b6846b8.svg
          fullname: Andy Miller
          isHf: false
          isPro: false
          name: andy-og
          type: user
        html: "<p>Can you give me a heads up on how to plug these in and perform some\
          \ local inference on my mac? Here is what I have so far: </p>\n<pre><code>def\
          \ hugging_local(text=\"Can you please let us know more details about your\
          \ \"):\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          \n    tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-tuned-alpha-7b\"\
          )\n\n    model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-tuned-alpha-7b\"\
          )\n\n    from langchain.llms import HuggingFacePipeline\n    llm = HuggingFacePipeline(model=model,\
          \ tokenizer=tokenizer)\n\n    template = \"\"\"Question: {question}\n\n\
          \    Answer: \"\"\"\n    prompt = PromptTemplate(template=template, input_variables=[\"\
          question\"])\n    llm_chain = LLMChain(prompt=prompt, llm=llm)\n\n    question\
          \ = \"Who won the FIFA World Cup in the year 1994? \"\n\n    print(llm_chain.run(question))\n\
          \n    return\n\nif __name__ == '__main__':\n\n    # result = hugging_lang()\n\
          \    # result = hugging_raw(text=test_text)\n    result = hugging_local(text=test_text)\n\
          \n    print(result)\n</code></pre>\n"
        raw: "Can you give me a heads up on how to plug these in and perform some\
          \ local inference on my mac? Here is what I have so far: \n\n```\ndef hugging_local(text=\"\
          Can you please let us know more details about your \"):\n    from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablelm-tuned-alpha-7b\")\n\n    model = AutoModelForCausalLM.from_pretrained(\"\
          stabilityai/stablelm-tuned-alpha-7b\")\n\n    from langchain.llms import\
          \ HuggingFacePipeline\n    llm = HuggingFacePipeline(model=model, tokenizer=tokenizer)\n\
          \n    template = \"\"\"Question: {question}\n\n    Answer: \"\"\"\n    prompt\
          \ = PromptTemplate(template=template, input_variables=[\"question\"])\n\
          \    llm_chain = LLMChain(prompt=prompt, llm=llm)\n\n    question = \"Who\
          \ won the FIFA World Cup in the year 1994? \"\n\n    print(llm_chain.run(question))\n\
          \n    return\n\nif __name__ == '__main__':\n\n    # result = hugging_lang()\n\
          \    # result = hugging_raw(text=test_text)\n    result = hugging_local(text=test_text)\n\
          \n    print(result)\n```"
        updatedAt: '2023-04-28T20:29:51.902Z'
      numEdits: 0
      reactions: []
    id: 644c2cbf194e124dacbe0091
    type: comment
  author: andy-og
  content: "Can you give me a heads up on how to plug these in and perform some local\
    \ inference on my mac? Here is what I have so far: \n\n```\ndef hugging_local(text=\"\
    Can you please let us know more details about your \"):\n    from transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\n\n    tokenizer = AutoTokenizer.from_pretrained(\"\
    stabilityai/stablelm-tuned-alpha-7b\")\n\n    model = AutoModelForCausalLM.from_pretrained(\"\
    stabilityai/stablelm-tuned-alpha-7b\")\n\n    from langchain.llms import HuggingFacePipeline\n\
    \    llm = HuggingFacePipeline(model=model, tokenizer=tokenizer)\n\n    template\
    \ = \"\"\"Question: {question}\n\n    Answer: \"\"\"\n    prompt = PromptTemplate(template=template,\
    \ input_variables=[\"question\"])\n    llm_chain = LLMChain(prompt=prompt, llm=llm)\n\
    \n    question = \"Who won the FIFA World Cup in the year 1994? \"\n\n    print(llm_chain.run(question))\n\
    \n    return\n\nif __name__ == '__main__':\n\n    # result = hugging_lang()\n\
    \    # result = hugging_raw(text=test_text)\n    result = hugging_local(text=test_text)\n\
    \n    print(result)\n```"
  created_at: 2023-04-28 19:29:51+00:00
  edited: false
  hidden: false
  id: 644c2cbf194e124dacbe0091
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bf2d10ed5d04d346edcdfc4a91f3e81.svg
      fullname: Timothy Haskins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oeathus
      type: user
    createdAt: '2023-04-28T20:33:53.000Z'
    data:
      edited: false
      editors:
      - oeathus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bf2d10ed5d04d346edcdfc4a91f3e81.svg
          fullname: Timothy Haskins
          isHf: false
          isPro: false
          name: oeathus
          type: user
        html: '<p>I''m still wrapping my head around the GGML format. My understanding
          is that it is a custom serialized binary format that sorta zips the parameters
          and other essentials on top of the actual neural net. I don''t think you
          can run these with the Hugging Face transformers library, but I''m not terribly
          confident about that.</p>

          '
        raw: I'm still wrapping my head around the GGML format. My understanding is
          that it is a custom serialized binary format that sorta zips the parameters
          and other essentials on top of the actual neural net. I don't think you
          can run these with the Hugging Face transformers library, but I'm not terribly
          confident about that.
        updatedAt: '2023-04-28T20:33:53.473Z'
      numEdits: 0
      reactions: []
    id: 644c2db1194e124dacbe38a8
    type: comment
  author: oeathus
  content: I'm still wrapping my head around the GGML format. My understanding is
    that it is a custom serialized binary format that sorta zips the parameters and
    other essentials on top of the actual neural net. I don't think you can run these
    with the Hugging Face transformers library, but I'm not terribly confident about
    that.
  created_at: 2023-04-28 19:33:53+00:00
  edited: false
  hidden: false
  id: 644c2db1194e124dacbe38a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e906a44ef973fe5d32444a225b6846b8.svg
      fullname: Andy Miller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andy-og
      type: user
    createdAt: '2023-04-28T20:36:01.000Z'
    data:
      edited: false
      editors:
      - andy-og
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e906a44ef973fe5d32444a225b6846b8.svg
          fullname: Andy Miller
          isHf: false
          isPro: false
          name: andy-og
          type: user
        html: '<p>Okay, yeah, I am struggling. I also was trying to use the hosted
          inference and it just times out constantly.</p>

          '
        raw: Okay, yeah, I am struggling. I also was trying to use the hosted inference
          and it just times out constantly.
        updatedAt: '2023-04-28T20:36:01.690Z'
      numEdits: 0
      reactions: []
    id: 644c2e31ed08a4fdf4e288b6
    type: comment
  author: andy-og
  content: Okay, yeah, I am struggling. I also was trying to use the hosted inference
    and it just times out constantly.
  created_at: 2023-04-28 19:36:01+00:00
  edited: false
  hidden: false
  id: 644c2e31ed08a4fdf4e288b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/538434a695c5abda23d81a0ce0fbc135.svg
      fullname: Lazar Dilov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ldilov
      type: user
    createdAt: '2023-05-14T05:31:06.000Z'
    data:
      edited: false
      editors:
      - ldilov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/538434a695c5abda23d81a0ce0fbc135.svg
          fullname: Lazar Dilov
          isHf: false
          isPro: false
          name: ldilov
          type: user
        html: '<p>ldilov/stablelm-tuned-alpha-7b-4bit-128g-descact-sym-true-sequential</p>

          '
        raw: ldilov/stablelm-tuned-alpha-7b-4bit-128g-descact-sym-true-sequential
        updatedAt: '2023-05-14T05:31:06.551Z'
      numEdits: 0
      reactions: []
    id: 6460721a446a4fa4695cc737
    type: comment
  author: ldilov
  content: ldilov/stablelm-tuned-alpha-7b-4bit-128g-descact-sym-true-sequential
  created_at: 2023-05-14 04:31:06+00:00
  edited: false
  hidden: false
  id: 6460721a446a4fa4695cc737
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/stablelm-tuned-alpha-7b
repo_type: model
status: open
target_branch: null
title: GGML f16, q4_0, q4_1, q4_2, q4_3
