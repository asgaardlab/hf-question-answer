!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BloodOfTheRock
conflicting_files: null
created_at: 2023-11-07 19:23:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
      fullname: ArtifartX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BloodOfTheRock
      type: user
    createdAt: '2023-11-07T19:23:27.000Z'
    data:
      edited: true
      editors:
      - BloodOfTheRock
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168794751167297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
          fullname: ArtifartX
          isHf: false
          isPro: false
          name: BloodOfTheRock
          type: user
        html: '<p>Was excited to see these new large context models released, but
          I can''t seem to get coherent results out of them if attempting to use a
          large amount of input text. If you chat to it "normally" with very short
          queries and it responding with short responses, it seems to work fine, but
          if you try to utilize the large context window it fails to properly function,
          which seems to defeat the purpose of the model, unless I making some mistake.
          </p>

          <p>For example, if I just ask it how it is doing, it responds normally like
          so:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/SyrWOk_OVV-wagzkLEjTK.png"><img
          alt="MkZG80v.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/SyrWOk_OVV-wagzkLEjTK.png"></a></p>

          <p>However, if I paste the raw test of an entire new article online (still
          FAR under the 128k context length) and ask it for a short summary of the
          article, it responds which gibberish, like this:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/rhITLcM9ro1XgnoxoFtMw.png"><img
          alt="T1rTcYI.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/rhITLcM9ro1XgnoxoFtMw.png"></a></p>

          <p>Or sometimes it fails in other ways ,like responding with just a single
          character. </p>

          <p>At any rate, I have been unable to utilize the large context window in
          any meaningful way, so I was wondering if I was perhaps doing something
          wrong? Just using it in ooba. The GGUF versions also behave in the exact
          same way.</p>

          '
        raw: "Was excited to see these new large context models released, but I can't\
          \ seem to get coherent results out of them if attempting to use a large\
          \ amount of input text. If you chat to it \"normally\" with very short queries\
          \ and it responding with short responses, it seems to work fine, but if\
          \ you try to utilize the large context window it fails to properly function,\
          \ which seems to defeat the purpose of the model, unless I making some mistake.\
          \ \n\nFor example, if I just ask it how it is doing, it responds normally\
          \ like so:\n\n![MkZG80v.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/SyrWOk_OVV-wagzkLEjTK.png)\n\
          \nHowever, if I paste the raw test of an entire new article online (still\
          \ FAR under the 128k context length) and ask it for a short summary of the\
          \ article, it responds which gibberish, like this:\n\n![T1rTcYI.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/rhITLcM9ro1XgnoxoFtMw.png)\n\
          \nOr sometimes it fails in other ways ,like responding with just a single\
          \ character. \n\nAt any rate, I have been unable to utilize the large context\
          \ window in any meaningful way, so I was wondering if I was perhaps doing\
          \ something wrong? Just using it in ooba. The GGUF versions also behave\
          \ in the exact same way."
        updatedAt: '2023-11-07T19:23:49.295Z'
      numEdits: 1
      reactions: []
    id: 654a8eaf20a6ffde9358d116
    type: comment
  author: BloodOfTheRock
  content: "Was excited to see these new large context models released, but I can't\
    \ seem to get coherent results out of them if attempting to use a large amount\
    \ of input text. If you chat to it \"normally\" with very short queries and it\
    \ responding with short responses, it seems to work fine, but if you try to utilize\
    \ the large context window it fails to properly function, which seems to defeat\
    \ the purpose of the model, unless I making some mistake. \n\nFor example, if\
    \ I just ask it how it is doing, it responds normally like so:\n\n![MkZG80v.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/SyrWOk_OVV-wagzkLEjTK.png)\n\
    \nHowever, if I paste the raw test of an entire new article online (still FAR\
    \ under the 128k context length) and ask it for a short summary of the article,\
    \ it responds which gibberish, like this:\n\n![T1rTcYI.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/rhITLcM9ro1XgnoxoFtMw.png)\n\
    \nOr sometimes it fails in other ways ,like responding with just a single character.\
    \ \n\nAt any rate, I have been unable to utilize the large context window in any\
    \ meaningful way, so I was wondering if I was perhaps doing something wrong? Just\
    \ using it in ooba. The GGUF versions also behave in the exact same way."
  created_at: 2023-11-07 19:23:27+00:00
  edited: true
  hidden: false
  id: 654a8eaf20a6ffde9358d116
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
      fullname: ArtifartX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BloodOfTheRock
      type: user
    createdAt: '2023-11-07T20:02:43.000Z'
    data:
      edited: false
      editors:
      - BloodOfTheRock
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9369859099388123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
          fullname: ArtifartX
          isHf: false
          isPro: false
          name: BloodOfTheRock
          type: user
        html: '<p>I thought maybe something was wrong with my input text or maybe
          it wasn''t properly sanitized, so I have tried with many different sources
          of input text and the behavior is the same. Here is an example with about
          30k tokens as input:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/ZUA8MlQL64afbb07YKBWn.png"><img
          alt="Dsqt6oZ.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/ZUA8MlQL64afbb07YKBWn.png"></a></p>

          '
        raw: 'I thought maybe something was wrong with my input text or maybe it wasn''t
          properly sanitized, so I have tried with many different sources of input
          text and the behavior is the same. Here is an example with about 30k tokens
          as input:



          ![Dsqt6oZ.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/ZUA8MlQL64afbb07YKBWn.png)

          '
        updatedAt: '2023-11-07T20:02:43.766Z'
      numEdits: 0
      reactions: []
    id: 654a97e3f39b362cffbebeb3
    type: comment
  author: BloodOfTheRock
  content: 'I thought maybe something was wrong with my input text or maybe it wasn''t
    properly sanitized, so I have tried with many different sources of input text
    and the behavior is the same. Here is an example with about 30k tokens as input:



    ![Dsqt6oZ.png](https://cdn-uploads.huggingface.co/production/uploads/63cae7e39f78909f9f8090b9/ZUA8MlQL64afbb07YKBWn.png)

    '
  created_at: 2023-11-07 20:02:43+00:00
  edited: false
  hidden: false
  id: 654a97e3f39b362cffbebeb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-07T20:43:41.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9770933985710144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I have not had much luck with the longer these Yarn context models
          yet either. From some descriptions from people on TheBloke''s Discord, the
          new Amazon MistralLite seems to have great usable context length (Turboderp
          mentions he got it to go past 38K.)  I suspect the Alpha or other parameters
          will need to be set properly with a long-context prompt to have coherent
          output.</p>

          '
        raw: I have not had much luck with the longer these Yarn context models yet
          either. From some descriptions from people on TheBloke's Discord, the new
          Amazon MistralLite seems to have great usable context length (Turboderp
          mentions he got it to go past 38K.)  I suspect the Alpha or other parameters
          will need to be set properly with a long-context prompt to have coherent
          output.
        updatedAt: '2023-11-07T20:45:04.494Z'
      numEdits: 2
      reactions: []
    id: 654aa17de4432d6c7bff357f
    type: comment
  author: LoneStriker
  content: I have not had much luck with the longer these Yarn context models yet
    either. From some descriptions from people on TheBloke's Discord, the new Amazon
    MistralLite seems to have great usable context length (Turboderp mentions he got
    it to go past 38K.)  I suspect the Alpha or other parameters will need to be set
    properly with a long-context prompt to have coherent output.
  created_at: 2023-11-07 20:43:41+00:00
  edited: true
  hidden: false
  id: 654aa17de4432d6c7bff357f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
      fullname: ArtifartX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BloodOfTheRock
      type: user
    createdAt: '2023-11-07T21:25:02.000Z'
    data:
      edited: true
      editors:
      - BloodOfTheRock
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9335552453994751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c8233b6dc9522c23ae6ee85f51055ca.svg
          fullname: ArtifartX
          isHf: false
          isPro: false
          name: BloodOfTheRock
          type: user
        html: '<p>Tested your MistralLite-5.0bpw-h6-exl2 at around 25k-30k tokens
          and it worked first test (same input that produced last image above)</p>

          '
        raw: Tested your MistralLite-5.0bpw-h6-exl2 at around 25k-30k tokens and it
          worked first test (same input that produced last image above)
        updatedAt: '2023-11-07T21:25:28.409Z'
      numEdits: 2
      reactions: []
    id: 654aab2edff2f49007d5164f
    type: comment
  author: BloodOfTheRock
  content: Tested your MistralLite-5.0bpw-h6-exl2 at around 25k-30k tokens and it
    worked first test (same input that produced last image above)
  created_at: 2023-11-07 21:25:02+00:00
  edited: true
  hidden: false
  id: 654aab2edff2f49007d5164f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34d6301aeb49fa06c07295066ab9e0b4.svg
      fullname: stephen fingleton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sfingali
      type: user
    createdAt: '2023-11-09T13:29:47.000Z'
    data:
      edited: false
      editors:
      - sfingali
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9571955800056458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34d6301aeb49fa06c07295066ab9e0b4.svg
          fullname: stephen fingleton
          isHf: false
          isPro: false
          name: sfingali
          type: user
        html: '<p>I have the same issues. It''s producing nonsense.</p>

          '
        raw: I have the same issues. It's producing nonsense.
        updatedAt: '2023-11-09T13:29:47.393Z'
      numEdits: 0
      reactions: []
    id: 654cdecbd8ff621f135a8d94
    type: comment
  author: sfingali
  content: I have the same issues. It's producing nonsense.
  created_at: 2023-11-09 13:29:47+00:00
  edited: false
  hidden: false
  id: 654cdecbd8ff621f135a8d94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-10T16:27:16.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9519546031951904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I tested both the unquantized base model and the 8.0bpw version
          and they both behaved the same and was able to return non-gibberish inference.  In
          ooba, I set the max token length to 32K. The only setting I changed was
          this one:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/BthxaJvDlQCPR_xds5V5k.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/BthxaJvDlQCPR_xds5V5k.png"></a></p>

          <p>I have not tried to go to very high tokens though. Basic tests seem to
          work as expected.</p>

          '
        raw: 'I tested both the unquantized base model and the 8.0bpw version and
          they both behaved the same and was able to return non-gibberish inference.  In
          ooba, I set the max token length to 32K. The only setting I changed was
          this one:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/BthxaJvDlQCPR_xds5V5k.png)


          I have not tried to go to very high tokens though. Basic tests seem to work
          as expected.'
        updatedAt: '2023-11-10T16:27:16.902Z'
      numEdits: 0
      reactions: []
    id: 654e59e4a4ae3e807241d3f3
    type: comment
  author: LoneStriker
  content: 'I tested both the unquantized base model and the 8.0bpw version and they
    both behaved the same and was able to return non-gibberish inference.  In ooba,
    I set the max token length to 32K. The only setting I changed was this one:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/BthxaJvDlQCPR_xds5V5k.png)


    I have not tried to go to very high tokens though. Basic tests seem to work as
    expected.'
  created_at: 2023-11-10 16:27:16+00:00
  edited: false
  hidden: false
  id: 654e59e4a4ae3e807241d3f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Yarn-Mistral-7b-128k-8.0bpw-h8-exl2
repo_type: model
status: open
target_branch: null
title: Issues at high context lengths
