!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-12-08 19:20:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-12-08T19:20:16.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9220083951950073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>This model sounds exciting! <span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ or <span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Panchovix\">@<span class=\"\
          underline\">Panchovix</span></a></span>\n\n\t</span></span> - would you\
          \ consider making an EXL2 3bpw, possibly with rpcal?</p>\n"
        raw: This model sounds exciting! @LoneStriker or @Panchovix - would you consider
          making an EXL2 3bpw, possibly with rpcal?
        updatedAt: '2023-12-08T19:20:16.803Z'
      numEdits: 0
      reactions: []
    id: 65736c70412ee7018590a43c
    type: comment
  author: wolfram
  content: This model sounds exciting! @LoneStriker or @Panchovix - would you consider
    making an EXL2 3bpw, possibly with rpcal?
  created_at: 2023-12-08 19:20:16+00:00
  edited: false
  hidden: false
  id: 65736c70412ee7018590a43c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-09T14:57:53.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922663569450378
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I can try running this with Turbo''s new quant method that does
          a better job at lower quants.  It''ll still take ages though, but not as
          long.</p>

          '
        raw: I can try running this with Turbo's new quant method that does a better
          job at lower quants.  It'll still take ages though, but not as long.
        updatedAt: '2023-12-09T14:57:53.073Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - wolfram
        - OrangeApples
        - RYSKZ
      - count: 1
        reaction: "\U0001F917"
        users:
        - wolfram
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - wolfram
    id: 65748071a90ae2daaeae60df
    type: comment
  author: LoneStriker
  content: I can try running this with Turbo's new quant method that does a better
    job at lower quants.  It'll still take ages though, but not as long.
  created_at: 2023-12-09 14:57:53+00:00
  edited: false
  hidden: false
  id: 65748071a90ae2daaeae60df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
      fullname: Orange Apples
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OrangeApples
      type: user
    createdAt: '2023-12-09T16:44:49.000Z'
    data:
      edited: false
      editors:
      - OrangeApples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9113078713417053
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
          fullname: Orange Apples
          isHf: false
          isPro: false
          name: OrangeApples
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \  Is the new quant method you're referring to QuIP? I hear pretty miraculous\
          \ things about it for 2-bit quants compared to everything else.</p>\n"
        raw: '@LoneStriker  Is the new quant method you''re referring to QuIP? I hear
          pretty miraculous things about it for 2-bit quants compared to everything
          else.'
        updatedAt: '2023-12-09T16:44:49.904Z'
      numEdits: 0
      reactions: []
    id: 65749981597698af2dfb38b2
    type: comment
  author: OrangeApples
  content: '@LoneStriker  Is the new quant method you''re referring to QuIP? I hear
    pretty miraculous things about it for 2-bit quants compared to everything else.'
  created_at: 2023-12-09 16:44:49+00:00
  edited: false
  hidden: false
  id: 65749981597698af2dfb38b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-09T17:07:00.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9622079133987427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \  Is the new quant method you're referring to QuIP? I hear pretty miraculous\
          \ things about it for 2-bit quants compared to everything else.</p>\n</blockquote>\n\
          <p>Improved exllamav2 quant method, should do much better at low bpw like\
          \ we'll need for this model.  exl2 is already comparable to QuIP in many\
          \ ways and is much faster.</p>\n"
        raw: '> @LoneStriker  Is the new quant method you''re referring to QuIP? I
          hear pretty miraculous things about it for 2-bit quants compared to everything
          else.


          Improved exllamav2 quant method, should do much better at low bpw like we''ll
          need for this model.  exl2 is already comparable to QuIP in many ways and
          is much faster.'
        updatedAt: '2023-12-09T17:07:00.010Z'
      numEdits: 0
      reactions: []
    id: 65749eb45da75f987fdd26d8
    type: comment
  author: LoneStriker
  content: '> @LoneStriker  Is the new quant method you''re referring to QuIP? I hear
    pretty miraculous things about it for 2-bit quants compared to everything else.


    Improved exllamav2 quant method, should do much better at low bpw like we''ll
    need for this model.  exl2 is already comparable to QuIP in many ways and is much
    faster.'
  created_at: 2023-12-09 17:07:00+00:00
  edited: false
  hidden: false
  id: 65749eb45da75f987fdd26d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
      fullname: Orange Apples
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OrangeApples
      type: user
    createdAt: '2023-12-09T20:24:41.000Z'
    data:
      edited: false
      editors:
      - OrangeApples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.931654691696167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6rJmMwNSqswKhvckak5vn.jpeg?w=200&h=200&f=face
          fullname: Orange Apples
          isHf: false
          isPro: false
          name: OrangeApples
          type: user
        html: '<p>Thanks for clarifying! Had no idea that there is an improved exllamav2
          quant method. Looking forward to testing out how 2-bit 70B models perform
          with the improvements.</p>

          '
        raw: Thanks for clarifying! Had no idea that there is an improved exllamav2
          quant method. Looking forward to testing out how 2-bit 70B models perform
          with the improvements.
        updatedAt: '2023-12-09T20:24:41.244Z'
      numEdits: 0
      reactions: []
    id: 6574cd0933e5a4bf5b43d975
    type: comment
  author: OrangeApples
  content: Thanks for clarifying! Had no idea that there is an improved exllamav2
    quant method. Looking forward to testing out how 2-bit 70B models perform with
    the improvements.
  created_at: 2023-12-09 20:24:41+00:00
  edited: false
  hidden: false
  id: 6574cd0933e5a4bf5b43d975
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-09T21:12:24.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8024895191192627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<blockquote>

          <p>Thanks for clarifying! Had no idea that there is an improved exllamav2
          quant method. Looking forward to testing out how 2-bit 70B models perform
          with the improvements.</p>

          </blockquote>

          <p>You can try this older airoboros 1.4.1 model re-quantized with the new
          method (still one of my favorite 70Bs):<br><a href="https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-2.4bpw-h6-exl2-2">https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-2.4bpw-h6-exl2-2</a></p>

          <p>Perplexity of the old vs. new exl2 quant is improved:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/A63B18uFVFCiMUzBThFZe.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/A63B18uFVFCiMUzBThFZe.png"></a></p>

          '
        raw: '> Thanks for clarifying! Had no idea that there is an improved exllamav2
          quant method. Looking forward to testing out how 2-bit 70B models perform
          with the improvements.


          You can try this older airoboros 1.4.1 model re-quantized with the new method
          (still one of my favorite 70Bs):

          https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-2.4bpw-h6-exl2-2


          Perplexity of the old vs. new exl2 quant is improved:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/A63B18uFVFCiMUzBThFZe.png)

          '
        updatedAt: '2023-12-09T21:12:24.269Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - OrangeApples
    id: 6574d8389fe27c093d75b8ec
    type: comment
  author: LoneStriker
  content: '> Thanks for clarifying! Had no idea that there is an improved exllamav2
    quant method. Looking forward to testing out how 2-bit 70B models perform with
    the improvements.


    You can try this older airoboros 1.4.1 model re-quantized with the new method
    (still one of my favorite 70Bs):

    https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-2.4bpw-h6-exl2-2


    Perplexity of the old vs. new exl2 quant is improved:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/A63B18uFVFCiMUzBThFZe.png)

    '
  created_at: 2023-12-09 21:12:24+00:00
  edited: false
  hidden: false
  id: 6574d8389fe27c093d75b8ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-12-09T22:38:01.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8218979239463806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ That's a massive perplexity improvement. Do you (or <span data-props=\"\
          {&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Panchovix\">@<span class=\"underline\">Panchovix</span></a></span>\n\
          \n\t</span></span>) plan to requantize other models that would benefit a\
          \ lot from the newer method, e. g. Goliath 120B?</p>\n<p>Anyway, thank you\
          \ very much for <a href=\"https://huggingface.co/LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2\"\
          >LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2</a>! I'll test this model next.</p>\n"
        raw: '@LoneStriker That''s a massive perplexity improvement. Do you (or @Panchovix)
          plan to requantize other models that would benefit a lot from the newer
          method, e. g. Goliath 120B?


          Anyway, thank you very much for [LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2](https://huggingface.co/LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2)!
          I''ll test this model next.'
        updatedAt: '2023-12-09T22:38:01.623Z'
      numEdits: 0
      reactions: []
    id: 6574ec4988805b3ba1e6ee87
    type: comment
  author: wolfram
  content: '@LoneStriker That''s a massive perplexity improvement. Do you (or @Panchovix)
    plan to requantize other models that would benefit a lot from the newer method,
    e. g. Goliath 120B?


    Anyway, thank you very much for [LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2](https://huggingface.co/LoneStriker/DiscoLM-120b-2.65bpw-h6-exl2-2)!
    I''ll test this model next.'
  created_at: 2023-12-09 22:38:01+00:00
  edited: false
  hidden: false
  id: 6574ec4988805b3ba1e6ee87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-12-09T22:45:10.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9878857731819153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I have plans to re-do Goliath with the new method, but I can''t
          give an exact date when they will be up.</p>

          '
        raw: I have plans to re-do Goliath with the new method, but I can't give an
          exact date when they will be up.
        updatedAt: '2023-12-09T22:45:10.953Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - RYSKZ
    id: 6574edf630a4401dfb685f84
    type: comment
  author: Panchovix
  content: I have plans to re-do Goliath with the new method, but I can't give an
    exact date when they will be up.
  created_at: 2023-12-09 22:45:10+00:00
  edited: false
  hidden: false
  id: 6574edf630a4401dfb685f84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-12-09T23:27:02.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.851800799369812
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Panchovix\">@<span class=\"\
          underline\">Panchovix</span></a></span>\n\n\t</span></span> Thanks, I can\
          \ wait, have a lot of other models to test still. Like this one, DiscoLM\
          \ 120B. Let's see if it can dethrone my current favorite local model, Goliath...</p>\n"
        raw: '@Panchovix Thanks, I can wait, have a lot of other models to test still.
          Like this one, DiscoLM 120B. Let''s see if it can dethrone my current favorite
          local model, Goliath...'
        updatedAt: '2023-12-09T23:27:02.185Z'
      numEdits: 0
      reactions: []
    id: 6574f7c6c13a2c7478386eae
    type: comment
  author: wolfram
  content: '@Panchovix Thanks, I can wait, have a lot of other models to test still.
    Like this one, DiscoLM 120B. Let''s see if it can dethrone my current favorite
    local model, Goliath...'
  created_at: 2023-12-09 23:27:02+00:00
  edited: false
  hidden: false
  id: 6574f7c6c13a2c7478386eae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-12-10T00:44:23.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9966561198234558
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I''m waiting for some new updates from turbo, since it seems more
          few commits are going to the experimental branch, that would help even more
          than now.</p>

          '
        raw: I'm waiting for some new updates from turbo, since it seems more few
          commits are going to the experimental branch, that would help even more
          than now.
        updatedAt: '2023-12-10T00:44:23.581Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RYSKZ
        - wolfram
    id: 657509e79fe27c093d7eb3e5
    type: comment
  author: Panchovix
  content: I'm waiting for some new updates from turbo, since it seems more few commits
    are going to the experimental branch, that would help even more than now.
  created_at: 2023-12-10 00:44:23+00:00
  edited: false
  hidden: false
  id: 657509e79fe27c093d7eb3e5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: DiscoResearch/DiscoLM-120b
repo_type: model
status: open
target_branch: null
title: Summoning EXL2 version ;)
