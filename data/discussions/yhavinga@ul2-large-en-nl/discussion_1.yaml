!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jhpassion0621
conflicting_files: null
created_at: 2023-03-02 16:55:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672424714266-noauth.png?w=200&h=200&f=face
      fullname: JH Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jhpassion0621
      type: user
    createdAt: '2023-03-02T16:55:45.000Z'
    data:
      edited: false
      editors:
      - jhpassion0621
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672424714266-noauth.png?w=200&h=200&f=face
          fullname: JH Lee
          isHf: false
          isPro: false
          name: jhpassion0621
          type: user
        html: '<p>I saw you used ''silu'' function on config file but you mentioned
          ''gelu'' function on config.gin file. I would like to know how you did this.</p>

          '
        raw: I saw you used 'silu' function on config file but you mentioned 'gelu'
          function on config.gin file. I would like to know how you did this.
        updatedAt: '2023-03-02T16:55:45.745Z'
      numEdits: 0
      reactions: []
    id: 6400d511f6401f92ccf78d5e
    type: comment
  author: jhpassion0621
  content: I saw you used 'silu' function on config file but you mentioned 'gelu'
    function on config.gin file. I would like to know how you did this.
  created_at: 2023-03-02 16:55:45+00:00
  edited: false
  hidden: false
  id: 6400d511f6401f92ccf78d5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2023-03-03T09:00:46.000Z'
    data:
      edited: false
      editors:
      - yhavinga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
          fullname: Yeb Havinga
          isHf: false
          isPro: false
          name: yhavinga
          type: user
        html: '<p>I pretrained with t5x and when converting to huggingface format
          and writing the config.json I made a mistake to put ''silu'' for the activation
          function (as I was peering at google/ul2* config.json for inspiration).
          With that config.json I then finetuned (using huggingface sequence to sequence
          finetuning, so using silu) on en-nl and that is this model. So it is actually
          pre-trained with gelu and finetuned using silu. As I didn''t notice this
          until after fine-tuning, and the results looked good (very good actually
          compared to what I''ve been able to do with the same translation dataset
          and t5, though I think this is probably attributed to the MoD objective,
          not my activation function mixup) I decided to make the model publically
          available. I''ve fixed the config.json to use gelu for the pre-trained ul2-dutch
          models, and kept silu in the fine-tuned en-nl models that were actually
          finetuned using silu.</p>

          '
        raw: I pretrained with t5x and when converting to huggingface format and writing
          the config.json I made a mistake to put 'silu' for the activation function
          (as I was peering at google/ul2* config.json for inspiration). With that
          config.json I then finetuned (using huggingface sequence to sequence finetuning,
          so using silu) on en-nl and that is this model. So it is actually pre-trained
          with gelu and finetuned using silu. As I didn't notice this until after
          fine-tuning, and the results looked good (very good actually compared to
          what I've been able to do with the same translation dataset and t5, though
          I think this is probably attributed to the MoD objective, not my activation
          function mixup) I decided to make the model publically available. I've fixed
          the config.json to use gelu for the pre-trained ul2-dutch models, and kept
          silu in the fine-tuned en-nl models that were actually finetuned using silu.
        updatedAt: '2023-03-03T09:00:46.514Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - jhpassion0621
    id: 6401b73efc948f5b1660b4a8
    type: comment
  author: yhavinga
  content: I pretrained with t5x and when converting to huggingface format and writing
    the config.json I made a mistake to put 'silu' for the activation function (as
    I was peering at google/ul2* config.json for inspiration). With that config.json
    I then finetuned (using huggingface sequence to sequence finetuning, so using
    silu) on en-nl and that is this model. So it is actually pre-trained with gelu
    and finetuned using silu. As I didn't notice this until after fine-tuning, and
    the results looked good (very good actually compared to what I've been able to
    do with the same translation dataset and t5, though I think this is probably attributed
    to the MoD objective, not my activation function mixup) I decided to make the
    model publically available. I've fixed the config.json to use gelu for the pre-trained
    ul2-dutch models, and kept silu in the fine-tuned en-nl models that were actually
    finetuned using silu.
  created_at: 2023-03-03 09:00:46+00:00
  edited: false
  hidden: false
  id: 6401b73efc948f5b1660b4a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2023-03-09T07:42:50.000Z'
    data:
      status: closed
    id: 64098dfa582fb894c0580e35
    type: status-change
  author: yhavinga
  created_at: 2023-03-09 07:42:50+00:00
  id: 64098dfa582fb894c0580e35
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/62bb26c83152acf586a2105f353c3ee0.svg
      fullname: Yeb Havinga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yhavinga
      type: user
    createdAt: '2023-03-09T07:43:01.000Z'
    data:
      status: open
    id: 64098e05a17ce2e0f4a17ff9
    type: status-change
  author: yhavinga
  created_at: 2023-03-09 07:43:01+00:00
  id: 64098e05a17ce2e0f4a17ff9
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672424714266-noauth.png?w=200&h=200&f=face
      fullname: JH Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jhpassion0621
      type: user
    createdAt: '2023-03-29T15:25:41.000Z'
    data:
      status: closed
    id: 6424587510c8e82b09477804
    type: status-change
  author: jhpassion0621
  created_at: 2023-03-29 14:25:41+00:00
  id: 6424587510c8e82b09477804
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yhavinga/ul2-large-en-nl
repo_type: model
status: closed
target_branch: null
title: MLP activiation
