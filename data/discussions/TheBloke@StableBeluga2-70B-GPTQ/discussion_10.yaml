!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ijustloveses
conflicting_files: null
created_at: 2023-07-25 09:00:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e147a5a1026e01f9929a07bebeedb0a1.svg
      fullname: Ai Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ijustloveses
      type: user
    createdAt: '2023-07-25T10:00:21.000Z'
    data:
      edited: false
      editors:
      - ijustloveses
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2257525473833084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e147a5a1026e01f9929a07bebeedb0a1.svg
          fullname: Ai Li
          isHf: false
          isPro: false
          name: ijustloveses
          type: user
        html: "<p>\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/transformers/models/llama/modeling_lla\
          \ \u2502<br>\u2502 ma.py:197 in forward                                \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   194 \u2502   \u2502   bsz, q_len,\
          \ _ = hidden_states.size()                                             \
          \  \u2502<br>\u2502   195 \u2502   \u2502                              \
          \                                                        \u2502<br>\u2502\
          \   196 \u2502   \u2502   query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.   \u2502<br>\u2502 \u2771 197 \u2502   \u2502\
          \   key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads,\
          \ self.he   \u2502<br>\u2502   198 \u2502   \u2502   value_states = self.v_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.   \u2502<br>\u2502   199 \u2502   \u2502\
          \                                                                      \
          \                \u2502<br>\u2502   200 \u2502   \u2502   kv_seq_len = key_states.shape[-2]\
          \                                                  \u2502<br>\u2502    \
          \                                                                      \
          \                        \u2502<br>\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in     \u2502<br>\u2502 _call_impl                                   \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502   1498 \u2502   \u2502   if\
          \ not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks\
          \   \u2502<br>\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502<br>\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502<br>\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502<br>\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502<br>\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502<br>\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear_old.py:24\
          \ \u2502<br>\u2502 9 in forward                                        \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   246 \u2502   \u2502   \u2502   weight\
          \ = weight.reshape(weight.shape[0] * weight.shape[1], weight.shape[2]) \
          \   \u2502<br>\u2502   247 \u2502   \u2502   \u2502                    \
          \                                                              \u2502<br>\u2502\
          \   248 \u2502   \u2502   \u2502   out = torch.matmul(x.half(), weight)\
          \                                           \u2502<br>\u2502 \u2771 249\
          \ \u2502   \u2502   out = out.half().reshape(out_shape)                \
          \                                \u2502<br>\u2502   250 \u2502   \u2502\
          \   out = out + self.bias if self.bias is not None else out            \
          \                \u2502<br>\u2502   251 \u2502   \u2502   return out   \
          \                                                                      \u2502\
          <br>\u2502   252                                                       \
          \                                     \u2502<br>\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          <br>RuntimeError: shape '[1, 613, 8192]' is invalid for input of size 627712</p>\n\
          <p>1 * 613 * 8192 / 627712 = 8, looks like the size of K projection vector\
          \ is not right.</p>\n<p>My code is exactly copied from  model card.</p>\n"
        raw: "\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/transformers/models/llama/modeling_lla\
          \ \u2502\r\n\u2502 ma.py:197 in forward                                \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   194 \u2502   \u2502   bsz, q_len,\
          \ _ = hidden_states.size()                                             \
          \  \u2502\r\n\u2502   195 \u2502   \u2502                              \
          \                                                        \u2502\r\n\u2502\
          \   196 \u2502   \u2502   query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.   \u2502\r\n\u2502 \u2771 197 \u2502   \u2502\
          \   key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads,\
          \ self.he   \u2502\r\n\u2502   198 \u2502   \u2502   value_states = self.v_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.   \u2502\r\n\u2502   199 \u2502   \u2502\
          \                                                                      \
          \                \u2502\r\n\u2502   200 \u2502   \u2502   kv_seq_len = key_states.shape[-2]\
          \                                                  \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in     \u2502\r\n\u2502 _call_impl                                   \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502   1498 \u2502   \u2502   if\
          \ not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks\
          \   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502\r\n\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502\r\n\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear_old.py:24\
          \ \u2502\r\n\u2502 9 in forward                                        \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   246 \u2502   \u2502   \u2502   weight\
          \ = weight.reshape(weight.shape[0] * weight.shape[1], weight.shape[2]) \
          \   \u2502\r\n\u2502   247 \u2502   \u2502   \u2502                    \
          \                                                              \u2502\r\n\
          \u2502   248 \u2502   \u2502   \u2502   out = torch.matmul(x.half(), weight)\
          \                                           \u2502\r\n\u2502 \u2771 249\
          \ \u2502   \u2502   out = out.half().reshape(out_shape)                \
          \                                \u2502\r\n\u2502   250 \u2502   \u2502\
          \   out = out + self.bias if self.bias is not None else out            \
          \                \u2502\r\n\u2502   251 \u2502   \u2502   return out   \
          \                                                                      \u2502\
          \r\n\u2502   252                                                       \
          \                                     \u2502\r\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \r\nRuntimeError: shape '[1, 613, 8192]' is invalid for input of size 627712\r\
          \n\r\n1 * 613 * 8192 / 627712 = 8, looks like the size of K projection vector\
          \ is not right.\r\n\r\nMy code is exactly copied from  model card."
        updatedAt: '2023-07-25T10:00:21.479Z'
      numEdits: 0
      reactions: []
    id: 64bf9d35b7375f6b84acfeba
    type: comment
  author: ijustloveses
  content: "\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/transformers/models/llama/modeling_lla\
    \ \u2502\r\n\u2502 ma.py:197 in forward                                      \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   194 \u2502   \u2502   bsz, q_len, _ = hidden_states.size()\
    \                                               \u2502\r\n\u2502   195 \u2502\
    \   \u2502                                                                   \
    \                   \u2502\r\n\u2502   196 \u2502   \u2502   query_states = self.q_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.   \u2502\r\n\u2502 \u2771 197 \u2502   \u2502 \
    \  key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.he\
    \   \u2502\r\n\u2502   198 \u2502   \u2502   value_states = self.v_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.   \u2502\r\n\u2502   199 \u2502   \u2502      \
    \                                                                            \
    \    \u2502\r\n\u2502   200 \u2502   \u2502   kv_seq_len = key_states.shape[-2]\
    \                                                  \u2502\r\n\u2502          \
    \                                                                            \
    \            \u2502\r\n\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
    \ in     \u2502\r\n\u2502 _call_impl                                         \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
    \ or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499\
    \ \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks\
    \                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
    \   or _global_forward_hooks or _global_forward_pre_hooks):                  \
    \ \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
    \ **kwargs)                                          \u2502\r\n\u2502   1502 \u2502\
    \   \u2502   # Do not call functions when jit is used                        \
    \                  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks,\
    \ non_full_backward_hooks = [], []                             \u2502\r\n\u2502\
    \   1504 \u2502   \u2502   backward_pre_hooks = []                           \
    \                                \u2502\r\n\u2502                            \
    \                                                                      \u2502\r\
    \n\u2502 /opt/miniconda3/envs/ptcu117/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear_old.py:24\
    \ \u2502\r\n\u2502 9 in forward                                              \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   246 \u2502   \u2502   \u2502   weight = weight.reshape(weight.shape[0]\
    \ * weight.shape[1], weight.shape[2])    \u2502\r\n\u2502   247 \u2502   \u2502\
    \   \u2502                                                                   \
    \               \u2502\r\n\u2502   248 \u2502   \u2502   \u2502   out = torch.matmul(x.half(),\
    \ weight)                                           \u2502\r\n\u2502 \u2771 249\
    \ \u2502   \u2502   out = out.half().reshape(out_shape)                      \
    \                          \u2502\r\n\u2502   250 \u2502   \u2502   out = out\
    \ + self.bias if self.bias is not None else out                            \u2502\
    \r\n\u2502   251 \u2502   \u2502   return out                                \
    \                                         \u2502\r\n\u2502   252             \
    \                                                                            \
    \   \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nRuntimeError:\
    \ shape '[1, 613, 8192]' is invalid for input of size 627712\r\n\r\n1 * 613 *\
    \ 8192 / 627712 = 8, looks like the size of K projection vector is not right.\r\
    \n\r\nMy code is exactly copied from  model card."
  created_at: 2023-07-25 09:00:21+00:00
  edited: false
  hidden: false
  id: 64bf9d35b7375f6b84acfeba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T10:55:03.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8116583228111267
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please update Transformers to 4.31.0. I just realised this is not
          mentioned in the model card. I will fix that.  Llama 70B requires the latest
          Transformers version</p>

          '
        raw: Please update Transformers to 4.31.0. I just realised this is not mentioned
          in the model card. I will fix that.  Llama 70B requires the latest Transformers
          version
        updatedAt: '2023-07-25T10:55:32.937Z'
      numEdits: 1
      reactions: []
    id: 64bfaa071363b5c799f33191
    type: comment
  author: TheBloke
  content: Please update Transformers to 4.31.0. I just realised this is not mentioned
    in the model card. I will fix that.  Llama 70B requires the latest Transformers
    version
  created_at: 2023-07-25 09:55:03+00:00
  edited: true
  hidden: false
  id: 64bfaa071363b5c799f33191
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e147a5a1026e01f9929a07bebeedb0a1.svg
      fullname: Ai Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ijustloveses
      type: user
    createdAt: '2023-07-26T01:31:30.000Z'
    data:
      edited: false
      editors:
      - ijustloveses
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9174721240997314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e147a5a1026e01f9929a07bebeedb0a1.svg
          fullname: Ai Li
          isHf: false
          isPro: false
          name: ijustloveses
          type: user
        html: '<blockquote>

          <p>Please update Transformers to 4.31.0. I just realised this is not mentioned
          in the model card. I will fix that.  Llama 70B requires the latest Transformers
          version</p>

          </blockquote>

          <p>OK, thanks!</p>

          '
        raw: '> Please update Transformers to 4.31.0. I just realised this is not
          mentioned in the model card. I will fix that.  Llama 70B requires the latest
          Transformers version


          OK, thanks!'
        updatedAt: '2023-07-26T01:31:30.087Z'
      numEdits: 0
      reactions: []
    id: 64c07772f070a750cdb362fe
    type: comment
  author: ijustloveses
  content: '> Please update Transformers to 4.31.0. I just realised this is not mentioned
    in the model card. I will fix that.  Llama 70B requires the latest Transformers
    version


    OK, thanks!'
  created_at: 2023-07-26 00:31:30+00:00
  edited: false
  hidden: false
  id: 64c07772f070a750cdb362fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e147a5a1026e01f9929a07bebeedb0a1.svg
      fullname: Ai Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ijustloveses
      type: user
    createdAt: '2023-07-26T01:37:27.000Z'
    data:
      status: closed
    id: 64c078d7e062a72f15ae196d
    type: status-change
  author: ijustloveses
  created_at: 2023-07-26 00:37:27+00:00
  id: 64c078d7e062a72f15ae196d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: closed
target_branch: null
title: shape invalid error during running
