!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hierholzer
conflicting_files: null
created_at: 2023-07-24 17:52:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/U2LXBhd0q4h5wVa0Pn4yr.png?w=200&h=200&f=face
      fullname: Dan Hierholzer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hierholzer
      type: user
    createdAt: '2023-07-24T18:52:54.000Z'
    data:
      edited: false
      editors:
      - hierholzer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985950350761414
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/U2LXBhd0q4h5wVa0Pn4yr.png?w=200&h=200&f=face
          fullname: Dan Hierholzer
          isHf: false
          isPro: false
          name: hierholzer
          type: user
        html: '<p>Hello,<br>When trying to load (main), I am getting an error code
          of 137; which typically means out of memory.<br>I have 48GB VRAM (2 * 3090TI
          w NVLink), as well as 256GB system Ram<br>Can I configure this in some sort
          of way to make it work with 48GB VRAM, Or can I use system memory in addition
          to my VRAM in GPTQ versions?</p>

          <p>Here is the Current configuration that I am using,<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png"><img
          alt="CurrentSetup.png" src="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png"></a></p>

          <p>Thanks for your help</p>

          '
        raw: "Hello,\r\nWhen trying to load (main), I am getting an error code of\
          \ 137; which typically means out of memory. \r\nI have 48GB VRAM (2 * 3090TI\
          \ w NVLink), as well as 256GB system Ram \r\nCan I configure this in some\
          \ sort of way to make it work with 48GB VRAM, Or can I use system memory\
          \ in addition to my VRAM in GPTQ versions?\r\n\r\nHere is the Current configuration\
          \ that I am using,\r\n![CurrentSetup.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png)\r\
          \n\r\n\r\nThanks for your help\r\n"
        updatedAt: '2023-07-24T18:52:54.889Z'
      numEdits: 0
      reactions: []
    id: 64bec88681caff7f184fe775
    type: comment
  author: hierholzer
  content: "Hello,\r\nWhen trying to load (main), I am getting an error code of 137;\
    \ which typically means out of memory. \r\nI have 48GB VRAM (2 * 3090TI w NVLink),\
    \ as well as 256GB system Ram \r\nCan I configure this in some sort of way to\
    \ make it work with 48GB VRAM, Or can I use system memory in addition to my VRAM\
    \ in GPTQ versions?\r\n\r\nHere is the Current configuration that I am using,\r\
    \n![CurrentSetup.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png)\r\
    \n\r\n\r\nThanks for your help\r\n"
  created_at: 2023-07-24 17:52:54+00:00
  edited: false
  hidden: false
  id: 64bec88681caff7f184fe775
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T19:19:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9002021551132202
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes you can split it over the two GPUs. But you need to choose the
          ExLlama loader, not Transformers.  Then in the "gpu-split" box enter "17.2,24"
          to put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs room for context also
          hence it needs to load less of the model).  </p>

          <p>Then save the settings and reload the model with them</p>

          <p>Expect performance to be in the region of 10 - 15 tokens/s </p>

          '
        raw: "Yes you can split it over the two GPUs. But you need to choose the ExLlama\
          \ loader, not Transformers.  Then in the \"gpu-split\" box enter \"17.2,24\"\
          \ to put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs room for context also\
          \ hence it needs to load less of the model).  \n\nThen save the settings\
          \ and reload the model with them\n\nExpect performance to be in the region\
          \ of 10 - 15 tokens/s "
        updatedAt: '2023-07-24T19:19:00.164Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - Thireus
    id: 64becea438953777feb239e8
    type: comment
  author: TheBloke
  content: "Yes you can split it over the two GPUs. But you need to choose the ExLlama\
    \ loader, not Transformers.  Then in the \"gpu-split\" box enter \"17.2,24\" to\
    \ put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs room for context also hence\
    \ it needs to load less of the model).  \n\nThen save the settings and reload\
    \ the model with them\n\nExpect performance to be in the region of 10 - 15 tokens/s "
  created_at: 2023-07-24 18:19:00+00:00
  edited: false
  hidden: false
  id: 64becea438953777feb239e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/U2LXBhd0q4h5wVa0Pn4yr.png?w=200&h=200&f=face
      fullname: Dan Hierholzer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hierholzer
      type: user
    createdAt: '2023-07-24T20:30:59.000Z'
    data:
      edited: false
      editors:
      - hierholzer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8299207091331482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/U2LXBhd0q4h5wVa0Pn4yr.png?w=200&h=200&f=face
          fullname: Dan Hierholzer
          isHf: false
          isPro: false
          name: hierholzer
          type: user
        html: '<p>Thanks for the help!<br>FYI - if anyone out there is having the
          same issues that I was, AND the page layout on the Model Tab looks the same
          as my screenshot from above.<br>Then you need to update your text-generation-webui
          git repo.<br>Afterward, your Model Tab screen will look like this:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png"><img
          alt="NewScreen.png" src="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png"></a></p>

          <p>Thanks for all of the awesome work you have contributed to the community
          TheBloke.</p>

          <p>Cheers!</p>

          '
        raw: "Thanks for the help! \nFYI - if anyone out there is having the same\
          \ issues that I was, AND the page layout on the Model Tab looks the same\
          \ as my screenshot from above.\nThen you need to update your text-generation-webui\
          \ git repo.\nAfterward, your Model Tab screen will look like this:\n![NewScreen.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png)\n\
          \n\nThanks for all of the awesome work you have contributed to the community\
          \ TheBloke.\n\nCheers!\n"
        updatedAt: '2023-07-24T20:30:59.096Z'
      numEdits: 0
      reactions: []
    id: 64bedf83ae436c8813449774
    type: comment
  author: hierholzer
  content: "Thanks for the help! \nFYI - if anyone out there is having the same issues\
    \ that I was, AND the page layout on the Model Tab looks the same as my screenshot\
    \ from above.\nThen you need to update your text-generation-webui git repo.\n\
    Afterward, your Model Tab screen will look like this:\n![NewScreen.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png)\n\
    \n\nThanks for all of the awesome work you have contributed to the community TheBloke.\n\
    \nCheers!\n"
  created_at: 2023-07-24 19:30:59+00:00
  edited: false
  hidden: false
  id: 64bedf83ae436c8813449774
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
      fullname: Miron Tewfik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: streetyogi
      type: user
    createdAt: '2023-07-26T13:45:19.000Z'
    data:
      edited: false
      editors:
      - streetyogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8649561405181885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
          fullname: Miron Tewfik
          isHf: false
          isPro: false
          name: streetyogi
          type: user
        html: '<blockquote>

          <p>Thanks for the help!<br>FYI - if anyone out there is having the same
          issues that I was, AND the page layout on the Model Tab looks the same as
          my screenshot from above.<br>Then you need to update your text-generation-webui
          git repo.<br>Afterward, your Model Tab screen will look like this:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png"><img
          alt="NewScreen.png" src="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png"></a></p>

          <p>Thanks for all of the awesome work you have contributed to the community
          TheBloke.</p>

          <p>Cheers!</p>

          </blockquote>

          <p>I would use max_seq_leng = 4096 and set compress_pos_emb maybe to 2,
          because that''s what the base model is fine tuned to.</p>

          '
        raw: "> Thanks for the help! \n> FYI - if anyone out there is having the same\
          \ issues that I was, AND the page layout on the Model Tab looks the same\
          \ as my screenshot from above.\n> Then you need to update your text-generation-webui\
          \ git repo.\n> Afterward, your Model Tab screen will look like this:\n>\
          \ ![NewScreen.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png)\n\
          > \n> \n> Thanks for all of the awesome work you have contributed to the\
          \ community TheBloke.\n> \n> Cheers!\n\nI would use max_seq_leng = 4096\
          \ and set compress_pos_emb maybe to 2, because that's what the base model\
          \ is fine tuned to."
        updatedAt: '2023-07-26T13:45:19.979Z'
      numEdits: 0
      reactions: []
    id: 64c1236fa04a514ba6398060
    type: comment
  author: streetyogi
  content: "> Thanks for the help! \n> FYI - if anyone out there is having the same\
    \ issues that I was, AND the page layout on the Model Tab looks the same as my\
    \ screenshot from above.\n> Then you need to update your text-generation-webui\
    \ git repo.\n> Afterward, your Model Tab screen will look like this:\n> ![NewScreen.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/AJm1Kk03B9kNukI3IMg8k.png)\n\
    > \n> \n> Thanks for all of the awesome work you have contributed to the community\
    \ TheBloke.\n> \n> Cheers!\n\nI would use max_seq_leng = 4096 and set compress_pos_emb\
    \ maybe to 2, because that's what the base model is fine tuned to."
  created_at: 2023-07-26 12:45:19+00:00
  edited: false
  hidden: false
  id: 64c1236fa04a514ba6398060
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-07-28T13:44:11.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9143642783164978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Curious if anyone uses Exllama in a notebook, seems mostly use cases
          in textgeneration ui.</p>

          '
        raw: Curious if anyone uses Exllama in a notebook, seems mostly use cases
          in textgeneration ui.
        updatedAt: '2023-07-28T13:44:11.383Z'
      numEdits: 0
      reactions: []
    id: 64c3c62bed521f27a4e26142
    type: comment
  author: Satya93
  content: Curious if anyone uses Exllama in a notebook, seems mostly use cases in
    textgeneration ui.
  created_at: 2023-07-28 12:44:11+00:00
  edited: false
  hidden: false
  id: 64c3c62bed521f27a4e26142
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-08-10T02:37:45.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.91294926404953
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<blockquote>\n<p>Yes you can split it over the two GPUs. But you need\
          \ to choose the ExLlama loader, not Transformers.  Then in the \"gpu-split\"\
          \ box enter \"17.2,24\" to put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs\
          \ room for context also hence it needs to load less of the model).  </p>\n\
          <p>Then save the settings and reload the model with them</p>\n<p>Expect\
          \ performance to be in the region of 10 - 15 tokens/s</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  Is there any\
          \ general guidance on how to figure these split number out? Or is it based\
          \ on field experience?</p>\n"
        raw: "> Yes you can split it over the two GPUs. But you need to choose the\
          \ ExLlama loader, not Transformers.  Then in the \"gpu-split\" box enter\
          \ \"17.2,24\" to put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs room for\
          \ context also hence it needs to load less of the model).  \n> \n> Then\
          \ save the settings and reload the model with them\n> \n> Expect performance\
          \ to be in the region of 10 - 15 tokens/s\n\n@TheBloke  Is there any general\
          \ guidance on how to figure these split number out? Or is it based on field\
          \ experience?"
        updatedAt: '2023-08-10T02:37:45.606Z'
      numEdits: 0
      reactions: []
    id: 64d44d79c8e6602413a6fd4a
    type: comment
  author: Yhyu13
  content: "> Yes you can split it over the two GPUs. But you need to choose the ExLlama\
    \ loader, not Transformers.  Then in the \"gpu-split\" box enter \"17.2,24\" to\
    \ put 17.2GB on GPU1 and 24GB on GPU2 (GPU1 needs room for context also hence\
    \ it needs to load less of the model).  \n> \n> Then save the settings and reload\
    \ the model with them\n> \n> Expect performance to be in the region of 10 - 15\
    \ tokens/s\n\n@TheBloke  Is there any general guidance on how to figure these\
    \ split number out? Or is it based on field experience?"
  created_at: 2023-08-10 01:37:45+00:00
  edited: false
  hidden: false
  id: 64d44d79c8e6602413a6fd4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:19:07.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.935494065284729
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That specific number for 65B and 70B on ExLlama is taken from the
          README of the ExLlama Github, so it''s based on the testing of the ExLlama
          developer himself: <a rel="nofollow" href="https://github.com/turboderp/exllama#dual-gpu-results">https://github.com/turboderp/exllama#dual-gpu-results</a></p>

          <p>Generally speaking, the rule is:</p>

          <ul>

          <li>The first GPU needs to leave some room for context - 6-7 GB is a common
          figure</li>

          <li>The second (and more, if present) GPU(s) can use their full capacity,
          ie 24GB</li>

          </ul>

          <p>This general rule applies for AutoGPTQ as well.  ExLlama needs less VRAM
          for context than AutoGPTQ, but it still needs to leave room for that.</p>

          <p>One big difference between ExLlama and AutoGPTQ (and all other Transformers-based
          methods) is that ExLlama pre-allocates VRAM for context, up to whatever
          the configured maximum sequence length is.  So the model either loads fine
          and will then work at the full context length, or it fails to load with
          an "out of VRAM" CUDA error, immediately.  </p>

          <p>With AutoGPTQ and other Transformers-based code, it only loads the model
          weights at model load, and then uses VRAM for context as it''s needed.  So
          with AutoGPTQ, you might get the model loaded and get some responses, and
          then find it runs out of VRAM when context goes over a certain threshold.  </p>

          '
        raw: "That specific number for 65B and 70B on ExLlama is taken from the README\
          \ of the ExLlama Github, so it's based on the testing of the ExLlama developer\
          \ himself: https://github.com/turboderp/exllama#dual-gpu-results\n\nGenerally\
          \ speaking, the rule is:\n- The first GPU needs to leave some room for context\
          \ - 6-7 GB is a common figure\n- The second (and more, if present) GPU(s)\
          \ can use their full capacity, ie 24GB\n\nThis general rule applies for\
          \ AutoGPTQ as well.  ExLlama needs less VRAM for context than AutoGPTQ,\
          \ but it still needs to leave room for that.\n\nOne big difference between\
          \ ExLlama and AutoGPTQ (and all other Transformers-based methods) is that\
          \ ExLlama pre-allocates VRAM for context, up to whatever the configured\
          \ maximum sequence length is.  So the model either loads fine and will then\
          \ work at the full context length, or it fails to load with an \"out of\
          \ VRAM\" CUDA error, immediately.  \n\nWith AutoGPTQ and other Transformers-based\
          \ code, it only loads the model weights at model load, and then uses VRAM\
          \ for context as it's needed.  So with AutoGPTQ, you might get the model\
          \ loaded and get some responses, and then find it runs out of VRAM when\
          \ context goes over a certain threshold.  "
        updatedAt: '2023-08-10T09:19:28.344Z'
      numEdits: 1
      reactions: []
    id: 64d4ab8ba9ba52fb71fee7a1
    type: comment
  author: TheBloke
  content: "That specific number for 65B and 70B on ExLlama is taken from the README\
    \ of the ExLlama Github, so it's based on the testing of the ExLlama developer\
    \ himself: https://github.com/turboderp/exllama#dual-gpu-results\n\nGenerally\
    \ speaking, the rule is:\n- The first GPU needs to leave some room for context\
    \ - 6-7 GB is a common figure\n- The second (and more, if present) GPU(s) can\
    \ use their full capacity, ie 24GB\n\nThis general rule applies for AutoGPTQ as\
    \ well.  ExLlama needs less VRAM for context than AutoGPTQ, but it still needs\
    \ to leave room for that.\n\nOne big difference between ExLlama and AutoGPTQ (and\
    \ all other Transformers-based methods) is that ExLlama pre-allocates VRAM for\
    \ context, up to whatever the configured maximum sequence length is.  So the model\
    \ either loads fine and will then work at the full context length, or it fails\
    \ to load with an \"out of VRAM\" CUDA error, immediately.  \n\nWith AutoGPTQ\
    \ and other Transformers-based code, it only loads the model weights at model\
    \ load, and then uses VRAM for context as it's needed.  So with AutoGPTQ, you\
    \ might get the model loaded and get some responses, and then find it runs out\
    \ of VRAM when context goes over a certain threshold.  "
  created_at: 2023-08-10 08:19:07+00:00
  edited: true
  hidden: false
  id: 64d4ab8ba9ba52fb71fee7a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8b1dbf42c61d2ff41f5ab3272917bf4.svg
      fullname: Carlos G Mendioroz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tronar
      type: user
    createdAt: '2023-09-18T22:14:06.000Z'
    data:
      edited: false
      editors:
      - tronar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9065584540367126
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8b1dbf42c61d2ff41f5ab3272917bf4.svg
          fullname: Carlos G Mendioroz
          isHf: false
          isPro: false
          name: tronar
          type: user
        html: '<blockquote>

          <p>Hello,<br>When trying to load (main), I am getting an error code of 137;
          which typically means out of memory.<br>I have 48GB VRAM (2 * 3090TI w NVLink),
          as well as 256GB system Ram<br>Can I configure this in some sort of way
          to make it work with 48GB VRAM, Or can I use system memory in addition to
          my VRAM in GPTQ versions?</p>

          <p>Here is the Current configuration that I am using,<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png"><img
          alt="CurrentSetup.png" src="https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png"></a></p>

          <p>Thanks for your help</p>

          </blockquote>

          <p>Hi, I''m considering buying an nvlink to link a couple of A5000''s. Would
          you share your experience with nvlink ? Does it make a difference ? Does
          exllama support loading using it (assuming you don''t have the same PCI
          BW on both 3090''s , that is)<br>Thanks!</p>

          '
        raw: "> Hello,\n> When trying to load (main), I am getting an error code of\
          \ 137; which typically means out of memory. \n> I have 48GB VRAM (2 * 3090TI\
          \ w NVLink), as well as 256GB system Ram \n> Can I configure this in some\
          \ sort of way to make it work with 48GB VRAM, Or can I use system memory\
          \ in addition to my VRAM in GPTQ versions?\n> \n> Here is the Current configuration\
          \ that I am using,\n> ![CurrentSetup.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png)\n\
          > \n> \n> Thanks for your help\n\nHi, I'm considering buying an nvlink to\
          \ link a couple of A5000's. Would you share your experience with nvlink\
          \ ? Does it make a difference ? Does exllama support loading using it (assuming\
          \ you don't have the same PCI BW on both 3090's , that is)\nThanks!"
        updatedAt: '2023-09-18T22:14:06.143Z'
      numEdits: 0
      reactions: []
    id: 6508cbae3f8a38f064663061
    type: comment
  author: tronar
  content: "> Hello,\n> When trying to load (main), I am getting an error code of\
    \ 137; which typically means out of memory. \n> I have 48GB VRAM (2 * 3090TI w\
    \ NVLink), as well as 256GB system Ram \n> Can I configure this in some sort of\
    \ way to make it work with 48GB VRAM, Or can I use system memory in addition to\
    \ my VRAM in GPTQ versions?\n> \n> Here is the Current configuration that I am\
    \ using,\n> ![CurrentSetup.png](https://cdn-uploads.huggingface.co/production/uploads/644ef16bf2ada99b2eb95116/XyQB0oweplUApJF9_KrTD.png)\n\
    > \n> \n> Thanks for your help\n\nHi, I'm considering buying an nvlink to link\
    \ a couple of A5000's. Would you share your experience with nvlink ? Does it make\
    \ a difference ? Does exllama support loading using it (assuming you don't have\
    \ the same PCI BW on both 3090's , that is)\nThanks!"
  created_at: 2023-09-18 21:14:06+00:00
  edited: false
  hidden: false
  id: 6508cbae3f8a38f064663061
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Mininum VRAM?
