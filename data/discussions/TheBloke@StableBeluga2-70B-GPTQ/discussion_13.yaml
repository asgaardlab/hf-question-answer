!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ibivibiv
conflicting_files: null
created_at: 2023-07-29 16:42:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60a4384dce31f0d28cda68c7/O3clz57FO_a3hTY_rei55.png?w=200&h=200&f=face
      fullname: Some Guy Not Wanting Any Undo Attention
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ibivibiv
      type: user
    createdAt: '2023-07-29T17:42:23.000Z'
    data:
      edited: false
      editors:
      - ibivibiv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9818938374519348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60a4384dce31f0d28cda68c7/O3clz57FO_a3hTY_rei55.png?w=200&h=200&f=face
          fullname: Some Guy Not Wanting Any Undo Attention
          isHf: false
          isPro: true
          name: ibivibiv
          type: user
        html: '<p>Just genuine curiosity, it seems like things are either full on
          32bit, fp16 or 4bit/3bit quantized for the most part.  Is there something
          special about 8bit quantized that makes it undesirable?  For instance I
          can easily fit the large models at 4bit but an fp16 of them stretches beyond
          my vram.  8bit would fit and use up in most cases a larger amount of my
          VRAM.  Is it a performance thing that there isn''t much difference between
          8 and 4 or is it more that there aren''t many people that could do 8 and
          not do 16 so there just isn''t a demand?  It just seemed really odd to me
          that 8bit just isn''t very prevalent at all in the community.</p>

          '
        raw: Just genuine curiosity, it seems like things are either full on 32bit,
          fp16 or 4bit/3bit quantized for the most part.  Is there something special
          about 8bit quantized that makes it undesirable?  For instance I can easily
          fit the large models at 4bit but an fp16 of them stretches beyond my vram.  8bit
          would fit and use up in most cases a larger amount of my VRAM.  Is it a
          performance thing that there isn't much difference between 8 and 4 or is
          it more that there aren't many people that could do 8 and not do 16 so there
          just isn't a demand?  It just seemed really odd to me that 8bit just isn't
          very prevalent at all in the community.
        updatedAt: '2023-07-29T17:42:23.078Z'
      numEdits: 0
      reactions: []
    id: 64c54f7f1d44fc06afdbbba0
    type: comment
  author: ibivibiv
  content: Just genuine curiosity, it seems like things are either full on 32bit,
    fp16 or 4bit/3bit quantized for the most part.  Is there something special about
    8bit quantized that makes it undesirable?  For instance I can easily fit the large
    models at 4bit but an fp16 of them stretches beyond my vram.  8bit would fit and
    use up in most cases a larger amount of my VRAM.  Is it a performance thing that
    there isn't much difference between 8 and 4 or is it more that there aren't many
    people that could do 8 and not do 16 so there just isn't a demand?  It just seemed
    really odd to me that 8bit just isn't very prevalent at all in the community.
  created_at: 2023-07-29 16:42:23+00:00
  edited: false
  hidden: false
  id: 64c54f7f1d44fc06afdbbba0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
      fullname: Ronen Zyroff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ronenzyroff
      type: user
    createdAt: '2023-07-29T20:27:14.000Z'
    data:
      edited: false
      editors:
      - ronenzyroff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9764631390571594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43fd0050e827ad080aabbd3fec287ed8.svg
          fullname: Ronen Zyroff
          isHf: false
          isPro: false
          name: ronenzyroff
          type: user
        html: '<blockquote>

          <p>Just genuine curiosity, it seems like things are either full on 32bit,
          fp16 or 4bit/3bit quantized for the most part.  Is there something special
          about 8bit quantized that makes it undesirable?  For instance I can easily
          fit the large models at 4bit but an fp16 of them stretches beyond my vram.  8bit
          would fit and use up in most cases a larger amount of my VRAM.  Is it a
          performance thing that there isn''t much difference between 8 and 4 or is
          it more that there aren''t many people that could do 8 and not do 16 so
          there just isn''t a demand?  It just seemed really odd to me that 8bit just
          isn''t very prevalent at all in the community.</p>

          </blockquote>

          <p>Very simple. 8-bit is slower than 4-bit because of memory bandwidth.
          There''s just more gigabytes to copy around.</p>

          <p>On CPU however (ggml llama.cpp), TheBloke does often provide an 8-bit
          option.</p>

          <p>I agree that 4-bit quantization is no good. 5-bit should be a minimum.</p>

          '
        raw: '> Just genuine curiosity, it seems like things are either full on 32bit,
          fp16 or 4bit/3bit quantized for the most part.  Is there something special
          about 8bit quantized that makes it undesirable?  For instance I can easily
          fit the large models at 4bit but an fp16 of them stretches beyond my vram.  8bit
          would fit and use up in most cases a larger amount of my VRAM.  Is it a
          performance thing that there isn''t much difference between 8 and 4 or is
          it more that there aren''t many people that could do 8 and not do 16 so
          there just isn''t a demand?  It just seemed really odd to me that 8bit just
          isn''t very prevalent at all in the community.


          Very simple. 8-bit is slower than 4-bit because of memory bandwidth. There''s
          just more gigabytes to copy around.


          On CPU however (ggml llama.cpp), TheBloke does often provide an 8-bit option.


          I agree that 4-bit quantization is no good. 5-bit should be a minimum.'
        updatedAt: '2023-07-29T20:27:14.774Z'
      numEdits: 0
      reactions: []
    id: 64c576224399efa2fd9bbe4e
    type: comment
  author: ronenzyroff
  content: '> Just genuine curiosity, it seems like things are either full on 32bit,
    fp16 or 4bit/3bit quantized for the most part.  Is there something special about
    8bit quantized that makes it undesirable?  For instance I can easily fit the large
    models at 4bit but an fp16 of them stretches beyond my vram.  8bit would fit and
    use up in most cases a larger amount of my VRAM.  Is it a performance thing that
    there isn''t much difference between 8 and 4 or is it more that there aren''t
    many people that could do 8 and not do 16 so there just isn''t a demand?  It just
    seemed really odd to me that 8bit just isn''t very prevalent at all in the community.


    Very simple. 8-bit is slower than 4-bit because of memory bandwidth. There''s
    just more gigabytes to copy around.


    On CPU however (ggml llama.cpp), TheBloke does often provide an 8-bit option.


    I agree that 4-bit quantization is no good. 5-bit should be a minimum.'
  created_at: 2023-07-29 19:27:14+00:00
  edited: false
  hidden: false
  id: 64c576224399efa2fd9bbe4e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Why so few 8 bit capable models?
