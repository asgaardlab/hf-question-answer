!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-07-24 16:10:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-07-24T17:10:44.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9702460765838623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p>Amazing work as always <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>!\
          \ Just wondering if a GGML version is planned.</p>\n<p>I wish I had the\
          \ capacity to play with the GPTQ version, sadly I don't have enough VRAM\
          \ available. :(</p>\n"
        raw: "Amazing work as always @TheBloke! Just wondering if a GGML version is\
          \ planned.\r\n\r\nI wish I had the capacity to play with the GPTQ version,\
          \ sadly I don't have enough VRAM available. :("
        updatedAt: '2023-07-24T17:10:44.067Z'
      numEdits: 0
      reactions: []
    id: 64beb0946999b520ed8aa00c
    type: comment
  author: Thireus
  content: "Amazing work as always @TheBloke! Just wondering if a GGML version is\
    \ planned.\r\n\r\nI wish I had the capacity to play with the GPTQ version, sadly\
    \ I don't have enough VRAM available. :("
  created_at: 2023-07-24 16:10:44+00:00
  edited: false
  hidden: false
  id: 64beb0946999b520ed8aa00c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T17:18:23.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445339441299438
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>As soon as I can, yes. It''s not possible yet because the Llama.cpp
          convert.py currently only works to convert 70b from the meta PTH format,
          not from HF format. I''m on my phone atm so can''t link it, but check issues
          in Llama.cpp Github and you''ll see an issue I raised in the last hour explaining
          the problem.</p>

          <p>As soon as someone can resolve that I''ll make ggmls for all the 70b
          fine tunes available </p>

          '
        raw: 'As soon as I can, yes. It''s not possible yet because the Llama.cpp
          convert.py currently only works to convert 70b from the meta PTH format,
          not from HF format. I''m on my phone atm so can''t link it, but check issues
          in Llama.cpp Github and you''ll see an issue I raised in the last hour explaining
          the problem.


          As soon as someone can resolve that I''ll make ggmls for all the 70b fine
          tunes available '
        updatedAt: '2023-07-24T17:18:23.914Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - GameTaco
        - SupPud
        - upbit
        - StephenSKelley
        - Dailyfocus
        - Thireus
    id: 64beb25f979949d2e23e9184
    type: comment
  author: TheBloke
  content: 'As soon as I can, yes. It''s not possible yet because the Llama.cpp convert.py
    currently only works to convert 70b from the meta PTH format, not from HF format.
    I''m on my phone atm so can''t link it, but check issues in Llama.cpp Github and
    you''ll see an issue I raised in the last hour explaining the problem.


    As soon as someone can resolve that I''ll make ggmls for all the 70b fine tunes
    available '
  created_at: 2023-07-24 16:18:23+00:00
  edited: false
  hidden: false
  id: 64beb25f979949d2e23e9184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
      fullname: Ai Creator
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AiCreatornator
      type: user
    createdAt: '2023-07-24T20:29:23.000Z'
    data:
      edited: true
      editors:
      - AiCreatornator
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8514073491096497
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
          fullname: Ai Creator
          isHf: false
          isPro: false
          name: AiCreatornator
          type: user
        html: '<p>I have probably a stupid question. Could HF-format be converted
          to PTH-format? And then do the GGML-version?</p>

          <p>PTH-format can be converted to HF-format with next script: <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py</a></p>

          <p>I''m a really bad coder, but could that script be modified to do the
          conversion backwards? Or are there some information lost during the conversion
          from PTH to HF?</p>

          <p>EDIT: nevermind,  they are already talking in there GIthub about this
          <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/2376">https://github.com/ggerganov/llama.cpp/issues/2376</a></p>

          '
        raw: 'I have probably a stupid question. Could HF-format be converted to PTH-format?
          And then do the GGML-version?


          PTH-format can be converted to HF-format with next script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py


          I''m a really bad coder, but could that script be modified to do the conversion
          backwards? Or are there some information lost during the conversion from
          PTH to HF?


          EDIT: nevermind,  they are already talking in there GIthub about this https://github.com/ggerganov/llama.cpp/issues/2376'
        updatedAt: '2023-07-24T21:14:43.860Z'
      numEdits: 2
      reactions: []
    id: 64bedf23140491ca9f85d90f
    type: comment
  author: AiCreatornator
  content: 'I have probably a stupid question. Could HF-format be converted to PTH-format?
    And then do the GGML-version?


    PTH-format can be converted to HF-format with next script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py


    I''m a really bad coder, but could that script be modified to do the conversion
    backwards? Or are there some information lost during the conversion from PTH to
    HF?


    EDIT: nevermind,  they are already talking in there GIthub about this https://github.com/ggerganov/llama.cpp/issues/2376'
  created_at: 2023-07-24 19:29:23+00:00
  edited: true
  hidden: false
  id: 64bedf23140491ca9f85d90f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: GGML version possible/coming?
