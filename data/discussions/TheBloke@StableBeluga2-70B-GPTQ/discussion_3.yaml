!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krassmann
conflicting_files: null
created_at: 2023-07-23 10:04:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T11:04:52.000Z'
    data:
      edited: false
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8700558543205261
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: "<p>What am I doing wrong? <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ I used your runpod template countless times and I always managed to run\
          \ prompts but this time I fail. I'm using an A6000 instance on runpod with\
          \ the thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest image. I follow the\
          \ instructions on the model card closely and use the prompt template. At\
          \ first glance everything is fine, the model is loaded and 70% of the 48\
          \ GB VRam are used. When I hit generate there is no reaction besides my\
          \ prompt is copied over to the output. There is also no activity visible\
          \ on the CPU and GPU utilization gauges.</p>\n"
        raw: What am I doing wrong? @TheBloke, I used your runpod template countless
          times and I always managed to run prompts but this time I fail. I'm using
          an A6000 instance on runpod with the thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest
          image. I follow the instructions on the model card closely and use the prompt
          template. At first glance everything is fine, the model is loaded and 70%
          of the 48 GB VRam are used. When I hit generate there is no reaction besides
          my prompt is copied over to the output. There is also no activity visible
          on the CPU and GPU utilization gauges.
        updatedAt: '2023-07-23T11:04:52.199Z'
      numEdits: 0
      reactions: []
    id: 64bd095481caff7f181d14f8
    type: comment
  author: krassmann
  content: What am I doing wrong? @TheBloke, I used your runpod template countless
    times and I always managed to run prompts but this time I fail. I'm using an A6000
    instance on runpod with the thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest image.
    I follow the instructions on the model card closely and use the prompt template.
    At first glance everything is fine, the model is loaded and 70% of the 48 GB VRam
    are used. When I hit generate there is no reaction besides my prompt is copied
    over to the output. There is also no activity visible on the CPU and GPU utilization
    gauges.
  created_at: 2023-07-23 10:04:52+00:00
  edited: false
  hidden: false
  id: 64bd095481caff7f181d14f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-23T11:08:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7931854128837585
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s my fault, I''ve not updated that template yet for the latest
          ExLlama changes required for Llama 2 70B</p>

          <p>Well, I did update it, but I never tested it and pushed it to :latest
          tag.</p>

          <p>Could you test it for me?  Edit your template or apply a template override,
          change the docker container to <code>thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023</code></p>

          <p>Then test again and let me know.  If it works, I will push that to <code>:latest</code>
          and then it will be the default with my template for all users.</p>

          '
        raw: 'It''s my fault, I''ve not updated that template yet for the latest ExLlama
          changes required for Llama 2 70B


          Well, I did update it, but I never tested it and pushed it to :latest tag.


          Could you test it for me?  Edit your template or apply a template override,
          change the docker container to `thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023`


          Then test again and let me know.  If it works, I will push that to `:latest`
          and then it will be the default with my template for all users.'
        updatedAt: '2023-07-23T11:08:33.407Z'
      numEdits: 0
      reactions: []
    id: 64bd0a3112afb2f11933392f
    type: comment
  author: TheBloke
  content: 'It''s my fault, I''ve not updated that template yet for the latest ExLlama
    changes required for Llama 2 70B


    Well, I did update it, but I never tested it and pushed it to :latest tag.


    Could you test it for me?  Edit your template or apply a template override, change
    the docker container to `thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023`


    Then test again and let me know.  If it works, I will push that to `:latest` and
    then it will be the default with my template for all users.'
  created_at: 2023-07-23 10:08:33+00:00
  edited: false
  hidden: false
  id: 64bd0a3112afb2f11933392f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T12:15:13.000Z'
    data:
      edited: true
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9767417907714844
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: '<p>Unfortunately, the behavior is still the same. Can it be correct
          that after model selection it is auto configured to AutoGPTQ with wbits
          set to none? I tried setting it to 4 and reloaded the model, but it didn''t
          change anything. I also tried ExLlama but it didn''t work.</p>

          '
        raw: Unfortunately, the behavior is still the same. Can it be correct that
          after model selection it is auto configured to AutoGPTQ with wbits set to
          none? I tried setting it to 4 and reloaded the model, but it didn't change
          anything. I also tried ExLlama but it didn't work.
        updatedAt: '2023-07-23T12:19:23.500Z'
      numEdits: 4
      reactions: []
    id: 64bd19d1140491ca9f4e14f1
    type: comment
  author: krassmann
  content: Unfortunately, the behavior is still the same. Can it be correct that after
    model selection it is auto configured to AutoGPTQ with wbits set to none? I tried
    setting it to 4 and reloaded the model, but it didn't change anything. I also
    tried ExLlama but it didn't work.
  created_at: 2023-07-23 11:15:13+00:00
  edited: true
  hidden: false
  id: 64bd19d1140491ca9f4e14f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-23T12:18:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9486360549926758
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You want to use ExLlama. It''ll be much faster.  I didn''t realise
          you were using AutoGPTQ as most people use ExLlama these days.</p>

          <p>AutoGPTQ can be used, but you have to tick "no inject fused attention"
          in the Loader settings. And yes it''s correct that wbits is set to None
          for AutoGPTQ, leave that at None (it''s automatically read from quantize_config.json)</p>

          <p>So: </p>

          <ol>

          <li>please try Loader = ExLlama with the updated container and let me know
          if that works</li>

          <li>If you have time, I''d be grateful if you also tested Loader = AutoGPTQ
          with "no_inject_fused_attention" ticked   (again with the updated container)</li>

          </ol>

          <p>(make sure to click Reload Model after changing the Loader and any loader
          settings)</p>

          '
        raw: "You want to use ExLlama. It'll be much faster.  I didn't realise you\
          \ were using AutoGPTQ as most people use ExLlama these days.\n\nAutoGPTQ\
          \ can be used, but you have to tick \"no inject fused attention\" in the\
          \ Loader settings. And yes it's correct that wbits is set to None for AutoGPTQ,\
          \ leave that at None (it's automatically read from quantize_config.json)\n\
          \nSo: \n1. please try Loader = ExLlama with the updated container and let\
          \ me know if that works\n2. If you have time, I'd be grateful if you also\
          \ tested Loader = AutoGPTQ with \"no_inject_fused_attention\" ticked   (again\
          \ with the updated container)\n\n(make sure to click Reload Model after\
          \ changing the Loader and any loader settings)"
        updatedAt: '2023-07-23T12:18:24.317Z'
      numEdits: 0
      reactions: []
    id: 64bd1a90da140e46194c030c
    type: comment
  author: TheBloke
  content: "You want to use ExLlama. It'll be much faster.  I didn't realise you were\
    \ using AutoGPTQ as most people use ExLlama these days.\n\nAutoGPTQ can be used,\
    \ but you have to tick \"no inject fused attention\" in the Loader settings. And\
    \ yes it's correct that wbits is set to None for AutoGPTQ, leave that at None\
    \ (it's automatically read from quantize_config.json)\n\nSo: \n1. please try Loader\
    \ = ExLlama with the updated container and let me know if that works\n2. If you\
    \ have time, I'd be grateful if you also tested Loader = AutoGPTQ with \"no_inject_fused_attention\"\
    \ ticked   (again with the updated container)\n\n(make sure to click Reload Model\
    \ after changing the Loader and any loader settings)"
  created_at: 2023-07-23 11:18:24+00:00
  edited: false
  hidden: false
  id: 64bd1a90da140e46194c030c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T12:45:03.000Z'
    data:
      edited: true
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786885976791382
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: '<p>Well, I tried ExLlama first but it didn''t work, then I read the
          instructions on the model card again and you wrote that it''s auto configured
          by config file. So, I tried that, too and it configured the loader to AutoGPTQ.
          Okay, let me try again to be sure.</p>

          '
        raw: Well, I tried ExLlama first but it didn't work, then I read the instructions
          on the model card again and you wrote that it's auto configured by config
          file. So, I tried that, too and it configured the loader to AutoGPTQ. Okay,
          let me try again to be sure.
        updatedAt: '2023-07-23T12:45:24.408Z'
      numEdits: 1
      reactions: []
    id: 64bd20cf5b8d826146cea266
    type: comment
  author: krassmann
  content: Well, I tried ExLlama first but it didn't work, then I read the instructions
    on the model card again and you wrote that it's auto configured by config file.
    So, I tried that, too and it configured the loader to AutoGPTQ. Okay, let me try
    again to be sure.
  created_at: 2023-07-23 11:45:03+00:00
  edited: true
  hidden: false
  id: 64bd20cf5b8d826146cea266
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T12:56:18.000Z'
    data:
      edited: true
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5494489073753357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: '<p>Okay, unfortunately no change when using ExLlama. I also configured
          AutoGPTQ the way you recommended and still the same behavior. Just no reaction
          at all to the prompt.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/ZdJG_c6azkm7E6BOyKk-3.png"><img
          alt="IMG_0574.png" src="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/ZdJG_c6azkm7E6BOyKk-3.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/j2NDTsxKCrNrVBG_eh3ak.png"><img
          alt="IMG_0575.png" src="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/j2NDTsxKCrNrVBG_eh3ak.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/0QouoZLWoTiHJ8QP5u8Fb.png"><img
          alt="IMG_0577.png" src="https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/0QouoZLWoTiHJ8QP5u8Fb.png"></a></p>

          '
        raw: 'Okay, unfortunately no change when using ExLlama. I also configured
          AutoGPTQ the way you recommended and still the same behavior. Just no reaction
          at all to the prompt.

          ![IMG_0574.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/ZdJG_c6azkm7E6BOyKk-3.png)

          ![IMG_0575.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/j2NDTsxKCrNrVBG_eh3ak.png)

          ![IMG_0577.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/0QouoZLWoTiHJ8QP5u8Fb.png)

          '
        updatedAt: '2023-07-23T12:59:05.837Z'
      numEdits: 1
      reactions: []
    id: 64bd237238953777fe7e21a6
    type: comment
  author: krassmann
  content: 'Okay, unfortunately no change when using ExLlama. I also configured AutoGPTQ
    the way you recommended and still the same behavior. Just no reaction at all to
    the prompt.

    ![IMG_0574.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/ZdJG_c6azkm7E6BOyKk-3.png)

    ![IMG_0575.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/j2NDTsxKCrNrVBG_eh3ak.png)

    ![IMG_0577.png](https://cdn-uploads.huggingface.co/production/uploads/6370fddd67cd0e8815fffe64/0QouoZLWoTiHJ8QP5u8Fb.png)

    '
  created_at: 2023-07-23 11:56:18+00:00
  edited: true
  hidden: false
  id: 64bd237238953777fe7e21a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-23T12:57:49.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8479576110839844
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Do you know how to SSH in? Or use the Web Terminal?</p>

          <p>If you do, can you do:</p>

          <pre><code>tail -100 /workspace/logs/*

          </code></pre>

          <p>and copy that output and paste it here</p>

          <p>If not I will try to check it myself a little later</p>

          '
        raw: 'Do you know how to SSH in? Or use the Web Terminal?


          If you do, can you do:

          ```

          tail -100 /workspace/logs/*

          ```


          and copy that output and paste it here


          If not I will try to check it myself a little later'
        updatedAt: '2023-07-23T12:58:11.273Z'
      numEdits: 1
      reactions: []
    id: 64bd23cd979949d2e21342c7
    type: comment
  author: TheBloke
  content: 'Do you know how to SSH in? Or use the Web Terminal?


    If you do, can you do:

    ```

    tail -100 /workspace/logs/*

    ```


    and copy that output and paste it here


    If not I will try to check it myself a little later'
  created_at: 2023-07-23 11:57:49+00:00
  edited: true
  hidden: false
  id: 64bd23cd979949d2e21342c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T13:40:26.000Z'
    data:
      edited: false
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945126533508301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: "<p>I\u2019m an avid Linux user, no problem. Unfortunately I\u2019m\
          \ busy in the next couple hours. I looked for logs at /var/log. I didn\u2019\
          t know app logs go to /workspace.</p>\n"
        raw: "I\u2019m an avid Linux user, no problem. Unfortunately I\u2019m busy\
          \ in the next couple hours. I looked for logs at /var/log. I didn\u2019\
          t know app logs go to /workspace."
        updatedAt: '2023-07-23T13:40:26.052Z'
      numEdits: 0
      reactions: []
    id: 64bd2dca1a62149c5e6ad90a
    type: comment
  author: krassmann
  content: "I\u2019m an avid Linux user, no problem. Unfortunately I\u2019m busy in\
    \ the next couple hours. I looked for logs at /var/log. I didn\u2019t know app\
    \ logs go to /workspace."
  created_at: 2023-07-23 12:40:26+00:00
  edited: false
  hidden: false
  id: 64bd2dca1a62149c5e6ad90a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-07-23T13:56:57.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5230374336242676
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: "<p>Since Krassmann is busy I've tested as well, and I can confirm that\
          \ <code>thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023</code>does not\
          \ work out of the box. Here are the contents of <code>build-llama-cpp-python.log</code>:</p>\n\
          <pre><code>WARNING: Running pip as the 'root' user can result in broken\
          \ permissions and conflicting behaviour with the system package manager.\
          \ It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\
          This system supports AVX2.\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.1.74.tar.gz\
          \ (1.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 MB 13.2 MB/s eta 0:00:00\n\
          \  Installing build dependencies: started\n  Installing build dependencies:\
          \ finished with status 'done'\n  Getting requirements to build wheel: started\n\
          \  Getting requirements to build wheel: finished with status 'done'\n  Preparing\
          \ metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml):\
          \ finished with status 'done'\nRequirement already satisfied: typing-extensions&gt;=4.5.0\
          \ in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n\
          Requirement already satisfied: numpy&gt;=1.20.0 in /usr/local/lib/python3.10/dist-packages\
          \ (from llama-cpp-python) (1.24.4)\nRequirement already satisfied: diskcache&gt;=5.6.1\
          \ in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.1)\n\
          Building wheels for collected packages: llama-cpp-python\n  Building wheel\
          \ for llama-cpp-python (pyproject.toml): started\n  Building wheel for llama-cpp-python\
          \ (pyproject.toml): finished with status 'done'\n  Created wheel for llama-cpp-python:\
          \ filename=llama_cpp_python-0.1.74-cp310-cp310-linux_x86_64.whl size=1330178\
          \ sha256=5f451ec3e0600060c27bb8f82154947e461dd058485872b3cb4f332df5b54040\n\
          \  Stored in directory: /tmp/pip-ephem-wheel-cache-b6nmb0k4/wheels/e4/fe/48/cf667dccd2d15d9b61afdf51b4a7c3c843db1377e1ced97118\n\
          Successfully built llama-cpp-python\nInstalling collected packages: llama-cpp-python\n\
          Successfully installed llama-cpp-python-0.1.74\nWARNING: Running pip as\
          \ the 'root' user can result in broken permissions and conflicting behaviour\
          \ with the system package manager. It is recommended to use a virtual environment\
          \ instead: https://pip.pypa.io/warnings/venv\n\n[notice] A new release of\
          \ pip is available: 23.1.2 -&gt; 23.2.1\n[notice] To update, run: python3\
          \ -m pip install --upgrade pip\n</code></pre>\n<p>Here are the contents\
          \ of <code>text-generation-webui.log</code> after trying both exllama and\
          \ AutoGPTQ with <code>no_inject_fused_attention</code>:</p>\n<pre><code>Launching\
          \ text-generation-webui with args: --listen --api\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          Starting streaming server at ws://0.0.0.0:5005/api/v1/stream\nStarting API\
          \ at http://0.0.0.0:5000/api\nRunning on local URL:  http://0.0.0.0:7860\n\
          \nTo create a public link, set `share=True` in `launch()`.\nDownloading\
          \ the model to models/TheBloke_FreeWilly2-GPTQ\n100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 7.02k /7.02k  19.4MiB/s\n100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.3k /15.3k  49.2MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.77k\
          \ /4.77k  15.1MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 679   /679    2.67MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 137   /137    535kiB/s\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35.3G /35.3G  352MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183 \
          \  /183    726kiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 411   /411    1.63MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 1.84M /1.84M  3.67MiB/s\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k  /500k   17.7MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 649 \
          \  /649    2.69MiB/s\n100.64.0.24 - - [23/Jul/2023 13:40:21] code 404, message\
          \ Not Found\n100.64.0.24 - - [23/Jul/2023 13:40:21] \"GET / HTTP/1.1\" 404\
          \ -\n100.64.0.24 - - [23/Jul/2023 13:40:27] code 404, message Not Found\n\
          100.64.0.24 - - [23/Jul/2023 13:40:27] \"GET / HTTP/1.1\" 404 -\n100.64.0.25\
          \ - - [23/Jul/2023 13:40:33] code 404, message Not Found\n100.64.0.25 -\
          \ - [23/Jul/2023 13:40:33] \"GET / HTTP/1.1\" 404 -\n100.64.0.25 - - [23/Jul/2023\
          \ 13:40:39] code 404, message Not Found\n100.64.0.25 - - [23/Jul/2023 13:40:39]\
          \ \"GET / HTTP/1.1\" 404 -\n2023-07-23 13:42:06 INFO:Loading TheBloke_FreeWilly2-GPTQ...\n\
          2023-07-23 13:42:11 INFO:Loaded the model in 5.12 seconds.\n\nTraceback\
          \ (most recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\"\
          , line 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
          , line 186, in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"\
          /usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 171,\
          \ in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
          \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin :\
          \ chunk_end],\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 930, in _forward\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 14, 64,\
          \ 128]' is invalid for input of size 14336\n2023-07-23 13:51:34 INFO:Loading\
          \ TheBloke_FreeWilly2-GPTQ...\n2023-07-23 13:51:34 INFO:The AutoGPTQ params\
          \ are: {'model_basename': 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton':\
          \ False, 'inject_fused_attention': False, 'inject_fused_mlp': True, 'use_safetensors':\
          \ True, 'trust_remote_code': False, 'max_memory': None, 'quantize_config':\
          \ None, 'use_cuda_fp16': True}\n2023-07-23 13:51:44 WARNING:The model weights\
          \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\n2023-07-23 13:52:11 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n2023-07-23 13:52:11 WARNING:models/TheBloke_FreeWilly2-GPTQ/tokenizer_config.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.\n2023-07-23 13:52:11 INFO:Loaded the model in 37.04 seconds.\n\
          \nTraceback (most recent call last):\n  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
          \  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 297, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 423, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 292, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 195, in forward\n    key_states = self.k_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
          , line 250, in forward\n    out = out + self.bias if self.bias is not None\
          \ else out\nRuntimeError: The size of tensor a (8192) must match the size\
          \ of tensor b (1024) at non-singleton dimension 2\n</code></pre>\n<p>Running\
          \ <code>pip show exllama</code> makes it clear that <code>exllama</code>\
          \ is still on the old <code>0.0.5+cu117</code> version which does not support\
          \ Llama-2. If I update this manually and then restart Ooba it works. So\
          \ the main issue as least as far as <code>exllama</code> seems to be that\
          \ it is not updated automatically.</p>\n"
        raw: "Since Krassmann is busy I've tested as well, and I can confirm that\
          \ `thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023`does not work out of\
          \ the box. Here are the contents of `build-llama-cpp-python.log`:\n```\n\
          WARNING: Running pip as the 'root' user can result in broken permissions\
          \ and conflicting behaviour with the system package manager. It is recommended\
          \ to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\
          This system supports AVX2.\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.1.74.tar.gz\
          \ (1.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
          \u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 MB 13.2 MB/s eta 0:00:00\n\
          \  Installing build dependencies: started\n  Installing build dependencies:\
          \ finished with status 'done'\n  Getting requirements to build wheel: started\n\
          \  Getting requirements to build wheel: finished with status 'done'\n  Preparing\
          \ metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml):\
          \ finished with status 'done'\nRequirement already satisfied: typing-extensions>=4.5.0\
          \ in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n\
          Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages\
          \ (from llama-cpp-python) (1.24.4)\nRequirement already satisfied: diskcache>=5.6.1\
          \ in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.1)\n\
          Building wheels for collected packages: llama-cpp-python\n  Building wheel\
          \ for llama-cpp-python (pyproject.toml): started\n  Building wheel for llama-cpp-python\
          \ (pyproject.toml): finished with status 'done'\n  Created wheel for llama-cpp-python:\
          \ filename=llama_cpp_python-0.1.74-cp310-cp310-linux_x86_64.whl size=1330178\
          \ sha256=5f451ec3e0600060c27bb8f82154947e461dd058485872b3cb4f332df5b54040\n\
          \  Stored in directory: /tmp/pip-ephem-wheel-cache-b6nmb0k4/wheels/e4/fe/48/cf667dccd2d15d9b61afdf51b4a7c3c843db1377e1ced97118\n\
          Successfully built llama-cpp-python\nInstalling collected packages: llama-cpp-python\n\
          Successfully installed llama-cpp-python-0.1.74\nWARNING: Running pip as\
          \ the 'root' user can result in broken permissions and conflicting behaviour\
          \ with the system package manager. It is recommended to use a virtual environment\
          \ instead: https://pip.pypa.io/warnings/venv\n\n[notice] A new release of\
          \ pip is available: 23.1.2 -> 23.2.1\n[notice] To update, run: python3 -m\
          \ pip install --upgrade pip\n```\n\nHere are the contents of `text-generation-webui.log`\
          \ after trying both exllama and AutoGPTQ with `no_inject_fused_attention`:\n\
          \n```\nLaunching text-generation-webui with args: --listen --api\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          Starting streaming server at ws://0.0.0.0:5005/api/v1/stream\nStarting API\
          \ at http://0.0.0.0:5000/api\nRunning on local URL:  http://0.0.0.0:7860\n\
          \nTo create a public link, set `share=True` in `launch()`.\nDownloading\
          \ the model to models/TheBloke_FreeWilly2-GPTQ\n100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 7.02k /7.02k  19.4MiB/s\n100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.3k /15.3k  49.2MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.77k\
          \ /4.77k  15.1MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 679   /679    2.67MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 137   /137    535kiB/s\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35.3G /35.3G  352MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183 \
          \  /183    726kiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 411   /411    1.63MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 1.84M /1.84M  3.67MiB/s\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k  /500k   17.7MiB/s\n\
          100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 649 \
          \  /649    2.69MiB/s\n100.64.0.24 - - [23/Jul/2023 13:40:21] code 404, message\
          \ Not Found\n100.64.0.24 - - [23/Jul/2023 13:40:21] \"GET / HTTP/1.1\" 404\
          \ -\n100.64.0.24 - - [23/Jul/2023 13:40:27] code 404, message Not Found\n\
          100.64.0.24 - - [23/Jul/2023 13:40:27] \"GET / HTTP/1.1\" 404 -\n100.64.0.25\
          \ - - [23/Jul/2023 13:40:33] code 404, message Not Found\n100.64.0.25 -\
          \ - [23/Jul/2023 13:40:33] \"GET / HTTP/1.1\" 404 -\n100.64.0.25 - - [23/Jul/2023\
          \ 13:40:39] code 404, message Not Found\n100.64.0.25 - - [23/Jul/2023 13:40:39]\
          \ \"GET / HTTP/1.1\" 404 -\n2023-07-23 13:42:06 INFO:Loading TheBloke_FreeWilly2-GPTQ...\n\
          2023-07-23 13:42:11 INFO:Loaded the model in 5.12 seconds.\n\nTraceback\
          \ (most recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\"\
          , line 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
          , line 186, in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"\
          /usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 171,\
          \ in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
          \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin :\
          \ chunk_end],\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 930, in _forward\n    hidden_states = decoder_layer.forward(hidden_states,\
          \ cache, buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
          \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
          , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
          \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 14, 64,\
          \ 128]' is invalid for input of size 14336\n2023-07-23 13:51:34 INFO:Loading\
          \ TheBloke_FreeWilly2-GPTQ...\n2023-07-23 13:51:34 INFO:The AutoGPTQ params\
          \ are: {'model_basename': 'gptq_model-4bit--1g', 'device': 'cuda:0', 'use_triton':\
          \ False, 'inject_fused_attention': False, 'inject_fused_mlp': True, 'use_safetensors':\
          \ True, 'trust_remote_code': False, 'max_memory': None, 'quantize_config':\
          \ None, 'use_cuda_fp16': True}\n2023-07-23 13:51:44 WARNING:The model weights\
          \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\n2023-07-23 13:52:11 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n2023-07-23 13:52:11 WARNING:models/TheBloke_FreeWilly2-GPTQ/tokenizer_config.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.\n2023-07-23 13:52:11 INFO:Loaded the model in 37.04 seconds.\n\
          \nTraceback (most recent call last):\n  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
          , line 55, in gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
          \  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 297, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 423, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1572, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 292, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
          , line 195, in forward\n    key_states = self.k_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
          , line 250, in forward\n    out = out + self.bias if self.bias is not None\
          \ else out\nRuntimeError: The size of tensor a (8192) must match the size\
          \ of tensor b (1024) at non-singleton dimension 2\n```\nRunning `pip show\
          \ exllama` makes it clear that `exllama` is still on the old `0.0.5+cu117`\
          \ version which does not support Llama-2. If I update this manually and\
          \ then restart Ooba it works. So the main issue as least as far as `exllama`\
          \ seems to be that it is not updated automatically."
        updatedAt: '2023-07-23T14:00:58.072Z'
      numEdits: 2
      reactions: []
    id: 64bd31a9c05a0df0d2959dee
    type: comment
  author: Mikael110
  content: "Since Krassmann is busy I've tested as well, and I can confirm that `thebloke/cuda11.8.0-ubuntu22.04-oneclick:21072023`does\
    \ not work out of the box. Here are the contents of `build-llama-cpp-python.log`:\n\
    ```\nWARNING: Running pip as the 'root' user can result in broken permissions\
    \ and conflicting behaviour with the system package manager. It is recommended\
    \ to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nThis\
    \ system supports AVX2.\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.1.74.tar.gz\
    \ (1.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\
    \u2501\u2501\u2501 1.6/1.6 MB 13.2 MB/s eta 0:00:00\n  Installing build dependencies:\
    \ started\n  Installing build dependencies: finished with status 'done'\n  Getting\
    \ requirements to build wheel: started\n  Getting requirements to build wheel:\
    \ finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n\
    \  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement\
    \ already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages\
    \ (from llama-cpp-python) (4.7.1)\nRequirement already satisfied: numpy>=1.20.0\
    \ in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.24.4)\n\
    Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages\
    \ (from llama-cpp-python) (5.6.1)\nBuilding wheels for collected packages: llama-cpp-python\n\
    \  Building wheel for llama-cpp-python (pyproject.toml): started\n  Building wheel\
    \ for llama-cpp-python (pyproject.toml): finished with status 'done'\n  Created\
    \ wheel for llama-cpp-python: filename=llama_cpp_python-0.1.74-cp310-cp310-linux_x86_64.whl\
    \ size=1330178 sha256=5f451ec3e0600060c27bb8f82154947e461dd058485872b3cb4f332df5b54040\n\
    \  Stored in directory: /tmp/pip-ephem-wheel-cache-b6nmb0k4/wheels/e4/fe/48/cf667dccd2d15d9b61afdf51b4a7c3c843db1377e1ced97118\n\
    Successfully built llama-cpp-python\nInstalling collected packages: llama-cpp-python\n\
    Successfully installed llama-cpp-python-0.1.74\nWARNING: Running pip as the 'root'\
    \ user can result in broken permissions and conflicting behaviour with the system\
    \ package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\
    \n[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n[notice] To update,\
    \ run: python3 -m pip install --upgrade pip\n```\n\nHere are the contents of `text-generation-webui.log`\
    \ after trying both exllama and AutoGPTQ with `no_inject_fused_attention`:\n\n\
    ```\nLaunching text-generation-webui with args: --listen --api\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
    Starting streaming server at ws://0.0.0.0:5005/api/v1/stream\nStarting API at\
    \ http://0.0.0.0:5000/api\nRunning on local URL:  http://0.0.0.0:7860\n\nTo create\
    \ a public link, set `share=True` in `launch()`.\nDownloading the model to models/TheBloke_FreeWilly2-GPTQ\n\
    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.02k /7.02k\
    \  19.4MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 15.3k /15.3k  49.2MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 4.77k /4.77k  15.1MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 679   /679    2.67MiB/s\n100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 137   /137    535kiB/s\n100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35.3G /35.3G  352MiB/s\n100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 183   /183    726kiB/s\n\
    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 411   /411\
    \    1.63MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 1.84M /1.84M  3.67MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 500k  /500k   17.7MiB/s\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 649   /649    2.69MiB/s\n100.64.0.24 - - [23/Jul/2023\
    \ 13:40:21] code 404, message Not Found\n100.64.0.24 - - [23/Jul/2023 13:40:21]\
    \ \"GET / HTTP/1.1\" 404 -\n100.64.0.24 - - [23/Jul/2023 13:40:27] code 404, message\
    \ Not Found\n100.64.0.24 - - [23/Jul/2023 13:40:27] \"GET / HTTP/1.1\" 404 -\n\
    100.64.0.25 - - [23/Jul/2023 13:40:33] code 404, message Not Found\n100.64.0.25\
    \ - - [23/Jul/2023 13:40:33] \"GET / HTTP/1.1\" 404 -\n100.64.0.25 - - [23/Jul/2023\
    \ 13:40:39] code 404, message Not Found\n100.64.0.25 - - [23/Jul/2023 13:40:39]\
    \ \"GET / HTTP/1.1\" 404 -\n2023-07-23 13:42:06 INFO:Loading TheBloke_FreeWilly2-GPTQ...\n\
    2023-07-23 13:42:11 INFO:Loaded the model in 5.12 seconds.\n\nTraceback (most\
    \ recent call last):\n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
    , line 331, in generate_reply_custom\n    for reply in shared.model.generate_with_streaming(question,\
    \ state):\n  File \"/workspace/text-generation-webui/modules/exllama.py\", line\
    \ 98, in generate_with_streaming\n    self.generator.gen_begin_reuse(ids)\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\", line 186,\
    \ in gen_begin_reuse\n    self.gen_begin(in_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/generator.py\"\
    , line 171, in gen_begin\n    self.model.forward(self.sequence[:, :-1], self.cache,\
    \ preprocess_only = True, lora = self.lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 849, in forward\n    r = self._forward(input_ids[:, chunk_begin : chunk_end],\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\", line 930,\
    \ in _forward\n    hidden_states = decoder_layer.forward(hidden_states, cache,\
    \ buffers[device], lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 470, in forward\n    hidden_states = self.self_attn.forward(hidden_states,\
    \ cache, buffer, lora)\n  File \"/usr/local/lib/python3.10/dist-packages/exllama/model.py\"\
    , line 388, in forward\n    key_states = key_states.view(bsz, q_len, self.config.num_attention_heads,\
    \ self.config.head_dim).transpose(1, 2)\nRuntimeError: shape '[1, 14, 64, 128]'\
    \ is invalid for input of size 14336\n2023-07-23 13:51:34 INFO:Loading TheBloke_FreeWilly2-GPTQ...\n\
    2023-07-23 13:51:34 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit--1g',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': False, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True}\n2023-07-23 13:51:44 WARNING:The\
    \ model weights are not tied. Please use the `tie_weights` method before using\
    \ the `infer_auto_device` function.\n2023-07-23 13:52:11 WARNING:skip module injection\
    \ for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n\
    2023-07-23 13:52:11 WARNING:models/TheBloke_FreeWilly2-GPTQ/tokenizer_config.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\n2023-07-23 13:52:11 INFO:Loaded the model in 37.04 seconds.\n\n\
    Traceback (most recent call last):\n  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
    , line 55, in gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
    \  File \"/workspace/text-generation-webui/modules/text_generation.py\", line\
    \ 297, in generate_with_callback\n    shared.model.generate(**kwargs)\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\", line\
    \ 423, in generate\n    return self.model.generate(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 1572, in generate\n    return self.sample(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 2619, in sample\n    outputs = self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 688, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 578, in forward\n    layer_outputs = decoder_layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 292, in forward\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\"\
    , line 195, in forward\n    key_states = self.k_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear_old.py\"\
    , line 250, in forward\n    out = out + self.bias if self.bias is not None else\
    \ out\nRuntimeError: The size of tensor a (8192) must match the size of tensor\
    \ b (1024) at non-singleton dimension 2\n```\nRunning `pip show exllama` makes\
    \ it clear that `exllama` is still on the old `0.0.5+cu117` version which does\
    \ not support Llama-2. If I update this manually and then restart Ooba it works.\
    \ So the main issue as least as far as `exllama` seems to be that it is not updated\
    \ automatically."
  created_at: 2023-07-23 12:56:57+00:00
  edited: true
  hidden: false
  id: 64bd31a9c05a0df0d2959dee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-23T14:06:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9003033638000488
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks, I will sort it out</p>

          '
        raw: OK thanks, I will sort it out
        updatedAt: '2023-07-23T14:06:22.846Z'
      numEdits: 0
      reactions: []
    id: 64bd33de5c457ccaa413bc0f
    type: comment
  author: TheBloke
  content: OK thanks, I will sort it out
  created_at: 2023-07-23 13:06:22+00:00
  edited: false
  hidden: false
  id: 64bd33de5c457ccaa413bc0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-23T16:02:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8291155695915222
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK it should now be fixed.  <code>thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest</code>
          is updated, so the default Runpod templates should work fine now.  I just
          tested myself with Llama-2-70B-Chat-GPTQ and it worked fine.</p>

          '
        raw: OK it should now be fixed.  `thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest`
          is updated, so the default Runpod templates should work fine now.  I just
          tested myself with Llama-2-70B-Chat-GPTQ and it worked fine.
        updatedAt: '2023-07-23T16:02:32.267Z'
      numEdits: 0
      reactions: []
    id: 64bd4f1894c0e3be4ae9889a
    type: comment
  author: TheBloke
  content: OK it should now be fixed.  `thebloke/cuda11.8.0-ubuntu22.04-oneclick:latest`
    is updated, so the default Runpod templates should work fine now.  I just tested
    myself with Llama-2-70B-Chat-GPTQ and it worked fine.
  created_at: 2023-07-23 15:02:32+00:00
  edited: false
  hidden: false
  id: 64bd4f1894c0e3be4ae9889a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T19:12:39.000Z'
    data:
      edited: false
      editors:
      - krassmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9944178462028503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
          fullname: krass mann
          isHf: false
          isPro: false
          name: krassmann
          type: user
        html: '<p>I confirm it''s working now. Thank you all.</p>

          '
        raw: I confirm it's working now. Thank you all.
        updatedAt: '2023-07-23T19:12:39.138Z'
      numEdits: 0
      reactions: []
    id: 64bd7ba7c733e8552fd69ea0
    type: comment
  author: krassmann
  content: I confirm it's working now. Thank you all.
  created_at: 2023-07-23 18:12:39+00:00
  edited: false
  hidden: false
  id: 64bd7ba7c733e8552fd69ea0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/68ce1f333b98f5e3104bc2b5a1cc104e.svg
      fullname: krass mann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krassmann
      type: user
    createdAt: '2023-07-23T19:12:53.000Z'
    data:
      status: closed
    id: 64bd7bb5c733e8552fd6a012
    type: status-change
  author: krassmann
  created_at: 2023-07-23 18:12:53+00:00
  id: 64bd7bb5c733e8552fd6a012
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Problems on runpod.io
