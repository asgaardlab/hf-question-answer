!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AIReach
conflicting_files: null
created_at: 2023-07-26 02:27:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-07-26T03:27:10.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6742449998855591
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>I compared freewilly2 with bitsandbytes nf4 and this GPTQ main branch.
          Hope improve GPTQ version accuracy but preserve the inference speed. Thank
          you.</p>

          <p>Running on two 4090s.<br>Based on text generation webui.<br>Inference
          Speed: GPTQ 15 tokens/s, nf4 7 tokens/s<br>Loading Speed: GPTQ 8s, nf4 152s<br>GPTQ
          main branch VS. bitsandbytes v0.40 nf4</p>

          <p>Math calculation:</p>

          <h3 id="system">System:</h3>

          <p>You are an AI assistant that follows instruction extremely well. Help
          as much as you can. Think carefully step by step.</p>

          <h3 id="user">User:</h3>

          <p>Do math 99*99-10</p>

          <h3 id="response">Response:</h3>

          <p>Comparison: GPTQ takes number 99 as 9, nf4 calculate 99*99-10 sucessfully.
          </p>

          <p>GPTQ results:<br>Sure, let''s do the calculation step by step:<br>First,
          we need to calculate 9 * 9 which is 81 (9 multiplied by itself). So, 9 *
          9 = 81.<br>Now, subtract 10 from this result: 81 - 10 = 71.<br>Therefore,
          the final answer is 71.</p>

          <p>NF4 results:<br> To perform the calculation, follow these steps:<br>Step
          1: Multiply 99 and 99 to find their product (99 * 99).<br>(99 * 99) = 9801<br>Step
          2: Subtract 10 from the result of Step 1.<br>(9801 - 10) = 9791<br>The final
          answer is 9791.</p>

          <p>Python coding problems:</p>

          <h3 id="system-1">System:</h3>

          <p>You are an AI assistant that follows instruction extremely well. Help
          as much as you can. Think carefully step by step.</p>

          <h3 id="user-1">User:</h3>

          <p>write a python code, which generate 10 random numbers and sort with 4
          different methods from low to high.</p>

          <h3 id="response-1">Response:</h3>

          <p>Comparison: GPTQ failed, nf4 defined four functions without issues.</p>

          <p>GPTQ results:<br>import random<br>numbers = [random.randint(0, 9) for
          _ in range(10)] # Generate 10 random integers between 0-9<br>print("Original
          list of numbers:", numbers)<br>sorted_list = sorted(numbers)          #
          Sort using built-in ''sorted'' function<br>print("Sorted using ''sorted'':
          ", sorted_list)<br>sorted_list2 = list(map(lambda x: abs(x), numbers) )   #
          Convert negative values to positive<br>sorted_list3 = sorted(sorted_list2)    #
          Sort using lambda function<br>print("Sorted using lambda function: ", sorted_list3)<br>sorted_list4
          = sorted(numbers, key=abs)         # Use ''key'' parameter to convert negatives
          to positives before sorting<br>print("Sorted using ''key'': ", sorted_list4)<br>sorted_list5
          = sorted(numbers, reverse=True)   # Reverse order (highest first)<br>print("Reversed
          sorted: ", sorted_list5[::-1])</p>

          <p>nf4 results:<br>import random<br>from time import sleep</p>

          <p>def bubblesort(arr):<br>    for i in range(len(arr)):<br>        for
          j in range(i+1, len(arr)):<br>            if arr[j] &lt; arr[i]:<br>                arr[i],
          arr[j] = arr[j], arr[i]</p>

          <p>def selectionsort(arr):<br>    for i in range(len(arr)):<br>        min_idx
          = i<br>        for j in range(i+1, len(arr)):<br>            if arr[j] &lt;
          arr[min_idx]:<br>                min_idx = j<br>        arr[i], arr[min_idx]
          = arr[min_idx], arr[i]</p>

          <p>def insertionsort(arr):<br>    for i in range(1, len(arr)):<br>        key
          = arr[i]<br>        j = i-1<br>        while j &gt;= 0 and arr[j] &gt; key:<br>            arr[j+1]
          = arr[j]<br>            arr[j] = key<br>            j -= 1</p>

          <p>def mergesort(arr):<br>    if len(arr) &lt;= 1:<br>        return arr<br>    mid
          = len(arr) // 2<br>    left = arr[:mid]<br>    right = arr[mid:]<br>    left
          = mergesort(left)<br>    right = mergesort(right)<br>    result = []<br>    i,
          j = 0, 0<br>    while i &lt; len(left) and j &lt; len(right):<br>        if
          left[i] &lt; right[j]:<br>            result.append(left[i])<br>            i
          += 1<br>        else:<br>            result.append(right[j])<br>            j
          += 1<br>    result += left[i:] + right[j:]<br>    return result</p>

          <p>numbers = [random.randint(-999, 999) for _ in range(10)]<br>print("Original
          list of random numbers:", numbers)<br>sleep(3)</p>

          <p>bubblesort(numbers)<br>print("Sorted list by Bubble Sort:", numbers)<br>sleep(3)</p>

          <p>selectionsort(numbers)<br>print("Sorted list by Selection Sort:", numbers)<br>sleep(3)</p>

          <p>insertionsort(numbers)<br>print("Sorted list by Insertion Sort:", numbers)<br>sleep(3)</p>

          <p>mergesort(numbers)<br>print("Sorted list by Merge Sort:", numbers)<br>sleep(3)</p>

          '
        raw: "I compared freewilly2 with bitsandbytes nf4 and this GPTQ main branch.\
          \ Hope improve GPTQ version accuracy but preserve the inference speed. Thank\
          \ you.\r\n\r\nRunning on two 4090s.\r\nBased on text generation webui.\r\
          \nInference Speed: GPTQ 15 tokens/s, nf4 7 tokens/s\r\nLoading Speed: GPTQ\
          \ 8s, nf4 152s\r\nGPTQ main branch VS. bitsandbytes v0.40 nf4\r\n\r\n\r\n\
          Math calculation:\r\n\r\n### System:\r\nYou are an AI assistant that follows\
          \ instruction extremely well. Help as much as you can. Think carefully step\
          \ by step.\r\n### User:\r\nDo math 99*99-10\r\n### Response:\r\n\r\nComparison:\
          \ GPTQ takes number 99 as 9, nf4 calculate 99*99-10 sucessfully. \r\n\r\n\
          GPTQ results:  \r\nSure, let's do the calculation step by step:\r\nFirst,\
          \ we need to calculate 9 * 9 which is 81 (9 multiplied by itself). So, 9\
          \ * 9 = 81.\r\nNow, subtract 10 from this result: 81 - 10 = 71.\r\nTherefore,\
          \ the final answer is 71.\r\n\r\nNF4 results:\r\n To perform the calculation,\
          \ follow these steps:\r\nStep 1: Multiply 99 and 99 to find their product\
          \ (99 * 99).\r\n(99 * 99) = 9801\r\nStep 2: Subtract 10 from the result\
          \ of Step 1.\r\n(9801 - 10) = 9791\r\nThe final answer is 9791.\r\n\r\n\r\
          \n\r\n\r\nPython coding problems:\r\n\r\n### System:\r\nYou are an AI assistant\
          \ that follows instruction extremely well. Help as much as you can. Think\
          \ carefully step by step.\r\n### User:\r\nwrite a python code, which generate\
          \ 10 random numbers and sort with 4 different methods from low to high.\r\
          \n### Response:\r\n\r\nComparison: GPTQ failed, nf4 defined four functions\
          \ without issues.\r\n\r\nGPTQ results:\r\nimport random\r\nnumbers = [random.randint(0,\
          \ 9) for _ in range(10)] # Generate 10 random integers between 0-9\r\nprint(\"\
          Original list of numbers:\", numbers)\r\nsorted_list = sorted(numbers) \
          \         # Sort using built-in 'sorted' function\r\nprint(\"Sorted using\
          \ 'sorted': \", sorted_list)\r\nsorted_list2 = list(map(lambda x: abs(x),\
          \ numbers) )   # Convert negative values to positive\r\nsorted_list3 = sorted(sorted_list2)\
          \    # Sort using lambda function\r\nprint(\"Sorted using lambda function:\
          \ \", sorted_list3)\r\nsorted_list4 = sorted(numbers, key=abs)         #\
          \ Use 'key' parameter to convert negatives to positives before sorting\r\
          \nprint(\"Sorted using 'key': \", sorted_list4)\r\nsorted_list5 = sorted(numbers,\
          \ reverse=True)   # Reverse order (highest first)\r\nprint(\"Reversed sorted:\
          \ \", sorted_list5[::-1])\r\n\r\nnf4 results:\r\nimport random\r\nfrom time\
          \ import sleep\r\n\r\ndef bubblesort(arr):\r\n    for i in range(len(arr)):\r\
          \n        for j in range(i+1, len(arr)):\r\n            if arr[j] < arr[i]:\r\
          \n                arr[i], arr[j] = arr[j], arr[i]\r\n\r\ndef selectionsort(arr):\r\
          \n    for i in range(len(arr)):\r\n        min_idx = i\r\n        for j\
          \ in range(i+1, len(arr)):\r\n            if arr[j] < arr[min_idx]:\r\n\
          \                min_idx = j\r\n        arr[i], arr[min_idx] = arr[min_idx],\
          \ arr[i]\r\n\r\ndef insertionsort(arr):\r\n    for i in range(1, len(arr)):\r\
          \n        key = arr[i]\r\n        j = i-1\r\n        while j >= 0 and arr[j]\
          \ > key:\r\n            arr[j+1] = arr[j]\r\n            arr[j] = key\r\n\
          \            j -= 1\r\n\r\ndef mergesort(arr):\r\n    if len(arr) <= 1:\r\
          \n        return arr\r\n    mid = len(arr) // 2\r\n    left = arr[:mid]\r\
          \n    right = arr[mid:]\r\n    left = mergesort(left)\r\n    right = mergesort(right)\r\
          \n    result = []\r\n    i, j = 0, 0\r\n    while i < len(left) and j <\
          \ len(right):\r\n        if left[i] < right[j]:\r\n            result.append(left[i])\r\
          \n            i += 1\r\n        else:\r\n            result.append(right[j])\r\
          \n            j += 1\r\n    result += left[i:] + right[j:]\r\n    return\
          \ result\r\n\r\nnumbers = [random.randint(-999, 999) for _ in range(10)]\r\
          \nprint(\"Original list of random numbers:\", numbers)\r\nsleep(3)\r\n\r\
          \nbubblesort(numbers)\r\nprint(\"Sorted list by Bubble Sort:\", numbers)\r\
          \nsleep(3)\r\n\r\nselectionsort(numbers)\r\nprint(\"Sorted list by Selection\
          \ Sort:\", numbers)\r\nsleep(3)\r\n\r\ninsertionsort(numbers)\r\nprint(\"\
          Sorted list by Insertion Sort:\", numbers)\r\nsleep(3)\r\n\r\nmergesort(numbers)\r\
          \nprint(\"Sorted list by Merge Sort:\", numbers)\r\nsleep(3)\r\n\r\n"
        updatedAt: '2023-07-26T03:27:10.361Z'
      numEdits: 0
      reactions: []
    id: 64c0928e92ae08de17479f1b
    type: comment
  author: AIReach
  content: "I compared freewilly2 with bitsandbytes nf4 and this GPTQ main branch.\
    \ Hope improve GPTQ version accuracy but preserve the inference speed. Thank you.\r\
    \n\r\nRunning on two 4090s.\r\nBased on text generation webui.\r\nInference Speed:\
    \ GPTQ 15 tokens/s, nf4 7 tokens/s\r\nLoading Speed: GPTQ 8s, nf4 152s\r\nGPTQ\
    \ main branch VS. bitsandbytes v0.40 nf4\r\n\r\n\r\nMath calculation:\r\n\r\n\
    ### System:\r\nYou are an AI assistant that follows instruction extremely well.\
    \ Help as much as you can. Think carefully step by step.\r\n### User:\r\nDo math\
    \ 99*99-10\r\n### Response:\r\n\r\nComparison: GPTQ takes number 99 as 9, nf4\
    \ calculate 99*99-10 sucessfully. \r\n\r\nGPTQ results:  \r\nSure, let's do the\
    \ calculation step by step:\r\nFirst, we need to calculate 9 * 9 which is 81 (9\
    \ multiplied by itself). So, 9 * 9 = 81.\r\nNow, subtract 10 from this result:\
    \ 81 - 10 = 71.\r\nTherefore, the final answer is 71.\r\n\r\nNF4 results:\r\n\
    \ To perform the calculation, follow these steps:\r\nStep 1: Multiply 99 and 99\
    \ to find their product (99 * 99).\r\n(99 * 99) = 9801\r\nStep 2: Subtract 10\
    \ from the result of Step 1.\r\n(9801 - 10) = 9791\r\nThe final answer is 9791.\r\
    \n\r\n\r\n\r\n\r\nPython coding problems:\r\n\r\n### System:\r\nYou are an AI\
    \ assistant that follows instruction extremely well. Help as much as you can.\
    \ Think carefully step by step.\r\n### User:\r\nwrite a python code, which generate\
    \ 10 random numbers and sort with 4 different methods from low to high.\r\n###\
    \ Response:\r\n\r\nComparison: GPTQ failed, nf4 defined four functions without\
    \ issues.\r\n\r\nGPTQ results:\r\nimport random\r\nnumbers = [random.randint(0,\
    \ 9) for _ in range(10)] # Generate 10 random integers between 0-9\r\nprint(\"\
    Original list of numbers:\", numbers)\r\nsorted_list = sorted(numbers)       \
    \   # Sort using built-in 'sorted' function\r\nprint(\"Sorted using 'sorted':\
    \ \", sorted_list)\r\nsorted_list2 = list(map(lambda x: abs(x), numbers) )   #\
    \ Convert negative values to positive\r\nsorted_list3 = sorted(sorted_list2) \
    \   # Sort using lambda function\r\nprint(\"Sorted using lambda function: \",\
    \ sorted_list3)\r\nsorted_list4 = sorted(numbers, key=abs)         # Use 'key'\
    \ parameter to convert negatives to positives before sorting\r\nprint(\"Sorted\
    \ using 'key': \", sorted_list4)\r\nsorted_list5 = sorted(numbers, reverse=True)\
    \   # Reverse order (highest first)\r\nprint(\"Reversed sorted: \", sorted_list5[::-1])\r\
    \n\r\nnf4 results:\r\nimport random\r\nfrom time import sleep\r\n\r\ndef bubblesort(arr):\r\
    \n    for i in range(len(arr)):\r\n        for j in range(i+1, len(arr)):\r\n\
    \            if arr[j] < arr[i]:\r\n                arr[i], arr[j] = arr[j], arr[i]\r\
    \n\r\ndef selectionsort(arr):\r\n    for i in range(len(arr)):\r\n        min_idx\
    \ = i\r\n        for j in range(i+1, len(arr)):\r\n            if arr[j] < arr[min_idx]:\r\
    \n                min_idx = j\r\n        arr[i], arr[min_idx] = arr[min_idx],\
    \ arr[i]\r\n\r\ndef insertionsort(arr):\r\n    for i in range(1, len(arr)):\r\n\
    \        key = arr[i]\r\n        j = i-1\r\n        while j >= 0 and arr[j] >\
    \ key:\r\n            arr[j+1] = arr[j]\r\n            arr[j] = key\r\n      \
    \      j -= 1\r\n\r\ndef mergesort(arr):\r\n    if len(arr) <= 1:\r\n        return\
    \ arr\r\n    mid = len(arr) // 2\r\n    left = arr[:mid]\r\n    right = arr[mid:]\r\
    \n    left = mergesort(left)\r\n    right = mergesort(right)\r\n    result = []\r\
    \n    i, j = 0, 0\r\n    while i < len(left) and j < len(right):\r\n        if\
    \ left[i] < right[j]:\r\n            result.append(left[i])\r\n            i +=\
    \ 1\r\n        else:\r\n            result.append(right[j])\r\n            j +=\
    \ 1\r\n    result += left[i:] + right[j:]\r\n    return result\r\n\r\nnumbers\
    \ = [random.randint(-999, 999) for _ in range(10)]\r\nprint(\"Original list of\
    \ random numbers:\", numbers)\r\nsleep(3)\r\n\r\nbubblesort(numbers)\r\nprint(\"\
    Sorted list by Bubble Sort:\", numbers)\r\nsleep(3)\r\n\r\nselectionsort(numbers)\r\
    \nprint(\"Sorted list by Selection Sort:\", numbers)\r\nsleep(3)\r\n\r\ninsertionsort(numbers)\r\
    \nprint(\"Sorted list by Insertion Sort:\", numbers)\r\nsleep(3)\r\n\r\nmergesort(numbers)\r\
    \nprint(\"Sorted list by Merge Sort:\", numbers)\r\nsleep(3)\r\n\r\n"
  created_at: 2023-07-26 02:27:10+00:00
  edited: false
  hidden: false
  id: 64c0928e92ae08de17479f1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
      fullname: Miron Tewfik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: streetyogi
      type: user
    createdAt: '2023-07-26T13:31:20.000Z'
    data:
      edited: false
      editors:
      - streetyogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9465687274932861
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
          fullname: Miron Tewfik
          isHf: false
          isPro: false
          name: streetyogi
          type: user
        html: '<p>Loading speed 8s? I have 2x3090, but the loading speed is 78.54s
          seconds in text-generation-webui/ExLlama, how did you load the model? Shoudn''t
          be that different as your cards have also  GDDR6X VRAM.</p>

          '
        raw: Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds
          in text-generation-webui/ExLlama, how did you load the model? Shoudn't be
          that different as your cards have also  GDDR6X VRAM.
        updatedAt: '2023-07-26T13:31:20.774Z'
      numEdits: 0
      reactions: []
    id: 64c12028e8e1818a36909ed9
    type: comment
  author: streetyogi
  content: Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds
    in text-generation-webui/ExLlama, how did you load the model? Shoudn't be that
    different as your cards have also  GDDR6X VRAM.
  created_at: 2023-07-26 12:31:20+00:00
  edited: false
  hidden: false
  id: 64c12028e8e1818a36909ed9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
      fullname: Thierry SALVOCH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tsalvoch
      type: user
    createdAt: '2023-07-26T13:46:39.000Z'
    data:
      edited: false
      editors:
      - tsalvoch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9149570465087891
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
          fullname: Thierry SALVOCH
          isHf: false
          isPro: false
          name: tsalvoch
          type: user
        html: '<p>Thank you for your feedback AIReach.<br>I notice an issue with the
          numbers in all versions of LLaMA-2 in GPTQ. Some are sometimes forgotten,
          or sometimes added.<br>I need to test with bitsandbytes.</p>

          '
        raw: 'Thank you for your feedback AIReach.

          I notice an issue with the numbers in all versions of LLaMA-2 in GPTQ. Some
          are sometimes forgotten, or sometimes added.

          I need to test with bitsandbytes.'
        updatedAt: '2023-07-26T13:46:39.245Z'
      numEdits: 0
      reactions: []
    id: 64c123bf36db3901ade63c84
    type: comment
  author: tsalvoch
  content: 'Thank you for your feedback AIReach.

    I notice an issue with the numbers in all versions of LLaMA-2 in GPTQ. Some are
    sometimes forgotten, or sometimes added.

    I need to test with bitsandbytes.'
  created_at: 2023-07-26 12:46:39+00:00
  edited: false
  hidden: false
  id: 64c123bf36db3901ade63c84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-07-26T14:07:28.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9348399043083191
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<blockquote>

          <p>Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds
          in text-generation-webui/ExLlama, how did you load the model? Shoudn''t
          be that different as your cards have also  GDDR6X VRAM.</p>

          </blockquote>

          <p>I tested again, exllama-hf and exllama methods require around 10 seconds
          for first time new loading, and 5.2 seconds for the following reloading.
          </p>

          <p>GPTQ model was stored on the 980PRO SSD, 500GB of 1T available. </p>

          '
        raw: "> Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds\
          \ in text-generation-webui/ExLlama, how did you load the model? Shoudn't\
          \ be that different as your cards have also  GDDR6X VRAM.\n\nI tested again,\
          \ exllama-hf and exllama methods require around 10 seconds for first time\
          \ new loading, and 5.2 seconds for the following reloading. \n\nGPTQ model\
          \ was stored on the 980PRO SSD, 500GB of 1T available. \n"
        updatedAt: '2023-07-26T14:07:28.141Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - streetyogi
    id: 64c128a05fcc1b62ee997894
    type: comment
  author: AIReach
  content: "> Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds\
    \ in text-generation-webui/ExLlama, how did you load the model? Shoudn't be that\
    \ different as your cards have also  GDDR6X VRAM.\n\nI tested again, exllama-hf\
    \ and exllama methods require around 10 seconds for first time new loading, and\
    \ 5.2 seconds for the following reloading. \n\nGPTQ model was stored on the 980PRO\
    \ SSD, 500GB of 1T available. \n"
  created_at: 2023-07-26 13:07:28+00:00
  edited: false
  hidden: false
  id: 64c128a05fcc1b62ee997894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
      fullname: Miron Tewfik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: streetyogi
      type: user
    createdAt: '2023-07-26T14:14:13.000Z'
    data:
      edited: true
      editors:
      - streetyogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9207049012184143
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
          fullname: Miron Tewfik
          isHf: false
          isPro: false
          name: streetyogi
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Loading speed 8s? I have 2x3090, but\
          \ the loading speed is 78.54s seconds in text-generation-webui/ExLlama,\
          \ how did you load the model? Shoudn't be that different as your cards have\
          \ also  GDDR6X VRAM.</p>\n</blockquote>\n<p>I tested again, exllama-hf and\
          \ exllama methods require around 10 seconds for first time new loading,\
          \ and 5.2 seconds for the following reloading. </p>\n<p>GPTQ model was stored\
          \ on the 980PRO SSD, 500GB of 1T available.</p>\n</blockquote>\n<p>\U0001F926\
          \u200D\u2642\uFE0FOf course the SSD is the bottleneck for loading :) I use\
          \ a 500GB free on Kingston 1TB, but it's a WSL ext4 vhdx, lot room for improvement\
          \ I guess :)</p>\n<p>Edit:<br>Loads in 48 seconds now, just by moving it\
          \ to a WD Black, only having 400GB of 2T available.</p>\n"
        raw: "> > Loading speed 8s? I have 2x3090, but the loading speed is 78.54s\
          \ seconds in text-generation-webui/ExLlama, how did you load the model?\
          \ Shoudn't be that different as your cards have also  GDDR6X VRAM.\n> \n\
          > I tested again, exllama-hf and exllama methods require around 10 seconds\
          \ for first time new loading, and 5.2 seconds for the following reloading.\
          \ \n> \n> GPTQ model was stored on the 980PRO SSD, 500GB of 1T available.\n\
          \n\U0001F926\u200D\u2642\uFE0FOf course the SSD is the bottleneck for loading\
          \ :) I use a 500GB free on Kingston 1TB, but it's a WSL ext4 vhdx, lot room\
          \ for improvement I guess :)\n\nEdit:\nLoads in 48 seconds now, just by\
          \ moving it to a WD Black, only having 400GB of 2T available."
        updatedAt: '2023-07-26T15:10:04.907Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AIReach
    id: 64c12a35f454c0dc7d5e219a
    type: comment
  author: streetyogi
  content: "> > Loading speed 8s? I have 2x3090, but the loading speed is 78.54s seconds\
    \ in text-generation-webui/ExLlama, how did you load the model? Shoudn't be that\
    \ different as your cards have also  GDDR6X VRAM.\n> \n> I tested again, exllama-hf\
    \ and exllama methods require around 10 seconds for first time new loading, and\
    \ 5.2 seconds for the following reloading. \n> \n> GPTQ model was stored on the\
    \ 980PRO SSD, 500GB of 1T available.\n\n\U0001F926\u200D\u2642\uFE0FOf course\
    \ the SSD is the bottleneck for loading :) I use a 500GB free on Kingston 1TB,\
    \ but it's a WSL ext4 vhdx, lot room for improvement I guess :)\n\nEdit:\nLoads\
    \ in 48 seconds now, just by moving it to a WD Black, only having 400GB of 2T\
    \ available."
  created_at: 2023-07-26 13:14:13+00:00
  edited: true
  hidden: false
  id: 64c12a35f454c0dc7d5e219a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-07-26T14:22:07.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9785258173942566
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>could be the reason. It is ran on a linux system. not a big deal,
          i will focus on bitsandbytes, which is slow.</p>

          '
        raw: could be the reason. It is ran on a linux system. not a big deal, i will
          focus on bitsandbytes, which is slow.
        updatedAt: '2023-07-26T14:22:07.537Z'
      numEdits: 0
      reactions: []
    id: 64c12c0f11f3f1d23a90607a
    type: comment
  author: AIReach
  content: could be the reason. It is ran on a linux system. not a big deal, i will
    focus on bitsandbytes, which is slow.
  created_at: 2023-07-26 13:22:07+00:00
  edited: false
  hidden: false
  id: 64c12c0f11f3f1d23a90607a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
      fullname: Thierry SALVOCH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tsalvoch
      type: user
    createdAt: '2023-07-26T18:52:17.000Z'
    data:
      edited: false
      editors:
      - tsalvoch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674675464630127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
          fullname: Thierry SALVOCH
          isHf: false
          isPro: false
          name: tsalvoch
          type: user
        html: '<p>I confirm that by using a HF version on tgwui with transformers
          and nf4, the issues with numbers no longer appear. That''s good to know.<br>Currently,
          I have not found a version of LLaMA-2 in GPTQ that does not suffer from
          this number problem. I am talking about numbers because the problem is glaring
          with them, but I imagine it is not limited to numbers. However, numbers
          are important... With a simple text translation, we lose the date, the amounts
          mentioned, etc. In short, in their current state, the GPTQ versions are
          almost unusable.<br>The GPTQ versions of LLaMA-1 did not have this problem.<br>So,
          for now, it''s bitsandbytes and we cross our fingers hoping that GPTQ will
          correct their issue.</p>

          '
        raw: 'I confirm that by using a HF version on tgwui with transformers and
          nf4, the issues with numbers no longer appear. That''s good to know.

          Currently, I have not found a version of LLaMA-2 in GPTQ that does not suffer
          from this number problem. I am talking about numbers because the problem
          is glaring with them, but I imagine it is not limited to numbers. However,
          numbers are important... With a simple text translation, we lose the date,
          the amounts mentioned, etc. In short, in their current state, the GPTQ versions
          are almost unusable.

          The GPTQ versions of LLaMA-1 did not have this problem.

          So, for now, it''s bitsandbytes and we cross our fingers hoping that GPTQ
          will correct their issue.'
        updatedAt: '2023-07-26T18:52:17.574Z'
      numEdits: 0
      reactions: []
    id: 64c16b61647baafd41c06cdf
    type: comment
  author: tsalvoch
  content: 'I confirm that by using a HF version on tgwui with transformers and nf4,
    the issues with numbers no longer appear. That''s good to know.

    Currently, I have not found a version of LLaMA-2 in GPTQ that does not suffer
    from this number problem. I am talking about numbers because the problem is glaring
    with them, but I imagine it is not limited to numbers. However, numbers are important...
    With a simple text translation, we lose the date, the amounts mentioned, etc.
    In short, in their current state, the GPTQ versions are almost unusable.

    The GPTQ versions of LLaMA-1 did not have this problem.

    So, for now, it''s bitsandbytes and we cross our fingers hoping that GPTQ will
    correct their issue.'
  created_at: 2023-07-26 17:52:17+00:00
  edited: false
  hidden: false
  id: 64c16b61647baafd41c06cdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-07-27T07:26:18.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9070432782173157
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>thanks for deeper reseaching, tsalvoch. Hope GPTQ will solve this
          issue.</p>

          '
        raw: thanks for deeper reseaching, tsalvoch. Hope GPTQ will solve this issue.
        updatedAt: '2023-07-27T07:26:18.428Z'
      numEdits: 0
      reactions: []
    id: 64c21c1a576884e0fac197fe
    type: comment
  author: AIReach
  content: thanks for deeper reseaching, tsalvoch. Hope GPTQ will solve this issue.
  created_at: 2023-07-27 06:26:18+00:00
  edited: false
  hidden: false
  id: 64c21c1a576884e0fac197fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-07-28T13:34:14.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7919968366622925
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Does this problem extend to the GGML quant version? Looking for
          the best resource/performance/quality quant version for this model.</p>

          '
        raw: Does this problem extend to the GGML quant version? Looking for the best
          resource/performance/quality quant version for this model.
        updatedAt: '2023-07-28T13:34:14.874Z'
      numEdits: 0
      reactions: []
    id: 64c3c3d6fafa16b51427149c
    type: comment
  author: Satya93
  content: Does this problem extend to the GGML quant version? Looking for the best
    resource/performance/quality quant version for this model.
  created_at: 2023-07-28 12:34:14+00:00
  edited: false
  hidden: false
  id: 64c3c3d6fafa16b51427149c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-07-28T17:22:48.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8269186615943909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>ggml has the best q4 quantization(only q4_2 not normal q4) while
          gptq and bitsandbytes are behind</p>

          '
        raw: ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
          and bitsandbytes are behind
        updatedAt: '2023-07-28T17:22:48.859Z'
      numEdits: 0
      reactions: []
    id: 64c3f96861cc71b9c08a1d6f
    type: comment
  author: YaTharThShaRma999
  content: ggml has the best q4 quantization(only q4_2 not normal q4) while gptq and
    bitsandbytes are behind
  created_at: 2023-07-28 16:22:48+00:00
  edited: false
  hidden: false
  id: 64c3f96861cc71b9c08a1d6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-07-28T17:34:11.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8891847729682922
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<blockquote>

          <p>ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
          and bitsandbytes are behind</p>

          </blockquote>

          <p>Thanks, that''s exactly the info I needed! Greatly appreciate theBloke
          providing these!</p>

          '
        raw: '> ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
          and bitsandbytes are behind


          Thanks, that''s exactly the info I needed! Greatly appreciate theBloke providing
          these!'
        updatedAt: '2023-07-28T17:34:11.449Z'
      numEdits: 0
      reactions: []
    id: 64c3fc1319565937fb0d278c
    type: comment
  author: Satya93
  content: '> ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
    and bitsandbytes are behind


    Thanks, that''s exactly the info I needed! Greatly appreciate theBloke providing
    these!'
  created_at: 2023-07-28 16:34:11+00:00
  edited: false
  hidden: false
  id: 64c3fc1319565937fb0d278c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-07-28T17:36:59.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9542718529701233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<blockquote>

          <p>ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
          and bitsandbytes are behind</p>

          </blockquote>

          <p>Wait, I don''t see q4_2 in his GGML list, did you mean ''q4_K_M''?</p>

          '
        raw: '> ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
          and bitsandbytes are behind


          Wait, I don''t see q4_2 in his GGML list, did you mean ''q4_K_M''?'
        updatedAt: '2023-07-28T17:36:59.285Z'
      numEdits: 0
      reactions: []
    id: 64c3fcbbe4e2212a4fe45011
    type: comment
  author: Satya93
  content: '> ggml has the best q4 quantization(only q4_2 not normal q4) while gptq
    and bitsandbytes are behind


    Wait, I don''t see q4_2 in his GGML list, did you mean ''q4_K_M''?'
  created_at: 2023-07-28 16:36:59+00:00
  edited: false
  hidden: false
  id: 64c3fcbbe4e2212a4fe45011
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
      fullname: Thierry SALVOCH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tsalvoch
      type: user
    createdAt: '2023-07-29T14:47:30.000Z'
    data:
      edited: false
      editors:
      - tsalvoch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676617383956909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
          fullname: Thierry SALVOCH
          isHf: false
          isPro: false
          name: tsalvoch
          type: user
        html: '<p>Yes, I confirm that the GGML 4-bit (5 and 6-bit as well) versions
          do not have this issue with numbers. They work quite well with LLaMA-2,
          although it is about 4 times slower than with GPTQ and ExLlama. It''s the
          best compromise for me for 13B models until I see what happens with GPTQ.
          For the 70B models, GGML does not seem to work yet with GPUs and requires
          using bitsandbytes instead.</p>

          '
        raw: Yes, I confirm that the GGML 4-bit (5 and 6-bit as well) versions do
          not have this issue with numbers. They work quite well with LLaMA-2, although
          it is about 4 times slower than with GPTQ and ExLlama. It's the best compromise
          for me for 13B models until I see what happens with GPTQ. For the 70B models,
          GGML does not seem to work yet with GPUs and requires using bitsandbytes
          instead.
        updatedAt: '2023-07-29T14:47:30.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AIReach
    id: 64c526825e5bc55a92011ffb
    type: comment
  author: tsalvoch
  content: Yes, I confirm that the GGML 4-bit (5 and 6-bit as well) versions do not
    have this issue with numbers. They work quite well with LLaMA-2, although it is
    about 4 times slower than with GPTQ and ExLlama. It's the best compromise for
    me for 13B models until I see what happens with GPTQ. For the 70B models, GGML
    does not seem to work yet with GPUs and requires using bitsandbytes instead.
  created_at: 2023-07-29 13:47:30+00:00
  edited: false
  hidden: false
  id: 64c526825e5bc55a92011ffb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/StableBeluga2-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: comparison with bitsandbytes nf4, hope to increase GPTQ accuracy
