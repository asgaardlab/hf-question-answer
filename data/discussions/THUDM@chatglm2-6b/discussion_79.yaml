!!python/object:huggingface_hub.community.DiscussionWithDetails
author: timpan
conflicting_files: null
created_at: 2023-08-28 02:21:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d029a41fb09a0faf148e0d92eba343a7.svg
      fullname: timpan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timpan
      type: user
    createdAt: '2023-08-28T03:21:55.000Z'
    data:
      edited: true
      editors:
      - timpan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9020056128501892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d029a41fb09a0faf148e0d92eba343a7.svg
          fullname: timpan
          isHf: false
          isPro: false
          name: timpan
          type: user
        html: '<p>Nice work, thanks!</p>

          <p>There is a template for prompt in Llama like<br>"""<br>&lt;s&gt;[INST]
          &lt;&gt;\n{your_system_message}\n&lt;&gt;\n\n{user_message_1} [/INST]<br>"""</p>

          <p>Is there any support like this in chatGLM2-6b? In some scenarios I would
          like the model take some part of the query as the highest priority, like
          role playing.</p>

          <p>Thx.</p>

          '
        raw: "Nice work, thanks!\n\nThere is a template for prompt in Llama like \n\
          \"\"\"\n\\<s>[INST] <<SYS>>\\n{your_system_message}\\n<</SYS>>\\n\\n{user_message_1}\
          \ [/INST]\n\"\"\"\n\nIs there any support like this in chatGLM2-6b? In some\
          \ scenarios I would like the model take some part of the query as the highest\
          \ priority, like role playing.\n\nThx."
        updatedAt: '2023-08-28T03:23:17.029Z'
      numEdits: 1
      reactions: []
    id: 64ec12d34f08b868064db2b7
    type: comment
  author: timpan
  content: "Nice work, thanks!\n\nThere is a template for prompt in Llama like \n\"\
    \"\"\n\\<s>[INST] <<SYS>>\\n{your_system_message}\\n<</SYS>>\\n\\n{user_message_1}\
    \ [/INST]\n\"\"\"\n\nIs there any support like this in chatGLM2-6b? In some scenarios\
    \ I would like the model take some part of the query as the highest priority,\
    \ like role playing.\n\nThx."
  created_at: 2023-08-28 02:21:55+00:00
  edited: true
  hidden: false
  id: 64ec12d34f08b868064db2b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 79
repo_id: THUDM/chatglm2-6b
repo_type: model
status: open
target_branch: null
title: Any support for adding system instructions similar as chatGPT?
