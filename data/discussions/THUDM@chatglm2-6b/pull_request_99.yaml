!!python/object:huggingface_hub.community.DiscussionWithDetails
author: katuni4ka
conflicting_files: []
created_at: 2023-11-08 13:20:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:20:37.000Z'
    data:
      edited: false
      editors:
      - katuni4ka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
          fullname: Ekaterina Aidova
          isHf: false
          isPro: false
          name: katuni4ka
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-11-08T13:20:37.467Z'
      numEdits: 0
      reactions: []
    id: 654b8b25df8059e03435d295
    type: comment
  author: katuni4ka
  content: ''
  created_at: 2023-11-08 13:20:37+00:00
  edited: false
  hidden: false
  id: 654b8b25df8059e03435d295
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:20:37.000Z'
    data:
      oid: c2354b92d1196803bf7d269b77101a1a4f82a3c7
      parents:
      - 7fabe56db91e085c9c027f56f1c654d137bdba40
      subject: 'fix tokenizer initialization issue AttributeError: can''t set attribute'
    id: 654b8b250000000000000000
    type: commit
  author: katuni4ka
  created_at: 2023-11-08 13:20:37+00:00
  id: 654b8b250000000000000000
  oid: c2354b92d1196803bf7d269b77101a1a4f82a3c7
  summary: 'fix tokenizer initialization issue AttributeError: can''t set attribute'
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:26:11.000Z'
    data:
      status: closed
    id: 654b8c73c5fd938286290a6e
    type: status-change
  author: katuni4ka
  created_at: 2023-11-08 13:26:11+00:00
  id: 654b8c73c5fd938286290a6e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:39:57.000Z'
    data:
      status: open
    id: 654b8fad48b4741202512bc9
    type: status-change
  author: katuni4ka
  created_at: 2023-11-08 13:39:57+00:00
  id: 654b8fad48b4741202512bc9
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:40:54.000Z'
    data:
      oid: 3418503137b48574524fef0386b342e8d9c0df8e
      parents:
      - c2354b92d1196803bf7d269b77101a1a4f82a3c7
      subject: Update tokenization_chatglm.py
    id: 654b8fe60000000000000000
    type: commit
  author: katuni4ka
  created_at: 2023-11-08 13:40:54+00:00
  id: 654b8fe60000000000000000
  oid: 3418503137b48574524fef0386b342e8d9c0df8e
  summary: Update tokenization_chatglm.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T13:46:56.000Z'
    data:
      oid: 0fc7408f5bd857866791915df275642805734e70
      parents:
      - 3418503137b48574524fef0386b342e8d9c0df8e
      subject: Update tokenization_chatglm.py
    id: 654b91500000000000000000
    type: commit
  author: katuni4ka
  created_at: 2023-11-08 13:46:56+00:00
  id: 654b91500000000000000000
  oid: 0fc7408f5bd857866791915df275642805734e70
  summary: Update tokenization_chatglm.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-08T15:13:50.000Z'
    data:
      oid: 1470032c4deceda0651b366101cf765bb0320853
      parents:
      - 0fc7408f5bd857866791915df275642805734e70
      subject: Update tokenization_chatglm.py
    id: 654ba5ae0000000000000000
    type: commit
  author: katuni4ka
  created_at: 2023-11-08 15:13:50+00:00
  id: 654ba5ae0000000000000000
  oid: 1470032c4deceda0651b366101cf765bb0320853
  summary: Update tokenization_chatglm.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35414f851a4487c88358f30a3de222db.svg
      fullname: Feng Zehui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fengzehui
      type: user
    createdAt: '2023-11-15T10:07:44.000Z'
    data:
      edited: false
      editors:
      - Fengzehui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31614595651626587
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35414f851a4487c88358f30a3de222db.svg
          fullname: Feng Zehui
          isHf: false
          isPro: false
          name: Fengzehui
          type: user
        html: '<p>After finishing the update above,I get this error. Can you help
          me ? Thank you !</p>

          <p>[INFO|trainer.py:1721] 2023-11-15 17:52:14,029 &gt;&gt;   Number of trainable
          parameters = 14,680,064<br>  0%|                                                                                         |
          0/3000 [00:00&lt;?, ?it/s]11/15/2023 17:52:15 - WARNING - transformers_modules.chatglm-6b.modeling_chatglm
          - <code>use_cache=True</code> is incompatible with gradient checkpointing.
          Setting <code>use_cache=False</code>...<br>Traceback (most recent call last):<br>  File
          "main.py", line 470, in <br>    main()<br>  File "main.py", line 401, in
          main<br>    train_result = trainer.train(resume_from_checkpoint=checkpoint)<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\transformers\trainer.py",
          line 1553, in train<br>    return inner_training_loop(<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\transformers\trainer.py",
          line 1835, in _inner_training_loop<br>    tr_loss_step = self.training_step(model,
          inputs)<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\transformers\trainer.py",
          line 2679, in training_step<br>    loss = self.compute_loss(model, inputs)<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\transformers\trainer.py",
          line 2704, in compute_loss<br>    outputs = model(**inputs)<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\peft\peft_model.py", line
          977, in forward<br>    return self.base_model(<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\peft\tuners\tuners_utils.py",
          line 106, in forward<br>    return self.model.forward(*args, **kwargs)<br>  File
          "C:\Users\PC/.cache\huggingface\modules\transformers_modules\chatglm-6b\modeling_chatglm.py",
          line 1190, in forward<br>    transformer_outputs = self.transformer(<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\PC/.cache\huggingface\modules\transformers_modules\chatglm-6b\modeling_chatglm.py",
          line 985, in forward<br>    layer_ret = torch.utils.checkpoint.checkpoint(<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\utils\checkpoint.py",
          line 249, in checkpoint<br>    return CheckpointFunction.apply(function,
          preserve, *args)<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\autograd\function.py",
          line 506, in apply<br>    return super().apply(*args, **kwargs)  # type:
          ignore[misc]<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\utils\checkpoint.py",
          line 107, in forward<br>    outputs = run_function(*args)<br>  File "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\PC/.cache\huggingface\modules\transformers_modules\chatglm-6b\modeling_chatglm.py",
          line 627, in forward<br>    attention_outputs = self.attention(<br>  File
          "C:\Users\PC.conda\envs\torch\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\PC/.cache\huggingface\modules\transformers_modules\chatglm-6b\modeling_chatglm.py",
          line 461, in forward<br>    position_ids = position_ids[:, 0, :].transpose(0,
          1).contiguous()<br>IndexError: too many indices for tensor of dimension
          2</p>

          '
        raw: "After finishing the update above,I get this error. Can you help me ?\
          \ Thank you !\n\n[INFO|trainer.py:1721] 2023-11-15 17:52:14,029 >>   Number\
          \ of trainable parameters = 14,680,064\n  0%|                          \
          \                                                               | 0/3000\
          \ [00:00<?, ?it/s]11/15/2023 17:52:15 - WARNING - transformers_modules.chatglm-6b.modeling_chatglm\
          \ - `use_cache=True` is incompatible with gradient checkpointing. Setting\
          \ `use_cache=False`...\nTraceback (most recent call last):\n  File \"main.py\"\
          , line 470, in <module>\n    main()\n  File \"main.py\", line 401, in main\n\
          \    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n \
          \ File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\\
          trainer.py\", line 1553, in train\n    return inner_training_loop(\n  File\
          \ \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\\
          trainer.py\", line 1835, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
          transformers\\trainer.py\", line 2679, in training_step\n    loss = self.compute_loss(model,\
          \ inputs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
          transformers\\trainer.py\", line 2704, in compute_loss\n    outputs = model(**inputs)\n\
          \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
          peft\\peft_model.py\", line 977, in forward\n    return self.base_model(\n\
          \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
          peft\\tuners\\tuners_utils.py\", line 106, in forward\n    return self.model.forward(*args,\
          \ **kwargs)\n  File \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\\
          chatglm-6b\\modeling_chatglm.py\", line 1190, in forward\n    transformer_outputs\
          \ = self.transformer(\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\PC/.cache\\\
          huggingface\\modules\\transformers_modules\\chatglm-6b\\modeling_chatglm.py\"\
          , line 985, in forward\n    layer_ret = torch.utils.checkpoint.checkpoint(\n\
          \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\\
          utils\\checkpoint.py\", line 249, in checkpoint\n    return CheckpointFunction.apply(function,\
          \ preserve, *args)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
          torch\\autograd\\function.py\", line 506, in apply\n    return super().apply(*args,\
          \ **kwargs)  # type: ignore[misc]\n  File \"C:\\Users\\PC\\.conda\\envs\\\
          torch\\lib\\site-packages\\torch\\utils\\checkpoint.py\", line 107, in forward\n\
          \    outputs = run_function(*args)\n  File \"C:\\Users\\PC\\.conda\\envs\\\
          torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\\
          PC/.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b\\modeling_chatglm.py\"\
          , line 627, in forward\n    attention_outputs = self.attention(\n  File\
          \ \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\\
          chatglm-6b\\modeling_chatglm.py\", line 461, in forward\n    position_ids\
          \ = position_ids[:, 0, :].transpose(0, 1).contiguous()\nIndexError: too\
          \ many indices for tensor of dimension 2"
        updatedAt: '2023-11-15T10:07:44.777Z'
      numEdits: 0
      reactions: []
    id: 655498708344eaaaed22298d
    type: comment
  author: Fengzehui
  content: "After finishing the update above,I get this error. Can you help me ? Thank\
    \ you !\n\n[INFO|trainer.py:1721] 2023-11-15 17:52:14,029 >>   Number of trainable\
    \ parameters = 14,680,064\n  0%|                                             \
    \                                            | 0/3000 [00:00<?, ?it/s]11/15/2023\
    \ 17:52:15 - WARNING - transformers_modules.chatglm-6b.modeling_chatglm - `use_cache=True`\
    \ is incompatible with gradient checkpointing. Setting `use_cache=False`...\n\
    Traceback (most recent call last):\n  File \"main.py\", line 470, in <module>\n\
    \    main()\n  File \"main.py\", line 401, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\
    \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\\
    trainer.py\", line 1553, in train\n    return inner_training_loop(\n  File \"\
    C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\"\
    , line 1835, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
    \ inputs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\\
    trainer.py\", line 2679, in training_step\n    loss = self.compute_loss(model,\
    \ inputs)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\\
    trainer.py\", line 2704, in compute_loss\n    outputs = model(**inputs)\n  File\
    \ \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\peft\\peft_model.py\"\
    , line 977, in forward\n    return self.base_model(\n  File \"C:\\Users\\PC\\\
    .conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\\
    Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\peft\\tuners\\tuners_utils.py\"\
    , line 106, in forward\n    return self.model.forward(*args, **kwargs)\n  File\
    \ \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b\\\
    modeling_chatglm.py\", line 1190, in forward\n    transformer_outputs = self.transformer(\n\
    \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\\
    chatglm-6b\\modeling_chatglm.py\", line 985, in forward\n    layer_ret = torch.utils.checkpoint.checkpoint(\n\
    \  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\\
    checkpoint.py\", line 249, in checkpoint\n    return CheckpointFunction.apply(function,\
    \ preserve, *args)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
    torch\\autograd\\function.py\", line 506, in apply\n    return super().apply(*args,\
    \ **kwargs)  # type: ignore[misc]\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\\
    lib\\site-packages\\torch\\utils\\checkpoint.py\", line 107, in forward\n    outputs\
    \ = run_function(*args)\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\\
    chatglm-6b\\modeling_chatglm.py\", line 627, in forward\n    attention_outputs\
    \ = self.attention(\n  File \"C:\\Users\\PC\\.conda\\envs\\torch\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Users\\PC/.cache\\huggingface\\modules\\transformers_modules\\\
    chatglm-6b\\modeling_chatglm.py\", line 461, in forward\n    position_ids = position_ids[:,\
    \ 0, :].transpose(0, 1).contiguous()\nIndexError: too many indices for tensor\
    \ of dimension 2"
  created_at: 2023-11-15 10:07:44+00:00
  edited: false
  hidden: false
  id: 655498708344eaaaed22298d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38b0514f5d1eb062f683520da27b8e33.svg
      fullname: Peter Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iampeterchen
      type: user
    createdAt: '2023-11-23T08:54:28.000Z'
    data:
      edited: false
      editors:
      - iampeterchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20601582527160645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38b0514f5d1eb062f683520da27b8e33.svg
          fullname: Peter Chen
          isHf: false
          isPro: false
          name: iampeterchen
          type: user
        html: '<p>For me,  check the existing and pop will avoid the error in some
          case.<br>    chatglm2-6b$ diff -ur tokenization_chatglm.py.backup tokenization_chatglm.py<br>    ---
          tokenization_chatglm.py.backup      2023-11-12 21:27:40.798629427 +0800<br>    +++
          tokenization_chatglm.py     2023-11-13 18:05:34.877616175 +0800<br>    @@
          -70,6 +70,12 @@<br>             self.vocab_file = vocab_file<br>             self.tokenizer
          = SPTokenizer(vocab_file)<br>    +        if "eos_token" in kwargs:<br>    +            kwargs.pop("eos_token")<br>    +        if
          "pad_token" in kwargs:<br>    +            kwargs.pop("pad_token")<br>    +        if
          "unk_token" in kwargs:<br>    +            kwargs.pop("unk_token")<br>             self.special_tokens
          = {<br>                 "": self.tokenizer.bos_id,<br>             "": self.tokenizer.eos_id,</p>

          '
        raw: "For me,  check the existing and pop will avoid the error in some case.\n\
          \tchatglm2-6b$ diff -ur tokenization_chatglm.py.backup tokenization_chatglm.py\n\
          \t--- tokenization_chatglm.py.backup      2023-11-12 21:27:40.798629427\
          \ +0800\n\t+++ tokenization_chatglm.py     2023-11-13 18:05:34.877616175\
          \ +0800\n\t@@ -70,6 +70,12 @@ \n\t         self.vocab_file = vocab_file\n\
          \t         self.tokenizer = SPTokenizer(vocab_file)\n\t+        if \"eos_token\"\
          \ in kwargs:\n\t+            kwargs.pop(\"eos_token\")\n\t+        if \"\
          pad_token\" in kwargs:\n\t+            kwargs.pop(\"pad_token\")\n\t+  \
          \      if \"unk_token\" in kwargs:\n\t+            kwargs.pop(\"unk_token\"\
          )\n\t         self.special_tokens = {\n\t             \"<bos>\": self.tokenizer.bos_id,\n\
          \             \"<eos>\": self.tokenizer.eos_id,"
        updatedAt: '2023-11-23T08:54:28.066Z'
      numEdits: 0
      reactions: []
    id: 655f134456e5ceaf053e959d
    type: comment
  author: iampeterchen
  content: "For me,  check the existing and pop will avoid the error in some case.\n\
    \tchatglm2-6b$ diff -ur tokenization_chatglm.py.backup tokenization_chatglm.py\n\
    \t--- tokenization_chatglm.py.backup      2023-11-12 21:27:40.798629427 +0800\n\
    \t+++ tokenization_chatglm.py     2023-11-13 18:05:34.877616175 +0800\n\t@@ -70,6\
    \ +70,12 @@ \n\t         self.vocab_file = vocab_file\n\t         self.tokenizer\
    \ = SPTokenizer(vocab_file)\n\t+        if \"eos_token\" in kwargs:\n\t+     \
    \       kwargs.pop(\"eos_token\")\n\t+        if \"pad_token\" in kwargs:\n\t\
    +            kwargs.pop(\"pad_token\")\n\t+        if \"unk_token\" in kwargs:\n\
    \t+            kwargs.pop(\"unk_token\")\n\t         self.special_tokens = {\n\
    \t             \"<bos>\": self.tokenizer.bos_id,\n             \"<eos>\": self.tokenizer.eos_id,"
  created_at: 2023-11-23 08:54:28+00:00
  edited: false
  hidden: false
  id: 655f134456e5ceaf053e959d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
      fullname: Ekaterina Aidova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: katuni4ka
      type: user
    createdAt: '2023-11-24T05:35:58.000Z'
    data:
      edited: true
      editors:
      - katuni4ka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8076176643371582
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650566593816-noauth.jpeg?w=200&h=200&f=face
          fullname: Ekaterina Aidova
          isHf: false
          isPro: false
          name: katuni4ka
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;iampeterchen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/iampeterchen\"\
          >@<span class=\"underline\">iampeterchen</span></a></span>\n\n\t</span></span>\
          \ dict.pop(key, default_value) command that used in request is equivalent\
          \ of what you write, just shorter</p>\n"
        raw: '@iampeterchen dict.pop(key, default_value) command that used in request
          is equivalent of what you write, just shorter'
        updatedAt: '2023-11-24T05:39:32.041Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - iampeterchen
    id: 6560363ea4c9a1dd943bdf46
    type: comment
  author: katuni4ka
  content: '@iampeterchen dict.pop(key, default_value) command that used in request
    is equivalent of what you write, just shorter'
  created_at: 2023-11-24 05:35:58+00:00
  edited: true
  hidden: false
  id: 6560363ea4c9a1dd943bdf46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38b0514f5d1eb062f683520da27b8e33.svg
      fullname: Peter Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iampeterchen
      type: user
    createdAt: '2023-11-24T08:57:41.000Z'
    data:
      edited: false
      editors:
      - iampeterchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8316680788993835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38b0514f5d1eb062f683520da27b8e33.svg
          fullname: Peter Chen
          isHf: false
          isPro: false
          name: iampeterchen
          type: user
        html: '<p>I wasn''t aware of the difference between kwargs.pop("pad_token")
          and kwargs.pop("pad_token", None) before. Thank you for this shorter one.</p>

          '
        raw: I wasn't aware of the difference between kwargs.pop("pad_token") and
          kwargs.pop("pad_token", None) before. Thank you for this shorter one.
        updatedAt: '2023-11-24T08:57:41.576Z'
      numEdits: 0
      reactions: []
    id: 6560658522ce47e5fa63622f
    type: comment
  author: iampeterchen
  content: I wasn't aware of the difference between kwargs.pop("pad_token") and kwargs.pop("pad_token",
    None) before. Thank you for this shorter one.
  created_at: 2023-11-24 08:57:41+00:00
  edited: false
  hidden: false
  id: 6560658522ce47e5fa63622f
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 99
repo_id: THUDM/chatglm2-6b
repo_type: model
status: open
target_branch: refs/heads/main
title: 'fix tokenizer initialization issue AttributeError: can''t set attribute'
