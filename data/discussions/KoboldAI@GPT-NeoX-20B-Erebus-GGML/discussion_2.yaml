!!python/object:huggingface_hub.community.DiscussionWithDetails
author: teneriffa
conflicting_files: null
created_at: 2023-06-10 13:12:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-06-10T14:12:09.000Z'
    data:
      edited: true
      editors:
      - teneriffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5464072227478027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p>I have tried to use models in this repo, but ggml files in this\
          \ repo always returns '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^'.\
          \ There are f16 and q4_1 versions, and both of them return same wrong responses.</p>\n\
          <p>Is there anyone who can run this weights well? My environment is macOS\
          \ Ventura 13.4, Python 3.10.12 and most recent development version of koboldcpp\
          \ (branch concedo_experimental, commit hash b9f74db89e1417be171363244aaa6848706266c7).</p>\n\
          <p>Thanks.</p>\n<pre><code>% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\n\
          Welcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file not\
          \ found. Non-BLAS library will be used.\nInitializing dynamic library: koboldcpp.so\n\
          ==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\
          \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
          \ FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \ngpt_neox_v2_model_load: loading\
          \ model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin'\
          \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
          \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
          \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
          \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load:\
          \ ftype   = 1\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load:\
          \ ggml ctx size = 49770.77 MB\ngpt_neox_v2_model_load: memory_size =  2112.00\
          \ MB, n_mem = 90112\ngpt_neox_v2_model_load: ..................................................................\
          \ done\ngpt_neox_v2_model_load: model size = 39211.45 MB / num tensors =\
          \ 532\nLoad Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold\
          \ HTTP Server on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\
          \nInput: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"\
          rep_pen\": 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"\
          top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\"\
          : 0.7, \"sampler_order\": [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character:\
          \ Emily; species: Human; age: 24; gender: female; physical appearance: cute,\
          \ attractive; personality: cheerful, upbeat, friendly; likes: chatting;\
          \ description: Emily has been your childhood friend for many years. She\
          \ is outgoing, adventurous, and enjoys many interesting hobbies. She has\
          \ had a secret crush on you for a long time.]\\n[The following is a chat\
          \ message log between Emily and you.]\\n\\nEmily: Heyo! You there? I think\
          \ my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
          \ you :)\\n\\n\\nYou: The sun rises from west.\\nEmily:\", \"quiet\": true,\
          \ \"stop_sequence\": [\"You:\"]}\n\nProcessing Prompt [BLAS] (136 / 136\
          \ tokens)\nGenerating (80 / 80 tokens)\nTime Taken - Processing:11.7s (86ms/T),\
          \ Generation:32.7s (409ms/T), Total:44.5s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          127.0.0.1 - - [10/Jun/2023 23:06:24] \"POST /api/v1/generate/ HTTP/1.1\"\
          \ 200 -\n</code></pre>\n<pre><code>% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
          \  \nWelcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file\
          \ not found. Non-BLAS library will be used.\nInitializing dynamic library:\
          \ koboldcpp.so\n==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
          \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
          \ FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \ngpt_neox_v2_model_load: loading\
          \ model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin'\
          \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
          \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
          \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
          \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load:\
          \ ftype   = 3\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load:\
          \ ggml ctx size = 25272.02 MB\ngpt_neox_v2_model_load: memory_size =  2112.00\
          \ MB, n_mem = 90112\ngpt_neox_v2_model_load: ..................................................................\
          \ done\ngpt_neox_v2_model_load: model size = 14712.70 MB / num tensors =\
          \ 532\nLoad Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold\
          \ HTTP Server on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\
          \nInput: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"\
          rep_pen\": 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"\
          top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\"\
          : 0.7, \"sampler_order\": [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character:\
          \ Emily; species: Human; age: 24; gender: female; physical appearance: cute,\
          \ attractive; personality: cheerful, upbeat, friendly; likes: chatting;\
          \ description: Emily has been your childhood friend for many years. She\
          \ is outgoing, adventurous, and enjoys many interesting hobbies. She has\
          \ had a secret crush on you for a long time.]\\n[The following is a chat\
          \ message log between Emily and you.]\\n\\nEmily: Heyo! You there? I think\
          \ my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
          \ you :)\\n\\n\\nYou: What's up today?\\nEmily:\", \"quiet\": true, \"stop_sequence\"\
          : [\"You:\"]}\n\nProcessing Prompt [BLAS] (135 / 135 tokens)\nGenerating\
          \ (80 / 80 tokens)\nTime Taken - Processing:9.7s (72ms/T), Generation:18.7s\
          \ (234ms/T), Total:28.4s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          127.0.0.1 - - [10/Jun/2023 23:02:23] \"POST /api/v1/generate/ HTTP/1.1\"\
          \ 200 -\n</code></pre>\n"
        raw: "I have tried to use models in this repo, but ggml files in this repo\
          \ always returns '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^'.\
          \ There are f16 and q4_1 versions, and both of them return same wrong responses.\n\
          \nIs there anyone who can run this weights well? My environment is macOS\
          \ Ventura 13.4, Python 3.10.12 and most recent development version of koboldcpp\
          \ (branch concedo_experimental, commit hash b9f74db89e1417be171363244aaa6848706266c7).\n\
          \nThanks.\n\n```\n% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\n\
          Welcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file not\
          \ found. Non-BLAS library will be used.\nInitializing dynamic library: koboldcpp.so\n\
          ==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\
          \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
          \ FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \ngpt_neox_v2_model_load: loading\
          \ model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin'\
          \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
          \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
          \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
          \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load:\
          \ ftype   = 1\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load:\
          \ ggml ctx size = 49770.77 MB\ngpt_neox_v2_model_load: memory_size =  2112.00\
          \ MB, n_mem = 90112\ngpt_neox_v2_model_load: ..................................................................\
          \ done\ngpt_neox_v2_model_load: model size = 39211.45 MB / num tensors =\
          \ 532\nLoad Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold\
          \ HTTP Server on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\
          \nInput: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"\
          rep_pen\": 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"\
          top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\"\
          : 0.7, \"sampler_order\": [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character:\
          \ Emily; species: Human; age: 24; gender: female; physical appearance: cute,\
          \ attractive; personality: cheerful, upbeat, friendly; likes: chatting;\
          \ description: Emily has been your childhood friend for many years. She\
          \ is outgoing, adventurous, and enjoys many interesting hobbies. She has\
          \ had a secret crush on you for a long time.]\\n[The following is a chat\
          \ message log between Emily and you.]\\n\\nEmily: Heyo! You there? I think\
          \ my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
          \ you :)\\n\\n\\nYou: The sun rises from west.\\nEmily:\", \"quiet\": true,\
          \ \"stop_sequence\": [\"You:\"]}\n\nProcessing Prompt [BLAS] (136 / 136\
          \ tokens)\nGenerating (80 / 80 tokens)\nTime Taken - Processing:11.7s (86ms/T),\
          \ Generation:32.7s (409ms/T), Total:44.5s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          127.0.0.1 - - [10/Jun/2023 23:06:24] \"POST /api/v1/generate/ HTTP/1.1\"\
          \ 200 -\n```\n\n```\n% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
          \  \nWelcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file\
          \ not found. Non-BLAS library will be used.\nInitializing dynamic library:\
          \ koboldcpp.so\n==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
          \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
          \ FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \ngpt_neox_v2_model_load: loading\
          \ model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin'\
          \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
          \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
          \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
          \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load:\
          \ ftype   = 3\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load:\
          \ ggml ctx size = 25272.02 MB\ngpt_neox_v2_model_load: memory_size =  2112.00\
          \ MB, n_mem = 90112\ngpt_neox_v2_model_load: ..................................................................\
          \ done\ngpt_neox_v2_model_load: model size = 14712.70 MB / num tensors =\
          \ 532\nLoad Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold\
          \ HTTP Server on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\
          \nInput: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"\
          rep_pen\": 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"\
          top_a\": 0, \"typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\"\
          : 0.7, \"sampler_order\": [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character:\
          \ Emily; species: Human; age: 24; gender: female; physical appearance: cute,\
          \ attractive; personality: cheerful, upbeat, friendly; likes: chatting;\
          \ description: Emily has been your childhood friend for many years. She\
          \ is outgoing, adventurous, and enjoys many interesting hobbies. She has\
          \ had a secret crush on you for a long time.]\\n[The following is a chat\
          \ message log between Emily and you.]\\n\\nEmily: Heyo! You there? I think\
          \ my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
          \ you :)\\n\\n\\nYou: What's up today?\\nEmily:\", \"quiet\": true, \"stop_sequence\"\
          : [\"You:\"]}\n\nProcessing Prompt [BLAS] (135 / 135 tokens)\nGenerating\
          \ (80 / 80 tokens)\nTime Taken - Processing:9.7s (72ms/T), Generation:18.7s\
          \ (234ms/T), Total:28.4s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          127.0.0.1 - - [10/Jun/2023 23:02:23] \"POST /api/v1/generate/ HTTP/1.1\"\
          \ 200 -\n\n```"
        updatedAt: '2023-06-11T12:20:55.545Z'
      numEdits: 1
      reactions: []
    id: 648484b9dd59b053f906922e
    type: comment
  author: teneriffa
  content: "I have tried to use models in this repo, but ggml files in this repo always\
    \ returns '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^'.\
    \ There are f16 and q4_1 versions, and both of them return same wrong responses.\n\
    \nIs there anyone who can run this weights well? My environment is macOS Ventura\
    \ 13.4, Python 3.10.12 and most recent development version of koboldcpp (branch\
    \ concedo_experimental, commit hash b9f74db89e1417be171363244aaa6848706266c7).\n\
    \nThanks.\n\n```\n% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\n\
    Welcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file not found.\
    \ Non-BLAS library will be used.\nInitializing dynamic library: koboldcpp.so\n\
    ==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin\
    \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified as GPT-NEO-X\
    \ model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX = 0 | AVX2 =\
    \ 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA\
    \ = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0\
    \ | \ngpt_neox_v2_model_load: loading model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-Neox-20B-Erebus-f16.bin'\
    \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
    \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
    \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
    \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load: ftype\
    \   = 1\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load: ggml ctx\
    \ size = 49770.77 MB\ngpt_neox_v2_model_load: memory_size =  2112.00 MB, n_mem\
    \ = 90112\ngpt_neox_v2_model_load: ..................................................................\
    \ done\ngpt_neox_v2_model_load: model size = 39211.45 MB / num tensors = 532\n\
    Load Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold HTTP Server\
    \ on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\n\
    Input: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"rep_pen\"\
    : 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"top_a\": 0, \"\
    typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\": 0.7, \"sampler_order\"\
    : [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character: Emily; species: Human; age:\
    \ 24; gender: female; physical appearance: cute, attractive; personality: cheerful,\
    \ upbeat, friendly; likes: chatting; description: Emily has been your childhood\
    \ friend for many years. She is outgoing, adventurous, and enjoys many interesting\
    \ hobbies. She has had a secret crush on you for a long time.]\\n[The following\
    \ is a chat message log between Emily and you.]\\n\\nEmily: Heyo! You there? I\
    \ think my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
    \ you :)\\n\\n\\nYou: The sun rises from west.\\nEmily:\", \"quiet\": true, \"\
    stop_sequence\": [\"You:\"]}\n\nProcessing Prompt [BLAS] (136 / 136 tokens)\n\
    Generating (80 / 80 tokens)\nTime Taken - Processing:11.7s (86ms/T), Generation:32.7s\
    \ (409ms/T), Total:44.5s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    127.0.0.1 - - [10/Jun/2023 23:06:24] \"POST /api/v1/generate/ HTTP/1.1\" 200 -\n\
    ```\n\n```\n% python koboldcpp.py --noblas ../models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
    \  \nWelcome to KoboldCpp - Version 1.30\nWarning: OpenBLAS library file not found.\
    \ Non-BLAS library will be used.\nInitializing dynamic library: koboldcpp.so\n\
    ==========\nLoading model: /Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin\
    \ \n[Threads: 4, BlasThreads: 4, SmartContext: False]\n\n---\nIdentified as GPT-NEO-X\
    \ model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX = 0 | AVX2 =\
    \ 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA\
    \ = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0\
    \ | \ngpt_neox_v2_model_load: loading model from '/Volumes/cuttingedge/large_lang_models/models/KoboldAI_GPT-NeoX-20B-ggml/GPT-NeoX-20B-Erebus-Q4_1.bin'\
    \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
    \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
    \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
    \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load: ftype\
    \   = 3\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load: ggml ctx\
    \ size = 25272.02 MB\ngpt_neox_v2_model_load: memory_size =  2112.00 MB, n_mem\
    \ = 90112\ngpt_neox_v2_model_load: ..................................................................\
    \ done\ngpt_neox_v2_model_load: model size = 14712.70 MB / num tensors = 532\n\
    Load Model OK: True\nEmbedded Kobold Lite loaded.\nStarting Kobold HTTP Server\
    \ on port 5001\nPlease connect to custom endpoint at http://localhost:5001\n\n\
    Input: {\"n\": 1, \"max_context_length\": 1024, \"max_length\": 80, \"rep_pen\"\
    : 1.08, \"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 0, \"top_a\": 0, \"\
    typical\": 1, \"tfs\": 1, \"rep_pen_range\": 256, \"rep_pen_slope\": 0.7, \"sampler_order\"\
    : [6, 0, 1, 2, 3, 4, 5], \"prompt\": \"[Character: Emily; species: Human; age:\
    \ 24; gender: female; physical appearance: cute, attractive; personality: cheerful,\
    \ upbeat, friendly; likes: chatting; description: Emily has been your childhood\
    \ friend for many years. She is outgoing, adventurous, and enjoys many interesting\
    \ hobbies. She has had a secret crush on you for a long time.]\\n[The following\
    \ is a chat message log between Emily and you.]\\n\\nEmily: Heyo! You there? I\
    \ think my internet is kinda slow today.\\nYou: Hello Emily. Good to hear from\
    \ you :)\\n\\n\\nYou: What's up today?\\nEmily:\", \"quiet\": true, \"stop_sequence\"\
    : [\"You:\"]}\n\nProcessing Prompt [BLAS] (135 / 135 tokens)\nGenerating (80 /\
    \ 80 tokens)\nTime Taken - Processing:9.7s (72ms/T), Generation:18.7s (234ms/T),\
    \ Total:28.4s\nOutput: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    127.0.0.1 - - [10/Jun/2023 23:02:23] \"POST /api/v1/generate/ HTTP/1.1\" 200 -\n\
    \n```"
  created_at: 2023-06-10 13:12:09+00:00
  edited: true
  hidden: false
  id: 648484b9dd59b053f906922e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c411a94bcba6078a51a86f5e846168d1.svg
      fullname: Gabriel Virrey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ppgabe
      type: user
    createdAt: '2023-07-15T08:11:37.000Z'
    data:
      edited: false
      editors:
      - ppgabe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44860634207725525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c411a94bcba6078a51a86f5e846168d1.svg
          fullname: Gabriel Virrey
          isHf: false
          isPro: false
          name: ppgabe
          type: user
        html: '<p>How were you able to run this model? I''m trying to run it on KoboldCPP,
          v1.35, both with --noblas and --useclblas. I''m only getting the following:</p>

          <p>Loading model: /home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin<br>[Threads:
          5, BlasThreads: 5, SmartContext: False]</p>

          <hr>

          <p>Identified as GPT-NEO-X model: (ver 401)<br>Attempting to Load...</p>

          <hr>

          <p>System Info: AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI
          = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |<br>gpt_neox_v2_model_load: loading
          model from ''/home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin''
          - please wait ...<br>gpt_neox_v2_model_load: n_vocab = 50432<br>gpt_neox_v2_model_load:
          n_ctx   = 2048<br>gpt_neox_v2_model_load: n_embd  = 6144<br>gpt_neox_v2_model_load:
          n_head  = 64<br>gpt_neox_v2_model_load: n_layer = 44<br>gpt_neox_v2_model_load:
          n_rot   = 24<br>gpt_neox_v2_model_load: par_res = 1<br>gpt_neox_v2_model_load:
          ftype   = 3<br>gpt_neox_v2_model_load: qntvr   = 0<br>gpt_neox_v2_model_load:
          ggml ctx size = 25272.02 MB<br>GGML_V2_ASSERT: otherarch/ggml_v2.c:3959:
          ctx-&gt;mem_buffer != NULL<br>Aborted (core dumped)</p>

          '
        raw: "How were you able to run this model? I'm trying to run it on KoboldCPP,\
          \ v1.35, both with --noblas and --useclblas. I'm only getting the following:\n\
          \n\nLoading model: /home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin\
          \ \n[Threads: 5, BlasThreads: 5, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 |\
          \ FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \ngpt_neox_v2_model_load: loading\
          \ model from '/home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin'\
          \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
          \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
          \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
          \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load:\
          \ ftype   = 3\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load:\
          \ ggml ctx size = 25272.02 MB\nGGML_V2_ASSERT: otherarch/ggml_v2.c:3959:\
          \ ctx->mem_buffer != NULL\nAborted (core dumped)"
        updatedAt: '2023-07-15T08:11:37.044Z'
      numEdits: 0
      reactions: []
    id: 64b254b94dd3e24895edfcee
    type: comment
  author: ppgabe
  content: "How were you able to run this model? I'm trying to run it on KoboldCPP,\
    \ v1.35, both with --noblas and --useclblas. I'm only getting the following:\n\
    \n\nLoading model: /home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin\
    \ \n[Threads: 5, BlasThreads: 5, SmartContext: False]\n\n---\nIdentified as GPT-NEO-X\
    \ model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX = 1 | AVX2 =\
    \ 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA\
    \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0\
    \ | \ngpt_neox_v2_model_load: loading model from '/home/verbosepanda/koboldcpp/models/GPT-NeoX-20B-Erebus-Q4_1.bin'\
    \ - please wait ...\ngpt_neox_v2_model_load: n_vocab = 50432\ngpt_neox_v2_model_load:\
    \ n_ctx   = 2048\ngpt_neox_v2_model_load: n_embd  = 6144\ngpt_neox_v2_model_load:\
    \ n_head  = 64\ngpt_neox_v2_model_load: n_layer = 44\ngpt_neox_v2_model_load:\
    \ n_rot   = 24\ngpt_neox_v2_model_load: par_res = 1\ngpt_neox_v2_model_load: ftype\
    \   = 3\ngpt_neox_v2_model_load: qntvr   = 0\ngpt_neox_v2_model_load: ggml ctx\
    \ size = 25272.02 MB\nGGML_V2_ASSERT: otherarch/ggml_v2.c:3959: ctx->mem_buffer\
    \ != NULL\nAborted (core dumped)"
  created_at: 2023-07-15 07:11:37+00:00
  edited: false
  hidden: false
  id: 64b254b94dd3e24895edfcee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-07-15T14:35:22.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496334791183472
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Just tested the model and had no issues with the 1.35.H3 release
          you can find here : <a rel="nofollow" href="https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe">https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe</a><br>Because
          the main developer is out of town these week the H releases are my own bugfixed
          uploads until he is back and can upload an official 1.35.1.</p>

          '
        raw: 'Just tested the model and had no issues with the 1.35.H3 release you
          can find here : https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe

          Because the main developer is out of town these week the H releases are
          my own bugfixed uploads until he is back and can upload an official 1.35.1.'
        updatedAt: '2023-07-15T14:35:22.730Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ppgabe
    id: 64b2aeaa9a88b423da555e86
    type: comment
  author: Henk717
  content: 'Just tested the model and had no issues with the 1.35.H3 release you can
    find here : https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe

    Because the main developer is out of town these week the H releases are my own
    bugfixed uploads until he is back and can upload an official 1.35.1.'
  created_at: 2023-07-15 13:35:22+00:00
  edited: false
  hidden: false
  id: 64b2aeaa9a88b423da555e86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c411a94bcba6078a51a86f5e846168d1.svg
      fullname: Gabriel Virrey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ppgabe
      type: user
    createdAt: '2023-07-16T05:08:23.000Z'
    data:
      edited: false
      editors:
      - ppgabe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9477201700210571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c411a94bcba6078a51a86f5e846168d1.svg
          fullname: Gabriel Virrey
          isHf: false
          isPro: false
          name: ppgabe
          type: user
        html: '<blockquote>

          <p>Just tested the model and had no issues with the 1.35.H3 release you
          can find here : <a rel="nofollow" href="https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe">https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe</a><br>Because
          the main developer is out of town these week the H releases are my own bugfixed
          uploads until he is back and can upload an official 1.35.1.</p>

          </blockquote>

          <p>I tested your version of KoboldCPP and can confirm that it works. </p>

          '
        raw: '> Just tested the model and had no issues with the 1.35.H3 release you
          can find here : https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe

          > Because the main developer is out of town these week the H releases are
          my own bugfixed uploads until he is back and can upload an official 1.35.1.


          I tested your version of KoboldCPP and can confirm that it works. '
        updatedAt: '2023-07-16T05:08:23.331Z'
      numEdits: 0
      reactions: []
    id: 64b37b47b090e6970e1e9a67
    type: comment
  author: ppgabe
  content: '> Just tested the model and had no issues with the 1.35.H3 release you
    can find here : https://github.com/henk717/koboldcpp/releases/download/1.35/koboldcpp.exe

    > Because the main developer is out of town these week the H releases are my own
    bugfixed uploads until he is back and can upload an official 1.35.1.


    I tested your version of KoboldCPP and can confirm that it works. '
  created_at: 2023-07-16 04:08:23+00:00
  edited: false
  hidden: false
  id: 64b37b47b090e6970e1e9a67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-07-16T13:14:50.000Z'
    data:
      edited: false
      editors:
      - teneriffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581401944160461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Henk717&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Henk717\">@<span class=\"\
          underline\">Henk717</span></a></span>\n\n\t</span></span> I visited your\
          \ forked repo and saw what you had changed, and I applied it to development\
          \ version of koboldcpp(branch: concedo_experimental). It did not solve my\
          \ problem.</p>\n<p>But I have never guessed koboldcpp may have a problem.\
          \ Thanks <span data-props=\"{&quot;user&quot;:&quot;Henk717&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Henk717\">@<span class=\"\
          underline\">Henk717</span></a></span>\n\n\t</span></span> for good idea.</p>\n\
          <p>Now I found that when I made <code>sched_yield()</code> active by uncommenting,\
          \ llama.cpp, the base of koboldcpp, was failed, even It cannot load the\
          \ model. It looks like that llama.cpp has a problem.</p>\n"
        raw: '@Henk717 I visited your forked repo and saw what you had changed, and
          I applied it to development version of koboldcpp(branch: concedo_experimental).
          It did not solve my problem.


          But I have never guessed koboldcpp may have a problem. Thanks @Henk717 for
          good idea.


          Now I found that when I made ```sched_yield()``` active by uncommenting,
          llama.cpp, the base of koboldcpp, was failed, even It cannot load the model.
          It looks like that llama.cpp has a problem.'
        updatedAt: '2023-07-16T13:14:50.785Z'
      numEdits: 0
      reactions: []
    id: 64b3ed4a727711e4ead76fcd
    type: comment
  author: teneriffa
  content: '@Henk717 I visited your forked repo and saw what you had changed, and
    I applied it to development version of koboldcpp(branch: concedo_experimental).
    It did not solve my problem.


    But I have never guessed koboldcpp may have a problem. Thanks @Henk717 for good
    idea.


    Now I found that when I made ```sched_yield()``` active by uncommenting, llama.cpp,
    the base of koboldcpp, was failed, even It cannot load the model. It looks like
    that llama.cpp has a problem.'
  created_at: 2023-07-16 12:14:50+00:00
  edited: false
  hidden: false
  id: 64b3ed4a727711e4ead76fcd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-07-16T13:48:11.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733501672744751
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>That change is not related to your issue, it was a big performance
          regression from upstream. The fact the model works so poorly for you leads
          me to think you have a corrupt download. Verify the hash.</p>

          '
        raw: That change is not related to your issue, it was a big performance regression
          from upstream. The fact the model works so poorly for you leads me to think
          you have a corrupt download. Verify the hash.
        updatedAt: '2023-07-16T13:48:11.456Z'
      numEdits: 0
      reactions: []
    id: 64b3f51b0a54158d661ed4c0
    type: comment
  author: Henk717
  content: That change is not related to your issue, it was a big performance regression
    from upstream. The fact the model works so poorly for you leads me to think you
    have a corrupt download. Verify the hash.
  created_at: 2023-07-16 12:48:11+00:00
  edited: false
  hidden: false
  id: 64b3f51b0a54158d661ed4c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-07-16T13:55:23.000Z'
    data:
      edited: true
      editors:
      - teneriffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9458695650100708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Henk717&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Henk717\">@<span class=\"\
          underline\">Henk717</span></a></span>\n\n\t</span></span> I compared the\
          \ hash values and they were exactly same. I think it may be a platform specific\
          \ problem especially on Apple Silicon.</p>\n"
        raw: '@Henk717 I compared the hash values and they were exactly same. I think
          it may be a platform specific problem especially on Apple Silicon.'
        updatedAt: '2023-07-16T13:55:53.683Z'
      numEdits: 1
      reactions: []
    id: 64b3f6cb34a92b848c957bea
    type: comment
  author: teneriffa
  content: '@Henk717 I compared the hash values and they were exactly same. I think
    it may be a platform specific problem especially on Apple Silicon.'
  created_at: 2023-07-16 12:55:23+00:00
  edited: true
  hidden: false
  id: 64b3f6cb34a92b848c957bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-07-16T14:03:16.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9260004162788391
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Could be, none of us have apple hardware so if regressions happen
          we can''t test it ourselves. If you can find out which commit on Koboldcpp
          makes it work again let us know.</p>

          '
        raw: Could be, none of us have apple hardware so if regressions happen we
          can't test it ourselves. If you can find out which commit on Koboldcpp makes
          it work again let us know.
        updatedAt: '2023-07-16T14:03:16.317Z'
      numEdits: 0
      reactions: []
    id: 64b3f8a44c3cc95a7563c634
    type: comment
  author: Henk717
  content: Could be, none of us have apple hardware so if regressions happen we can't
    test it ourselves. If you can find out which commit on Koboldcpp makes it work
    again let us know.
  created_at: 2023-07-16 13:03:16+00:00
  edited: false
  hidden: false
  id: 64b3f8a44c3cc95a7563c634
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-07-16T14:08:18.000Z'
    data:
      edited: false
      editors:
      - teneriffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6058292388916016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: '<p>What I have tested today have most recent commits.</p>

          <ul>

          <li>koboldcpp<ul>

          <li>branch: concedo_experimental, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3</li>

          <li>branch: concedo, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3</li>

          </ul>

          </li>

          <li>llama.cpp<ul>

          <li>branch: master, commit hash: 6e7cca404748dd4b1a3affd0d1296e37f4ac0a6f</li>

          </ul>

          </li>

          </ul>

          '
        raw: "What I have tested today have most recent commits.\n\n* koboldcpp\n\
          \  * branch: concedo_experimental, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3\n\
          \  * branch: concedo, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3\n\
          * llama.cpp\n  * branch: master, commit hash: 6e7cca404748dd4b1a3affd0d1296e37f4ac0a6f"
        updatedAt: '2023-07-16T14:08:18.400Z'
      numEdits: 0
      reactions: []
    id: 64b3f9d265a7e15eacf64056
    type: comment
  author: teneriffa
  content: "What I have tested today have most recent commits.\n\n* koboldcpp\n  *\
    \ branch: concedo_experimental, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3\n\
    \  * branch: concedo, commit hash: 5941514e95809472aca70c3a5c5fab580ff56df3\n\
    * llama.cpp\n  * branch: master, commit hash: 6e7cca404748dd4b1a3affd0d1296e37f4ac0a6f"
  created_at: 2023-07-16 13:08:18+00:00
  edited: false
  hidden: false
  id: 64b3f9d265a7e15eacf64056
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: KoboldAI/GPT-NeoX-20B-Erebus-GGML
repo_type: model
status: open
target_branch: null
title: Always returns ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
