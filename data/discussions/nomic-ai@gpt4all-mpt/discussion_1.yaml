!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SamuelAzran
conflicting_files: null
created_at: 2023-05-25 13:16:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
      fullname: Samuel Azran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelAzran
      type: user
    createdAt: '2023-05-25T14:16:51.000Z'
    data:
      edited: false
      editors:
      - SamuelAzran
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
          fullname: Samuel Azran
          isHf: false
          isPro: false
          name: SamuelAzran
          type: user
        html: '<p>Like other MPT models.</p>

          '
        raw: Like other MPT models.
        updatedAt: '2023-05-25T14:16:51.696Z'
      numEdits: 0
      reactions: []
    id: 646f6dd37a64358741c282f1
    type: comment
  author: SamuelAzran
  content: Like other MPT models.
  created_at: 2023-05-25 13:16:51+00:00
  edited: false
  hidden: false
  id: 646f6dd37a64358741c282f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
      fullname: Zach Nussbaum
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zpn
      type: user
    createdAt: '2023-05-26T18:01:01.000Z'
    data:
      edited: false
      editors:
      - zpn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
          fullname: Zach Nussbaum
          isHf: false
          isPro: false
          name: zpn
          type: user
        html: '<p>Possibly, although we haven''t tested this extensively. Let us know
          if you find that it works well!</p>

          '
        raw: Possibly, although we haven't tested this extensively. Let us know if
          you find that it works well!
        updatedAt: '2023-05-26T18:01:01.850Z'
      numEdits: 0
      reactions: []
    id: 6470f3dd806c7d87fa1bf9c2
    type: comment
  author: zpn
  content: Possibly, although we haven't tested this extensively. Let us know if you
    find that it works well!
  created_at: 2023-05-26 17:01:01+00:00
  edited: false
  hidden: false
  id: 6470f3dd806c7d87fa1bf9c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bed026a33c154abec6852b4e313bf1ce.svg
      fullname: Jacky Jiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: t83714
      type: user
    createdAt: '2023-06-05T05:28:18.000Z'
    data:
      edited: false
      editors:
      - t83714
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5873991847038269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bed026a33c154abec6852b4e313bf1ce.svg
          fullname: Jacky Jiang
          isHf: false
          isPro: false
          name: t83714
          type: user
        html: "<p>I've tried with the following code and it (any sequence length rather\
          \ than 2048) doesn't work for me:</p>\n<p>The same code works for <code>mosaicml/mpt-7b-instruct</code>\
          \ though.</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoConfig,\
          \ AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n<span class=\"\
          hljs-keyword\">import</span> torch\n\ndevice = <span class=\"hljs-string\"\
          >f'cuda:<span class=\"hljs-subst\">{torch.cuda.current_device()}</span>'</span>\
          \ <span class=\"hljs-keyword\">if</span> torch.cuda.is_available() <span\
          \ class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">'cpu'</span>\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f'Selected device is: <span class=\"hljs-subst\">{device}</span>'</span>)\n\
          \nmodel_name = <span class=\"hljs-string\">\"nomic-ai/gpt4all-mpt\"</span>\n\
          \nconfig = AutoConfig.from_pretrained(\n  model_name,\n  trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>\n)\n<span class=\"hljs-comment\">#\
          \ use the optimized triton implementation of FlashAttention, you can load\
          \ the model with attn_impl='triton' and move the model to bfloat16</span>\n\
          <span class=\"hljs-comment\">#config.attn_config['attn_impl'] = 'triton'</span>\n\
          config.init_device = device\n<span class=\"hljs-comment\"># config.max_seq_len\
          \ = 2048</span>\n<span class=\"hljs-comment\"># update the maximum sequence\
          \ length during inference to 4096</span>\nconfig.max_seq_len = <span class=\"\
          hljs-number\">3072</span>\n\n<span class=\"hljs-built_in\">print</span>(config)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=config,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code = <span class=\"\
          hljs-literal\">True</span>\n)\n\nmodel.<span class=\"hljs-built_in\">eval</span>()\n\
          </code></pre>\n<p>I got the following error:</p>\n<pre><code>RuntimeError:\
          \ Error(s) in loading state_dict for MPTForCausalLM:\n    size mismatch\
          \ for transformer.wpe.weight: copying a param with shape torch.Size([2048,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([3072,\
          \ 4096]).\n    You may consider adding `ignore_mismatched_sizes=True` in\
          \ the model `from_pretrained` method.\n</code></pre>\n<p>Set <code>ignore_mismatched_sizes=True</code>\
          \ still won't fix it. Instead, you got a different error:</p>\n<pre><code>File\
          \ /opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:3031,\
          \ in PreTrainedModel._load_pretrained_model.&lt;locals&gt;._find_mismatched_keys(state_dict,\
          \ model_state_dict, loaded_keys, add_prefix_to_model, remove_prefix_from_model,\
          \ ignore_mismatched_sizes)\n   3025 elif add_prefix_to_model:\n   3026 \
          \    # The model key doesn't start with `prefix` but `checkpoint_key` does\
          \ so we remove it.\n   3027     model_key = \".\".join(checkpoint_key.split(\"\
          .\")[1:])\n   3029 if (\n   3030     model_key in model_state_dict\n-&gt;\
          \ 3031     and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape\n\
          \   3032 ):\n   3033     mismatched_keys.append(\n   3034         (checkpoint_key,\
          \ state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)\n\
          \   3035     )\n   3036     del state_dict[checkpoint_key]\n\nKeyError:\
          \ 'transformer.blocks.11.ffn.down_proj.weight'\n</code></pre>\n<p>By the\
          \ way, this model also doesn't support the optimized triton implementation\
          \ of FlashAttention like <code>mosaicml/mpt-7b-instruct</code>.<br>If you\
          \ turn it on via <code>config.attn_config['attn_impl'] = 'triton'</code>,\
          \ you will get the same <code>KeyError: 'transformer.blocks.11.ffn.down_proj.weight'</code>\
          \ error.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;zpn&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zpn\"\
          >@<span class=\"underline\">zpn</span></a></span>\n\n\t</span></span> any\
          \ chance you could shed some light on the possible cause of this error?\
          \ Thanks a lot~</p>\n"
        raw: "I've tried with the following code and it (any sequence length rather\
          \ than 2048) doesn't work for me:\n\nThe same code works for `mosaicml/mpt-7b-instruct`\
          \ though.\n\n```python\nfrom transformers import AutoConfig, AutoTokenizer,\
          \ AutoModelForCausalLM, TextIteratorStreamer\nimport torch\n\ndevice = f'cuda:{torch.cuda.current_device()}'\
          \ if torch.cuda.is_available() else 'cpu'\n\nprint(f'Selected device is:\
          \ {device}')\n\nmodel_name = \"nomic-ai/gpt4all-mpt\"\n\nconfig = AutoConfig.from_pretrained(\n\
          \  model_name,\n  trust_remote_code=True\n)\n# use the optimized triton\
          \ implementation of FlashAttention, you can load the model with attn_impl='triton'\
          \ and move the model to bfloat16\n#config.attn_config['attn_impl'] = 'triton'\n\
          config.init_device = device\n# config.max_seq_len = 2048\n# update the maximum\
          \ sequence length during inference to 4096\nconfig.max_seq_len = 3072\n\n\
          print(config)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n\
          \    config=config,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code\
          \ = True\n)\n\nmodel.eval()\n```\n\nI got the following error:\n```\nRuntimeError:\
          \ Error(s) in loading state_dict for MPTForCausalLM:\n\tsize mismatch for\
          \ transformer.wpe.weight: copying a param with shape torch.Size([2048, 4096])\
          \ from checkpoint, the shape in current model is torch.Size([3072, 4096]).\n\
          \tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained`\
          \ method.\n```\n\nSet `ignore_mismatched_sizes=True` still won't fix it.\
          \ Instead, you got a different error:\n```\nFile /opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:3031,\
          \ in PreTrainedModel._load_pretrained_model.<locals>._find_mismatched_keys(state_dict,\
          \ model_state_dict, loaded_keys, add_prefix_to_model, remove_prefix_from_model,\
          \ ignore_mismatched_sizes)\n   3025 elif add_prefix_to_model:\n   3026 \
          \    # The model key doesn't start with `prefix` but `checkpoint_key` does\
          \ so we remove it.\n   3027     model_key = \".\".join(checkpoint_key.split(\"\
          .\")[1:])\n   3029 if (\n   3030     model_key in model_state_dict\n-> 3031\
          \     and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape\n\
          \   3032 ):\n   3033     mismatched_keys.append(\n   3034         (checkpoint_key,\
          \ state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)\n\
          \   3035     )\n   3036     del state_dict[checkpoint_key]\n\nKeyError:\
          \ 'transformer.blocks.11.ffn.down_proj.weight'\n```\n\nBy the way, this\
          \ model also doesn't support the optimized triton implementation of FlashAttention\
          \ like `mosaicml/mpt-7b-instruct`.\nIf you turn it on via `config.attn_config['attn_impl']\
          \ = 'triton'`, you will get the same `KeyError: 'transformer.blocks.11.ffn.down_proj.weight'`\
          \ error.\n\n@zpn any chance you could shed some light on the possible cause\
          \ of this error? Thanks a lot~"
        updatedAt: '2023-06-05T05:28:18.425Z'
      numEdits: 0
      reactions: []
    id: 647d727283c62f3249351001
    type: comment
  author: t83714
  content: "I've tried with the following code and it (any sequence length rather\
    \ than 2048) doesn't work for me:\n\nThe same code works for `mosaicml/mpt-7b-instruct`\
    \ though.\n\n```python\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM,\
    \ TextIteratorStreamer\nimport torch\n\ndevice = f'cuda:{torch.cuda.current_device()}'\
    \ if torch.cuda.is_available() else 'cpu'\n\nprint(f'Selected device is: {device}')\n\
    \nmodel_name = \"nomic-ai/gpt4all-mpt\"\n\nconfig = AutoConfig.from_pretrained(\n\
    \  model_name,\n  trust_remote_code=True\n)\n# use the optimized triton implementation\
    \ of FlashAttention, you can load the model with attn_impl='triton' and move the\
    \ model to bfloat16\n#config.attn_config['attn_impl'] = 'triton'\nconfig.init_device\
    \ = device\n# config.max_seq_len = 2048\n# update the maximum sequence length\
    \ during inference to 4096\nconfig.max_seq_len = 3072\n\nprint(config)\n\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=config,\n\
    \    torch_dtype=torch.bfloat16,\n    trust_remote_code = True\n)\n\nmodel.eval()\n\
    ```\n\nI got the following error:\n```\nRuntimeError: Error(s) in loading state_dict\
    \ for MPTForCausalLM:\n\tsize mismatch for transformer.wpe.weight: copying a param\
    \ with shape torch.Size([2048, 4096]) from checkpoint, the shape in current model\
    \ is torch.Size([3072, 4096]).\n\tYou may consider adding `ignore_mismatched_sizes=True`\
    \ in the model `from_pretrained` method.\n```\n\nSet `ignore_mismatched_sizes=True`\
    \ still won't fix it. Instead, you got a different error:\n```\nFile /opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:3031,\
    \ in PreTrainedModel._load_pretrained_model.<locals>._find_mismatched_keys(state_dict,\
    \ model_state_dict, loaded_keys, add_prefix_to_model, remove_prefix_from_model,\
    \ ignore_mismatched_sizes)\n   3025 elif add_prefix_to_model:\n   3026     # The\
    \ model key doesn't start with `prefix` but `checkpoint_key` does so we remove\
    \ it.\n   3027     model_key = \".\".join(checkpoint_key.split(\".\")[1:])\n \
    \  3029 if (\n   3030     model_key in model_state_dict\n-> 3031     and state_dict[checkpoint_key].shape\
    \ != model_state_dict[model_key].shape\n   3032 ):\n   3033     mismatched_keys.append(\n\
    \   3034         (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)\n\
    \   3035     )\n   3036     del state_dict[checkpoint_key]\n\nKeyError: 'transformer.blocks.11.ffn.down_proj.weight'\n\
    ```\n\nBy the way, this model also doesn't support the optimized triton implementation\
    \ of FlashAttention like `mosaicml/mpt-7b-instruct`.\nIf you turn it on via `config.attn_config['attn_impl']\
    \ = 'triton'`, you will get the same `KeyError: 'transformer.blocks.11.ffn.down_proj.weight'`\
    \ error.\n\n@zpn any chance you could shed some light on the possible cause of\
    \ this error? Thanks a lot~"
  created_at: 2023-06-05 04:28:18+00:00
  edited: false
  hidden: false
  id: 647d727283c62f3249351001
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nomic-ai/gpt4all-mpt
repo_type: model
status: open
target_branch: null
title: Would it work well with sequence length > 2048?
