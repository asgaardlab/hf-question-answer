!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sappho192
conflicting_files: []
created_at: 2023-12-19 05:16:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
      fullname: Taein Kim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sappho192
      type: user
    createdAt: '2023-12-19T05:16:24.000Z'
    data:
      edited: false
      editors:
      - sappho192
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8335996270179749
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
          fullname: Taein Kim
          isHf: false
          isPro: false
          name: sappho192
          type: user
        html: "<p>Beep boop I am the <a href=\"https://huggingface.co/spaces/onnx/export\"\
          >ONNX export bot \U0001F916\U0001F3CE\uFE0F</a>. On behalf of <a href=\"\
          https://huggingface.co/sappho192\">sappho192</a>, I would like to add to\
          \ this repository the model converted to ONNX.</p>\n<p>What is ONNX? It\
          \ stands for \"Open Neural Network Exchange\", and is the most commonly\
          \ used open standard for machine learning interoperability. You can find\
          \ out more at <a rel=\"nofollow\" href=\"https://onnx.ai/\">onnx.ai</a>!</p>\n\
          <p>The exported ONNX model can be then be consumed by various backends as\
          \ TensorRT or TVM, or simply be used in a few lines with \U0001F917 Optimum\
          \ through ONNX Runtime, check out how <a href=\"https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models\"\
          >here</a>!</p>\n"
        raw: "Beep boop I am the [ONNX export bot \U0001F916\U0001F3CE\uFE0F](https://huggingface.co/spaces/onnx/export).\
          \ On behalf of [sappho192](https://huggingface.co/sappho192), I would like\
          \ to add to this repository the model converted to ONNX.\n\nWhat is ONNX?\
          \ It stands for \"Open Neural Network Exchange\", and is the most commonly\
          \ used open standard for machine learning interoperability. You can find\
          \ out more at [onnx.ai](https://onnx.ai/)!\n\nThe exported ONNX model can\
          \ be then be consumed by various backends as TensorRT or TVM, or simply\
          \ be used in a few lines with \U0001F917 Optimum through ONNX Runtime, check\
          \ out how [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models)!"
        updatedAt: '2023-12-19T05:16:24.595Z'
      numEdits: 0
      reactions: []
    id: 65812728c1b5aed69347e36f
    type: comment
  author: sappho192
  content: "Beep boop I am the [ONNX export bot \U0001F916\U0001F3CE\uFE0F](https://huggingface.co/spaces/onnx/export).\
    \ On behalf of [sappho192](https://huggingface.co/sappho192), I would like to\
    \ add to this repository the model converted to ONNX.\n\nWhat is ONNX? It stands\
    \ for \"Open Neural Network Exchange\", and is the most commonly used open standard\
    \ for machine learning interoperability. You can find out more at [onnx.ai](https://onnx.ai/)!\n\
    \nThe exported ONNX model can be then be consumed by various backends as TensorRT\
    \ or TVM, or simply be used in a few lines with \U0001F917 Optimum through ONNX\
    \ Runtime, check out how [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models)!"
  created_at: 2023-12-19 05:16:24+00:00
  edited: false
  hidden: false
  id: 65812728c1b5aed69347e36f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
      fullname: Taein Kim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sappho192
      type: user
    createdAt: '2023-12-19T05:16:25.000Z'
    data:
      oid: 0090c3df930c2f331bdf91b82ef8e2978d5d1dba
      parents:
      - 58aafbd5c98268a8e94e8f4f2b3ad919279792e7
      subject: Adding ONNX file of this model
    id: '658127290000000000000000'
    type: commit
  author: sappho192
  created_at: 2023-12-19 05:16:25+00:00
  id: '658127290000000000000000'
  oid: 0090c3df930c2f331bdf91b82ef8e2978d5d1dba
  summary: Adding ONNX file of this model
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
      fullname: Taein Kim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sappho192
      type: user
    createdAt: '2023-12-19T05:27:09.000Z'
    data:
      edited: false
      editors:
      - sappho192
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.903089702129364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
          fullname: Taein Kim
          isHf: false
          isPro: false
          name: sappho192
          type: user
        html: "<pre><code>This Space allows you to automatically export \U0001F917\
          \ transformers PyTorch models hosted on the Hugging Face Hub to ONNX. It\
          \ opens a PR on the target model, and it is up to the owner of the original\
          \ model to merge the PR to allow people to leverage the ONNX standard to\
          \ share and use the model on a wide range of devices!\n\nOnce exported,\
          \ the model can, for example, be used in the \U0001F917 Optimum library\
          \ closely following the transformers API. Check out this guide to see how!\n\
          \nThe steps are as following:\n\nPaste a read-access token from https://huggingface.co/settings/tokens.\
          \ Read access is enough given that we will open a PR against the source\
          \ repo.\nInput a model id from the Hub (for example: textattack/distilbert-base-cased-CoLA)\n\
          Click \"Export to ONNX\"\nThat's it! You'll get feedback on if the export\
          \ was successful or not, and if it was, you'll get the URL of the opened\
          \ PR!\nNote: in case the model to export is larger than 2 GB, it will be\
          \ saved in a subfolder called onnx/. To load it from Optimum, the argument\
          \ subfolder=\"onnx\" should be provided.\n</code></pre>\n<pre><code>Success\
          \ \U0001F525 Yay! This model was successfully exported and a PR was open\
          \ using your token, here: https://huggingface.co/sappho192/ffxiv-ja-ko-translator/discussions/1.\
          \ If you would like to use the exported model without waiting for the PR\
          \ to be approved, head to https://huggingface.co/sappho192/ffxiv-ja-ko-translator/tree/refs%2Fpr%2F1\n\
          </code></pre>\n"
        raw: "```\nThis Space allows you to automatically export \U0001F917 transformers\
          \ PyTorch models hosted on the Hugging Face Hub to ONNX. It opens a PR on\
          \ the target model, and it is up to the owner of the original model to merge\
          \ the PR to allow people to leverage the ONNX standard to share and use\
          \ the model on a wide range of devices!\n\nOnce exported, the model can,\
          \ for example, be used in the \U0001F917 Optimum library closely following\
          \ the transformers API. Check out this guide to see how!\n\nThe steps are\
          \ as following:\n\nPaste a read-access token from https://huggingface.co/settings/tokens.\
          \ Read access is enough given that we will open a PR against the source\
          \ repo.\nInput a model id from the Hub (for example: textattack/distilbert-base-cased-CoLA)\n\
          Click \"Export to ONNX\"\nThat's it! You'll get feedback on if the export\
          \ was successful or not, and if it was, you'll get the URL of the opened\
          \ PR!\nNote: in case the model to export is larger than 2 GB, it will be\
          \ saved in a subfolder called onnx/. To load it from Optimum, the argument\
          \ subfolder=\"onnx\" should be provided.\n```\n\n```\nSuccess \U0001F525\
          \ Yay! This model was successfully exported and a PR was open using your\
          \ token, here: https://huggingface.co/sappho192/ffxiv-ja-ko-translator/discussions/1.\
          \ If you would like to use the exported model without waiting for the PR\
          \ to be approved, head to https://huggingface.co/sappho192/ffxiv-ja-ko-translator/tree/refs%2Fpr%2F1\n\
          ```"
        updatedAt: '2023-12-19T05:27:09.467Z'
      numEdits: 0
      reactions: []
    id: 658129ad0be5959522cc966c
    type: comment
  author: sappho192
  content: "```\nThis Space allows you to automatically export \U0001F917 transformers\
    \ PyTorch models hosted on the Hugging Face Hub to ONNX. It opens a PR on the\
    \ target model, and it is up to the owner of the original model to merge the PR\
    \ to allow people to leverage the ONNX standard to share and use the model on\
    \ a wide range of devices!\n\nOnce exported, the model can, for example, be used\
    \ in the \U0001F917 Optimum library closely following the transformers API. Check\
    \ out this guide to see how!\n\nThe steps are as following:\n\nPaste a read-access\
    \ token from https://huggingface.co/settings/tokens. Read access is enough given\
    \ that we will open a PR against the source repo.\nInput a model id from the Hub\
    \ (for example: textattack/distilbert-base-cased-CoLA)\nClick \"Export to ONNX\"\
    \nThat's it! You'll get feedback on if the export was successful or not, and if\
    \ it was, you'll get the URL of the opened PR!\nNote: in case the model to export\
    \ is larger than 2 GB, it will be saved in a subfolder called onnx/. To load it\
    \ from Optimum, the argument subfolder=\"onnx\" should be provided.\n```\n\n```\n\
    Success \U0001F525 Yay! This model was successfully exported and a PR was open\
    \ using your token, here: https://huggingface.co/sappho192/ffxiv-ja-ko-translator/discussions/1.\
    \ If you would like to use the exported model without waiting for the PR to be\
    \ approved, head to https://huggingface.co/sappho192/ffxiv-ja-ko-translator/tree/refs%2Fpr%2F1\n\
    ```"
  created_at: 2023-12-19 05:27:09+00:00
  edited: false
  hidden: false
  id: 658129ad0be5959522cc966c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4950d0a78d3666db6b5c7287af30c173.svg
      fullname: Taein Kim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sappho192
      type: user
    createdAt: '2024-01-05T05:56:37.000Z'
    data:
      status: merged
    id: 65979a15092b4516b87f37c8
    type: status-change
  author: sappho192
  created_at: 2024-01-05 05:56:37+00:00
  id: 65979a15092b4516b87f37c8
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 8499b8bac7cd6993232725573693c86790917814
num: 1
repo_id: sappho192/ffxiv-ja-ko-translator
repo_type: model
status: merged
target_branch: refs/heads/main
title: Adding ONNX file of this model
