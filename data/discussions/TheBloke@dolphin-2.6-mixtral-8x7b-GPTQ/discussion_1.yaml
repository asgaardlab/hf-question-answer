!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KronusCon
conflicting_files: null
created_at: 2023-12-26 19:50:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/687e315ecc1184edec69c421c25999cf.svg
      fullname: T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KronusCon
      type: user
    createdAt: '2023-12-26T19:50:43.000Z'
    data:
      edited: false
      editors:
      - KronusCon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5103636384010315
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/687e315ecc1184edec69c421c25999cf.svg
          fullname: T
          isHf: false
          isPro: false
          name: KronusCon
          type: user
        html: '<blockquote>

          <p>python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --model="TheBloke/dolphin-2.6-mixtral-8x7b-AWQ"
          --dtype half --quantization awq --tensor-parallel-size 2</p>

          </blockquote>

          <p>Traceback (most recent call last):<br>  File "/usr/lib/python3.10/runpy.py",
          line 196, in _run_module_as_main<br>    return _run_code(code, main_globals,
          None,<br>  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code<br>    exec(code,
          run_globals)<br>  File "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py",
          line 729, in <br>    engine = AsyncLLMEngine.from_engine_args(engine_args)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py",
          line 496, in from_engine_args<br>    engine = cls(parallel_config.worker_use_ray,<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py",
          line 269, in <strong>init</strong><br>    self.engine = self._init_engine(*args,
          **kwargs)<br>  File "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py",
          line 314, in _init_engine<br>    return engine_class(*args, **kwargs)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 108, in <strong>init</strong><br>    self._init_workers_ray(placement_group)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 195, in _init_workers_ray<br>    self._run_workers(<br>  File "/home/<a
          rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 755, in _run_workers<br>    self._run_workers_in_batch(workers, method,
          *args, **kwargs))<br>  File "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 732, in _run_workers_in_batch<br>    all_outputs = ray.get(all_outputs)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py",
          line 22, in auto_init_wrapper<br>    return fn(*args, **kwargs)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py",
          line 103, in wrapper<br>    return func(*args, **kwargs)<br>  File "/home/<a
          rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/ray/_private/worker.py",
          line 2624, in get<br>    raise value.as_instanceof_cause()<br>ray.exceptions.RayTaskError(KeyError):
          ray::RayWorkerVllm.execute_method() (pid=37680, ip=216.153.52.143, actor_id=03dd8d10b8ec311e8896b66101000000,
          repr=&lt;vllm.engine.ray_utils.RayWorkerVllm object at 0x7fb0e6d68430&gt;)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/engine/ray_utils.py",
          line 31, in execute_method<br>    return executor(*args, **kwargs)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/worker/worker.py",
          line 79, in load_model<br>    self.model_runner.load_model()<br>  File "/home/<a
          rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py",
          line 57, in load_model<br>    self.model = get_model(self.model_config)<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/model_executor/model_loader.py",
          line 72, in get_model<br>    model.load_weights(model_config.model, model_config.download_dir,<br>  File
          "/home/<a rel="nofollow" href="mailto:dev1@gmail.com">dev1@gmail.com</a>/.local/lib/python3.10/site-packages/vllm/model_executor/models/mixtral.py",
          line 430, in load_weights<br>    param = params_dict[name]<br>KeyError:
          ''model.layers.0.block_sparse_moe.experts.0.w1.g_idx''</p>

          <p>Can you please suggest how can I unblock. Thanks.</p>

          '
        raw: "> python3 -m vllm.entrypoints.openai.api_server --trust-remote-code\
          \ --model=\"TheBloke/dolphin-2.6-mixtral-8x7b-AWQ\" --dtype half --quantization\
          \ awq --tensor-parallel-size 2\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\
          \n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\"\
          , line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\"\
          , line 729, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
          , line 496, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
          , line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
          , line 314, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 108, in __init__\r\n    self._init_workers_ray(placement_group)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 195, in _init_workers_ray\r\n    self._run_workers(\r\n  File \"\
          /home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 755, in _run_workers\r\n    self._run_workers_in_batch(workers, method,\
          \ *args, **kwargs))\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 732, in _run_workers_in_batch\r\n    all_outputs = ray.get(all_outputs)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\"\
          , line 22, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n  File\
          \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\"\
          , line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\
          /home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/worker.py\"\
          , line 2624, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(KeyError):\
          \ ray::RayWorkerVllm.execute_method() (pid=37680, ip=216.153.52.143, actor_id=03dd8d10b8ec311e8896b66101000000,\
          \ repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7fb0e6d68430>)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/ray_utils.py\"\
          , line 31, in execute_method\r\n    return executor(*args, **kwargs)\r\n\
          \  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/worker/worker.py\"\
          , line 79, in load_model\r\n    self.model_runner.load_model()\r\n  File\
          \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py\"\
          , line 57, in load_model\r\n    self.model = get_model(self.model_config)\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/model_executor/model_loader.py\"\
          , line 72, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\
          \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/model_executor/models/mixtral.py\"\
          , line 430, in load_weights\r\n    param = params_dict[name]\r\nKeyError:\
          \ 'model.layers.0.block_sparse_moe.experts.0.w1.g_idx'\r\n\r\n\r\nCan you\
          \ please suggest how can I unblock. Thanks."
        updatedAt: '2023-12-26T19:50:43.481Z'
      numEdits: 0
      reactions: []
    id: 658b2e933f011822638cd035
    type: comment
  author: KronusCon
  content: "> python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --model=\"\
    TheBloke/dolphin-2.6-mixtral-8x7b-AWQ\" --dtype half --quantization awq --tensor-parallel-size\
    \ 2\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
    , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
    \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n\
    \    exec(code, run_globals)\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\"\
    , line 729, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
    \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
    , line 496, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\
    \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
    , line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
    \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\"\
    , line 314, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File\
    \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 108, in __init__\r\n    self._init_workers_ray(placement_group)\r\n  File\
    \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 195, in _init_workers_ray\r\n    self._run_workers(\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 755, in _run_workers\r\n    self._run_workers_in_batch(workers, method,\
    \ *args, **kwargs))\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 732, in _run_workers_in_batch\r\n    all_outputs = ray.get(all_outputs)\r\
    \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\"\
    , line 22, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"\
    /home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\"\
    , line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/ray/_private/worker.py\"\
    , line 2624, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(KeyError):\
    \ ray::RayWorkerVllm.execute_method() (pid=37680, ip=216.153.52.143, actor_id=03dd8d10b8ec311e8896b66101000000,\
    \ repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7fb0e6d68430>)\r\n  File\
    \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/engine/ray_utils.py\"\
    , line 31, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File\
    \ \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/worker/worker.py\"\
    , line 79, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py\"\
    , line 57, in load_model\r\n    self.model = get_model(self.model_config)\r\n\
    \  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/model_executor/model_loader.py\"\
    , line 72, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\
    \n  File \"/home/dev1@gmail.com/.local/lib/python3.10/site-packages/vllm/model_executor/models/mixtral.py\"\
    , line 430, in load_weights\r\n    param = params_dict[name]\r\nKeyError: 'model.layers.0.block_sparse_moe.experts.0.w1.g_idx'\r\
    \n\r\n\r\nCan you please suggest how can I unblock. Thanks."
  created_at: 2023-12-26 19:50:43+00:00
  edited: false
  hidden: false
  id: 658b2e933f011822638cd035
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/575da6627464c34c0f7a6e982704debd.svg
      fullname: Darkness Chou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darknessc
      type: user
    createdAt: '2023-12-31T16:18:09.000Z'
    data:
      edited: false
      editors:
      - darknessc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.412604421377182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/575da6627464c34c0f7a6e982704debd.svg
          fullname: Darkness Chou
          isHf: false
          isPro: false
          name: darknessc
          type: user
        html: '<p>I also encountered ValueError: Unrecognized layer: Model.Layers.0.block_sparse_moe.expert.0.w1.bias
          when using text-generation-webui</p>

          '
        raw: 'I also encountered ValueError: Unrecognized layer: Model.Layers.0.block_sparse_moe.expert.0.w1.bias
          when using text-generation-webui'
        updatedAt: '2023-12-31T16:18:09.339Z'
      numEdits: 0
      reactions: []
    id: 65919441e7c71d6d9e3e9dda
    type: comment
  author: darknessc
  content: 'I also encountered ValueError: Unrecognized layer: Model.Layers.0.block_sparse_moe.expert.0.w1.bias
    when using text-generation-webui'
  created_at: 2023-12-31 16:18:09+00:00
  edited: false
  hidden: false
  id: 65919441e7c71d6d9e3e9dda
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/dolphin-2.6-mixtral-8x7b-GPTQ
repo_type: model
status: open
target_branch: null
title: 'KeyError: ''model.layers.0.block_sparse_moe.experts.0.w1.g_idx'' when running
  with tensor parallelism on vllm'
