!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ali-Maq
conflicting_files: null
created_at: 2023-12-07 01:05:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/227fc7b0bd888f7124ee31aac11de7e4.svg
      fullname: Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ali-Maq
      type: user
    createdAt: '2023-12-07T01:05:40.000Z'
    data:
      edited: false
      editors:
      - Ali-Maq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9188965559005737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/227fc7b0bd888f7124ee31aac11de7e4.svg
          fullname: Ali
          isHf: false
          isPro: false
          name: Ali-Maq
          type: user
        html: '<p>Has anyone seen any post , explaining or talking about model architecture
          of Gemini ?</p>

          '
        raw: Has anyone seen any post , explaining or talking about model architecture
          of Gemini ?
        updatedAt: '2023-12-07T01:05:40.469Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - clem
    id: 65711a64b6f7eb7f310fc2f8
    type: comment
  author: Ali-Maq
  content: Has anyone seen any post , explaining or talking about model architecture
    of Gemini ?
  created_at: 2023-12-07 01:05:40+00:00
  edited: false
  hidden: false
  id: 65711a64b6f7eb7f310fc2f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65184c2b2b4fffcb41ddd848/URDlXcuYsDoqtv3yJGdl-.jpeg?w=200&h=200&f=face
      fullname: Huy Vo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sitloboi2012
      type: user
    createdAt: '2023-12-07T01:13:58.000Z'
    data:
      edited: false
      editors:
      - sitloboi2012
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9817925691604614
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65184c2b2b4fffcb41ddd848/URDlXcuYsDoqtv3yJGdl-.jpeg?w=200&h=200&f=face
          fullname: Huy Vo
          isHf: false
          isPro: false
          name: sitloboi2012
          type: user
        html: '<p>They did not really mentioned much about the model architecture
          of Gemini in the Technical Report paper. Mostly information are known such
          as train on Transformers-based decoder with optimization for inferences
          on TPU</p>

          '
        raw: They did not really mentioned much about the model architecture of Gemini
          in the Technical Report paper. Mostly information are known such as train
          on Transformers-based decoder with optimization for inferences on TPU
        updatedAt: '2023-12-07T01:13:58.504Z'
      numEdits: 0
      reactions: []
    id: 65711c56670035a6073f8836
    type: comment
  author: sitloboi2012
  content: They did not really mentioned much about the model architecture of Gemini
    in the Technical Report paper. Mostly information are known such as train on Transformers-based
    decoder with optimization for inferences on TPU
  created_at: 2023-12-07 01:13:58+00:00
  edited: false
  hidden: false
  id: 65711c56670035a6073f8836
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ded3550aedf77a58175a66319d01820.svg
      fullname: Sherman Siu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shermansiu
      type: user
    createdAt: '2023-12-07T02:53:55.000Z'
    data:
      edited: false
      editors:
      - shermansiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9040634632110596
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ded3550aedf77a58175a66319d01820.svg
          fullname: Sherman Siu
          isHf: false
          isPro: false
          name: shermansiu
          type: user
        html: '<p>Yeah, there''s not much info.</p>

          <p>But, there are four input modalities, text (encoded with Sentencepiece),
          images (ViT), video (multiple image frames), and audio (Universal Speech
          Model features @16kHz). I suspect it uses PaLI-style encoding because out
          of the 3 papers it says its inspired by (Flamingo, CoCA, PaLI), PaLI is
          the most recent and simplest to implement.<br>There are two output modalities,
          text and image (paper cites DALL-E and Parti, probably uses Parti because
          it''s auto-regressive)<br>It also uses multi-query attention and possibly
          other new efficient transformer techniques.</p>

          '
        raw: 'Yeah, there''s not much info.


          But, there are four input modalities, text (encoded with Sentencepiece),
          images (ViT), video (multiple image frames), and audio (Universal Speech
          Model features @16kHz). I suspect it uses PaLI-style encoding because out
          of the 3 papers it says its inspired by (Flamingo, CoCA, PaLI), PaLI is
          the most recent and simplest to implement.

          There are two output modalities, text and image (paper cites DALL-E and
          Parti, probably uses Parti because it''s auto-regressive)

          It also uses multi-query attention and possibly other new efficient transformer
          techniques.'
        updatedAt: '2023-12-07T02:53:55.295Z'
      numEdits: 0
      reactions: []
    id: 657133c3f6b924b7693693d7
    type: comment
  author: shermansiu
  content: 'Yeah, there''s not much info.


    But, there are four input modalities, text (encoded with Sentencepiece), images
    (ViT), video (multiple image frames), and audio (Universal Speech Model features
    @16kHz). I suspect it uses PaLI-style encoding because out of the 3 papers it
    says its inspired by (Flamingo, CoCA, PaLI), PaLI is the most recent and simplest
    to implement.

    There are two output modalities, text and image (paper cites DALL-E and Parti,
    probably uses Parti because it''s auto-regressive)

    It also uses multi-query attention and possibly other new efficient transformer
    techniques.'
  created_at: 2023-12-07 02:53:55+00:00
  edited: false
  hidden: false
  id: 657133c3f6b924b7693693d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663744761039-noauth.jpeg?w=200&h=200&f=face
      fullname: Adityam Ghosh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bvk1ng
      type: user
    createdAt: '2023-12-07T09:14:36.000Z'
    data:
      edited: true
      editors:
      - bvk1ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774543642997742
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663744761039-noauth.jpeg?w=200&h=200&f=face
          fullname: Adityam Ghosh
          isHf: false
          isPro: false
          name: bvk1ng
          type: user
        html: '<p>The paper discusses that they trained explicitly based on multimodality.
          So, what I feel like is they trained the vision, text, and audio decoders
          from scratch. Now as far as I can remember the CLIP paper they also did
          something similar like this one.</p>

          <p>So, my guess is they used a contrastive learning for training the network
          following the loss function similar to that of CLIP.</p>

          <p>Then I guess they used RLHF to further fine tune the model.</p>

          <p>For the dataset they mentioned they used data such as web documents,
          images, etc. What I think is something like LAION-400M could be a good starting
          point for our open-source replication.</p>

          <p>Let me know what you think guys.</p>

          '
        raw: 'The paper discusses that they trained explicitly based on multimodality.
          So, what I feel like is they trained the vision, text, and audio decoders
          from scratch. Now as far as I can remember the CLIP paper they also did
          something similar like this one.


          So, my guess is they used a contrastive learning for training the network
          following the loss function similar to that of CLIP.


          Then I guess they used RLHF to further fine tune the model.


          For the dataset they mentioned they used data such as web documents, images,
          etc. What I think is something like LAION-400M could be a good starting
          point for our open-source replication.


          Let me know what you think guys.'
        updatedAt: '2023-12-07T09:15:08.681Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - clem
    id: 65718cfcbaa3d6089cdf2d5c
    type: comment
  author: bvk1ng
  content: 'The paper discusses that they trained explicitly based on multimodality.
    So, what I feel like is they trained the vision, text, and audio decoders from
    scratch. Now as far as I can remember the CLIP paper they also did something similar
    like this one.


    So, my guess is they used a contrastive learning for training the network following
    the loss function similar to that of CLIP.


    Then I guess they used RLHF to further fine tune the model.


    For the dataset they mentioned they used data such as web documents, images, etc.
    What I think is something like LAION-400M could be a good starting point for our
    open-source replication.


    Let me know what you think guys.'
  created_at: 2023-12-07 09:14:36+00:00
  edited: true
  hidden: false
  id: 65718cfcbaa3d6089cdf2d5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ded3550aedf77a58175a66319d01820.svg
      fullname: Sherman Siu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shermansiu
      type: user
    createdAt: '2023-12-07T17:26:48.000Z'
    data:
      edited: false
      editors:
      - shermansiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9293230772018433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ded3550aedf77a58175a66319d01820.svg
          fullname: Sherman Siu
          isHf: false
          isPro: false
          name: shermansiu
          type: user
        html: '<p>If you read all of the latest papers on LMMs, e.g. Flamingo, PaLI,
          Qwen-VL, BLIP-2, Llava-1.5, CogVLM, you''ll notice that the vision encoder
          is probably a pre-trained CLIP model.<br>I doubt that everything is trained
          from scratch, given even Google''s previous papers on LMMs don''t.</p>

          <p>The vision encoder is probably a pre-trained SigLIP model, just like
          PaLI 3. (Hint: Google tends to use other Google models) Lots of other LMMs
          have used pre-trained Clip-ViT-H weights , so probably a model similar to
          that. They used USM "features", which doesn''t make sense unless they used
          a pre-trained USM model.</p>

          <p>The language model itself is probably trained from scratch, esp. since
          it has an image output head.</p>

          <p>Do you have 8x[A|H]100 GPUs?</p>

          '
        raw: 'If you read all of the latest papers on LMMs, e.g. Flamingo, PaLI, Qwen-VL,
          BLIP-2, Llava-1.5, CogVLM, you''ll notice that the vision encoder is probably
          a pre-trained CLIP model.

          I doubt that everything is trained from scratch, given even Google''s previous
          papers on LMMs don''t.


          The vision encoder is probably a pre-trained SigLIP model, just like PaLI
          3. (Hint: Google tends to use other Google models) Lots of other LMMs have
          used pre-trained Clip-ViT-H weights , so probably a model similar to that.
          They used USM "features", which doesn''t make sense unless they used a pre-trained
          USM model.


          The language model itself is probably trained from scratch, esp. since it
          has an image output head.


          Do you have 8x[A|H]100 GPUs?'
        updatedAt: '2023-12-07T17:26:48.913Z'
      numEdits: 0
      reactions: []
    id: 65720058643da8aedabfa427
    type: comment
  author: shermansiu
  content: 'If you read all of the latest papers on LMMs, e.g. Flamingo, PaLI, Qwen-VL,
    BLIP-2, Llava-1.5, CogVLM, you''ll notice that the vision encoder is probably
    a pre-trained CLIP model.

    I doubt that everything is trained from scratch, given even Google''s previous
    papers on LMMs don''t.


    The vision encoder is probably a pre-trained SigLIP model, just like PaLI 3. (Hint:
    Google tends to use other Google models) Lots of other LMMs have used pre-trained
    Clip-ViT-H weights , so probably a model similar to that. They used USM "features",
    which doesn''t make sense unless they used a pre-trained USM model.


    The language model itself is probably trained from scratch, esp. since it has
    an image output head.


    Do you have 8x[A|H]100 GPUs?'
  created_at: 2023-12-07 17:26:48+00:00
  edited: false
  hidden: false
  id: 65720058643da8aedabfa427
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663744761039-noauth.jpeg?w=200&h=200&f=face
      fullname: Adityam Ghosh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bvk1ng
      type: user
    createdAt: '2023-12-22T10:01:49.000Z'
    data:
      edited: false
      editors:
      - bvk1ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9790464043617249
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663744761039-noauth.jpeg?w=200&h=200&f=face
          fullname: Adityam Ghosh
          isHf: false
          isPro: false
          name: bvk1ng
          type: user
        html: "<p>I see; they stated it's multimodal from the beginning. Now, I haven't\
          \ read the FLAMINGO paper, so I won't be able to comment on that. Still,\
          \ it seems from your conversation that they used two different pre-trained\
          \ encoders in that paper and then let them learn the joint representation.\
          \ But, in the Gemini, they explicitly mentioned that the model is \"multimodal\"\
          \  by default. Therefore, I concluded that they might have trained the network\
          \ from scratch. </p>\n<p>Apparently, no, I don't have access to 8x[A|H]100\
          \ GPUs \U0001F605.</p>\n"
        raw: "I see; they stated it's multimodal from the beginning. Now, I haven't\
          \ read the FLAMINGO paper, so I won't be able to comment on that. Still,\
          \ it seems from your conversation that they used two different pre-trained\
          \ encoders in that paper and then let them learn the joint representation.\
          \ But, in the Gemini, they explicitly mentioned that the model is \"multimodal\"\
          \  by default. Therefore, I concluded that they might have trained the network\
          \ from scratch. \n\nApparently, no, I don't have access to 8x[A|H]100 GPUs\
          \ \U0001F605."
        updatedAt: '2023-12-22T10:01:49.060Z'
      numEdits: 0
      reactions: []
    id: 65855e8d1c461dfe88ba39cf
    type: comment
  author: bvk1ng
  content: "I see; they stated it's multimodal from the beginning. Now, I haven't\
    \ read the FLAMINGO paper, so I won't be able to comment on that. Still, it seems\
    \ from your conversation that they used two different pre-trained encoders in\
    \ that paper and then let them learn the joint representation. But, in the Gemini,\
    \ they explicitly mentioned that the model is \"multimodal\"  by default. Therefore,\
    \ I concluded that they might have trained the network from scratch. \n\nApparently,\
    \ no, I don't have access to 8x[A|H]100 GPUs \U0001F605."
  created_at: 2023-12-22 10:01:49+00:00
  edited: false
  hidden: false
  id: 65855e8d1c461dfe88ba39cf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: clem/gemini
repo_type: model
status: open
target_branch: null
title: Model Architecture for Gemini
