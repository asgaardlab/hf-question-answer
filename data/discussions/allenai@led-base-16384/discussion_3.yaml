!!python/object:huggingface_hub.community.DiscussionWithDetails
author: traopia
conflicting_files: null
created_at: 2023-02-27 17:00:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8653c016639678a677ce01e4bce49f22.svg
      fullname: Teresa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: traopia
      type: user
    createdAt: '2023-02-27T17:00:40.000Z'
    data:
      edited: false
      editors:
      - traopia
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8653c016639678a677ce01e4bce49f22.svg
          fullname: Teresa
          isHf: false
          isPro: false
          name: traopia
          type: user
        html: '<p>Hey!<br>I am fine-tuning this model with my own data - but if I
          set the max number of tokens to be higher than 1024 I get this error<br>''IndexError:
          index out of range in self''<br>which indeed I would not expect - since
          I chose this model as to handle longer sequence inputs.<br>Does anyone know
          why that could be the case?</p>

          '
        raw: "Hey!\r\nI am fine-tuning this model with my own data - but if I set\
          \ the max number of tokens to be higher than 1024 I get this error\r\n'IndexError:\
          \ index out of range in self'\r\nwhich indeed I would not expect - since\
          \ I chose this model as to handle longer sequence inputs.\r\nDoes anyone\
          \ know why that could be the case?"
        updatedAt: '2023-02-27T17:00:40.035Z'
      numEdits: 0
      reactions: []
    id: 63fce1b894cc8f815d4aa613
    type: comment
  author: traopia
  content: "Hey!\r\nI am fine-tuning this model with my own data - but if I set the\
    \ max number of tokens to be higher than 1024 I get this error\r\n'IndexError:\
    \ index out of range in self'\r\nwhich indeed I would not expect - since I chose\
    \ this model as to handle longer sequence inputs.\r\nDoes anyone know why that\
    \ could be the case?"
  created_at: 2023-02-27 17:00:40+00:00
  edited: false
  hidden: false
  id: 63fce1b894cc8f815d4aa613
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd2c454a463fbe3e4d3236dee05b5874.svg
      fullname: Krishanu Das Baksi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krishanudb
      type: user
    createdAt: '2023-03-15T21:00:47.000Z'
    data:
      edited: false
      editors:
      - krishanudb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd2c454a463fbe3e4d3236dee05b5874.svg
          fullname: Krishanu Das Baksi
          isHf: false
          isPro: false
          name: krishanudb
          type: user
        html: '<p>Can you paste your code here.</p>

          '
        raw: Can you paste your code here.
        updatedAt: '2023-03-15T21:00:47.133Z'
      numEdits: 0
      reactions: []
    id: 641231ff2c8c1babb582ca50
    type: comment
  author: krishanudb
  content: Can you paste your code here.
  created_at: 2023-03-15 20:00:47+00:00
  edited: false
  hidden: false
  id: 641231ff2c8c1babb582ca50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f1a6a4446832a9068b9ad40f094e8a5.svg
      fullname: Theodoros Tsiolakis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheoTsio
      type: user
    createdAt: '2023-04-23T16:05:30.000Z'
    data:
      edited: true
      editors:
      - TheoTsio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f1a6a4446832a9068b9ad40f094e8a5.svg
          fullname: Theodoros Tsiolakis
          isHf: false
          isPro: false
          name: TheoTsio
          type: user
        html: '<p>I get the exact same error ''IndexError: index out of range in self''
          when I set the max number of tokens to be higher than 1024. Is there a solution
          to this problem?</p>

          '
        raw: 'I get the exact same error ''IndexError: index out of range in self''
          when I set the max number of tokens to be higher than 1024. Is there a solution
          to this problem?'
        updatedAt: '2023-04-23T16:05:51.263Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - chandanksahu
    id: 6445574af993c804b035d6ed
    type: comment
  author: TheoTsio
  content: 'I get the exact same error ''IndexError: index out of range in self''
    when I set the max number of tokens to be higher than 1024. Is there a solution
    to this problem?'
  created_at: 2023-04-23 15:05:30+00:00
  edited: true
  hidden: false
  id: 6445574af993c804b035d6ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd2c454a463fbe3e4d3236dee05b5874.svg
      fullname: Krishanu Das Baksi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krishanudb
      type: user
    createdAt: '2023-04-23T16:30:11.000Z'
    data:
      edited: false
      editors:
      - krishanudb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd2c454a463fbe3e4d3236dee05b5874.svg
          fullname: Krishanu Das Baksi
          isHf: false
          isPro: false
          name: krishanudb
          type: user
        html: '<p>Paste code here.</p>

          '
        raw: Paste code here.
        updatedAt: '2023-04-23T16:30:11.142Z'
      numEdits: 0
      reactions: []
    id: 64455d130f2fc80feb266315
    type: comment
  author: krishanudb
  content: Paste code here.
  created_at: 2023-04-23 15:30:11+00:00
  edited: false
  hidden: false
  id: 64455d130f2fc80feb266315
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f1a6a4446832a9068b9ad40f094e8a5.svg
      fullname: Theodoros Tsiolakis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheoTsio
      type: user
    createdAt: '2023-04-23T16:51:41.000Z'
    data:
      edited: true
      editors:
      - TheoTsio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f1a6a4446832a9068b9ad40f094e8a5.svg
          fullname: Theodoros Tsiolakis
          isHf: false
          isPro: false
          name: TheoTsio
          type: user
        html: '<p>from datasets import load_dataset<br>import torch<br>from transformers
          import AutoTokenizer, DataCollatorWithPadding</p>

          <p>data_files = {"train":"/content/drive/MyDrive/THESIS/train_baseline_documents.csv",
          "test":"/content/drive/MyDrive/THESIS/test_baseline_documents.csv", "validation":"/content/drive/MyDrive/THESIS/val_baseline_documents.csv"}<br>splitted_dataset
          = load_dataset("csv", data_files=data_files)</p>

          <h1 id="load-the-tokenizer-of-the-model-biobert-and-tokenize-the-dataset">Load
          the tokenizer of the model BioBERT and tokenize the dataset</h1>

          <p>checkpoint = "allenai/led-base-16384"<br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)</p>

          <p>def tokenize_function(example):<br>    return tokenizer(example["Document"],
          truncation=True, padding=True)</p>

          <p>tokenized_dataset = splitted_dataset.map(tokenize_function, batched=True)<br>tokenized_dataset
          = tokenized_dataset.rename_column("Credibility", "labels")<br>print(tokenized_dataset)</p>

          <p>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</p>

          <h1 id="trainning-arguments">Trainning arguments</h1>

          <p>from transformers import TrainingArguments<br>training_args = TrainingArguments(output_dir="/home/theotsio",
          per_device_train_batch_size=6, seed=42)</p>

          <h1 id="loading-modelfrom">Loading Modelfrom</h1>

          <p>from transformers import AutoModelForSequenceClassification</p>

          <p>model = AutoModelForSequenceClassification.from_pretrained(checkpoint,
          num_labels=3)</p>

          <h1 id="train-the-pretrained-model-on-the-specific-task">Train the pretrained
          model on the specific task</h1>

          <p>from transformers import Trainer</p>

          <p>trainer = Trainer(<br>    model,<br>    training_args,<br>    train_dataset=tokenized_dataset["train"],<br>    eval_dataset=tokenized_dataset["validation"],<br>    data_collator=data_collator,<br>    tokenizer=tokenizer<br>)</p>

          <p>from codecarbon import EmissionsTracker</p>

          <p>tracker = EmissionsTracker()<br>tracker.start()</p>

          <h1 id="run-the-train">Run the train</h1>

          <p>trainer.train()</p>

          <p>tracker.stop()</p>

          <h1 id="evaluation-of-validation">Evaluation of validation</h1>

          <p>predictions_val = trainer.predict(tokenized_dataset["validation"])<br>predictions_test
          = trainer.predict(tokenized_dataset["test"])</p>

          <p>import numpy as np</p>

          <h1 id="evaluation-of-validation-1">Evaluation of validation</h1>

          <p>preds_val = np.argmax(predictions_val.predictions, axis=-1)<br>preds_test
          = np.argmax(predictions_test.predictions, axis=-1)</p>

          <p>import datasets<br>metric = datasets.load_metric("accuracy")</p>

          <p>print("The validation accuracy", metric.compute(predictions=preds_val,
          references=predictions_val.label_ids))<br>print("The test set accuracy",
          metric.compute(predictions=preds_test, references=predictions_test.label_ids))</p>

          <p>This is my code. Thank you for your time</p>

          '
        raw: "from datasets import load_dataset\nimport torch\nfrom transformers import\
          \ AutoTokenizer, DataCollatorWithPadding\n\ndata_files = {\"train\":\"/content/drive/MyDrive/THESIS/train_baseline_documents.csv\"\
          , \"test\":\"/content/drive/MyDrive/THESIS/test_baseline_documents.csv\"\
          , \"validation\":\"/content/drive/MyDrive/THESIS/val_baseline_documents.csv\"\
          }\nsplitted_dataset = load_dataset(\"csv\", data_files=data_files)\n\n#\
          \ Load the tokenizer of the model BioBERT and tokenize the dataset\ncheckpoint\
          \ = \"allenai/led-base-16384\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          \ndef tokenize_function(example):\n    return tokenizer(example[\"Document\"\
          ], truncation=True, padding=True)\n\ntokenized_dataset = splitted_dataset.map(tokenize_function,\
          \ batched=True)\ntokenized_dataset = tokenized_dataset.rename_column(\"\
          Credibility\", \"labels\")\nprint(tokenized_dataset)\n\ndata_collator =\
          \ DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Trainning arguments\n\
          from transformers import TrainingArguments\ntraining_args = TrainingArguments(output_dir=\"\
          /home/theotsio\", per_device_train_batch_size=6, seed=42)\n\n# Loading Modelfrom\n\
          from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint,\
          \ num_labels=3)\n\n# Train the pretrained model on the specific task\nfrom\
          \ transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n\
          \    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"\
          validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n\
          )\n\nfrom codecarbon import EmissionsTracker\n\ntracker = EmissionsTracker()\n\
          tracker.start()\n\n# Run the train\ntrainer.train()\n\ntracker.stop()\n\n\
          # Evaluation of validation\npredictions_val = trainer.predict(tokenized_dataset[\"\
          validation\"])\npredictions_test = trainer.predict(tokenized_dataset[\"\
          test\"])\n\nimport numpy as np\n\n# Evaluation of validation\npreds_val\
          \ = np.argmax(predictions_val.predictions, axis=-1)\npreds_test = np.argmax(predictions_test.predictions,\
          \ axis=-1)\n\nimport datasets\nmetric = datasets.load_metric(\"accuracy\"\
          )\n\nprint(\"The validation accuracy\", metric.compute(predictions=preds_val,\
          \ references=predictions_val.label_ids))\nprint(\"The test set accuracy\"\
          , metric.compute(predictions=preds_test, references=predictions_test.label_ids))\n\
          \nThis is my code. Thank you for your time"
        updatedAt: '2023-04-23T17:55:11.936Z'
      numEdits: 1
      reactions: []
    id: 6445621df993c804b036a9ef
    type: comment
  author: TheoTsio
  content: "from datasets import load_dataset\nimport torch\nfrom transformers import\
    \ AutoTokenizer, DataCollatorWithPadding\n\ndata_files = {\"train\":\"/content/drive/MyDrive/THESIS/train_baseline_documents.csv\"\
    , \"test\":\"/content/drive/MyDrive/THESIS/test_baseline_documents.csv\", \"validation\"\
    :\"/content/drive/MyDrive/THESIS/val_baseline_documents.csv\"}\nsplitted_dataset\
    \ = load_dataset(\"csv\", data_files=data_files)\n\n# Load the tokenizer of the\
    \ model BioBERT and tokenize the dataset\ncheckpoint = \"allenai/led-base-16384\"\
    \ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(example):\n\
    \    return tokenizer(example[\"Document\"], truncation=True, padding=True)\n\n\
    tokenized_dataset = splitted_dataset.map(tokenize_function, batched=True)\ntokenized_dataset\
    \ = tokenized_dataset.rename_column(\"Credibility\", \"labels\")\nprint(tokenized_dataset)\n\
    \ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Trainning\
    \ arguments\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments(output_dir=\"\
    /home/theotsio\", per_device_train_batch_size=6, seed=42)\n\n# Loading Modelfrom\n\
    from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint,\
    \ num_labels=3)\n\n# Train the pretrained model on the specific task\nfrom transformers\
    \ import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_dataset[\"\
    train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n\
    \    tokenizer=tokenizer\n)\n\nfrom codecarbon import EmissionsTracker\n\ntracker\
    \ = EmissionsTracker()\ntracker.start()\n\n# Run the train\ntrainer.train()\n\n\
    tracker.stop()\n\n# Evaluation of validation\npredictions_val = trainer.predict(tokenized_dataset[\"\
    validation\"])\npredictions_test = trainer.predict(tokenized_dataset[\"test\"\
    ])\n\nimport numpy as np\n\n# Evaluation of validation\npreds_val = np.argmax(predictions_val.predictions,\
    \ axis=-1)\npreds_test = np.argmax(predictions_test.predictions, axis=-1)\n\n\
    import datasets\nmetric = datasets.load_metric(\"accuracy\")\n\nprint(\"The validation\
    \ accuracy\", metric.compute(predictions=preds_val, references=predictions_val.label_ids))\n\
    print(\"The test set accuracy\", metric.compute(predictions=preds_test, references=predictions_test.label_ids))\n\
    \nThis is my code. Thank you for your time"
  created_at: 2023-04-23 15:51:41+00:00
  edited: true
  hidden: false
  id: 6445621df993c804b036a9ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: allenai/led-base-16384
repo_type: model
status: open
target_branch: null
title: Unable to set max number of tokens in input higher than 1024
