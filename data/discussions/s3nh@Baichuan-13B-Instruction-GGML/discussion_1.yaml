!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WindyGleam
conflicting_files: null
created_at: 2023-07-27 10:37:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
      fullname: WoolCool
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WindyGleam
      type: user
    createdAt: '2023-07-27T11:37:44.000Z'
    data:
      edited: false
      editors:
      - WindyGleam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3069710433483124
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
          fullname: WoolCool
          isHf: false
          isPro: false
          name: WindyGleam
          type: user
        html: "<p>F:\\llamacpp-k&gt;main --mlock --instruct -i --interactive-first\
          \ --top_k 60 --top_p 1.1  -c 2048 --color --temp 0.8 -n -1 --keep -1 --repeat_penalty\
          \ 1.1 -t 6 -m Baichuan-13B-Instruction.ggmlv3.q5_1.bin  -ngl 22<br>main:\
          \ build = 913 (eb542d3)<br>main: seed  = 1690457767<br>ggml_init_cublas:\
          \ found 1 CUDA devices:<br>  Device 0: NVIDIA GeForce RTX 2060 SUPER, compute\
          \ capability 7.5<br>llama.cpp: loading model from Baichuan-13B-Instruction.ggmlv3.q5_1.bin<br>llama_model_load_internal:\
          \ format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab  \
          \  = 64000<br>llama_model_load_internal: n_ctx      = 2048<br>llama_model_load_internal:\
          \ n_embd     = 5120<br>llama_model_load_internal: n_mult     = 214<br>llama_model_load_internal:\
          \ n_head     = 40<br>llama_model_load_internal: n_head_kv  = 40<br>llama_model_load_internal:\
          \ n_layer    = 40<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:\
          \ n_gqa      = 1<br>llama_model_load_internal: rnorm_eps  = 5.0e-06<br>llama_model_load_internal:\
          \ n_ff       = 13696<br>llama_model_load_internal: freq_base  = 10000.0<br>llama_model_load_internal:\
          \ freq_scale = 1<br>llama_model_load_internal: ftype      = 9 (mostly Q5_1)<br>llama_model_load_internal:\
          \ model size = 13B<br>llama_model_load_internal: ggml ctx size =    0.11\
          \ MB<br>llama_model_load_internal: using CUDA for GPU acceleration<br>llama_model_load_internal:\
          \ mem required  = 5043.99 MB (+ 1600.00 MB per state)<br>llama_model_load_internal:\
          \ allocating batch_size x (640 kB + n_ctx x 160 B) = 480 MB VRAM for the\
          \ scratch buffer<br>llama_model_load_internal: offloading 22 repeating layers\
          \ to GPU<br>llama_model_load_internal: offloaded 22/43 layers to GPU<br>llama_model_load_internal:\
          \ total VRAM used: 5442 MB<br>llama_new_context_with_model: kv self size\
          \  = 1600.00 MB</p>\n<p>system_info: n_threads = 6 / 12 | AVX = 1 | AVX2\
          \ = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON\
          \ = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1\
          \ | SSE3 = 1 | VSX = 0 |<br>main: interactive mode on.<br>Reverse prompt:\
          \ '### Instruction:</p>\n<p>'<br>sampling: repeat_last_n = 64, repeat_penalty\
          \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000,\
          \ top_k = 60, tfs_z = 1.000000, top_p = 1.100000, typical_p = 1.000000,\
          \ temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent =\
          \ 5.000000<br>generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep\
          \ = 2</p>\n<p>== Running in interactive mode. ==</p>\n<ul>\n<li>Press Ctrl+C\
          \ to interject at any time.</li>\n<li>Press Return to return control to\
          \ LLaMa.</li>\n<li>To return control without starting a new line, end your\
          \ input with '/'.</li>\n<li>If you want to submit another line, end your\
          \ input with ''.</li>\n</ul>\n<blockquote>\n<p>\u4F60\u597D</p>\n</blockquote>\n\
          <p>```csharp<br>Console.WriteLine(\"#  \u7528C#\u7F16\u5199\u8F93\u51FA\u6587\
          \u672C\u4E3A\uFF1A\"Hello, world!\"</p>\n<blockquote>\n<p>\u5E38\u89C1\u7684\
          \u6C34\u679C\u6709\u54EA\u51E0\u79CD\uFF1F</p>\n</blockquote>\n<blockquote>\n\
          <p>\u4E0B\u96E8\u65F6\u4EBA\u4E3A\u4EC0\u4E48\u8981\u6253\u4F1E<br>\u306B\
          \u306F\u306A\u308C\u304C\u751F\u308B\u308A\u308A\u30EA\u30EA\u308A\u308A\
          \u308A\u308C\u308C```<br>\u5F53\u4F7F\u7528C# \u5F53\u4F7F\u7528c cccc\u5728\
          \u5C06\uFF1A</p>\n</blockquote>\n<blockquote>\n</blockquote>\n"
        raw: "F:\\llamacpp-k>main --mlock --instruct -i --interactive-first --top_k\
          \ 60 --top_p 1.1  -c 2048 --color --temp 0.8 -n -1 --keep -1 --repeat_penalty\
          \ 1.1 -t 6 -m Baichuan-13B-Instruction.ggmlv3.q5_1.bin  -ngl 22\r\nmain:\
          \ build = 913 (eb542d3)\r\nmain: seed  = 1690457767\r\nggml_init_cublas:\
          \ found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 2060 SUPER, compute\
          \ capability 7.5\r\nllama.cpp: loading model from Baichuan-13B-Instruction.ggmlv3.q5_1.bin\r\
          \nllama_model_load_internal: format     = ggjt v3 (latest)\r\nllama_model_load_internal:\
          \ n_vocab    = 64000\r\nllama_model_load_internal: n_ctx      = 2048\r\n\
          llama_model_load_internal: n_embd     = 5120\r\nllama_model_load_internal:\
          \ n_mult     = 214\r\nllama_model_load_internal: n_head     = 40\r\nllama_model_load_internal:\
          \ n_head_kv  = 40\r\nllama_model_load_internal: n_layer    = 40\r\nllama_model_load_internal:\
          \ n_rot      = 128\r\nllama_model_load_internal: n_gqa      = 1\r\nllama_model_load_internal:\
          \ rnorm_eps  = 5.0e-06\r\nllama_model_load_internal: n_ff       = 13696\r\
          \nllama_model_load_internal: freq_base  = 10000.0\r\nllama_model_load_internal:\
          \ freq_scale = 1\r\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\r\
          \nllama_model_load_internal: model size = 13B\r\nllama_model_load_internal:\
          \ ggml ctx size =    0.11 MB\r\nllama_model_load_internal: using CUDA for\
          \ GPU acceleration\r\nllama_model_load_internal: mem required  = 5043.99\
          \ MB (+ 1600.00 MB per state)\r\nllama_model_load_internal: allocating batch_size\
          \ x (640 kB + n_ctx x 160 B) = 480 MB VRAM for the scratch buffer\r\nllama_model_load_internal:\
          \ offloading 22 repeating layers to GPU\r\nllama_model_load_internal: offloaded\
          \ 22/43 layers to GPU\r\nllama_model_load_internal: total VRAM used: 5442\
          \ MB\r\nllama_new_context_with_model: kv self size  = 1600.00 MB\r\n\r\n\
          system_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\nmain:\
          \ interactive mode on.\r\nReverse prompt: '### Instruction:\r\n\r\n'\r\n\
          sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
          \ = 0.000000, frequency_penalty = 0.000000, top_k = 60, tfs_z = 1.000000,\
          \ top_p = 1.100000, typical_p = 1.000000, temp = 0.800000, mirostat = 0,\
          \ mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 2048,\
          \ n_batch = 512, n_predict = -1, n_keep = 2\r\n\r\n\r\n== Running in interactive\
          \ mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return\
          \ to return control to LLaMa.\r\n - To return control without starting a\
          \ new line, end your input with '/'.\r\n - If you want to submit another\
          \ line, end your input with '\\'.\r\n\r\n\r\n> \u4F60\u597D\r\n\r\n```csharp\r\
          \nConsole.WriteLine(\"#  \u7528C#\u7F16\u5199\u8F93\u51FA\u6587\u672C\u4E3A\
          \uFF1A\"Hello, world!\"\r\n\r\n> \u5E38\u89C1\u7684\u6C34\u679C\u6709\u54EA\
          \u51E0\u79CD\uFF1F\r\n\r\n\r\n> \u4E0B\u96E8\u65F6\u4EBA\u4E3A\u4EC0\u4E48\
          \u8981\u6253\u4F1E\r\n\u306B\u306F\u306A\u308C\u304C\u751F\u308B\u308A\u308A\
          \u30EA\u30EA\u308A\u308A\u308A\u308C\u308C```\r\n\u5F53\u4F7F\u7528C# \u5F53\
          \u4F7F\u7528c cccc\u5728\u5C06\uFF1A\r\n\r\n>"
        updatedAt: '2023-07-27T11:37:44.515Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - s3nh
    id: 64c25708617b36543de3c37b
    type: comment
  author: WindyGleam
  content: "F:\\llamacpp-k>main --mlock --instruct -i --interactive-first --top_k\
    \ 60 --top_p 1.1  -c 2048 --color --temp 0.8 -n -1 --keep -1 --repeat_penalty\
    \ 1.1 -t 6 -m Baichuan-13B-Instruction.ggmlv3.q5_1.bin  -ngl 22\r\nmain: build\
    \ = 913 (eb542d3)\r\nmain: seed  = 1690457767\r\nggml_init_cublas: found 1 CUDA\
    \ devices:\r\n  Device 0: NVIDIA GeForce RTX 2060 SUPER, compute capability 7.5\r\
    \nllama.cpp: loading model from Baichuan-13B-Instruction.ggmlv3.q5_1.bin\r\nllama_model_load_internal:\
    \ format     = ggjt v3 (latest)\r\nllama_model_load_internal: n_vocab    = 64000\r\
    \nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal: n_embd\
    \     = 5120\r\nllama_model_load_internal: n_mult     = 214\r\nllama_model_load_internal:\
    \ n_head     = 40\r\nllama_model_load_internal: n_head_kv  = 40\r\nllama_model_load_internal:\
    \ n_layer    = 40\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal:\
    \ n_gqa      = 1\r\nllama_model_load_internal: rnorm_eps  = 5.0e-06\r\nllama_model_load_internal:\
    \ n_ff       = 13696\r\nllama_model_load_internal: freq_base  = 10000.0\r\nllama_model_load_internal:\
    \ freq_scale = 1\r\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\r\n\
    llama_model_load_internal: model size = 13B\r\nllama_model_load_internal: ggml\
    \ ctx size =    0.11 MB\r\nllama_model_load_internal: using CUDA for GPU acceleration\r\
    \nllama_model_load_internal: mem required  = 5043.99 MB (+ 1600.00 MB per state)\r\
    \nllama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B)\
    \ = 480 MB VRAM for the scratch buffer\r\nllama_model_load_internal: offloading\
    \ 22 repeating layers to GPU\r\nllama_model_load_internal: offloaded 22/43 layers\
    \ to GPU\r\nllama_model_load_internal: total VRAM used: 5442 MB\r\nllama_new_context_with_model:\
    \ kv self size  = 1600.00 MB\r\n\r\nsystem_info: n_threads = 6 / 12 | AVX = 1\
    \ | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON\
    \ = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
    \ = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nReverse prompt: '### Instruction:\r\
    \n\r\n'\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
    \ = 0.000000, frequency_penalty = 0.000000, top_k = 60, tfs_z = 1.000000, top_p\
    \ = 1.100000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr\
    \ = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 2048, n_batch = 512,\
    \ n_predict = -1, n_keep = 2\r\n\r\n\r\n== Running in interactive mode. ==\r\n\
    \ - Press Ctrl+C to interject at any time.\r\n - Press Return to return control\
    \ to LLaMa.\r\n - To return control without starting a new line, end your input\
    \ with '/'.\r\n - If you want to submit another line, end your input with '\\\
    '.\r\n\r\n\r\n> \u4F60\u597D\r\n\r\n```csharp\r\nConsole.WriteLine(\"#  \u7528\
    C#\u7F16\u5199\u8F93\u51FA\u6587\u672C\u4E3A\uFF1A\"Hello, world!\"\r\n\r\n> \u5E38\
    \u89C1\u7684\u6C34\u679C\u6709\u54EA\u51E0\u79CD\uFF1F\r\n\r\n\r\n> \u4E0B\u96E8\
    \u65F6\u4EBA\u4E3A\u4EC0\u4E48\u8981\u6253\u4F1E\r\n\u306B\u306F\u306A\u308C\u304C\
    \u751F\u308B\u308A\u308A\u30EA\u30EA\u308A\u308A\u308A\u308C\u308C```\r\n\u5F53\
    \u4F7F\u7528C# \u5F53\u4F7F\u7528c cccc\u5728\u5C06\uFF1A\r\n\r\n>"
  created_at: 2023-07-27 10:37:44+00:00
  edited: false
  hidden: false
  id: 64c25708617b36543de3c37b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-27T12:08:32.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5842670798301697
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: "<p>Hi, did you try to use another prompt template? Based on Alpachino\
          \ implementation, example input, instruct should look like this: </p>\n\
          <p>'''python<br>import torch<br>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer<br>from transformers.generation.utils import GenerationConfig<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"AlpachinoNLP/Baichuan-13B-Instruction\"\
          , use_fast=False, trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained(\"\
          AlpachinoNLP/Baichuan-13B-Instruction\", device_map=\"auto\", torch_dtype=torch.float16,\
          \ trust_remote_code=True)<br>model.generation_config = GenerationConfig.from_pretrained(\"\
          AlpachinoNLP/Baichuan-13B-Instruction\")<br>messages = []<br>messages.append({\"\
          role\": \"Human\", \"content\": \"\u4E16\u754C\u4E0A\u7B2C\u4E8C\u9AD8\u7684\
          \u5C71\u5CF0\u662F\u54EA\u5EA7\"})<br>response = model.chat(tokenizer, messages)<br>print(response)<br>'''</p>\n\
          <p>I  did not check the  efficiency of original model, so to rethink if\
          \ it is based on quantization or the model in overall return rubish instruct.\
          \ ^^</p>\n"
        raw: "Hi, did you try to use another prompt template? Based on Alpachino implementation,\
          \ example input, instruct should look like this: \n\n'''python\nimport torch\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils\
          \ import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"\
          AlpachinoNLP/Baichuan-13B-Instruction\", use_fast=False, trust_remote_code=True)\n\
          model = AutoModelForCausalLM.from_pretrained(\"AlpachinoNLP/Baichuan-13B-Instruction\"\
          , device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n\
          model.generation_config = GenerationConfig.from_pretrained(\"AlpachinoNLP/Baichuan-13B-Instruction\"\
          )\nmessages = []\nmessages.append({\"role\": \"Human\", \"content\": \"\u4E16\
          \u754C\u4E0A\u7B2C\u4E8C\u9AD8\u7684\u5C71\u5CF0\u662F\u54EA\u5EA7\"})\n\
          response = model.chat(tokenizer, messages)\nprint(response)\n'''\n\nI  did\
          \ not check the  efficiency of original model, so to rethink if it is based\
          \ on quantization or the model in overall return rubish instruct. ^^"
        updatedAt: '2023-07-27T12:08:32.698Z'
      numEdits: 0
      reactions: []
    id: 64c25e4042396fe0bc235473
    type: comment
  author: s3nh
  content: "Hi, did you try to use another prompt template? Based on Alpachino implementation,\
    \ example input, instruct should look like this: \n\n'''python\nimport torch\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils\
    \ import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"AlpachinoNLP/Baichuan-13B-Instruction\"\
    , use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    AlpachinoNLP/Baichuan-13B-Instruction\", device_map=\"auto\", torch_dtype=torch.float16,\
    \ trust_remote_code=True)\nmodel.generation_config = GenerationConfig.from_pretrained(\"\
    AlpachinoNLP/Baichuan-13B-Instruction\")\nmessages = []\nmessages.append({\"role\"\
    : \"Human\", \"content\": \"\u4E16\u754C\u4E0A\u7B2C\u4E8C\u9AD8\u7684\u5C71\u5CF0\
    \u662F\u54EA\u5EA7\"})\nresponse = model.chat(tokenizer, messages)\nprint(response)\n\
    '''\n\nI  did not check the  efficiency of original model, so to rethink if it\
    \ is based on quantization or the model in overall return rubish instruct. ^^"
  created_at: 2023-07-27 11:08:32+00:00
  edited: false
  hidden: false
  id: 64c25e4042396fe0bc235473
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-27T12:24:05.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7454276084899902
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>I  built simple space here <a href="https://huggingface.co/spaces/s3nh/Baichuan-13B-Instruction-GGML">https://huggingface.co/spaces/s3nh/Baichuan-13B-Instruction-GGML</a>
          and after simple tests I can confirm that it generate not efficient  prompts.
          </p>

          '
        raw: 'I  built simple space here https://huggingface.co/spaces/s3nh/Baichuan-13B-Instruction-GGML
          and after simple tests I can confirm that it generate not efficient  prompts. '
        updatedAt: '2023-07-27T12:24:05.751Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - WindyGleam
    id: 64c261e58a39ef0b5b359c23
    type: comment
  author: s3nh
  content: 'I  built simple space here https://huggingface.co/spaces/s3nh/Baichuan-13B-Instruction-GGML
    and after simple tests I can confirm that it generate not efficient  prompts. '
  created_at: 2023-07-27 11:24:05+00:00
  edited: false
  hidden: false
  id: 64c261e58a39ef0b5b359c23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
      fullname: WoolCool
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WindyGleam
      type: user
    createdAt: '2023-07-28T04:44:38.000Z'
    data:
      edited: false
      editors:
      - WindyGleam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9767885804176331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
          fullname: WoolCool
          isHf: false
          isPro: false
          name: WindyGleam
          type: user
        html: '<p>I tested with the space, it seems to produce gibberish outputs as
          well, it might be the problem with the original model</p>

          '
        raw: I tested with the space, it seems to produce gibberish outputs as well,
          it might be the problem with the original model
        updatedAt: '2023-07-28T04:44:38.167Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - s3nh
    id: 64c347b6e9e1506c56713461
    type: comment
  author: WindyGleam
  content: I tested with the space, it seems to produce gibberish outputs as well,
    it might be the problem with the original model
  created_at: 2023-07-28 03:44:38+00:00
  edited: false
  hidden: false
  id: 64c347b6e9e1506c56713461
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
      fullname: WoolCool
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WindyGleam
      type: user
    createdAt: '2023-07-28T04:45:35.000Z'
    data:
      edited: false
      editors:
      - WindyGleam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8824147582054138
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6437b8d5fac5ea753f1d084e/445JbRMiepH0lVk036PtZ.jpeg?w=200&h=200&f=face
          fullname: WoolCool
          isHf: false
          isPro: false
          name: WindyGleam
          type: user
        html: '<p>Thanks for the quantization </p>

          '
        raw: 'Thanks for the quantization '
        updatedAt: '2023-07-28T04:45:35.941Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - s3nh
    id: 64c347efe6932d5453ea1ffb
    type: comment
  author: WindyGleam
  content: 'Thanks for the quantization '
  created_at: 2023-07-28 03:45:35+00:00
  edited: false
  hidden: false
  id: 64c347efe6932d5453ea1ffb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: s3nh/Baichuan-13B-Instruction-GGML
repo_type: model
status: open
target_branch: null
title: "\u751F\u6210\u4F3C\u4E4E\u662F\u4E71\u7801"
