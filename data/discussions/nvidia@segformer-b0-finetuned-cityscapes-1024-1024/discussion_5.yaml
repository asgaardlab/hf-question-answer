!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hporta
conflicting_files: null
created_at: 2023-07-04 07:40:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ae33bf696d0be252eb11aa7a3f63e9d.svg
      fullname: Hugo Porta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hporta
      type: user
    createdAt: '2023-07-04T08:40:56.000Z'
    data:
      edited: false
      editors:
      - hporta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6195942759513855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ae33bf696d0be252eb11aa7a3f63e9d.svg
          fullname: Hugo Porta
          isHf: false
          isPro: false
          name: hporta
          type: user
        html: "<p>First, thanks for sharing the weights. I have difficulties replicating\
          \ (or even approaching) the Segformer paper performance on cityscapes. The\
          \ first issue I observed is that from B0 to B5 Image Processor only B1 and\
          \ B5 resize the input image to 1024<em>1024 and the rest to 512</em>512,\
          \ as you can see in the attached screenshot.</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/64a3d5d51cbf675203052969/_hS843gznqKkPMoZSzoxI.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64a3d5d51cbf675203052969/_hS843gznqKkPMoZSzoxI.png\"\
          ></a></p>\n<p>Moreover, with a standard pytorch Dataset (see below using\
          \ the LabelTrainingIds after running cityscapesscripts) loading the image\
          \ and label as PIL image and passing both through the image processor I\
          \ obtain an mIoU of 58 on cityscapes validation.</p>\n<p>When using a custom\
          \ albumentation transform pipeline of Resize(1024, 1024) , Normalize, ToTensorV2\
          \ I improve the result to mean IoU 68 still far from the 76.2 in the paper.</p>\n\
          <p>From the paper and mmsegmentation the test was done via 1024*1024 sliding\
          \ window with stride 768. Were the result replicated here with this type\
          \ of inference, it is unclear from the image processor attributes ? Is there\
          \ an available implementation of the Cityscapes inference pipeline with\
          \ this model implementation ?</p>\n<p>If not what was the achieved results\
          \ and the used pre-processing pipeline ?</p>\n<p>CityScapes Dataset:</p>\n\
          <p>class CityscapesDataset(Dataset):<br>    \"\"\"CityScapes Dataset from\
          \ Raw Data\"\"\"</p>\n<pre><code>def __init__(\n    self,\n    root_dir:\
          \ os.PathLike,\n    image_processor: Optional[BaseImageProcessor] = None,\n\
          \    transform: Optional[A.Compose] = None,\n    split: str = \"train\"\
          ,\n):  # TODO: we could specify a callable for transform\n    \"\"\"Initialize\
          \ the dataset Object.\n\n    Args:\n        root_dir (os.PathLike): Local\
          \ path to raw data.\n        image_processor (Optional[BaseImageProcessor],\
          \ optional): HuggingFace Image Processor. Defaults to None.\n        transform\
          \ (Optional[A.Compose], optional): Set of transforms for processing images\
          \ and masks. Defaults to None.\n        split (str, optional): Dataset split\
          \ to load. Defaults to \"train\".\n    \"\"\"\n    self.root_dir = root_dir\n\
          \    self.image_processor = image_processor\n    self.split = split\n  \
          \  self.transform = transform\n\n    self.images_dir = self.root_dir / \"\
          leftImg8bit\" / self.split\n    self.labels_dir = self.root_dir / \"gtFine\"\
          \ / self.split\n\n    self.images = []\n    self.labels = []\n    self.ids\
          \ = []\n\n    for city in self.images_dir.iterdir():\n        for image_path\
          \ in city.iterdir():\n            self.images.append(image_path)\n     \
          \       self.labels.append(\n                self.labels_dir / city.name\
          \ / image_path.name.replace(\"leftImg8bit\", \"gtFine_labelTrainIds\")\n\
          \            )\n            self.ids.append(image_path.name.replace(\"_leftImg8bit.png\"\
          , \"\"))\n\ndef __len__(self) -&gt; int:\n    return len(self.images)\n\n\
          def __getitem__(self, idx) -&gt; Dict[str, torch.Tensor]:\n    image = Image.open(self.images[idx]).convert(\"\
          RGB\")\n    label = Image.open(self.labels[idx])\n\n    if self.transform\
          \ is not None:\n        transformed = self.transform(image=np.asarray(image).copy(),\
          \ mask=np.asarray(label).copy())\n        image = transformed[\"image\"\
          ]\n        label = transformed[\"mask\"]\n\n    if self.image_processor\
          \ is not None:\n        encoded_inputs = {}\n        process_inputs = self.image_processor.preprocess(images=image,\
          \ segmentation_maps=label, return_tensors=\"pt\")\n        for k, _ in process_inputs.items():\n\
          \            encoded_inputs[k] = process_inputs[k].squeeze()  # remove batch\
          \ dimension\n\n    else:\n        encoded_inputs = {\"pixel_values\": image,\
          \ \"labels\": label}\n\n    encoded_inputs[\"id\"] = self.ids[idx]\n   \
          \ # encoded_inputs[\"labels\"].apply_(lambda x: CityscapesDataset.mapping_ids[x])\n\
          \n    return encoded_inputs\n</code></pre>\n"
        raw: "First, thanks for sharing the weights. I have difficulties replicating\
          \ (or even approaching) the Segformer paper performance on cityscapes. The\
          \ first issue I observed is that from B0 to B5 Image Processor only B1 and\
          \ B5 resize the input image to 1024*1024 and the rest to 512*512, as you\
          \ can see in the attached screenshot.\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64a3d5d51cbf675203052969/_hS843gznqKkPMoZSzoxI.png)\r\
          \n\r\n\r\nMoreover, with a standard pytorch Dataset (see below using the\
          \ LabelTrainingIds after running cityscapesscripts) loading the image and\
          \ label as PIL image and passing both through the image processor I obtain\
          \ an mIoU of 58 on cityscapes validation.\r\n\r\nWhen using a custom albumentation\
          \ transform pipeline of Resize(1024, 1024) , Normalize, ToTensorV2 I improve\
          \ the result to mean IoU 68 still far from the 76.2 in the paper.\r\n\r\n\
          From the paper and mmsegmentation the test was done via 1024*1024 sliding\
          \ window with stride 768. Were the result replicated here with this type\
          \ of inference, it is unclear from the image processor attributes ? Is there\
          \ an available implementation of the Cityscapes inference pipeline with\
          \ this model implementation ?\r\n\r\nIf not what was the achieved results\
          \ and the used pre-processing pipeline ?\r\n\r\nCityScapes Dataset:\r\n\r\
          \nclass CityscapesDataset(Dataset):\r\n    \"\"\"CityScapes Dataset from\
          \ Raw Data\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        root_dir:\
          \ os.PathLike,\r\n        image_processor: Optional[BaseImageProcessor]\
          \ = None,\r\n        transform: Optional[A.Compose] = None,\r\n        split:\
          \ str = \"train\",\r\n    ):  # TODO: we could specify a callable for transform\r\
          \n        \"\"\"Initialize the dataset Object.\r\n\r\n        Args:\r\n\
          \            root_dir (os.PathLike): Local path to raw data.\r\n       \
          \     image_processor (Optional[BaseImageProcessor], optional): HuggingFace\
          \ Image Processor. Defaults to None.\r\n            transform (Optional[A.Compose],\
          \ optional): Set of transforms for processing images and masks. Defaults\
          \ to None.\r\n            split (str, optional): Dataset split to load.\
          \ Defaults to \"train\".\r\n        \"\"\"\r\n        self.root_dir = root_dir\r\
          \n        self.image_processor = image_processor\r\n        self.split =\
          \ split\r\n        self.transform = transform\r\n\r\n        self.images_dir\
          \ = self.root_dir / \"leftImg8bit\" / self.split\r\n        self.labels_dir\
          \ = self.root_dir / \"gtFine\" / self.split\r\n\r\n        self.images =\
          \ []\r\n        self.labels = []\r\n        self.ids = []\r\n\r\n      \
          \  for city in self.images_dir.iterdir():\r\n            for image_path\
          \ in city.iterdir():\r\n                self.images.append(image_path)\r\
          \n                self.labels.append(\r\n                    self.labels_dir\
          \ / city.name / image_path.name.replace(\"leftImg8bit\", \"gtFine_labelTrainIds\"\
          )\r\n                )\r\n                self.ids.append(image_path.name.replace(\"\
          _leftImg8bit.png\", \"\"))\r\n\r\n    def __len__(self) -> int:\r\n    \
          \    return len(self.images)\r\n\r\n    def __getitem__(self, idx) -> Dict[str,\
          \ torch.Tensor]:\r\n        image = Image.open(self.images[idx]).convert(\"\
          RGB\")\r\n        label = Image.open(self.labels[idx])\r\n\r\n        if\
          \ self.transform is not None:\r\n            transformed = self.transform(image=np.asarray(image).copy(),\
          \ mask=np.asarray(label).copy())\r\n            image = transformed[\"image\"\
          ]\r\n            label = transformed[\"mask\"]\r\n\r\n        if self.image_processor\
          \ is not None:\r\n            encoded_inputs = {}\r\n            process_inputs\
          \ = self.image_processor.preprocess(images=image, segmentation_maps=label,\
          \ return_tensors=\"pt\")\r\n            for k, _ in process_inputs.items():\r\
          \n                encoded_inputs[k] = process_inputs[k].squeeze()  # remove\
          \ batch dimension\r\n\r\n        else:\r\n            encoded_inputs = {\"\
          pixel_values\": image, \"labels\": label}\r\n\r\n        encoded_inputs[\"\
          id\"] = self.ids[idx]\r\n        # encoded_inputs[\"labels\"].apply_(lambda\
          \ x: CityscapesDataset.mapping_ids[x])\r\n\r\n        return encoded_inputs"
        updatedAt: '2023-07-04T08:40:56.175Z'
      numEdits: 0
      reactions: []
    id: 64a3db186badc2ebd1e7db28
    type: comment
  author: hporta
  content: "First, thanks for sharing the weights. I have difficulties replicating\
    \ (or even approaching) the Segformer paper performance on cityscapes. The first\
    \ issue I observed is that from B0 to B5 Image Processor only B1 and B5 resize\
    \ the input image to 1024*1024 and the rest to 512*512, as you can see in the\
    \ attached screenshot.\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64a3d5d51cbf675203052969/_hS843gznqKkPMoZSzoxI.png)\r\
    \n\r\n\r\nMoreover, with a standard pytorch Dataset (see below using the LabelTrainingIds\
    \ after running cityscapesscripts) loading the image and label as PIL image and\
    \ passing both through the image processor I obtain an mIoU of 58 on cityscapes\
    \ validation.\r\n\r\nWhen using a custom albumentation transform pipeline of Resize(1024,\
    \ 1024) , Normalize, ToTensorV2 I improve the result to mean IoU 68 still far\
    \ from the 76.2 in the paper.\r\n\r\nFrom the paper and mmsegmentation the test\
    \ was done via 1024*1024 sliding window with stride 768. Were the result replicated\
    \ here with this type of inference, it is unclear from the image processor attributes\
    \ ? Is there an available implementation of the Cityscapes inference pipeline\
    \ with this model implementation ?\r\n\r\nIf not what was the achieved results\
    \ and the used pre-processing pipeline ?\r\n\r\nCityScapes Dataset:\r\n\r\nclass\
    \ CityscapesDataset(Dataset):\r\n    \"\"\"CityScapes Dataset from Raw Data\"\"\
    \"\r\n\r\n    def __init__(\r\n        self,\r\n        root_dir: os.PathLike,\r\
    \n        image_processor: Optional[BaseImageProcessor] = None,\r\n        transform:\
    \ Optional[A.Compose] = None,\r\n        split: str = \"train\",\r\n    ):  #\
    \ TODO: we could specify a callable for transform\r\n        \"\"\"Initialize\
    \ the dataset Object.\r\n\r\n        Args:\r\n            root_dir (os.PathLike):\
    \ Local path to raw data.\r\n            image_processor (Optional[BaseImageProcessor],\
    \ optional): HuggingFace Image Processor. Defaults to None.\r\n            transform\
    \ (Optional[A.Compose], optional): Set of transforms for processing images and\
    \ masks. Defaults to None.\r\n            split (str, optional): Dataset split\
    \ to load. Defaults to \"train\".\r\n        \"\"\"\r\n        self.root_dir =\
    \ root_dir\r\n        self.image_processor = image_processor\r\n        self.split\
    \ = split\r\n        self.transform = transform\r\n\r\n        self.images_dir\
    \ = self.root_dir / \"leftImg8bit\" / self.split\r\n        self.labels_dir =\
    \ self.root_dir / \"gtFine\" / self.split\r\n\r\n        self.images = []\r\n\
    \        self.labels = []\r\n        self.ids = []\r\n\r\n        for city in\
    \ self.images_dir.iterdir():\r\n            for image_path in city.iterdir():\r\
    \n                self.images.append(image_path)\r\n                self.labels.append(\r\
    \n                    self.labels_dir / city.name / image_path.name.replace(\"\
    leftImg8bit\", \"gtFine_labelTrainIds\")\r\n                )\r\n            \
    \    self.ids.append(image_path.name.replace(\"_leftImg8bit.png\", \"\"))\r\n\r\
    \n    def __len__(self) -> int:\r\n        return len(self.images)\r\n\r\n   \
    \ def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\r\n        image = Image.open(self.images[idx]).convert(\"\
    RGB\")\r\n        label = Image.open(self.labels[idx])\r\n\r\n        if self.transform\
    \ is not None:\r\n            transformed = self.transform(image=np.asarray(image).copy(),\
    \ mask=np.asarray(label).copy())\r\n            image = transformed[\"image\"\
    ]\r\n            label = transformed[\"mask\"]\r\n\r\n        if self.image_processor\
    \ is not None:\r\n            encoded_inputs = {}\r\n            process_inputs\
    \ = self.image_processor.preprocess(images=image, segmentation_maps=label, return_tensors=\"\
    pt\")\r\n            for k, _ in process_inputs.items():\r\n                encoded_inputs[k]\
    \ = process_inputs[k].squeeze()  # remove batch dimension\r\n\r\n        else:\r\
    \n            encoded_inputs = {\"pixel_values\": image, \"labels\": label}\r\n\
    \r\n        encoded_inputs[\"id\"] = self.ids[idx]\r\n        # encoded_inputs[\"\
    labels\"].apply_(lambda x: CityscapesDataset.mapping_ids[x])\r\n\r\n        return\
    \ encoded_inputs"
  created_at: 2023-07-04 07:40:56+00:00
  edited: false
  hidden: false
  id: 64a3db186badc2ebd1e7db28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: nvidia/segformer-b0-finetuned-cityscapes-1024-1024
repo_type: model
status: open
target_branch: null
title: Performance & Image Processor
