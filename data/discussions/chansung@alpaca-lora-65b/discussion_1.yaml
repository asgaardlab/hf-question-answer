!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-04-19 10:00:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-19T11:00:58.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: "<p>I wonder if this thing can run on a beefy server cpu once quantized.\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> What do you think,\
          \ does this thing run on CPU in 4bit at useful speeds? </p>\n"
        raw: 'I wonder if this thing can run on a beefy server cpu once quantized.
          @TheBloke What do you think, does this thing run on CPU in 4bit at useful
          speeds? '
        updatedAt: '2023-04-19T11:00:58.828Z'
      numEdits: 0
      reactions: []
    id: 643fc9ea388f31f64ccb1e77
    type: comment
  author: KnutJaegersberg
  content: 'I wonder if this thing can run on a beefy server cpu once quantized. @TheBloke
    What do you think, does this thing run on CPU in 4bit at useful speeds? '
  created_at: 2023-04-19 10:00:58+00:00
  edited: false
  hidden: false
  id: 643fc9ea388f31f64ccb1e77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-19T11:38:17.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Ah I know... @Camelidae you have quantized llama 65 even to 2 bit.
          What are your experiences, performance wise? When does quantization do too
          much harm? On what hardware do you run the 3 and 4 bit versions?</p>

          '
        raw: Ah I know... @Camelidae you have quantized llama 65 even to 2 bit. What
          are your experiences, performance wise? When does quantization do too much
          harm? On what hardware do you run the 3 and 4 bit versions?
        updatedAt: '2023-04-19T11:38:17.138Z'
      numEdits: 0
      reactions: []
    id: 643fd2a92113f7dfcb4a5108
    type: comment
  author: KnutJaegersberg
  content: Ah I know... @Camelidae you have quantized llama 65 even to 2 bit. What
    are your experiences, performance wise? When does quantization do too much harm?
    On what hardware do you run the 3 and 4 bit versions?
  created_at: 2023-04-19 10:38:17+00:00
  edited: false
  hidden: false
  id: 643fd2a92113f7dfcb4a5108
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-19T11:39:31.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>tag does not work... gonna go to your page <a href="https://huggingface.co/camelids/llama-65b-ggml-q4_0">https://huggingface.co/camelids/llama-65b-ggml-q4_0</a></p>

          '
        raw: tag does not work... gonna go to your page https://huggingface.co/camelids/llama-65b-ggml-q4_0
        updatedAt: '2023-04-19T11:39:31.312Z'
      numEdits: 0
      reactions: []
    id: 643fd2f3dbd88206a832707f
    type: comment
  author: KnutJaegersberg
  content: tag does not work... gonna go to your page https://huggingface.co/camelids/llama-65b-ggml-q4_0
  created_at: 2023-04-19 10:39:31+00:00
  edited: false
  hidden: false
  id: 643fd2f3dbd88206a832707f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-19T11:44:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m having a look at it now, will let you know how I get in</p>

          '
        raw: I'm having a look at it now, will let you know how I get in
        updatedAt: '2023-04-19T11:44:18.665Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - KnutJaegersberg
    id: 643fd412dbd88206a832913d
    type: comment
  author: TheBloke
  content: I'm having a look at it now, will let you know how I get in
  created_at: 2023-04-19 10:44:18+00:00
  edited: false
  hidden: false
  id: 643fd412dbd88206a832913d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-19T23:23:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, here are GGML 4bit and 2bit quantised versions: <a href="https://huggingface.co/TheBloke/alpaca-lora-65B-GGML">https://huggingface.co/TheBloke/alpaca-lora-65B-GGML</a></p>

          <p>I had some problems making GPTQs due to my attempts killing the Runpod
          host I was on :)  I got one GPTQ made, then the host died and I couldn''t
          get back in to access the file I''d saved. I didn''t want to start from
          scratch, and they''ve promised to fix it for tomorrow, so I will continue
          from where I left off then.</p>

          '
        raw: 'OK, here are GGML 4bit and 2bit quantised versions: https://huggingface.co/TheBloke/alpaca-lora-65B-GGML


          I had some problems making GPTQs due to my attempts killing the Runpod host
          I was on :)  I got one GPTQ made, then the host died and I couldn''t get
          back in to access the file I''d saved. I didn''t want to start from scratch,
          and they''ve promised to fix it for tomorrow, so I will continue from where
          I left off then.'
        updatedAt: '2023-04-19T23:23:59.828Z'
      numEdits: 0
      reactions: []
    id: 6440780f194b02fd309783f1
    type: comment
  author: TheBloke
  content: 'OK, here are GGML 4bit and 2bit quantised versions: https://huggingface.co/TheBloke/alpaca-lora-65B-GGML


    I had some problems making GPTQs due to my attempts killing the Runpod host I
    was on :)  I got one GPTQ made, then the host died and I couldn''t get back in
    to access the file I''d saved. I didn''t want to start from scratch, and they''ve
    promised to fix it for tomorrow, so I will continue from where I left off then.'
  created_at: 2023-04-19 22:23:59+00:00
  edited: false
  hidden: false
  id: 6440780f194b02fd309783f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665525574730-noauth.png?w=200&h=200&f=face
      fullname: Trahloc colDhart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: trahloc
      type: user
    createdAt: '2023-04-20T01:18:38.000Z'
    data:
      edited: false
      editors:
      - trahloc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665525574730-noauth.png?w=200&h=200&f=face
          fullname: Trahloc colDhart
          isHf: false
          isPro: false
          name: trahloc
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Any suggestions\
          \ on what project to get this 2bit version working?  Oogabooga doesn't seem\
          \ to like it and I'm doing this on a remote server as my local machine doesn't\
          \ have an a100 or a decent connection.  I only know how to get llama.cpp\
          \ working locally and it'll take me a day to download it so any hints would\
          \ be appreciated. I've been trying to get it working since shortly after\
          \ your upload and keep slamming my face into brick walls.</p>\n"
        raw: '@TheBloke Any suggestions on what project to get this 2bit version working?  Oogabooga
          doesn''t seem to like it and I''m doing this on a remote server as my local
          machine doesn''t have an a100 or a decent connection.  I only know how to
          get llama.cpp working locally and it''ll take me a day to download it so
          any hints would be appreciated. I''ve been trying to get it working since
          shortly after your upload and keep slamming my face into brick walls.'
        updatedAt: '2023-04-20T01:18:38.854Z'
      numEdits: 0
      reactions: []
    id: 644092eed4229e14aea43e1c
    type: comment
  author: trahloc
  content: '@TheBloke Any suggestions on what project to get this 2bit version working?  Oogabooga
    doesn''t seem to like it and I''m doing this on a remote server as my local machine
    doesn''t have an a100 or a decent connection.  I only know how to get llama.cpp
    working locally and it''ll take me a day to download it so any hints would be
    appreciated. I''ve been trying to get it working since shortly after your upload
    and keep slamming my face into brick walls.'
  created_at: 2023-04-20 00:18:38+00:00
  edited: false
  hidden: false
  id: 644092eed4229e14aea43e1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-20T08:22:50.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I believe that right now the only way to get the 2bit working is
          to use the <code>q2q3</code> branch of the llama.cpp fork I listed in the
          README.</p>

          <p>I imagine that sometime soon this will be merged into the main llama.cpp.
          And once it''s been merged, it''ll likely only be a few days until it''s
          supported by the various tools that interface with llama, like text-generation-webui
          and the python bindings.  If you''re a developer then I suppose it may be
          possible to merge the new q2q3 code into one of the llama.cpp interfaces
          to enable it to work as a server. But that''s not something I''ve looked
          at, or plan to look at.</p>

          <p>But right now the only simple option I know of would be to run it on
          the command line using:</p>

          <pre><code>git clone https://github.com/sw/llama.cpp llama-q2q3

          cd llama-q2q3

          git checkout q2q3

          make

          ./main ...

          </code></pre>

          <p>If that doesn''t work for you then I''d suggest just waiting. Once the
          q2 code is merged things will be easier. And it''s quite possible that by
          that time, a better 65B model will be available anyway :)</p>

          '
        raw: 'I believe that right now the only way to get the 2bit working is to
          use the `q2q3` branch of the llama.cpp fork I listed in the README.


          I imagine that sometime soon this will be merged into the main llama.cpp.
          And once it''s been merged, it''ll likely only be a few days until it''s
          supported by the various tools that interface with llama, like text-generation-webui
          and the python bindings.  If you''re a developer then I suppose it may be
          possible to merge the new q2q3 code into one of the llama.cpp interfaces
          to enable it to work as a server. But that''s not something I''ve looked
          at, or plan to look at.


          But right now the only simple option I know of would be to run it on the
          command line using:

          ```

          git clone https://github.com/sw/llama.cpp llama-q2q3

          cd llama-q2q3

          git checkout q2q3

          make

          ./main ...

          ```


          If that doesn''t work for you then I''d suggest just waiting. Once the q2
          code is merged things will be easier. And it''s quite possible that by that
          time, a better 65B model will be available anyway :)'
        updatedAt: '2023-04-20T08:23:09.529Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - trahloc
    id: 6440f65a7f13a7b5a257b5ab
    type: comment
  author: TheBloke
  content: 'I believe that right now the only way to get the 2bit working is to use
    the `q2q3` branch of the llama.cpp fork I listed in the README.


    I imagine that sometime soon this will be merged into the main llama.cpp. And
    once it''s been merged, it''ll likely only be a few days until it''s supported
    by the various tools that interface with llama, like text-generation-webui and
    the python bindings.  If you''re a developer then I suppose it may be possible
    to merge the new q2q3 code into one of the llama.cpp interfaces to enable it to
    work as a server. But that''s not something I''ve looked at, or plan to look at.


    But right now the only simple option I know of would be to run it on the command
    line using:

    ```

    git clone https://github.com/sw/llama.cpp llama-q2q3

    cd llama-q2q3

    git checkout q2q3

    make

    ./main ...

    ```


    If that doesn''t work for you then I''d suggest just waiting. Once the q2 code
    is merged things will be easier. And it''s quite possible that by that time, a
    better 65B model will be available anyway :)'
  created_at: 2023-04-20 07:22:50+00:00
  edited: true
  hidden: false
  id: 6440f65a7f13a7b5a257b5ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-20T12:01:15.000Z'
    data:
      status: closed
    id: 6441298b7f13a7b5a25f0985
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-04-20 11:01:15+00:00
  id: 6441298b7f13a7b5a25f0985
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chansung/alpaca-lora-65b
repo_type: model
status: closed
target_branch: null
title: 4bit version
