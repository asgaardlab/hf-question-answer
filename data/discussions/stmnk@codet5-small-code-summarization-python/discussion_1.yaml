!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mongoose54
conflicting_files: null
created_at: 2022-10-21 06:25:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/715a1aebb6057cb3967dc69b24ac5272.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mongoose54
      type: user
    createdAt: '2022-10-21T07:25:44.000Z'
    data:
      edited: false
      editors:
      - mongoose54
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/715a1aebb6057cb3967dc69b24ac5272.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: mongoose54
          type: user
        html: '<p>Is there a quick example to use this pretrained model? </p>

          '
        raw: 'Is there a quick example to use this pretrained model? '
        updatedAt: '2022-10-21T07:25:44.497Z'
      numEdits: 0
      reactions: []
    id: 635249787e4cc3135fcff1c6
    type: comment
  author: mongoose54
  content: 'Is there a quick example to use this pretrained model? '
  created_at: 2022-10-21 06:25:44+00:00
  edited: false
  hidden: false
  id: 635249787e4cc3135fcff1c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e00da23259e6f8f9fca9e281b7f2f58.svg
      fullname: Stefan Minica
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: stmnk
      type: user
    createdAt: '2022-10-21T09:11:12.000Z'
    data:
      edited: false
      editors:
      - stmnk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e00da23259e6f8f9fca9e281b7f2f58.svg
          fullname: Stefan Minica
          isHf: false
          isPro: false
          name: stmnk
          type: user
        html: '<p>see this: <a href="https://huggingface.co/spaces/stmnk/pygen">https://huggingface.co/spaces/stmnk/pygen</a></p>

          '
        raw: 'see this: https://huggingface.co/spaces/stmnk/pygen'
        updatedAt: '2022-10-21T09:11:12.891Z'
      numEdits: 0
      reactions: []
    id: 63526230b3678a43742c8f5c
    type: comment
  author: stmnk
  content: 'see this: https://huggingface.co/spaces/stmnk/pygen'
  created_at: 2022-10-21 08:11:12+00:00
  edited: false
  hidden: false
  id: 63526230b3678a43742c8f5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/715a1aebb6057cb3967dc69b24ac5272.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mongoose54
      type: user
    createdAt: '2022-10-21T11:35:17.000Z'
    data:
      edited: false
      editors:
      - mongoose54
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/715a1aebb6057cb3967dc69b24ac5272.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: mongoose54
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;stmnk&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stmnk\"\
          >@<span class=\"underline\">stmnk</span></a></span>\n\n\t</span></span>\
          \  for sharing. I meant an API example  (i.e how to use it within Python).\
          \ Thanks</p>\n"
        raw: Thanks @stmnk  for sharing. I meant an API example  (i.e how to use it
          within Python). Thanks
        updatedAt: '2022-10-21T11:35:17.034Z'
      numEdits: 0
      reactions: []
    id: 635283f5d6e8b48009b132cd
    type: comment
  author: mongoose54
  content: Thanks @stmnk  for sharing. I meant an API example  (i.e how to use it
    within Python). Thanks
  created_at: 2022-10-21 10:35:17+00:00
  edited: false
  hidden: false
  id: 635283f5d6e8b48009b132cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e00da23259e6f8f9fca9e281b7f2f58.svg
      fullname: Stefan Minica
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: stmnk
      type: user
    createdAt: '2022-12-17T13:58:06.000Z'
    data:
      edited: true
      editors:
      - stmnk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e00da23259e6f8f9fca9e281b7f2f58.svg
          fullname: Stefan Minica
          isHf: false
          isPro: false
          name: stmnk
          type: user
        html: "<p>the link from my previous comment has a rest API call example in\
          \ the <code>app.py</code> file (under the \"Files and versions\" tab):<br>alternatively,\
          \ you can use this url: <a href=\"https://huggingface.co/spaces/stmnk/pygen/blob/main/app.py\"\
          >https://huggingface.co/spaces/stmnk/pygen/blob/main/app.py</a></p>\n<p>the\
          \ list of parameters is around line 160+, in principle, is the same as the\
          \ arguments accepted by the parent model checkpoint (T5-code): temperature,\
          \ repetition penalty, tokens in output, etc. </p>\n<p>I include below the\
          \ function definition, with descriptions for all params (for reference):</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">docgen_func</span>(<span class=\"\
          hljs-params\">function_code, min_length, max_length, top_k, top_p, temp,\
          \ repetition_penalty</span>):\n    m, M, k, p, t, r = <span class=\"hljs-built_in\"\
          >int</span>(min_length), <span class=\"hljs-built_in\">int</span>(max_length),\
          \ <span class=\"hljs-built_in\">int</span>(top_k), <span class=\"hljs-built_in\"\
          >float</span>(top_p/<span class=\"hljs-number\">100</span>), <span class=\"\
          hljs-built_in\">float</span>(temp), <span class=\"hljs-built_in\">float</span>(repetition_penalty)\n\
          \    req_data = {\n      <span class=\"hljs-string\">\"inputs\"</span>:\
          \ function_code,\n      <span class=\"hljs-string\">\"parameters\"</span>:\
          \ {\n        <span class=\"hljs-string\">\"min_length\"</span>: m,   <span\
          \ class=\"hljs-comment\"># (Default: None). Integer to define the minimum\
          \ length in tokens of the output summary.</span>\n        <span class=\"\
          hljs-string\">\"max_length\"</span>: M,   <span class=\"hljs-comment\">#\
          \ (Default: None). Integer to define the maximum length in tokens of the\
          \ output summary.</span>\n        <span class=\"hljs-string\">\"top_k\"\
          </span>: k,        <span class=\"hljs-comment\"># (Default: None). Integer\
          \ to define the top tokens considered within the sample operation to create\
          \ new text.</span>\n        <span class=\"hljs-string\">\"top_p\"</span>:\
          \ p,        <span class=\"hljs-comment\"># (Default: None). Float to define\
          \ the tokens that are within the sample` operation of text generation. </span>\n\
          \                           <span class=\"hljs-comment\"># Add tokens in\
          \ the sample for more probable to least probable until the sum of the probabilities\
          \ is greater than top_p.</span>\n        <span class=\"hljs-string\">\"\
          temperature\"</span>: t,  <span class=\"hljs-comment\"># (Default: 1.0).\
          \ Float (0.0-100.0). The temperature of the sampling operation. </span>\n\
          \                           <span class=\"hljs-comment\"># 1 means regular\
          \ sampling, 0 means top_k=1, 100.0 is getting closer to uniform probability.</span>\n\
          \        <span class=\"hljs-string\">\"repetition_penalty\"</span>: r, <span\
          \ class=\"hljs-comment\"># (Default: None). Float (0.0-100.0). The more\
          \ a token is used within generation </span>\n                          \
          \       <span class=\"hljs-comment\"># the more it is penalized to not be\
          \ picked in successive generation passes.</span>\n        <span class=\"\
          hljs-string\">\"max_time\"</span>: <span class=\"hljs-number\">80</span>,\
          \    <span class=\"hljs-comment\"># (Default: None). Float (0-120.0). The\
          \ amount of time in seconds that the query should take maximum. </span>\n\
          \                           <span class=\"hljs-comment\"># Network can cause\
          \ some overhead so it will be a soft limit.</span>\n      },\n      <span\
          \ class=\"hljs-string\">\"options\"</span>: {\n        <span class=\"hljs-string\"\
          >\"use_gpu\"</span>: <span class=\"hljs-literal\">False</span>,        <span\
          \ class=\"hljs-comment\"># (Default: false). Boolean to use GPU instead\
          \ of CPU for inference (requires Startup plan at least)</span>\n       \
          \ <span class=\"hljs-string\">\"use_cache\"</span>: <span class=\"hljs-literal\"\
          >True</span>,       <span class=\"hljs-comment\"># (Default: true). Boolean.\
          \ There is a cache layer on the inference API to speedup requests we have\
          \ already seen. Most models can use those results as is as models are deterministic\
          \ (meaning the results will be the same anyway). However if you use a non\
          \ deterministic model, you can set this parameter to prevent the caching\
          \ mechanism from being used resulting in a real new query.</span>\n    \
          \    <span class=\"hljs-string\">\"wait_for_model\"</span>: <span class=\"\
          hljs-literal\">False</span>, <span class=\"hljs-comment\"># (Default: false)\
          \ Boolean. If the model is not ready, wait for it instead of receiving 503.\
          \ It limits the number of requests required to get your inference done.\
          \ It is advised to only set this flag to true after receiving a 503 error\
          \ as it will limit hanging in your application to known places.</span>\n\
          \      }\n    }\n    output = query(req_data)\n</code></pre>\n"
        raw: "the link from my previous comment has a rest API call example in the\
          \ `app.py` file (under the \"Files and versions\" tab):\nalternatively,\
          \ you can use this url: https://huggingface.co/spaces/stmnk/pygen/blob/main/app.py\n\
          \nthe list of parameters is around line 160+, in principle, is the same\
          \ as the arguments accepted by the parent model checkpoint (T5-code): temperature,\
          \ repetition penalty, tokens in output, etc. \n\nI include below the function\
          \ definition, with descriptions for all params (for reference):\n\n```python\n\
          def docgen_func(function_code, min_length, max_length, top_k, top_p, temp,\
          \ repetition_penalty):\n    m, M, k, p, t, r = int(min_length), int(max_length),\
          \ int(top_k), float(top_p/100), float(temp), float(repetition_penalty)\n\
          \    req_data = {\n      \"inputs\": function_code,\n      \"parameters\"\
          : {\n        \"min_length\": m,   # (Default: None). Integer to define the\
          \ minimum length in tokens of the output summary.\n        \"max_length\"\
          : M,   # (Default: None). Integer to define the maximum length in tokens\
          \ of the output summary.\n        \"top_k\": k,        # (Default: None).\
          \ Integer to define the top tokens considered within the sample operation\
          \ to create new text.\n        \"top_p\": p,        # (Default: None). Float\
          \ to define the tokens that are within the sample` operation of text generation.\
          \ \n                           # Add tokens in the sample for more probable\
          \ to least probable until the sum of the probabilities is greater than top_p.\n\
          \        \"temperature\": t,  # (Default: 1.0). Float (0.0-100.0). The temperature\
          \ of the sampling operation. \n                           # 1 means regular\
          \ sampling, 0 means top_k=1, 100.0 is getting closer to uniform probability.\n\
          \        \"repetition_penalty\": r, # (Default: None). Float (0.0-100.0).\
          \ The more a token is used within generation \n                        \
          \         # the more it is penalized to not be picked in successive generation\
          \ passes.\n        \"max_time\": 80,    # (Default: None). Float (0-120.0).\
          \ The amount of time in seconds that the query should take maximum. \n \
          \                          # Network can cause some overhead so it will\
          \ be a soft limit.\n      },\n      \"options\": {\n        \"use_gpu\"\
          : False,        # (Default: false). Boolean to use GPU instead of CPU for\
          \ inference (requires Startup plan at least)\n        \"use_cache\": True,\
          \       # (Default: true). Boolean. There is a cache layer on the inference\
          \ API to speedup requests we have already seen. Most models can use those\
          \ results as is as models are deterministic (meaning the results will be\
          \ the same anyway). However if you use a non deterministic model, you can\
          \ set this parameter to prevent the caching mechanism from being used resulting\
          \ in a real new query.\n        \"wait_for_model\": False, # (Default: false)\
          \ Boolean. If the model is not ready, wait for it instead of receiving 503.\
          \ It limits the number of requests required to get your inference done.\
          \ It is advised to only set this flag to true after receiving a 503 error\
          \ as it will limit hanging in your application to known places.\n      }\n\
          \    }\n    output = query(req_data)\n```"
        updatedAt: '2022-12-17T13:59:26.621Z'
      numEdits: 1
      reactions: []
    id: 639dcaee727066701117b44e
    type: comment
  author: stmnk
  content: "the link from my previous comment has a rest API call example in the `app.py`\
    \ file (under the \"Files and versions\" tab):\nalternatively, you can use this\
    \ url: https://huggingface.co/spaces/stmnk/pygen/blob/main/app.py\n\nthe list\
    \ of parameters is around line 160+, in principle, is the same as the arguments\
    \ accepted by the parent model checkpoint (T5-code): temperature, repetition penalty,\
    \ tokens in output, etc. \n\nI include below the function definition, with descriptions\
    \ for all params (for reference):\n\n```python\ndef docgen_func(function_code,\
    \ min_length, max_length, top_k, top_p, temp, repetition_penalty):\n    m, M,\
    \ k, p, t, r = int(min_length), int(max_length), int(top_k), float(top_p/100),\
    \ float(temp), float(repetition_penalty)\n    req_data = {\n      \"inputs\":\
    \ function_code,\n      \"parameters\": {\n        \"min_length\": m,   # (Default:\
    \ None). Integer to define the minimum length in tokens of the output summary.\n\
    \        \"max_length\": M,   # (Default: None). Integer to define the maximum\
    \ length in tokens of the output summary.\n        \"top_k\": k,        # (Default:\
    \ None). Integer to define the top tokens considered within the sample operation\
    \ to create new text.\n        \"top_p\": p,        # (Default: None). Float to\
    \ define the tokens that are within the sample` operation of text generation.\
    \ \n                           # Add tokens in the sample for more probable to\
    \ least probable until the sum of the probabilities is greater than top_p.\n \
    \       \"temperature\": t,  # (Default: 1.0). Float (0.0-100.0). The temperature\
    \ of the sampling operation. \n                           # 1 means regular sampling,\
    \ 0 means top_k=1, 100.0 is getting closer to uniform probability.\n        \"\
    repetition_penalty\": r, # (Default: None). Float (0.0-100.0). The more a token\
    \ is used within generation \n                                 # the more it is\
    \ penalized to not be picked in successive generation passes.\n        \"max_time\"\
    : 80,    # (Default: None). Float (0-120.0). The amount of time in seconds that\
    \ the query should take maximum. \n                           # Network can cause\
    \ some overhead so it will be a soft limit.\n      },\n      \"options\": {\n\
    \        \"use_gpu\": False,        # (Default: false). Boolean to use GPU instead\
    \ of CPU for inference (requires Startup plan at least)\n        \"use_cache\"\
    : True,       # (Default: true). Boolean. There is a cache layer on the inference\
    \ API to speedup requests we have already seen. Most models can use those results\
    \ as is as models are deterministic (meaning the results will be the same anyway).\
    \ However if you use a non deterministic model, you can set this parameter to\
    \ prevent the caching mechanism from being used resulting in a real new query.\n\
    \        \"wait_for_model\": False, # (Default: false) Boolean. If the model is\
    \ not ready, wait for it instead of receiving 503. It limits the number of requests\
    \ required to get your inference done. It is advised to only set this flag to\
    \ true after receiving a 503 error as it will limit hanging in your application\
    \ to known places.\n      }\n    }\n    output = query(req_data)\n```"
  created_at: 2022-12-17 13:58:06+00:00
  edited: true
  hidden: false
  id: 639dcaee727066701117b44e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: stmnk/codet5-small-code-summarization-python
repo_type: model
status: open
target_branch: null
title: Example
