!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvarobartt
conflicting_files: null
created_at: 2023-12-15 08:13:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-12-15T08:13:50.000Z'
    data:
      edited: false
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9239491820335388
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: "<p>Hi here! Great job on UltraFeedback and friends \U0001F44F\U0001F3FB\
          </p>\n<p>I decided to quantize the weights using GGUF to allow more users\
          \ to use those locally within <code>llama.cpp</code> or their preferred\
          \ LLM engine, check those at <a href=\"https://huggingface.co/alvarobartt/UltraCM-13B-GGUF\"\
          >https://huggingface.co/alvarobartt/UltraCM-13B-GGUF</a></p>\n<p>I saw few\
          \ to no performance degradation on structured output generation, but it's\
          \ true that I got better results on both <code>UltraCM-13b</code> as well\
          \ as the GGUF quantized variants by removing the lines <code>### Feedback\\\
          nOverall Score:  </code> from the user prompt to the start of the assistant\
          \ prompt, so that the assistant starts with <code>### Feedback\\nOverall\
          \ Score: </code>, otherwise the prompt was not really consistent with neither\
          \ of the precisions mentioned above.</p>\n<p>Feel free to try those out!</p>\n"
        raw: "Hi here! Great job on UltraFeedback and friends \U0001F44F\U0001F3FB\
          \r\n\r\nI decided to quantize the weights using GGUF to allow more users\
          \ to use those locally within `llama.cpp` or their preferred LLM engine,\
          \ check those at https://huggingface.co/alvarobartt/UltraCM-13B-GGUF\r\n\
          \r\nI saw few to no performance degradation on structured output generation,\
          \ but it's true that I got better results on both `UltraCM-13b` as well\
          \ as the GGUF quantized variants by removing the lines `### Feedback\\nOverall\
          \ Score:  ` from the user prompt to the start of the assistant prompt, so\
          \ that the assistant starts with `### Feedback\\nOverall Score: `, otherwise\
          \ the prompt was not really consistent with neither of the precisions mentioned\
          \ above.\r\n\r\nFeel free to try those out!"
        updatedAt: '2023-12-15T08:13:50.428Z'
      numEdits: 0
      reactions: []
    id: 657c0abe9f62ed61a21e949c
    type: comment
  author: alvarobartt
  content: "Hi here! Great job on UltraFeedback and friends \U0001F44F\U0001F3FB\r\
    \n\r\nI decided to quantize the weights using GGUF to allow more users to use\
    \ those locally within `llama.cpp` or their preferred LLM engine, check those\
    \ at https://huggingface.co/alvarobartt/UltraCM-13B-GGUF\r\n\r\nI saw few to no\
    \ performance degradation on structured output generation, but it's true that\
    \ I got better results on both `UltraCM-13b` as well as the GGUF quantized variants\
    \ by removing the lines `### Feedback\\nOverall Score:  ` from the user prompt\
    \ to the start of the assistant prompt, so that the assistant starts with `###\
    \ Feedback\\nOverall Score: `, otherwise the prompt was not really consistent\
    \ with neither of the precisions mentioned above.\r\n\r\nFeel free to try those\
    \ out!"
  created_at: 2023-12-15 08:13:50+00:00
  edited: false
  hidden: false
  id: 657c0abe9f62ed61a21e949c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: openbmb/UltraCM-13b
repo_type: model
status: open
target_branch: null
title: GGUF variants of UltraCM-13b
