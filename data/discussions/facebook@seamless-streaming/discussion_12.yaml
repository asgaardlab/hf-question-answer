!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wanderer797
conflicting_files: null
created_at: 2024-01-18 07:50:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0ccab0b07a03265c0dc82342373cc76.svg
      fullname: Aayush Regmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wanderer797
      type: user
    createdAt: '2024-01-18T07:50:10.000Z'
    data:
      edited: false
      editors:
      - wanderer797
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9092413783073425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0ccab0b07a03265c0dc82342373cc76.svg
          fullname: Aayush Regmi
          isHf: false
          isPro: false
          name: wanderer797
          type: user
        html: '<p>I am facing "CUDA out of memory. Tried to allocate 20.00 MiB. GPU
          0 has a total capacty of 3.81 GiB of which 12.62 MiB is free. Including
          non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated
          memory 3.32 GiB is allocated by PyTorch, and 2.76 MiB is reserved by PyTorch
          but unallocated. If reserved but unallocated memory is large try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF" error.</p>

          <p>My GPU specs is: NVIDIA GeForce GTX 1650 Ti (4 GB memory)</p>

          <p>What is to be done in this case.</p>

          <p> Do i need to set "max_split_size_mb"? or reduce the batch size? Also
          where can I find the file where I can set split size in my system. As I
          am running on Ubuntu</p>

          '
        raw: "I am facing \"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0\
          \ has a total capacty of 3.81 GiB of which 12.62 MiB is free. Including\
          \ non-PyTorch memory, this process has 3.38 GiB memory in use. Of the allocated\
          \ memory 3.32 GiB is allocated by PyTorch, and 2.76 MiB is reserved by PyTorch\
          \ but unallocated. If reserved but unallocated memory is large try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\" error.\r\n\r\nMy GPU specs is:\
          \ NVIDIA GeForce GTX 1650 Ti (4 GB memory)\r\n\r\nWhat is to be done in\
          \ this case.\r\n\r\n Do i need to set \"max_split_size_mb\"? or reduce the\
          \ batch size? Also where can I find the file where I can set split size\
          \ in my system. As I am running on Ubuntu"
        updatedAt: '2024-01-18T07:50:10.396Z'
      numEdits: 0
      reactions: []
    id: 65a8d832eea6e7637696d777
    type: comment
  author: wanderer797
  content: "I am facing \"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has\
    \ a total capacty of 3.81 GiB of which 12.62 MiB is free. Including non-PyTorch\
    \ memory, this process has 3.38 GiB memory in use. Of the allocated memory 3.32\
    \ GiB is allocated by PyTorch, and 2.76 MiB is reserved by PyTorch but unallocated.\
    \ If reserved but unallocated memory is large try setting max_split_size_mb to\
    \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\
    \ error.\r\n\r\nMy GPU specs is: NVIDIA GeForce GTX 1650 Ti (4 GB memory)\r\n\r\
    \nWhat is to be done in this case.\r\n\r\n Do i need to set \"max_split_size_mb\"\
    ? or reduce the batch size? Also where can I find the file where I can set split\
    \ size in my system. As I am running on Ubuntu"
  created_at: 2024-01-18 07:50:10+00:00
  edited: false
  hidden: false
  id: 65a8d832eea6e7637696d777
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: facebook/seamless-streaming
repo_type: model
status: open
target_branch: null
title: Facing "Failed to build agent SeamlessStreaming" error
