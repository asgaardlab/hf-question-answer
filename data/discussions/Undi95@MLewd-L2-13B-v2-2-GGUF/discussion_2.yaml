!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Anonimus12345678902
conflicting_files: null
created_at: 2023-09-08 10:19:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73a512b64754e24668964197af831854.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anonimus12345678902
      type: user
    createdAt: '2023-09-08T11:19:40.000Z'
    data:
      edited: false
      editors:
      - Anonimus12345678902
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8589131236076355
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73a512b64754e24668964197af831854.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: Anonimus12345678902
          type: user
        html: '<p>What is the maximum context of this model? 2048? 4096? Or can absolutely
          any quantity be specified?</p>

          '
        raw: What is the maximum context of this model? 2048? 4096? Or can absolutely
          any quantity be specified?
        updatedAt: '2023-09-08T11:19:40.388Z'
      numEdits: 0
      reactions: []
    id: 64fb034c010f41e435251662
    type: comment
  author: Anonimus12345678902
  content: What is the maximum context of this model? 2048? 4096? Or can absolutely
    any quantity be specified?
  created_at: 2023-09-08 10:19:40+00:00
  edited: false
  hidden: false
  id: 64fb034c010f41e435251662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-09-08T11:40:31.000Z'
    data:
      edited: false
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8211854100227356
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: '<p>When running this model I am receiving following:<br>llm_load_print_meta:
          format         = GGUF V2 (latest)<br>llm_load_print_meta: n_ctx_train    =
          4096<br>...</p>

          <p>That means that this model performs best if context is set to 4096  .
          You are still able to extend the context window with this model though.
          </p>

          <p>I have tested extending context to 16K using rope (parameters in llama.cpp
          in case you using this software)  to summarize longer articles and it worked
          fine:<br>llm_load_print_meta: format         = GGUF V2 (latest)<br>llm_load_print_meta:
          n_ctx_train    = 4096<br>llm_load_print_meta: n_ctx          = 16384<br>....</p>

          <p>But overall, it is probably best to stick with 4K  context window to
          avoid degradation and higher memory requirements when using this model for
          higher context window utilizing rope extension. </p>

          '
        raw: "When running this model I am receiving following:\nllm_load_print_meta:\
          \ format         = GGUF V2 (latest)\nllm_load_print_meta: n_ctx_train  \
          \  = 4096\n...\n\nThat means that this model performs best if context is\
          \ set to 4096  . You are still able to extend the context window with this\
          \ model though. \n\nI have tested extending context to 16K using rope (parameters\
          \ in llama.cpp in case you using this software)  to summarize longer articles\
          \ and it worked fine:\nllm_load_print_meta: format         = GGUF V2 (latest)\n\
          llm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx \
          \         = 16384\n....\n\nBut overall, it is probably best to stick with\
          \ 4K  context window to avoid degradation and higher memory requirements\
          \ when using this model for higher context window utilizing rope extension.\
          \ \n\n"
        updatedAt: '2023-09-08T11:40:31.660Z'
      numEdits: 0
      reactions: []
    id: 64fb082f4c8924c4fe6eaa2b
    type: comment
  author: dzupin
  content: "When running this model I am receiving following:\nllm_load_print_meta:\
    \ format         = GGUF V2 (latest)\nllm_load_print_meta: n_ctx_train    = 4096\n\
    ...\n\nThat means that this model performs best if context is set to 4096  . You\
    \ are still able to extend the context window with this model though. \n\nI have\
    \ tested extending context to 16K using rope (parameters in llama.cpp in case\
    \ you using this software)  to summarize longer articles and it worked fine:\n\
    llm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: n_ctx_train\
    \    = 4096\nllm_load_print_meta: n_ctx          = 16384\n....\n\nBut overall,\
    \ it is probably best to stick with 4K  context window to avoid degradation and\
    \ higher memory requirements when using this model for higher context window utilizing\
    \ rope extension. \n\n"
  created_at: 2023-09-08 10:40:31+00:00
  edited: false
  hidden: false
  id: 64fb082f4c8924c4fe6eaa2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Undi95/MLewd-L2-13B-v2-2-GGUF
repo_type: model
status: open
target_branch: null
title: Maximum context
