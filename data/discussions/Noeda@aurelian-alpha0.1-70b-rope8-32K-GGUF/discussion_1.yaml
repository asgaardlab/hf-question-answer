!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lazyDataScientist
conflicting_files: null
created_at: 2023-12-12 20:26:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
      fullname: Cedrick Hesketh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyDataScientist
      type: user
    createdAt: '2023-12-12T20:26:28.000Z'
    data:
      edited: true
      editors:
      - lazyDataScientist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8181570172309875
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
          fullname: Cedrick Hesketh
          isHf: false
          isPro: false
          name: lazyDataScientist
          type: user
        html: "<p>This is my recommended setting using the Q4 gguf file.<br>Parameters</p>\n\
          <pre><code>temperature=0.67, \ntop_p=1, \ntop_k=0, \nrepetition_penalty\
          \ = 1.5\n</code></pre>\n<p>Parameters related to the model when loaded into\
          \ memory</p>\n<pre><code>compress_pos_emb = 8 #(also know as Linear Rope\
          \ Scaling; I believe)\nrope_freq_base = 45,0000\nn_ctx = 32768\n</code></pre>\n\
          <p>With these settings I get a very coherent story for 1000-2000 token.\
          \ After the 2000-2500 token mark it will start to gradually make stuff up\
          \ but keeps a solid story structure. </p>\n<p>Hope this helps!!</p>\n"
        raw: "This is my recommended setting using the Q4 gguf file.\nParameters\n\
          ```\ntemperature=0.67, \ntop_p=1, \ntop_k=0, \nrepetition_penalty = 1.5\n\
          ```\nParameters related to the model when loaded into memory\n```\ncompress_pos_emb\
          \ = 8 #(also know as Linear Rope Scaling; I believe)\nrope_freq_base = 45,0000\n\
          n_ctx = 32768\n```\nWith these settings I get a very coherent story for\
          \ 1000-2000 token. After the 2000-2500 token mark it will start to gradually\
          \ make stuff up but keeps a solid story structure. \n\nHope this helps!!"
        updatedAt: '2023-12-12T20:27:45.598Z'
      numEdits: 4
      reactions: []
    id: 6578c1f44d989b0a68af4e93
    type: comment
  author: lazyDataScientist
  content: "This is my recommended setting using the Q4 gguf file.\nParameters\n```\n\
    temperature=0.67, \ntop_p=1, \ntop_k=0, \nrepetition_penalty = 1.5\n```\nParameters\
    \ related to the model when loaded into memory\n```\ncompress_pos_emb = 8 #(also\
    \ know as Linear Rope Scaling; I believe)\nrope_freq_base = 45,0000\nn_ctx = 32768\n\
    ```\nWith these settings I get a very coherent story for 1000-2000 token. After\
    \ the 2000-2500 token mark it will start to gradually make stuff up but keeps\
    \ a solid story structure. \n\nHope this helps!!"
  created_at: 2023-12-12 20:26:28+00:00
  edited: true
  hidden: false
  id: 6578c1f44d989b0a68af4e93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2023-12-12T21:19:22.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9072237014770508
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I use <code>temperature=0.7, top_p=0.8, top_k=90, repetition_penalty
          = 1.16, repeition_penalty_range = 4096</code>, though it probably works
          for a broad range of values. ~2000 tokens is about the max you can get while
          following a given prompt/outline out of any model these days, I think. But
          Aurelian is trained for multi-round, so you can keep going for ever pretty
          much. Just need to prompt and re-direct every 2k tokens or so.</p>

          <p>Here is an <a rel="nofollow" href="https://files.catbox.moe/scu11c.txt">example
          for a one-shot story</a>. Most of the model''s training is actually not
          for a one-shot story, but for scene-by-scene writing, so you can continue
          the story by following up with the  next scene, and it will maintain consistency
          for the entire 32K context window.</p>

          '
        raw: 'I use `temperature=0.7, top_p=0.8, top_k=90, repetition_penalty = 1.16,
          repeition_penalty_range = 4096`, though it probably works for a broad range
          of values. ~2000 tokens is about the max you can get while following a given
          prompt/outline out of any model these days, I think. But Aurelian is trained
          for multi-round, so you can keep going for ever pretty much. Just need to
          prompt and re-direct every 2k tokens or so.


          Here is an [example for a one-shot story](https://files.catbox.moe/scu11c.txt).
          Most of the model''s training is actually not for a one-shot story, but
          for scene-by-scene writing, so you can continue the story by following up
          with the  next scene, and it will maintain consistency for the entire 32K
          context window.

          '
        updatedAt: '2023-12-12T21:19:22.029Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - lazyDataScientist
      - count: 1
        reaction: "\U0001F917"
        users:
        - lazyDataScientist
    id: 6578ce5aee40482b5c5536ec
    type: comment
  author: grimulkan
  content: 'I use `temperature=0.7, top_p=0.8, top_k=90, repetition_penalty = 1.16,
    repeition_penalty_range = 4096`, though it probably works for a broad range of
    values. ~2000 tokens is about the max you can get while following a given prompt/outline
    out of any model these days, I think. But Aurelian is trained for multi-round,
    so you can keep going for ever pretty much. Just need to prompt and re-direct
    every 2k tokens or so.


    Here is an [example for a one-shot story](https://files.catbox.moe/scu11c.txt).
    Most of the model''s training is actually not for a one-shot story, but for scene-by-scene
    writing, so you can continue the story by following up with the  next scene, and
    it will maintain consistency for the entire 32K context window.

    '
  created_at: 2023-12-12 21:19:22+00:00
  edited: false
  hidden: false
  id: 6578ce5aee40482b5c5536ec
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Noeda/aurelian-alpha0.1-70b-rope8-32K-GGUF
repo_type: model
status: open
target_branch: null
title: GGUF parameters suggestion
