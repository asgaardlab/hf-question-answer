!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-12-07 00:27:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-07T00:27:32.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9706507325172424
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>CogVLM which was recently released is currently putting anything
          I''ve seen into shadow.<br>Great handwriting OCR, typing OCR and visual
          understanding so far away from any other vision model (including QWEN VL),
          it feels like a multi generational step.</p>

          <p>One of the core differences is that it uses laion/CLIP-ViT-bigG-14-laion2B-39B-b160k
          as CLIP model.<br>That''s unwieldly for most hardware applications but maybe
          it could be quantized (when quantizing LLMs  5+ bit inferences very similar
          to fp16+ - that might work well with ViT too)<br>The second difference appears
          to be a second "expert" model they embedded into the stack.</p>

          <p>I just wanted to leave that here, given we had a short discussion on
          the ViT used.<br>The CogVLM example shows how much can be done with a larger
          one.</p>

          '
        raw: "CogVLM which was recently released is currently putting anything I've\
          \ seen into shadow.\r\nGreat handwriting OCR, typing OCR and visual understanding\
          \ so far away from any other vision model (including QWEN VL), it feels\
          \ like a multi generational step.\r\n\r\nOne of the core differences is\
          \ that it uses laion/CLIP-ViT-bigG-14-laion2B-39B-b160k as CLIP model.\r\
          \nThat's unwieldly for most hardware applications but maybe it could be\
          \ quantized (when quantizing LLMs  5+ bit inferences very similar to fp16+\
          \ - that might work well with ViT too)\r\nThe second difference appears\
          \ to be a second \"expert\" model they embedded into the stack.\r\n\r\n\
          I just wanted to leave that here, given we had a short discussion on the\
          \ ViT used. \r\nThe CogVLM example shows how much can be done with a larger\
          \ one."
        updatedAt: '2023-12-07T00:27:32.591Z'
      numEdits: 0
      reactions: []
    id: 657111748489a9ee97008367
    type: comment
  author: cmp-nct
  content: "CogVLM which was recently released is currently putting anything I've\
    \ seen into shadow.\r\nGreat handwriting OCR, typing OCR and visual understanding\
    \ so far away from any other vision model (including QWEN VL), it feels like a\
    \ multi generational step.\r\n\r\nOne of the core differences is that it uses\
    \ laion/CLIP-ViT-bigG-14-laion2B-39B-b160k as CLIP model.\r\nThat's unwieldly\
    \ for most hardware applications but maybe it could be quantized (when quantizing\
    \ LLMs  5+ bit inferences very similar to fp16+ - that might work well with ViT\
    \ too)\r\nThe second difference appears to be a second \"expert\" model they embedded\
    \ into the stack.\r\n\r\nI just wanted to leave that here, given we had a short\
    \ discussion on the ViT used. \r\nThe CogVLM example shows how much can be done\
    \ with a larger one."
  created_at: 2023-12-07 00:27:32+00:00
  edited: false
  hidden: false
  id: 657111748489a9ee97008367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-21T21:50:21.000Z'
    data:
      status: closed
    id: 6584b31d6b7f2ceae392c7ec
    type: status-change
  author: cmp-nct
  created_at: 2023-12-21 21:50:21+00:00
  id: 6584b31d6b7f2ceae392c7ec
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12
repo_type: model
status: closed
target_branch: null
title: Compared to a huge vision tower
