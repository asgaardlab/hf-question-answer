!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomhosking
conflicting_files: null
created_at: 2023-12-21 12:41:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4fd26cae2de4c5f8a999a316c499801c.svg
      fullname: Tom Hosking
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomhosking
      type: user
    createdAt: '2023-12-21T12:41:50.000Z'
    data:
      edited: true
      editors:
      - tomhosking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7933849096298218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4fd26cae2de4c5f8a999a316c499801c.svg
          fullname: Tom Hosking
          isHf: false
          isPro: false
          name: tomhosking
          type: user
        html: '<p>Loading the model using the recommended approach results in the
          error "Some weights of the model checkpoint at roberta-large-mnli were not
          used":</p>

          <pre><code>from transformers import  AutoModelForSequenceClassification

          model = AutoModelForSequenceClassification.from_pretrained("roberta-large-mnli")

          </code></pre>

          <pre><code>Some weights of the model checkpoint at roberta-large-mnli were
          not used when initializing RobertaForSequenceClassification: [''roberta.pooler.dense.bias'',
          ''roberta.pooler.dense.weight'']

          - This IS expected if you are initializing RobertaForSequenceClassification
          from the checkpoint of a model trained on another task or with another architecture
          (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining
          model).

          - This IS NOT expected if you are initializing RobertaForSequenceClassification
          from the checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).

          </code></pre>

          <p>Should a different model be used to load this checkpoint? Or can the
          weights be safely removed from the checkpoint?</p>

          <p>Tested on <code>transformers==4.36.2</code></p>

          '
        raw: 'Loading the model using the recommended approach results in the error
          "Some weights of the model checkpoint at roberta-large-mnli were not used":


          ```

          from transformers import  AutoModelForSequenceClassification

          model = AutoModelForSequenceClassification.from_pretrained("roberta-large-mnli")

          ```


          ```

          Some weights of the model checkpoint at roberta-large-mnli were not used
          when initializing RobertaForSequenceClassification: [''roberta.pooler.dense.bias'',
          ''roberta.pooler.dense.weight'']

          - This IS expected if you are initializing RobertaForSequenceClassification
          from the checkpoint of a model trained on another task or with another architecture
          (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining
          model).

          - This IS NOT expected if you are initializing RobertaForSequenceClassification
          from the checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).

          ```


          Should a different model be used to load this checkpoint? Or can the weights
          be safely removed from the checkpoint?


          Tested on `transformers==4.36.2`'
        updatedAt: '2023-12-21T12:44:06.681Z'
      numEdits: 1
      reactions: []
    id: 6584328e56d225548b40e5c7
    type: comment
  author: tomhosking
  content: 'Loading the model using the recommended approach results in the error
    "Some weights of the model checkpoint at roberta-large-mnli were not used":


    ```

    from transformers import  AutoModelForSequenceClassification

    model = AutoModelForSequenceClassification.from_pretrained("roberta-large-mnli")

    ```


    ```

    Some weights of the model checkpoint at roberta-large-mnli were not used when
    initializing RobertaForSequenceClassification: [''roberta.pooler.dense.bias'',
    ''roberta.pooler.dense.weight'']

    - This IS expected if you are initializing RobertaForSequenceClassification from
    the checkpoint of a model trained on another task or with another architecture
    (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining
    model).

    - This IS NOT expected if you are initializing RobertaForSequenceClassification
    from the checkpoint of a model that you expect to be exactly identical (initializing
    a BertForSequenceClassification model from a BertForSequenceClassification model).

    ```


    Should a different model be used to load this checkpoint? Or can the weights be
    safely removed from the checkpoint?


    Tested on `transformers==4.36.2`'
  created_at: 2023-12-21 12:41:50+00:00
  edited: true
  hidden: false
  id: 6584328e56d225548b40e5c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-12-22T13:53:13.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7697086930274963
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tomhosking&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tomhosking\"\
          >@<span class=\"underline\">tomhosking</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>You can ignore this warning. Everything is fine for using this\
          \ checkpoint for sequence classification`task.</p>\n"
        raw: "Hi @tomhosking \n\nYou can ignore this warning. Everything is fine for\
          \ using this checkpoint for sequence classification`task.\n"
        updatedAt: '2023-12-22T13:53:13.165Z'
      numEdits: 0
      reactions: []
    id: 658594c9f8fae1c1e5cdbc49
    type: comment
  author: ydshieh
  content: "Hi @tomhosking \n\nYou can ignore this warning. Everything is fine for\
    \ using this checkpoint for sequence classification`task.\n"
  created_at: 2023-12-22 13:53:13+00:00
  edited: false
  hidden: false
  id: 658594c9f8fae1c1e5cdbc49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4fd26cae2de4c5f8a999a316c499801c.svg
      fullname: Tom Hosking
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomhosking
      type: user
    createdAt: '2023-12-29T12:21:24.000Z'
    data:
      edited: false
      editors:
      - tomhosking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7796887755393982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4fd26cae2de4c5f8a999a316c499801c.svg
          fullname: Tom Hosking
          isHf: false
          isPro: false
          name: tomhosking
          type: user
        html: '<p>OK, thanks! Is there a way to disable the error message?</p>

          '
        raw: OK, thanks! Is there a way to disable the error message?
        updatedAt: '2023-12-29T12:21:24.059Z'
      numEdits: 0
      reactions: []
    id: 658eb9c4de82e1ef7be18d98
    type: comment
  author: tomhosking
  content: OK, thanks! Is there a way to disable the error message?
  created_at: 2023-12-29 12:21:24+00:00
  edited: false
  hidden: false
  id: 658eb9c4de82e1ef7be18d98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2024-01-08T13:52:52.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9628773331642151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tomhosking&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tomhosking\"\
          >@<span class=\"underline\">tomhosking</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>There is currently no way to disable the warning just for this\
          \ combination model/checkpoint. </p>\n<p>(we didn't have this warning several\
          \ months back but a merged PR make this appear)</p>\n"
        raw: "Hi @tomhosking \n\nThere is currently no way to disable the warning\
          \ just for this combination model/checkpoint. \n\n(we didn't have this warning\
          \ several months back but a merged PR make this appear)"
        updatedAt: '2024-01-08T13:52:52.377Z'
      numEdits: 0
      reactions: []
    id: 659bfe34f311d426db7a501d
    type: comment
  author: ydshieh
  content: "Hi @tomhosking \n\nThere is currently no way to disable the warning just\
    \ for this combination model/checkpoint. \n\n(we didn't have this warning several\
    \ months back but a merged PR make this appear)"
  created_at: 2024-01-08 13:52:52+00:00
  edited: false
  hidden: false
  id: 659bfe34f311d426db7a501d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: roberta-large-mnli
repo_type: model
status: open
target_branch: null
title: Some weights of the model checkpoint at roberta-large-mnli were not used
