!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sidddd2020
conflicting_files: null
created_at: 2023-09-11 06:54:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80dd0acf5f7b5b258f67fc2d22f4a7b0.svg
      fullname: Sidd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sidddd2020
      type: user
    createdAt: '2023-09-11T07:54:48.000Z'
    data:
      edited: false
      editors:
      - Sidddd2020
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8844171166419983
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80dd0acf5f7b5b258f67fc2d22f4a7b0.svg
          fullname: Sidd
          isHf: false
          isPro: false
          name: Sidddd2020
          type: user
        html: '<p>Hey everyone,</p>

          <p>I''m facing a challenging issue and could really use your help. Here''s
          the situation:</p>

          <p>My Setup:</p>

          <p>CPU: AMD Ryzen R5 3600<br>RAM: 8GB (with a 30GB swap file)<br>GPU: Nvidia
          RTX 3060 Ti<br>OS: Ubuntu 22.04 (Linux Lite)<br>Nvidia drivers: Version
          470 (CUDA 11.4)</p>

          <p>The Problem:</p>

          <p>I''m working with the ''airoboros-l2-13b-gpt4-2.0'' and ''airoboros-l2-7b-gpt4-m2.0''
          model using vLLM.<br>I keep encountering CUDA out-of-memory errors.<br>Recently,
          I ran into a mysterious "Magic no. error."</p>

          <p>What I''ve Tried So Far:</p>

          <p>I tweaked the ''config.json'' file.<br>Adjusted parameters like ''hidden_size,''
          ''num_hidden_layers,'' and ''num_attention_heads'' to reduce model size.</p>

          <p>Where I Need Help:</p>

          <p>Understanding the Problem: Can someone help me break down these CUDA
          out-of-memory errors and the "Magic no. error"?</p>

          <p>Optimizing ''config.json'': I experimented, but maybe there are better
          settings for my hardware.</p>

          <p>First Principles Approach: Let''s start from scratch. How can we ensure
          the model runs efficiently on my setup?</p>

          <p>Monitoring GPU Resources: What tools or techniques can I use to keep
          track of GPU memory usage?</p>

          <p>Community Knowledge: Share your experiences. Let''s build a collaborative
          space where we all learn together.</p>

          <p>If you''ve faced similar challenges or have experience with optimizing
          models for limited GPU resources, your insights would be greatly appreciated.</p>

          <p>Your assistance could not only help me but also benefit anyone working
          with resource-intensive models. Together, we''ll conquer this challenge
          and make the most of our hardware.</p>

          <p>Thanks for your help in advance. I''m looking forward to our discussion!</p>

          '
        raw: "Hey everyone,\r\n\r\nI'm facing a challenging issue and could really\
          \ use your help. Here's the situation:\r\n\r\nMy Setup:\r\n\r\nCPU: AMD\
          \ Ryzen R5 3600\r\nRAM: 8GB (with a 30GB swap file)\r\nGPU: Nvidia RTX 3060\
          \ Ti\r\nOS: Ubuntu 22.04 (Linux Lite)\r\nNvidia drivers: Version 470 (CUDA\
          \ 11.4)\r\n\r\nThe Problem:\r\n\r\nI'm working with the 'airoboros-l2-13b-gpt4-2.0'\
          \ and 'airoboros-l2-7b-gpt4-m2.0' model using vLLM.\r\nI keep encountering\
          \ CUDA out-of-memory errors.\r\nRecently, I ran into a mysterious \"Magic\
          \ no. error.\"\r\n\r\nWhat I've Tried So Far:\r\n\r\nI tweaked the 'config.json'\
          \ file.\r\nAdjusted parameters like 'hidden_size,' 'num_hidden_layers,'\
          \ and 'num_attention_heads' to reduce model size.\r\n\r\nWhere I Need Help:\r\
          \n\r\nUnderstanding the Problem: Can someone help me break down these CUDA\
          \ out-of-memory errors and the \"Magic no. error\"?\r\n\r\nOptimizing 'config.json':\
          \ I experimented, but maybe there are better settings for my hardware.\r\
          \n\r\nFirst Principles Approach: Let's start from scratch. How can we ensure\
          \ the model runs efficiently on my setup?\r\n\r\nMonitoring GPU Resources:\
          \ What tools or techniques can I use to keep track of GPU memory usage?\r\
          \n\r\nCommunity Knowledge: Share your experiences. Let's build a collaborative\
          \ space where we all learn together.\r\n\r\nIf you've faced similar challenges\
          \ or have experience with optimizing models for limited GPU resources, your\
          \ insights would be greatly appreciated.\r\n\r\nYour assistance could not\
          \ only help me but also benefit anyone working with resource-intensive models.\
          \ Together, we'll conquer this challenge and make the most of our hardware.\r\
          \n\r\nThanks for your help in advance. I'm looking forward to our discussion!"
        updatedAt: '2023-09-11T07:54:48.512Z'
      numEdits: 0
      reactions: []
    id: 64fec7c8f9abd642c37e915e
    type: comment
  author: Sidddd2020
  content: "Hey everyone,\r\n\r\nI'm facing a challenging issue and could really use\
    \ your help. Here's the situation:\r\n\r\nMy Setup:\r\n\r\nCPU: AMD Ryzen R5 3600\r\
    \nRAM: 8GB (with a 30GB swap file)\r\nGPU: Nvidia RTX 3060 Ti\r\nOS: Ubuntu 22.04\
    \ (Linux Lite)\r\nNvidia drivers: Version 470 (CUDA 11.4)\r\n\r\nThe Problem:\r\
    \n\r\nI'm working with the 'airoboros-l2-13b-gpt4-2.0' and 'airoboros-l2-7b-gpt4-m2.0'\
    \ model using vLLM.\r\nI keep encountering CUDA out-of-memory errors.\r\nRecently,\
    \ I ran into a mysterious \"Magic no. error.\"\r\n\r\nWhat I've Tried So Far:\r\
    \n\r\nI tweaked the 'config.json' file.\r\nAdjusted parameters like 'hidden_size,'\
    \ 'num_hidden_layers,' and 'num_attention_heads' to reduce model size.\r\n\r\n\
    Where I Need Help:\r\n\r\nUnderstanding the Problem: Can someone help me break\
    \ down these CUDA out-of-memory errors and the \"Magic no. error\"?\r\n\r\nOptimizing\
    \ 'config.json': I experimented, but maybe there are better settings for my hardware.\r\
    \n\r\nFirst Principles Approach: Let's start from scratch. How can we ensure the\
    \ model runs efficiently on my setup?\r\n\r\nMonitoring GPU Resources: What tools\
    \ or techniques can I use to keep track of GPU memory usage?\r\n\r\nCommunity\
    \ Knowledge: Share your experiences. Let's build a collaborative space where we\
    \ all learn together.\r\n\r\nIf you've faced similar challenges or have experience\
    \ with optimizing models for limited GPU resources, your insights would be greatly\
    \ appreciated.\r\n\r\nYour assistance could not only help me but also benefit\
    \ anyone working with resource-intensive models. Together, we'll conquer this\
    \ challenge and make the most of our hardware.\r\n\r\nThanks for your help in\
    \ advance. I'm looking forward to our discussion!"
  created_at: 2023-09-11 06:54:48+00:00
  edited: false
  hidden: false
  id: 64fec7c8f9abd642c37e915e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6379683a81c1783a4a2ddba8/Tr_HXFEOJDZyP_lg9-hwJ.png?w=200&h=200&f=face
      fullname: nisten
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nisten
      type: user
    createdAt: '2023-09-11T13:44:25.000Z'
    data:
      edited: false
      editors:
      - nisten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9694392681121826
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6379683a81c1783a4a2ddba8/Tr_HXFEOJDZyP_lg9-hwJ.png?w=200&h=200&f=face
          fullname: nisten
          isHf: false
          isPro: false
          name: nisten
          type: user
        html: '<p>Magic no. error is a llama.cpp thing if I recall correctly ( i''m
          only ~70% sure on this ).<br>Did a hack  while trying to do this on ios
          where I just deleted that checker entirely from llama.cpp but had limited
          success. </p>

          <p>Best ppl do ask for this is anyone working on llama.cpp or MLC ai as
          they have deff ran into it and solved it before.</p>

          '
        raw: "Magic no. error is a llama.cpp thing if I recall correctly ( i'm only\
          \ ~70% sure on this ). \nDid a hack  while trying to do this on ios where\
          \ I just deleted that checker entirely from llama.cpp but had limited success.\
          \ \n\nBest ppl do ask for this is anyone working on llama.cpp or MLC ai\
          \ as they have deff ran into it and solved it before."
        updatedAt: '2023-09-11T13:44:25.291Z'
      numEdits: 0
      reactions: []
    id: 64ff19b9f1523eb736b8809d
    type: comment
  author: nisten
  content: "Magic no. error is a llama.cpp thing if I recall correctly ( i'm only\
    \ ~70% sure on this ). \nDid a hack  while trying to do this on ios where I just\
    \ deleted that checker entirely from llama.cpp but had limited success. \n\nBest\
    \ ppl do ask for this is anyone working on llama.cpp or MLC ai as they have deff\
    \ ran into it and solved it before."
  created_at: 2023-09-11 12:44:25+00:00
  edited: false
  hidden: false
  id: 64ff19b9f1523eb736b8809d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jondurbin/airoboros-l2-13b-gpt4-2.0
repo_type: model
status: open
target_branch: null
title: 'Optimizing ''airoboros-l2-?b-gpt4-2.0'' for Limited Resources: Seeking Guidance'
