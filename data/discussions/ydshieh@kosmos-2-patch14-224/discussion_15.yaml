!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ashwath-Shetty
conflicting_files: null
created_at: 2023-09-05 03:33:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
      fullname: Ashwath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashwath-Shetty
      type: user
    createdAt: '2023-09-05T04:33:54.000Z'
    data:
      edited: false
      editors:
      - Ashwath-Shetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4180978834629059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
          fullname: Ashwath
          isHf: false
          isPro: false
          name: Ashwath-Shetty
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ydshieh\">@<span class=\"\
          underline\">ydshieh</span></a></span>\n\n\t</span></span><br>it was working\
          \ fine till yesterday &amp; today i have got this error:<br>AttributeError:\
          \ can't set attribute 'can_save_slow_tokenizer'</p>\n<p>library installation:<br>!pip\
          \ install transformers<br>!pip install sentencepiece</p>\n<p>system specs:<br>OS:\
          \ AWS Sagemaker(Amazon Linux 2, Jupyter Lab 3<br>(notebook-al2-v2))<br>Python:\
          \ 3.10<br>Transformers: 4.31.0<br>sentencepiece: 0.1.99<br>PyTorch: 2.0.1<br>CUDA\
          \ (python -c 'import torch; print(torch.version.cuda)'): 11.8</p>\n<p>code:<br>import\
          \ requests</p>\n<p>from PIL import Image<br>from transformers import AutoProcessor,\
          \ AutoModelForVision2Seq</p>\n<p>model = AutoModelForVision2Seq.from_pretrained(\"\
          ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)<br>processor = AutoProcessor.from_pretrained(\"\
          ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)</p>\n<p>prompt =\
          \ \"An image of\"</p>\n<p>url = \"<a href=\"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png&quot;\"\
          >https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          </a><br>image = Image.open(requests.get(url, stream=True).raw)</p>\n<h1\
          \ id=\"the-original-kosmos-2-demo-saves-the-image-first-then-reload-it-for-some-images-this-will-give-slightly-different-image-input-and-change-the-generation-outputs\"\
          >The original Kosmos-2 demo saves the image first then reload it. For some\
          \ images, this will give slightly different image input and change the generation\
          \ outputs.</h1>\n<h1 id=\"uncomment-the-following-2-lines-if-you-want-to-match-the-original-demos-outputs\"\
          >Uncomment the following 2 lines if you want to match the original demo's\
          \ outputs.</h1>\n<h1 id=\"one-example-is-the-two_dogsjpg-from-the-demo\"\
          >(One example is the <code>two_dogs.jpg</code> from the demo)</h1>\n<h1\
          \ id=\"imagesavenew_imagejpg\">image.save(\"new_image.jpg\")</h1>\n<h1 id=\"\
          image--imageopennew_imagejpg\">image = Image.open(\"new_image.jpg\")</h1>\n\
          <p>inputs = processor(text=prompt, images=image, return_tensors=\"pt\")</p>\n\
          <p>generated_ids = model.generate(<br>    pixel_values=inputs[\"pixel_values\"\
          ],<br>    input_ids=inputs[\"input_ids\"][:, :-1],<br>    attention_mask=inputs[\"\
          attention_mask\"][:, :-1],<br>    img_features=None,<br>    img_attn_mask=inputs[\"\
          img_attn_mask\"][:, :-1],<br>    use_cache=True,<br>    max_new_tokens=64,<br>)<br>generated_text\
          \ = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]</p>\n\
          <h1 id=\"specify-cleanup_and_extractfalse-in-order-to-see-the-raw-model-generation\"\
          >Specify <code>cleanup_and_extract=False</code> in order to see the raw\
          \ model generation.</h1>\n<p>processed_text = processor.post_process_generation(generated_text,\
          \ cleanup_and_extract=False)</p>\n<p>print(processed_text)</p>\n<h1 id=\"\
          grounding-an-image-ofphrase-a-snowmanphraseobjectpatch_index_0044patch_index_0863object-warming-himself-byphrase-a-firephraseobjectpatch_index_0005patch_index_0911object\"\
          ><code>&lt;grounding&gt; An image of&lt;phrase&gt; a snowman&lt;/phrase&gt;&lt;object&gt;&lt;patch_index_0044&gt;&lt;patch_index_0863&gt;&lt;/object&gt;\
          \ warming himself by&lt;phrase&gt; a fire&lt;/phrase&gt;&lt;object&gt;&lt;patch_index_0005&gt;&lt;patch_index_0911&gt;&lt;/object&gt;.</code></h1>\n\
          <h1 id=\"by-default-the-generated--text-is-cleanup-and-the-entities-are-extracted\"\
          >By default, the generated  text is cleanup and the entities are extracted.</h1>\n\
          <p>processed_text, entities = processor.post_process_generation(generated_text)</p>\n\
          <p>print(processed_text)</p>\n<h1 id=\"an-image-of-a-snowman-warming-himself-by-a-fire\"\
          ><code>An image of a snowman warming himself by a fire.</code></h1>\n<p>print(entities)</p>\n\
          <h1 id=\"a-snowman-12-21-0390625-0046875-0984375-0828125-a-fire-41-47-0171875-0015625-0484375-0890625\"\
          ><code>[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]),\
          \ ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]</code></h1>\n\
          <h2 id=\"error\">Error:</h2>\n<p>AttributeError                        \
          \    Traceback (most recent call last)<br>Cell In[14], line 8<br>      4\
          \ from transformers import AutoProcessor, AutoModelForVision2Seq<br>   \
          \   7 model = AutoModelForVision2Seq.from_pretrained(\"ydshieh/kosmos-2-patch14-224\"\
          , trust_remote_code=True)<br>----&gt; 8 processor = AutoProcessor.from_pretrained(\"\
          ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)<br>     10 prompt\
          \ = \"An image of\"<br>     12 url = \"<a href=\"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png&quot;\"\
          >https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          </a></p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:283,\
          \ in AutoProcessor.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>\
          \    281     if os.path.isdir(pretrained_model_name_or_path):<br>    282\
          \         processor_class.register_for_auto_class()<br>--&gt; 283     return\
          \ processor_class.from_pretrained(<br>    284         pretrained_model_name_or_path,\
          \ trust_remote_code=trust_remote_code, **kwargs<br>    285     )<br>   \
          \ 286 elif processor_class is not None:<br>    287     return processor_class.from_pretrained(<br>\
          \    288         pretrained_model_name_or_path, trust_remote_code=trust_remote_code,\
          \ **kwargs<br>    289     )</p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:226,\
          \ in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, **kwargs)<br>\
          \    223 if token is not None:<br>    224     kwargs[\"token\"] = token<br>--&gt;\
          \ 226 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)<br>    227 return cls(*args)</p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:270,\
          \ in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,\
          \ **kwargs)<br>    267     else:<br>    268         attribute_class = getattr(transformers_module,\
          \ class_name)<br>--&gt; 270     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs))<br>    271 return args</p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:723,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)<br>    721     if os.path.isdir(pretrained_model_name_or_path):<br>\
          \    722         tokenizer_class.register_for_auto_class()<br>--&gt; 723\
          \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)<br>    724 elif config_tokenizer_class is not None:<br>\
          \    725     tokenizer_class = None</p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)<br>   1851     else:<br>   1852         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")<br>-&gt;\
          \ 1854 return cls._from_pretrained(<br>   1855     resolved_vocab_files,<br>\
          \   1856     pretrained_model_name_or_path,<br>   1857     init_configuration,<br>\
          \   1858     *init_inputs,<br>   1859     token=token,<br>   1860     cache_dir=cache_dir,<br>\
          \   1861     local_files_only=local_files_only,<br>   1862     _commit_hash=commit_hash,<br>\
          \   1863     _is_local=is_local,<br>   1864     **kwargs,<br>   1865 )</p>\n\
          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)<br>   2015 # Instantiate\
          \ tokenizer.<br>   2016 try:<br>-&gt; 2017     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)<br>   2018 except OSError:<br>   2019     raise OSError(<br>\
          \   2020         \"Unable to load vocabulary from file. \"<br>   2021  \
          \       \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"<br>   2022     )</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/d591f18e3ce08debe6bbdc7117b87a1595450179/tokenization_kosmos2_fast.py:140,\
          \ in Kosmos2TokenizerFast.<strong>init</strong>(self, vocab_file, tokenizer_file,\
          \ bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token,\
          \ num_patch_index_tokens, add_tag_and_patch_index_tokens, **kwargs)<br>\
          \    126 super().<strong>init</strong>(<br>    127     vocab_file,<br> \
          \   128     tokenizer_file=tokenizer_file,<br>   (...)<br>    136     **kwargs,<br>\
          \    137 )<br>    139 self.vocab_file = vocab_file<br>--&gt; 140 self.can_save_slow_tokenizer\
          \ = False if not self.vocab_file else True<br>    142 self.eod_token = \"\
          \"<br>    144 self.boi_token = \"<img>\"</p>\n<p>AttributeError: can't set\
          \ attribute 'can_save_slow_tokenizer'</p>\n"
        raw: "@ydshieh \r\nit was working fine till yesterday & today i have got this\
          \ error:\r\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\r\
          \n\r\nlibrary installation: \r\n!pip install transformers\r\n!pip install\
          \ sentencepiece\r\n\r\nsystem specs:\r\nOS: AWS Sagemaker(Amazon Linux 2,\
          \ Jupyter Lab 3\r\n(notebook-al2-v2))\r\nPython: 3.10\r\nTransformers: 4.31.0\r\
          \nsentencepiece: 0.1.99\r\nPyTorch: 2.0.1\r\nCUDA (python -c 'import torch;\
          \ print(torch.version.cuda)'): 11.8\r\n\r\n\r\ncode:\r\nimport requests\r\
          \n\r\nfrom PIL import Image\r\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\r\
          \n\r\n\r\nmodel = AutoModelForVision2Seq.from_pretrained(\"ydshieh/kosmos-2-patch14-224\"\
          , trust_remote_code=True)\r\nprocessor = AutoProcessor.from_pretrained(\"\
          ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\n\r\nprompt =\
          \ \"<grounding>An image of\"\r\n\r\nurl = \"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \r\nimage = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# The\
          \ original Kosmos-2 demo saves the image first then reload it. For some\
          \ images, this will give slightly different image input and change the generation\
          \ outputs.\r\n# Uncomment the following 2 lines if you want to match the\
          \ original demo's outputs.\r\n# (One example is the `two_dogs.jpg` from\
          \ the demo)\r\n# image.save(\"new_image.jpg\")\r\n# image = Image.open(\"\
          new_image.jpg\")\r\n\r\ninputs = processor(text=prompt, images=image, return_tensors=\"\
          pt\")\r\n\r\ngenerated_ids = model.generate(\r\n    pixel_values=inputs[\"\
          pixel_values\"],\r\n    input_ids=inputs[\"input_ids\"][:, :-1],\r\n   \
          \ attention_mask=inputs[\"attention_mask\"][:, :-1],\r\n    img_features=None,\r\
          \n    img_attn_mask=inputs[\"img_attn_mask\"][:, :-1],\r\n    use_cache=True,\r\
          \n    max_new_tokens=64,\r\n)\r\ngenerated_text = processor.batch_decode(generated_ids,\
          \ skip_special_tokens=True)[0]\r\n\r\n# Specify `cleanup_and_extract=False`\
          \ in order to see the raw model generation.\r\nprocessed_text = processor.post_process_generation(generated_text,\
          \ cleanup_and_extract=False)\r\n\r\nprint(processed_text)\r\n# `<grounding>\
          \ An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object>\
          \ warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\r\
          \n\r\n# By default, the generated  text is cleanup and the entities are\
          \ extracted.\r\nprocessed_text, entities = processor.post_process_generation(generated_text)\r\
          \n\r\nprint(processed_text)\r\n# `An image of a snowman warming himself\
          \ by a fire.`\r\n\r\nprint(entities)\r\n# `[('a snowman', (12, 21), [(0.390625,\
          \ 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625,\
          \ 0.484375, 0.890625)])]`\r\n\r\n\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\nCell In[14], line 8\r\n      4 from transformers import AutoProcessor,\
          \ AutoModelForVision2Seq\r\n      7 model = AutoModelForVision2Seq.from_pretrained(\"\
          ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\n----> 8 processor\
          \ = AutoProcessor.from_pretrained(\"ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\
          \n     10 prompt = \"<grounding>An image of\"\r\n     12 url = \"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:283,\
          \ in AutoProcessor.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n    281     if os.path.isdir(pretrained_model_name_or_path):\r\n    282\
          \         processor_class.register_for_auto_class()\r\n--> 283     return\
          \ processor_class.from_pretrained(\r\n    284         pretrained_model_name_or_path,\
          \ trust_remote_code=trust_remote_code, **kwargs\r\n    285     )\r\n   \
          \ 286 elif processor_class is not None:\r\n    287     return processor_class.from_pretrained(\r\
          \n    288         pretrained_model_name_or_path, trust_remote_code=trust_remote_code,\
          \ **kwargs\r\n    289     )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:226,\
          \ in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, **kwargs)\r\
          \n    223 if token is not None:\r\n    224     kwargs[\"token\"] = token\r\
          \n--> 226 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n    227 return cls(*args)\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:270,\
          \ in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    267     else:\r\n    268         attribute_class = getattr(transformers_module,\
          \ class_name)\r\n--> 270     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs))\r\n    271 return args\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:723,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    721     if os.path.isdir(pretrained_model_name_or_path):\r\
          \n    722         tokenizer_class.register_for_auto_class()\r\n--> 723 \
          \    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    724 elif config_tokenizer_class is not None:\r\
          \n    725     tokenizer_class = None\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 1854 return cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\
          \n   1856     pretrained_model_name_or_path,\r\n   1857     init_configuration,\r\
          \n   1858     *init_inputs,\r\n   1859     token=token,\r\n   1860     cache_dir=cache_dir,\r\
          \n   1861     local_files_only=local_files_only,\r\n   1862     _commit_hash=commit_hash,\r\
          \n   1863     _is_local=is_local,\r\n   1864     **kwargs,\r\n   1865 )\r\
          \n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   2015 # Instantiate\
          \ tokenizer.\r\n   2016 try:\r\n-> 2017     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   2018 except OSError:\r\n   2019     raise OSError(\r\
          \n   2020         \"Unable to load vocabulary from file. \"\r\n   2021 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   2022     )\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/d591f18e3ce08debe6bbdc7117b87a1595450179/tokenization_kosmos2_fast.py:140,\
          \ in Kosmos2TokenizerFast.__init__(self, vocab_file, tokenizer_file, bos_token,\
          \ eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, num_patch_index_tokens,\
          \ add_tag_and_patch_index_tokens, **kwargs)\r\n    126 super().__init__(\r\
          \n    127     vocab_file,\r\n    128     tokenizer_file=tokenizer_file,\r\
          \n   (...)\r\n    136     **kwargs,\r\n    137 )\r\n    139 self.vocab_file\
          \ = vocab_file\r\n--> 140 self.can_save_slow_tokenizer = False if not self.vocab_file\
          \ else True\r\n    142 self.eod_token = \"</doc>\"\r\n    144 self.boi_token\
          \ = \"<image>\"\r\n\r\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\r\
          \n\r\n"
        updatedAt: '2023-09-05T04:33:54.011Z'
      numEdits: 0
      reactions: []
    id: 64f6afb2797e5c5b0941aba6
    type: comment
  author: Ashwath-Shetty
  content: "@ydshieh \r\nit was working fine till yesterday & today i have got this\
    \ error:\r\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\r\n\r\
    \nlibrary installation: \r\n!pip install transformers\r\n!pip install sentencepiece\r\
    \n\r\nsystem specs:\r\nOS: AWS Sagemaker(Amazon Linux 2, Jupyter Lab 3\r\n(notebook-al2-v2))\r\
    \nPython: 3.10\r\nTransformers: 4.31.0\r\nsentencepiece: 0.1.99\r\nPyTorch: 2.0.1\r\
    \nCUDA (python -c 'import torch; print(torch.version.cuda)'): 11.8\r\n\r\n\r\n\
    code:\r\nimport requests\r\n\r\nfrom PIL import Image\r\nfrom transformers import\
    \ AutoProcessor, AutoModelForVision2Seq\r\n\r\n\r\nmodel = AutoModelForVision2Seq.from_pretrained(\"\
    ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\nprocessor = AutoProcessor.from_pretrained(\"\
    ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\n\r\nprompt = \"<grounding>An\
    \ image of\"\r\n\r\nurl = \"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
    \r\nimage = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# The original\
    \ Kosmos-2 demo saves the image first then reload it. For some images, this will\
    \ give slightly different image input and change the generation outputs.\r\n#\
    \ Uncomment the following 2 lines if you want to match the original demo's outputs.\r\
    \n# (One example is the `two_dogs.jpg` from the demo)\r\n# image.save(\"new_image.jpg\"\
    )\r\n# image = Image.open(\"new_image.jpg\")\r\n\r\ninputs = processor(text=prompt,\
    \ images=image, return_tensors=\"pt\")\r\n\r\ngenerated_ids = model.generate(\r\
    \n    pixel_values=inputs[\"pixel_values\"],\r\n    input_ids=inputs[\"input_ids\"\
    ][:, :-1],\r\n    attention_mask=inputs[\"attention_mask\"][:, :-1],\r\n    img_features=None,\r\
    \n    img_attn_mask=inputs[\"img_attn_mask\"][:, :-1],\r\n    use_cache=True,\r\
    \n    max_new_tokens=64,\r\n)\r\ngenerated_text = processor.batch_decode(generated_ids,\
    \ skip_special_tokens=True)[0]\r\n\r\n# Specify `cleanup_and_extract=False` in\
    \ order to see the raw model generation.\r\nprocessed_text = processor.post_process_generation(generated_text,\
    \ cleanup_and_extract=False)\r\n\r\nprint(processed_text)\r\n# `<grounding> An\
    \ image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object>\
    \ warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\r\
    \n\r\n# By default, the generated  text is cleanup and the entities are extracted.\r\
    \nprocessed_text, entities = processor.post_process_generation(generated_text)\r\
    \n\r\nprint(processed_text)\r\n# `An image of a snowman warming himself by a fire.`\r\
    \n\r\nprint(entities)\r\n# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375,\
    \ 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`\r\
    \n\r\n\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \nCell In[14], line 8\r\n      4 from transformers import AutoProcessor, AutoModelForVision2Seq\r\
    \n      7 model = AutoModelForVision2Seq.from_pretrained(\"ydshieh/kosmos-2-patch14-224\"\
    , trust_remote_code=True)\r\n----> 8 processor = AutoProcessor.from_pretrained(\"\
    ydshieh/kosmos-2-patch14-224\", trust_remote_code=True)\r\n     10 prompt = \"\
    <grounding>An image of\"\r\n     12 url = \"https://huggingface.co/ydshieh/kosmos-2-patch14-224/resolve/main/snowman.png\"\
    \r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:283,\
    \ in AutoProcessor.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    281     if os.path.isdir(pretrained_model_name_or_path):\r\n    282    \
    \     processor_class.register_for_auto_class()\r\n--> 283     return processor_class.from_pretrained(\r\
    \n    284         pretrained_model_name_or_path, trust_remote_code=trust_remote_code,\
    \ **kwargs\r\n    285     )\r\n    286 elif processor_class is not None:\r\n \
    \   287     return processor_class.from_pretrained(\r\n    288         pretrained_model_name_or_path,\
    \ trust_remote_code=trust_remote_code, **kwargs\r\n    289     )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:226,\
    \ in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path, cache_dir,\
    \ force_download, local_files_only, token, revision, **kwargs)\r\n    223 if token\
    \ is not None:\r\n    224     kwargs[\"token\"] = token\r\n--> 226 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    227 return cls(*args)\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/processing_utils.py:270,\
    \ in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,\
    \ **kwargs)\r\n    267     else:\r\n    268         attribute_class = getattr(transformers_module,\
    \ class_name)\r\n--> 270     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs))\r\n    271 return args\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:723,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    721     if os.path.isdir(pretrained_model_name_or_path):\r\n\
    \    722         tokenizer_class.register_for_auto_class()\r\n--> 723     return\
    \ tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\
    \n    724 elif config_tokenizer_class is not None:\r\n    725     tokenizer_class\
    \ = None\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 1854 return\
    \ cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\n   1856     pretrained_model_name_or_path,\r\
    \n   1857     init_configuration,\r\n   1858     *init_inputs,\r\n   1859    \
    \ token=token,\r\n   1860     cache_dir=cache_dir,\r\n   1861     local_files_only=local_files_only,\r\
    \n   1862     _commit_hash=commit_hash,\r\n   1863     _is_local=is_local,\r\n\
    \   1864     **kwargs,\r\n   1865 )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2015 # Instantiate tokenizer.\r\n   2016 try:\r\
    \n-> 2017     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   2018 except OSError:\r\
    \n   2019     raise OSError(\r\n   2020         \"Unable to load vocabulary from\
    \ file. \"\r\n   2021         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   2022     )\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/d591f18e3ce08debe6bbdc7117b87a1595450179/tokenization_kosmos2_fast.py:140,\
    \ in Kosmos2TokenizerFast.__init__(self, vocab_file, tokenizer_file, bos_token,\
    \ eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, num_patch_index_tokens,\
    \ add_tag_and_patch_index_tokens, **kwargs)\r\n    126 super().__init__(\r\n \
    \   127     vocab_file,\r\n    128     tokenizer_file=tokenizer_file,\r\n   (...)\r\
    \n    136     **kwargs,\r\n    137 )\r\n    139 self.vocab_file = vocab_file\r\
    \n--> 140 self.can_save_slow_tokenizer = False if not self.vocab_file else True\r\
    \n    142 self.eod_token = \"</doc>\"\r\n    144 self.boi_token = \"<image>\"\r\
    \n\r\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\r\n\r\n"
  created_at: 2023-09-05 03:33:54+00:00
  edited: false
  hidden: false
  id: 64f6afb2797e5c5b0941aba6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T04:39:59.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9731464982032776
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>Hi, yes, I also realized this yesterday.</p>

          <p>I have to discuss this with some team members, and update the files on
          the Hub too. Sorry for this happening.</p>

          '
        raw: 'Hi, yes, I also realized this yesterday.


          I have to discuss this with some team members, and update the files on the
          Hub too. Sorry for this happening.'
        updatedAt: '2023-09-05T04:39:59.915Z'
      numEdits: 0
      reactions: []
    id: 64f6b11f7cff6e8fe040815a
    type: comment
  author: ydshieh
  content: 'Hi, yes, I also realized this yesterday.


    I have to discuss this with some team members, and update the files on the Hub
    too. Sorry for this happening.'
  created_at: 2023-09-05 03:39:59+00:00
  edited: false
  hidden: false
  id: 64f6b11f7cff6e8fe040815a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T05:17:01.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9897963404655457
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>Hello again. I have done something in the meantime - it should work
          for now.</p>

          '
        raw: Hello again. I have done something in the meantime - it should work for
          now.
        updatedAt: '2023-09-05T05:17:01.465Z'
      numEdits: 0
      reactions: []
    id: 64f6b9cde20513303f27aa51
    type: comment
  author: ydshieh
  content: Hello again. I have done something in the meantime - it should work for
    now.
  created_at: 2023-09-05 04:17:01+00:00
  edited: false
  hidden: false
  id: 64f6b9cde20513303f27aa51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
      fullname: Ashwath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashwath-Shetty
      type: user
    createdAt: '2023-09-05T05:18:16.000Z'
    data:
      edited: false
      editors:
      - Ashwath-Shetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9288212060928345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
          fullname: Ashwath
          isHf: false
          isPro: false
          name: Ashwath-Shetty
          type: user
        html: "<p>thanks for replying <span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ydshieh\"\
          >@<span class=\"underline\">ydshieh</span></a></span>\n\n\t</span></span>\
          \ , no problem  , i'm running some experiments, do you think will this be\
          \ fixed by the end of this week?</p>\n"
        raw: thanks for replying @ydshieh , no problem  , i'm running some experiments,
          do you think will this be fixed by the end of this week?
        updatedAt: '2023-09-05T05:18:16.330Z'
      numEdits: 0
      reactions: []
    id: 64f6ba1813e91ef83ba10c02
    type: comment
  author: Ashwath-Shetty
  content: thanks for replying @ydshieh , no problem  , i'm running some experiments,
    do you think will this be fixed by the end of this week?
  created_at: 2023-09-05 04:18:16+00:00
  edited: false
  hidden: false
  id: 64f6ba1813e91ef83ba10c02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T05:23:58.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463357925415039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>It should work now already. Let me know if you still encounter some
          issues.</p>

          '
        raw: It should work now already. Let me know if you still encounter some issues.
        updatedAt: '2023-09-05T05:23:58.919Z'
      numEdits: 0
      reactions: []
    id: 64f6bb6e5afaa96886686371
    type: comment
  author: ydshieh
  content: It should work now already. Let me know if you still encounter some issues.
  created_at: 2023-09-05 04:23:58+00:00
  edited: false
  hidden: false
  id: 64f6bb6e5afaa96886686371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
      fullname: Ashwath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashwath-Shetty
      type: user
    createdAt: '2023-09-05T05:59:49.000Z'
    data:
      edited: false
      editors:
      - Ashwath-Shetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.35140514373779297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
          fullname: Ashwath
          isHf: false
          isPro: false
          name: Ashwath-Shetty
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ydshieh\">@<span class=\"\
          underline\">ydshieh</span></a></span>\n\n\t</span></span><br>i restarted,\
          \  re ran the whole thing &amp; got a new error this time.<br>code,environment\
          \ &amp; everything else is same.</p>\n<p>error:</p>\n<hr>\n<p>TypeError\
          \                                 Traceback (most recent call last)<br>Cell\
          \ In[7], line 21<br>     13 image = Image.open(requests.get(url, stream=True).raw)<br>\
          \     15 # The original Kosmos-2 demo saves the image first then reload\
          \ it. For some images, this will give slightly different image input and\
          \ change the generation outputs.<br>     16 # Uncomment the following 2\
          \ lines if you want to match the original demo's outputs.<br>     17 # (One\
          \ example is the <code>two_dogs.jpg</code> from the demo)<br>     18 # image.save(\"\
          new_image.jpg\")<br>     19 # image = Image.open(\"new_image.jpg\")<br>---&gt;\
          \ 21 inputs = processor(text=prompt, images=image, return_tensors=\"pt\"\
          )<br>     23 generated_ids = model.generate(<br>     24     pixel_values=inputs[\"\
          pixel_values\"],<br>     25     input_ids=inputs[\"input_ids\"][:, :-1],<br>\
          \   (...)<br>     30     max_new_tokens=64,<br>     31 )<br>     32 generated_text\
          \ = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/processing_kosmos2.py:131,\
          \ in Kosmos2Processor.<strong>call</strong>(self, images, text, bboxes,\
          \ num_image_tokens, first_image_token_id, add_special_tokens, padding, truncation,\
          \ max_length, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_token_type_ids,\
          \ return_length, verbose, return_tensors, **kwargs)<br>    128 encoding.update(text_encoding)<br>\
          \    130 if images is not None:<br>--&gt; 131     image_encoding = self.image_processor(images,\
          \ return_tensors=return_tensors)<br>    132     encoding.update(image_encoding)<br>\
          \    134     # Use the id of the first token after </p>\n<p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/image_processing_utils.py:494,\
          \ in BaseImageProcessor.<strong>call</strong>(self, images, **kwargs)<br>\
          \    492 def <strong>call</strong>(self, images, **kwargs) -&gt; BatchFeature:<br>\
          \    493     \"\"\"Preprocess an image or a batch of images.\"\"\"<br>--&gt;\
          \ 494     return self.preprocess(images, **kwargs)</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:277,\
          \ in Kosmos2ImageProcessor.preprocess(self, images, do_resize, size, resample,\
          \ do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean,\
          \ image_std, do_convert_rgb, return_tensors, data_format, input_data_format,\
          \ **kwargs)<br>    274     input_data_format = infer_channel_dimension_format(images[0])<br>\
          \    276 if do_resize:<br>--&gt; 277     images = [<br>    278         self.resize(image=image,\
          \ size=size, resample=resample, input_data_format=input_data_format)<br>\
          \    279         for image in images<br>    280     ]<br>    282 if do_center_crop:<br>\
          \    283     images = [<br>    284         self.center_crop(image=image,\
          \ size=crop_size, input_data_format=input_data_format) for image in images<br>\
          \    285     ]</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:278,\
          \ in (.0)<br>    274     input_data_format = infer_channel_dimension_format(images[0])<br>\
          \    276 if do_resize:<br>    277     images = [<br>--&gt; 278         self.resize(image=image,\
          \ size=size, resample=resample, input_data_format=input_data_format)<br>\
          \    279         for image in images<br>    280     ]<br>    282 if do_center_crop:<br>\
          \    283     images = [<br>    284         self.center_crop(image=image,\
          \ size=crop_size, input_data_format=input_data_format) for image in images<br>\
          \    285     ]</p>\n<p>File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:150,\
          \ in Kosmos2ImageProcessor.resize(self, image, size, resample, data_format,\
          \ input_data_format, **kwargs)<br>    148 if \"shortest_edge\" not in size:<br>\
          \    149     raise ValueError(f\"The <code>size</code> parameter must contain\
          \ the key <code>shortest_edge</code>. Got {size.keys()}\")<br>--&gt; 150\
          \ output_size = get_resize_output_image_size(<br>    151     image, size=size[\"\
          shortest_edge\"], input_data_format=input_data_format<br>    152 )<br> \
          \   153 return resize(<br>    154     image,<br>    155     size=output_size,<br>\
          \   (...)<br>    159     **kwargs,<br>    160 )</p>\n<p>TypeError: get_resize_output_image_size()\
          \ got an unexpected keyword argument 'input_data_format'</p>\n<p>\u200B\
          </p>\n"
        raw: "@ydshieh \ni restarted,  re ran the whole thing & got a new error this\
          \ time.\ncode,environment & everything else is same.\n\nerror:\n\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          Cell In[7], line 21\n     13 image = Image.open(requests.get(url, stream=True).raw)\n\
          \     15 # The original Kosmos-2 demo saves the image first then reload\
          \ it. For some images, this will give slightly different image input and\
          \ change the generation outputs.\n     16 # Uncomment the following 2 lines\
          \ if you want to match the original demo's outputs.\n     17 # (One example\
          \ is the `two_dogs.jpg` from the demo)\n     18 # image.save(\"new_image.jpg\"\
          )\n     19 # image = Image.open(\"new_image.jpg\")\n---> 21 inputs = processor(text=prompt,\
          \ images=image, return_tensors=\"pt\")\n     23 generated_ids = model.generate(\n\
          \     24     pixel_values=inputs[\"pixel_values\"],\n     25     input_ids=inputs[\"\
          input_ids\"][:, :-1],\n   (...)\n     30     max_new_tokens=64,\n     31\
          \ )\n     32 generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
          \nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/processing_kosmos2.py:131,\
          \ in Kosmos2Processor.__call__(self, images, text, bboxes, num_image_tokens,\
          \ first_image_token_id, add_special_tokens, padding, truncation, max_length,\
          \ stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_token_type_ids,\
          \ return_length, verbose, return_tensors, **kwargs)\n    128 encoding.update(text_encoding)\n\
          \    130 if images is not None:\n--> 131     image_encoding = self.image_processor(images,\
          \ return_tensors=return_tensors)\n    132     encoding.update(image_encoding)\n\
          \    134     # Use the id of the first token after <unk>\n\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/image_processing_utils.py:494,\
          \ in BaseImageProcessor.__call__(self, images, **kwargs)\n    492 def __call__(self,\
          \ images, **kwargs) -> BatchFeature:\n    493     \"\"\"Preprocess an image\
          \ or a batch of images.\"\"\"\n--> 494     return self.preprocess(images,\
          \ **kwargs)\n\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:277,\
          \ in Kosmos2ImageProcessor.preprocess(self, images, do_resize, size, resample,\
          \ do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean,\
          \ image_std, do_convert_rgb, return_tensors, data_format, input_data_format,\
          \ **kwargs)\n    274     input_data_format = infer_channel_dimension_format(images[0])\n\
          \    276 if do_resize:\n--> 277     images = [\n    278         self.resize(image=image,\
          \ size=size, resample=resample, input_data_format=input_data_format)\n \
          \   279         for image in images\n    280     ]\n    282 if do_center_crop:\n\
          \    283     images = [\n    284         self.center_crop(image=image, size=crop_size,\
          \ input_data_format=input_data_format) for image in images\n    285    \
          \ ]\n\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:278,\
          \ in <listcomp>(.0)\n    274     input_data_format = infer_channel_dimension_format(images[0])\n\
          \    276 if do_resize:\n    277     images = [\n--> 278         self.resize(image=image,\
          \ size=size, resample=resample, input_data_format=input_data_format)\n \
          \   279         for image in images\n    280     ]\n    282 if do_center_crop:\n\
          \    283     images = [\n    284         self.center_crop(image=image, size=crop_size,\
          \ input_data_format=input_data_format) for image in images\n    285    \
          \ ]\n\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:150,\
          \ in Kosmos2ImageProcessor.resize(self, image, size, resample, data_format,\
          \ input_data_format, **kwargs)\n    148 if \"shortest_edge\" not in size:\n\
          \    149     raise ValueError(f\"The `size` parameter must contain the key\
          \ `shortest_edge`. Got {size.keys()}\")\n--> 150 output_size = get_resize_output_image_size(\n\
          \    151     image, size=size[\"shortest_edge\"], input_data_format=input_data_format\n\
          \    152 )\n    153 return resize(\n    154     image,\n    155     size=output_size,\n\
          \   (...)\n    159     **kwargs,\n    160 )\n\nTypeError: get_resize_output_image_size()\
          \ got an unexpected keyword argument 'input_data_format'\n\n\u200B"
        updatedAt: '2023-09-05T05:59:49.964Z'
      numEdits: 0
      reactions: []
    id: 64f6c3d512c53650bf69a969
    type: comment
  author: Ashwath-Shetty
  content: "@ydshieh \ni restarted,  re ran the whole thing & got a new error this\
    \ time.\ncode,environment & everything else is same.\n\nerror:\n\n---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    Cell In[7], line 21\n     13 image = Image.open(requests.get(url, stream=True).raw)\n\
    \     15 # The original Kosmos-2 demo saves the image first then reload it. For\
    \ some images, this will give slightly different image input and change the generation\
    \ outputs.\n     16 # Uncomment the following 2 lines if you want to match the\
    \ original demo's outputs.\n     17 # (One example is the `two_dogs.jpg` from\
    \ the demo)\n     18 # image.save(\"new_image.jpg\")\n     19 # image = Image.open(\"\
    new_image.jpg\")\n---> 21 inputs = processor(text=prompt, images=image, return_tensors=\"\
    pt\")\n     23 generated_ids = model.generate(\n     24     pixel_values=inputs[\"\
    pixel_values\"],\n     25     input_ids=inputs[\"input_ids\"][:, :-1],\n   (...)\n\
    \     30     max_new_tokens=64,\n     31 )\n     32 generated_text = processor.batch_decode(generated_ids,\
    \ skip_special_tokens=True)[0]\n\nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/processing_kosmos2.py:131,\
    \ in Kosmos2Processor.__call__(self, images, text, bboxes, num_image_tokens, first_image_token_id,\
    \ add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of,\
    \ return_attention_mask, return_overflowing_tokens, return_special_tokens_mask,\
    \ return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors,\
    \ **kwargs)\n    128 encoding.update(text_encoding)\n    130 if images is not\
    \ None:\n--> 131     image_encoding = self.image_processor(images, return_tensors=return_tensors)\n\
    \    132     encoding.update(image_encoding)\n    134     # Use the id of the\
    \ first token after <unk>\n\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/image_processing_utils.py:494,\
    \ in BaseImageProcessor.__call__(self, images, **kwargs)\n    492 def __call__(self,\
    \ images, **kwargs) -> BatchFeature:\n    493     \"\"\"Preprocess an image or\
    \ a batch of images.\"\"\"\n--> 494     return self.preprocess(images, **kwargs)\n\
    \nFile ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:277,\
    \ in Kosmos2ImageProcessor.preprocess(self, images, do_resize, size, resample,\
    \ do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean,\
    \ image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\n\
    \    274     input_data_format = infer_channel_dimension_format(images[0])\n \
    \   276 if do_resize:\n--> 277     images = [\n    278         self.resize(image=image,\
    \ size=size, resample=resample, input_data_format=input_data_format)\n    279\
    \         for image in images\n    280     ]\n    282 if do_center_crop:\n   \
    \ 283     images = [\n    284         self.center_crop(image=image, size=crop_size,\
    \ input_data_format=input_data_format) for image in images\n    285     ]\n\n\
    File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:278,\
    \ in <listcomp>(.0)\n    274     input_data_format = infer_channel_dimension_format(images[0])\n\
    \    276 if do_resize:\n    277     images = [\n--> 278         self.resize(image=image,\
    \ size=size, resample=resample, input_data_format=input_data_format)\n    279\
    \         for image in images\n    280     ]\n    282 if do_center_crop:\n   \
    \ 283     images = [\n    284         self.center_crop(image=image, size=crop_size,\
    \ input_data_format=input_data_format) for image in images\n    285     ]\n\n\
    File ~/.cache/huggingface/modules/transformers_modules/ydshieh/kosmos-2-patch14-224/48e3edebaeb02dc9fe105f40e85a43a3b440dc72/image_processing_kosmos2.py:150,\
    \ in Kosmos2ImageProcessor.resize(self, image, size, resample, data_format, input_data_format,\
    \ **kwargs)\n    148 if \"shortest_edge\" not in size:\n    149     raise ValueError(f\"\
    The `size` parameter must contain the key `shortest_edge`. Got {size.keys()}\"\
    )\n--> 150 output_size = get_resize_output_image_size(\n    151     image, size=size[\"\
    shortest_edge\"], input_data_format=input_data_format\n    152 )\n    153 return\
    \ resize(\n    154     image,\n    155     size=output_size,\n   (...)\n    159\
    \     **kwargs,\n    160 )\n\nTypeError: get_resize_output_image_size() got an\
    \ unexpected keyword argument 'input_data_format'\n\n\u200B"
  created_at: 2023-09-05 04:59:49+00:00
  edited: false
  hidden: false
  id: 64f6c3d512c53650bf69a969
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T06:40:47.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8651939034461975
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>I am not able to reproduce the issue with <code>input_data_format</code>.
          Are you still using the exact same code snippet I put in the README.md file?
          Also, what''s your <code>tranformers</code> version?</p>

          '
        raw: I am not able to reproduce the issue with `input_data_format`. Are you
          still using the exact same code snippet I put in the README.md file? Also,
          what's your `tranformers` version?
        updatedAt: '2023-09-05T06:40:47.112Z'
      numEdits: 0
      reactions: []
    id: 64f6cd6fcd32b782be95d206
    type: comment
  author: ydshieh
  content: I am not able to reproduce the issue with `input_data_format`. Are you
    still using the exact same code snippet I put in the README.md file? Also, what's
    your `tranformers` version?
  created_at: 2023-09-05 05:40:47+00:00
  edited: false
  hidden: false
  id: 64f6cd6fcd32b782be95d206
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
      fullname: Ashwath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashwath-Shetty
      type: user
    createdAt: '2023-09-05T06:56:52.000Z'
    data:
      edited: false
      editors:
      - Ashwath-Shetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.804031252861023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
          fullname: Ashwath
          isHf: false
          isPro: false
          name: Ashwath-Shetty
          type: user
        html: "<p>yes, i'm using the exact same code from the tutorial, transformers=4.31.0.<br>everything\
          \ is same from my first comment in this thread.<br><span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ydshieh\"\
          >@<span class=\"underline\">ydshieh</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'yes, i''m using the exact same code from the tutorial, transformers=4.31.0.

          everything is same from my first comment in this thread.

          @ydshieh '
        updatedAt: '2023-09-05T06:56:52.906Z'
      numEdits: 0
      reactions: []
    id: 64f6d1349c2aeb2a3d00ab66
    type: comment
  author: Ashwath-Shetty
  content: 'yes, i''m using the exact same code from the tutorial, transformers=4.31.0.

    everything is same from my first comment in this thread.

    @ydshieh '
  created_at: 2023-09-05 05:56:52+00:00
  edited: false
  hidden: false
  id: 64f6d1349c2aeb2a3d00ab66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T07:05:43.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9097932577133179
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>In this case , better if you provide a google colab notebook that
          shows the issue and share a link to that notebook.</p>

          '
        raw: In this case , better if you provide a google colab notebook that shows
          the issue and share a link to that notebook.
        updatedAt: '2023-09-05T07:05:43.051Z'
      numEdits: 0
      reactions: []
    id: 64f6d347eedb7a2638121273
    type: comment
  author: ydshieh
  content: In this case , better if you provide a google colab notebook that shows
    the issue and share a link to that notebook.
  created_at: 2023-09-05 06:05:43+00:00
  edited: false
  hidden: false
  id: 64f6d347eedb7a2638121273
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
      fullname: Ashwath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashwath-Shetty
      type: user
    createdAt: '2023-09-05T10:19:38.000Z'
    data:
      edited: false
      editors:
      - Ashwath-Shetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9906917214393616
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8425c02aea8d6b36b144cf834970fcf.svg
          fullname: Ashwath
          isHf: false
          isPro: false
          name: Ashwath-Shetty
          type: user
        html: '<p>thanks, i have restarted the instance &amp; re did the whole thing,
          it''s working now.</p>

          '
        raw: thanks, i have restarted the instance & re did the whole thing, it's
          working now.
        updatedAt: '2023-09-05T10:19:38.511Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ydshieh
    id: 64f700ba28bdc09c39ee709f
    type: comment
  author: Ashwath-Shetty
  content: thanks, i have restarted the instance & re did the whole thing, it's working
    now.
  created_at: 2023-09-05 09:19:38+00:00
  edited: false
  hidden: false
  id: 64f700ba28bdc09c39ee709f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-09-05T11:06:22.000Z'
    data:
      status: closed
    id: 64f70baedb38c520bd135c24
    type: status-change
  author: ydshieh
  created_at: 2023-09-05 10:06:22+00:00
  id: 64f70baedb38c520bd135c24
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: ydshieh/kosmos-2-patch14-224
repo_type: model
status: closed
target_branch: null
title: 'AttributeError: can''t set attribute ''can_save_slow_tokenizer'''
