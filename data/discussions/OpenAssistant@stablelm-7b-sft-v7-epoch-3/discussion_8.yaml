!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carlosmoises
conflicting_files: null
created_at: 2023-06-23 03:52:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78ac76b05a373aa5288f4772c3eb2e27.svg
      fullname: Carlos Castro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosmoises
      type: user
    createdAt: '2023-06-23T04:52:29.000Z'
    data:
      edited: false
      editors:
      - carlosmoises
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9312602281570435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78ac76b05a373aa5288f4772c3eb2e27.svg
          fullname: Carlos Castro
          isHf: false
          isPro: false
          name: carlosmoises
          type: user
        html: '<p>I''m having this error in a Jupyter Notebook exercise  </p>

          <p>ValueError: The current <code>device_map</code> had weights offloaded
          to the disk. Please provide an <code>offload_folder</code> for them. Alternatively,
          make sure you have <code>safetensors</code> installed if the model you are
          using offers the weights in this format.</p>

          <p>Can anybody help me?</p>

          '
        raw: "I'm having this error in a Jupyter Notebook exercise  \r\n\r\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\r\
          \n\r\nCan anybody help me?"
        updatedAt: '2023-06-23T04:52:29.959Z'
      numEdits: 0
      reactions: []
    id: 6495250dbda16df2935d2d00
    type: comment
  author: carlosmoises
  content: "I'm having this error in a Jupyter Notebook exercise  \r\n\r\nValueError:\
    \ The current `device_map` had weights offloaded to the disk. Please provide an\
    \ `offload_folder` for them. Alternatively, make sure you have `safetensors` installed\
    \ if the model you are using offers the weights in this format.\r\n\r\nCan anybody\
    \ help me?"
  created_at: 2023-06-23 03:52:29+00:00
  edited: false
  hidden: false
  id: 6495250dbda16df2935d2d00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b3778ca45e831688002f24ec981442b.svg
      fullname: asoriap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gurumuru
      type: user
    createdAt: '2023-07-21T09:01:58.000Z'
    data:
      edited: false
      editors:
      - gurumuru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23897889256477356
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b3778ca45e831688002f24ec981442b.svg
          fullname: asoriap
          isHf: false
          isPro: false
          name: gurumuru
          type: user
        html: "<p>I have the same issue (not Jupiter notebook in my case) although\
          \ I have followed advices found in forums and youtube videos, but no success\
          \ yet :(</p>\n<p>I share my testing code, launched from Console Prompt (Windows\
          \ 11 Enterprise with 16GB RAM), and error details, in case anyone could\
          \ help here. </p>\n<p>Code:</p>\n<p>import langchain<br>from langchain import\
          \ HuggingFacePipeline<br>from langchain import PromptTemplate<br>from langchain\
          \ import LLMChain<br>from langchain.document_loaders import OnlinePDFLoader<br>import\
          \ torch<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>\n\
          <p>model_name = \"OpenAssistant/stablelm-7b-sft-v7-epoch-3\"<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>\
          \    model_name, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16<br>)<br>model.tie_weights()</p>\n\
          <p>llm = HuggingFacePipeline.from_model_id(model_id = model_name,<br>  \
          \                                               task = \"text-generation\"\
          , model_kwargs = {<br>                                                 \"\
          temperature\" : 0.0, \"max_length\" : 2048, \"device_map\" : \"auto\"})\
          \ </p>\n<p>loader = OnlinePDFLoader(\"<a rel=\"nofollow\" href=\"https://arxiv.org/pdf/1911.01547.pdf&quot;\"\
          >https://arxiv.org/pdf/1911.01547.pdf\"</a>)<br>document = loader.load()</p>\n\
          <p>template = \"\"\"&lt;|prompter|&gt;{question}&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          \"\"<br>prompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])<br>llm_chain = LLMChain(prompt=prompt, llm=llm)<br>question = \"What\
          \ is the meaning of life?\"<br>llm_chain.run(question)</p>\n<p>Output:<br>The\
          \ model weights are not tied. Please use the <code>tie_weights</code> method\
          \ before using the <code>infer_auto_device</code> function.<br>Loading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 9/9 [00:50&lt;00:00,  5.66s/it]<br>The model weights\
          \ are not tied. Please use the <code>tie_weights</code> method before using\
          \ the <code>infer_auto_device</code> function.</p>\n<p>  20            \
          \                                                                      \
          \           \u2502<br>\u2502   21                                      \
          \                                                       \u2502<br>\u2502\
          \   22                                                                 \
          \                            \u2502<br>\u2502 \u2771 23 llm = HuggingFacePipeline.from_model_id(model_id\
          \ = model_name,                              \u2502<br>\u2502   24 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502    task = \"text-generation\", model_kwargs\
          \     \u2502<br>\u2502   25 \u2502   \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    \"temperature\"\
          \ : 0.0, \"max_length\" : 204    \u2502<br>\u2502   26                 \
          \                                                                      \
          \      \u2502<br>\u2502                                                \
          \                                                  \u2502<br>\u2502 c:\\\
          Program Files\\Python38\\lib\\site-packages\\langchain\\llms\\huggingface_pipeline.py:92\
          \ in         \u2502<br>\u2502 from_model_id                            \
          \                                                        \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    89 \u2502   \u2502    \
          \                                                                      \
          \            \u2502<br>\u2502    90 \u2502   \u2502   try:             \
          \                                                                  \u2502\
          <br>\u2502    91 \u2502   \u2502   \u2502   if task == \"text-generation\"\
          :                                                  \u2502<br>\u2502 \u2771\
          \  92 \u2502   \u2502   \u2502   \u2502   model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ **_model_kwargs)    \u2502<br>\u2502    93 \u2502   \u2502   \u2502  \
          \ elif task in (\"text2text-generation\", \"summarization\"):          \
          \              \u2502<br>\u2502    94 \u2502   \u2502   \u2502   \u2502\
          \   model = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)\
          \   \u2502<br>\u2502    95 \u2502   \u2502   \u2502   else:            \
          \                                                              \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 c:\\Program Files\\Python38\\\
          lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:467 in \
          \     \u2502<br>\u2502 from_pretrained                                 \
          \                                                 \u2502<br>\u2502     \
          \                                                                      \
          \                       \u2502<br>\u2502   464 \u2502   \u2502   \u2502\
          \   )                                                                  \
          \            \u2502<br>\u2502   465 \u2502   \u2502   elif type(config)\
          \ in cls._model_mapping.keys():                                    \u2502\
          <br>\u2502   466 \u2502   \u2502   \u2502   model_class = _get_model_class(config,\
          \ cls._model_mapping)                     \u2502<br>\u2502 \u2771 467 \u2502\
          \   \u2502   \u2502   return model_class.from_pretrained(              \
          \                              \u2502<br>\u2502   468 \u2502   \u2502  \
          \ \u2502   \u2502   pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs,   \u2502<br>\u2502   469 \u2502   \u2502   \u2502   )  \
          \                                                                      \
          \      \u2502<br>\u2502   470 \u2502   \u2502   raise ValueError(      \
          \                                                            \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 c:\\Program Files\\Python38\\\
          lib\\site-packages\\transformers\\modeling_utils.py:2777 in            \
          \   \u2502<br>\u2502 from_pretrained                                   \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502   2774 \u2502   \u2502   \u2502 \
          \  \u2502   mismatched_keys,                                           \
          \               \u2502<br>\u2502   2775 \u2502   \u2502   \u2502   \u2502\
          \   offload_index,                                                     \
          \       \u2502<br>\u2502   2776 \u2502   \u2502   \u2502   \u2502   error_msgs,\
          \                                                               \u2502<br>\u2502\
          \ \u2771 2777 \u2502   \u2502   \u2502   ) = cls._load_pretrained_model(\
          \                                               \u2502<br>\u2502   2778\
          \ \u2502   \u2502   \u2502   \u2502   model,                           \
          \                                         \u2502<br>\u2502   2779 \u2502\
          \   \u2502   \u2502   \u2502   state_dict,                             \
          \                                  \u2502<br>\u2502   2780 \u2502   \u2502\
          \   \u2502   \u2502   loaded_state_dict_keys,  # XXX: rename?          \
          \                         \u2502<br>\u2502                             \
          \                                                                     \u2502\
          <br>\u2502 c:\\Program Files\\Python38\\lib\\site-packages\\transformers\\\
          modeling_utils.py:2871 in               \u2502<br>\u2502 _load_pretrained_model\
          \                                                                      \
          \     \u2502<br>\u2502                                                 \
          \                                                 \u2502<br>\u2502   2868\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                    \u2502<br>\u2502   2869 \u2502   \u2502\
          \   \u2502   is_safetensors = archive_file.endswith(\".safetensors\")  \
          \                      \u2502<br>\u2502   2870 \u2502   \u2502   \u2502\
          \   if offload_folder is None and not is_safetensors:                  \
          \           \u2502<br>\u2502 \u2771 2871 \u2502   \u2502   \u2502   \u2502\
          \   raise ValueError(                                                  \
          \       \u2502<br>\u2502   2872 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \"The current <code>device_map</code> had weights offloaded to the disk.\
          \ Please   \u2502<br>\u2502   2873 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \" for them. Alternatively, make sure you have <code>safetensors</code>\
          \ installe  \u2502<br>\u2502   2874 \u2502   \u2502   \u2502   \u2502  \
          \ \u2502   \" offers the weights in this format.\"                     \
          \            \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>ValueError: The current <code>device_map</code>\
          \ had weights offloaded to the disk. Please provide an <code>offload_folder</code>\
          \ for them. Alternatively, make sure you have <code>safetensors</code> installed\
          \ if the model you are using offers<br>the weights in this format.</p>\n\
          <p>Thanks a lot in advance!!</p>\n"
        raw: "I have the same issue (not Jupiter notebook in my case) although I have\
          \ followed advices found in forums and youtube videos, but no success yet\
          \ :(\n\nI share my testing code, launched from Console Prompt (Windows 11\
          \ Enterprise with 16GB RAM), and error details, in case anyone could help\
          \ here. \n\nCode:\n\nimport langchain\nfrom langchain import HuggingFacePipeline\n\
          from langchain import PromptTemplate\nfrom langchain import LLMChain\nfrom\
          \ langchain.document_loaders import OnlinePDFLoader\nimport torch\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name\
          \ = \"OpenAssistant/stablelm-7b-sft-v7-epoch-3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"\
          auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n)\nmodel.tie_weights()\n\
          \nllm = HuggingFacePipeline.from_model_id(model_id = model_name,\n     \
          \                                            task = \"text-generation\"\
          , model_kwargs = {\n                                                 \"\
          temperature\" : 0.0, \"max_length\" : 2048, \"device_map\" : \"auto\"})\
          \ \n\nloader = OnlinePDFLoader(\"https://arxiv.org/pdf/1911.01547.pdf\"\
          )\ndocument = loader.load()\n\ntemplate = \"\"\"<|prompter|>{question}<|endoftext|><|assistant|>\"\
          \"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"What is the\
          \ meaning of life?\"\nllm_chain.run(question)\n\n\nOutput:\nThe model weights\
          \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:50<00:00,\
          \  5.66s/it]\nThe model weights are not tied. Please use the `tie_weights`\
          \ method before using the `infer_auto_device` function.\n\n  20        \
          \                                                                      \
          \               \u2502\n\u2502   21                                    \
          \                                                         \u2502\n\u2502\
          \   22                                                                 \
          \                            \u2502\n\u2502 \u2771 23 llm = HuggingFacePipeline.from_model_id(model_id\
          \ = model_name,                              \u2502\n\u2502   24 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502    task = \"text-generation\", model_kwargs\
          \     \u2502\n\u2502   25 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    \"temperature\"\
          \ : 0.0, \"max_length\" : 204    \u2502\n\u2502   26                   \
          \                                                                      \
          \    \u2502\n\u2502                                                    \
          \                                              \u2502\n\u2502 c:\\Program\
          \ Files\\Python38\\lib\\site-packages\\langchain\\llms\\huggingface_pipeline.py:92\
          \ in         \u2502\n\u2502 from_model_id                              \
          \                                                      \u2502\n\u2502  \
          \                                                                      \
          \                          \u2502\n\u2502    89 \u2502   \u2502        \
          \                                                                      \
          \        \u2502\n\u2502    90 \u2502   \u2502   try:                   \
          \                                                            \u2502\n\u2502\
          \    91 \u2502   \u2502   \u2502   if task == \"text-generation\":     \
          \                                             \u2502\n\u2502 \u2771  92\
          \ \u2502   \u2502   \u2502   \u2502   model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ **_model_kwargs)    \u2502\n\u2502    93 \u2502   \u2502   \u2502   elif\
          \ task in (\"text2text-generation\", \"summarization\"):               \
          \         \u2502\n\u2502    94 \u2502   \u2502   \u2502   \u2502   model\
          \ = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)   \u2502\
          \n\u2502    95 \u2502   \u2502   \u2502   else:                        \
          \                                                  \u2502\n\u2502      \
          \                                                                      \
          \                      \u2502\n\u2502 c:\\Program Files\\Python38\\lib\\\
          site-packages\\transformers\\models\\auto\\auto_factory.py:467 in      \u2502\
          \n\u2502 from_pretrained                                               \
          \                                   \u2502\n\u2502                     \
          \                                                                      \
          \       \u2502\n\u2502   464 \u2502   \u2502   \u2502   )              \
          \                                                                \u2502\n\
          \u2502   465 \u2502   \u2502   elif type(config) in cls._model_mapping.keys():\
          \                                    \u2502\n\u2502   466 \u2502   \u2502\
          \   \u2502   model_class = _get_model_class(config, cls._model_mapping)\
          \                     \u2502\n\u2502 \u2771 467 \u2502   \u2502   \u2502\
          \   return model_class.from_pretrained(                                \
          \            \u2502\n\u2502   468 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs,   \u2502\n\u2502   469 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\n\u2502   470 \u2502   \u2502   raise\
          \ ValueError(                                                          \
          \        \u2502\n\u2502                                                \
          \                                                  \u2502\n\u2502 c:\\Program\
          \ Files\\Python38\\lib\\site-packages\\transformers\\modeling_utils.py:2777\
          \ in               \u2502\n\u2502 from_pretrained                      \
          \                                                            \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   2774 \u2502   \u2502   \u2502\
          \   \u2502   mismatched_keys,                                          \
          \                \u2502\n\u2502   2775 \u2502   \u2502   \u2502   \u2502\
          \   offload_index,                                                     \
          \       \u2502\n\u2502   2776 \u2502   \u2502   \u2502   \u2502   error_msgs,\
          \                                                               \u2502\n\
          \u2502 \u2771 2777 \u2502   \u2502   \u2502   ) = cls._load_pretrained_model(\
          \                                               \u2502\n\u2502   2778 \u2502\
          \   \u2502   \u2502   \u2502   model,                                  \
          \                                  \u2502\n\u2502   2779 \u2502   \u2502\
          \   \u2502   \u2502   state_dict,                                      \
          \                         \u2502\n\u2502   2780 \u2502   \u2502   \u2502\
          \   \u2502   loaded_state_dict_keys,  # XXX: rename?                   \
          \                \u2502\n\u2502                                        \
          \                                                          \u2502\n\u2502\
          \ c:\\Program Files\\Python38\\lib\\site-packages\\transformers\\modeling_utils.py:2871\
          \ in               \u2502\n\u2502 _load_pretrained_model               \
          \                                                            \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   2868 \u2502   \u2502   \u2502\
          \   )                                                                  \
          \           \u2502\n\u2502   2869 \u2502   \u2502   \u2502   is_safetensors\
          \ = archive_file.endswith(\".safetensors\")                        \u2502\
          \n\u2502   2870 \u2502   \u2502   \u2502   if offload_folder is None and\
          \ not is_safetensors:                             \u2502\n\u2502 \u2771\
          \ 2871 \u2502   \u2502   \u2502   \u2502   raise ValueError(           \
          \                                              \u2502\n\u2502   2872 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   \"The current `device_map` had weights\
          \ offloaded to the disk. Please   \u2502\n\u2502   2873 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \" for them. Alternatively, make sure you\
          \ have `safetensors` installe  \u2502\n\u2502   2874 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \" offers the weights in this format.\"          \
          \                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: The\
          \ current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers\nthe weights in this format.\n\
          \nThanks a lot in advance!!"
        updatedAt: '2023-07-21T09:01:58.004Z'
      numEdits: 0
      reactions: []
    id: 64ba49866fc6de2f9b681d3c
    type: comment
  author: gurumuru
  content: "I have the same issue (not Jupiter notebook in my case) although I have\
    \ followed advices found in forums and youtube videos, but no success yet :(\n\
    \nI share my testing code, launched from Console Prompt (Windows 11 Enterprise\
    \ with 16GB RAM), and error details, in case anyone could help here. \n\nCode:\n\
    \nimport langchain\nfrom langchain import HuggingFacePipeline\nfrom langchain\
    \ import PromptTemplate\nfrom langchain import LLMChain\nfrom langchain.document_loaders\
    \ import OnlinePDFLoader\nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\nmodel_name = \"OpenAssistant/stablelm-7b-sft-v7-epoch-3\"\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    model_name, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n\
    )\nmodel.tie_weights()\n\nllm = HuggingFacePipeline.from_model_id(model_id = model_name,\n\
    \                                                 task = \"text-generation\",\
    \ model_kwargs = {\n                                                 \"temperature\"\
    \ : 0.0, \"max_length\" : 2048, \"device_map\" : \"auto\"}) \n\nloader = OnlinePDFLoader(\"\
    https://arxiv.org/pdf/1911.01547.pdf\")\ndocument = loader.load()\n\ntemplate\
    \ = \"\"\"<|prompter|>{question}<|endoftext|><|assistant|>\"\"\"\nprompt = PromptTemplate(template=template,\
    \ input_variables=[\"question\"])\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\
    question = \"What is the meaning of life?\"\nllm_chain.run(question)\n\n\nOutput:\n\
    The model weights are not tied. Please use the `tie_weights` method before using\
    \ the `infer_auto_device` function.\nLoading checkpoint shards: 100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 9/9 [00:50<00:00,  5.66s/it]\nThe model weights are not tied. Please use\
    \ the `tie_weights` method before using the `infer_auto_device` function.\n\n\
    \  20                                                                        \
    \                     \u2502\n\u2502   21                                    \
    \                                                         \u2502\n\u2502   22\
    \                                                                            \
    \                 \u2502\n\u2502 \u2771 23 llm = HuggingFacePipeline.from_model_id(model_id\
    \ = model_name,                              \u2502\n\u2502   24 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502    task = \"text-generation\", model_kwargs     \u2502\n\u2502   25\
    \ \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   \u2502    \"temperature\" : 0.0, \"max_length\" : 204  \
    \  \u2502\n\u2502   26                                                       \
    \                                      \u2502\n\u2502                        \
    \                                                                          \u2502\
    \n\u2502 c:\\Program Files\\Python38\\lib\\site-packages\\langchain\\llms\\huggingface_pipeline.py:92\
    \ in         \u2502\n\u2502 from_model_id                                    \
    \                                                \u2502\n\u2502              \
    \                                                                            \
    \        \u2502\n\u2502    89 \u2502   \u2502                                \
    \                                                      \u2502\n\u2502    90 \u2502\
    \   \u2502   try:                                                            \
    \                   \u2502\n\u2502    91 \u2502   \u2502   \u2502   if task ==\
    \ \"text-generation\":                                                  \u2502\
    \n\u2502 \u2771  92 \u2502   \u2502   \u2502   \u2502   model = AutoModelForCausalLM.from_pretrained(model_id,\
    \ **_model_kwargs)    \u2502\n\u2502    93 \u2502   \u2502   \u2502   elif task\
    \ in (\"text2text-generation\", \"summarization\"):                        \u2502\
    \n\u2502    94 \u2502   \u2502   \u2502   \u2502   model = AutoModelForSeq2SeqLM.from_pretrained(model_id,\
    \ **_model_kwargs)   \u2502\n\u2502    95 \u2502   \u2502   \u2502   else:   \
    \                                                                       \u2502\
    \n\u2502                                                                     \
    \                             \u2502\n\u2502 c:\\Program Files\\Python38\\lib\\\
    site-packages\\transformers\\models\\auto\\auto_factory.py:467 in      \u2502\n\
    \u2502 from_pretrained                                                       \
    \                           \u2502\n\u2502                                   \
    \                                                               \u2502\n\u2502\
    \   464 \u2502   \u2502   \u2502   )                                         \
    \                                     \u2502\n\u2502   465 \u2502   \u2502   elif\
    \ type(config) in cls._model_mapping.keys():                                 \
    \   \u2502\n\u2502   466 \u2502   \u2502   \u2502   model_class = _get_model_class(config,\
    \ cls._model_mapping)                     \u2502\n\u2502 \u2771 467 \u2502   \u2502\
    \   \u2502   return model_class.from_pretrained(                             \
    \               \u2502\n\u2502   468 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs,   \u2502\n\u2502   469 \u2502   \u2502\
    \   \u2502   )                                                               \
    \               \u2502\n\u2502   470 \u2502   \u2502   raise ValueError(     \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                     \u2502\n\u2502 c:\\Program Files\\Python38\\lib\\site-packages\\\
    transformers\\modeling_utils.py:2777 in               \u2502\n\u2502 from_pretrained\
    \                                                                            \
    \      \u2502\n\u2502                                                        \
    \                                          \u2502\n\u2502   2774 \u2502   \u2502\
    \   \u2502   \u2502   mismatched_keys,                                       \
    \                   \u2502\n\u2502   2775 \u2502   \u2502   \u2502   \u2502  \
    \ offload_index,                                                            \u2502\
    \n\u2502   2776 \u2502   \u2502   \u2502   \u2502   error_msgs,              \
    \                                                 \u2502\n\u2502 \u2771 2777 \u2502\
    \   \u2502   \u2502   ) = cls._load_pretrained_model(                        \
    \                       \u2502\n\u2502   2778 \u2502   \u2502   \u2502   \u2502\
    \   model,                                                                   \
    \ \u2502\n\u2502   2779 \u2502   \u2502   \u2502   \u2502   state_dict,      \
    \                                                         \u2502\n\u2502   2780\
    \ \u2502   \u2502   \u2502   \u2502   loaded_state_dict_keys,  # XXX: rename?\
    \                                   \u2502\n\u2502                           \
    \                                                                       \u2502\
    \n\u2502 c:\\Program Files\\Python38\\lib\\site-packages\\transformers\\modeling_utils.py:2871\
    \ in               \u2502\n\u2502 _load_pretrained_model                     \
    \                                                      \u2502\n\u2502        \
    \                                                                            \
    \              \u2502\n\u2502   2868 \u2502   \u2502   \u2502   )            \
    \                                                                 \u2502\n\u2502\
    \   2869 \u2502   \u2502   \u2502   is_safetensors = archive_file.endswith(\"\
    .safetensors\")                        \u2502\n\u2502   2870 \u2502   \u2502 \
    \  \u2502   if offload_folder is None and not is_safetensors:                \
    \             \u2502\n\u2502 \u2771 2871 \u2502   \u2502   \u2502   \u2502   raise\
    \ ValueError(                                                         \u2502\n\
    \u2502   2872 \u2502   \u2502   \u2502   \u2502   \u2502   \"The current `device_map`\
    \ had weights offloaded to the disk. Please   \u2502\n\u2502   2873 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \" for them. Alternatively, make sure you have\
    \ `safetensors` installe  \u2502\n\u2502   2874 \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \" offers the weights in this format.\"                         \
    \        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError:\
    \ The current `device_map` had weights offloaded to the disk. Please provide an\
    \ `offload_folder` for them. Alternatively, make sure you have `safetensors` installed\
    \ if the model you are using offers\nthe weights in this format.\n\nThanks a lot\
    \ in advance!!"
  created_at: 2023-07-21 08:01:58+00:00
  edited: false
  hidden: false
  id: 64ba49866fc6de2f9b681d3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: OpenAssistant/stablelm-7b-sft-v7-epoch-3
repo_type: model
status: open
target_branch: null
title: The model weights are not tied. Please use the `tie_weights` method before
  using the `infer_auto_device` function.
