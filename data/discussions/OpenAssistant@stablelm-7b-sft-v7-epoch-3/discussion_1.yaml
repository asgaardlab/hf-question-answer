!!python/object:huggingface_hub.community.DiscussionWithDetails
author: antplsdev
conflicting_files: null
created_at: 2023-04-22 21:55:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478c7f6b9161e59873a1c999c134c81b.svg
      fullname: ant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: antplsdev
      type: user
    createdAt: '2023-04-22T22:55:18.000Z'
    data:
      edited: false
      editors:
      - antplsdev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478c7f6b9161e59873a1c999c134c81b.svg
          fullname: ant
          isHf: false
          isPro: false
          name: antplsdev
          type: user
        html: '<p>Hello,</p>

          <p>Would it be possible to re-upload or create a branch with shards size
          of 2GB and safetensors?</p>

          <p>I would have converted to safetensors myself, but 10GB shards are impossible
          to convert to safetensors format on average configuration (&lt;20GB of CPU
          RAM), and subsequently the entire model is impossible to run too.</p>

          <p>As an example, you can take a look at <a href="https://huggingface.co/waifu-workshop/pygmalion-6b/tree/main">https://huggingface.co/waifu-workshop/pygmalion-6b/tree/main</a><br>This
          has two branches : the original with 10GB shards (cannot be run on low configuration),
          and the "sharded" branch, with 2GB safetensors shards, which can run on
          low end configuration.</p>

          <p>2GB safetensors can be created by adding the following parameters to
          the save_pretrained function :</p>

          <ul>

          <li>max_shard_size="2GB"</li>

          <li>safe_serialization=True</li>

          </ul>

          <p>as documented :  <a href="https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size">https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size</a>
          </p>

          <p>Thank you</p>

          '
        raw: "Hello,\r\n\r\nWould it be possible to re-upload or create a branch with\
          \ shards size of 2GB and safetensors?\r\n\r\nI would have converted to safetensors\
          \ myself, but 10GB shards are impossible to convert to safetensors format\
          \ on average configuration (<20GB of CPU RAM), and subsequently the entire\
          \ model is impossible to run too.\r\n\r\nAs an example, you can take a look\
          \ at https://huggingface.co/waifu-workshop/pygmalion-6b/tree/main\r\nThis\
          \ has two branches : the original with 10GB shards (cannot be run on low\
          \ configuration), and the \"sharded\" branch, with 2GB safetensors shards,\
          \ which can run on low end configuration.\r\n\r\n2GB safetensors can be\
          \ created by adding the following parameters to the save_pretrained function\
          \ :\r\n- max_shard_size=\"2GB\"\r\n- safe_serialization=True\r\n\r\nas documented\
          \ :  https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\
          \ \r\n\r\nThank you"
        updatedAt: '2023-04-22T22:55:18.914Z'
      numEdits: 0
      reactions: []
    id: 644465d63c323e0918f9a707
    type: comment
  author: antplsdev
  content: "Hello,\r\n\r\nWould it be possible to re-upload or create a branch with\
    \ shards size of 2GB and safetensors?\r\n\r\nI would have converted to safetensors\
    \ myself, but 10GB shards are impossible to convert to safetensors format on average\
    \ configuration (<20GB of CPU RAM), and subsequently the entire model is impossible\
    \ to run too.\r\n\r\nAs an example, you can take a look at https://huggingface.co/waifu-workshop/pygmalion-6b/tree/main\r\
    \nThis has two branches : the original with 10GB shards (cannot be run on low\
    \ configuration), and the \"sharded\" branch, with 2GB safetensors shards, which\
    \ can run on low end configuration.\r\n\r\n2GB safetensors can be created by adding\
    \ the following parameters to the save_pretrained function :\r\n- max_shard_size=\"\
    2GB\"\r\n- safe_serialization=True\r\n\r\nas documented :  https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained.max_shard_size\
    \ \r\n\r\nThank you"
  created_at: 2023-04-22 21:55:18+00:00
  edited: false
  hidden: false
  id: 644465d63c323e0918f9a707
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678365850657-640501091a3babee78e41e99.jpeg?w=200&h=200&f=face
      fullname: Dimitri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dvruette
      type: user
    createdAt: '2023-04-23T09:41:21.000Z'
    data:
      edited: false
      editors:
      - dvruette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678365850657-640501091a3babee78e41e99.jpeg?w=200&h=200&f=face
          fullname: Dimitri
          isHf: false
          isPro: false
          name: dvruette
          type: user
        html: '<p>It''s a bit weird that a 10BG shard would take 20GB of memory. Are
          you sure you''re loading the model in fp16? Either way, yes I can upload
          a safetensor version no problem.</p>

          '
        raw: It's a bit weird that a 10BG shard would take 20GB of memory. Are you
          sure you're loading the model in fp16? Either way, yes I can upload a safetensor
          version no problem.
        updatedAt: '2023-04-23T09:41:21.402Z'
      numEdits: 0
      reactions: []
    id: 6444fd410f2fc80feb1f39bf
    type: comment
  author: dvruette
  content: It's a bit weird that a 10BG shard would take 20GB of memory. Are you sure
    you're loading the model in fp16? Either way, yes I can upload a safetensor version
    no problem.
  created_at: 2023-04-23 08:41:21+00:00
  edited: false
  hidden: false
  id: 6444fd410f2fc80feb1f39bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478c7f6b9161e59873a1c999c134c81b.svg
      fullname: ant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: antplsdev
      type: user
    createdAt: '2023-04-23T19:28:48.000Z'
    data:
      edited: false
      editors:
      - antplsdev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478c7f6b9161e59873a1c999c134c81b.svg
          fullname: ant
          isHf: false
          isPro: false
          name: antplsdev
          type: user
        html: "<p>I followed tips from : </p>\n<ul>\n<li><a href=\"https://huggingface.co/docs/transformers/main/en/main_classes/quantization\"\
          >https://huggingface.co/docs/transformers/main/en/main_classes/quantization</a></li>\n\
          <li><a href=\"https://huggingface.co/docs/accelerate/usage_guides/big_modeling\"\
          >https://huggingface.co/docs/accelerate/usage_guides/big_modeling</a></li>\n\
          </ul>\n<p>The machine has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of\
          \ GB for offload_folder.</p>\n<p>In the end, I used the following script\
          \ : </p>\n<pre><code>from transformers import AutoModelForCausalLM\nimport\
          \ torch\n\ncheckpoint = \"stablelm_oa_7b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          checkpoint, \ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nmax_memory={0:\
          \ \"7GB\", \"cpu\": \"3GB\"},\noffload_folder=\"/offload\"\n)\n\nmodel.save_pretrained(\"\
          ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n</code></pre>\n\
          <p>(For some reasons I have to set the CPU to 3GB or it would OOM)</p>\n\
          <p>The model is indeed loaded, but saving it fails with : </p>\n<p><code>Traceback\
          \ (most recent call last):   File \"/src/run_convert.py\", line 24, in &lt;module&gt;\
          \     model.save_pretrained(\"./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\
          \   File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 1841, in save_pretrained     safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 72, in save_file     serialize_file(_flatten(tensors), filename,\
          \ metadata=metadata)   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 237, in _flatten     return {   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 241, in &lt;dictcomp&gt;     \"data\": _tobytes(v, k),   File \"\
          /usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 193,\
          \ in _tobytes     tensor = tensor.to(\"cpu\") NotImplementedError: Cannot\
          \ copy out of meta tensor; no data!</code></p>\n<p>It could be a bug in\
          \ safetensors library</p>\n"
        raw: "I followed tips from : \n- https://huggingface.co/docs/transformers/main/en/main_classes/quantization\n\
          - https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n\nThe\
          \ machine has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of GB for offload_folder.\n\
          \nIn the end, I used the following script : \n\n    from transformers import\
          \ AutoModelForCausalLM\n    import torch\n\n    checkpoint = \"stablelm_oa_7b\"\
          \n\n    model = AutoModelForCausalLM.from_pretrained(\n\tcheckpoint, \n\t\
          torch_dtype=torch.float16,\n\tdevice_map=\"auto\",\n\tmax_memory={0: \"\
          7GB\", \"cpu\": \"3GB\"},\n\toffload_folder=\"/offload\"\n\t)\n\n    model.save_pretrained(\"\
          ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n\n(For some\
          \ reasons I have to set the CPU to 3GB or it would OOM)\n\nThe model is\
          \ indeed loaded, but saving it fails with : \n\n``\nTraceback (most recent\
          \ call last):\n  File \"/src/run_convert.py\", line 24, in <module>\n  \
          \  model.save_pretrained(\"./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 1841, in save_pretrained\n    safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 72, in save_file\n    serialize_file(_flatten(tensors), filename,\
          \ metadata=metadata)\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 237, in _flatten\n    return {\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 241, in <dictcomp>\n    \"data\": _tobytes(v, k),\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 193, in _tobytes\n    tensor = tensor.to(\"cpu\")\nNotImplementedError:\
          \ Cannot copy out of meta tensor; no data!\n``\n\nIt could be a bug in safetensors\
          \ library"
        updatedAt: '2023-04-23T19:28:48.113Z'
      numEdits: 0
      reactions: []
    id: 644586f0d1460e859d2099b6
    type: comment
  author: antplsdev
  content: "I followed tips from : \n- https://huggingface.co/docs/transformers/main/en/main_classes/quantization\n\
    - https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n\nThe machine\
    \ has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of GB for offload_folder.\n\
    \nIn the end, I used the following script : \n\n    from transformers import AutoModelForCausalLM\n\
    \    import torch\n\n    checkpoint = \"stablelm_oa_7b\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n\
    \tcheckpoint, \n\ttorch_dtype=torch.float16,\n\tdevice_map=\"auto\",\n\tmax_memory={0:\
    \ \"7GB\", \"cpu\": \"3GB\"},\n\toffload_folder=\"/offload\"\n\t)\n\n    model.save_pretrained(\"\
    ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n\n(For some reasons\
    \ I have to set the CPU to 3GB or it would OOM)\n\nThe model is indeed loaded,\
    \ but saving it fails with : \n\n``\nTraceback (most recent call last):\n  File\
    \ \"/src/run_convert.py\", line 24, in <module>\n    model.save_pretrained(\"\
    ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
    , line 1841, in save_pretrained\n    safe_save_file(shard, os.path.join(save_directory,\
    \ shard_file), metadata={\"format\": \"pt\"})\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 72, in save_file\n    serialize_file(_flatten(tensors), filename, metadata=metadata)\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line\
    \ 237, in _flatten\n    return {\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 241, in <dictcomp>\n    \"data\": _tobytes(v, k),\n  File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 193, in _tobytes\n    tensor = tensor.to(\"cpu\")\nNotImplementedError:\
    \ Cannot copy out of meta tensor; no data!\n``\n\nIt could be a bug in safetensors\
    \ library"
  created_at: 2023-04-23 18:28:48+00:00
  edited: false
  hidden: false
  id: 644586f0d1460e859d2099b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96af1ecc8f9d2263d6467a66ee895453.svg
      fullname: Anuj Sahani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anujsahani01
      type: user
    createdAt: '2023-05-27T21:00:35.000Z'
    data:
      edited: false
      editors:
      - anujsahani01
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96af1ecc8f9d2263d6467a66ee895453.svg
          fullname: Anuj Sahani
          isHf: false
          isPro: false
          name: anujsahani01
          type: user
        html: "<blockquote>\n<p>I followed tips from : </p>\n<ul>\n<li><a href=\"\
          https://huggingface.co/docs/transformers/main/en/main_classes/quantization\"\
          >https://huggingface.co/docs/transformers/main/en/main_classes/quantization</a></li>\n\
          <li><a href=\"https://huggingface.co/docs/accelerate/usage_guides/big_modeling\"\
          >https://huggingface.co/docs/accelerate/usage_guides/big_modeling</a></li>\n\
          </ul>\n<p>The machine has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of\
          \ GB for offload_folder.</p>\n<p>In the end, I used the following script\
          \ : </p>\n<pre><code>from transformers import AutoModelForCausalLM\nimport\
          \ torch\n\ncheckpoint = \"stablelm_oa_7b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          checkpoint, \ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nmax_memory={0:\
          \ \"7GB\", \"cpu\": \"3GB\"},\noffload_folder=\"/offload\"\n)\n\nmodel.save_pretrained(\"\
          ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n</code></pre>\n\
          <p>(For some reasons I have to set the CPU to 3GB or it would OOM)</p>\n\
          <p>The model is indeed loaded, but saving it fails with : </p>\n<p><code>Traceback\
          \ (most recent call last):   File \"/src/run_convert.py\", line 24, in &lt;module&gt;\
          \     model.save_pretrained(\"./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\
          \   File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 1841, in save_pretrained     safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 72, in save_file     serialize_file(_flatten(tensors), filename,\
          \ metadata=metadata)   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 237, in _flatten     return {   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 241, in &lt;dictcomp&gt;     \"data\": _tobytes(v, k),   File \"\
          /usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 193,\
          \ in _tobytes     tensor = tensor.to(\"cpu\") NotImplementedError: Cannot\
          \ copy out of meta tensor; no data!</code></p>\n<p>It could be a bug in\
          \ safetensors library</p>\n</blockquote>\n<p>hey,<br> i am facing the same\
          \ issue, did you come up with any solution?</p>\n"
        raw: "> I followed tips from : \n> - https://huggingface.co/docs/transformers/main/en/main_classes/quantization\n\
          > - https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n>\
          \ \n> The machine has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of GB\
          \ for offload_folder.\n> \n> In the end, I used the following script : \n\
          > \n>     from transformers import AutoModelForCausalLM\n>     import torch\n\
          > \n>     checkpoint = \"stablelm_oa_7b\"\n> \n>     model = AutoModelForCausalLM.from_pretrained(\n\
          > \tcheckpoint, \n> \ttorch_dtype=torch.float16,\n> \tdevice_map=\"auto\"\
          ,\n> \tmax_memory={0: \"7GB\", \"cpu\": \"3GB\"},\n> \toffload_folder=\"\
          /offload\"\n> \t)\n> \n>     model.save_pretrained(\"./sharded\", max_shard_size=\"\
          2GB\", safe_serialization=True)\n> \n> (For some reasons I have to set the\
          \ CPU to 3GB or it would OOM)\n> \n> The model is indeed loaded, but saving\
          \ it fails with : \n> \n> ``\n> Traceback (most recent call last):\n>  \
          \ File \"/src/run_convert.py\", line 24, in <module>\n>     model.save_pretrained(\"\
          ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n>   File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 1841, in save_pretrained\n>     safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 72, in save_file\n>     serialize_file(_flatten(tensors), filename,\
          \ metadata=metadata)\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 237, in _flatten\n>     return {\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
          , line 241, in <dictcomp>\n>     \"data\": _tobytes(v, k),\n>   File \"\
          /usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line 193,\
          \ in _tobytes\n>     tensor = tensor.to(\"cpu\")\n> NotImplementedError:\
          \ Cannot copy out of meta tensor; no data!\n> ``\n> \n> It could be a bug\
          \ in safetensors library\n\nhey,\n i am facing the same issue, did you come\
          \ up with any solution?"
        updatedAt: '2023-05-27T21:00:35.858Z'
      numEdits: 0
      reactions: []
    id: 64726f735afd6a69658b7a88
    type: comment
  author: anujsahani01
  content: "> I followed tips from : \n> - https://huggingface.co/docs/transformers/main/en/main_classes/quantization\n\
    > - https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n> \n> The\
    \ machine has 16GB of RAM, 2GB swap, and 8GB VRAM + dozens of GB for offload_folder.\n\
    > \n> In the end, I used the following script : \n> \n>     from transformers\
    \ import AutoModelForCausalLM\n>     import torch\n> \n>     checkpoint = \"stablelm_oa_7b\"\
    \n> \n>     model = AutoModelForCausalLM.from_pretrained(\n> \tcheckpoint, \n\
    > \ttorch_dtype=torch.float16,\n> \tdevice_map=\"auto\",\n> \tmax_memory={0: \"\
    7GB\", \"cpu\": \"3GB\"},\n> \toffload_folder=\"/offload\"\n> \t)\n> \n>     model.save_pretrained(\"\
    ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n> \n> (For some\
    \ reasons I have to set the CPU to 3GB or it would OOM)\n> \n> The model is indeed\
    \ loaded, but saving it fails with : \n> \n> ``\n> Traceback (most recent call\
    \ last):\n>   File \"/src/run_convert.py\", line 24, in <module>\n>     model.save_pretrained(\"\
    ./sharded\", max_shard_size=\"2GB\", safe_serialization=True)\n>   File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
    , line 1841, in save_pretrained\n>     safe_save_file(shard, os.path.join(save_directory,\
    \ shard_file), metadata={\"format\": \"pt\"})\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 72, in save_file\n>     serialize_file(_flatten(tensors), filename, metadata=metadata)\n\
    >   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\", line\
    \ 237, in _flatten\n>     return {\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 241, in <dictcomp>\n>     \"data\": _tobytes(v, k),\n>   File \"/usr/local/lib/python3.10/dist-packages/safetensors/torch.py\"\
    , line 193, in _tobytes\n>     tensor = tensor.to(\"cpu\")\n> NotImplementedError:\
    \ Cannot copy out of meta tensor; no data!\n> ``\n> \n> It could be a bug in safetensors\
    \ library\n\nhey,\n i am facing the same issue, did you come up with any solution?"
  created_at: 2023-05-27 20:00:35+00:00
  edited: false
  hidden: false
  id: 64726f735afd6a69658b7a88
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OpenAssistant/stablelm-7b-sft-v7-epoch-3
repo_type: model
status: open
target_branch: null
title: safetensors shards of 2GB
