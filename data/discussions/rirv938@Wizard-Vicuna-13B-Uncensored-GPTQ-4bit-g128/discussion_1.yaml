!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Eilian
conflicting_files: null
created_at: 2023-06-06 07:41:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/072bc64da268bda624c93ae6bfda278c.svg
      fullname: ye
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eilian
      type: user
    createdAt: '2023-06-06T08:41:07.000Z'
    data:
      edited: false
      editors:
      - Eilian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2936728596687317
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/072bc64da268bda624c93ae6bfda278c.svg
          fullname: ye
          isHf: false
          isPro: false
          name: Eilian
          type: user
        html: '<p>Hi, it shows the following.<br>"Exception in thread Thread-29:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 615, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3191,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 1980, in load_model<br>    model.load(<br>  File "G:\KoboldAI\modeling\inference_model.py",
          line 177, in load<br>    self._load(save_model=save_model, initial_load=initial_load)<br>  File
          "G:\KoboldAI\modeling\inference_models\hf_torch_4bit.py", line 199, in _load<br>    self.tokenizer
          = self._get_tokenizer(self.get_local_model_path())<br>  File "G:\KoboldAI\modeling\inference_models\hf_torch_4bit.py",
          line 391, in _get_tokenizer<br>    tokenizer = LlamaTokenizer.from_pretrained(utils.koboldai_vars.custmodpth)<br>  File
          "aiserver.py", line 112, in new_pretrainedtokenizerbase_from_pretrained<br>    tokenizer
          = old_pretrainedtokenizerbase_from_pretrained(cls, *args, **kwargs)<br>  File
          "B:\python\lib\site-packages\transformers\tokenization_utils_base.py", line
          1811, in from_pretrained<br>    return cls.<em>from_pretrained(<br>  File
          "B:\python\lib\site-packages\transformers\tokenization_utils_base.py", line
          1965, in <em>from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>  File
          "B:\python\lib\site-packages\transformers\models\llama\tokenization_llama.py",
          line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>  File
          "B:\python\lib\site-packages\sentencepiece_<em>init</em></em>.py", line
          905, in Load<br>    return self.LoadFromFile(model_file)<br>  File "B:\python\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)<br>TypeError: not a string<br>Connection Attempt: 127.0.0.1"</p>

          '
        raw: "Hi, it shows the following.\r\n\"Exception in thread Thread-29:\r\n\
          Traceback (most recent call last):\r\n  File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"B:\\python\\\
          lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\
          \n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731,\
          \ in _handle_event_internal\r\n    r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\r\n  File \"B:\\python\\lib\\site-packages\\socketio\\\
          server.py\", line 756, in _trigger_event\r\n    return self.handlers[namespace][event](*args)\r\
          \n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\r\n    return self._handle_event(handler, message,\
          \ namespace, sid,\r\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\r\n    ret = handler(*args)\r\n\
          \  File \"aiserver.py\", line 615, in g\r\n    return f(*a, **k)\r\n  File\
          \ \"aiserver.py\", line 3191, in get_message\r\n    load_model(use_gpu=msg['use_gpu'],\
          \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\r\
          \n  File \"aiserver.py\", line 1980, in load_model\r\n    model.load(\r\n\
          \  File \"G:\\KoboldAI\\modeling\\inference_model.py\", line 177, in load\r\
          \n    self._load(save_model=save_model, initial_load=initial_load)\r\n \
          \ File \"G:\\KoboldAI\\modeling\\inference_models\\hf_torch_4bit.py\", line\
          \ 199, in _load\r\n    self.tokenizer = self._get_tokenizer(self.get_local_model_path())\r\
          \n  File \"G:\\KoboldAI\\modeling\\inference_models\\hf_torch_4bit.py\"\
          , line 391, in _get_tokenizer\r\n    tokenizer = LlamaTokenizer.from_pretrained(utils.koboldai_vars.custmodpth)\r\
          \n  File \"aiserver.py\", line 112, in new_pretrainedtokenizerbase_from_pretrained\r\
          \n    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args,\
          \ **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"B:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\llama\\\
          tokenization_llama.py\", line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\
          \n  File \"B:\\python\\lib\\site-packages\\sentencepiece\\__init__.py\"\
          , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File\
          \ \"B:\\python\\lib\\site-packages\\sentencepiece\\__init__.py\", line 310,\
          \ in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nTypeError: not a string\r\nConnection Attempt: 127.0.0.1\""
        updatedAt: '2023-06-06T08:41:07.822Z'
      numEdits: 0
      reactions: []
    id: 647ef123aa8c04bbf935e4b7
    type: comment
  author: Eilian
  content: "Hi, it shows the following.\r\n\"Exception in thread Thread-29:\r\nTraceback\
    \ (most recent call last):\r\n  File \"B:\\python\\lib\\threading.py\", line 932,\
    \ in _bootstrap_inner\r\n    self.run()\r\n  File \"B:\\python\\lib\\threading.py\"\
    , line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File\
    \ \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731, in _handle_event_internal\r\
    \n    r = server._trigger_event(data[0], namespace, sid, *data[1:])\r\n  File\
    \ \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 756, in _trigger_event\r\
    \n    return self.handlers[namespace][event](*args)\r\n  File \"B:\\python\\lib\\\
    site-packages\\flask_socketio\\__init__.py\", line 282, in _handler\r\n    return\
    \ self._handle_event(handler, message, namespace, sid,\r\n  File \"B:\\python\\\
    lib\\site-packages\\flask_socketio\\__init__.py\", line 828, in _handle_event\r\
    \n    ret = handler(*args)\r\n  File \"aiserver.py\", line 615, in g\r\n    return\
    \ f(*a, **k)\r\n  File \"aiserver.py\", line 3191, in get_message\r\n    load_model(use_gpu=msg['use_gpu'],\
    \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\r\
    \n  File \"aiserver.py\", line 1980, in load_model\r\n    model.load(\r\n  File\
    \ \"G:\\KoboldAI\\modeling\\inference_model.py\", line 177, in load\r\n    self._load(save_model=save_model,\
    \ initial_load=initial_load)\r\n  File \"G:\\KoboldAI\\modeling\\inference_models\\\
    hf_torch_4bit.py\", line 199, in _load\r\n    self.tokenizer = self._get_tokenizer(self.get_local_model_path())\r\
    \n  File \"G:\\KoboldAI\\modeling\\inference_models\\hf_torch_4bit.py\", line\
    \ 391, in _get_tokenizer\r\n    tokenizer = LlamaTokenizer.from_pretrained(utils.koboldai_vars.custmodpth)\r\
    \n  File \"aiserver.py\", line 112, in new_pretrainedtokenizerbase_from_pretrained\r\
    \n    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args, **kwargs)\r\
    \n  File \"B:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"B:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
    , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"B:\\\
    python\\lib\\site-packages\\sentencepiece\\__init__.py\", line 905, in Load\r\n\
    \    return self.LoadFromFile(model_file)\r\n  File \"B:\\python\\lib\\site-packages\\\
    sentencepiece\\__init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nTypeError: not a string\r\nConnection Attempt: 127.0.0.1\""
  created_at: 2023-06-06 07:41:07+00:00
  edited: false
  hidden: false
  id: 647ef123aa8c04bbf935e4b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rirv938/Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-g128
repo_type: model
status: open
target_branch: null
title: Error loading model for koboldai4bit version"Exception in thread Thread-29:"
