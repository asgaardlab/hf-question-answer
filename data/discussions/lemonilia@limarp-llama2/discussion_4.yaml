!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lsaa
conflicting_files: null
created_at: 2023-07-29 04:42:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e61f0e2b3feabed438f82fed0e124aef.svg
      fullname: lsaaa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lsaa
      type: user
    createdAt: '2023-07-29T05:42:57.000Z'
    data:
      edited: false
      editors:
      - lsaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723154306411743
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e61f0e2b3feabed438f82fed0e124aef.svg
          fullname: lsaaa
          isHf: false
          isPro: false
          name: lsaa
          type: user
        html: '<p>could you tell a bit more about the training process?</p>

          <p>What GPU did you use?<br>How long did it take?<br>did you use <a rel="nofollow"
          href="https://github.com/artidoro/qlora">https://github.com/artidoro/qlora</a>
          ? If so, would you mind sharing the config...?</p>

          '
        raw: "could you tell a bit more about the training process?\r\n\r\nWhat GPU\
          \ did you use?\r\nHow long did it take?\r\ndid you use https://github.com/artidoro/qlora\
          \ ? If so, would you mind sharing the config...?"
        updatedAt: '2023-07-29T05:42:57.375Z'
      numEdits: 0
      reactions: []
    id: 64c4a6e1d07620bdc99a6311
    type: comment
  author: lsaa
  content: "could you tell a bit more about the training process?\r\n\r\nWhat GPU\
    \ did you use?\r\nHow long did it take?\r\ndid you use https://github.com/artidoro/qlora\
    \ ? If so, would you mind sharing the config...?"
  created_at: 2023-07-29 04:42:57+00:00
  edited: false
  hidden: false
  id: 64c4a6e1d07620bdc99a6311
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
      fullname: Suikamelon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lemonilia
      type: user
    createdAt: '2023-07-29T08:52:30.000Z'
    data:
      edited: false
      editors:
      - lemonilia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3575704097747803
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
          fullname: Suikamelon
          isHf: false
          isPro: false
          name: lemonilia
          type: user
        html: "<p>Initially I used my NVidia RTX3090. Training time is proportional\
          \ to settings like number of epochs, batch size, number training examples,\
          \ etc. With the current dataset, settings and GPU limited to 225W it takes\
          \ about 6.5 hours for the 13B model and just over 3 hours for the 7B model,\
          \ using Artidoro QLoRA. I made a couple slight changes to the code (mainly\
          \ to make it properly work with a separate evals dataset) but otherwise\
          \ the starting command with settings was something like this:</p>\n<pre><code>python\
          \ qlora2.py \\\n    --model_name_or_path /home/anon/models/Llama2-13B/ \\\
          \n    --lr_scheduler_type cosine \\\n    --lora_r 8 \\\n    --lora_dropout\
          \ 0.01 \\\n    --output_dir /home/anon/bin/qlora/limarp-Llama2-13B \\\n\
          \    --dataset limarp \\\n    --dataset-format input-output \\\n    --train_on_source\
          \ False \\\n    --learning_rate 0.000060 \\\n    --warmup_steps 0 \\\n \
          \   --num_train_epochs 2 \\\n    --do_train True \\\n    --do_eval True\
          \ \\\n    --do_mmlu_eval False \\\n    --bf16 True \\\n    --bits 4 \\\n\
          \    --source_max_len 6000 \\\n    --target_max_len 6000 \\\n    --per_device_train_batch_size\
          \ 1 \\\n    --gradient_accumulation_steps 1 \\\n    --logging_steps 1 \\\
          \n    --max_steps -1 \\\n    --save_steps 1000 \\\n    --eval_steps 35 \\\
          \n    --save_strategy epoch \\\n    --save_total_limit 1000 \\\n    --data_seed\
          \ 41 \\\n    --per_device_eval_batch_size 1 \\\n    --evaluation_strategy\
          \ steps \\\n    --eval_dataset_size 72 \\\n    --max_eval_samples 35 \\\n\
          \    --max_memory_MB 24300 \\\n    --optim paged_adamw_32bit \\\n    --report_to\
          \ \"wandb\"\n</code></pre>\n"
        raw: "Initially I used my NVidia RTX3090. Training time is proportional to\
          \ settings like number of epochs, batch size, number training examples,\
          \ etc. With the current dataset, settings and GPU limited to 225W it takes\
          \ about 6.5 hours for the 13B model and just over 3 hours for the 7B model,\
          \ using Artidoro QLoRA. I made a couple slight changes to the code (mainly\
          \ to make it properly work with a separate evals dataset) but otherwise\
          \ the starting command with settings was something like this:\n\n```\npython\
          \ qlora2.py \\\n    --model_name_or_path /home/anon/models/Llama2-13B/ \\\
          \n    --lr_scheduler_type cosine \\\n    --lora_r 8 \\\n    --lora_dropout\
          \ 0.01 \\\n    --output_dir /home/anon/bin/qlora/limarp-Llama2-13B \\\n\
          \    --dataset limarp \\\n    --dataset-format input-output \\\n    --train_on_source\
          \ False \\\n    --learning_rate 0.000060 \\\n    --warmup_steps 0 \\\n \
          \   --num_train_epochs 2 \\\n    --do_train True \\\n    --do_eval True\
          \ \\\n    --do_mmlu_eval False \\\n    --bf16 True \\\n    --bits 4 \\\n\
          \    --source_max_len 6000 \\\n    --target_max_len 6000 \\\n    --per_device_train_batch_size\
          \ 1 \\\n    --gradient_accumulation_steps 1 \\\n    --logging_steps 1 \\\
          \n    --max_steps -1 \\\n    --save_steps 1000 \\\n    --eval_steps 35 \\\
          \n    --save_strategy epoch \\\n    --save_total_limit 1000 \\\n    --data_seed\
          \ 41 \\\n    --per_device_eval_batch_size 1 \\\n    --evaluation_strategy\
          \ steps \\\n    --eval_dataset_size 72 \\\n    --max_eval_samples 35 \\\n\
          \    --max_memory_MB 24300 \\\n    --optim paged_adamw_32bit \\\n    --report_to\
          \ \"wandb\"\n```"
        updatedAt: '2023-07-29T08:52:30.802Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - lsaa
        - MatthewK
        - michaelmistaken
    id: 64c4d34ee356b52a986189b0
    type: comment
  author: lemonilia
  content: "Initially I used my NVidia RTX3090. Training time is proportional to settings\
    \ like number of epochs, batch size, number training examples, etc. With the current\
    \ dataset, settings and GPU limited to 225W it takes about 6.5 hours for the 13B\
    \ model and just over 3 hours for the 7B model, using Artidoro QLoRA. I made a\
    \ couple slight changes to the code (mainly to make it properly work with a separate\
    \ evals dataset) but otherwise the starting command with settings was something\
    \ like this:\n\n```\npython qlora2.py \\\n    --model_name_or_path /home/anon/models/Llama2-13B/\
    \ \\\n    --lr_scheduler_type cosine \\\n    --lora_r 8 \\\n    --lora_dropout\
    \ 0.01 \\\n    --output_dir /home/anon/bin/qlora/limarp-Llama2-13B \\\n    --dataset\
    \ limarp \\\n    --dataset-format input-output \\\n    --train_on_source False\
    \ \\\n    --learning_rate 0.000060 \\\n    --warmup_steps 0 \\\n    --num_train_epochs\
    \ 2 \\\n    --do_train True \\\n    --do_eval True \\\n    --do_mmlu_eval False\
    \ \\\n    --bf16 True \\\n    --bits 4 \\\n    --source_max_len 6000 \\\n    --target_max_len\
    \ 6000 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps\
    \ 1 \\\n    --logging_steps 1 \\\n    --max_steps -1 \\\n    --save_steps 1000\
    \ \\\n    --eval_steps 35 \\\n    --save_strategy epoch \\\n    --save_total_limit\
    \ 1000 \\\n    --data_seed 41 \\\n    --per_device_eval_batch_size 1 \\\n    --evaluation_strategy\
    \ steps \\\n    --eval_dataset_size 72 \\\n    --max_eval_samples 35 \\\n    --max_memory_MB\
    \ 24300 \\\n    --optim paged_adamw_32bit \\\n    --report_to \"wandb\"\n```"
  created_at: 2023-07-29 07:52:30+00:00
  edited: false
  hidden: false
  id: 64c4d34ee356b52a986189b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: lemonilia/limarp-llama2
repo_type: model
status: open
target_branch: null
title: Questions
