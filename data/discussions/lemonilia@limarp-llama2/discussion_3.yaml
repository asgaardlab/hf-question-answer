!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Herman555
conflicting_files: null
created_at: 2023-07-26 00:13:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c072297255ff868721c57f/pyKz4cz5WM4Z6mfRBG08z.jpeg?w=200&h=200&f=face
      fullname: Yamam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Herman555
      type: user
    createdAt: '2023-07-26T01:13:34.000Z'
    data:
      edited: false
      editors:
      - Herman555
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981016993522644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c072297255ff868721c57f/pyKz4cz5WM4Z6mfRBG08z.jpeg?w=200&h=200&f=face
          fullname: Yamam
          isHf: false
          isPro: false
          name: Herman555
          type: user
        html: '<p>Tried this and I loved it, I think this is the best roleplaying
          results I have gotten trying a 7b model. Maybe you could ping TheBloke or
          something, this needs to be out there it''s great.</p>

          '
        raw: Tried this and I loved it, I think this is the best roleplaying results
          I have gotten trying a 7b model. Maybe you could ping TheBloke or something,
          this needs to be out there it's great.
        updatedAt: '2023-07-26T01:13:34.347Z'
      numEdits: 0
      reactions: []
    id: 64c0733ea20269b8bd79d811
    type: comment
  author: Herman555
  content: Tried this and I loved it, I think this is the best roleplaying results
    I have gotten trying a 7b model. Maybe you could ping TheBloke or something, this
    needs to be out there it's great.
  created_at: 2023-07-26 00:13:34+00:00
  edited: false
  hidden: false
  id: 64c0733ea20269b8bd79d811
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
      fullname: Suikamelon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lemonilia
      type: user
    createdAt: '2023-07-26T02:45:43.000Z'
    data:
      edited: true
      editors:
      - lemonilia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9159277677536011
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63065221435ec751b72998f5/687FH-lJaPiU-D-Lqkduw.png?w=200&h=200&f=face
          fullname: Suikamelon
          isHf: false
          isPro: false
          name: lemonilia
          type: user
        html: '<p>Thanks, good to know that the 7B model yields good results too.</p>

          <p>The LoRA should be able to be applied onto a GPTQ quantization of the
          base Llama2 model. I''ve tested it that way and it works (AutoGPTQ). There
          was previously a bug with Exllama due to the LoRA being saved in BF16 format
          and Exllama not handling that properly, but that <a rel="nofollow" href="https://github.com/turboderp/exllama/commit/3f83ebb378b0dddd9b44c25ae17132bdd6c36ddb">has
          been solved recently</a>. If you use text-generation-webui, you should be
          able to load the LoRA with that as well after updating  (<code>git pull</code>,
          <code>pip install -r requirements.txt -U</code>).</p>

          '
        raw: 'Thanks, good to know that the 7B model yields good results too.


          The LoRA should be able to be applied onto a GPTQ quantization of the base
          Llama2 model. I''ve tested it that way and it works (AutoGPTQ). There was
          previously a bug with Exllama due to the LoRA being saved in BF16 format
          and Exllama not handling that properly, but that [has been solved recently](https://github.com/turboderp/exllama/commit/3f83ebb378b0dddd9b44c25ae17132bdd6c36ddb).
          If you use text-generation-webui, you should be able to load the LoRA with
          that as well after updating  (`git pull`, `pip install -r requirements.txt
          -U`).'
        updatedAt: '2023-07-26T02:46:26.441Z'
      numEdits: 1
      reactions: []
    id: 64c088d77255ff8687257f82
    type: comment
  author: lemonilia
  content: 'Thanks, good to know that the 7B model yields good results too.


    The LoRA should be able to be applied onto a GPTQ quantization of the base Llama2
    model. I''ve tested it that way and it works (AutoGPTQ). There was previously
    a bug with Exllama due to the LoRA being saved in BF16 format and Exllama not
    handling that properly, but that [has been solved recently](https://github.com/turboderp/exllama/commit/3f83ebb378b0dddd9b44c25ae17132bdd6c36ddb).
    If you use text-generation-webui, you should be able to load the LoRA with that
    as well after updating  (`git pull`, `pip install -r requirements.txt -U`).'
  created_at: 2023-07-26 01:45:43+00:00
  edited: true
  hidden: false
  id: 64c088d77255ff8687257f82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c072297255ff868721c57f/pyKz4cz5WM4Z6mfRBG08z.jpeg?w=200&h=200&f=face
      fullname: Yamam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Herman555
      type: user
    createdAt: '2023-07-26T11:45:27.000Z'
    data:
      edited: false
      editors:
      - Herman555
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9794973731040955
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c072297255ff868721c57f/pyKz4cz5WM4Z6mfRBG08z.jpeg?w=200&h=200&f=face
          fullname: Yamam
          isHf: false
          isPro: false
          name: Herman555
          type: user
        html: '<p>Thank you for your help, I am now able to run the LoRA with Exllama
          which is huge for me as it all fits in my 8gb of VRAM.</p>

          '
        raw: Thank you for your help, I am now able to run the LoRA with Exllama which
          is huge for me as it all fits in my 8gb of VRAM.
        updatedAt: '2023-07-26T11:45:27.955Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c107574f1deeecbbc4fa07
    id: 64c107574f1deeecbbc4fa06
    type: comment
  author: Herman555
  content: Thank you for your help, I am now able to run the LoRA with Exllama which
    is huge for me as it all fits in my 8gb of VRAM.
  created_at: 2023-07-26 10:45:27+00:00
  edited: false
  hidden: false
  id: 64c107574f1deeecbbc4fa06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c072297255ff868721c57f/pyKz4cz5WM4Z6mfRBG08z.jpeg?w=200&h=200&f=face
      fullname: Yamam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Herman555
      type: user
    createdAt: '2023-07-26T11:45:27.000Z'
    data:
      status: closed
    id: 64c107574f1deeecbbc4fa07
    type: status-change
  author: Herman555
  created_at: 2023-07-26 10:45:27+00:00
  id: 64c107574f1deeecbbc4fa07
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: lemonilia/limarp-llama2
repo_type: model
status: closed
target_branch: null
title: GPTQ 4bit model please if possible.
