!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 1TBGPU4EVR
conflicting_files: null
created_at: 2023-10-12 13:05:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/788f2440b96df4044f486985341f5c86.svg
      fullname: Johnny TensorMnenomic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: 1TBGPU4EVR
      type: user
    createdAt: '2023-10-12T14:05:34.000Z'
    data:
      edited: false
      editors:
      - 1TBGPU4EVR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9014717936515808
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/788f2440b96df4044f486985341f5c86.svg
          fullname: Johnny TensorMnenomic
          isHf: false
          isPro: true
          name: 1TBGPU4EVR
          type: user
        html: "<p>3090 on wsl2- i9-12900k, 128gb, cuda 12.1- am I doing something\
          \ wrong, stupid?</p>\n<p>On the HF non quantized model I get about 15t/s</p>\n\
          <p>(exllama) root@beaut:/mnt/g/exllama2/exllamav2# python test_inference.py\
          \ -m /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/ -p \"once\
          \ upon a time\"<br> -- Model: /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/<br>\
          \ -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']<br> -- Loading model...<br>\
          \ -- Loading tokenizer...<br> -- Warmup...<br> -- Generating...</p>\n<p>once\
          \ upon a time, the whole family went to the circus. My dad was thrilled\
          \ when they were giving away balloons. I still remember him coming back\
          \ with one that said \u201CBALLOONS FOR EVERYONE\u201D. It\u2019s such a\
          \ simple idea, but it has always been my goal to be able to give out things\
          \ like that to people.<br>When my dad and I would go on hikes together,\
          \ he used to say to me, \u201CI want you to take over for me after I die.\u201D\
          \ That may sound silly to some of you, but it is important to me to live\
          \ up to</p>\n<p> -- Response generated in 177.84 seconds, 128 tokens, 0.72\
          \ tokens/second (includes prompt eval.)</p>\n"
        raw: "3090 on wsl2- i9-12900k, 128gb, cuda 12.1- am I doing something wrong,\
          \ stupid?\r\n\r\nOn the HF non quantized model I get about 15t/s\r\n\r\n\
          (exllama) root@beaut:/mnt/g/exllama2/exllamav2# python test_inference.py\
          \ -m /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/ -p \"once\
          \ upon a time\"\r\n -- Model: /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/\r\
          \n -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']\r\n -- Loading model...\r\
          \n -- Loading tokenizer...\r\n -- Warmup...\r\n -- Generating...\r\n\r\n\
          once upon a time, the whole family went to the circus. My dad was thrilled\
          \ when they were giving away balloons. I still remember him coming back\
          \ with one that said \u201CBALLOONS FOR EVERYONE\u201D. It\u2019s such a\
          \ simple idea, but it has always been my goal to be able to give out things\
          \ like that to people.\r\nWhen my dad and I would go on hikes together,\
          \ he used to say to me, \u201CI want you to take over for me after I die.\u201D\
          \ That may sound silly to some of you, but it is important to me to live\
          \ up to\r\n\r\n -- Response generated in 177.84 seconds, 128 tokens, 0.72\
          \ tokens/second (includes prompt eval.)"
        updatedAt: '2023-10-12T14:05:34.082Z'
      numEdits: 0
      reactions: []
    id: 6527fd2e5c74fe25a48870a1
    type: comment
  author: 1TBGPU4EVR
  content: "3090 on wsl2- i9-12900k, 128gb, cuda 12.1- am I doing something wrong,\
    \ stupid?\r\n\r\nOn the HF non quantized model I get about 15t/s\r\n\r\n(exllama)\
    \ root@beaut:/mnt/g/exllama2/exllamav2# python test_inference.py -m /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/\
    \ -p \"once upon a time\"\r\n -- Model: /mnt/g/llama.cpp/models/Codellama-34B-instruct-exl2-6bpw/\r\
    \n -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']\r\n -- Loading model...\r\n\
    \ -- Loading tokenizer...\r\n -- Warmup...\r\n -- Generating...\r\n\r\nonce upon\
    \ a time, the whole family went to the circus. My dad was thrilled when they were\
    \ giving away balloons. I still remember him coming back with one that said \u201C\
    BALLOONS FOR EVERYONE\u201D. It\u2019s such a simple idea, but it has always been\
    \ my goal to be able to give out things like that to people.\r\nWhen my dad and\
    \ I would go on hikes together, he used to say to me, \u201CI want you to take\
    \ over for me after I die.\u201D That may sound silly to some of you, but it is\
    \ important to me to live up to\r\n\r\n -- Response generated in 177.84 seconds,\
    \ 128 tokens, 0.72 tokens/second (includes prompt eval.)"
  created_at: 2023-10-12 13:05:34+00:00
  edited: false
  hidden: false
  id: 6527fd2e5c74fe25a48870a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
      fullname: turboderp
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: turboderp
      type: user
    createdAt: '2023-10-12T14:38:37.000Z'
    data:
      edited: false
      editors:
      - turboderp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9601837396621704
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
          fullname: turboderp
          isHf: false
          isPro: false
          name: turboderp
          type: user
        html: '<p>Doesn''t look like you''re doing anything stupid, no. But I''m a
          little confused as to how you can run the non-quantized model on a 3090,
          let alone at 15 tokens/second.. ?</p>

          <p>But as for the speed with the quantized model, I can only suspect your
          NVIDIA driver. The later versions of the driver will start swapping VRAM
          to system RAM if it gets close to running out, which prevents out-of-memory
          errors but absolutely tanks performance. It seems especially likely since
          the 6.0bpw model weights alone are larger than the VRAM on the 3090. So
          it shouldn''t even load.</p>

          <p>If you mean you have multiple 3090s, you need to define a GPU split with
          the <code>-gs</code> argument. E.g. <code>-gs 17,24</code> or something
          along those lines.</p>

          '
        raw: 'Doesn''t look like you''re doing anything stupid, no. But I''m a little
          confused as to how you can run the non-quantized model on a 3090, let alone
          at 15 tokens/second.. ?


          But as for the speed with the quantized model, I can only suspect your NVIDIA
          driver. The later versions of the driver will start swapping VRAM to system
          RAM if it gets close to running out, which prevents out-of-memory errors
          but absolutely tanks performance. It seems especially likely since the 6.0bpw
          model weights alone are larger than the VRAM on the 3090. So it shouldn''t
          even load.


          If you mean you have multiple 3090s, you need to define a GPU split with
          the `-gs` argument. E.g. `-gs 17,24` or something along those lines.'
        updatedAt: '2023-10-12T14:38:37.286Z'
      numEdits: 0
      reactions: []
    id: 652804edb1a0e9715f6acfe1
    type: comment
  author: turboderp
  content: 'Doesn''t look like you''re doing anything stupid, no. But I''m a little
    confused as to how you can run the non-quantized model on a 3090, let alone at
    15 tokens/second.. ?


    But as for the speed with the quantized model, I can only suspect your NVIDIA
    driver. The later versions of the driver will start swapping VRAM to system RAM
    if it gets close to running out, which prevents out-of-memory errors but absolutely
    tanks performance. It seems especially likely since the 6.0bpw model weights alone
    are larger than the VRAM on the 3090. So it shouldn''t even load.


    If you mean you have multiple 3090s, you need to define a GPU split with the `-gs`
    argument. E.g. `-gs 17,24` or something along those lines.'
  created_at: 2023-10-12 13:38:37+00:00
  edited: false
  hidden: false
  id: 652804edb1a0e9715f6acfe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/788f2440b96df4044f486985341f5c86.svg
      fullname: Johnny TensorMnenomic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: 1TBGPU4EVR
      type: user
    createdAt: '2023-10-12T20:54:03.000Z'
    data:
      edited: false
      editors:
      - 1TBGPU4EVR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9670270681381226
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/788f2440b96df4044f486985341f5c86.svg
          fullname: Johnny TensorMnenomic
          isHf: false
          isPro: true
          name: 1TBGPU4EVR
          type: user
        html: '<p>I had my own skeletons to deal with in that env to get cuda all
          squared up as I tried to use the exllama non-v2 and that wasn''t smart-
          latest cudnn and cuda 12.2- single 3090- 5t/s FWIW- No OOM.<br>  I have
          weird problems with GPU split when I hard set it- I feel like it''s auto-devices
          or it eats shit early.<br>Anyway, one person''s experience.</p>

          <p>BTW I was using my 4090 on the HF model.  I''m spoiled from a public
          HF space that had an A100 on 34B for a while.  Easily on par or better than
          gpt4 coding.</p>

          '
        raw: "I had my own skeletons to deal with in that env to get cuda all squared\
          \ up as I tried to use the exllama non-v2 and that wasn't smart- latest\
          \ cudnn and cuda 12.2- single 3090- 5t/s FWIW- No OOM.\n  I have weird problems\
          \ with GPU split when I hard set it- I feel like it's auto-devices or it\
          \ eats shit early.\nAnyway, one person's experience.\n\nBTW I was using\
          \ my 4090 on the HF model.  I'm spoiled from a public HF space that had\
          \ an A100 on 34B for a while.  Easily on par or better than gpt4 coding.\n\
          \n\n\n\n \n\n\n "
        updatedAt: '2023-10-12T20:54:03.233Z'
      numEdits: 0
      reactions: []
    id: 65285ceb4f8fe1bea9afcd44
    type: comment
  author: 1TBGPU4EVR
  content: "I had my own skeletons to deal with in that env to get cuda all squared\
    \ up as I tried to use the exllama non-v2 and that wasn't smart- latest cudnn\
    \ and cuda 12.2- single 3090- 5t/s FWIW- No OOM.\n  I have weird problems with\
    \ GPU split when I hard set it- I feel like it's auto-devices or it eats shit\
    \ early.\nAnyway, one person's experience.\n\nBTW I was using my 4090 on the HF\
    \ model.  I'm spoiled from a public HF space that had an A100 on 34B for a while.\
    \  Easily on par or better than gpt4 coding.\n\n\n\n\n \n\n\n "
  created_at: 2023-10-12 19:54:03+00:00
  edited: false
  hidden: false
  id: 65285ceb4f8fe1bea9afcd44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/788f2440b96df4044f486985341f5c86.svg
      fullname: Johnny TensorMnenomic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: 1TBGPU4EVR
      type: user
    createdAt: '2023-10-14T12:26:22.000Z'
    data:
      status: closed
    id: 652a88ee72280df4262272c3
    type: status-change
  author: 1TBGPU4EVR
  created_at: 2023-10-14 11:26:22+00:00
  id: 652a88ee72280df4262272c3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: turboderp/CodeLlama-34B-instruct-exl2
repo_type: model
status: closed
target_branch: null
title: I can't replicate your speed.  Nowhere close.
