!!python/object:huggingface_hub.community.DiscussionWithDetails
author: urimerhav
conflicting_files: null
created_at: 2022-12-12 17:38:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75aa1b6949736f44bbf64b8809070f43.svg
      fullname: uri merhav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: urimerhav
      type: user
    createdAt: '2022-12-12T17:38:12.000Z'
    data:
      edited: false
      editors:
      - urimerhav
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75aa1b6949736f44bbf64b8809070f43.svg
          fullname: uri merhav
          isHf: false
          isPro: false
          name: urimerhav
          type: user
        html: '<p>I was wondering what''s the minimal GPU to even support inference?
          And for prompting with let''s say 1000 tokens, how long roughly does it
          take to generate 1000 tokens with a V100, A100, or any other benchmark datapoint?</p>

          '
        raw: I was wondering what's the minimal GPU to even support inference? And
          for prompting with let's say 1000 tokens, how long roughly does it take
          to generate 1000 tokens with a V100, A100, or any other benchmark datapoint?
        updatedAt: '2022-12-12T17:38:12.207Z'
      numEdits: 0
      reactions: []
    id: 63976704013a81d417268a21
    type: comment
  author: urimerhav
  content: I was wondering what's the minimal GPU to even support inference? And for
    prompting with let's say 1000 tokens, how long roughly does it take to generate
    1000 tokens with a V100, A100, or any other benchmark datapoint?
  created_at: 2022-12-12 17:38:12+00:00
  edited: false
  hidden: false
  id: 63976704013a81d417268a21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-12T17:48:56.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>The 3B model is 6GB (in fp16), so you will need 6GB of GPU memory
          + some extra for running the tokens through it if you do it in fp16. So
          I think a V100 with 32GB should work. Don''t have an exact datapoint, but
          I think 1000 tokens on a V100 w/ 32GB may take around 1 - 3 minutes.</p>

          '
        raw: The 3B model is 6GB (in fp16), so you will need 6GB of GPU memory + some
          extra for running the tokens through it if you do it in fp16. So I think
          a V100 with 32GB should work. Don't have an exact datapoint, but I think
          1000 tokens on a V100 w/ 32GB may take around 1 - 3 minutes.
        updatedAt: '2022-12-12T17:48:56.395Z'
      numEdits: 0
      reactions: []
    id: 63976988013a81d41726e7b9
    type: comment
  author: Muennighoff
  content: The 3B model is 6GB (in fp16), so you will need 6GB of GPU memory + some
    extra for running the tokens through it if you do it in fp16. So I think a V100
    with 32GB should work. Don't have an exact datapoint, but I think 1000 tokens
    on a V100 w/ 32GB may take around 1 - 3 minutes.
  created_at: 2022-12-12 17:48:56+00:00
  edited: false
  hidden: false
  id: 63976988013a81d41726e7b9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: bigscience/bloomz-3b
repo_type: model
status: open
target_branch: null
title: Inference hardware?
