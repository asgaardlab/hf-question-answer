!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bg90
conflicting_files: null
created_at: 2023-11-29 18:48:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abab3a7115123010c728a17203eb6aa8.svg
      fullname: Simon CHAUVIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bg90
      type: user
    createdAt: '2023-11-29T18:48:38.000Z'
    data:
      edited: false
      editors:
      - bg90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4953317642211914
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abab3a7115123010c728a17203eb6aa8.svg
          fullname: Simon CHAUVIN
          isHf: false
          isPro: false
          name: bg90
          type: user
        html: "<p>The <code>meditron-7b</code> model loads in <code>text-generation-webui</code>.\
          \ Then I write the input and press \"generate\". But got this error:</p>\n\
          <pre><code>  File \"/home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n\
          \  File \"/home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/functional.py\"\
          , line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          RuntimeError: CUDA error: device-side assert triggered\n</code></pre>\n\
          <p>(run with <code>CUDA_LAUNCH_BLOCKING=1</code>)</p>\n<p>From what I read,\
          \ these device-side asserts are often triggered by an invalid indexing operation.</p>\n\
          <p>I am not sure if this is a <strong>problem of the model</strong>, or\
          \ instead a <strong>problem of my settings</strong> (but many models such\
          \ as llama2 and mistral variants work well on my machine).</p>\n"
        raw: "The `meditron-7b` model loads in `text-generation-webui`. Then I write\
          \ the input and press \"generate\". But got this error:\r\n\r\n```\r\n \
          \ File \"/home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\r\n    return F.embedding(\r\n           ^^^^^^^^^^^^\r\
          \n  File \"/home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/functional.py\"\
          , line 2233, in embedding\r\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nRuntimeError: CUDA error: device-side assert triggered\r\n```\r\n(run\
          \ with `CUDA_LAUNCH_BLOCKING=1`)\r\n\r\nFrom what I read, these device-side\
          \ asserts are often triggered by an invalid indexing operation.\r\n\r\n\
          I am not sure if this is a **problem of the model**, or instead a **problem\
          \ of my settings** (but many models such as llama2 and mistral variants\
          \ work well on my machine)."
        updatedAt: '2023-11-29T18:48:38.276Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - XYPB
        - ponshane
        - mpmisko
        - sadra
    id: 6567878692d93199072d0af6
    type: comment
  author: bg90
  content: "The `meditron-7b` model loads in `text-generation-webui`. Then I write\
    \ the input and press \"generate\". But got this error:\r\n\r\n```\r\n  File \"\
    /home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\r\n    return F.embedding(\r\n           ^^^^^^^^^^^^\r\
    \n  File \"/home/me/text-generation-webui-main/installer_files/env/lib/python3.11/site-packages/torch/nn/functional.py\"\
    , line 2233, in embedding\r\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \nRuntimeError: CUDA error: device-side assert triggered\r\n```\r\n(run with `CUDA_LAUNCH_BLOCKING=1`)\r\
    \n\r\nFrom what I read, these device-side asserts are often triggered by an invalid\
    \ indexing operation.\r\n\r\nI am not sure if this is a **problem of the model**,\
    \ or instead a **problem of my settings** (but many models such as llama2 and\
    \ mistral variants work well on my machine)."
  created_at: 2023-11-29 18:48:38+00:00
  edited: false
  hidden: false
  id: 6567878692d93199072d0af6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f84cff919b44a471d1972e6caf96e3f.svg
      fullname: Garcia Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XYPB
      type: user
    createdAt: '2023-11-29T22:21:15.000Z'
    data:
      edited: false
      editors:
      - XYPB
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602159857749939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f84cff919b44a471d1972e6caf96e3f.svg
          fullname: Garcia Du
          isHf: false
          isPro: false
          name: XYPB
          type: user
        html: '<p>I have encountered a similar problem as well, I believe this is
          because the tokenizer has a size of <code>32019</code> while the embedding
          layer of the model only has a size of <code>32000</code>, which results
          in an invalid access of the memory. Any possibility to fix this?</p>

          '
        raw: I have encountered a similar problem as well, I believe this is because
          the tokenizer has a size of `32019` while the embedding layer of the model
          only has a size of `32000`, which results in an invalid access of the memory.
          Any possibility to fix this?
        updatedAt: '2023-11-29T22:21:15.420Z'
      numEdits: 0
      reactions: []
    id: 6567b95b88bfbc261a29e16e
    type: comment
  author: XYPB
  content: I have encountered a similar problem as well, I believe this is because
    the tokenizer has a size of `32019` while the embedding layer of the model only
    has a size of `32000`, which results in an invalid access of the memory. Any possibility
    to fix this?
  created_at: 2023-11-29 22:21:15+00:00
  edited: false
  hidden: false
  id: 6567b95b88bfbc261a29e16e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0996d298a1deb39d076f8fc3c54a1ffc.svg
      fullname: Chia-Hsuan Chang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ponshane
      type: user
    createdAt: '2023-11-29T22:25:54.000Z'
    data:
      edited: false
      editors:
      - ponshane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.879622220993042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0996d298a1deb39d076f8fc3c54a1ffc.svg
          fullname: Chia-Hsuan Chang
          isHf: false
          isPro: false
          name: ponshane
          type: user
        html: '<p>I also got the same issue when I try to decode (generate text) using
          meditron-70B. </p>

          '
        raw: 'I also got the same issue when I try to decode (generate text) using
          meditron-70B. '
        updatedAt: '2023-11-29T22:25:54.956Z'
      numEdits: 0
      reactions: []
    id: 6567ba7257c58ae7f845c5c4
    type: comment
  author: ponshane
  content: 'I also got the same issue when I try to decode (generate text) using meditron-70B. '
  created_at: 2023-11-29 22:25:54+00:00
  edited: false
  hidden: false
  id: 6567ba7257c58ae7f845c5c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01a414b5b5dcb00646f75aae171db3de.svg
      fullname: Paul Hager
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulhager
      type: user
    createdAt: '2023-11-30T12:30:14.000Z'
    data:
      edited: false
      editors:
      - paulhager
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8903029561042786
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01a414b5b5dcb00646f75aae171db3de.svg
          fullname: Paul Hager
          isHf: false
          isPro: false
          name: paulhager
          type: user
        html: '<p>It looks like the versions uploaded so far are only the base models
          without any type of instruction finetuning or chat finetuning. </p>

          <p>In the 70B model card (<a href="https://huggingface.co/epfl-llm/meditron-70b#downstream-use">https://huggingface.co/epfl-llm/meditron-70b#downstream-use</a>)
          they''ve specified "Note 1: The above formatting is not required for running
          the base model (this repository)". Maybe they will upload finetuned versions
          in the future, considering they also write "Future versions of the tuned
          models will be released as we enhance model''s performance."</p>

          <p>It''s all a bit strange though considering they recommend the deployment
          guide on GitHub (<a rel="nofollow" href="https://github.com/epfLLM/meditron/blob/main/deployment/README.md">https://github.com/epfLLM/meditron/blob/main/deployment/README.md</a>)
          for how to use the base model: "To run proper generation with this base
          model, we recommend using a high-throughput and memory-efficient inference
          engine, such as vLLM, with a UI that supports chat and text generation,
          such as BetterChatGPT To see more details about model deployment and generation,
          please see our documentation." But the deployment guide on github assumes
          the model is already instruction finetuned (and has the &lt;|im_start|&gt;
          and &lt;|im_end|&gt; tokens...</p>

          '
        raw: "It looks like the versions uploaded so far are only the base models\
          \ without any type of instruction finetuning or chat finetuning. \n\nIn\
          \ the 70B model card (https://huggingface.co/epfl-llm/meditron-70b#downstream-use)\
          \ they've specified \"Note 1: The above formatting is not required for running\
          \ the base model (this repository)\". Maybe they will upload finetuned versions\
          \ in the future, considering they also write \"Future versions of the tuned\
          \ models will be released as we enhance model's performance.\"\n\nIt's all\
          \ a bit strange though considering they recommend the deployment guide on\
          \ GitHub (https://github.com/epfLLM/meditron/blob/main/deployment/README.md)\
          \ for how to use the base model: \"To run proper generation with this base\
          \ model, we recommend using a high-throughput and memory-efficient inference\
          \ engine, such as vLLM, with a UI that supports chat and text generation,\
          \ such as BetterChatGPT To see more details about model deployment and generation,\
          \ please see our documentation.\" But the deployment guide on github assumes\
          \ the model is already instruction finetuned (and has the <|im_start|> and\
          \ <|im_end|> tokens..."
        updatedAt: '2023-11-30T12:30:14.707Z'
      numEdits: 0
      reactions: []
    id: 6568805605fb89fb7e4c38dc
    type: comment
  author: paulhager
  content: "It looks like the versions uploaded so far are only the base models without\
    \ any type of instruction finetuning or chat finetuning. \n\nIn the 70B model\
    \ card (https://huggingface.co/epfl-llm/meditron-70b#downstream-use) they've specified\
    \ \"Note 1: The above formatting is not required for running the base model (this\
    \ repository)\". Maybe they will upload finetuned versions in the future, considering\
    \ they also write \"Future versions of the tuned models will be released as we\
    \ enhance model's performance.\"\n\nIt's all a bit strange though considering\
    \ they recommend the deployment guide on GitHub (https://github.com/epfLLM/meditron/blob/main/deployment/README.md)\
    \ for how to use the base model: \"To run proper generation with this base model,\
    \ we recommend using a high-throughput and memory-efficient inference engine,\
    \ such as vLLM, with a UI that supports chat and text generation, such as BetterChatGPT\
    \ To see more details about model deployment and generation, please see our documentation.\"\
    \ But the deployment guide on github assumes the model is already instruction\
    \ finetuned (and has the <|im_start|> and <|im_end|> tokens..."
  created_at: 2023-11-30 12:30:14+00:00
  edited: false
  hidden: false
  id: 6568805605fb89fb7e4c38dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg?w=200&h=200&f=face
      fullname: Zeming Chen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zechen-nlp
      type: user
    createdAt: '2023-12-01T01:00:29.000Z'
    data:
      edited: false
      editors:
      - zechen-nlp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9596847295761108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg?w=200&h=200&f=face
          fullname: Zeming Chen
          isHf: false
          isPro: false
          name: zechen-nlp
          type: user
        html: '<p>Hi there,<br>Thank you for bringing this to our attention.<br>There
          was indeed a size mismatch between the tokenizer and the model embedding.
          We uploaded an updated version of the tokenizer along with its configurations.
          Let us know if this resolves the issue. We appreciate your feedback!</p>

          <p>Regarding the confusion with the downstream-use instructions, we want
          to clarify that the models we uploaded (7b &amp; 70B) are pretrained versions
          without additional finetuning or instruction-tuning. Therefore, the specified
          format with &lt;|im_start|&gt; and &lt;|im_end|&gt; were not intended for
          use with these models. </p>

          '
        raw: "Hi there,\nThank you for bringing this to our attention. \nThere was\
          \ indeed a size mismatch between the tokenizer and the model embedding.\
          \ We uploaded an updated version of the tokenizer along with its configurations.\
          \ Let us know if this resolves the issue. We appreciate your feedback!\n\
          \nRegarding the confusion with the downstream-use instructions, we want\
          \ to clarify that the models we uploaded (7b & 70B) are pretrained versions\
          \ without additional finetuning or instruction-tuning. Therefore, the specified\
          \ format with <|im_start|> and <|im_end|> were not intended for use with\
          \ these models. "
        updatedAt: '2023-12-01T01:00:29.852Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - XYPB
        - j16h
    id: 6569302db226361dc76249b8
    type: comment
  author: zechen-nlp
  content: "Hi there,\nThank you for bringing this to our attention. \nThere was indeed\
    \ a size mismatch between the tokenizer and the model embedding. We uploaded an\
    \ updated version of the tokenizer along with its configurations. Let us know\
    \ if this resolves the issue. We appreciate your feedback!\n\nRegarding the confusion\
    \ with the downstream-use instructions, we want to clarify that the models we\
    \ uploaded (7b & 70B) are pretrained versions without additional finetuning or\
    \ instruction-tuning. Therefore, the specified format with <|im_start|> and <|im_end|>\
    \ were not intended for use with these models. "
  created_at: 2023-12-01 01:00:29+00:00
  edited: false
  hidden: false
  id: 6569302db226361dc76249b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg?w=200&h=200&f=face
      fullname: Zeming Chen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zechen-nlp
      type: user
    createdAt: '2023-12-01T01:00:58.000Z'
    data:
      edited: false
      editors:
      - zechen-nlp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8793869614601135
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg?w=200&h=200&f=face
          fullname: Zeming Chen
          isHf: false
          isPro: false
          name: zechen-nlp
          type: user
        html: '<p>We''ve updated our deployment document to reflect this better and
          provide relevant examples for the pretrained models. We''re always looking
          to improve, so your suggestions for enhancing our documentation are most
          welcome.</p>

          <p>Looking forward to your feedback!</p>

          '
        raw: 'We''ve updated our deployment document to reflect this better and provide
          relevant examples for the pretrained models. We''re always looking to improve,
          so your suggestions for enhancing our documentation are most welcome.


          Looking forward to your feedback!'
        updatedAt: '2023-12-01T01:00:58.235Z'
      numEdits: 0
      reactions: []
    id: 6569304a7fd1d42138fbed4a
    type: comment
  author: zechen-nlp
  content: 'We''ve updated our deployment document to reflect this better and provide
    relevant examples for the pretrained models. We''re always looking to improve,
    so your suggestions for enhancing our documentation are most welcome.


    Looking forward to your feedback!'
  created_at: 2023-12-01 01:00:58+00:00
  edited: false
  hidden: false
  id: 6569304a7fd1d42138fbed4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f84cff919b44a471d1972e6caf96e3f.svg
      fullname: Garcia Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XYPB
      type: user
    createdAt: '2023-12-01T02:23:30.000Z'
    data:
      edited: true
      editors:
      - XYPB
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6444848775863647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f84cff919b44a471d1972e6caf96e3f.svg
          fullname: Garcia Du
          isHf: false
          isPro: false
          name: XYPB
          type: user
        html: "<p>Thanks for your update, but I still seem to have this issue, as\
          \ shown below</p>\n<pre><code class=\"language-python\"><span class=\"hljs-meta\"\
          >&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"epfl-llm/meditron-7b\"</span>, token=MY_TOKEN)\n<span class=\"\
          hljs-meta\">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"epfl-llm/meditron-7b\"</span>, token=MY_TOKEN)\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>output = tokenizer([<span\
          \ class=\"hljs-string\">\"This is a chest X-ray of a patient with Cardiomegaly\"\
          </span>], return_tensors=<span class=\"hljs-string\">\"pt\"</span>, truncation=<span\
          \ class=\"hljs-literal\">True</span>, padding=<span class=\"hljs-string\"\
          >\"max_length\"</span>, max_length=<span class=\"hljs-number\">64</span>,)\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>model(**output)\nTraceback\
          \ (most recent call last):\n  File <span class=\"hljs-string\">\"&lt;stdin&gt;\"\
          </span>, line <span class=\"hljs-number\">1</span>, <span class=\"hljs-keyword\"\
          >in</span> &lt;module&gt;\n  File <span class=\"hljs-string\">\"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          </span>, line <span class=\"hljs-number\">1518</span>, <span class=\"hljs-keyword\"\
          >in</span> _wrapped_call_impl\n    <span class=\"hljs-keyword\">return</span>\
          \ self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  ...\n  File <span class=\"hljs-string\">\"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
          </span>, line <span class=\"hljs-number\">162</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    <span class=\"hljs-keyword\">return</span> F.embedding(\n\
          \           ^^^^^^^^^^^^\n  File <span class=\"hljs-string\">\"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/functional.py\"\
          </span>, line <span class=\"hljs-number\">2233</span>, <span class=\"hljs-keyword\"\
          >in</span> embedding\n    <span class=\"hljs-keyword\">return</span> torch.embedding(weight,\
          \ <span class=\"hljs-built_in\">input</span>, padding_idx, scale_grad_by_freq,\
          \ sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          IndexError: index out of <span class=\"hljs-built_in\">range</span> <span\
          \ class=\"hljs-keyword\">in</span> self\n</code></pre>\n<p>It seems that\
          \ the tokenizer, <em>which is indeed smaller than before</em>, is still\
          \ longer than the embedding layer of the model:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-meta\">&gt;&gt;&gt; </span><span class=\"\
          hljs-built_in\">len</span>(tokenizer)\n<span class=\"hljs-number\">32005</span>\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>model.model.embed_tokens\n\
          Embedding(<span class=\"hljs-number\">32000</span>, <span class=\"hljs-number\"\
          >4096</span>)\n</code></pre>\n<p>When I ask the tokenizer to pad the input,\
          \ it will use the padding token with id 32004, which will result in of range\
          \ error above. If I skip the padding mode, it will then execute properly.\
          \ Alternatively, if I set <code>tokenizer.pad_token = tokenizer.eos_token</code>,\
          \ the code can also run properly.</p>\n<p>It seems that those five extra\
          \ tokens are all special tokens:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword = {v: k <span class=\"\
          hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> tokenizer.get_vocab().items()}\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword[<span class=\"hljs-number\"\
          >32000</span>]\n<span class=\"hljs-string\">'&lt;CLS&gt;'</span>\n<span\
          \ class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword[<span class=\"hljs-number\"\
          >32001</span>]\n<span class=\"hljs-string\">'&lt;SEP&gt;'</span>\n<span\
          \ class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword[<span class=\"hljs-number\"\
          >32002</span>]\n<span class=\"hljs-string\">'&lt;EOD&gt;'</span>\n<span\
          \ class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword[<span class=\"hljs-number\"\
          >32003</span>]\n<span class=\"hljs-string\">'&lt;MASK&gt;'</span>\n<span\
          \ class=\"hljs-meta\">&gt;&gt;&gt; </span>idxtoword[<span class=\"hljs-number\"\
          >32004</span>]\n<span class=\"hljs-string\">'&lt;PAD&gt;'</span>\n</code></pre>\n\
          <p>If I understand correctly, these tokens are not used during training\
          \ (since LLAMA doesn't use those tokens as well.) So, I think you may be\
          \ able to get around this issue by simply removing these tokens.</p>\n"
        raw: "Thanks for your update, but I still seem to have this issue, as shown\
          \ below\n\n```python\n>>> tokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\"\
          , token=MY_TOKEN)\n>>> model = AutoModelForCausalLM.from_pretrained(\"epfl-llm/meditron-7b\"\
          , token=MY_TOKEN)\n>>> output = tokenizer([\"This is a chest X-ray of a\
          \ patient with Cardiomegaly\"], return_tensors=\"pt\", truncation=True,\
          \ padding=\"max_length\", max_length=64,)\n>>> model(**output)\nTraceback\
          \ (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File\
          \ \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  ...\n  File \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n\
          \  File \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/functional.py\"\
          , line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          IndexError: index out of range in self\n```\n\nIt seems that the tokenizer,\
          \ *which is indeed smaller than before*, is still longer than the embedding\
          \ layer of the model:\n```python\n>>> len(tokenizer)\n32005\n>>> model.model.embed_tokens\n\
          Embedding(32000, 4096)\n```\n\nWhen I ask the tokenizer to pad the input,\
          \ it will use the padding token with id 32004, which will result in of range\
          \ error above. If I skip the padding mode, it will then execute properly.\
          \ Alternatively, if I set `tokenizer.pad_token = tokenizer.eos_token`, the\
          \ code can also run properly.\n\nIt seems that those five extra tokens are\
          \ all special tokens:\n```python\n>>> idxtoword = {v: k for k, v in tokenizer.get_vocab().items()}\n\
          >>> idxtoword[32000]\n'<CLS>'\n>>> idxtoword[32001]\n'<SEP>'\n>>> idxtoword[32002]\n\
          '<EOD>'\n>>> idxtoword[32003]\n'<MASK>'\n>>> idxtoword[32004]\n'<PAD>'\n\
          ```\n\nIf I understand correctly, these tokens are not used during training\
          \ (since LLAMA doesn't use those tokens as well.) So, I think you may be\
          \ able to get around this issue by simply removing these tokens.\n\n\n"
        updatedAt: '2023-12-01T13:56:50.525Z'
      numEdits: 1
      reactions: []
    id: 656943a211b1195d496fa219
    type: comment
  author: XYPB
  content: "Thanks for your update, but I still seem to have this issue, as shown\
    \ below\n\n```python\n>>> tokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\"\
    , token=MY_TOKEN)\n>>> model = AutoModelForCausalLM.from_pretrained(\"epfl-llm/meditron-7b\"\
    , token=MY_TOKEN)\n>>> output = tokenizer([\"This is a chest X-ray of a patient\
    \ with Cardiomegaly\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\"\
    , max_length=64,)\n>>> model(**output)\nTraceback (most recent call last):\n \
    \ File \"<stdin>\", line 1, in <module>\n  File \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  ...\n  File \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File\
    \ \"~/.conda/envs/clip/lib/python3.11/site-packages/torch/nn/functional.py\",\
    \ line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    IndexError: index out of range in self\n```\n\nIt seems that the tokenizer, *which\
    \ is indeed smaller than before*, is still longer than the embedding layer of\
    \ the model:\n```python\n>>> len(tokenizer)\n32005\n>>> model.model.embed_tokens\n\
    Embedding(32000, 4096)\n```\n\nWhen I ask the tokenizer to pad the input, it will\
    \ use the padding token with id 32004, which will result in of range error above.\
    \ If I skip the padding mode, it will then execute properly. Alternatively, if\
    \ I set `tokenizer.pad_token = tokenizer.eos_token`, the code can also run properly.\n\
    \nIt seems that those five extra tokens are all special tokens:\n```python\n>>>\
    \ idxtoword = {v: k for k, v in tokenizer.get_vocab().items()}\n>>> idxtoword[32000]\n\
    '<CLS>'\n>>> idxtoword[32001]\n'<SEP>'\n>>> idxtoword[32002]\n'<EOD>'\n>>> idxtoword[32003]\n\
    '<MASK>'\n>>> idxtoword[32004]\n'<PAD>'\n```\n\nIf I understand correctly, these\
    \ tokens are not used during training (since LLAMA doesn't use those tokens as\
    \ well.) So, I think you may be able to get around this issue by simply removing\
    \ these tokens.\n\n\n"
  created_at: 2023-12-01 02:23:30+00:00
  edited: true
  hidden: false
  id: 656943a211b1195d496fa219
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abab3a7115123010c728a17203eb6aa8.svg
      fullname: Simon CHAUVIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bg90
      type: user
    createdAt: '2023-12-01T08:50:29.000Z'
    data:
      edited: false
      editors:
      - bg90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7896246910095215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abab3a7115123010c728a17203eb6aa8.svg
          fullname: Simon CHAUVIN
          isHf: false
          isPro: false
          name: bg90
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zechen-nlp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zechen-nlp\">@<span class=\"\
          underline\">zechen-nlp</span></a></span>\n\n\t</span></span>, Thank your\
          \ for your answer.<br>I have downloaded the new files. And now I can run\
          \ inference using <code>oobabooga/text-generation-webui</code>.<br>Many\
          \ thanks.</p>\n"
        raw: '@zechen-nlp, Thank your for your answer.

          I have downloaded the new files. And now I can run inference using `oobabooga/text-generation-webui`.

          Many thanks.'
        updatedAt: '2023-12-01T08:50:29.860Z'
      numEdits: 0
      reactions: []
    id: 65699e55046899997b97d946
    type: comment
  author: bg90
  content: '@zechen-nlp, Thank your for your answer.

    I have downloaded the new files. And now I can run inference using `oobabooga/text-generation-webui`.

    Many thanks.'
  created_at: 2023-12-01 08:50:29+00:00
  edited: false
  hidden: false
  id: 65699e55046899997b97d946
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b227ae11e3b455f88e1a9bf7483420a.svg
      fullname: Niall Taylor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NTaylor
      type: user
    createdAt: '2023-12-01T09:18:45.000Z'
    data:
      edited: false
      editors:
      - NTaylor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9664828181266785
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b227ae11e3b455f88e1a9bf7483420a.svg
          fullname: Niall Taylor
          isHf: false
          isPro: false
          name: NTaylor
          type: user
        html: '<p>Similar as above - the tokenizer is still adding special tokens
          to the base model that it is not expecting</p>

          '
        raw: Similar as above - the tokenizer is still adding special tokens to the
          base model that it is not expecting
        updatedAt: '2023-12-01T09:18:45.094Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mpmisko
    id: 6569a4f51bac1166939c38b6
    type: comment
  author: NTaylor
  content: Similar as above - the tokenizer is still adding special tokens to the
    base model that it is not expecting
  created_at: 2023-12-01 09:18:45+00:00
  edited: false
  hidden: false
  id: 6569a4f51bac1166939c38b6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: epfl-llm/meditron-7b
repo_type: model
status: open
target_branch: null
title: 'CUDA error: device-side assert triggered'
