!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ReadySetFly
conflicting_files: null
created_at: 2023-06-07 15:47:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9b109d166d851ee3d6710a764243d13.svg
      fullname: Ian G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReadySetFly
      type: user
    createdAt: '2023-06-07T16:47:08.000Z'
    data:
      edited: true
      editors:
      - ReadySetFly
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7619707584381104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9b109d166d851ee3d6710a764243d13.svg
          fullname: Ian G
          isHf: false
          isPro: false
          name: ReadySetFly
          type: user
        html: "<p>Hello, thank you for quantizing this model! I am able to run the\
          \ q5_1 model but the q5_K_M model is not working. I am using version llama-master-5c64a09-bin-win-cublas-cu12.1.0-x64.\
          \ </p>\n<pre><code>main: build = 635 (5c64a09)\nmain: seed  = 1686155608\n\
          ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA ...\nllama.cpp:\
          \ loading model from C:\\...\\OpenLLAMA7B-q5_K_M-ggml.bin\n</code></pre>\n\
          <p>That's the only message and then it returns. Again, the q5_1 is working\
          \ fine for CPU. Another issue is when using GPU, q5_1 spits out gibberish\
          \ (sounds like this one is tracked on llama.cpp <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/llama.cpp/issues/1735\">https://github.com/ggerganov/llama.cpp/issues/1735</a>)</p>\n"
        raw: "Hello, thank you for quantizing this model! I am able to run the q5_1\
          \ model but the q5_K_M model is not working. I am using version llama-master-5c64a09-bin-win-cublas-cu12.1.0-x64.\
          \ \n\n```\nmain: build = 635 (5c64a09)\nmain: seed  = 1686155608\nggml_init_cublas:\
          \ found 1 CUDA devices:\n  Device 0: NVIDIA ...\nllama.cpp: loading model\
          \ from C:\\...\\OpenLLAMA7B-q5_K_M-ggml.bin\n```\n\nThat's the only message\
          \ and then it returns. Again, the q5_1 is working fine for CPU. Another\
          \ issue is when using GPU, q5_1 spits out gibberish (sounds like this one\
          \ is tracked on llama.cpp https://github.com/ggerganov/llama.cpp/issues/1735)"
        updatedAt: '2023-06-07T17:14:51.083Z'
      numEdits: 2
      reactions: []
    id: 6480b48cc0d3c70316d2c6e2
    type: comment
  author: ReadySetFly
  content: "Hello, thank you for quantizing this model! I am able to run the q5_1\
    \ model but the q5_K_M model is not working. I am using version llama-master-5c64a09-bin-win-cublas-cu12.1.0-x64.\
    \ \n\n```\nmain: build = 635 (5c64a09)\nmain: seed  = 1686155608\nggml_init_cublas:\
    \ found 1 CUDA devices:\n  Device 0: NVIDIA ...\nllama.cpp: loading model from\
    \ C:\\...\\OpenLLAMA7B-q5_K_M-ggml.bin\n```\n\nThat's the only message and then\
    \ it returns. Again, the q5_1 is working fine for CPU. Another issue is when using\
    \ GPU, q5_1 spits out gibberish (sounds like this one is tracked on llama.cpp\
    \ https://github.com/ggerganov/llama.cpp/issues/1735)"
  created_at: 2023-06-07 15:47:08+00:00
  edited: true
  hidden: false
  id: 6480b48cc0d3c70316d2c6e2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: RachidAR/open_llama_7b-ggml
repo_type: model
status: open
target_branch: null
title: q5_K_M not working with llama.cpp  Not working
