!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boffy
conflicting_files: null
created_at: 2024-01-13 20:25:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2024-01-13T20:25:20.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9704849123954773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>It was a simple question, ooba latest, only setting changed was
          max_new_tokens to 4096....</p>

          <p>had to kill the process for it to stop, the stop button didn''t work...
          </p>

          <p><a rel="nofollow" href="https://pastebin.com/c3UNqShv">https://pastebin.com/c3UNqShv</a></p>

          '
        raw: "It was a simple question, ooba latest, only setting changed was max_new_tokens\
          \ to 4096....\r\n\r\nhad to kill the process for it to stop, the stop button\
          \ didn't work... \r\n\r\nhttps://pastebin.com/c3UNqShv"
        updatedAt: '2024-01-13T20:25:20.571Z'
      numEdits: 0
      reactions: []
    id: 65a2f1b0974f8f811f0e75de
    type: comment
  author: Boffy
  content: "It was a simple question, ooba latest, only setting changed was max_new_tokens\
    \ to 4096....\r\n\r\nhad to kill the process for it to stop, the stop button didn't\
    \ work... \r\n\r\nhttps://pastebin.com/c3UNqShv"
  created_at: 2024-01-13 20:25:20+00:00
  edited: false
  hidden: false
  id: 65a2f1b0974f8f811f0e75de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-14T03:07:54.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295634627342224
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>No issues with the model stopping for me here. Try using exui as
          an alternative UI and loader or redownload the model again.</p>

          '
        raw: No issues with the model stopping for me here. Try using exui as an alternative
          UI and loader or redownload the model again.
        updatedAt: '2024-01-14T03:07:54.833Z'
      numEdits: 0
      reactions: []
    id: 65a3500a6e52f833408d98be
    type: comment
  author: LoneStriker
  content: No issues with the model stopping for me here. Try using exui as an alternative
    UI and loader or redownload the model again.
  created_at: 2024-01-14 03:07:54+00:00
  edited: false
  hidden: false
  id: 65a3500a6e52f833408d98be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2024-01-14T13:24:21.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168580770492554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;boffy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/boffy\">@<span class=\"\
          underline\">boffy</span></a></span>\n\n\t</span></span> Try changing prompt\
          \ template in ooba to chatml. I see \"You\" and \"AI\" in your text, this\
          \ suggest that you are using default chat template that doesn't have chatml\
          \ prompt structure with &lt;|im_start|&gt; and &lt;|im_end|&gt; tokens.\
          \ &lt;|im_end|&gt; is the EOS token that should stop the generation here,\
          \ but since you are using different prompt format, it's kind of expected\
          \ that it won't be outputted. Also, make sure you didn't disable adding\
          \ BOS token in ooba settings - according to json files it should be enabled\
          \ for this model. </p>\n"
        raw: '@boffy Try changing prompt template in ooba to chatml. I see "You" and
          "AI" in your text, this suggest that you are using default chat template
          that doesn''t have chatml prompt structure with <|im_start|> and <|im_end|>
          tokens. <|im_end|> is the EOS token that should stop the generation here,
          but since you are using different prompt format, it''s kind of expected
          that it won''t be outputted. Also, make sure you didn''t disable adding
          BOS token in ooba settings - according to json files it should be enabled
          for this model. '
        updatedAt: '2024-01-14T13:24:21.245Z'
      numEdits: 0
      reactions: []
    id: 65a3e085c8a09bd5e86a998e
    type: comment
  author: adamo1139
  content: '@boffy Try changing prompt template in ooba to chatml. I see "You" and
    "AI" in your text, this suggest that you are using default chat template that
    doesn''t have chatml prompt structure with <|im_start|> and <|im_end|> tokens.
    <|im_end|> is the EOS token that should stop the generation here, but since you
    are using different prompt format, it''s kind of expected that it won''t be outputted.
    Also, make sure you didn''t disable adding BOS token in ooba settings - according
    to json files it should be enabled for this model. '
  created_at: 2024-01-14 13:24:21+00:00
  edited: false
  hidden: false
  id: 65a3e085c8a09bd5e86a998e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/UNA-dolphin-2.6-mistral-7b-dpo-laser-8.0bpw-h8-exl2
repo_type: model
status: open
target_branch: null
title: It just didn't want to stop talking...
