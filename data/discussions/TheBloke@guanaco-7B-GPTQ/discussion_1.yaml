!!python/object:huggingface_hub.community.DiscussionWithDetails
author: balu548411
conflicting_files: null
created_at: 2023-06-07 09:20:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
      fullname: balu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balu548411
      type: user
    createdAt: '2023-06-07T10:20:07.000Z'
    data:
      edited: false
      editors:
      - balu548411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5421357154846191
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
          fullname: balu
          isHf: false
          isPro: false
          name: balu548411
          type: user
        html: '<p>can someone suggest how to load this model</p>

          <p>code:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("TheBloke/guanaco-33B-GPTQ")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("TheBloke/guanaco-33B-GPTQ")</p>

          <p>error:</p>

          <p>OSError                                   Traceback (most recent call
          last)<br> in &lt;cell line: 5&gt;()<br>      3 tokenizer = AutoTokenizer.from_pretrained("TheBloke/guanaco-33B-GPTQ")<br>      4<br>----&gt;
          5 model = AutoModelForCausalLM.from_pretrained("TheBloke/guanaco-33B-GPTQ")</p>

          <p>1 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py
          in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>   2491                             )<br>   2492                         else:<br>-&gt;
          2493                             raise EnvironmentError(<br>   2494                                 f"{pretrained_model_name_or_path}
          does not appear to have a file named"<br>   2495                                 f"
          {_add_variant(WEIGHTS_NAME, variant)}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}
          or"</p>

          <p>OSError: TheBloke/guanaco-33B-GPTQ does not appear to have a file named
          pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.</p>

          '
        raw: "can someone suggest how to load this model\r\n\r\ncode:\r\n\r\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"TheBloke/guanaco-33B-GPTQ\")\r\n\r\n\
          model = AutoModelForCausalLM.from_pretrained(\"TheBloke/guanaco-33B-GPTQ\"\
          )\r\n\r\nerror:\r\n\r\nOSError                                   Traceback\
          \ (most recent call last)\r\n<ipython-input-14-344e1e6ffc93> in <cell line:\
          \ 5>()\r\n      3 tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/guanaco-33B-GPTQ\"\
          )\r\n      4 \r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/guanaco-33B-GPTQ\")\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
          \n   2491                             )\r\n   2492                     \
          \    else:\r\n-> 2493                             raise EnvironmentError(\r\
          \n   2494                                 f\"{pretrained_model_name_or_path}\
          \ does not appear to have a file named\"\r\n   2495                    \
          \             f\" {_add_variant(WEIGHTS_NAME, variant)}, {TF2_WEIGHTS_NAME},\
          \ {TF_WEIGHTS_NAME} or\"\r\n\r\nOSError: TheBloke/guanaco-33B-GPTQ does\
          \ not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
          \ or flax_model.msgpack."
        updatedAt: '2023-06-07T10:20:07.822Z'
      numEdits: 0
      reactions: []
    id: 648059d7de559d48dbafe926
    type: comment
  author: balu548411
  content: "can someone suggest how to load this model\r\n\r\ncode:\r\n\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/guanaco-33B-GPTQ\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/guanaco-33B-GPTQ\")\r\n\r\nerror:\r\n\r\nOSError                    \
    \               Traceback (most recent call last)\r\n<ipython-input-14-344e1e6ffc93>\
    \ in <cell line: 5>()\r\n      3 tokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/guanaco-33B-GPTQ\")\r\n      4 \r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/guanaco-33B-GPTQ\")\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
    \n   2491                             )\r\n   2492                         else:\r\
    \n-> 2493                             raise EnvironmentError(\r\n   2494     \
    \                            f\"{pretrained_model_name_or_path} does not appear\
    \ to have a file named\"\r\n   2495                                 f\" {_add_variant(WEIGHTS_NAME,\
    \ variant)}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or\"\r\n\r\nOSError: TheBloke/guanaco-33B-GPTQ\
    \ does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
    \ or flax_model.msgpack."
  created_at: 2023-06-07 09:20:07+00:00
  edited: false
  hidden: false
  id: 648059d7de559d48dbafe926
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T10:28:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.184747114777565
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You can't use standard transformers, you need to use AutoGPTQ:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nparser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Simple AutoGPTQ example'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'model_name_or_path'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Model folder or repo'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--model_basename'</span>,\
          \ <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'Model file basename if model is not named gptq_model-Xb-Ygr'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--use_slow'</span>, action=<span\
          \ class=\"hljs-string\">\"store_true\"</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Use slow tokenizer'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--use_safetensors'</span>,\
          \ action=<span class=\"hljs-string\">\"store_true\"</span>, <span class=\"\
          hljs-built_in\">help</span>=<span class=\"hljs-string\">'Model file basename\
          \ if model is not named gptq_model-Xb-Ygr'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--use_triton'</span>, action=<span class=\"hljs-string\"\
          >\"store_true\"</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Use Triton for inference?'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--bits'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\"\
          >4</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'Specify GPTQ bits. Only needed if no quantize_config.json is provided'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--group_size'</span>, <span\
          \ class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\
          \ default=<span class=\"hljs-number\">128</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Specify GPTQ group_size. Only\
          \ needed if no quantize_config.json is provided'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--desc_act'</span>, action=<span class=\"hljs-string\"\
          >\"store_true\"</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Specify GPTQ desc_act. Only needed if no quantize_config.json\
          \ is provided'</span>)\n\nargs = parser.parse_args()\n\nquantized_model_dir\
          \ = args.model_name_or_path\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=<span class=\"hljs-keyword\">not</span> args.use_slow)\n\n<span\
          \ class=\"hljs-keyword\">try</span>:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
          <span class=\"hljs-keyword\">except</span>:\n    quantize_config = BaseQuantizeConfig(\n\
          \            bits=args.bits,\n            group_size=args.group_size,\n\
          \            desc_act=args.desc_act\n        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=args.model_basename,\n        device=<span class=\"\
          hljs-string\">\"cuda:0\"</span>,\n        use_triton=args.use_triton,\n\
          \        quantize_config=quantize_config)\n\n<span class=\"hljs-comment\"\
          ># Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Example execution:</p>\n\
          <pre><code>[pytorch2] ubuntu@h100:/workspace/AIScripts git:(main) $ python\
          \ simple_autogptq.py TheBloke/guanaco-7B-GPTQ --model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order\
          \ --use_safetensors\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 700/700 [00:00&lt;00:00, 1.59MB/s]\n\
          Downloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00&lt;00:00, 8.68MB/s]\n\
          Downloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M\
          \ [00:00&lt;00:00, 7.91MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 411/411 [00:00&lt;00:00,\
          \ 1.34MB/s]\nDownloading (\u2026)quantize_config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 223/223 [00:00&lt;00:00, 735kB/s]\n\
          Downloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 593/593 [00:00&lt;00:00, 1.92MB/s]\nDownloading (\u2026\
          )ct-order.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00G/4.00G [00:36&lt;00:00,\
          \ 111MB/s]\nThe safetensors archive passed at /home/ubuntu/.cache/huggingface/hub/models--TheBloke--guanaco-7B-GPTQ/snapshots/e9e797cac5e4385a10e3a74927860c6552f860c6/Guanaco-7B-GPTQ-4bit-128g.no-act-order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n*** Pipeline:\n### Human: Tell\
          \ me about AI\n### Assistant: Artificial intelligence (AI) is a type of\
          \ technology that simulates human intelligence and enables computers to\
          \ perform tasks that would require human intelligence if done by people.\
          \ It includes the study of how to make computer systems do things that would\
          \ require human intelligence if done by humans.\n\nThe term \"artificial\
          \ intelligence\" was first used in 1956, when John McCarthy coined it while\
          \ at MIT's Research Laboratory for Electronics. He defined it as \"the science\
          \ and engineering of making intelligent machines.\"\n\nToday, AI has applications\
          \ in many fields including healthcare, finance, manufacturing, transportation,\
          \ and more. For example, AI can be used to diagnose medical conditions,\
          \ automate financial transactions, design products, and manage fleets of\
          \ vehicles.\n\n\n*** Generate:\n&lt;s&gt; ### Human: Tell me about AI\n\
          ### Assistant: Artificial Intelligence (AI) is a field of computer science\
          \ that deals with the simulation of human intelligence and the automation\
          \ of tasks that require it. It is a subfield of computer science that deals\
          \ with the design, development, and study of intelligent computer systems.\
          \ AI is a branch of computer science that deals with the simulation of human\
          \ intelligence by machines.\n</code></pre>\n<p>Note the need to specify\
          \ <code>--model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order</code> to\
          \ tell it the name of the model file (enter everything before <code>.safetensors</code>\
          \ in the model file)</p>\n"
        raw: "You can't use standard transformers, you need to use AutoGPTQ:\n```python\n\
          from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nparser\
          \ = argparse.ArgumentParser(description='Simple AutoGPTQ example')\nparser.add_argument('model_name_or_path',\
          \ type=str, help='Model folder or repo')\nparser.add_argument('--model_basename',\
          \ type=str, help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
          parser.add_argument('--use_slow', action=\"store_true\", help='Use slow\
          \ tokenizer')\nparser.add_argument('--use_safetensors', action=\"store_true\"\
          , help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
          parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton\
          \ for inference?')\nparser.add_argument('--bits', type=int, default=4, help='Specify\
          \ GPTQ bits. Only needed if no quantize_config.json is provided')\nparser.add_argument('--group_size',\
          \ type=int, default=128, help='Specify GPTQ group_size. Only needed if no\
          \ quantize_config.json is provided')\nparser.add_argument('--desc_act',\
          \ action=\"store_true\", help='Specify GPTQ desc_act. Only needed if no\
          \ quantize_config.json is provided')\n\nargs = parser.parse_args()\n\nquantized_model_dir\
          \ = args.model_name_or_path\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=not args.use_slow)\n\ntry:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
          except:\n    quantize_config = BaseQuantizeConfig(\n            bits=args.bits,\n\
          \            group_size=args.group_size,\n            desc_act=args.desc_act\n\
          \        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=args.model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=args.use_triton,\n     \
          \   quantize_config=quantize_config)\n\n# Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n\
          ### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"\
          text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\
          \nprint(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n***\
          \ Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```\n\nExample execution:\n```\n[pytorch2]\
          \ ubuntu@h100:/workspace/AIScripts git:(main) $ python simple_autogptq.py\
          \ TheBloke/guanaco-7B-GPTQ --model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order\
          \ --use_safetensors\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 700/700 [00:00<00:00, 1.59MB/s]\n\
          Downloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00<00:00, 8.68MB/s]\n\
          Downloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M\
          \ [00:00<00:00, 7.91MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 411/411 [00:00<00:00, 1.34MB/s]\n\
          Downloading (\u2026)quantize_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 223/223 [00:00<00:00, 735kB/s]\nDownloading (\u2026\
          )lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 593/593 [00:00<00:00, 1.92MB/s]\nDownloading (\u2026)ct-order.safetensors:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 4.00G/4.00G [00:36<00:00, 111MB/s]\n\
          The safetensors archive passed at /home/ubuntu/.cache/huggingface/hub/models--TheBloke--guanaco-7B-GPTQ/snapshots/e9e797cac5e4385a10e3a74927860c6552f860c6/Guanaco-7B-GPTQ-4bit-128g.no-act-order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n*** Pipeline:\n### Human: Tell\
          \ me about AI\n### Assistant: Artificial intelligence (AI) is a type of\
          \ technology that simulates human intelligence and enables computers to\
          \ perform tasks that would require human intelligence if done by people.\
          \ It includes the study of how to make computer systems do things that would\
          \ require human intelligence if done by humans.\n\nThe term \"artificial\
          \ intelligence\" was first used in 1956, when John McCarthy coined it while\
          \ at MIT's Research Laboratory for Electronics. He defined it as \"the science\
          \ and engineering of making intelligent machines.\"\n\nToday, AI has applications\
          \ in many fields including healthcare, finance, manufacturing, transportation,\
          \ and more. For example, AI can be used to diagnose medical conditions,\
          \ automate financial transactions, design products, and manage fleets of\
          \ vehicles.\n\n\n*** Generate:\n<s> ### Human: Tell me about AI\n### Assistant:\
          \ Artificial Intelligence (AI) is a field of computer science that deals\
          \ with the simulation of human intelligence and the automation of tasks\
          \ that require it. It is a subfield of computer science that deals with\
          \ the design, development, and study of intelligent computer systems. AI\
          \ is a branch of computer science that deals with the simulation of human\
          \ intelligence by machines.\n```\n\nNote the need to specify `--model_basename\
          \ Guanaco-7B-GPTQ-4bit-128g.no-act-order` to tell it the name of the model\
          \ file (enter everything before `.safetensors` in the model file)"
        updatedAt: '2023-06-07T10:28:58.118Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - jocastroc
        - nacs
    id: 64805beabb25a636c9d8046d
    type: comment
  author: TheBloke
  content: "You can't use standard transformers, you need to use AutoGPTQ:\n```python\n\
    from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nparser = argparse.ArgumentParser(description='Simple\
    \ AutoGPTQ example')\nparser.add_argument('model_name_or_path', type=str, help='Model\
    \ folder or repo')\nparser.add_argument('--model_basename', type=str, help='Model\
    \ file basename if model is not named gptq_model-Xb-Ygr')\nparser.add_argument('--use_slow',\
    \ action=\"store_true\", help='Use slow tokenizer')\nparser.add_argument('--use_safetensors',\
    \ action=\"store_true\", help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
    parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton for\
    \ inference?')\nparser.add_argument('--bits', type=int, default=4, help='Specify\
    \ GPTQ bits. Only needed if no quantize_config.json is provided')\nparser.add_argument('--group_size',\
    \ type=int, default=128, help='Specify GPTQ group_size. Only needed if no quantize_config.json\
    \ is provided')\nparser.add_argument('--desc_act', action=\"store_true\", help='Specify\
    \ GPTQ desc_act. Only needed if no quantize_config.json is provided')\n\nargs\
    \ = parser.parse_args()\n\nquantized_model_dir = args.model_name_or_path\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=not args.use_slow)\n\
    \ntry:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
    except:\n    quantize_config = BaseQuantizeConfig(\n            bits=args.bits,\n\
    \            group_size=args.group_size,\n            desc_act=args.desc_act\n\
    \        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        model_basename=args.model_basename,\n\
    \        device=\"cuda:0\",\n        use_triton=args.use_triton,\n        quantize_config=quantize_config)\n\
    \n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n\
    logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
    \ Human: {prompt}\n### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```\n\nExample execution:\n```\n[pytorch2]\
    \ ubuntu@h100:/workspace/AIScripts git:(main) $ python simple_autogptq.py TheBloke/guanaco-7B-GPTQ\
    \ --model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order --use_safetensors\n\
    Downloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 700/700 [00:00<00:00, 1.59MB/s]\nDownloading tokenizer.model:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 500k/500k [00:00<00:00, 8.68MB/s]\nDownloading (\u2026)/main/tokenizer.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M [00:00<00:00, 7.91MB/s]\nDownloading\
    \ (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 411/411 [00:00<00:00, 1.34MB/s]\nDownloading (\u2026)quantize_config.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 223/223 [00:00<00:00,\
    \ 735kB/s]\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 593/593 [00:00<00:00, 1.92MB/s]\nDownloading (\u2026\
    )ct-order.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00G/4.00G [00:36<00:00,\
    \ 111MB/s]\nThe safetensors archive passed at /home/ubuntu/.cache/huggingface/hub/models--TheBloke--guanaco-7B-GPTQ/snapshots/e9e797cac5e4385a10e3a74927860c6552f860c6/Guanaco-7B-GPTQ-4bit-128g.no-act-order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\nskip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet.\n*** Pipeline:\n### Human: Tell me\
    \ about AI\n### Assistant: Artificial intelligence (AI) is a type of technology\
    \ that simulates human intelligence and enables computers to perform tasks that\
    \ would require human intelligence if done by people. It includes the study of\
    \ how to make computer systems do things that would require human intelligence\
    \ if done by humans.\n\nThe term \"artificial intelligence\" was first used in\
    \ 1956, when John McCarthy coined it while at MIT's Research Laboratory for Electronics.\
    \ He defined it as \"the science and engineering of making intelligent machines.\"\
    \n\nToday, AI has applications in many fields including healthcare, finance, manufacturing,\
    \ transportation, and more. For example, AI can be used to diagnose medical conditions,\
    \ automate financial transactions, design products, and manage fleets of vehicles.\n\
    \n\n*** Generate:\n<s> ### Human: Tell me about AI\n### Assistant: Artificial\
    \ Intelligence (AI) is a field of computer science that deals with the simulation\
    \ of human intelligence and the automation of tasks that require it. It is a subfield\
    \ of computer science that deals with the design, development, and study of intelligent\
    \ computer systems. AI is a branch of computer science that deals with the simulation\
    \ of human intelligence by machines.\n```\n\nNote the need to specify `--model_basename\
    \ Guanaco-7B-GPTQ-4bit-128g.no-act-order` to tell it the name of the model file\
    \ (enter everything before `.safetensors` in the model file)"
  created_at: 2023-06-07 09:28:58+00:00
  edited: false
  hidden: false
  id: 64805beabb25a636c9d8046d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
      fullname: Adam Filipkowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramzeez88
      type: user
    createdAt: '2023-06-07T18:42:58.000Z'
    data:
      edited: false
      editors:
      - ramzeez88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8165478110313416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
          fullname: Adam Filipkowski
          isHf: false
          isPro: false
          name: ramzeez88
          type: user
        html: '<p>how do you pass the directory to the model ? for me it either says
          error: the following arguments are required: model_name_or_path or unrecognized
          arguments: --model_name_or_path after giving it the directory.</p>

          '
        raw: 'how do you pass the directory to the model ? for me it either says error:
          the following arguments are required: model_name_or_path or unrecognized
          arguments: --model_name_or_path after giving it the directory.'
        updatedAt: '2023-06-07T18:42:58.433Z'
      numEdits: 0
      reactions: []
    id: 6480cfb2bb25a636c9e082ff
    type: comment
  author: ramzeez88
  content: 'how do you pass the directory to the model ? for me it either says error:
    the following arguments are required: model_name_or_path or unrecognized arguments:
    --model_name_or_path after giving it the directory.'
  created_at: 2023-06-07 17:42:58+00:00
  edited: false
  hidden: false
  id: 6480cfb2bb25a636c9e082ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T19:00:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5383839011192322
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can see in the output above how I''m passing it. It''s the first
          argument</p>

          <pre><code>python simple_autogptq.py TheBloke/guanaco-7B-GPTQ --model_basename
          Guanaco-7B-GPTQ-4bit-128g.no-act-order --use_safetensors

          </code></pre>

          '
        raw: 'You can see in the output above how I''m passing it. It''s the first
          argument


          ```

          python simple_autogptq.py TheBloke/guanaco-7B-GPTQ --model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order
          --use_safetensors

          ```'
        updatedAt: '2023-06-07T19:00:22.708Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ramzeez88
    id: 6480d3c69aafd41918ae052c
    type: comment
  author: TheBloke
  content: 'You can see in the output above how I''m passing it. It''s the first argument


    ```

    python simple_autogptq.py TheBloke/guanaco-7B-GPTQ --model_basename Guanaco-7B-GPTQ-4bit-128g.no-act-order
    --use_safetensors

    ```'
  created_at: 2023-06-07 18:00:22+00:00
  edited: false
  hidden: false
  id: 6480d3c69aafd41918ae052c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/guanaco-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'not able to load the model '
