!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kemalcankara
conflicting_files: null
created_at: 2023-05-26 13:38:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594886096000-noauth.jpeg?w=200&h=200&f=face
      fullname: Kemal Can Kara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kemalcankara
      type: user
    createdAt: '2023-05-26T14:38:34.000Z'
    data:
      edited: false
      editors:
      - kemalcankara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594886096000-noauth.jpeg?w=200&h=200&f=face
          fullname: Kemal Can Kara
          isHf: false
          isPro: false
          name: kemalcankara
          type: user
        html: '<p>Hello, congratulations for this amazing work. </p>

          <p>Do you have any plans to incorporate the Turkish language into this model?
          The Turkish language is widely studied in academia, and there is a significant
          community of individuals developing commercial applications with natural
          language processing (NLP). Additionally, it is worth noting that the government
          supports an annual competition specifically focused on Turkish NLP.</p>

          '
        raw: "Hello, congratulations for this amazing work. \r\n\r\nDo you have any\
          \ plans to incorporate the Turkish language into this model? The Turkish\
          \ language is widely studied in academia, and there is a significant community\
          \ of individuals developing commercial applications with natural language\
          \ processing (NLP). Additionally, it is worth noting that the government\
          \ supports an annual competition specifically focused on Turkish NLP."
        updatedAt: '2023-05-26T14:38:34.620Z'
      numEdits: 0
      reactions: []
    id: 6470c46acfd57849518f7876
    type: comment
  author: kemalcankara
  content: "Hello, congratulations for this amazing work. \r\n\r\nDo you have any\
    \ plans to incorporate the Turkish language into this model? The Turkish language\
    \ is widely studied in academia, and there is a significant community of individuals\
    \ developing commercial applications with natural language processing (NLP). Additionally,\
    \ it is worth noting that the government supports an annual competition specifically\
    \ focused on Turkish NLP."
  created_at: 2023-05-26 13:38:34+00:00
  edited: false
  hidden: false
  id: 6470c46acfd57849518f7876
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
      fullname: Daniel Hesslow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DanielHesslow
      type: user
    createdAt: '2023-05-26T16:24:58.000Z'
    data:
      edited: false
      editors:
      - DanielHesslow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
          fullname: Daniel Hesslow
          isHf: false
          isPro: false
          name: DanielHesslow
          type: user
        html: '<p>Not at this time, this is a primarily an English only model. We''ve
          added a some European languages which are related and should not incur too
          much of a  performance penalty. </p>

          <p>High quality multilingual models are an interesting topic which I''m
          sure we will get back to at some point though.</p>

          '
        raw: "Not at this time, this is a primarily an English only model. We've added\
          \ a some European languages which are related and should not incur too much\
          \ of a  performance penalty. \n\nHigh quality multilingual models are an\
          \ interesting topic which I'm sure we will get back to at some point though."
        updatedAt: '2023-05-26T16:24:58.757Z'
      numEdits: 0
      reactions: []
    id: 6470dd5a806c7d87fa1a7cc3
    type: comment
  author: DanielHesslow
  content: "Not at this time, this is a primarily an English only model. We've added\
    \ a some European languages which are related and should not incur too much of\
    \ a  performance penalty. \n\nHigh quality multilingual models are an interesting\
    \ topic which I'm sure we will get back to at some point though."
  created_at: 2023-05-26 15:24:58+00:00
  edited: false
  hidden: false
  id: 6470dd5a806c7d87fa1a7cc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ba1b455fcac94bf0499447298977181.svg
      fullname: Hatem Harbi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HatemH
      type: user
    createdAt: '2023-05-26T18:30:51.000Z'
    data:
      edited: false
      editors:
      - HatemH
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ba1b455fcac94bf0499447298977181.svg
          fullname: Hatem Harbi
          isHf: false
          isPro: false
          name: HatemH
          type: user
        html: '<p>I was excited to hear that there was a model coming from an institution
          based in the UAE. I came here racing, expecting it to be versatile with
          Arabic, but was quite disappointed to find that it wasn''t trained on it
          at all. Should we expect an upcoming version - in the near future - trained
          extensively on Arabic sources?</p>

          '
        raw: I was excited to hear that there was a model coming from an institution
          based in the UAE. I came here racing, expecting it to be versatile with
          Arabic, but was quite disappointed to find that it wasn't trained on it
          at all. Should we expect an upcoming version - in the near future - trained
          extensively on Arabic sources?
        updatedAt: '2023-05-26T18:30:51.206Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Tami3
    id: 6470fadb1f0e7ee7fb1ff72a
    type: comment
  author: HatemH
  content: I was excited to hear that there was a model coming from an institution
    based in the UAE. I came here racing, expecting it to be versatile with Arabic,
    but was quite disappointed to find that it wasn't trained on it at all. Should
    we expect an upcoming version - in the near future - trained extensively on Arabic
    sources?
  created_at: 2023-05-26 17:30:51+00:00
  edited: false
  hidden: false
  id: 6470fadb1f0e7ee7fb1ff72a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2dae53077a9826df53c1b8a821d8a0c5.svg
      fullname: back
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hassanback
      type: user
    createdAt: '2023-05-27T10:28:27.000Z'
    data:
      edited: false
      editors:
      - hassanback
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2dae53077a9826df53c1b8a821d8a0c5.svg
          fullname: back
          isHf: false
          isPro: false
          name: hassanback
          type: user
        html: '<p>Does it support swedish language as good as open AI does?</p>

          '
        raw: Does it support swedish language as good as open AI does?
        updatedAt: '2023-05-27T10:28:27.685Z'
      numEdits: 0
      reactions: []
    id: 6471db4b97a75cc77aa84db5
    type: comment
  author: hassanback
  content: Does it support swedish language as good as open AI does?
  created_at: 2023-05-27 09:28:27+00:00
  edited: false
  hidden: false
  id: 6471db4b97a75cc77aa84db5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:03:29.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p>Hi Kemal and Hatem, </p>\n<p>For this model, we focused on English\
          \ first and foremost, and added European languages for which we could gather\
          \ enough data in our web crawl. To avoid issues with tokenization, we only\
          \ included European languages using the latin alphabet.<br>We have also\
          \ been working on state-of-the-art Arabic language models, and hopefully\
          \ you get to hear about them soon \U0001F91E.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;hassanback&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/hassanback\">@<span class=\"underline\"\
          >hassanback</span></a></span>\n\n\t</span></span>, we do not have good evaluation\
          \ coverage in Swedish, so this is difficult to answer. Happy to hear back\
          \ from you if you end up testing this!  </p>\n"
        raw: "Hi Kemal and Hatem, \n\nFor this model, we focused on English first\
          \ and foremost, and added European languages for which we could gather enough\
          \ data in our web crawl. To avoid issues with tokenization, we only included\
          \ European languages using the latin alphabet. \nWe have also been working\
          \ on state-of-the-art Arabic language models, and hopefully you get to hear\
          \ about them soon \U0001F91E.\n\n@hassanback, we do not have good evaluation\
          \ coverage in Swedish, so this is difficult to answer. Happy to hear back\
          \ from you if you end up testing this!  "
        updatedAt: '2023-05-30T07:03:29.972Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64759fc1e9b57ce0caa11812
    id: 64759fc1e9b57ce0caa11811
    type: comment
  author: FalconLLM
  content: "Hi Kemal and Hatem, \n\nFor this model, we focused on English first and\
    \ foremost, and added European languages for which we could gather enough data\
    \ in our web crawl. To avoid issues with tokenization, we only included European\
    \ languages using the latin alphabet. \nWe have also been working on state-of-the-art\
    \ Arabic language models, and hopefully you get to hear about them soon \U0001F91E\
    .\n\n@hassanback, we do not have good evaluation coverage in Swedish, so this\
    \ is difficult to answer. Happy to hear back from you if you end up testing this!\
    \  "
  created_at: 2023-05-30 06:03:29+00:00
  edited: false
  hidden: false
  id: 64759fc1e9b57ce0caa11811
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:03:29.000Z'
    data:
      status: closed
    id: 64759fc1e9b57ce0caa11812
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 06:03:29+00:00
  id: 64759fc1e9b57ce0caa11812
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
      fullname: "Gediz G\xDCRSU"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TeaCult
      type: user
    createdAt: '2023-07-03T20:14:00.000Z'
    data:
      edited: true
      editors:
      - TeaCult
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6460131406784058
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
          fullname: "Gediz G\xDCRSU"
          isHf: false
          isPro: false
          name: TeaCult
          type: user
        html: "<p>First of all thank you very much for this model.<br>Turkish is a\
          \ European language with latin alphabet, Turkey and its culture is very\
          \ different than Arabic countries (by far) .Secular, latin alphabet , no\
          \ islamic rule , totally free and governed by law. And far more democratic\
          \ than most of the western countries however which is not enough for citizen\
          \ thats why people find it anti-democratic (it is not relatively). </p>\n\
          <p>So I ve been giving a try to fine tune it with %15 of Stanford-alpaca\
          \ instruction set translated to Turkish. It seems promising.  Would it differ\
          \ to fine tune it afterwards using QLORA orther than pretrain it ?<br>I\
          \ am using instruction based json dataset. Would it be logical to give simple\
          \ text such as wikipedia in Turkish, before giving instruction based data\
          \ ? </p>\n<p>Btw it is going like this: </p>\n<p>Saving model checkpoint\
          \ to ./falcon-40b-instruct-4bit-alpaca/checkpoint-5300<br>Trainer.model\
          \ is not a <code>PreTrainedModel</code>, only saving its state dict.<br>Deleting\
          \ older checkpoint [falcon-40b-instruct-4bit-alpaca/checkpoint-5150] due\
          \ to args.save_total_limit<br>{'loss': 0.7811, 'learning_rate': 9.39177797950979e-05,\
          \ 'epoch': 2.06}<br>{'loss': 0.9781, 'learning_rate': 9.372325249643366e-05,\
          \ 'epoch': 2.06}<br>{'loss': 0.9802, 'learning_rate': 9.35287251977694e-05,\
          \ 'epoch': 2.07}<br>{'loss': 0.7647, 'learning_rate': 9.333419789910517e-05,\
          \ 'epoch': 2.07}<br>{'loss': 0.8621, 'learning_rate': 9.313967060044092e-05,\
          \ 'epoch': 2.07}<br>{'loss': 1.0175, 'learning_rate': 9.294514330177668e-05,\
          \ 'epoch': 2.07}<br>{'loss': 0.8003, 'learning_rate': 9.275061600311242e-05,\
          \ 'epoch': 2.07}<br>{'loss': 0.9179, 'learning_rate': 9.255608870444818e-05,\
          \ 'epoch': 2.08}<br>{'loss': 0.9157, 'learning_rate': 9.236156140578393e-05,\
          \ 'epoch': 2.08}<br>{'loss': 0.9958, 'learning_rate': 9.216703410711969e-05,\
          \ 'epoch': 2.08}<br> 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588 ...</p>\n<p>Questions reminder\
          \ :  Pretrain vs QLORA  ?  First Simple Text than Instruction based using\
          \ QLORA ? </p>\n"
        raw: "First of all thank you very much for this model. \nTurkish is a European\
          \ language with latin alphabet, Turkey and its culture is very different\
          \ than Arabic countries (by far) .Secular, latin alphabet , no islamic rule\
          \ , totally free and governed by law. And far more democratic than most\
          \ of the western countries however which is not enough for citizen thats\
          \ why people find it anti-democratic (it is not relatively). \n\nSo I ve\
          \ been giving a try to fine tune it with %15 of Stanford-alpaca instruction\
          \ set translated to Turkish. It seems promising.  Would it differ to fine\
          \ tune it afterwards using QLORA orther than pretrain it ? \nI am using\
          \ instruction based json dataset. Would it be logical to give simple text\
          \ such as wikipedia in Turkish, before giving instruction based data ? \n\
          \nBtw it is going like this: \n\nSaving model checkpoint to ./falcon-40b-instruct-4bit-alpaca/checkpoint-5300\n\
          Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n\
          Deleting older checkpoint [falcon-40b-instruct-4bit-alpaca/checkpoint-5150]\
          \ due to args.save_total_limit\n{'loss': 0.7811, 'learning_rate': 9.39177797950979e-05,\
          \ 'epoch': 2.06}                                                       \
          \                                                                      \
          \                                                                      \
          \           \n{'loss': 0.9781, 'learning_rate': 9.372325249643366e-05, 'epoch':\
          \ 2.06}                                                                \
          \                                                                      \
          \                                                                      \
          \ \n{'loss': 0.9802, 'learning_rate': 9.35287251977694e-05, 'epoch': 2.07}\
          \                                                                      \
          \                                                                      \
          \                                                                  \n{'loss':\
          \ 0.7647, 'learning_rate': 9.333419789910517e-05, 'epoch': 2.07}       \
          \                                                                      \
          \                                                                      \
          \                                                          \n{'loss': 0.8621,\
          \ 'learning_rate': 9.313967060044092e-05, 'epoch': 2.07}               \
          \                                                                      \
          \                                                                      \
          \                                                  \n{'loss': 1.0175, 'learning_rate':\
          \ 9.294514330177668e-05, 'epoch': 2.07}                                \
          \                                                                      \
          \                                                                      \
          \                                 \n{'loss': 0.8003, 'learning_rate': 9.275061600311242e-05,\
          \ 'epoch': 2.07}                                                       \
          \                                                                      \
          \                                                                      \
          \          \n{'loss': 0.9179, 'learning_rate': 9.255608870444818e-05, 'epoch':\
          \ 2.08}                                                                \
          \                                                                      \
          \                                                                      \
          \ \n{'loss': 0.9157, 'learning_rate': 9.236156140578393e-05, 'epoch': 2.08}\
          \                                                                      \
          \                                                                      \
          \                                                                 \n{'loss':\
          \ 0.9958, 'learning_rate': 9.216703410711969e-05, 'epoch': 2.08}       \
          \                                                                      \
          \                                                                      \
          \                                                          \n 69%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588 ...\n\nQuestions reminder :  Pretrain vs QLORA  ?  First Simple\
          \ Text than Instruction based using QLORA ? "
        updatedAt: '2023-07-03T20:14:24.061Z'
      numEdits: 1
      reactions: []
    id: 64a32c08d9dd1da350835b4d
    type: comment
  author: TeaCult
  content: "First of all thank you very much for this model. \nTurkish is a European\
    \ language with latin alphabet, Turkey and its culture is very different than\
    \ Arabic countries (by far) .Secular, latin alphabet , no islamic rule , totally\
    \ free and governed by law. And far more democratic than most of the western countries\
    \ however which is not enough for citizen thats why people find it anti-democratic\
    \ (it is not relatively). \n\nSo I ve been giving a try to fine tune it with %15\
    \ of Stanford-alpaca instruction set translated to Turkish. It seems promising.\
    \  Would it differ to fine tune it afterwards using QLORA orther than pretrain\
    \ it ? \nI am using instruction based json dataset. Would it be logical to give\
    \ simple text such as wikipedia in Turkish, before giving instruction based data\
    \ ? \n\nBtw it is going like this: \n\nSaving model checkpoint to ./falcon-40b-instruct-4bit-alpaca/checkpoint-5300\n\
    Trainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting\
    \ older checkpoint [falcon-40b-instruct-4bit-alpaca/checkpoint-5150] due to args.save_total_limit\n\
    {'loss': 0.7811, 'learning_rate': 9.39177797950979e-05, 'epoch': 2.06}       \
    \                                                                            \
    \                                                                            \
    \                                               \n{'loss': 0.9781, 'learning_rate':\
    \ 9.372325249643366e-05, 'epoch': 2.06}                                      \
    \                                                                            \
    \                                                                            \
    \               \n{'loss': 0.9802, 'learning_rate': 9.35287251977694e-05, 'epoch':\
    \ 2.07}                                                                      \
    \                                                                            \
    \                                                            \n{'loss': 0.7647,\
    \ 'learning_rate': 9.333419789910517e-05, 'epoch': 2.07}                     \
    \                                                                            \
    \                                                                            \
    \                                \n{'loss': 0.8621, 'learning_rate': 9.313967060044092e-05,\
    \ 'epoch': 2.07}                                                             \
    \                                                                            \
    \                                                                    \n{'loss':\
    \ 1.0175, 'learning_rate': 9.294514330177668e-05, 'epoch': 2.07}             \
    \                                                                            \
    \                                                                            \
    \                                        \n{'loss': 0.8003, 'learning_rate': 9.275061600311242e-05,\
    \ 'epoch': 2.07}                                                             \
    \                                                                            \
    \                                                                    \n{'loss':\
    \ 0.9179, 'learning_rate': 9.255608870444818e-05, 'epoch': 2.08}             \
    \                                                                            \
    \                                                                            \
    \                                        \n{'loss': 0.9157, 'learning_rate': 9.236156140578393e-05,\
    \ 'epoch': 2.08}                                                             \
    \                                                                            \
    \                                                                    \n{'loss':\
    \ 0.9958, 'learning_rate': 9.216703410711969e-05, 'epoch': 2.08}             \
    \                                                                            \
    \                                                                            \
    \                                        \n 69%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 ...\n\nQuestions\
    \ reminder :  Pretrain vs QLORA  ?  First Simple Text than Instruction based using\
    \ QLORA ? "
  created_at: 2023-07-03 19:14:00+00:00
  edited: true
  hidden: false
  id: 64a32c08d9dd1da350835b4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
      fullname: "Gediz G\xDCRSU"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TeaCult
      type: user
    createdAt: '2023-07-06T08:46:16.000Z'
    data:
      edited: false
      editors:
      - TeaCult
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9333019256591797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
          fullname: "Gediz G\xDCRSU"
          isHf: false
          isPro: false
          name: TeaCult
          type: user
        html: '<p>I have finished that. Results are promising entire Stanford alpaca
          dataset should take a day using A100 40GB with falcon 40B.</p>

          '
        raw: 'I have finished that. Results are promising entire Stanford alpaca dataset
          should take a day using A100 40GB with falcon 40B.

          '
        updatedAt: '2023-07-06T08:46:16.464Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kemalcankara
    id: 64a67f5832cf858d638cb9f9
    type: comment
  author: TeaCult
  content: 'I have finished that. Results are promising entire Stanford alpaca dataset
    should take a day using A100 40GB with falcon 40B.

    '
  created_at: 2023-07-06 07:46:16+00:00
  edited: false
  hidden: false
  id: 64a67f5832cf858d638cb9f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594886096000-noauth.jpeg?w=200&h=200&f=face
      fullname: Kemal Can Kara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kemalcankara
      type: user
    createdAt: '2023-07-06T09:21:38.000Z'
    data:
      edited: false
      editors:
      - kemalcankara
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8178631663322449
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594886096000-noauth.jpeg?w=200&h=200&f=face
          fullname: Kemal Can Kara
          isHf: false
          isPro: false
          name: kemalcankara
          type: user
        html: '<p>Great job, can we try it somewhere if you do the entire dataset?</p>

          '
        raw: Great job, can we try it somewhere if you do the entire dataset?
        updatedAt: '2023-07-06T09:21:38.873Z'
      numEdits: 0
      reactions: []
    id: 64a687a29f3b568c203df8fa
    type: comment
  author: kemalcankara
  content: Great job, can we try it somewhere if you do the entire dataset?
  created_at: 2023-07-06 08:21:38+00:00
  edited: false
  hidden: false
  id: 64a687a29f3b568c203df8fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
      fullname: "Gediz G\xDCRSU"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TeaCult
      type: user
    createdAt: '2023-07-27T14:35:59.000Z'
    data:
      edited: true
      editors:
      - TeaCult
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9316230416297913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
          fullname: "Gediz G\xDCRSU"
          isHf: false
          isPro: false
          name: TeaCult
          type: user
        html: '<p>I dont plan to train entire dataset. However it would be very wise
          to make the model generate most probable answers to instructions (top_p
          top_k temperature) and then using gpt3.5-turbo api to translate them turkish
          and feeding them into model. Such very smaller dataset gave a lot better
          output than standford aplaca. Only 4k instructions exceeded my preivious
          12k Stf-Alpc finetuning. Answered coherently to many questions. So If I
          try this again, I will try this that way with 20K instruction or so. Then
          I will share.</p>

          '
        raw: I dont plan to train entire dataset. However it would be very wise to
          make the model generate most probable answers to instructions (top_p top_k
          temperature) and then using gpt3.5-turbo api to translate them turkish and
          feeding them into model. Such very smaller dataset gave a lot better output
          than standford aplaca. Only 4k instructions exceeded my preivious 12k Stf-Alpc
          finetuning. Answered coherently to many questions. So If I try this again,
          I will try this that way with 20K instruction or so. Then I will share.
        updatedAt: '2023-07-27T14:36:51.265Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - potemkin
    id: 64c280cf30deccec8e4e9dbc
    type: comment
  author: TeaCult
  content: I dont plan to train entire dataset. However it would be very wise to make
    the model generate most probable answers to instructions (top_p top_k temperature)
    and then using gpt3.5-turbo api to translate them turkish and feeding them into
    model. Such very smaller dataset gave a lot better output than standford aplaca.
    Only 4k instructions exceeded my preivious 12k Stf-Alpc finetuning. Answered coherently
    to many questions. So If I try this again, I will try this that way with 20K instruction
    or so. Then I will share.
  created_at: 2023-07-27 13:35:59+00:00
  edited: true
  hidden: false
  id: 64c280cf30deccec8e4e9dbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0cdd39f76ada568b0c8529226afe1a0.svg
      fullname: Sanjib Narzary
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alayaran
      type: user
    createdAt: '2023-09-22T10:07:30.000Z'
    data:
      edited: false
      editors:
      - alayaran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9071468710899353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0cdd39f76ada568b0c8529226afe1a0.svg
          fullname: Sanjib Narzary
          isHf: false
          isPro: false
          name: alayaran
          type: user
        html: '<p>Bard says yes via fine tuning. Can LLMs be fine-tuned to add new
          languages?</p>

          '
        raw: Bard says yes via fine tuning. Can LLMs be fine-tuned to add new languages?
        updatedAt: '2023-09-22T10:07:30.383Z'
      numEdits: 0
      reactions: []
    id: 650d676223e8028a896d8dfc
    type: comment
  author: alayaran
  content: Bard says yes via fine tuning. Can LLMs be fine-tuned to add new languages?
  created_at: 2023-09-22 09:07:30+00:00
  edited: false
  hidden: false
  id: 650d676223e8028a896d8dfc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: Additional Languages - Turkish
