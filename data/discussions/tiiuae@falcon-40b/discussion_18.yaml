!!python/object:huggingface_hub.community.DiscussionWithDetails
author: airtable
conflicting_files: null
created_at: 2023-05-30 08:46:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-05-30T09:46:20.000Z'
    data:
      edited: false
      editors:
      - airtable
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
          fullname: Air Table
          isHf: false
          isPro: false
          name: airtable
          type: user
        html: "<p>Even in <code>load_8_bit=True</code> setting, the model doesn't\
          \ load on the GPU, how to load it for inference? <span data-props=\"{&quot;user&quot;:&quot;FalconLLM&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/FalconLLM\"\
          >@<span class=\"underline\">FalconLLM</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Even in `load_8_bit=True` setting, the model doesn''t load on the GPU,
          how to load it for inference? @FalconLLM '
        updatedAt: '2023-05-30T09:46:20.445Z'
      numEdits: 0
      reactions: []
    id: 6475c5ecf60449361e55cf6c
    type: comment
  author: airtable
  content: 'Even in `load_8_bit=True` setting, the model doesn''t load on the GPU,
    how to load it for inference? @FalconLLM '
  created_at: 2023-05-30 08:46:20+00:00
  edited: false
  hidden: false
  id: 6475c5ecf60449361e55cf6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
      fullname: Daniel Hesslow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DanielHesslow
      type: user
    createdAt: '2023-05-30T10:07:39.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
          fullname: Daniel Hesslow
          isHf: false
          isPro: false
          name: DanielHesslow
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-30T10:42:11.583Z'
      numEdits: 0
      reactions: []
    id: 6475caebc894b5c9cf72d74e
    type: comment
  author: DanielHesslow
  content: This comment has been hidden
  created_at: 2023-05-30 09:07:39+00:00
  edited: true
  hidden: true
  id: 6475caebc894b5c9cf72d74e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T10:42:25.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Unfortunately, I do not currently have access to an H100, so it
          will be hard to debug issues there specifically. Some people do seem to
          be able to run on H100: <a rel="nofollow" href="https://www.youtube.com/watch?v=iEuf1PrmZ0Q">https://www.youtube.com/watch?v=iEuf1PrmZ0Q</a>,
          maybe seeing what they do might be of some help?</p>

          <p>80GB is going to be very tight though, so will require some cpu offloading
          with accelerate. If I understand things correctly accelerate is able to
          automatically offload to cpu memory, but I am not too familiar with this
          process.</p>

          <p>The smallest we''ve run it on is 4xA10(4x24GB=96GB).<br>Sorry to not
          be of more help, hopefully some other people that has managed to make it
          run can chime in</p>

          '
        raw: 'Unfortunately, I do not currently have access to an H100, so it will
          be hard to debug issues there specifically. Some people do seem to be able
          to run on H100: https://www.youtube.com/watch?v=iEuf1PrmZ0Q, maybe seeing
          what they do might be of some help?


          80GB is going to be very tight though, so will require some cpu offloading
          with accelerate. If I understand things correctly accelerate is able to
          automatically offload to cpu memory, but I am not too familiar with this
          process.


          The smallest we''ve run it on is 4xA10(4x24GB=96GB).

          Sorry to not be of more help, hopefully some other people that has managed
          to make it run can chime in'
        updatedAt: '2023-05-30T10:42:25.343Z'
      numEdits: 0
      reactions: []
    id: 6475d311e9b57ce0caa61afa
    type: comment
  author: FalconLLM
  content: 'Unfortunately, I do not currently have access to an H100, so it will be
    hard to debug issues there specifically. Some people do seem to be able to run
    on H100: https://www.youtube.com/watch?v=iEuf1PrmZ0Q, maybe seeing what they do
    might be of some help?


    80GB is going to be very tight though, so will require some cpu offloading with
    accelerate. If I understand things correctly accelerate is able to automatically
    offload to cpu memory, but I am not too familiar with this process.


    The smallest we''ve run it on is 4xA10(4x24GB=96GB).

    Sorry to not be of more help, hopefully some other people that has managed to
    make it run can chime in'
  created_at: 2023-05-30 09:42:25+00:00
  edited: false
  hidden: false
  id: 6475d311e9b57ce0caa61afa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
      fullname: Daniel Satchkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dsatch
      type: user
    createdAt: '2023-05-31T07:55:36.000Z'
    data:
      edited: false
      editors:
      - dsatch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
          fullname: Daniel Satchkov
          isHf: false
          isPro: false
          name: dsatch
          type: user
        html: '<p>I have not been able to run at all, even on massive deployment of
          240 VGPU. I used the code from the main page. It is clearly a memory issue,
          because 7B runs (but event that takes up more than 50% of VGPU on 240 GB
          setup). Any ideas, can you help?</p>

          '
        raw: I have not been able to run at all, even on massive deployment of 240
          VGPU. I used the code from the main page. It is clearly a memory issue,
          because 7B runs (but event that takes up more than 50% of VGPU on 240 GB
          setup). Any ideas, can you help?
        updatedAt: '2023-05-31T07:55:36.297Z'
      numEdits: 0
      reactions: []
    id: 6476fd7840c99df876feeb7c
    type: comment
  author: dsatch
  content: I have not been able to run at all, even on massive deployment of 240 VGPU.
    I used the code from the main page. It is clearly a memory issue, because 7B runs
    (but event that takes up more than 50% of VGPU on 240 GB setup). Any ideas, can
    you help?
  created_at: 2023-05-31 06:55:36+00:00
  edited: false
  hidden: false
  id: 6476fd7840c99df876feeb7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-31T08:03:25.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>@dstatch, Which/How many GPUs were you trying to run it on?</p>

          '
        raw: '@dstatch, Which/How many GPUs were you trying to run it on?'
        updatedAt: '2023-05-31T08:03:25.094Z'
      numEdits: 0
      reactions: []
    id: 6476ff4dbeaeebff4a511058
    type: comment
  author: FalconLLM
  content: '@dstatch, Which/How many GPUs were you trying to run it on?'
  created_at: 2023-05-31 07:03:25+00:00
  edited: false
  hidden: false
  id: 6476ff4dbeaeebff4a511058
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
      fullname: Daniel Satchkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dsatch
      type: user
    createdAt: '2023-05-31T08:08:11.000Z'
    data:
      edited: false
      editors:
      - dsatch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
          fullname: Daniel Satchkov
          isHf: false
          isPro: false
          name: dsatch
          type: user
        html: '<p>4 X 80 GB   I tried to use Runpod and Datacrunch, fails in both
          places. It seems that it is not even a VRAM issue, but in inter-GPU communication.
          Really excited about the potential of this, but as it stands even throwing
          very large resources at it does not help.</p>

          '
        raw: 4 X 80 GB   I tried to use Runpod and Datacrunch, fails in both places.
          It seems that it is not even a VRAM issue, but in inter-GPU communication.
          Really excited about the potential of this, but as it stands even throwing
          very large resources at it does not help.
        updatedAt: '2023-05-31T08:08:11.942Z'
      numEdits: 0
      reactions: []
    id: 6477006bbeaeebff4a512b39
    type: comment
  author: dsatch
  content: 4 X 80 GB   I tried to use Runpod and Datacrunch, fails in both places.
    It seems that it is not even a VRAM issue, but in inter-GPU communication. Really
    excited about the potential of this, but as it stands even throwing very large
    resources at it does not help.
  created_at: 2023-05-31 07:08:11+00:00
  edited: false
  hidden: false
  id: 6477006bbeaeebff4a512b39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
      fullname: Daniel Satchkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dsatch
      type: user
    createdAt: '2023-05-31T08:24:26.000Z'
    data:
      edited: false
      editors:
      - dsatch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/708a308b61fb874eb3364af27ae7e9e2.svg
          fullname: Daniel Satchkov
          isHf: false
          isPro: false
          name: dsatch
          type: user
        html: '<p>but I am not sure what the issue, just positive that I am not the
          only one experiencing it, since I have tried in multiple places</p>

          '
        raw: but I am not sure what the issue, just positive that I am not the only
          one experiencing it, since I have tried in multiple places
        updatedAt: '2023-05-31T08:24:26.212Z'
      numEdits: 0
      reactions: []
    id: 6477043a40c99df876ff91e4
    type: comment
  author: dsatch
  content: but I am not sure what the issue, just positive that I am not the only
    one experiencing it, since I have tried in multiple places
  created_at: 2023-05-31 07:24:26+00:00
  edited: false
  hidden: false
  id: 6477043a40c99df876ff91e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8df088ad8b1d0d74d692379a94e0e1e2.svg
      fullname: TinkerTank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tinkertank
      type: user
    createdAt: '2023-05-31T14:01:13.000Z'
    data:
      edited: true
      editors:
      - tinkertank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8df088ad8b1d0d74d692379a94e0e1e2.svg
          fullname: TinkerTank
          isHf: false
          isPro: false
          name: tinkertank
          type: user
        html: "<p>I'm running it with the following code on A datacrunch 80G A100\
          \ (using 8bit mode).<br>Credit where credit is due, I basically lifted this\
          \ code from Sam Witteveen's excellent youtube video &amp; colab:<br><a rel=\"\
          nofollow\" href=\"https://www.youtube.com/watch?v=5M1ZpG2Zz90\">https://www.youtube.com/watch?v=5M1ZpG2Zz90</a></p>\n\
          <p>Should work on H100 as well.</p>\n<pre><code>import torch\nimport transformers\n\
          from transformers import GenerationConfig, pipeline\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n\
          import bitsandbytes as bnb\nfrom torch.cuda.amp import autocast\n\nmodel\
          \ = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model,\n        load_in_8bit=True,\n\
          \        trust_remote_code=True,\n        device_map='auto',\n        torch_dtype=torch.float16,\n\
          \        low_cpu_mem_usage=True,\n)\n\n\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\n\nwith autocast(dtype=torch.float16):\n    sequences = pipeline(\n\
          \       \"Girafatron is obsessed with giraffes, the most glorious animal\
          \ on the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n        max_length=200,\n        do_sample=True,\n\
          \        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n\
          \    )\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n<p>I'm running the following conda env.<br>(kind of a\
          \ mess, but seems to work)</p>\n<pre><code>conda create --name llm python=3.10\n\
          conda activate llm\nconda install pytorch==2.0.0 pytorch-cuda=11.8 transformers\
          \ -c pytorch -c nvidia\npip install einops accelerate\npip install -q -U\
          \ bitsandbytes\npip install -q -U git+https://github.com/huggingface/transformers.git\n\
          pip install -q -U git+https://github.com/huggingface/peft.git\npip install\
          \ -q -U git+https://github.com/huggingface/accelerate.git\npip -q install\
          \ sentencepiece Xformers einops\npip -q install langchain\n</code></pre>\n"
        raw: "I'm running it with the following code on A datacrunch 80G A100 (using\
          \ 8bit mode).\nCredit where credit is due, I basically lifted this code\
          \ from Sam Witteveen's excellent youtube video & colab:\nhttps://www.youtube.com/watch?v=5M1ZpG2Zz90\n\
          \nShould work on H100 as well.\n\n```\nimport torch\nimport transformers\n\
          from transformers import GenerationConfig, pipeline\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n\
          import bitsandbytes as bnb\nfrom torch.cuda.amp import autocast\n\nmodel\
          \ = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model,\n        load_in_8bit=True,\n\
          \        trust_remote_code=True,\n        device_map='auto',\n        torch_dtype=torch.float16,\n\
          \        low_cpu_mem_usage=True,\n)\n\n\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\n\nwith autocast(dtype=torch.float16):\n    sequences = pipeline(\n\
          \       \"Girafatron is obsessed with giraffes, the most glorious animal\
          \ on the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n        max_length=200,\n        do_sample=True,\n\
          \        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n\
          \    )\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\"\
          )\n```\n\nI'm running the following conda env.\n(kind of a mess, but seems\
          \ to work)\n```\nconda create --name llm python=3.10\nconda activate llm\n\
          conda install pytorch==2.0.0 pytorch-cuda=11.8 transformers -c pytorch -c\
          \ nvidia\npip install einops accelerate\npip install -q -U bitsandbytes\n\
          pip install -q -U git+https://github.com/huggingface/transformers.git\n\
          pip install -q -U git+https://github.com/huggingface/peft.git\npip install\
          \ -q -U git+https://github.com/huggingface/accelerate.git\npip -q install\
          \ sentencepiece Xformers einops\npip -q install langchain\n```"
        updatedAt: '2023-05-31T15:13:48.284Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Thytu
    id: 64775329bb7681ad6704ef28
    type: comment
  author: tinkertank
  content: "I'm running it with the following code on A datacrunch 80G A100 (using\
    \ 8bit mode).\nCredit where credit is due, I basically lifted this code from Sam\
    \ Witteveen's excellent youtube video & colab:\nhttps://www.youtube.com/watch?v=5M1ZpG2Zz90\n\
    \nShould work on H100 as well.\n\n```\nimport torch\nimport transformers\nfrom\
    \ transformers import GenerationConfig, pipeline\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nimport bitsandbytes\
    \ as bnb\nfrom torch.cuda.amp import autocast\n\nmodel = \"tiiuae/falcon-40b\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\nmodel = AutoModelForCausalLM.from_pretrained(model,\n\
    \        load_in_8bit=True,\n        trust_remote_code=True,\n        device_map='auto',\n\
    \        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n)\n\n\n\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    device_map=\"auto\",\n)\n\nwith autocast(dtype=torch.float16):\n    sequences\
    \ = pipeline(\n       \"Girafatron is obsessed with giraffes, the most glorious\
    \ animal on the face of this Earth. Giraftron believes all other animals are irrelevant\
    \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
    nGirafatron:\",\n        max_length=200,\n        do_sample=True,\n        top_k=10,\n\
    \        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n\
    \    )\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\"\
    )\n```\n\nI'm running the following conda env.\n(kind of a mess, but seems to\
    \ work)\n```\nconda create --name llm python=3.10\nconda activate llm\nconda install\
    \ pytorch==2.0.0 pytorch-cuda=11.8 transformers -c pytorch -c nvidia\npip install\
    \ einops accelerate\npip install -q -U bitsandbytes\npip install -q -U git+https://github.com/huggingface/transformers.git\n\
    pip install -q -U git+https://github.com/huggingface/peft.git\npip install -q\
    \ -U git+https://github.com/huggingface/accelerate.git\npip -q install sentencepiece\
    \ Xformers einops\npip -q install langchain\n```"
  created_at: 2023-05-31 13:01:13+00:00
  edited: true
  hidden: false
  id: 64775329bb7681ad6704ef28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b43aa1e506b3fb76cefec800c496231.svg
      fullname: Ian Nai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cian0
      type: user
    createdAt: '2023-06-01T02:28:23.000Z'
    data:
      edited: false
      editors:
      - cian0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b43aa1e506b3fb76cefec800c496231.svg
          fullname: Ian Nai
          isHf: false
          isPro: false
          name: cian0
          type: user
        html: '<p>Would this run with 5 12GB VRAM (3060) gpus? I run a mining rig
          at home..</p>

          '
        raw: Would this run with 5 12GB VRAM (3060) gpus? I run a mining rig at home..
        updatedAt: '2023-06-01T02:28:23.385Z'
      numEdits: 0
      reactions: []
    id: 6478024704aa03da2ac964d9
    type: comment
  author: cian0
  content: Would this run with 5 12GB VRAM (3060) gpus? I run a mining rig at home..
  created_at: 2023-06-01 01:28:23+00:00
  edited: false
  hidden: false
  id: 6478024704aa03da2ac964d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d7aae4ce4d4e02b8a6f94eadec436c4.svg
      fullname: avaer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Adrians
      type: user
    createdAt: '2023-06-01T18:29:44.000Z'
    data:
      edited: false
      editors:
      - Adrians
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d7aae4ce4d4e02b8a6f94eadec436c4.svg
          fullname: avaer
          isHf: false
          isPro: false
          name: Adrians
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;tinkertank&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tinkertank\"\
          >@<span class=\"underline\">tinkertank</span></a></span>\n\n\t</span></span>!\
          \ I tried your install + run on H100 (lamnbda labs) but I'm getting cublas\
          \ errors...</p>\n<pre><code>File \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
          , line 388, in forward\n    out = bnb.matmul(x, self.weight, bias=self.bias,\
          \ state=self.state)\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 559, in matmul\n    return MatMul8bitLt.apply(A, B, out, bias, state)\n\
          \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/function.py\"\
          , line 506, in apply\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 397, in forward\n    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA,\
          \ state.SB)\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/functional.py\"\
          , line 1781, in igemmlt\n    raise Exception('cublasLt ran into an error!')\n\
          Exception: cublasLt ran into an error!\n</code></pre>\n<p>Any ideas?</p>\n"
        raw: "Thanks @tinkertank! I tried your install + run on H100 (lamnbda labs)\
          \ but I'm getting cublas errors...\n\n```\nFile \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py\"\
          , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
          , line 388, in forward\n    out = bnb.matmul(x, self.weight, bias=self.bias,\
          \ state=self.state)\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 559, in matmul\n    return MatMul8bitLt.apply(A, B, out, bias, state)\n\
          \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/function.py\"\
          , line 506, in apply\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 397, in forward\n    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA,\
          \ state.SB)\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/functional.py\"\
          , line 1781, in igemmlt\n    raise Exception('cublasLt ran into an error!')\n\
          Exception: cublasLt ran into an error!\n```\n\nAny ideas?"
        updatedAt: '2023-06-01T18:29:44.758Z'
      numEdits: 0
      reactions: []
    id: 6478e398c68a021fbba24867
    type: comment
  author: Adrians
  content: "Thanks @tinkertank! I tried your install + run on H100 (lamnbda labs)\
    \ but I'm getting cublas errors...\n\n```\nFile \"/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py\"\
    , line 252, in forward\n    fused_qkv = self.query_key_value(hidden_states)  #\
    \ [batch_size, seq_length, 3 x hidden_size]\n  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\"\
    , line 388, in forward\n    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n\
    \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
    , line 559, in matmul\n    return MatMul8bitLt.apply(A, B, out, bias, state)\n\
    \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/function.py\"\
    , line 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
    \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\"\
    , line 397, in forward\n    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)\n\
    \  File \"/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/functional.py\"\
    , line 1781, in igemmlt\n    raise Exception('cublasLt ran into an error!')\n\
    Exception: cublasLt ran into an error!\n```\n\nAny ideas?"
  created_at: 2023-06-01 17:29:44+00:00
  edited: false
  hidden: false
  id: 6478e398c68a021fbba24867
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
      fullname: Nate Raw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nateraw
      type: user
    createdAt: '2023-06-02T00:37:22.000Z'
    data:
      edited: true
      editors:
      - nateraw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
          fullname: Nate Raw
          isHf: false
          isPro: false
          name: nateraw
          type: user
        html: "<p>Also was getting error <span data-props=\"{&quot;user&quot;:&quot;Adrians&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Adrians\"\
          >@<span class=\"underline\">Adrians</span></a></span>\n\n\t</span></span>\
          \ was getting. Looked to me like some issue in 8-bit, probably because some\
          \ wrong operation is being called. So, I skipped it, and the below worked\
          \ for me on H100 from Lambda.</p>\n<p>Just checked, and the below worked\
          \ on a fresh instance (I ran no other commands).</p>\n<h2 id=\"install-miniconda\"\
          >Install miniconda</h2>\n<p>We only do this because the install for torch/cuda\
          \ works smoothly. </p>\n<pre><code class=\"language-bash\"><span class=\"\
          hljs-comment\"># Download latest miniconda.</span>\nwget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
          \n<span class=\"hljs-comment\"># Install. -b is used to skip prompt</span>\n\
          bash Miniconda3-latest-Linux-x86_64.sh -b\n\n<span class=\"hljs-comment\"\
          ># Activate.</span>\n<span class=\"hljs-built_in\">eval</span> <span class=\"\
          hljs-string\">\"<span class=\"hljs-subst\">$(/home/ubuntu/miniconda3/bin/conda\
          \ shell.bash hook)</span>\"</span>\n\n<span class=\"hljs-comment\"># (optional)\
          \ Add activation cmd to bashrc so you don't have to run the above every\
          \ time.</span>\n<span class=\"hljs-built_in\">printf</span> <span class=\"\
          hljs-string\">'\\neval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
          \ hook)\"'</span> &gt;&gt; ~/.bashrc\n</code></pre>\n<h2 id=\"setup-env\"\
          >Setup env</h2>\n<p>Note: I don't think you <em>need</em> to install transformers\
          \ from github if you do <code>device_map={\"\": 0}</code> later instead\
          \ of <code>device_map=0</code>, but I haven't checked.</p>\n<pre><code class=\"\
          language-bash\"><span class=\"hljs-comment\"># Create and activate env.\
          \ -y skips confirmation prompt.</span>\nconda create -n falcon-env python=3.9\
          \ -y\nconda activate falcon-env\n\n<span class=\"hljs-comment\"># newest\
          \ torch with cuda 11.8</span>\nconda install pytorch torchvision torchaudio\
          \ pytorch-cuda=11.8 -c pytorch -c nvidia\n\n<span class=\"hljs-comment\"\
          ># For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce</span>\n\
          pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git\n\
          </code></pre>\n<h2 id=\"run-it\">Run it</h2>\n<p>This will use up basically\
          \ all the memory, but it works.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >import</span> transformers\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \n\nmodel = <span class=\"hljs-string\">\"tiiuae/falcon-40b\"</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\nmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>, torch_dtype=torch.bfloat16,\
          \ device_map=<span class=\"hljs-number\">0</span>)\npipeline = transformers.pipeline(\n\
          \    <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    device_map=<span class=\"hljs-number\"\
          >0</span>,\n)\nsequences = pipeline(\n    <span class=\"hljs-string\">\"\
          To make the perfect chocolate chip cookies,\"</span>,\n    max_length=<span\
          \ class=\"hljs-number\">200</span>,\n    do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n    top_k=<span class=\"hljs-number\">10</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    pad_token_id=tokenizer.eos_token_id,\n\
          )\n<span class=\"hljs-keyword\">for</span> seq <span class=\"hljs-keyword\"\
          >in</span> sequences:\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Result: <span class=\"hljs-subst\">{seq[<span\
          \ class=\"hljs-string\">'generated_text'</span>]}</span>\"</span>)\n</code></pre>\n"
        raw: "Also was getting error @Adrians was getting. Looked to me like some\
          \ issue in 8-bit, probably because some wrong operation is being called.\
          \ So, I skipped it, and the below worked for me on H100 from Lambda.\n\n\
          Just checked, and the below worked on a fresh instance (I ran no other commands).\n\
          \n## Install miniconda\n\nWe only do this because the install for torch/cuda\
          \ works smoothly. \n\n```bash\n# Download latest miniconda.\nwget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
          \n# Install. -b is used to skip prompt\nbash Miniconda3-latest-Linux-x86_64.sh\
          \ -b\n\n# Activate.\neval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
          \ hook)\"\n\n# (optional) Add activation cmd to bashrc so you don't have\
          \ to run the above every time.\nprintf '\\neval \"$(/home/ubuntu/miniconda3/bin/conda\
          \ shell.bash hook)\"' >> ~/.bashrc\n```\n\n## Setup env\n\nNote: I don't\
          \ think you *need* to install transformers from github if you do `device_map={\"\
          \": 0}` later instead of `device_map=0`, but I haven't checked.\n\n```bash\n\
          # Create and activate env. -y skips confirmation prompt.\nconda create -n\
          \ falcon-env python=3.9 -y\nconda activate falcon-env\n\n# newest torch\
          \ with cuda 11.8\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8\
          \ -c pytorch -c nvidia\n\n# For transformers, the commit I installed was\
          \ f49a3453caa6fe606bb31c571423f72264152fce\npip install -U accelerate einops\
          \ sentencepiece git+https://github.com/huggingface/transformers.git\n```\n\
          \n## Run it\n\nThis will use up basically all the memory, but it works.\n\
          \n```python\nimport torch\nimport transformers\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\n\n\nmodel = \"tiiuae/falcon-40b\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\nmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=0)\npipeline\
          \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=0,\n)\nsequences = pipeline(\n    \"To make the perfect\
          \ chocolate chip cookies,\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    pad_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```"
        updatedAt: '2023-06-02T01:17:40.037Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - DanielHesslow
        - maxdasein
        - rantlab
        - ylove
    id: 647939c2c68a021fbba88182
    type: comment
  author: nateraw
  content: "Also was getting error @Adrians was getting. Looked to me like some issue\
    \ in 8-bit, probably because some wrong operation is being called. So, I skipped\
    \ it, and the below worked for me on H100 from Lambda.\n\nJust checked, and the\
    \ below worked on a fresh instance (I ran no other commands).\n\n## Install miniconda\n\
    \nWe only do this because the install for torch/cuda works smoothly. \n\n```bash\n\
    # Download latest miniconda.\nwget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    \n# Install. -b is used to skip prompt\nbash Miniconda3-latest-Linux-x86_64.sh\
    \ -b\n\n# Activate.\neval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash hook)\"\
    \n\n# (optional) Add activation cmd to bashrc so you don't have to run the above\
    \ every time.\nprintf '\\neval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
    \ hook)\"' >> ~/.bashrc\n```\n\n## Setup env\n\nNote: I don't think you *need*\
    \ to install transformers from github if you do `device_map={\"\": 0}` later instead\
    \ of `device_map=0`, but I haven't checked.\n\n```bash\n# Create and activate\
    \ env. -y skips confirmation prompt.\nconda create -n falcon-env python=3.9 -y\n\
    conda activate falcon-env\n\n# newest torch with cuda 11.8\nconda install pytorch\
    \ torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\n# For transformers,\
    \ the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce\npip install\
    \ -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git\n\
    ```\n\n## Run it\n\nThis will use up basically all the memory, but it works.\n\
    \n```python\nimport torch\nimport transformers\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\n\nmodel = \"tiiuae/falcon-40b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True, torch_dtype=torch.bfloat16,\
    \ device_map=0)\npipeline = transformers.pipeline(\n    \"text-generation\",\n\
    \    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=0,\n)\nsequences = pipeline(\n  \
    \  \"To make the perfect chocolate chip cookies,\",\n    max_length=200,\n   \
    \ do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    pad_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n```"
  created_at: 2023-06-01 23:37:22+00:00
  edited: true
  hidden: false
  id: 647939c2c68a021fbba88182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-06-09T10:56:32.000Z'
    data:
      edited: false
      editors:
      - airtable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8968032598495483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
          fullname: Air Table
          isHf: false
          isPro: false
          name: airtable
          type: user
        html: "<p>Hi guys</p>\n<p>I am back, <a href=\"https://huggingface.co/tiiuae/falcon-40b/discussions/18#647939c2c68a021fbba88182\"\
          >the code</a> from <span data-props=\"{&quot;user&quot;:&quot;nateraw&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nateraw\"\
          >@<span class=\"underline\">nateraw</span></a></span>\n\n\t</span></span>\
          \ worked on my Lambda H100 instance, only needed to upgrade Transformers\
          \ to 4.30.0 from 4.29.2, without that it was giving a <code>device_map</code>\
          \ <code>int type doesn't have .values()</code> error and took me a while\
          \ to figure out.</p>\n<p>But looks the model tightly fits, here's the GPU\
          \ usage at 99.1%</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6466d68a14e059dde8bbab65/_AAZsfFUPljPaUolS2CJR.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6466d68a14e059dde8bbab65/_AAZsfFUPljPaUolS2CJR.png\"\
          ></a></p>\n<p>Next up, load in <code>langchain</code></p>\n"
        raw: 'Hi guys


          I am back, [the code](https://huggingface.co/tiiuae/falcon-40b/discussions/18#647939c2c68a021fbba88182)
          from @nateraw worked on my Lambda H100 instance, only needed to upgrade
          Transformers to 4.30.0 from 4.29.2, without that it was giving a `device_map`
          `int type doesn''t have .values()` error and took me a while to figure out.


          But looks the model tightly fits, here''s the GPU usage at 99.1%


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6466d68a14e059dde8bbab65/_AAZsfFUPljPaUolS2CJR.png)


          Next up, load in `langchain`

          '
        updatedAt: '2023-06-09T10:56:32.982Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6483056104f67f5f60607aa1
    id: 6483056004f67f5f60607a9f
    type: comment
  author: airtable
  content: 'Hi guys


    I am back, [the code](https://huggingface.co/tiiuae/falcon-40b/discussions/18#647939c2c68a021fbba88182)
    from @nateraw worked on my Lambda H100 instance, only needed to upgrade Transformers
    to 4.30.0 from 4.29.2, without that it was giving a `device_map` `int type doesn''t
    have .values()` error and took me a while to figure out.


    But looks the model tightly fits, here''s the GPU usage at 99.1%


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6466d68a14e059dde8bbab65/_AAZsfFUPljPaUolS2CJR.png)


    Next up, load in `langchain`

    '
  created_at: 2023-06-09 09:56:32+00:00
  edited: false
  hidden: false
  id: 6483056004f67f5f60607a9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-06-09T10:56:33.000Z'
    data:
      status: closed
    id: 6483056104f67f5f60607aa1
    type: status-change
  author: airtable
  created_at: 2023-06-09 09:56:33+00:00
  id: 6483056104f67f5f60607aa1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb57b55883f314fab9d6be9d8a3159b0.svg
      fullname: Vinay Premchandran Nair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinwizard
      type: user
    createdAt: '2023-07-06T14:01:25.000Z'
    data:
      edited: false
      editors:
      - vinwizard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7325847148895264
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb57b55883f314fab9d6be9d8a3159b0.svg
          fullname: Vinay Premchandran Nair
          isHf: false
          isPro: false
          name: vinwizard
          type: user
        html: "<blockquote>\n<p>Also was getting error <span data-props=\"{&quot;user&quot;:&quot;Adrians&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Adrians\"\
          >@<span class=\"underline\">Adrians</span></a></span>\n\n\t</span></span>\
          \ was getting. Looked to me like some issue in 8-bit, probably because some\
          \ wrong operation is being called. So, I skipped it, and the below worked\
          \ for me on H100 from Lambda.</p>\n<p>Just checked, and the below worked\
          \ on a fresh instance (I ran no other commands).</p>\n<h2 id=\"install-miniconda\"\
          >Install miniconda</h2>\n<p>We only do this because the install for torch/cuda\
          \ works smoothly. </p>\n<pre><code class=\"language-bash\"><span class=\"\
          hljs-comment\"># Download latest miniconda.</span>\nwget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
          \n<span class=\"hljs-comment\"># Install. -b is used to skip prompt</span>\n\
          bash Miniconda3-latest-Linux-x86_64.sh -b\n\n<span class=\"hljs-comment\"\
          ># Activate.</span>\n<span class=\"hljs-built_in\">eval</span> <span class=\"\
          hljs-string\">\"<span class=\"hljs-subst\">$(/home/ubuntu/miniconda3/bin/conda\
          \ shell.bash hook)</span>\"</span>\n\n<span class=\"hljs-comment\"># (optional)\
          \ Add activation cmd to bashrc so you don't have to run the above every\
          \ time.</span>\n<span class=\"hljs-built_in\">printf</span> <span class=\"\
          hljs-string\">'\\neval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
          \ hook)\"'</span> &gt;&gt; ~/.bashrc\n</code></pre>\n<h2 id=\"setup-env\"\
          >Setup env</h2>\n<p>Note: I don't think you <em>need</em> to install transformers\
          \ from github if you do <code>device_map={\"\": 0}</code> later instead\
          \ of <code>device_map=0</code>, but I haven't checked.</p>\n<pre><code class=\"\
          language-bash\"><span class=\"hljs-comment\"># Create and activate env.\
          \ -y skips confirmation prompt.</span>\nconda create -n falcon-env python=3.9\
          \ -y\nconda activate falcon-env\n\n<span class=\"hljs-comment\"># newest\
          \ torch with cuda 11.8</span>\nconda install pytorch torchvision torchaudio\
          \ pytorch-cuda=11.8 -c pytorch -c nvidia\n\n<span class=\"hljs-comment\"\
          ># For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce</span>\n\
          pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git\n\
          </code></pre>\n<h2 id=\"run-it\">Run it</h2>\n<p>This will use up basically\
          \ all the memory, but it works.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >import</span> transformers\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \n\nmodel = <span class=\"hljs-string\">\"tiiuae/falcon-40b\"</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\nmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>, torch_dtype=torch.bfloat16,\
          \ device_map=<span class=\"hljs-number\">0</span>)\npipeline = transformers.pipeline(\n\
          \    <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    device_map=<span class=\"hljs-number\"\
          >0</span>,\n)\nsequences = pipeline(\n    <span class=\"hljs-string\">\"\
          To make the perfect chocolate chip cookies,\"</span>,\n    max_length=<span\
          \ class=\"hljs-number\">200</span>,\n    do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n    top_k=<span class=\"hljs-number\">10</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    pad_token_id=tokenizer.eos_token_id,\n\
          )\n<span class=\"hljs-keyword\">for</span> seq <span class=\"hljs-keyword\"\
          >in</span> sequences:\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Result: <span class=\"hljs-subst\">{seq[<span\
          \ class=\"hljs-string\">'generated_text'</span>]}</span>\"</span>)\n</code></pre>\n\
          </blockquote>\n<p>How much was the inference time on this? <span data-props=\"\
          {&quot;user&quot;:&quot;nateraw&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/nateraw\">@<span class=\"underline\">nateraw</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: "> Also was getting error @Adrians was getting. Looked to me like some\
          \ issue in 8-bit, probably because some wrong operation is being called.\
          \ So, I skipped it, and the below worked for me on H100 from Lambda.\n>\
          \ \n> Just checked, and the below worked on a fresh instance (I ran no other\
          \ commands).\n> \n> ## Install miniconda\n> \n> We only do this because\
          \ the install for torch/cuda works smoothly. \n> \n> ```bash\n> # Download\
          \ latest miniconda.\n> wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
          > \n> # Install. -b is used to skip prompt\n> bash Miniconda3-latest-Linux-x86_64.sh\
          \ -b\n> \n> # Activate.\n> eval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
          \ hook)\"\n> \n> # (optional) Add activation cmd to bashrc so you don't\
          \ have to run the above every time.\n> printf '\\neval \"$(/home/ubuntu/miniconda3/bin/conda\
          \ shell.bash hook)\"' >> ~/.bashrc\n> ```\n> \n> ## Setup env\n> \n> Note:\
          \ I don't think you *need* to install transformers from github if you do\
          \ `device_map={\"\": 0}` later instead of `device_map=0`, but I haven't\
          \ checked.\n> \n> ```bash\n> # Create and activate env. -y skips confirmation\
          \ prompt.\n> conda create -n falcon-env python=3.9 -y\n> conda activate\
          \ falcon-env\n> \n> # newest torch with cuda 11.8\n> conda install pytorch\
          \ torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n> \n> #\
          \ For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce\n\
          > pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git\n\
          > ```\n> \n> ## Run it\n> \n> This will use up basically all the memory,\
          \ but it works.\n> \n> ```python\n> import torch\n> import transformers\n\
          > from transformers import AutoModelForCausalLM, AutoTokenizer\n> \n> \n\
          > model = \"tiiuae/falcon-40b\"\n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True,\
          \ torch_dtype=torch.bfloat16, device_map=0)\n> pipeline = transformers.pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n\
          >     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n>    \
          \ device_map=0,\n> )\n> sequences = pipeline(\n>     \"To make the perfect\
          \ chocolate chip cookies,\",\n>     max_length=200,\n>     do_sample=True,\n\
          >     top_k=10,\n>     num_return_sequences=1,\n>     pad_token_id=tokenizer.eos_token_id,\n\
          > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
          )\n> ```\n\nHow much was the inference time on this? @nateraw"
        updatedAt: '2023-07-06T14:01:25.621Z'
      numEdits: 0
      reactions: []
    id: 64a6c9357742a064d57e7cd4
    type: comment
  author: vinwizard
  content: "> Also was getting error @Adrians was getting. Looked to me like some\
    \ issue in 8-bit, probably because some wrong operation is being called. So, I\
    \ skipped it, and the below worked for me on H100 from Lambda.\n> \n> Just checked,\
    \ and the below worked on a fresh instance (I ran no other commands).\n> \n> ##\
    \ Install miniconda\n> \n> We only do this because the install for torch/cuda\
    \ works smoothly. \n> \n> ```bash\n> # Download latest miniconda.\n> wget -nc\
    \ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n> \n\
    > # Install. -b is used to skip prompt\n> bash Miniconda3-latest-Linux-x86_64.sh\
    \ -b\n> \n> # Activate.\n> eval \"$(/home/ubuntu/miniconda3/bin/conda shell.bash\
    \ hook)\"\n> \n> # (optional) Add activation cmd to bashrc so you don't have to\
    \ run the above every time.\n> printf '\\neval \"$(/home/ubuntu/miniconda3/bin/conda\
    \ shell.bash hook)\"' >> ~/.bashrc\n> ```\n> \n> ## Setup env\n> \n> Note: I don't\
    \ think you *need* to install transformers from github if you do `device_map={\"\
    \": 0}` later instead of `device_map=0`, but I haven't checked.\n> \n> ```bash\n\
    > # Create and activate env. -y skips confirmation prompt.\n> conda create -n\
    \ falcon-env python=3.9 -y\n> conda activate falcon-env\n> \n> # newest torch\
    \ with cuda 11.8\n> conda install pytorch torchvision torchaudio pytorch-cuda=11.8\
    \ -c pytorch -c nvidia\n> \n> # For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce\n\
    > pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git\n\
    > ```\n> \n> ## Run it\n> \n> This will use up basically all the memory, but it\
    \ works.\n> \n> ```python\n> import torch\n> import transformers\n> from transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n> \n> \n> model = \"tiiuae/falcon-40b\"\
    \n> tokenizer = AutoTokenizer.from_pretrained(model)\n> model = AutoModelForCausalLM.from_pretrained(model,\
    \ trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=0)\n> pipeline\
    \ = transformers.pipeline(\n>     \"text-generation\",\n>     model=model,\n>\
    \     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
    >     device_map=0,\n> )\n> sequences = pipeline(\n>     \"To make the perfect\
    \ chocolate chip cookies,\",\n>     max_length=200,\n>     do_sample=True,\n>\
    \     top_k=10,\n>     num_return_sequences=1,\n>     pad_token_id=tokenizer.eos_token_id,\n\
    > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
    )\n> ```\n\nHow much was the inference time on this? @nateraw"
  created_at: 2023-07-06 13:01:25+00:00
  edited: false
  hidden: false
  id: 64a6c9357742a064d57e7cd4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: How to load Falcon-40B on Nvidia H100 GPU with 80GB VRAM?
