!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yiz4869
conflicting_files: null
created_at: 2023-06-21 04:16:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8afc299016d0f04a6cebdffd8df96f62.svg
      fullname: Yi Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiz4869
      type: user
    createdAt: '2023-06-21T05:16:07.000Z'
    data:
      edited: false
      editors:
      - yiz4869
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5941541194915771
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8afc299016d0f04a6cebdffd8df96f62.svg
          fullname: Yi Zhang
          isHf: false
          isPro: false
          name: yiz4869
          type: user
        html: '<p>I trained the falcon 7b model but when I load them for inference
          I got the following error.<br>ValueError: The following model_kwargs are
          not used by the model: [''token_type_ids''] (note: typos in the generate
          arguments will also show up in this list). </p>

          <p>Based on a previous discussion (<a href="https://huggingface.co/tiiuae/falcon-40b/discussions/7">https://huggingface.co/tiiuae/falcon-40b/discussions/7</a>),
          I tried to set ''return_token_type_ids=False'' in tokenizer() as following:
          </p>

          <p>hf_predictor = HuggingFaceLLMPredictor(<br>    max_input_size=2048,<br>    max_new_tokens=256,<br>    generate_kwargs={"temperature":
          0.25, "do_sample": False},<br>    query_wrapper_prompt=query_wrapper_prompt,<br>    device_map="auto",<br>    model_name="tiiuae/falcon-7b",<br>    tokenizer="tiiuae/falcon-7b",<br>    tokenizer_kwargs={"max_length":
          2048,  "return_token_type_ids": False},<br>    model_kwargs={"torch_dtype":
          torch.bfloat16}<br>)</p>

          <p>I end up getting the same error after the changes. Any help would be
          greatly appreciated. </p>

          '
        raw: "I trained the falcon 7b model but when I load them for inference I got\
          \ the following error. \r\nValueError: The following model_kwargs are not\
          \ used by the model: ['token_type_ids'] (note: typos in the generate arguments\
          \ will also show up in this list). \r\n\r\nBased on a previous discussion\
          \ (https://huggingface.co/tiiuae/falcon-40b/discussions/7), I tried to set\
          \ 'return_token_type_ids=False' in tokenizer() as following: \r\n\r\nhf_predictor\
          \ = HuggingFaceLLMPredictor(\r\n    max_input_size=2048, \r\n    max_new_tokens=256,\r\
          \n    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\r\n\
          \    query_wrapper_prompt=query_wrapper_prompt,\r\n    device_map=\"auto\"\
          ,\r\n    model_name=\"tiiuae/falcon-7b\",\r\n    tokenizer=\"tiiuae/falcon-7b\"\
          ,\r\n    tokenizer_kwargs={\"max_length\": 2048,  \"return_token_type_ids\"\
          : False},\r\n    model_kwargs={\"torch_dtype\": torch.bfloat16}\r\n)\r\n\
          \r\nI end up getting the same error after the changes. Any help would be\
          \ greatly appreciated. \r\n\r\n"
        updatedAt: '2023-06-21T05:16:07.209Z'
      numEdits: 0
      reactions: []
    id: 64928797354ac5752a22f1ca
    type: comment
  author: yiz4869
  content: "I trained the falcon 7b model but when I load them for inference I got\
    \ the following error. \r\nValueError: The following model_kwargs are not used\
    \ by the model: ['token_type_ids'] (note: typos in the generate arguments will\
    \ also show up in this list). \r\n\r\nBased on a previous discussion (https://huggingface.co/tiiuae/falcon-40b/discussions/7),\
    \ I tried to set 'return_token_type_ids=False' in tokenizer() as following: \r\
    \n\r\nhf_predictor = HuggingFaceLLMPredictor(\r\n    max_input_size=2048, \r\n\
    \    max_new_tokens=256,\r\n    generate_kwargs={\"temperature\": 0.25, \"do_sample\"\
    : False},\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    device_map=\"\
    auto\",\r\n    model_name=\"tiiuae/falcon-7b\",\r\n    tokenizer=\"tiiuae/falcon-7b\"\
    ,\r\n    tokenizer_kwargs={\"max_length\": 2048,  \"return_token_type_ids\": False},\r\
    \n    model_kwargs={\"torch_dtype\": torch.bfloat16}\r\n)\r\n\r\nI end up getting\
    \ the same error after the changes. Any help would be greatly appreciated. \r\n\
    \r\n"
  created_at: 2023-06-21 04:16:07+00:00
  edited: false
  hidden: false
  id: 64928797354ac5752a22f1ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dbf794d060b3f7bd83f566c18710b4fe.svg
      fullname: Anthony Hughes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ahughes
      type: user
    createdAt: '2023-07-15T05:35:22.000Z'
    data:
      edited: false
      editors:
      - ahughes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.791797399520874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dbf794d060b3f7bd83f566c18710b4fe.svg
          fullname: Anthony Hughes
          isHf: false
          isPro: false
          name: ahughes
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;yiz4869&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/yiz4869\">@<span class=\"\
          underline\">yiz4869</span></a></span>\n\n\t</span></span>  I was able to\
          \ successfully finetune the model without passing in the <code>token_type_ids</code>.\
          \ If using an <code>AutoModelForCausalLM</code> model class and a <code>DataCollatorForLanguageModeling</code>\
          \ being passed to the trainer then you should be able to train successfully!\
          \ </p>\n<p>My example is available here <a rel=\"nofollow\" href=\"https://github.com/anthonyhughes/pico-evidence-training-data/blob/main/falcon_main.py\"\
          >https://github.com/anthonyhughes/pico-evidence-training-data/blob/main/falcon_main.py</a>\
          \ in the train section.</p>\n"
        raw: "@yiz4869  I was able to successfully finetune the model without passing\
          \ in the `token_type_ids`. If using an `AutoModelForCausalLM` model class\
          \ and a `DataCollatorForLanguageModeling` being passed to the trainer then\
          \ you should be able to train successfully! \n\nMy example is available\
          \ here https://github.com/anthonyhughes/pico-evidence-training-data/blob/main/falcon_main.py\
          \ in the train section."
        updatedAt: '2023-07-15T05:35:22.998Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ahughes
    id: 64b2301a1917256b410b2802
    type: comment
  author: ahughes
  content: "@yiz4869  I was able to successfully finetune the model without passing\
    \ in the `token_type_ids`. If using an `AutoModelForCausalLM` model class and\
    \ a `DataCollatorForLanguageModeling` being passed to the trainer then you should\
    \ be able to train successfully! \n\nMy example is available here https://github.com/anthonyhughes/pico-evidence-training-data/blob/main/falcon_main.py\
    \ in the train section."
  created_at: 2023-07-15 04:35:22+00:00
  edited: false
  hidden: false
  id: 64b2301a1917256b410b2802
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: 'ValueError: The following model_kwargs are not used by the model: [''token_type_ids'']
  (note: typos in the generate arguments will also show up in this list)'
