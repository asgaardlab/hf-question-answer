!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LinuxMagic
conflicting_files: null
created_at: 2023-06-08 20:13:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db8644769bf7bb5099a9f03c1f66bdcf.svg
      fullname: Michael Hugging
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LinuxMagic
      type: user
    createdAt: '2023-06-08T21:13:09.000Z'
    data:
      edited: false
      editors:
      - LinuxMagic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6513261795043945
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db8644769bf7bb5099a9f03c1f66bdcf.svg
          fullname: Michael Hugging
          isHf: false
          isPro: false
          name: LinuxMagic
          type: user
        html: '<p>Pleasantly surprised, the example ran right out of the box, albeit
          a few errors... </p>

          <p>I can start, 512GB Ram, 48GB GPU (4090), AMD ThreadRipper PRO.. </p>

          <p>Output of the example: (How to Get Started with the Model)</p>

          <p> python3 ./testFalcon.py </p>

          <p>2023-06-08 12:11:44.566332: I tensorflow/core/platform/cpu_feature_guard.cc:182]
          This TensorFlow binary is optimized to use available CPU instructions in
          performance-critical operations.<br>To enable the following instructions:
          AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler
          flags.<br>2023-06-08 12:11:45.006793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]
          TF-TRT Warning: Could not find TensorRT<br>Loading checkpoint shards: 100%|...
          | 9/9 [08:20&lt;00:00, 55.56s/it]<br>Xformers is not installed correctly.
          If you want to use memory_efficient_attention to accelerate training use
          the following command to install Xformers<br>pip install xformers.</p>

          <p>The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1255:
          UserWarning: You have modified the pretrained model configuration to control
          generation. This is a deprecated strategy to control generation and will
          be removed soon, in a future version. Please use a generation configuration
          file (see <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">https://huggingface.co/docs/transformers/main_classes/text_generation</a>)<br>  warnings.warn(<br>Setting
          <code>pad_token_id</code> to <code>eos_token_id</code>:11 for open-end generation.</p>

          <p>Result: Girafatron is obsessed with giraffes, the most glorious animal
          on the face of this Earth. Giraftron believes all other animals are irrelevant
          when compared to the glorious majesty of the giraffe.<br>Daniel: Hello,
          Girafatron!<br>Girafatron: Hey Daniel, what''s up?<br>Daniel: Well, we have
          a problem.<br>Girafatron: What''s that?<br>Daniel: Well we are out of giraffes.<br>Girafatron:
          What!? No! How is that possible!?</p>

          <p>Little suprised that only one CPU core used at a time, but.. Memory stayed
          well below available, and took 8 minutes to run.</p>

          <p>Because of the output warning, decided to add xformers, and rerun.. Of
          course it didn''t change performance much, we aren''t training .. warning,
          around the 4th iteration, Girafatron gets a little mouthy.. ;) And notice
          some clipping of responses.. </p>

          <p>Next step, let''s quantize this to 4bit... Of course, always hairy to
          set ''trust_remote_code=True'' ;)</p>

          '
        raw: "Pleasantly surprised, the example ran right out of the box, albeit a\
          \ few errors... \r\n\r\nI can start, 512GB Ram, 48GB GPU (4090), AMD ThreadRipper\
          \ PRO.. \r\n\r\nOutput of the example: (How to Get Started with the Model)\r\
          \n\r\n python3 ./testFalcon.py \r\n\r\n2023-06-08 12:11:44.566332: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
          \ This TensorFlow binary is optimized to use available CPU instructions\
          \ in performance-critical operations.\r\nTo enable the following instructions:\
          \ AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate\
          \ compiler flags.\r\n2023-06-08 12:11:45.006793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\r\nLoading checkpoint shards:\
          \ 100%|... | 9/9 [08:20<00:00, 55.56s/it]\r\nXformers is not installed correctly.\
          \ If you want to use memory_efficient_attention to accelerate training use\
          \ the following command to install Xformers\r\npip install xformers.\r\n\
          \r\nThe model 'RWForCausalLM' is not supported for text-generation. Supported\
          \ models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1255:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\r\
          \n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\r\n\r\nResult: Girafatron is obsessed with giraffes, the most\
          \ glorious animal on the face of this Earth. Giraftron believes all other\
          \ animals are irrelevant when compared to the glorious majesty of the giraffe.\r\
          \nDaniel: Hello, Girafatron!\r\nGirafatron: Hey Daniel, what's up?\r\nDaniel:\
          \ Well, we have a problem.\r\nGirafatron: What's that?\r\nDaniel: Well we\
          \ are out of giraffes.\r\nGirafatron: What!? No! How is that possible!?\r\
          \n\r\nLittle suprised that only one CPU core used at a time, but.. Memory\
          \ stayed well below available, and took 8 minutes to run.\r\n\r\nBecause\
          \ of the output warning, decided to add xformers, and rerun.. Of course\
          \ it didn't change performance much, we aren't training .. warning, around\
          \ the 4th iteration, Girafatron gets a little mouthy.. ;) And notice some\
          \ clipping of responses.. \r\n\r\nNext step, let's quantize this to 4bit...\
          \ Of course, always hairy to set 'trust_remote_code=True' ;)\r\n\r\n\r\n\
          \r\n\r\n\r\n\r\n"
        updatedAt: '2023-06-08T21:13:09.566Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - muralidandu
    id: 64824465ce114e718a403e3a
    type: comment
  author: LinuxMagic
  content: "Pleasantly surprised, the example ran right out of the box, albeit a few\
    \ errors... \r\n\r\nI can start, 512GB Ram, 48GB GPU (4090), AMD ThreadRipper\
    \ PRO.. \r\n\r\nOutput of the example: (How to Get Started with the Model)\r\n\
    \r\n python3 ./testFalcon.py \r\n\r\n2023-06-08 12:11:44.566332: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
    \ This TensorFlow binary is optimized to use available CPU instructions in performance-critical\
    \ operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations,\
    \ rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-08 12:11:45.006793:\
    \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could\
    \ not find TensorRT\r\nLoading checkpoint shards: 100%|... | 9/9 [08:20<00:00,\
    \ 55.56s/it]\r\nXformers is not installed correctly. If you want to use memory_efficient_attention\
    \ to accelerate training use the following command to install Xformers\r\npip\
    \ install xformers.\r\n\r\nThe model 'RWForCausalLM' is not supported for text-generation.\
    \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
    \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM',\
    \ 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM',\
    \ 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM',\
    \ 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
    \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
    \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
    \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
    \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
    \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
    \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
    \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
    \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
    \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\
    \n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1255:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation)\r\
    \n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
    \ generation.\r\n\r\nResult: Girafatron is obsessed with giraffes, the most glorious\
    \ animal on the face of this Earth. Giraftron believes all other animals are irrelevant\
    \ when compared to the glorious majesty of the giraffe.\r\nDaniel: Hello, Girafatron!\r\
    \nGirafatron: Hey Daniel, what's up?\r\nDaniel: Well, we have a problem.\r\nGirafatron:\
    \ What's that?\r\nDaniel: Well we are out of giraffes.\r\nGirafatron: What!? No!\
    \ How is that possible!?\r\n\r\nLittle suprised that only one CPU core used at\
    \ a time, but.. Memory stayed well below available, and took 8 minutes to run.\r\
    \n\r\nBecause of the output warning, decided to add xformers, and rerun.. Of course\
    \ it didn't change performance much, we aren't training .. warning, around the\
    \ 4th iteration, Girafatron gets a little mouthy.. ;) And notice some clipping\
    \ of responses.. \r\n\r\nNext step, let's quantize this to 4bit... Of course,\
    \ always hairy to set 'trust_remote_code=True' ;)\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
  created_at: 2023-06-08 20:13:09+00:00
  edited: false
  hidden: false
  id: 64824465ce114e718a403e3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:46:24.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6942290663719177
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>This <a href="https://huggingface.co/blog/falcon">blogpost</a> from
          HuggingFace contains a bunch of pointers on using the model in the ecosystem
          :).</p>

          '
        raw: This [blogpost](https://huggingface.co/blog/falcon) from HuggingFace
          contains a bunch of pointers on using the model in the ecosystem :).
        updatedAt: '2023-06-09T14:46:24.674Z'
      numEdits: 0
      reactions: []
    id: 64833b4034b4cd486408ec58
    type: comment
  author: FalconLLM
  content: This [blogpost](https://huggingface.co/blog/falcon) from HuggingFace contains
    a bunch of pointers on using the model in the ecosystem :).
  created_at: 2023-06-09 13:46:24+00:00
  edited: false
  hidden: false
  id: 64833b4034b4cd486408ec58
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 53
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: 'Might be interesting to have a thread on people with Successful Implementations,
  and on what kind of hardware.. '
