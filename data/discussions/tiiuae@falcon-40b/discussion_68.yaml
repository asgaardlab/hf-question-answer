!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GuySerk
conflicting_files: null
created_at: 2023-06-21 11:01:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5cc5d1813cb196e1e5cc2ad6b5e1dbc.svg
      fullname: Guy Serkinskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GuySerk
      type: user
    createdAt: '2023-06-21T12:01:34.000Z'
    data:
      edited: true
      editors:
      - GuySerk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9293306469917297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5cc5d1813cb196e1e5cc2ad6b5e1dbc.svg
          fullname: Guy Serkinskiy
          isHf: false
          isPro: false
          name: GuySerk
          type: user
        html: '<p>Hi guys,</p>

          <p>What is the minimum GPU requirement to run falcon-40b model?<br>And which
          GPU is best suited for this model? </p>

          '
        raw: 'Hi guys,


          What is the minimum GPU requirement to run falcon-40b model?

          And which GPU is best suited for this model? '
        updatedAt: '2023-06-21T12:31:03.027Z'
      numEdits: 2
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - realasd222
        - BadrH123
        - DamianKoble
        - panrosk
        - kochhar
        - Woodetect
    id: 6492e69e1328ca87221c1177
    type: comment
  author: GuySerk
  content: 'Hi guys,


    What is the minimum GPU requirement to run falcon-40b model?

    And which GPU is best suited for this model? '
  created_at: 2023-06-21 11:01:34+00:00
  edited: true
  hidden: false
  id: 6492e69e1328ca87221c1177
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/b5cc5d1813cb196e1e5cc2ad6b5e1dbc.svg
      fullname: Guy Serkinskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GuySerk
      type: user
    createdAt: '2023-06-21T12:29:53.000Z'
    data:
      from: GPU requirement
      to: GPU requirements
    id: 6492ed41f3f0d5bd4e9373db
    type: title-change
  author: GuySerk
  created_at: 2023-06-21 11:29:53+00:00
  id: 6492ed41f3f0d5bd4e9373db
  new_title: GPU requirements
  old_title: GPU requirement
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ee14306e6081690907b03c6aabb6d80.svg
      fullname: Mubarak Alketbi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mubarak-alketbi
      type: user
    createdAt: '2023-07-02T08:28:39.000Z'
    data:
      edited: false
      editors:
      - mubarak-alketbi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985588550567627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ee14306e6081690907b03c6aabb6d80.svg
          fullname: Mubarak Alketbi
          isHf: false
          isPro: false
          name: mubarak-alketbi
          type: user
        html: '<p>80+GB of Vram is required for 16bit model.</p>

          <p>try 48GB with 8bit.</p>

          '
        raw: '80+GB of Vram is required for 16bit model.


          try 48GB with 8bit.'
        updatedAt: '2023-07-02T08:28:39.313Z'
      numEdits: 0
      reactions: []
    id: 64a13537faf025cd29b93652
    type: comment
  author: mubarak-alketbi
  content: '80+GB of Vram is required for 16bit model.


    try 48GB with 8bit.'
  created_at: 2023-07-02 07:28:39+00:00
  edited: false
  hidden: false
  id: 64a13537faf025cd29b93652
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-12T01:23:38.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8947275280952454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>You can run it on a single 3090 at 17tokens/sec (up to 25 on a 4090)
          when using the Q2_K variant with ggllm.cpp<br>Output quality is great, it
          probably loses a bit in precision but writes flawless poems and summaries
          in multiple languages.</p>

          '
        raw: 'You can run it on a single 3090 at 17tokens/sec (up to 25 on a 4090)
          when using the Q2_K variant with ggllm.cpp

          Output quality is great, it probably loses a bit in precision but writes
          flawless poems and summaries in multiple languages.'
        updatedAt: '2023-07-12T01:23:38.158Z'
      numEdits: 0
      reactions: []
    id: 64ae009a6864362a7e22c746
    type: comment
  author: cmp-nct
  content: 'You can run it on a single 3090 at 17tokens/sec (up to 25 on a 4090) when
    using the Q2_K variant with ggllm.cpp

    Output quality is great, it probably loses a bit in precision but writes flawless
    poems and summaries in multiple languages.'
  created_at: 2023-07-12 00:23:38+00:00
  edited: false
  hidden: false
  id: 64ae009a6864362a7e22c746
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57ec42d0ebf6cfabd6701316dcd2df5a.svg
      fullname: Monty Craig
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaReps
      type: user
    createdAt: '2023-07-12T02:21:34.000Z'
    data:
      edited: false
      editors:
      - MetaReps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47601717710494995
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57ec42d0ebf6cfabd6701316dcd2df5a.svg
          fullname: Monty Craig
          isHf: false
          isPro: false
          name: MetaReps
          type: user
        html: '<p>Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160
          GB Disk  -<br>Ubuntu 22.10 x64</p>

          '
        raw: "Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB\
          \ Disk  - \nUbuntu 22.10 x64"
        updatedAt: '2023-07-12T02:21:34.892Z'
      numEdits: 0
      reactions: []
    id: 64ae0e2e616d3eb3616581e2
    type: comment
  author: MetaReps
  content: "Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB Disk\
    \  - \nUbuntu 22.10 x64"
  created_at: 2023-07-12 01:21:34+00:00
  edited: false
  hidden: false
  id: 64ae0e2e616d3eb3616581e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648333745311-623f9262a3e12d59ef282e3f.png?w=200&h=200&f=face
      fullname: Geoffrey Riva Moraes Porto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: geoffreyporto
      type: user
    createdAt: '2023-07-12T03:04:32.000Z'
    data:
      edited: true
      editors:
      - geoffreyporto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45300817489624023
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648333745311-623f9262a3e12d59ef282e3f.png?w=200&h=200&f=face
          fullname: Geoffrey Riva Moraes Porto
          isHf: false
          isPro: false
          name: geoffreyporto
          type: user
        html: '<blockquote>

          <p>Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB
          Disk  -<br>Ubuntu 22.10 x64<br>No.<br>This could work:<br>Buy for $79USD
          x month a Skylake SSD 3XL9 100</p>

          </blockquote>

          <ul>

          <li>64GB DDR 4</li>

          <li>intel Core i7-6700</li>

          <li>Software-Raid 1</li>

          <li>2x 500GB SSD<br><a rel="nofollow" href="https://buy.server4you.com/en/customer/create/skylake-ssd-3xl9-100/e66fcda9-7b1e-463a-8747-bce776de4ac0">https://buy.server4you.com/en/customer/create/skylake-ssd-3xl9-100/e66fcda9-7b1e-463a-8747-bce776de4ac0</a></li>

          </ul>

          '
        raw: "> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160\
          \ GB Disk  - \n> Ubuntu 22.10 x64\nNo. \nThis could work:\nBuy for $79USD\
          \ x month a Skylake SSD 3XL9 100\n* 64GB DDR 4\n* intel Core i7-6700\n*\
          \ Software-Raid 1\n* 2x 500GB SSD\nhttps://buy.server4you.com/en/customer/create/skylake-ssd-3xl9-100/e66fcda9-7b1e-463a-8747-bce776de4ac0\n"
        updatedAt: '2023-07-12T03:05:23.499Z'
      numEdits: 1
      reactions: []
    id: 64ae184034f4e8f3c90fbea0
    type: comment
  author: geoffreyporto
  content: "> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB\
    \ Disk  - \n> Ubuntu 22.10 x64\nNo. \nThis could work:\nBuy for $79USD x month\
    \ a Skylake SSD 3XL9 100\n* 64GB DDR 4\n* intel Core i7-6700\n* Software-Raid\
    \ 1\n* 2x 500GB SSD\nhttps://buy.server4you.com/en/customer/create/skylake-ssd-3xl9-100/e66fcda9-7b1e-463a-8747-bce776de4ac0\n"
  created_at: 2023-07-12 02:04:32+00:00
  edited: true
  hidden: false
  id: 64ae184034f4e8f3c90fbea0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-13T12:40:03.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579387903213501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <p>Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB
          Disk  -<br>Ubuntu 22.10 x64<br>No clearly not, also the affiliate link posted
          below is not suitable.</p>

          </blockquote>

          <ol>

          <li>If you want to run the 16 bit version you''ll need around 85-90GB of
          RAM/VRAM. Your server has 8GB and the affiliate link has 64GB.</li>

          <li>Even if you''d use a server like that with enough RAM your speed would
          be like 1 token processed every 15-30 seconds. Literally not useable for
          anything but as a room heater.</li>

          <li>There are only academic reasons that would come to my mind why you''d
          want to run a 16 bit version of Falcon on a CPU, it''s hard to find a good
          reason why you''d want to inference that on GPU either.<br>There are no
          quality benefits over a high quality quantized version, the RAM requirements
          are extreme and the processing speed slow.</li>

          </ol>

          <p>On your server you can not expect to run Falcon 40B, the smallest version
          of 40B using the cmp-nct repository is around 13GB with processing buffers
          included. And that''s at 2.5bit quantization.<br>15GB on 3.5 bit, almost
          24 at 4.5 bit<br>However, you can run Falcon 7B on that machine, in 5 bit
          quanitzation you''ll have about the same quality as 16 bit and need roughly
          7GB RAM for processing it. Speed a couple tokens per second.</p>

          '
        raw: "> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160\
          \ GB Disk  - \n> Ubuntu 22.10 x64\nNo clearly not, also the affiliate link\
          \ posted below is not suitable.\n\n1) If you want to run the 16 bit version\
          \ you'll need around 85-90GB of RAM/VRAM. Your server has 8GB and the affiliate\
          \ link has 64GB.\n2) Even if you'd use a server like that with enough RAM\
          \ your speed would be like 1 token processed every 15-30 seconds. Literally\
          \ not useable for anything but as a room heater.\n3) There are only academic\
          \ reasons that would come to my mind why you'd want to run a 16 bit version\
          \ of Falcon on a CPU, it's hard to find a good reason why you'd want to\
          \ inference that on GPU either.\nThere are no quality benefits over a high\
          \ quality quantized version, the RAM requirements are extreme and the processing\
          \ speed slow.\n\nOn your server you can not expect to run Falcon 40B, the\
          \ smallest version of 40B using the cmp-nct repository is around 13GB with\
          \ processing buffers included. And that's at 2.5bit quantization. \n15GB\
          \ on 3.5 bit, almost 24 at 4.5 bit\nHowever, you can run Falcon 7B on that\
          \ machine, in 5 bit quanitzation you'll have about the same quality as 16\
          \ bit and need roughly 7GB RAM for processing it. Speed a couple tokens\
          \ per second."
        updatedAt: '2023-07-14T13:57:22.784Z'
      numEdits: 1
      reactions: []
    id: 64aff0a3c8e76b6295ad19c3
    type: comment
  author: cmp-nct
  content: "> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB\
    \ Disk  - \n> Ubuntu 22.10 x64\nNo clearly not, also the affiliate link posted\
    \ below is not suitable.\n\n1) If you want to run the 16 bit version you'll need\
    \ around 85-90GB of RAM/VRAM. Your server has 8GB and the affiliate link has 64GB.\n\
    2) Even if you'd use a server like that with enough RAM your speed would be like\
    \ 1 token processed every 15-30 seconds. Literally not useable for anything but\
    \ as a room heater.\n3) There are only academic reasons that would come to my\
    \ mind why you'd want to run a 16 bit version of Falcon on a CPU, it's hard to\
    \ find a good reason why you'd want to inference that on GPU either.\nThere are\
    \ no quality benefits over a high quality quantized version, the RAM requirements\
    \ are extreme and the processing speed slow.\n\nOn your server you can not expect\
    \ to run Falcon 40B, the smallest version of 40B using the cmp-nct repository\
    \ is around 13GB with processing buffers included. And that's at 2.5bit quantization.\
    \ \n15GB on 3.5 bit, almost 24 at 4.5 bit\nHowever, you can run Falcon 7B on that\
    \ machine, in 5 bit quanitzation you'll have about the same quality as 16 bit\
    \ and need roughly 7GB RAM for processing it. Speed a couple tokens per second."
  created_at: 2023-07-13 11:40:03+00:00
  edited: true
  hidden: false
  id: 64aff0a3c8e76b6295ad19c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b7ff579576bd1072606662877ff2188.svg
      fullname: Al Fansome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Monomial
      type: user
    createdAt: '2023-07-23T04:08:03.000Z'
    data:
      edited: false
      editors:
      - Monomial
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9544676542282104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b7ff579576bd1072606662877ff2188.svg
          fullname: Al Fansome
          isHf: false
          isPro: false
          name: Monomial
          type: user
        html: '<p>Quick question for the community</p>

          <p>I have an opportunity to pick up 4 second hand 24GB 3090''s at a reasonable
          discount.</p>

          <p>The only reason I would do this is if I could run Falcon-40B on them.  I
          understand from above the performance on a single 3090 would be around 17
          tokens/sec.  Would a (x4) configuration be a viable to run the 40B parameter
          model?  Would there be a faster configuration to run Falcon-40B in the ~$3k
          price range?  As a bonus question, can anyone speculate on the kind of performance
          I might realistically expect to achieve?  This is a training system for
          me, but I''d also like to actually try and actually use if for real world
          problems if possible.</p>

          <p>Thank you for any assistance and advice. I don''t want to spend the money
          if this is not going to work.</p>

          '
        raw: 'Quick question for the community


          I have an opportunity to pick up 4 second hand 24GB 3090''s at a reasonable
          discount.


          The only reason I would do this is if I could run Falcon-40B on them.  I
          understand from above the performance on a single 3090 would be around 17
          tokens/sec.  Would a (x4) configuration be a viable to run the 40B parameter
          model?  Would there be a faster configuration to run Falcon-40B in the ~$3k
          price range?  As a bonus question, can anyone speculate on the kind of performance
          I might realistically expect to achieve?  This is a training system for
          me, but I''d also like to actually try and actually use if for real world
          problems if possible.


          Thank you for any assistance and advice. I don''t want to spend the money
          if this is not going to work.

          '
        updatedAt: '2023-07-23T04:08:03.691Z'
      numEdits: 0
      reactions: []
    id: 64bca7a3b8cd00871927c508
    type: comment
  author: Monomial
  content: 'Quick question for the community


    I have an opportunity to pick up 4 second hand 24GB 3090''s at a reasonable discount.


    The only reason I would do this is if I could run Falcon-40B on them.  I understand
    from above the performance on a single 3090 would be around 17 tokens/sec.  Would
    a (x4) configuration be a viable to run the 40B parameter model?  Would there
    be a faster configuration to run Falcon-40B in the ~$3k price range?  As a bonus
    question, can anyone speculate on the kind of performance I might realistically
    expect to achieve?  This is a training system for me, but I''d also like to actually
    try and actually use if for real world problems if possible.


    Thank you for any assistance and advice. I don''t want to spend the money if this
    is not going to work.

    '
  created_at: 2023-07-23 03:08:03+00:00
  edited: false
  hidden: false
  id: 64bca7a3b8cd00871927c508
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-23T22:34:28.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9491744637489319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <p>Quick question for the community</p>

          <p>I have an opportunity to pick up 4 second hand 24GB 3090''s at a reasonable
          discount.</p>

          <p>The only reason I would do this is if I could run Falcon-40B on them.  I
          understand from above the performance on a single 3090 would be around 17
          tokens/sec.  Would a (x4) configuration be a viable to run the 40B parameter
          model?  Would there be a faster configuration to run Falcon-40B in the ~$3k
          price range?  As a bonus question, can anyone speculate on the kind of performance
          I might realistically expect to achieve?  This is a training system for
          me, but I''d also like to actually try and actually use if for real world
          problems if possible.</p>

          <p>Thank you for any assistance and advice. I don''t want to spend the money
          if this is not going to work.</p>

          </blockquote>

          <p>If you run it using ggllm (<a rel="nofollow" href="https://github.com/cmp-nct/ggllm.cpp">https://github.com/cmp-nct/ggllm.cpp</a>)
          a single 3090 can run 3 bit, on windows also 4 bit with a bit squeezing.<br>Performance
          on a single 3090 is probably around 15tk/sec, on two 3090 (for larger variants)
          I''m not sure about performance. Likely about the same.<br>More than 2 would
          never be needed as long as no larger Falcon is released.<br>The upcoming
          release will have special support for 4090ies that use the better compute
          capability, mostly relevant for prompt processing speed.</p>

          <p>On python I don''t know, I''d assume you can run it on 4x 3090 using
          pytorch or 2x 3090 using gptq.</p>

          '
        raw: "> Quick question for the community\n> \n> I have an opportunity to pick\
          \ up 4 second hand 24GB 3090's at a reasonable discount.\n> \n> The only\
          \ reason I would do this is if I could run Falcon-40B on them.  I understand\
          \ from above the performance on a single 3090 would be around 17 tokens/sec.\
          \  Would a (x4) configuration be a viable to run the 40B parameter model?\
          \  Would there be a faster configuration to run Falcon-40B in the ~$3k price\
          \ range?  As a bonus question, can anyone speculate on the kind of performance\
          \ I might realistically expect to achieve?  This is a training system for\
          \ me, but I'd also like to actually try and actually use if for real world\
          \ problems if possible.\n> \n> Thank you for any assistance and advice.\
          \ I don't want to spend the money if this is not going to work.\n\nIf you\
          \ run it using ggllm (https://github.com/cmp-nct/ggllm.cpp) a single 3090\
          \ can run 3 bit, on windows also 4 bit with a bit squeezing.\nPerformance\
          \ on a single 3090 is probably around 15tk/sec, on two 3090 (for larger\
          \ variants) I'm not sure about performance. Likely about the same. \nMore\
          \ than 2 would never be needed as long as no larger Falcon is released.\n\
          The upcoming release will have special support for 4090ies that use the\
          \ better compute capability, mostly relevant for prompt processing speed.\n\
          \nOn python I don't know, I'd assume you can run it on 4x 3090 using pytorch\
          \ or 2x 3090 using gptq."
        updatedAt: '2023-07-23T22:34:28.935Z'
      numEdits: 0
      reactions: []
    id: 64bdaaf4796f20daad7755d7
    type: comment
  author: cmp-nct
  content: "> Quick question for the community\n> \n> I have an opportunity to pick\
    \ up 4 second hand 24GB 3090's at a reasonable discount.\n> \n> The only reason\
    \ I would do this is if I could run Falcon-40B on them.  I understand from above\
    \ the performance on a single 3090 would be around 17 tokens/sec.  Would a (x4)\
    \ configuration be a viable to run the 40B parameter model?  Would there be a\
    \ faster configuration to run Falcon-40B in the ~$3k price range?  As a bonus\
    \ question, can anyone speculate on the kind of performance I might realistically\
    \ expect to achieve?  This is a training system for me, but I'd also like to actually\
    \ try and actually use if for real world problems if possible.\n> \n> Thank you\
    \ for any assistance and advice. I don't want to spend the money if this is not\
    \ going to work.\n\nIf you run it using ggllm (https://github.com/cmp-nct/ggllm.cpp)\
    \ a single 3090 can run 3 bit, on windows also 4 bit with a bit squeezing.\nPerformance\
    \ on a single 3090 is probably around 15tk/sec, on two 3090 (for larger variants)\
    \ I'm not sure about performance. Likely about the same. \nMore than 2 would never\
    \ be needed as long as no larger Falcon is released.\nThe upcoming release will\
    \ have special support for 4090ies that use the better compute capability, mostly\
    \ relevant for prompt processing speed.\n\nOn python I don't know, I'd assume\
    \ you can run it on 4x 3090 using pytorch or 2x 3090 using gptq."
  created_at: 2023-07-23 21:34:28+00:00
  edited: false
  hidden: false
  id: 64bdaaf4796f20daad7755d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: GPU requirements
