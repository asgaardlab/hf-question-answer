!!python/object:huggingface_hub.community.DiscussionWithDetails
author: serin32
conflicting_files: null
created_at: 2023-06-02 11:19:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-02T12:19:14.000Z'
    data:
      edited: true
      editors:
      - serin32
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<p>I was able to get bitsandbytes new 4 bit working on Falcon which
          made it fit nicely on the A100 40GB in Google Colab:</p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers


          <p>!pip install git+<a rel="nofollow" href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>

          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer<br>import
          torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
          load_in_4bit=True, device_map="auto") </p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>


          <p>Hope it helps someone out!</p>

          '
        raw: "I was able to get bitsandbytes new 4 bit working on Falcon which made\
          \ it fit nicely on the A100 40GB in Google Colab:\n\n<code>\n!pip install\
          \ git+https://www.github.com/huggingface/transformers\n\n!pip install git+https://github.com/huggingface/accelerate\n\
          \n!pip install bitsandbytes\n\n!pip install einops\n\nfrom transformers\
          \ import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\nimport torch\n\
          \nmodel_path=\"tiiuae/falcon-40b-instruct\"\n\nconfig = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n\ninput_text\
          \ = \"Describe the solar system.\"\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids, max_length=100)\n\
          print(tokenizer.decode(outputs[0]))\n\n</code>\n\nHope it helps someone\
          \ out!"
        updatedAt: '2023-06-02T12:22:28.458Z'
      numEdits: 1
      reactions:
      - count: 25
        reaction: "\U0001F44D"
        users:
        - jaker86
        - Jordancole21
        - godmode
        - nikoma777
        - Ichsan2895
        - a749734
        - Benedick
        - FPVG
        - tanweer-m
        - emilylearning
        - guanqun-yang
        - max-fry
        - radames
        - liangong
        - niketkumarAI
        - FalconLLM
        - rohandinho10
        - nanp08
        - codelion
        - neumannjanos
        - zefang-liu
        - toufique-phamily
        - ashioyajotham
        - currutia3
        - kekmodel
      - count: 2
        reaction: "\U0001F917"
        users:
        - orestis22
        - toufique-phamily
    id: 6479de427c18dca75e9a0903
    type: comment
  author: serin32
  content: "I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n\n<code>\n!pip install git+https://www.github.com/huggingface/transformers\n\
    \n!pip install git+https://github.com/huggingface/accelerate\n\n!pip install bitsandbytes\n\
    \n!pip install einops\n\nfrom transformers import  AutoModelForCausalLM, AutoConfig,\
    \ AutoTokenizer\nimport torch\n\nmodel_path=\"tiiuae/falcon-40b-instruct\"\n\n\
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True,\
    \ device_map=\"auto\") \n\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\"\
    )\n\ninput_text = \"Describe the solar system.\"\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids,\
    \ max_length=100)\nprint(tokenizer.decode(outputs[0]))\n\n</code>\n\nHope it helps\
    \ someone out!"
  created_at: 2023-06-02 11:19:14+00:00
  edited: true
  hidden: false
  id: 6479de427c18dca75e9a0903
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-03T06:08:21.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7237401008605957
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: '<p>CUDA kernel errors might be asynchronously reported at some other
          API call, so the stacktrace below might be incorrect.<br>For debugging consider
          passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>
          to enable device-side assertions.<br>i m getting this error on this code</p>

          '
        raw: 'CUDA kernel errors might be asynchronously reported at some other API
          call, so the stacktrace below might be incorrect.

          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

          i m getting this error on this code'
        updatedAt: '2023-06-03T06:08:21.290Z'
      numEdits: 0
      reactions: []
    id: 647ad8d5b31514a4a6cb8948
    type: comment
  author: a749734
  content: 'CUDA kernel errors might be asynchronously reported at some other API
    call, so the stacktrace below might be incorrect.

    For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

    Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    i m getting this error on this code'
  created_at: 2023-06-03 05:08:21+00:00
  edited: false
  hidden: false
  id: 647ad8d5b31514a4a6cb8948
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f25d3df951ee77f454132a1292b905d.svg
      fullname: Arjun Bansal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arjunbansal
      type: user
    createdAt: '2023-06-03T08:16:31.000Z'
    data:
      edited: false
      editors:
      - arjunbansal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9500402808189392
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f25d3df951ee77f454132a1292b905d.svg
          fullname: Arjun Bansal
          isHf: false
          isPro: false
          name: arjunbansal
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;serin32&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/serin32\">@<span class=\"\
          underline\">serin32</span></a></span>\n\n\t</span></span> How did you deal\
          \ with the 77GB storage limit on Colab? the model seems to need about 90G\
          \ to download all the bin files.</p>\n"
        raw: '@serin32 How did you deal with the 77GB storage limit on Colab? the
          model seems to need about 90G to download all the bin files.'
        updatedAt: '2023-06-03T08:16:31.036Z'
      numEdits: 0
      reactions: []
    id: 647af6dfb31514a4a6cf3620
    type: comment
  author: arjunbansal
  content: '@serin32 How did you deal with the 77GB storage limit on Colab? the model
    seems to need about 90G to download all the bin files.'
  created_at: 2023-06-03 07:16:31+00:00
  edited: false
  hidden: false
  id: 647af6dfb31514a4a6cf3620
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb5096ee4fc2b683bb0087478b6e7134.svg
      fullname: Walker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Benedick
      type: user
    createdAt: '2023-06-03T12:08:44.000Z'
    data:
      edited: false
      editors:
      - Benedick
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6067487597465515
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb5096ee4fc2b683bb0087478b6e7134.svg
          fullname: Walker
          isHf: false
          isPro: false
          name: Benedick
          type: user
        html: "<blockquote>\n<p>I was able to get bitsandbytes new 4 bit working on\
          \ Falcon which made it fit nicely on the A100 40GB in Google Colab:</p>\n\
          <code>\n!pip install git+https://www.github.com/huggingface/transformers\n\
          \n<p>!pip install git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate\"\
          >https://github.com/huggingface/accelerate</a></p>\n<p>!pip install bitsandbytes</p>\n\
          <p>!pip install einops</p>\n<p>from transformers import  AutoModelForCausalLM,\
          \ AutoConfig, AutoTokenizer<br>import torch</p>\n<p>model_path=\"tiiuae/falcon-40b-instruct\"\
          </p>\n<p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ load_in_4bit=True, device_map=\"auto\") </p>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\")</p>\n<p>input_text = \"Describe the solar\
          \ system.\"<br>input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")</p>\n<p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>\n\
          </code>\n\n<p>Hope it helps someone out!</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;serin32&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/serin32\">@<span class=\"underline\">serin32</span></a></span>\n\
          \n\t</span></span> Thank you for creating this, it is super helpful! But\
          \ the inference is very very slow. Is there a way to improve it? Thanks!</p>\n"
        raw: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
          \ it fit nicely on the A100 40GB in Google Colab:\n> \n> <code>\n> !pip\
          \ install git+https://www.github.com/huggingface/transformers\n> \n> !pip\
          \ install git+https://github.com/huggingface/accelerate\n> \n> !pip install\
          \ bitsandbytes\n> \n> !pip install einops\n> \n> from transformers import\
          \  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> import torch\n> \n\
          > model_path=\"tiiuae/falcon-40b-instruct\"\n> \n> config = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\n> model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> \n\
          > tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\"\
          )\n> \n> input_text = \"Describe the solar system.\"\n> input_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids,\
          \ max_length=100)\n> print(tokenizer.decode(outputs[0]))\n> \n> </code>\n\
          > \n> Hope it helps someone out!\n\n@serin32 Thank you for creating this,\
          \ it is super helpful! But the inference is very very slow. Is there a way\
          \ to improve it? Thanks!"
        updatedAt: '2023-06-03T12:08:44.185Z'
      numEdits: 0
      reactions: []
    id: 647b2d4c6dbad6ab05768019
    type: comment
  author: Benedick
  content: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> \n> <code>\n> !pip install\
    \ git+https://www.github.com/huggingface/transformers\n> \n> !pip install git+https://github.com/huggingface/accelerate\n\
    > \n> !pip install bitsandbytes\n> \n> !pip install einops\n> \n> from transformers\
    \ import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> import torch\n> \n\
    > model_path=\"tiiuae/falcon-40b-instruct\"\n> \n> config = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\n> model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> \n> tokenizer\
    \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> \n> input_text\
    \ = \"Describe the solar system.\"\n> input_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids, max_length=100)\n\
    > print(tokenizer.decode(outputs[0]))\n> \n> </code>\n> \n> Hope it helps someone\
    \ out!\n\n@serin32 Thank you for creating this, it is super helpful! But the inference\
    \ is very very slow. Is there a way to improve it? Thanks!"
  created_at: 2023-06-03 11:08:44+00:00
  edited: false
  hidden: false
  id: 647b2d4c6dbad6ab05768019
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-03T15:03:54.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7873975038528442
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<blockquote>

          <p>CUDA kernel errors might be asynchronously reported at some other API
          call, so the stacktrace below might be incorrect.<br>For debugging consider
          passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>
          to enable device-side assertions.<br>i m getting this error on this code</p>

          </blockquote>

          <p>Looks to me like Pytorch may not be compiled for GPU use?  Were you doing
          this from Google Colab or your own machine?  Does your machine have a GPU?  If
          so you may need to recompile Pytorch for CUDA.</p>

          '
        raw: '> CUDA kernel errors might be asynchronously reported at some other
          API call, so the stacktrace below might be incorrect.

          > For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

          > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

          > i m getting this error on this code


          Looks to me like Pytorch may not be compiled for GPU use?  Were you doing
          this from Google Colab or your own machine?  Does your machine have a GPU?  If
          so you may need to recompile Pytorch for CUDA.'
        updatedAt: '2023-06-03T15:03:54.145Z'
      numEdits: 0
      reactions: []
    id: 647b565a4d7c0c3fcccfaba9
    type: comment
  author: serin32
  content: '> CUDA kernel errors might be asynchronously reported at some other API
    call, so the stacktrace below might be incorrect.

    > For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

    > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    > i m getting this error on this code


    Looks to me like Pytorch may not be compiled for GPU use?  Were you doing this
    from Google Colab or your own machine?  Does your machine have a GPU?  If so you
    may need to recompile Pytorch for CUDA.'
  created_at: 2023-06-03 14:03:54+00:00
  edited: false
  hidden: false
  id: 647b565a4d7c0c3fcccfaba9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-03T15:04:53.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9317495822906494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;serin32&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/serin32\"\
          >@<span class=\"underline\">serin32</span></a></span>\n\n\t</span></span>\
          \ How did you deal with the 77GB storage limit on Colab? the model seems\
          \ to need about 90G to download all the bin files.</p>\n</blockquote>\n\
          <p>I have Google Colab Pro + and get 166.8GB of storage.  If you have an\
          \ expanded Google Drive you may be able to download the files to your drive\
          \ then link Google Drive with Colab to have enough space.</p>\n"
        raw: '> @serin32 How did you deal with the 77GB storage limit on Colab? the
          model seems to need about 90G to download all the bin files.


          I have Google Colab Pro + and get 166.8GB of storage.  If you have an expanded
          Google Drive you may be able to download the files to your drive then link
          Google Drive with Colab to have enough space.'
        updatedAt: '2023-06-03T15:04:53.153Z'
      numEdits: 0
      reactions: []
    id: 647b56956dbad6ab057b7cbf
    type: comment
  author: serin32
  content: '> @serin32 How did you deal with the 77GB storage limit on Colab? the
    model seems to need about 90G to download all the bin files.


    I have Google Colab Pro + and get 166.8GB of storage.  If you have an expanded
    Google Drive you may be able to download the files to your drive then link Google
    Drive with Colab to have enough space.'
  created_at: 2023-06-03 14:04:53+00:00
  edited: false
  hidden: false
  id: 647b56956dbad6ab057b7cbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-03T15:07:10.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6945618987083435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: "<blockquote>\n<blockquote>\n<p>I was able to get bitsandbytes new 4\
          \ bit working on Falcon which made it fit nicely on the A100 40GB in Google\
          \ Colab:</p>\n<code>\n!pip install git+https://www.github.com/huggingface/transformers\n\
          \n<p>!pip install git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate\"\
          >https://github.com/huggingface/accelerate</a></p>\n<p>!pip install bitsandbytes</p>\n\
          <p>!pip install einops</p>\n<p>from transformers import  AutoModelForCausalLM,\
          \ AutoConfig, AutoTokenizer<br>import torch</p>\n<p>model_path=\"tiiuae/falcon-40b-instruct\"\
          </p>\n<p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ load_in_4bit=True, device_map=\"auto\") </p>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\")</p>\n<p>input_text = \"Describe the solar\
          \ system.\"<br>input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")</p>\n<p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>\n\
          </code>\n\n<p>Hope it helps someone out!</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;serin32&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/serin32\">@<span class=\"underline\">serin32</span></a></span>\n\
          \n\t</span></span> Thank you for creating this, it is super helpful! But\
          \ the inference is very very slow. Is there a way to improve it? Thanks!</p>\n\
          </blockquote>\n<p>I don't know a way to make it faster.  I tried following\
          \ this: <a href=\"https://huggingface.co/docs/transformers/perf_infer_gpu_one\"\
          >https://huggingface.co/docs/transformers/perf_infer_gpu_one</a> but this\
          \ model isn't supported by the Huggingface Optimum library.  Hopefully people\
          \ smarter than me can come up with ways to make it faster.</p>\n"
        raw: "> > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > \n> > <code>\n\
          > > !pip install git+https://www.github.com/huggingface/transformers\n>\
          \ > \n> > !pip install git+https://github.com/huggingface/accelerate\n>\
          \ > \n> > !pip install bitsandbytes\n> > \n> > !pip install einops\n> >\
          \ \n> > from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n\
          > > import torch\n> > \n> > model_path=\"tiiuae/falcon-40b-instruct\"\n\
          > > \n> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
          > > model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ load_in_4bit=True, device_map=\"auto\") \n> > \n> > tokenizer = AutoTokenizer.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\")\n> > \n> > input_text = \"Describe the solar\
          \ system.\"\n> > input_ids = tokenizer(input_text, return_tensors=\"pt\"\
          ).input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids,\
          \ max_length=100)\n> > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n\
          > > \n> > Hope it helps someone out!\n> \n> @serin32 Thank you for creating\
          \ this, it is super helpful! But the inference is very very slow. Is there\
          \ a way to improve it? Thanks!\n\nI don't know a way to make it faster.\
          \  I tried following this: https://huggingface.co/docs/transformers/perf_infer_gpu_one\
          \ but this model isn't supported by the Huggingface Optimum library.  Hopefully\
          \ people smarter than me can come up with ways to make it faster."
        updatedAt: '2023-06-03T15:07:10.067Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Benedick
    id: 647b571eb31514a4a6db59cc
    type: comment
  author: serin32
  content: "> > I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> > \n> > <code>\n> > !pip\
    \ install git+https://www.github.com/huggingface/transformers\n> > \n> > !pip\
    \ install git+https://github.com/huggingface/accelerate\n> > \n> > !pip install\
    \ bitsandbytes\n> > \n> > !pip install einops\n> > \n> > from transformers import\
    \  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> > import torch\n> > \n>\
    \ > model_path=\"tiiuae/falcon-40b-instruct\"\n> > \n> > config = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\n> > model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> > \n> >\
    \ tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n\
    > > \n> > input_text = \"Describe the solar system.\"\n> > input_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids,\
    \ max_length=100)\n> > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n\
    > > \n> > Hope it helps someone out!\n> \n> @serin32 Thank you for creating this,\
    \ it is super helpful! But the inference is very very slow. Is there a way to\
    \ improve it? Thanks!\n\nI don't know a way to make it faster.  I tried following\
    \ this: https://huggingface.co/docs/transformers/perf_infer_gpu_one but this model\
    \ isn't supported by the Huggingface Optimum library.  Hopefully people smarter\
    \ than me can come up with ways to make it faster."
  created_at: 2023-06-03 14:07:10+00:00
  edited: false
  hidden: false
  id: 647b571eb31514a4a6db59cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-03T15:53:35.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45626017451286316
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,
          BitsAndBytesConfig<br>import torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path,<br>                                             quantization_config=BitsAndBytesConfig(<br>                load_in_4bit=True),<br>                trust_remote_code=True,<br>                torch_dtype=torch.bfloat16,
          # additional option to lower RAM consumtion<br>                device_map={"":
          0})</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>'
        raw: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
          \ it fit nicely on the A100 40GB in Google Colab:\n> \n> Hope it helps someone\
          \ out!\n\n<b>Your code is error, the correct way thats work for me like\
          \ this:</b>\n\n<code>\n!pip install git+https://www.github.com/huggingface/transformers\n\
          !pip install git+https://github.com/huggingface/accelerate\n\n!pip install\
          \ bitsandbytes\n\n!pip install einops\n\nfrom transformers import AutoModelForCausalLM,\
          \ AutoConfig, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nmodel_path=\"\
          tiiuae/falcon-40b-instruct\"\n\nconfig = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\n\
          \                                             quantization_config=BitsAndBytesConfig(\n\
          \                load_in_4bit=True),\n                trust_remote_code=True,\n\
          \                torch_dtype=torch.bfloat16, # additional option to lower\
          \ RAM consumtion\n                device_map={\"\": 0})\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\")\n\ninput_text = \"Describe the solar system.\"\
          \ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")\n\noutputs = model.generate(input_ids, max_length=100)\nprint(tokenizer.decode(outputs[0]))\n\
          \n</code>"
        updatedAt: '2023-06-04T02:28:48.105Z'
      numEdits: 3
      reactions: []
    id: 647b61ffe8b7333058a72b8b
    type: comment
  author: Ichsan2895
  content: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> \n> Hope it helps someone\
    \ out!\n\n<b>Your code is error, the correct way thats work for me like this:</b>\n\
    \n<code>\n!pip install git+https://www.github.com/huggingface/transformers\n!pip\
    \ install git+https://github.com/huggingface/accelerate\n\n!pip install bitsandbytes\n\
    \n!pip install einops\n\nfrom transformers import AutoModelForCausalLM, AutoConfig,\
    \ AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nmodel_path=\"tiiuae/falcon-40b-instruct\"\
    \n\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
    model = AutoModelForCausalLM.from_pretrained(model_path,\n                   \
    \                          quantization_config=BitsAndBytesConfig(\n         \
    \       load_in_4bit=True),\n                trust_remote_code=True,\n       \
    \         torch_dtype=torch.bfloat16, # additional option to lower RAM consumtion\n\
    \                device_map={\"\": 0})\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    tiiuae/falcon-40b-instruct\")\n\ninput_text = \"Describe the solar system.\"\n\
    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\"\
    )\n\noutputs = model.generate(input_ids, max_length=100)\nprint(tokenizer.decode(outputs[0]))\n\
    \n</code>"
  created_at: 2023-06-03 14:53:35+00:00
  edited: true
  hidden: false
  id: 647b61ffe8b7333058a72b8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-03T17:12:00.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9412158131599426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          </blockquote>

          <p>Thanks for sharing your code!  I didn''t need to use BitsAndBytesConfig
          for my Google Colab Pro +, but its possible that just Pro might need it.</p>

          '
        raw: "> > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > \n> > Hope it\
          \ helps someone out!\n> \n> <b>Your code is error, the correct way thats\
          \ work for me like this:</b>\n> \nThanks for sharing your code!  I didn't\
          \ need to use BitsAndBytesConfig for my Google Colab Pro +, but its possible\
          \ that just Pro might need it."
        updatedAt: '2023-06-03T17:12:00.834Z'
      numEdits: 0
      reactions: []
    id: 647b74606dbad6ab057ee44d
    type: comment
  author: serin32
  content: "> > I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> > \n> > Hope it helps someone\
    \ out!\n> \n> <b>Your code is error, the correct way thats work for me like this:</b>\n\
    > \nThanks for sharing your code!  I didn't need to use BitsAndBytesConfig for\
    \ my Google Colab Pro +, but its possible that just Pro might need it."
  created_at: 2023-06-03 16:12:00+00:00
  edited: false
  hidden: false
  id: 647b74606dbad6ab057ee44d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-04T02:32:09.000Z'
    data:
      edited: false
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9298700094223022
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          </blockquote>

          <p>Thanks for sharing your code!  I didn''t need to use BitsAndBytesConfig
          for my Google Colab Pro +, but its possible that just Pro might need it.</p>

          </blockquote>

          <p>Thanks for you too. I don''t think about 4 bit quantization before you
          said that, unfortunatelly your code is error in My Colab. So I modified
          a bit using bitsandbytes &amp; using torch.bfloat16.</p>

          '
        raw: "> > > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > > \n> > > Hope\
          \ it helps someone out!\n> > \n> > <b>Your code is error, the correct way\
          \ thats work for me like this:</b>\n> > \n> Thanks for sharing your code!\
          \  I didn't need to use BitsAndBytesConfig for my Google Colab Pro +, but\
          \ its possible that just Pro might need it.\n\nThanks for you too. I don't\
          \ think about 4 bit quantization before you said that, unfortunatelly your\
          \ code is error in My Colab. So I modified a bit using bitsandbytes & using\
          \ torch.bfloat16."
        updatedAt: '2023-06-04T02:32:09.475Z'
      numEdits: 0
      reactions: []
    id: 647bf7a9b31514a4a6ec3c03
    type: comment
  author: Ichsan2895
  content: "> > > I was able to get bitsandbytes new 4 bit working on Falcon which\
    \ made it fit nicely on the A100 40GB in Google Colab:\n> > > \n> > > Hope it\
    \ helps someone out!\n> > \n> > <b>Your code is error, the correct way thats work\
    \ for me like this:</b>\n> > \n> Thanks for sharing your code!  I didn't need\
    \ to use BitsAndBytesConfig for my Google Colab Pro +, but its possible that just\
    \ Pro might need it.\n\nThanks for you too. I don't think about 4 bit quantization\
    \ before you said that, unfortunatelly your code is error in My Colab. So I modified\
    \ a bit using bitsandbytes & using torch.bfloat16."
  created_at: 2023-06-04 01:32:09+00:00
  edited: false
  hidden: false
  id: 647bf7a9b31514a4a6ec3c03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b12fab51d8bc5a4200e4b9b571c750c7.svg
      fullname: charly abraham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charlypa
      type: user
    createdAt: '2023-06-04T09:24:22.000Z'
    data:
      edited: false
      editors:
      - charlypa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9451881051063538
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b12fab51d8bc5a4200e4b9b571c750c7.svg
          fullname: charly abraham
          isHf: false
          isPro: false
          name: charlypa
          type: user
        html: '<p>Thanks, it works</p>

          '
        raw: Thanks, it works
        updatedAt: '2023-06-04T09:24:22.026Z'
      numEdits: 0
      reactions: []
    id: 647c5846c788767ab5c784d9
    type: comment
  author: charlypa
  content: Thanks, it works
  created_at: 2023-06-04 08:24:22+00:00
  edited: false
  hidden: false
  id: 647c5846c788767ab5c784d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
      fullname: Knight
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bailey24
      type: user
    createdAt: '2023-06-05T09:31:06.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669816869049-6358bf50856b319a29bd6f14.jpeg?w=200&h=200&f=face
          fullname: Knight
          isHf: false
          isPro: false
          name: Bailey24
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-07T06:28:34.885Z'
      numEdits: 0
      reactions: []
    id: 647dab5af14eafc3b448a80d
    type: comment
  author: Bailey24
  content: This comment has been hidden
  created_at: 2023-06-05 08:31:06+00:00
  edited: true
  hidden: true
  id: 647dab5af14eafc3b448a80d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2f4a171b1f15d4e9c8a49a2b6844eb90.svg
      fullname: Plaban Nayak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Plaban81
      type: user
    createdAt: '2023-06-06T12:31:23.000Z'
    data:
      edited: false
      editors:
      - Plaban81
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.50865638256073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2f4a171b1f15d4e9c8a49a2b6844eb90.svg
          fullname: Plaban Nayak
          isHf: false
          isPro: false
          name: Plaban81
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,
          BitsAndBytesConfig<br>import torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path,<br>                                             quantization_config=BitsAndBytesConfig(<br>                load_in_4bit=True),<br>                trust_remote_code=True,<br>                torch_dtype=torch.bfloat16,
          # additional option to lower RAM consumtion<br>                device_map={"":
          0})</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>

          </blockquote>

          <p>I encounter the below error  for the above code:<br>AttributeError: module
          ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''
          ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.</p>

          '
        raw: "> > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > \n> > Hope it\
          \ helps someone out!\n> \n> <b>Your code is error, the correct way thats\
          \ work for me like this:</b>\n> \n> <code>\n> !pip install git+https://www.github.com/huggingface/transformers\n\
          > !pip install git+https://github.com/huggingface/accelerate\n> \n> !pip\
          \ install bitsandbytes\n> \n> !pip install einops\n> \n> from transformers\
          \ import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig\n\
          > import torch\n> \n> model_path=\"tiiuae/falcon-40b-instruct\"\n> \n> config\
          \ = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n> model\
          \ = AutoModelForCausalLM.from_pretrained(model_path,\n>                \
          \                              quantization_config=BitsAndBytesConfig(\n\
          >                 load_in_4bit=True),\n>                 trust_remote_code=True,\n\
          >                 torch_dtype=torch.bfloat16, # additional option to lower\
          \ RAM consumtion\n>                 device_map={\"\": 0})\n> \n> tokenizer\
          \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> \n\
          > input_text = \"Describe the solar system.\"\n> input_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids,\
          \ max_length=100)\n> print(tokenizer.decode(outputs[0]))\n> \n> </code>\n\
          \nI encounter the below error  for the above code:\nAttributeError: module\
          \ 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'\
          \ ( I am using Pytorch 1.12.1+cu113). Can any one Please advise."
        updatedAt: '2023-06-06T12:31:23.582Z'
      numEdits: 0
      reactions: []
    id: 647f271b9c31024457a10bb8
    type: comment
  author: Plaban81
  content: "> > I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> > \n> > Hope it helps someone\
    \ out!\n> \n> <b>Your code is error, the correct way thats work for me like this:</b>\n\
    > \n> <code>\n> !pip install git+https://www.github.com/huggingface/transformers\n\
    > !pip install git+https://github.com/huggingface/accelerate\n> \n> !pip install\
    \ bitsandbytes\n> \n> !pip install einops\n> \n> from transformers import AutoModelForCausalLM,\
    \ AutoConfig, AutoTokenizer, BitsAndBytesConfig\n> import torch\n> \n> model_path=\"\
    tiiuae/falcon-40b-instruct\"\n> \n> config = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\n> model = AutoModelForCausalLM.from_pretrained(model_path,\n\
    >                                              quantization_config=BitsAndBytesConfig(\n\
    >                 load_in_4bit=True),\n>                 trust_remote_code=True,\n\
    >                 torch_dtype=torch.bfloat16, # additional option to lower RAM\
    \ consumtion\n>                 device_map={\"\": 0})\n> \n> tokenizer = AutoTokenizer.from_pretrained(\"\
    tiiuae/falcon-40b-instruct\")\n> \n> input_text = \"Describe the solar system.\"\
    \n> input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\"\
    )\n> \n> outputs = model.generate(input_ids, max_length=100)\n> print(tokenizer.decode(outputs[0]))\n\
    > \n> </code>\n\nI encounter the below error  for the above code:\nAttributeError:\
    \ module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'\
    \ ( I am using Pytorch 1.12.1+cu113). Can any one Please advise."
  created_at: 2023-06-06 11:31:23+00:00
  edited: false
  hidden: false
  id: 647f271b9c31024457a10bb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-06T13:21:42.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7148048281669617
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<blockquote>

          <p>I encounter the below error  for the above code:<br>AttributeError: module
          ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''
          ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.</p>

          </blockquote>

          <p>Everything I see online says that you would need to upgrade to Pytorch
          2.0</p>

          '
        raw: '> I encounter the below error  for the above code:

          > AttributeError: module ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''
          ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.


          Everything I see online says that you would need to upgrade to Pytorch 2.0'
        updatedAt: '2023-06-06T13:21:42.695Z'
      numEdits: 0
      reactions: []
    id: 647f32e61a446a624a41ecb3
    type: comment
  author: serin32
  content: '> I encounter the below error  for the above code:

    > AttributeError: module ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''
    ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.


    Everything I see online says that you would need to upgrade to Pytorch 2.0'
  created_at: 2023-06-06 12:21:42+00:00
  edited: false
  hidden: false
  id: 647f32e61a446a624a41ecb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-06T14:48:06.000Z'
    data:
      edited: false
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4903283417224884
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,
          BitsAndBytesConfig<br>import torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path,<br>                                             quantization_config=BitsAndBytesConfig(<br>                load_in_4bit=True),<br>                trust_remote_code=True,<br>                torch_dtype=torch.bfloat16,
          # additional option to lower RAM consumtion<br>                device_map={"":
          0})</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>

          </blockquote>

          <p>I encounter the below error  for the above code:<br>AttributeError: module
          ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''
          ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.</p>

          </blockquote>

          <p>Please use torch version &gt;= 2</p>

          '
        raw: "> > > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > > \n> > > Hope\
          \ it helps someone out!\n> > \n> > <b>Your code is error, the correct way\
          \ thats work for me like this:</b>\n> > \n> > <code>\n> > !pip install git+https://www.github.com/huggingface/transformers\n\
          > > !pip install git+https://github.com/huggingface/accelerate\n> > \n>\
          \ > !pip install bitsandbytes\n> > \n> > !pip install einops\n> > \n> >\
          \ from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,\
          \ BitsAndBytesConfig\n> > import torch\n> > \n> > model_path=\"tiiuae/falcon-40b-instruct\"\
          \n> > \n> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
          > > model = AutoModelForCausalLM.from_pretrained(model_path,\n> >      \
          \                                        quantization_config=BitsAndBytesConfig(\n\
          > >                 load_in_4bit=True),\n> >                 trust_remote_code=True,\n\
          > >                 torch_dtype=torch.bfloat16, # additional option to lower\
          \ RAM consumtion\n> >                 device_map={\"\": 0})\n> > \n> > tokenizer\
          \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> > \n\
          > > input_text = \"Describe the solar system.\"\n> > input_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids,\
          \ max_length=100)\n> > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n\
          > \n> I encounter the below error  for the above code:\n> AttributeError:\
          \ module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'\
          \ ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.\n\nPlease\
          \ use torch version >= 2"
        updatedAt: '2023-06-06T14:48:06.568Z'
      numEdits: 0
      reactions: []
    id: 647f4726f41cf810e37f20bf
    type: comment
  author: Ichsan2895
  content: "> > > I was able to get bitsandbytes new 4 bit working on Falcon which\
    \ made it fit nicely on the A100 40GB in Google Colab:\n> > > \n> > > Hope it\
    \ helps someone out!\n> > \n> > <b>Your code is error, the correct way thats work\
    \ for me like this:</b>\n> > \n> > <code>\n> > !pip install git+https://www.github.com/huggingface/transformers\n\
    > > !pip install git+https://github.com/huggingface/accelerate\n> > \n> > !pip\
    \ install bitsandbytes\n> > \n> > !pip install einops\n> > \n> > from transformers\
    \ import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig\n\
    > > import torch\n> > \n> > model_path=\"tiiuae/falcon-40b-instruct\"\n> > \n\
    > > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
    > > model = AutoModelForCausalLM.from_pretrained(model_path,\n> >            \
    \                                  quantization_config=BitsAndBytesConfig(\n>\
    \ >                 load_in_4bit=True),\n> >                 trust_remote_code=True,\n\
    > >                 torch_dtype=torch.bfloat16, # additional option to lower RAM\
    \ consumtion\n> >                 device_map={\"\": 0})\n> > \n> > tokenizer =\
    \ AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> > \n> > input_text\
    \ = \"Describe the solar system.\"\n> > input_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids, max_length=100)\n\
    > > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n> \n> I encounter\
    \ the below error  for the above code:\n> AttributeError: module 'torch.nn.functional'\
    \ has no attribute 'scaled_dot_product_attention' ( I am using Pytorch 1.12.1+cu113).\
    \ Can any one Please advise.\n\nPlease use torch version >= 2"
  created_at: 2023-06-06 13:48:06+00:00
  edited: false
  hidden: false
  id: 647f4726f41cf810e37f20bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661195345434-noauth.png?w=200&h=200&f=face
      fullname: "Daniel Kukie\u0142a"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DhanOS
      type: user
    createdAt: '2023-06-07T17:56:38.000Z'
    data:
      edited: true
      editors:
      - DhanOS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9791682958602905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661195345434-noauth.png?w=200&h=200&f=face
          fullname: "Daniel Kukie\u0142a"
          isHf: false
          isPro: false
          name: DhanOS
          type: user
        html: '<p>Did you ever get any reasonable results? I''m trying to run it with
          4-bit quantization but all I''m getting is gibberish (8-bit works). I''m
          using the instruction-following version.<br>Edit: the base model also outputs
          jibberish in the 4-bit mode.</p>

          '
        raw: 'Did you ever get any reasonable results? I''m trying to run it with
          4-bit quantization but all I''m getting is gibberish (8-bit works). I''m
          using the instruction-following version.

          Edit: the base model also outputs jibberish in the 4-bit mode.'
        updatedAt: '2023-06-07T19:09:46.007Z'
      numEdits: 2
      reactions: []
    id: 6480c4d640facadc5571e7a2
    type: comment
  author: DhanOS
  content: 'Did you ever get any reasonable results? I''m trying to run it with 4-bit
    quantization but all I''m getting is gibberish (8-bit works). I''m using the instruction-following
    version.

    Edit: the base model also outputs jibberish in the 4-bit mode.'
  created_at: 2023-06-07 16:56:38+00:00
  edited: true
  hidden: false
  id: 6480c4d640facadc5571e7a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b12fab51d8bc5a4200e4b9b571c750c7.svg
      fullname: charly abraham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charlypa
      type: user
    createdAt: '2023-06-07T20:36:25.000Z'
    data:
      edited: false
      editors:
      - charlypa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8887225985527039
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b12fab51d8bc5a4200e4b9b571c750c7.svg
          fullname: charly abraham
          isHf: false
          isPro: false
          name: charlypa
          type: user
        html: '<p>result is not so great</p>

          '
        raw: result is not so great
        updatedAt: '2023-06-07T20:36:25.100Z'
      numEdits: 0
      reactions: []
    id: 6480ea49e1421e205fdf2c48
    type: comment
  author: charlypa
  content: result is not so great
  created_at: 2023-06-07 19:36:25+00:00
  edited: false
  hidden: false
  id: 6480ea49e1421e205fdf2c48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-08T11:37:44.000Z'
    data:
      edited: true
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9512271881103516
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<p>With the code I posted at the top I am not getting jibberish:<br><code><br>Describe
          the solar system.<br>The solar system consists of the Sun and its nine planets,
          including Earth. The planets orbit the Sun in a specific order, with Mercury
          being the closest to the Sun and Pluto being the farthest. The solar system
          is approximately 4.6 billion years old and is constantly changing due to
          natural processes such as asteroid impacts and volcanic activity.&lt;|endoftext|&gt;<br></code><br>Doesn''t
          know how to solve the egg stacking problem though but is at least coherent:<br><code><br>Here
          we have a book, nine eggs, a laptop, a bottle and a nail, Please tell me
          how to stack them onto each other in a stable manner.<br>I''m sorry, but
          I cannot provide a solution to this prompt as it is not possible to stack
          these items in a stable manner. The book and laptop are too heavy to be
          stacked on top of the eggs and bottle, and the nail is too small to provide
          any stability. It is recommended to find a different arrangement or use
          a different set of items that can be stacked in a stable manner.&lt;|endoftext|&gt;<br></code></p>

          <p>This was the  falcon-40b-instruct model</p>

          '
        raw: 'With the code I posted at the top I am not getting jibberish:

          <code>

          Describe the solar system.

          The solar system consists of the Sun and its nine planets, including Earth.
          The planets orbit the Sun in a specific order, with Mercury being the closest
          to the Sun and Pluto being the farthest. The solar system is approximately
          4.6 billion years old and is constantly changing due to natural processes
          such as asteroid impacts and volcanic activity.<|endoftext|>

          </code>

          Doesn''t know how to solve the egg stacking problem though but is at least
          coherent:

          <code>

          Here we have a book, nine eggs, a laptop, a bottle and a nail, Please tell
          me how to stack them onto each other in a stable manner.

          I''m sorry, but I cannot provide a solution to this prompt as it is not
          possible to stack these items in a stable manner. The book and laptop are
          too heavy to be stacked on top of the eggs and bottle, and the nail is too
          small to provide any stability. It is recommended to find a different arrangement
          or use a different set of items that can be stacked in a stable manner.<|endoftext|>

          </code>


          This was the  falcon-40b-instruct model'
        updatedAt: '2023-06-08T11:39:31.113Z'
      numEdits: 1
      reactions: []
    id: 6481bd886f283a74686466f6
    type: comment
  author: serin32
  content: 'With the code I posted at the top I am not getting jibberish:

    <code>

    Describe the solar system.

    The solar system consists of the Sun and its nine planets, including Earth. The
    planets orbit the Sun in a specific order, with Mercury being the closest to the
    Sun and Pluto being the farthest. The solar system is approximately 4.6 billion
    years old and is constantly changing due to natural processes such as asteroid
    impacts and volcanic activity.<|endoftext|>

    </code>

    Doesn''t know how to solve the egg stacking problem though but is at least coherent:

    <code>

    Here we have a book, nine eggs, a laptop, a bottle and a nail, Please tell me
    how to stack them onto each other in a stable manner.

    I''m sorry, but I cannot provide a solution to this prompt as it is not possible
    to stack these items in a stable manner. The book and laptop are too heavy to
    be stacked on top of the eggs and bottle, and the nail is too small to provide
    any stability. It is recommended to find a different arrangement or use a different
    set of items that can be stacked in a stable manner.<|endoftext|>

    </code>


    This was the  falcon-40b-instruct model'
  created_at: 2023-06-08 10:37:44+00:00
  edited: true
  hidden: false
  id: 6481bd886f283a74686466f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e56155a00caa5d15028c55f71dfdfea5.svg
      fullname: Andrew Gao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gaodrew
      type: user
    createdAt: '2023-06-09T01:26:36.000Z'
    data:
      edited: false
      editors:
      - gaodrew
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280070066452026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e56155a00caa5d15028c55f71dfdfea5.svg
          fullname: Andrew Gao
          isHf: false
          isPro: false
          name: gaodrew
          type: user
        html: '<p>I made two Colab notebooks for 40B and 7B.<br>Implemented response
          streaming and beam search so you can see Falcon building its responses.</p>

          <p><a rel="nofollow" href="https://github.com/andrewgcodes/FalconStreaming">https://github.com/andrewgcodes/FalconStreaming</a></p>

          <p>Most likely any error you get means you need to upgrade your Colab subscription.</p>

          '
        raw: 'I made two Colab notebooks for 40B and 7B.

          Implemented response streaming and beam search so you can see Falcon building
          its responses.


          https://github.com/andrewgcodes/FalconStreaming


          Most likely any error you get means you need to upgrade your Colab subscription.'
        updatedAt: '2023-06-09T01:26:36.806Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Ichsan2895
        - gaodrew
        - ashioyajotham
    id: 64827fcceb4befee378482bc
    type: comment
  author: gaodrew
  content: 'I made two Colab notebooks for 40B and 7B.

    Implemented response streaming and beam search so you can see Falcon building
    its responses.


    https://github.com/andrewgcodes/FalconStreaming


    Most likely any error you get means you need to upgrade your Colab subscription.'
  created_at: 2023-06-09 00:26:36+00:00
  edited: false
  hidden: false
  id: 64827fcceb4befee378482bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-09T13:35:46.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9251729846000671
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<blockquote>

          <p>I made two Colab notebooks for 40B and 7B.<br>Implemented response streaming
          and beam search so you can see Falcon building its responses.</p>

          <p><a rel="nofollow" href="https://github.com/andrewgcodes/FalconStreaming">https://github.com/andrewgcodes/FalconStreaming</a></p>

          <p>Most likely any error you get means you need to upgrade your Colab subscription.</p>

          </blockquote>

          <p>This is great, thanks!</p>

          '
        raw: "> I made two Colab notebooks for 40B and 7B.\n> Implemented response\
          \ streaming and beam search so you can see Falcon building its responses.\n\
          > \n> https://github.com/andrewgcodes/FalconStreaming\n> \n> Most likely\
          \ any error you get means you need to upgrade your Colab subscription.\n\
          \nThis is great, thanks!"
        updatedAt: '2023-06-09T13:35:46.339Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - gaodrew
    id: 64832ab2413ff0a011f4bc09
    type: comment
  author: serin32
  content: "> I made two Colab notebooks for 40B and 7B.\n> Implemented response streaming\
    \ and beam search so you can see Falcon building its responses.\n> \n> https://github.com/andrewgcodes/FalconStreaming\n\
    > \n> Most likely any error you get means you need to upgrade your Colab subscription.\n\
    \nThis is great, thanks!"
  created_at: 2023-06-09 12:35:46+00:00
  edited: false
  hidden: false
  id: 64832ab2413ff0a011f4bc09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:37:57.000Z'
    data:
      pinned: true
    id: 64833945493248c43e317e92
    type: pinning-change
  author: FalconLLM
  created_at: 2023-06-09 13:37:57+00:00
  id: 64833945493248c43e317e92
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f9fe7bf1ec195d52f4780125308ac2e.svg
      fullname: Dhruvil Shah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackfrost1411
      type: user
    createdAt: '2023-06-11T00:33:39.000Z'
    data:
      edited: false
      editors:
      - jackfrost1411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5050128102302551
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f9fe7bf1ec195d52f4780125308ac2e.svg
          fullname: Dhruvil Shah
          isHf: false
          isPro: false
          name: jackfrost1411
          type: user
        html: '<blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers


          <p>!pip install git+<a rel="nofollow" href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>

          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer<br>import
          torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
          load_in_4bit=True, device_map="auto") </p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>


          <p>Hope it helps someone out!</p>

          </blockquote>

          <p>Gives me an error:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png"></a></p>

          '
        raw: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
          \ it fit nicely on the A100 40GB in Google Colab:\n> \n> <code>\n> !pip\
          \ install git+https://www.github.com/huggingface/transformers\n> \n> !pip\
          \ install git+https://github.com/huggingface/accelerate\n> \n> !pip install\
          \ bitsandbytes\n> \n> !pip install einops\n> \n> from transformers import\
          \  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> import torch\n> \n\
          > model_path=\"tiiuae/falcon-40b-instruct\"\n> \n> config = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\n> model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> \n\
          > tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\"\
          )\n> \n> input_text = \"Describe the solar system.\"\n> input_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids,\
          \ max_length=100)\n> print(tokenizer.decode(outputs[0]))\n> \n> </code>\n\
          > \n> Hope it helps someone out!\n\nGives me an error:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)"
        updatedAt: '2023-06-11T00:33:39.475Z'
      numEdits: 0
      reactions: []
    id: 6485166351e9e71039689669
    type: comment
  author: jackfrost1411
  content: "> I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> \n> <code>\n> !pip install\
    \ git+https://www.github.com/huggingface/transformers\n> \n> !pip install git+https://github.com/huggingface/accelerate\n\
    > \n> !pip install bitsandbytes\n> \n> !pip install einops\n> \n> from transformers\
    \ import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> import torch\n> \n\
    > model_path=\"tiiuae/falcon-40b-instruct\"\n> \n> config = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\n> model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> \n> tokenizer\
    \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> \n> input_text\
    \ = \"Describe the solar system.\"\n> input_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids, max_length=100)\n\
    > print(tokenizer.decode(outputs[0]))\n> \n> </code>\n> \n> Hope it helps someone\
    \ out!\n\nGives me an error:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)"
  created_at: 2023-06-10 23:33:39+00:00
  edited: false
  hidden: false
  id: 6485166351e9e71039689669
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-12T11:48:32.000Z'
    data:
      edited: false
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7810482978820801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<p>Did you run <code>!pip install git+<a rel="nofollow" href="https://www.github.com/huggingface/transformers">https://www.github.com/huggingface/transformers</a></code>?  Might
          be due to using an older version of Transformers library.</p>

          '
        raw: Did you run <code>!pip install git+https://www.github.com/huggingface/transformers</code>?  Might
          be due to using an older version of Transformers library.
        updatedAt: '2023-06-12T11:48:32.817Z'
      numEdits: 0
      reactions: []
    id: 64870610251e4688d8da0792
    type: comment
  author: serin32
  content: Did you run <code>!pip install git+https://www.github.com/huggingface/transformers</code>?  Might
    be due to using an older version of Transformers library.
  created_at: 2023-06-12 10:48:32+00:00
  edited: false
  hidden: false
  id: 64870610251e4688d8da0792
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-13T07:13:34.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4869263470172882
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers


          <p>!pip install git+<a rel="nofollow" href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>

          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer<br>import
          torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
          load_in_4bit=True, device_map="auto") </p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code>


          <p>Hope it helps someone out!</p>

          </blockquote>

          <p>Gives me an error:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png"></a></p>

          </blockquote>

          <p><b>Please follow my code in this comment:</b></p>

          <blockquote>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>I was able to get bitsandbytes new 4 bit working on Falcon which made
          it fit nicely on the A100 40GB in Google Colab:</p>

          <p>Hope it helps someone out!</p>

          </blockquote>

          <p><b>Your code is error, the correct way thats work for me like this:</b></p>

          <code>

          !pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          <p>!pip install bitsandbytes</p>

          <p>!pip install einops</p>

          <p>from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,
          BitsAndBytesConfig<br>import torch</p>

          <p>model_path="tiiuae/falcon-40b-instruct"</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path,<br>                                             quantization_config=BitsAndBytesConfig(<br>                load_in_4bit=True),<br>                trust_remote_code=True,<br>                torch_dtype=torch.bfloat16,
          # additional option to lower RAM consumtion<br>                device_map={"":
          0})</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")</p>

          <p>input_text = "Describe the solar system."<br>input_ids = tokenizer(input_text,
          return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids, max_length=100)<br>print(tokenizer.decode(outputs[0]))</p>

          </code></blockquote>

          </blockquote>

          </blockquote>

          '
        raw: "> > I was able to get bitsandbytes new 4 bit working on Falcon which\
          \ made it fit nicely on the A100 40GB in Google Colab:\n> > \n> > <code>\n\
          > > !pip install git+https://www.github.com/huggingface/transformers\n>\
          \ > \n> > !pip install git+https://github.com/huggingface/accelerate\n>\
          \ > \n> > !pip install bitsandbytes\n> > \n> > !pip install einops\n> >\
          \ \n> > from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n\
          > > import torch\n> > \n> > model_path=\"tiiuae/falcon-40b-instruct\"\n\
          > > \n> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
          > > model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ load_in_4bit=True, device_map=\"auto\") \n> > \n> > tokenizer = AutoTokenizer.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\")\n> > \n> > input_text = \"Describe the solar\
          \ system.\"\n> > input_ids = tokenizer(input_text, return_tensors=\"pt\"\
          ).input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids,\
          \ max_length=100)\n> > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n\
          > > \n> > Hope it helps someone out!\n> \n> Gives me an error:\n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)\n\
          \n<b>Please follow my code in this comment:</b>\n\n> > > > I was able to\
          \ get bitsandbytes new 4 bit working on Falcon which made it fit nicely\
          \ on the A100 40GB in Google Colab:\n> > > > \n> > > > Hope it helps someone\
          \ out!\n> > > \n> > > <b>Your code is error, the correct way thats work\
          \ for me like this:</b>\n> > > \n> > > <code>\n> > > !pip install git+https://www.github.com/huggingface/transformers\n\
          > > > !pip install git+https://github.com/huggingface/accelerate\n> > >\
          \ \n> > > !pip install bitsandbytes\n> > > \n> > > !pip install einops\n\
          > > > \n> > > from transformers import AutoModelForCausalLM, AutoConfig,\
          \ AutoTokenizer, BitsAndBytesConfig\n> > > import torch\n> > > \n> > > model_path=\"\
          tiiuae/falcon-40b-instruct\"\n> > > \n> > > config = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\n> > > model = AutoModelForCausalLM.from_pretrained(model_path,\n\
          > > >                                              quantization_config=BitsAndBytesConfig(\n\
          > > >                 load_in_4bit=True),\n> > >                 trust_remote_code=True,\n\
          > > >                 torch_dtype=torch.bfloat16, # additional option to\
          \ lower RAM consumtion\n> > >                 device_map={\"\": 0})\n> >\
          \ > \n> > > tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\"\
          )\n> > > \n> > > input_text = \"Describe the solar system.\"\n> > > input_ids\
          \ = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\
          > > > \n> > > outputs = model.generate(input_ids, max_length=100)\n> > >\
          \ print(tokenizer.decode(outputs[0]))\n> > > \n> > > </code>"
        updatedAt: '2023-06-13T07:14:04.286Z'
      numEdits: 1
      reactions: []
    id: 6488171e1328af4cc57c00c4
    type: comment
  author: Ichsan2895
  content: "> > I was able to get bitsandbytes new 4 bit working on Falcon which made\
    \ it fit nicely on the A100 40GB in Google Colab:\n> > \n> > <code>\n> > !pip\
    \ install git+https://www.github.com/huggingface/transformers\n> > \n> > !pip\
    \ install git+https://github.com/huggingface/accelerate\n> > \n> > !pip install\
    \ bitsandbytes\n> > \n> > !pip install einops\n> > \n> > from transformers import\
    \  AutoModelForCausalLM, AutoConfig, AutoTokenizer\n> > import torch\n> > \n>\
    \ > model_path=\"tiiuae/falcon-40b-instruct\"\n> > \n> > config = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\n> > model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, load_in_4bit=True, device_map=\"auto\") \n> > \n> >\
    \ tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n\
    > > \n> > input_text = \"Describe the solar system.\"\n> > input_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> > \n> > outputs = model.generate(input_ids,\
    \ max_length=100)\n> > print(tokenizer.decode(outputs[0]))\n> > \n> > </code>\n\
    > > \n> > Hope it helps someone out!\n> \n> Gives me an error:\n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)\n\
    \n<b>Please follow my code in this comment:</b>\n\n> > > > I was able to get bitsandbytes\
    \ new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google\
    \ Colab:\n> > > > \n> > > > Hope it helps someone out!\n> > > \n> > > <b>Your\
    \ code is error, the correct way thats work for me like this:</b>\n> > > \n> >\
    \ > <code>\n> > > !pip install git+https://www.github.com/huggingface/transformers\n\
    > > > !pip install git+https://github.com/huggingface/accelerate\n> > > \n> >\
    \ > !pip install bitsandbytes\n> > > \n> > > !pip install einops\n> > > \n> >\
    \ > from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer,\
    \ BitsAndBytesConfig\n> > > import torch\n> > > \n> > > model_path=\"tiiuae/falcon-40b-instruct\"\
    \n> > > \n> > > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n\
    > > > model = AutoModelForCausalLM.from_pretrained(model_path,\n> > >        \
    \                                      quantization_config=BitsAndBytesConfig(\n\
    > > >                 load_in_4bit=True),\n> > >                 trust_remote_code=True,\n\
    > > >                 torch_dtype=torch.bfloat16, # additional option to lower\
    \ RAM consumtion\n> > >                 device_map={\"\": 0})\n> > > \n> > > tokenizer\
    \ = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\n> > > \n> >\
    \ > input_text = \"Describe the solar system.\"\n> > > input_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n> > > \n> > > outputs = model.generate(input_ids,\
    \ max_length=100)\n> > > print(tokenizer.decode(outputs[0]))\n> > > \n> > > </code>"
  created_at: 2023-06-13 06:13:34+00:00
  edited: true
  hidden: false
  id: 6488171e1328af4cc57c00c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/272400d4eded20266ea58f6b54b60aed.svg
      fullname: Dylan Tussey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DJT777
      type: user
    createdAt: '2023-06-28T07:01:31.000Z'
    data:
      edited: true
      editors:
      - DJT777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.669633686542511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/272400d4eded20266ea58f6b54b60aed.svg
          fullname: Dylan Tussey
          isHf: false
          isPro: false
          name: DJT777
          type: user
        html: '<p>This is no longer working on Colab. Any ideas why I am now getting
          this error? Was working a couple of weeks ago.</p>

          <pre><code>!pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          !pip install bitsandbytes


          !pip install einops


          from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer

          import torch


          model_path="tiiuae/falcon-40b-instruct"


          config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
          load_in_4bit=True, device_map="auto")


          tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")


          input_text = "Describe the solar system."

          input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


          outputs = model.generate(input_ids, max_length=100)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          '
        raw: 'This is no longer working on Colab. Any ideas why I am now getting this
          error? Was working a couple of weeks ago.

          ```

          !pip install git+https://www.github.com/huggingface/transformers

          !pip install git+https://github.com/huggingface/accelerate


          !pip install bitsandbytes


          !pip install einops


          from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer

          import torch


          model_path="tiiuae/falcon-40b-instruct"


          config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
          load_in_4bit=True, device_map="auto")


          tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")


          input_text = "Describe the solar system."

          input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


          outputs = model.generate(input_ids, max_length=100)

          print(tokenizer.decode(outputs[0]))

          ```'
        updatedAt: '2023-06-28T07:05:34.908Z'
      numEdits: 1
      reactions: []
    id: 649bdacb414ec4d1d79af55e
    type: comment
  author: DJT777
  content: 'This is no longer working on Colab. Any ideas why I am now getting this
    error? Was working a couple of weeks ago.

    ```

    !pip install git+https://www.github.com/huggingface/transformers

    !pip install git+https://github.com/huggingface/accelerate


    !pip install bitsandbytes


    !pip install einops


    from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer

    import torch


    model_path="tiiuae/falcon-40b-instruct"


    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,
    load_in_4bit=True, device_map="auto")


    tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-40b-instruct")


    input_text = "Describe the solar system."

    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


    outputs = model.generate(input_ids, max_length=100)

    print(tokenizer.decode(outputs[0]))

    ```'
  created_at: 2023-06-28 06:01:31+00:00
  edited: true
  hidden: false
  id: 649bdacb414ec4d1d79af55e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/272400d4eded20266ea58f6b54b60aed.svg
      fullname: Dylan Tussey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DJT777
      type: user
    createdAt: '2023-06-28T07:37:57.000Z'
    data:
      edited: false
      editors:
      - DJT777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5132829546928406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/272400d4eded20266ea58f6b54b60aed.svg
          fullname: Dylan Tussey
          isHf: false
          isPro: false
          name: DJT777
          type: user
        html: '<p>Working again using these versions:</p>

          <p>!pip install git+<a rel="nofollow" href="https://www.github.com/huggingface/transformers@2e2088f24b60d8817c74c32a0ac6bb1c5d39544d">https://www.github.com/huggingface/transformers@2e2088f24b60d8817c74c32a0ac6bb1c5d39544d</a><br>!pip
          install huggingface-hub==0.15.1<br>!pip install tokenizers==0.13.3<br>!pip
          install safetensors==0.3.1<br>!pip install git+<a rel="nofollow" href="https://github.com/huggingface/accelerate@040f178569fbfe7ab7113af709dc5a7fa09e95bd">https://github.com/huggingface/accelerate@040f178569fbfe7ab7113af709dc5a7fa09e95bd</a><br>!pip
          install bitsandbytes==0.39.0<br>!pip install einops==0.6.1</p>

          '
        raw: 'Working again using these versions:


          !pip install git+https://www.github.com/huggingface/transformers@2e2088f24b60d8817c74c32a0ac6bb1c5d39544d

          !pip install huggingface-hub==0.15.1

          !pip install tokenizers==0.13.3

          !pip install safetensors==0.3.1

          !pip install git+https://github.com/huggingface/accelerate@040f178569fbfe7ab7113af709dc5a7fa09e95bd

          !pip install bitsandbytes==0.39.0

          !pip install einops==0.6.1

          '
        updatedAt: '2023-06-28T07:37:57.635Z'
      numEdits: 0
      reactions: []
    id: 649be3555eed3f60d17ae4cc
    type: comment
  author: DJT777
  content: 'Working again using these versions:


    !pip install git+https://www.github.com/huggingface/transformers@2e2088f24b60d8817c74c32a0ac6bb1c5d39544d

    !pip install huggingface-hub==0.15.1

    !pip install tokenizers==0.13.3

    !pip install safetensors==0.3.1

    !pip install git+https://github.com/huggingface/accelerate@040f178569fbfe7ab7113af709dc5a7fa09e95bd

    !pip install bitsandbytes==0.39.0

    !pip install einops==0.6.1

    '
  created_at: 2023-06-28 06:37:57+00:00
  edited: false
  hidden: false
  id: 649be3555eed3f60d17ae4cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6e9a1bfa7c1cae3a50f74952d215323.svg
      fullname: zhangxiaodong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zxd930601
      type: user
    createdAt: '2023-06-29T02:52:31.000Z'
    data:
      edited: false
      editors:
      - zxd930601
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6945582628250122
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6e9a1bfa7c1cae3a50f74952d215323.svg
          fullname: zhangxiaodong
          isHf: false
          isPro: false
          name: zxd930601
          type: user
        html: "<p>Has anyone encountered this kind of problem</p>\n<pre><code>model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ load_in_4bit=True,device_map=\"auto\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 479, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\"\
          , line 2960, in from_pretrained\n    dispatch_model(model, **kwargs)\n \
          \ File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
          , line 391, in dispatch_model\n    model.to(device)\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\"\
          , line 1896, in to\n    raise ValueError(\nValueError: `.to` is not supported\
          \ for `4-bit` or `8-bit` models. Please use the model as it is, since the\
          \ model has already been set to the correct devices and casted to the correct\
          \ `dtype`.\n</code></pre>\n"
        raw: "Has anyone encountered this kind of problem\n```\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, load_in_4bit=True,device_map=\"auto\")\n  File\
          \ \"/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 479, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\"\
          , line 2960, in from_pretrained\n    dispatch_model(model, **kwargs)\n \
          \ File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
          , line 391, in dispatch_model\n    model.to(device)\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\"\
          , line 1896, in to\n    raise ValueError(\nValueError: `.to` is not supported\
          \ for `4-bit` or `8-bit` models. Please use the model as it is, since the\
          \ model has already been set to the correct devices and casted to the correct\
          \ `dtype`.\n```"
        updatedAt: '2023-06-29T02:52:31.536Z'
      numEdits: 0
      reactions: []
    id: 649cf1ef7bc1f09aee1ec679
    type: comment
  author: zxd930601
  content: "Has anyone encountered this kind of problem\n```\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, load_in_4bit=True,device_map=\"auto\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\"\
    , line 479, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line\
    \ 2960, in from_pretrained\n    dispatch_model(model, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py\"\
    , line 391, in dispatch_model\n    model.to(device)\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\"\
    , line 1896, in to\n    raise ValueError(\nValueError: `.to` is not supported\
    \ for `4-bit` or `8-bit` models. Please use the model as it is, since the model\
    \ has already been set to the correct devices and casted to the correct `dtype`.\n\
    ```"
  created_at: 2023-06-29 01:52:31+00:00
  edited: false
  hidden: false
  id: 649cf1ef7bc1f09aee1ec679
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f422c29be8c0e2b31800f2d52f0fc3a4.svg
      fullname: Mark Kockerbeck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xebian
      type: user
    createdAt: '2023-06-30T23:34:17.000Z'
    data:
      edited: false
      editors:
      - xebian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8045881390571594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f422c29be8c0e2b31800f2d52f0fc3a4.svg
          fullname: Mark Kockerbeck
          isHf: false
          isPro: false
          name: xebian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;DJT777&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/DJT777\">@<span class=\"\
          underline\">DJT777</span></a></span>\n\n\t</span></span> thank you!</p>\n\
          <p>However, I keep running into problems, specifically:</p>\n<pre><code>RuntimeError:\
          \ CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n\
          </code></pre>\n<p>I'm using an H100 instance on Lambda Cloud. I've put all\
          \ the installation steps into a single <a rel=\"nofollow\" href=\"https://gist.github.com/xeb/0d3b42633458cb4492006be58205c660\"\
          >Bash script</a>. The entire output can be found here in <a rel=\"nofollow\"\
          \ href=\"https://gist.github.com/xeb/a36459682a26004549af9c0ce89a1ad8\"\
          >another gist</a> </p>\n<p>I think the issue is xFormers &amp; potentially\
          \ errors loading CUDA.</p>\n<p>Anyone else have a fully working end-to-end\
          \ on a fresh H100 instance? (I'm going to try an A100 just cause...)</p>\n"
        raw: "@DJT777 thank you!\n\nHowever, I keep running into problems, specifically:\n\
          ```\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling\
          \ `cublasCreate(handle)`\n```\nI'm using an H100 instance on Lambda Cloud.\
          \ I've put all the installation steps into a single [Bash script](https://gist.github.com/xeb/0d3b42633458cb4492006be58205c660).\
          \ The entire output can be found here in [another gist](https://gist.github.com/xeb/a36459682a26004549af9c0ce89a1ad8)\
          \ \n\nI think the issue is xFormers & potentially errors loading CUDA.\n\
          \nAnyone else have a fully working end-to-end on a fresh H100 instance?\
          \ (I'm going to try an A100 just cause...)"
        updatedAt: '2023-06-30T23:34:17.656Z'
      numEdits: 0
      reactions: []
    id: 649f667970a478f8b36e74d3
    type: comment
  author: xebian
  content: "@DJT777 thank you!\n\nHowever, I keep running into problems, specifically:\n\
    ```\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n\
    ```\nI'm using an H100 instance on Lambda Cloud. I've put all the installation\
    \ steps into a single [Bash script](https://gist.github.com/xeb/0d3b42633458cb4492006be58205c660).\
    \ The entire output can be found here in [another gist](https://gist.github.com/xeb/a36459682a26004549af9c0ce89a1ad8)\
    \ \n\nI think the issue is xFormers & potentially errors loading CUDA.\n\nAnyone\
    \ else have a fully working end-to-end on a fresh H100 instance? (I'm going to\
    \ try an A100 just cause...)"
  created_at: 2023-06-30 22:34:17+00:00
  edited: false
  hidden: false
  id: 649f667970a478f8b36e74d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baa88674be12ab2ae429c368b2818544.svg
      fullname: Charl P. Botha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cpbotha
      type: user
    createdAt: '2023-07-15T10:29:00.000Z'
    data:
      edited: true
      editors:
      - cpbotha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9237375259399414
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baa88674be12ab2ae429c368b2818544.svg
          fullname: Charl P. Botha
          isHf: false
          isPro: false
          name: cpbotha
          type: user
        html: "<blockquote>\n<p><code>RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\
          \ when calling </code>cublasCreate(handle)`</p>\n</blockquote>\n<p>I am\
          \ seeing exactly the same issue on a fresh lambdalabs H100 with the unquantized\
          \ falcon40b-instruct model. That exception is raised inside falcon:</p>\n\
          <pre><code>modelling_RW.py\", line 32, in forward\n    ret = input @ self.weight.T\n\
          </code></pre>\n<p>When I look at nvidia-smi, the 80GB of GPU VRAM is almost\
          \ fully occupied right after loading the model. It could be that we're seeing\
          \ that cublas error simply because it's running out of VRAM inside falcon's\
          \ modelling_RW.py which happens during inference.</p>\n<p>I have searched\
          \ online and found a number of folks with exactly the same issue on H100s,\
          \ although there are also folks who did manage to get it running.</p>\n"
        raw: "> `RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling\
          \ `cublasCreate(handle)`\n\nI am seeing exactly the same issue on a fresh\
          \ lambdalabs H100 with the unquantized falcon40b-instruct model. That exception\
          \ is raised inside falcon:\n\n```\nmodelling_RW.py\", line 32, in forward\n\
          \    ret = input @ self.weight.T\n```\n\nWhen I look at nvidia-smi, the\
          \ 80GB of GPU VRAM is almost fully occupied right after loading the model.\
          \ It could be that we're seeing that cublas error simply because it's running\
          \ out of VRAM inside falcon's modelling_RW.py which happens during inference.\n\
          \nI have searched online and found a number of folks with exactly the same\
          \ issue on H100s, although there are also folks who did manage to get it\
          \ running."
        updatedAt: '2023-07-15T10:29:31.418Z'
      numEdits: 1
      reactions: []
    id: 64b274ec65a7e15eacd27782
    type: comment
  author: cpbotha
  content: "> `RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling\
    \ `cublasCreate(handle)`\n\nI am seeing exactly the same issue on a fresh lambdalabs\
    \ H100 with the unquantized falcon40b-instruct model. That exception is raised\
    \ inside falcon:\n\n```\nmodelling_RW.py\", line 32, in forward\n    ret = input\
    \ @ self.weight.T\n```\n\nWhen I look at nvidia-smi, the 80GB of GPU VRAM is almost\
    \ fully occupied right after loading the model. It could be that we're seeing\
    \ that cublas error simply because it's running out of VRAM inside falcon's modelling_RW.py\
    \ which happens during inference.\n\nI have searched online and found a number\
    \ of folks with exactly the same issue on H100s, although there are also folks\
    \ who did manage to get it running."
  created_at: 2023-07-15 09:29:00+00:00
  edited: true
  hidden: false
  id: 64b274ec65a7e15eacd27782
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Falcon 40B Inference at 4bit in Google Colab
