!!python/object:huggingface_hub.community.DiscussionWithDetails
author: puru22
conflicting_files:
- modelling_RW.py
created_at: 2023-07-12 06:12:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-07-12T07:12:40.000Z'
    data:
      edited: false
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8414334058761597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: '<p>The current code has missed out passing past_key_values in every
          forward pass for fast generation of tokens. This results in lot of recompute.
          This "modelling_RW.py" I am uploading deals with this in the way pytorch
          huggingface transformers package generation/utils.py wants. All the changes
          are basically around including past_key_values everywhere. I think this
          will apply on all falcon models These are the changes specifically. The
          same changes apply to pretty much all of the falcon family models with slow
          generation.</p>

          <p>Class RotaryEmbedding forward method<br>Include past_seq_length in forward
          pass and apply rotary embedding according to the position of the query token
          ---- if else condition added (line number 98-101)</p>

          <p>_make_causal_mask function<br>to give masking according to the way F.scaled
          dot product attention behaves. F.scaled_dot_product attention treats the
          attention_mask matrix as receiving attentions. For example if attention_mask
          is<br>[[True, False], [True, True]]. It would mean the first token is "receiving"
          attentions from first token and not second token. This is unlike what we
          generally end up thinking which is first token is giving attention to itself
          and not to the second one. Due to reason the past_key_values attentions
          are all True in make_causal mask function. Also I have reversed the inequality
          above that due to the same reason. ---- (line number 111 inequality, line
          number 114 attention mask to be True)</p>

          <p>Class Attention forward method<br>a) past_key_value length is passed
          in rotary function ---- if,else loop added (line number 276-280)<br>b) concatenation
          of past key and current key is done after permuting the past key shape to
          match the current key shape ---- (line number 283-290)<br>c) to keep key_layer
          shape consistent with the output expectation which is (batch_size, head_dim,
          seq_length), another permutation done before creating "present" to return
          in the output ---- (line number 294-298)<br>d)add an if else depending on
          whether attention mask has been created or not, currently it just ignores
          ---- (line number 305-311)</p>

          <p>Class RWModel prepare_attn_mask method<br>Have removed src_length &gt;
          1 criteria for making causal mask (line number 554).</p>

          <p>RW causal LM prepare inputs for generation<br>Read pastkey values from
          the input coming from huggingface generate method and dont call convert_to_rw_cache
          method (line number 749-757)</p>

          '
        raw: 'The current code has missed out passing past_key_values in every forward
          pass for fast generation of tokens. This results in lot of recompute. This
          "modelling_RW.py" I am uploading deals with this in the way pytorch huggingface
          transformers package generation/utils.py wants. All the changes are basically
          around including past_key_values everywhere. I think this will apply on
          all falcon models These are the changes specifically. The same changes apply
          to pretty much all of the falcon family models with slow generation.


          Class RotaryEmbedding forward method

          Include past_seq_length in forward pass and apply rotary embedding according
          to the position of the query token ---- if else condition added (line number
          98-101)


          _make_causal_mask function

          to give masking according to the way F.scaled dot product attention behaves.
          F.scaled_dot_product attention treats the attention_mask matrix as receiving
          attentions. For example if attention_mask is

          [[True, False], [True, True]]. It would mean the first token is "receiving"
          attentions from first token and not second token. This is unlike what we
          generally end up thinking which is first token is giving attention to itself
          and not to the second one. Due to reason the past_key_values attentions
          are all True in make_causal mask function. Also I have reversed the inequality
          above that due to the same reason. ---- (line number 111 inequality, line
          number 114 attention mask to be True)


          Class Attention forward method

          a) past_key_value length is passed in rotary function ---- if,else loop
          added (line number 276-280)

          b) concatenation of past key and current key is done after permuting the
          past key shape to match the current key shape ---- (line number 283-290)

          c) to keep key_layer shape consistent with the output expectation which
          is (batch_size, head_dim, seq_length), another permutation done before creating
          "present" to return in the output ---- (line number 294-298)

          d)add an if else depending on whether attention mask has been created or
          not, currently it just ignores ---- (line number 305-311)


          Class RWModel prepare_attn_mask method

          Have removed src_length > 1 criteria for making causal mask (line number
          554).


          RW causal LM prepare inputs for generation

          Read pastkey values from the input coming from huggingface generate method
          and dont call convert_to_rw_cache method (line number 749-757)'
        updatedAt: '2023-07-12T07:12:40.046Z'
      numEdits: 0
      reactions: []
    id: 64ae5268179421d320b71297
    type: comment
  author: puru22
  content: 'The current code has missed out passing past_key_values in every forward
    pass for fast generation of tokens. This results in lot of recompute. This "modelling_RW.py"
    I am uploading deals with this in the way pytorch huggingface transformers package
    generation/utils.py wants. All the changes are basically around including past_key_values
    everywhere. I think this will apply on all falcon models These are the changes
    specifically. The same changes apply to pretty much all of the falcon family models
    with slow generation.


    Class RotaryEmbedding forward method

    Include past_seq_length in forward pass and apply rotary embedding according to
    the position of the query token ---- if else condition added (line number 98-101)


    _make_causal_mask function

    to give masking according to the way F.scaled dot product attention behaves. F.scaled_dot_product
    attention treats the attention_mask matrix as receiving attentions. For example
    if attention_mask is

    [[True, False], [True, True]]. It would mean the first token is "receiving" attentions
    from first token and not second token. This is unlike what we generally end up
    thinking which is first token is giving attention to itself and not to the second
    one. Due to reason the past_key_values attentions are all True in make_causal
    mask function. Also I have reversed the inequality above that due to the same
    reason. ---- (line number 111 inequality, line number 114 attention mask to be
    True)


    Class Attention forward method

    a) past_key_value length is passed in rotary function ---- if,else loop added
    (line number 276-280)

    b) concatenation of past key and current key is done after permuting the past
    key shape to match the current key shape ---- (line number 283-290)

    c) to keep key_layer shape consistent with the output expectation which is (batch_size,
    head_dim, seq_length), another permutation done before creating "present" to return
    in the output ---- (line number 294-298)

    d)add an if else depending on whether attention mask has been created or not,
    currently it just ignores ---- (line number 305-311)


    Class RWModel prepare_attn_mask method

    Have removed src_length > 1 criteria for making causal mask (line number 554).


    RW causal LM prepare inputs for generation

    Read pastkey values from the input coming from huggingface generate method and
    dont call convert_to_rw_cache method (line number 749-757)'
  created_at: 2023-07-12 06:12:40+00:00
  edited: false
  hidden: false
  id: 64ae5268179421d320b71297
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-07-12T07:12:40.000Z'
    data:
      oid: 88b2700ac32418514bc1a13e0ec9eabcdf7874a6
      parents:
      - c47b371b31a68349c233104050ac76680b8485db
      subject: Changes in modelling_RW.py to be able to handle past_key_values for
        faster model generations
    id: 64ae52680000000000000000
    type: commit
  author: puru22
  created_at: 2023-07-12 06:12:40+00:00
  id: 64ae52680000000000000000
  oid: 88b2700ac32418514bc1a13e0ec9eabcdf7874a6
  summary: Changes in modelling_RW.py to be able to handle past_key_values for faster
    model generations
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e4a00e7ac240c3a48adefa5154edc46.svg
      fullname: sdfds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sfd
      type: user
    createdAt: '2023-08-13T11:42:37.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/5e4a00e7ac240c3a48adefa5154edc46.svg
          fullname: sdfds
          isHf: false
          isPro: false
          name: sfd
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-13T12:04:23.973Z'
      numEdits: 0
      reactions: []
    id: 64d8c1ad6db135cfc8eebf5b
    type: comment
  author: sfd
  content: This comment has been hidden
  created_at: 2023-08-13 10:42:37+00:00
  edited: true
  hidden: true
  id: 64d8c1ad6db135cfc8eebf5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8ed016c6d496e6205f60b3e26ea4586.svg
      fullname: Yoyo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YoYo1234Qwerty
      type: user
    createdAt: '2023-08-16T17:29:41.000Z'
    data:
      edited: false
      editors:
      - YoYo1234Qwerty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772563576698303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8ed016c6d496e6205f60b3e26ea4586.svg
          fullname: Yoyo
          isHf: false
          isPro: false
          name: YoYo1234Qwerty
          type: user
        html: '<p>Ya, Falcon seemed slow af, and this seemed to help, thanks!</p>

          <p>Is this why Llama is beating Falcon, because they''re overlooking simple
          things? I''m quite surprised since this has far superior licensing to Llama.</p>

          '
        raw: 'Ya, Falcon seemed slow af, and this seemed to help, thanks!


          Is this why Llama is beating Falcon, because they''re overlooking simple
          things? I''m quite surprised since this has far superior licensing to Llama.'
        updatedAt: '2023-08-16T17:29:41.588Z'
      numEdits: 0
      reactions: []
    id: 64dd07854e2c7863c3155073
    type: comment
  author: YoYo1234Qwerty
  content: 'Ya, Falcon seemed slow af, and this seemed to help, thanks!


    Is this why Llama is beating Falcon, because they''re overlooking simple things?
    I''m quite surprised since this has far superior licensing to Llama.'
  created_at: 2023-08-16 16:29:41+00:00
  edited: false
  hidden: false
  id: 64dd07854e2c7863c3155073
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-08-17T02:59:14.000Z'
    data:
      edited: false
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725574851036072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: "<p>Dont know why this was overlooked. This updated code runs at around\
          \ 2.5x speed on CPU and I have not been able to measure the speed up on\
          \ GPU of fp16 model. The quantized version of the model did not seem to\
          \ give any speed up after this improvement on GPU. What precision did you\
          \ use <span data-props=\"{&quot;user&quot;:&quot;YoYo1234Qwerty&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YoYo1234Qwerty\"\
          >@<span class=\"underline\">YoYo1234Qwerty</span></a></span>\n\n\t</span></span>\
          \ and how much speed up did you get in case it was GPU ?</p>\n<p>Also Falcon\
          \ Models by default run with Flash Attention algo so its heavily I/O optimized\
          \ already. So any change in the algorithm has to keep up with the I/O optimization\
          \ to get the speed up on GPU. I am guessing they might have chosen to not\
          \ reuse the past key values because it might have not led to any difference\
          \ in inference speed and would have led to more complicate code. Not sure.\
          \ </p>\n"
        raw: 'Dont know why this was overlooked. This updated code runs at around
          2.5x speed on CPU and I have not been able to measure the speed up on GPU
          of fp16 model. The quantized version of the model did not seem to give any
          speed up after this improvement on GPU. What precision did you use @YoYo1234Qwerty
          and how much speed up did you get in case it was GPU ?


          Also Falcon Models by default run with Flash Attention algo so its heavily
          I/O optimized already. So any change in the algorithm has to keep up with
          the I/O optimization to get the speed up on GPU. I am guessing they might
          have chosen to not reuse the past key values because it might have not led
          to any difference in inference speed and would have led to more complicate
          code. Not sure. '
        updatedAt: '2023-08-17T02:59:14.477Z'
      numEdits: 0
      reactions: []
    id: 64dd8d025b0f9136f53e2111
    type: comment
  author: puru22
  content: 'Dont know why this was overlooked. This updated code runs at around 2.5x
    speed on CPU and I have not been able to measure the speed up on GPU of fp16 model.
    The quantized version of the model did not seem to give any speed up after this
    improvement on GPU. What precision did you use @YoYo1234Qwerty and how much speed
    up did you get in case it was GPU ?


    Also Falcon Models by default run with Flash Attention algo so its heavily I/O
    optimized already. So any change in the algorithm has to keep up with the I/O
    optimization to get the speed up on GPU. I am guessing they might have chosen
    to not reuse the past key values because it might have not led to any difference
    in inference speed and would have led to more complicate code. Not sure. '
  created_at: 2023-08-17 01:59:14+00:00
  edited: false
  hidden: false
  id: 64dd8d025b0f9136f53e2111
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e240e3bf84c6ab85556211df8cf9d98f.svg
      fullname: Mahdy Shabeeb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mshabeeb
      type: user
    createdAt: '2023-09-05T08:46:10.000Z'
    data:
      edited: false
      editors:
      - mshabeeb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.819671630859375
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e240e3bf84c6ab85556211df8cf9d98f.svg
          fullname: Mahdy Shabeeb
          isHf: false
          isPro: false
          name: mshabeeb
          type: user
        html: '<p>Is there a similar fix for Falcon-7b?</p>

          '
        raw: Is there a similar fix for Falcon-7b?
        updatedAt: '2023-09-05T08:46:10.685Z'
      numEdits: 0
      reactions: []
    id: 64f6ead2065a2aa82ccef508
    type: comment
  author: mshabeeb
  content: Is there a similar fix for Falcon-7b?
  created_at: 2023-09-05 07:46:10+00:00
  edited: false
  hidden: false
  id: 64f6ead2065a2aa82ccef508
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-09-07T03:49:55.000Z'
    data:
      edited: false
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8329274654388428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: '<p>Yeah, here is the one for falcon 7b instruct, <a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c">https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c</a>.
          Note that I did not get speed up on gpu, just on cpu with all this change
          for falcon-7b-instruct</p>

          '
        raw: 'Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c.
          Note that I did not get speed up on gpu, just on cpu with all this change
          for falcon-7b-instruct

          '
        updatedAt: '2023-09-07T03:49:55.108Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mshabeeb
    id: 64f9486305961fa127a9dfd1
    type: comment
  author: puru22
  content: 'Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c.
    Note that I did not get speed up on gpu, just on cpu with all this change for
    falcon-7b-instruct

    '
  created_at: 2023-09-07 02:49:55+00:00
  edited: false
  hidden: false
  id: 64f9486305961fa127a9dfd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e240e3bf84c6ab85556211df8cf9d98f.svg
      fullname: Mahdy Shabeeb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mshabeeb
      type: user
    createdAt: '2023-09-07T06:44:38.000Z'
    data:
      edited: false
      editors:
      - mshabeeb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9040461778640747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e240e3bf84c6ab85556211df8cf9d98f.svg
          fullname: Mahdy Shabeeb
          isHf: false
          isPro: false
          name: mshabeeb
          type: user
        html: '<blockquote>

          <p>Yeah, here is the one for falcon 7b instruct, <a href="https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c">https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c</a>.
          Note that I did not get speed up on gpu, just on cpu with all this change
          for falcon-7b-instruct</p>

          </blockquote>

          <p>Thanks for your response! Just to make sure: How can I find the exact
          file path for modelling_RW.py which is used by the falcon model? When I
          search on my system I find multiple copies of this file (at least one in
          hub directory and one in modules directory) so how to determine which one
          should be replaced with the new version?</p>

          '
        raw: '> Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c.
          Note that I did not get speed up on gpu, just on cpu with all this change
          for falcon-7b-instruct


          Thanks for your response! Just to make sure: How can I find the exact file
          path for modelling_RW.py which is used by the falcon model? When I search
          on my system I find multiple copies of this file (at least one in hub directory
          and one in modules directory) so how to determine which one should be replaced
          with the new version?'
        updatedAt: '2023-09-07T06:44:38.025Z'
      numEdits: 0
      reactions: []
    id: 64f9715654d0fd40d826288a
    type: comment
  author: mshabeeb
  content: '> Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c.
    Note that I did not get speed up on gpu, just on cpu with all this change for
    falcon-7b-instruct


    Thanks for your response! Just to make sure: How can I find the exact file path
    for modelling_RW.py which is used by the falcon model? When I search on my system
    I find multiple copies of this file (at least one in hub directory and one in
    modules directory) so how to determine which one should be replaced with the new
    version?'
  created_at: 2023-09-07 05:44:38+00:00
  edited: false
  hidden: false
  id: 64f9715654d0fd40d826288a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-09-09T16:50:48.000Z'
    data:
      edited: false
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9204989671707153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: '<p>Did not understand what you mean here, so the directory which you
          are using to load the model using AutoModelForCausalLM.from_pretrained()
          should have one modelling_RW.py file which should be replaced by the one
          in the above pull request. </p>

          '
        raw: 'Did not understand what you mean here, so the directory which you are
          using to load the model using AutoModelForCausalLM.from_pretrained() should
          have one modelling_RW.py file which should be replaced by the one in the
          above pull request. '
        updatedAt: '2023-09-09T16:50:48.086Z'
      numEdits: 0
      reactions: []
    id: 64fca268b3eee10ba5315cee
    type: comment
  author: puru22
  content: 'Did not understand what you mean here, so the directory which you are
    using to load the model using AutoModelForCausalLM.from_pretrained() should have
    one modelling_RW.py file which should be replaced by the one in the above pull
    request. '
  created_at: 2023-09-09 15:50:48+00:00
  edited: false
  hidden: false
  id: 64fca268b3eee10ba5315cee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-09-10T05:37:07.000Z'
    data:
      edited: false
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467701315879822
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: '<p>I understand what you mean, I guess you are using model hub directly
          to download the huggingface model, which is ending up creating two locations
          for modelling_RW.py in huggingface cache modules directory. I would recommend
          just directly cloning the repo (with git lfs installed in your system) and
          use that directory for everything. </p>

          '
        raw: 'I understand what you mean, I guess you are using model hub directly
          to download the huggingface model, which is ending up creating two locations
          for modelling_RW.py in huggingface cache modules directory. I would recommend
          just directly cloning the repo (with git lfs installed in your system) and
          use that directory for everything. '
        updatedAt: '2023-09-10T05:37:07.247Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mshabeeb
    id: 64fd56030e486522f86031b1
    type: comment
  author: puru22
  content: 'I understand what you mean, I guess you are using model hub directly to
    download the huggingface model, which is ending up creating two locations for
    modelling_RW.py in huggingface cache modules directory. I would recommend just
    directly cloning the repo (with git lfs installed in your system) and use that
    directory for everything. '
  created_at: 2023-09-10 04:37:07+00:00
  edited: false
  hidden: false
  id: 64fd56030e486522f86031b1
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 85
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: refs/heads/main
title: Changes in modelling_RW.py to be able to handle past_key_values for faster
  model generations
