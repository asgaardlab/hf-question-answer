!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AshBam
conflicting_files: null
created_at: 2023-07-14 06:36:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fcfb22f7b1c26b419a6773121457ae4.svg
      fullname: Ayush Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AshBam
      type: user
    createdAt: '2023-07-14T07:36:00.000Z'
    data:
      edited: true
      editors:
      - AshBam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9679763913154602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fcfb22f7b1c26b419a6773121457ae4.svg
          fullname: Ayush Raj
          isHf: false
          isPro: false
          name: AshBam
          type: user
        html: '<p> I understand that the Falcon model is not meant to work on unseen
          languages (listed as a limitation). However, I need to do so. An instruct
          only finetuning is giving pretty unstable results at the moment. I''ve scourged
          the internet to find a resource to help with the same but have not been
          able to find the same. </p>

          <p>Does anyone has any idea on how it can be made possible? I''ve been trying
          to go through the PEFT, LORA, deepspeed libraries related to Falcon to get
          some idea on reverse engineering the process. Understand how to adding new
          layers on top of the frozen layers and if it might be possible to unfreeze
          and tune other layers. However, I''ve not been able to find something workable.
          </p>

          <p>Please help me out if there are any resources for this. </p>

          '
        raw: " I understand that the Falcon model is not meant to work on unseen languages\
          \ (listed as a limitation). However, I need to do so. An instruct only finetuning\
          \ is giving pretty unstable results at the moment. I've scourged the internet\
          \ to find a resource to help with the same but have not been able to find\
          \ the same. \n\nDoes anyone has any idea on how it can be made possible?\
          \ I've been trying to go through the PEFT, LORA, deepspeed libraries related\
          \ to Falcon to get some idea on reverse engineering the process. Understand\
          \ how to adding new layers on top of the frozen layers and if it might be\
          \ possible to unfreeze and tune other layers. However, I've not been able\
          \ to find something workable. \n\nPlease help me out if there are any resources\
          \ for this. "
        updatedAt: '2023-07-14T08:28:13.582Z'
      numEdits: 1
      reactions: []
    id: 64b0fae03060676038ce13b8
    type: comment
  author: AshBam
  content: " I understand that the Falcon model is not meant to work on unseen languages\
    \ (listed as a limitation). However, I need to do so. An instruct only finetuning\
    \ is giving pretty unstable results at the moment. I've scourged the internet\
    \ to find a resource to help with the same but have not been able to find the\
    \ same. \n\nDoes anyone has any idea on how it can be made possible? I've been\
    \ trying to go through the PEFT, LORA, deepspeed libraries related to Falcon to\
    \ get some idea on reverse engineering the process. Understand how to adding new\
    \ layers on top of the frozen layers and if it might be possible to unfreeze and\
    \ tune other layers. However, I've not been able to find something workable. \n\
    \nPlease help me out if there are any resources for this. "
  created_at: 2023-07-14 06:36:00+00:00
  edited: true
  hidden: false
  id: 64b0fae03060676038ce13b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-14T13:10:23.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9464454054832458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I suppose no one tried, doesn''t mean it does not work.<br>Personally
          I''d try careful fine tuning of the embeddings using dictionaries of that
          particular language in combination with all languages Falcon knows well,
          so it can find connections of the new words with existing words.<br>Then
          the same on sentences with a large corpus of untrained examples to regularly
          test the progress.</p>

          '
        raw: 'I suppose no one tried, doesn''t mean it does not work.

          Personally I''d try careful fine tuning of the embeddings using dictionaries
          of that particular language in combination with all languages Falcon knows
          well, so it can find connections of the new words with existing words.

          Then the same on sentences with a large corpus of untrained examples to
          regularly test the progress.'
        updatedAt: '2023-07-14T13:10:23.189Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AshBam
    id: 64b1493f26893eb6a6aa33fa
    type: comment
  author: cmp-nct
  content: 'I suppose no one tried, doesn''t mean it does not work.

    Personally I''d try careful fine tuning of the embeddings using dictionaries of
    that particular language in combination with all languages Falcon knows well,
    so it can find connections of the new words with existing words.

    Then the same on sentences with a large corpus of untrained examples to regularly
    test the progress.'
  created_at: 2023-07-14 12:10:23+00:00
  edited: false
  hidden: false
  id: 64b1493f26893eb6a6aa33fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fcfb22f7b1c26b419a6773121457ae4.svg
      fullname: Ayush Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AshBam
      type: user
    createdAt: '2023-07-19T05:54:51.000Z'
    data:
      edited: false
      editors:
      - AshBam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824683666229248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fcfb22f7b1c26b419a6773121457ae4.svg
          fullname: Ayush Raj
          isHf: false
          isPro: false
          name: AshBam
          type: user
        html: '<p>Thanks will try something out.</p>

          '
        raw: Thanks will try something out.
        updatedAt: '2023-07-19T05:54:51.267Z'
      numEdits: 0
      reactions: []
    id: 64b77aab6ab5d14ca7f7eb8e
    type: comment
  author: AshBam
  content: Thanks will try something out.
  created_at: 2023-07-19 04:54:51+00:00
  edited: false
  hidden: false
  id: 64b77aab6ab5d14ca7f7eb8e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 91
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Finetuning Base Falcon on Unseen Language/New data (non instruct/RLHF)
