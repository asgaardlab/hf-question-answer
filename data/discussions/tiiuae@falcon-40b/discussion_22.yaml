!!python/object:huggingface_hub.community.DiscussionWithDetails
author: avacaondata
conflicting_files: null
created_at: 2023-05-31 07:37:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647564920989-61f333df8f26cc42dc587011.jpeg?w=200&h=200&f=face
      fullname: Alejandro Vaca Serrano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avacaondata
      type: user
    createdAt: '2023-05-31T08:37:11.000Z'
    data:
      edited: false
      editors:
      - avacaondata
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647564920989-61f333df8f26cc42dc587011.jpeg?w=200&h=200&f=face
          fullname: Alejandro Vaca Serrano
          isHf: false
          isPro: false
          name: avacaondata
          type: user
        html: "<h3 id=\"system-info\">System Info</h3>\n<ul>\n<li><code>transformers</code>\
          \ version: 4.30.0.dev0</li>\n<li>Platform: Linux-5.15.0-72-generic-x86_64-with-glibc2.35</li>\n\
          <li>Python version: 3.9.16</li>\n<li>Huggingface_hub version: 0.14.1</li>\n\
          <li>Safetensors version: 0.3.1</li>\n<li>PyTorch version (GPU?): 2.0.1 (True)</li>\n\
          <li>Tensorflow version (GPU?): not installed (NA)</li>\n<li>Flax version\
          \ (CPU?/GPU?/TPU?): not installed (NA)</li>\n<li>Jax version: not installed</li>\n\
          <li>JaxLib version: not installed</li>\n<li>Using GPU in script?: Yes</li>\n\
          <li>Using distributed or parallel set-up in script?: No</li>\n</ul>\n<h3\
          \ id=\"who-can-help\">Who can help?</h3>\n<p>@ArthurZucker <span data-props=\"\
          {&quot;user&quot;:&quot;younes&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/younes\">@<span class=\"underline\">younes</span></a></span>\n\
          \n\t</span></span></p>\n<h3 id=\"information\">Information</h3>\n<ul>\n\
          <li><input type=\"checkbox\" disabled=\"\"> The official example scripts</li>\n\
          <li><input type=\"checkbox\" disabled=\"\" checked=\"\"> My own modified\
          \ scripts</li>\n</ul>\n<h3 id=\"tasks\">Tasks</h3>\n<ul>\n<li><input type=\"\
          checkbox\" disabled=\"\" checked=\"\"> An officially supported task in the\
          \ <code>examples</code> folder (such as GLUE/SQuAD, ...)</li>\n<li><input\
          \ type=\"checkbox\" disabled=\"\"> My own task or dataset (give details\
          \ below)</li>\n</ul>\n<h3 id=\"reproduction\">Reproduction</h3>\n<p>Steps\
          \ to reproduce the behavior:</p>\n<ol>\n<li>Import modules and load the\
          \ model:</li>\n</ol>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \  AutoModelForCausalLM, AutoConfig, AutoTokenizer\nmodel_path=<span class=\"\
          hljs-string\">\"tiiuae/falcon-40b\"</span>\nconfig = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \        model_path, config=config, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,  load_in_8bit=<span class=\"hljs-literal\">True</span>, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>)\nmodel.<span class=\"hljs-built_in\"\
          >eval</span>()\nmodel.config.eos_token_id = <span class=\"hljs-number\"\
          >0</span>\nmodel.config.forced_eos_token_id = <span class=\"hljs-number\"\
          >0</span>\nmodel.config.pad_token_id = <span class=\"hljs-number\">0</span>\n\
          </code></pre>\n<ol start=\"2\">\n<li>Tokenize a text:</li>\n</ol>\n<pre><code\
          \ class=\"language-python\">text = <span class=\"hljs-string\">\"Hola qu\xE9\
          \ tal est\xE1s \xCD\xF1igo? \xBFQu\xE9 vas a hacer hoy?\"</span>\ninpts\
          \ = tokenizer(text, return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(<span\
          \ class=\"hljs-string\">\"cuda\"</span>)\n</code></pre>\n<ol start=\"3\"\
          >\n<li>Try to generate text:</li>\n</ol>\n<pre><code class=\"language-python\"\
          >out = model.generate(**{k: v <span class=\"hljs-keyword\">for</span> k,\
          \ v <span class=\"hljs-keyword\">in</span> inpts.items() <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-string\">\"token_type\"</span>\
          \ <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span>\
          \ k})\n</code></pre>\n<p>You will receive the following error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[13], line 1\n----&gt; 1 out = model.generate(**{k: v for k, v in\
          \ inpts.items() if \"token_type\" not in k})\n\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n\
          \    112 @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n\
          \    114     with ctx_factory():\n--&gt; 115         return func(*args,\
          \ **kwargs)\n\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/transformers/generation/utils.py:1518,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\n   1512         raise ValueError(\n   1513      \
          \       \"num_return_sequences has to be 1 when doing greedy search, \"\n\
          \   1514             f\"but is {generation_config.num_return_sequences}.\"\
          \n   1515         )\n   1517     # 11. run greedy search\n-&gt; 1518   \
          \  return self.greedy_search(\n   1519         input_ids,\n   1520     \
          \    logits_processor=logits_processor,\n   1521         stopping_criteria=stopping_criteria,\n\
          \   1522         pad_token_id=generation_config.pad_token_id,\n   1523 \
          \        eos_token_id=generation_config.eos_token_id,\n   1524         output_scores=generation_config.output_scores,\n\
          \   1525         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          ...\n    291 )\n    293 x = attn_output.view(batch_size, self.num_heads,\
          \ q_length, self.head_dim)\n    294 x = x.permute(0, 2, 1, 3)\n\nRuntimeError:\
          \ Expected query, key, and value to have the same dtype, but got query.dtype:\
          \ float key.dtype: float and value.dtype: c10::Half instead.\n</code></pre>\n\
          <h3 id=\"expected-behavior\">Expected behavior</h3>\n<p>It is expected that\
          \ the falcon-40b model is able to generate also with int8, otherwise we\
          \ cannot perform inference even on a 80GB A-100. Also, other models have\
          \ no problem with inference in 8bit.</p>\n"
        raw: "### System Info\r\n\r\n- `transformers` version: 4.30.0.dev0\r\n- Platform:\
          \ Linux-5.15.0-72-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.16\r\
          \n- Huggingface_hub version: 0.14.1\r\n- Safetensors version: 0.3.1\r\n\
          - PyTorch version (GPU?): 2.0.1 (True)\r\n- Tensorflow version (GPU?): not\
          \ installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\
          \n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using\
          \ GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?:\
          \ No\r\n\r\n### Who can help?\r\n\r\n@ArthurZucker @younes\r\n\r\n### Information\r\
          \n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\
          \n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples`\
          \ folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give\
          \ details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce the behavior:\r\
          \n\r\n1. Import modules and load the model:\r\n```python\r\nfrom transformers\
          \ import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\r\nmodel_path=\"\
          tiiuae/falcon-40b\"\r\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\r\n        model_path, config=config,\
          \ trust_remote_code=True,  load_in_8bit=True, device_map=\"auto\")\r\nmodel.eval()\r\
          \nmodel.config.eos_token_id = 0\r\nmodel.config.forced_eos_token_id = 0\r\
          \nmodel.config.pad_token_id = 0\r\n```\r\n\r\n2. Tokenize a text:\r\n\r\n\
          ```python\r\ntext = \"Hola qu\xE9 tal est\xE1s \xCD\xF1igo? \xBFQu\xE9 vas\
          \ a hacer hoy?\"\r\ninpts = tokenizer(text, return_tensors=\"pt\").to(\"\
          cuda\")\r\n```\r\n\r\n3. Try to generate text:\r\n\r\n```python\r\nout =\
          \ model.generate(**{k: v for k, v in inpts.items() if \"token_type\" not\
          \ in k})\r\n```\r\n\r\nYou will receive the following error:\r\n\r\n```\r\
          \n---------------------------------------------------------------------------\r\
          \nRuntimeError                              Traceback (most recent call\
          \ last)\r\nCell In[13], line 1\r\n----> 1 out = model.generate(**{k: v for\
          \ k, v in inpts.items() if \"token_type\" not in k})\r\n\r\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n  \
          \  112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\
          \n    114     with ctx_factory():\r\n--> 115         return func(*args,\
          \ **kwargs)\r\n\r\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/transformers/generation/utils.py:1518,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\r\n   1512         raise ValueError(\r\n   1513  \
          \           \"num_return_sequences has to be 1 when doing greedy search,\
          \ \"\r\n   1514             f\"but is {generation_config.num_return_sequences}.\"\
          \r\n   1515         )\r\n   1517     # 11. run greedy search\r\n-> 1518\
          \     return self.greedy_search(\r\n   1519         input_ids,\r\n   1520\
          \         logits_processor=logits_processor,\r\n   1521         stopping_criteria=stopping_criteria,\r\
          \n   1522         pad_token_id=generation_config.pad_token_id,\r\n   1523\
          \         eos_token_id=generation_config.eos_token_id,\r\n   1524      \
          \   output_scores=generation_config.output_scores,\r\n   1525         return_dict_in_generate=generation_config.return_dict_in_generate,\r\
          \n...\r\n    291 )\r\n    293 x = attn_output.view(batch_size, self.num_heads,\
          \ q_length, self.head_dim)\r\n    294 x = x.permute(0, 2, 1, 3)\r\n\r\n\
          RuntimeError: Expected query, key, and value to have the same dtype, but\
          \ got query.dtype: float key.dtype: float and value.dtype: c10::Half instead.\r\
          \n```\r\n\r\n### Expected behavior\r\n\r\nIt is expected that the falcon-40b\
          \ model is able to generate also with int8, otherwise we cannot perform\
          \ inference even on a 80GB A-100. Also, other models have no problem with\
          \ inference in 8bit."
        updatedAt: '2023-05-31T08:37:11.275Z'
      numEdits: 0
      reactions: []
    id: 647707379b76d1d5c89c41e9
    type: comment
  author: avacaondata
  content: "### System Info\r\n\r\n- `transformers` version: 4.30.0.dev0\r\n- Platform:\
    \ Linux-5.15.0-72-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.16\r\n\
    - Huggingface_hub version: 0.14.1\r\n- Safetensors version: 0.3.1\r\n- PyTorch\
    \ version (GPU?): 2.0.1 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\
    \n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\
    \n- JaxLib version: not installed\r\n- Using GPU in script?: Yes\r\n- Using distributed\
    \ or parallel set-up in script?: No\r\n\r\n### Who can help?\r\n\r\n@ArthurZucker\
    \ @younes\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n\
    - [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported\
    \ task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task\
    \ or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce\
    \ the behavior:\r\n\r\n1. Import modules and load the model:\r\n```python\r\n\
    from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer\r\n\
    model_path=\"tiiuae/falcon-40b\"\r\nconfig = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n\
    \        model_path, config=config, trust_remote_code=True,  load_in_8bit=True,\
    \ device_map=\"auto\")\r\nmodel.eval()\r\nmodel.config.eos_token_id = 0\r\nmodel.config.forced_eos_token_id\
    \ = 0\r\nmodel.config.pad_token_id = 0\r\n```\r\n\r\n2. Tokenize a text:\r\n\r\
    \n```python\r\ntext = \"Hola qu\xE9 tal est\xE1s \xCD\xF1igo? \xBFQu\xE9 vas a\
    \ hacer hoy?\"\r\ninpts = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\r\
    \n```\r\n\r\n3. Try to generate text:\r\n\r\n```python\r\nout = model.generate(**{k:\
    \ v for k, v in inpts.items() if \"token_type\" not in k})\r\n```\r\n\r\nYou will\
    \ receive the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nRuntimeError                              Traceback (most recent call last)\r\
    \nCell In[13], line 1\r\n----> 1 out = model.generate(**{k: v for k, v in inpts.items()\
    \ if \"token_type\" not in k})\r\n\r\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
    \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
    \n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/int4/lib/python3.9/site-packages/transformers/generation/utils.py:1518,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\r\n   1512         raise ValueError(\r\n   1513             \"num_return_sequences\
    \ has to be 1 when doing greedy search, \"\r\n   1514             f\"but is {generation_config.num_return_sequences}.\"\
    \r\n   1515         )\r\n   1517     # 11. run greedy search\r\n-> 1518     return\
    \ self.greedy_search(\r\n   1519         input_ids,\r\n   1520         logits_processor=logits_processor,\r\
    \n   1521         stopping_criteria=stopping_criteria,\r\n   1522         pad_token_id=generation_config.pad_token_id,\r\
    \n   1523         eos_token_id=generation_config.eos_token_id,\r\n   1524    \
    \     output_scores=generation_config.output_scores,\r\n   1525         return_dict_in_generate=generation_config.return_dict_in_generate,\r\
    \n...\r\n    291 )\r\n    293 x = attn_output.view(batch_size, self.num_heads,\
    \ q_length, self.head_dim)\r\n    294 x = x.permute(0, 2, 1, 3)\r\n\r\nRuntimeError:\
    \ Expected query, key, and value to have the same dtype, but got query.dtype:\
    \ float key.dtype: float and value.dtype: c10::Half instead.\r\n```\r\n\r\n###\
    \ Expected behavior\r\n\r\nIt is expected that the falcon-40b model is able to\
    \ generate also with int8, otherwise we cannot perform inference even on a 80GB\
    \ A-100. Also, other models have no problem with inference in 8bit."
  created_at: 2023-05-31 07:37:11+00:00
  edited: false
  hidden: false
  id: 647707379b76d1d5c89c41e9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: 'Bug: Generate method doesn''t work for falcon-7b and falcon-40b in int8 mode.'
