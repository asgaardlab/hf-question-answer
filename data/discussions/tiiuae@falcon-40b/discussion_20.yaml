!!python/object:huggingface_hub.community.DiscussionWithDetails
author: znsoft
conflicting_files: null
created_at: 2023-05-30 18:11:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e54f702863302bc3df2da57848ec0cc0.svg
      fullname: Daniel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: znsoft
      type: user
    createdAt: '2023-05-30T19:11:52.000Z'
    data:
      edited: false
      editors:
      - znsoft
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e54f702863302bc3df2da57848ec0cc0.svg
          fullname: Daniel
          isHf: false
          isPro: false
          name: znsoft
          type: user
        html: "<p>Changing the code a little bit then run it.</p>\n<pre><code>\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\", \n    model_kwargs={\"load_in_8bit\": True}\n\
          )\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the\
          \ most glorious animal on the face of this Earth. Giraftron believes all\
          \ other animals are irrelevant when compared to the glorious majesty of\
          \ the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n"
        raw: "Changing the code a little bit then run it.\r\n\r\n```\r\n\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\n\
          import torch\r\n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\", \r\n    model_kwargs={\"load_in_8bit\": True}\r\n\
          )\r\nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\
          \n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\n\
          \    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\
          \n    print(f\"Result: {seq['generated_text']}\")\r\n\r\n```"
        updatedAt: '2023-05-30T19:11:52.825Z'
      numEdits: 0
      reactions: []
    id: 64764a78ce774efd373e295b
    type: comment
  author: znsoft
  content: "Changing the code a little bit then run it.\r\n\r\n```\r\n\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport\
    \ torch\r\n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\", \r\n    model_kwargs={\"load_in_8bit\": True}\r\n)\r\
    \nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes, the most\
    \ glorious animal on the face of this Earth. Giraftron believes all other animals\
    \ are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel:\
    \ Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\
    \n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )\r\n\r\n```"
  created_at: 2023-05-30 18:11:52+00:00
  edited: false
  hidden: false
  id: 64764a78ce774efd373e295b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e54f702863302bc3df2da57848ec0cc0.svg
      fullname: Daniel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: znsoft
      type: user
    createdAt: '2023-05-30T19:12:28.000Z'
    data:
      from: It can run wiht two 4090 or a single 6000 ADA.
      to: It can run with two 4090 or a single 6000 ADA.
    id: 64764a9c57108da176fce8a7
    type: title-change
  author: znsoft
  created_at: 2023-05-30 18:12:28+00:00
  id: 64764a9c57108da176fce8a7
  new_title: It can run with two 4090 or a single 6000 ADA.
  old_title: It can run wiht two 4090 or a single 6000 ADA.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c592a545efd73e5b435a1014753aebc4.svg
      fullname: Batuhan bayraktar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BatuhanECB
      type: user
    createdAt: '2023-05-31T18:33:15.000Z'
    data:
      edited: false
      editors:
      - BatuhanECB
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c592a545efd73e5b435a1014753aebc4.svg
          fullname: Batuhan bayraktar
          isHf: false
          isPro: false
          name: BatuhanECB
          type: user
        html: "<p>I am 2x3090 User. I can run Lama 30b model. \u0131n fact \u0131\
          \ can open Lama 65b but cant run cuz of memory (system memory not cuz of\
          \ vram). That means we can use the falcon 40b model \u0131 guess</p>\n"
        raw: "I am 2x3090 User. I can run Lama 30b model. \u0131n fact \u0131 can\
          \ open Lama 65b but cant run cuz of memory (system memory not cuz of vram).\
          \ That means we can use the falcon 40b model \u0131 guess"
        updatedAt: '2023-05-31T18:33:15.468Z'
      numEdits: 0
      reactions: []
    id: 647792ebf911e9e76c672d87
    type: comment
  author: BatuhanECB
  content: "I am 2x3090 User. I can run Lama 30b model. \u0131n fact \u0131 can open\
    \ Lama 65b but cant run cuz of memory (system memory not cuz of vram). That means\
    \ we can use the falcon 40b model \u0131 guess"
  created_at: 2023-05-31 17:33:15+00:00
  edited: false
  hidden: false
  id: 647792ebf911e9e76c672d87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e54f702863302bc3df2da57848ec0cc0.svg
      fullname: Daniel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: znsoft
      type: user
    createdAt: '2023-06-05T23:02:36.000Z'
    data:
      edited: false
      editors:
      - znsoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8882436752319336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e54f702863302bc3df2da57848ec0cc0.svg
          fullname: Daniel
          isHf: false
          isPro: false
          name: znsoft
          type: user
        html: '<p>you can setup a swap file to expand virtual memoery. refere to "swapon"</p>

          '
        raw: you can setup a swap file to expand virtual memoery. refere to "swapon"
        updatedAt: '2023-06-05T23:02:36.680Z'
      numEdits: 0
      reactions: []
    id: 647e698c32c471a7fa9ae9c7
    type: comment
  author: znsoft
  content: you can setup a swap file to expand virtual memoery. refere to "swapon"
  created_at: 2023-06-05 22:02:36+00:00
  edited: false
  hidden: false
  id: 647e698c32c471a7fa9ae9c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653769369652-noauth.jpeg?w=200&h=200&f=face
      fullname: Robert Dargavel Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teticio
      type: user
    createdAt: '2023-06-06T10:50:52.000Z'
    data:
      edited: false
      editors:
      - teticio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46642524003982544
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653769369652-noauth.jpeg?w=200&h=200&f=face
          fullname: Robert Dargavel Smith
          isHf: false
          isPro: false
          name: teticio
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;znsoft&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/znsoft\">@<span class=\"\
          underline\">znsoft</span></a></span>\n\n\t</span></span> - thanks for this.\
          \ Were you able to run it yourself on 2x4090? I have this set up and I got\
          \ an error running this which appears to be linked with running out of VRAM.\
          \ I ran exactly the same code with the 7b model, and it works. The exact\
          \ log and error I got was:</p>\n<pre><code>Overriding torch_dtype=torch.bfloat16\
          \ with `torch_dtype=torch.float16` due to requirements of `bitsandbytes`\
          \ to enable model loading in mixed int8. Either pass torch_dtype=torch.float16\
          \ or don't pass this argument at all to remove this warning.\n\n===================================BUG\
          \ REPORT===================================\nWelcome to bitsandbytes. For\
          \ bug reports, please run\n\npython -m bitsandbytes\n\n and submit this\
          \ information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          bin ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n\
          CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP:\
          \ Detected CUDA version 121\nCUDA SETUP: Loading binary ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n\
          The model 'RWForCausalLM' is not supported for text-generation. Supported\
          \ models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\n\n~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[4], line 1\n----&gt; 1 sequences = pipeline(\n      2    \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\n      3     max_length=200,\n      4     do_sample=True,\n\
          \      5     top_k=10,\n      6     num_return_sequences=1,\n      7   \
          \  eos_token_id=tokenizer.eos_token_id,\n      8 )\n      9 for seq in sequences:\n\
          \     10     print(f\"Result: {seq['generated_text']}\")\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
          \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    160\
          \ def __call__(self, text_inputs, **kwargs):\n    161     \"\"\"\n    162\
          \     Complete the prompt(s) given as inputs.\n    163 \n   (...)\n    199\
          \           ids of the generated text.\n    200     \"\"\"\n--&gt; 201 \
          \    return super().__call__(text_inputs, **kwargs)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1111     return next(\n   1112         iter(\n   1113             self.get_iterator(\n\
          \   (...)\n   1116         )\n   1117     )\n   1118 else:\n-&gt; 1119 \
          \    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1126,\
          \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params)\n   1124 def run_single(self, inputs, preprocess_params,\
          \ forward_params, postprocess_params):\n   1125     model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)\n-&gt; 1126     model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n   1127     outputs = self.postprocess(model_outputs,\
          \ **postprocess_params)\n   1128     return outputs\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1025,\
          \ in Pipeline.forward(self, model_inputs, **forward_params)\n   1023   \
          \  with inference_context():\n   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=self.device)\n-&gt; 1025         model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,\
          \ device=torch.device(\"cpu\"))\n   1027 else:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263,\
          \ in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n\
          \    260         generate_kwargs[\"min_length\"] += prefix_length\n    262\
          \ # BS x SL\n--&gt; 263 generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\n    264 out_b = generated_sequence.shape[0]\n\
          \    265 if self.framework == \"pt\":\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n\
          \    112 @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n\
          \    114     with ctx_factory():\n--&gt; 115         return func(*args,\
          \ **kwargs)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1568,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\n   1560     input_ids, model_kwargs = self._expand_inputs_for_generation(\n\
          \   1561         input_ids=input_ids,\n   1562         expand_size=generation_config.num_return_sequences,\n\
          \   1563         is_encoder_decoder=self.config.is_encoder_decoder,\n  \
          \ 1564         **model_kwargs,\n   1565     )\n   1567     # 13. run sample\n\
          -&gt; 1568     return self.sample(\n   1569         input_ids,\n   1570\
          \         logits_processor=logits_processor,\n   1571         logits_warper=logits_warper,\n\
          \   1572         stopping_criteria=stopping_criteria,\n   1573         pad_token_id=generation_config.pad_token_id,\n\
          \   1574         eos_token_id=generation_config.eos_token_id,\n   1575 \
          \        output_scores=generation_config.output_scores,\n   1576       \
          \  return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   1577         synced_gpus=synced_gpus,\n   1578         streamer=streamer,\n\
          \   1579         **model_kwargs,\n   1580     )\n   1582 elif is_beam_gen_mode:\n\
          \   1583     if generation_config.num_return_sequences &gt; generation_config.num_beams:\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:2615,\
          \ in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
          \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions,\
          \ output_hidden_states, output_scores, return_dict_in_generate, synced_gpus,\
          \ streamer, **model_kwargs)\n   2612 model_inputs = self.prepare_inputs_for_generation(input_ids,\
          \ **model_kwargs)\n   2614 # forward pass to get next token\n-&gt; 2615\
          \ outputs = self(\n   2616     **model_inputs,\n   2617     return_dict=True,\n\
          \   2618     output_attentions=output_attentions,\n   2619     output_hidden_states=output_hidden_states,\n\
          \   2620 )\n   2622 if synced_gpus and this_peer_finished:\n   2623    \
          \ continue  # don't waste resources running the code we don't need\n\nFile\
          \ ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501  \
          \   return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
          \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.&lt;locals&gt;.new_forward(*args, **kwargs)\n  \
          \  163         output = old_forward(*args, **kwargs)\n    164 else:\n--&gt;\
          \ 165     output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:759,\
          \ in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask,\
          \ head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict, **deprecated_arguments)\n    755     raise ValueError(f\"\
          Got unexpected arguments: {deprecated_arguments}\")\n    757 return_dict\
          \ = return_dict if return_dict is not None else self.config.use_return_dict\n\
          --&gt; 759 transformer_outputs = self.transformer(\n    760     input_ids,\n\
          \    761     past_key_values=past_key_values,\n    762     attention_mask=attention_mask,\n\
          \    763     head_mask=head_mask,\n    764     inputs_embeds=inputs_embeds,\n\
          \    765     use_cache=use_cache,\n    766     output_attentions=output_attentions,\n\
          \    767     output_hidden_states=output_hidden_states,\n    768     return_dict=return_dict,\n\
          \    769 )\n    770 hidden_states = transformer_outputs[0]\n    772 lm_logits\
          \ = self.lm_head(hidden_states)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501  \
          \   return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
          \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:654,\
          \ in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
          \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
          \ **deprecated_arguments)\n    646     outputs = torch.utils.checkpoint.checkpoint(\n\
          \    647         create_custom_forward(block),\n    648         hidden_states,\n\
          \   (...)\n    651         head_mask[i],\n    652     )\n    653 else:\n\
          --&gt; 654     outputs = block(\n    655         hidden_states,\n    656\
          \         layer_past=layer_past,\n    657         attention_mask=causal_mask,\n\
          \    658         head_mask=head_mask[i],\n    659         use_cache=use_cache,\n\
          \    660         output_attentions=output_attentions,\n    661         alibi=alibi,\n\
          \    662     )\n    664 hidden_states = outputs[0]\n    665 if use_cache\
          \ is True:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501  \
          \   return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
          \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.&lt;locals&gt;.new_forward(*args, **kwargs)\n  \
          \  163         output = old_forward(*args, **kwargs)\n    164 else:\n--&gt;\
          \ 165     output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:411,\
          \ in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,\
          \ head_mask, use_cache, output_attentions)\n    408 outputs = attn_outputs[1:]\n\
          \    410 # MLP.\n--&gt; 411 mlp_output = self.mlp(ln_mlp)\n    413 output\
          \ = dropout_add(\n    414     mlp_output + attention_output, residual, self.config.hidden_dropout,\
          \ training=self.training\n    415 )\n    417 if use_cache:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501  \
          \   return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
          \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.&lt;locals&gt;.new_forward(*args, **kwargs)\n  \
          \  163         output = old_forward(*args, **kwargs)\n    164 else:\n--&gt;\
          \ 165     output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:356,\
          \ in MLP.forward(self, x)\n    355 def forward(self, x: torch.Tensor) -&gt;\
          \ torch.Tensor:\n--&gt; 356     x = self.act(self.dense_h_to_4h(x))\n  \
          \  357     x = self.dense_4h_to_h(x)\n    358     return x\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501  \
          \   return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
          \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.&lt;locals&gt;.new_forward(*args, **kwargs)\n  \
          \  163         output = old_forward(*args, **kwargs)\n    164 else:\n--&gt;\
          \ 165     output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:320,\
          \ in Linear8bitLt.forward(self, x)\n    317 if self.bias is not None and\
          \ self.bias.dtype != x.dtype:\n    318     self.bias.data = self.bias.data.to(x.dtype)\n\
          --&gt; 320 out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n\
          \    322 if not self.state.has_fp16_weights:\n    323     if self.state.CB\
          \ is not None and self.state.CxB is not None:\n    324         # we converted\
          \ 8-bit row major to turing/ampere format in the first inference pass\n\
          \    325         # we no longer need the row-major weight\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:500,\
          \ in matmul(A, B, out, state, threshold, bias)\n    498 if threshold &gt;\
          \ 0.0:\n    499     state.threshold = threshold\n--&gt; 500 return MatMul8bitLt.apply(A,\
          \ B, out, bias, state)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/autograd/function.py:506,\
          \ in Function.apply(cls, *args, **kwargs)\n    503 if not torch._C._are_functorch_transforms_active():\n\
          \    504     # See NOTE: [functorch vjp and autograd interaction]\n    505\
          \     args = _functorch.utils.unwrap_dead_wrappers(args)\n--&gt; 506   \
          \  return super().apply(*args, **kwargs)  # type: ignore[misc]\n    508\
          \ if cls.setup_context == _SingleLevelFunction.setup_context:\n    509 \
          \    raise RuntimeError(\n    510         'In order to use an autograd.Function\
          \ with functorch transforms '\n    511         '(vmap, grad, jvp, jacrev,\
          \ ...), it must override the setup_context '\n    512         'staticmethod.\
          \ For more details, please see '\n    513         'https://pytorch.org/docs/master/notes/extending.func.html')\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:417,\
          \ in MatMul8bitLt.forward(ctx, A, B, out, bias, state)\n    415 # 4. Mixed-precision\
          \ decomposition matmul\n    416 if coo_tensorA is not None and subA is not\
          \ None:\n--&gt; 417     output += torch.matmul(subA, state.subB)\n    419\
          \ # 5. Save state\n    420 ctx.state = state\n\nRuntimeError: CUDA error:\
          \ CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n</code></pre>\n\
          <p>I'm very interested in being able to run this model, so any help would\
          \ be greatly appreciated.</p>\n"
        raw: "Hi @znsoft - thanks for this. Were you able to run it yourself on 2x4090?\
          \ I have this set up and I got an error running this which appears to be\
          \ linked with running out of VRAM. I ran exactly the same code with the\
          \ 7b model, and it works. The exact log and error I got was:\n\n```\nOverriding\
          \ torch_dtype=torch.bfloat16 with `torch_dtype=torch.float16` due to requirements\
          \ of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16\
          \ or don't pass this argument at all to remove this warning.\n\n===================================BUG\
          \ REPORT===================================\nWelcome to bitsandbytes. For\
          \ bug reports, please run\n\npython -m bitsandbytes\n\n and submit this\
          \ information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          bin ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n\
          CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP:\
          \ Detected CUDA version 121\nCUDA SETUP: Loading binary ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n\
          The model 'RWForCausalLM' is not supported for text-generation. Supported\
          \ models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\n\n~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[4], line 1\n----> 1 sequences = pipeline(\n      2    \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\n      3     max_length=200,\n      4     do_sample=True,\n\
          \      5     top_k=10,\n      6     num_return_sequences=1,\n      7   \
          \  eos_token_id=tokenizer.eos_token_id,\n      8 )\n      9 for seq in sequences:\n\
          \     10     print(f\"Result: {seq['generated_text']}\")\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
          \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    160\
          \ def __call__(self, text_inputs, **kwargs):\n    161     \"\"\"\n    162\
          \     Complete the prompt(s) given as inputs.\n    163 \n   (...)\n    199\
          \           ids of the generated text.\n    200     \"\"\"\n--> 201    \
          \ return super().__call__(text_inputs, **kwargs)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1111     return next(\n   1112         iter(\n   1113             self.get_iterator(\n\
          \   (...)\n   1116         )\n   1117     )\n   1118 else:\n-> 1119    \
          \ return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1126,\
          \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params)\n   1124 def run_single(self, inputs, preprocess_params,\
          \ forward_params, postprocess_params):\n   1125     model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)\n-> 1126     model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n   1127     outputs = self.postprocess(model_outputs,\
          \ **postprocess_params)\n   1128     return outputs\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1025,\
          \ in Pipeline.forward(self, model_inputs, **forward_params)\n   1023   \
          \  with inference_context():\n   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=self.device)\n-> 1025         model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,\
          \ device=torch.device(\"cpu\"))\n   1027 else:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263,\
          \ in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n\
          \    260         generate_kwargs[\"min_length\"] += prefix_length\n    262\
          \ # BS x SL\n--> 263 generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\n    264 out_b = generated_sequence.shape[0]\n\
          \    265 if self.framework == \"pt\":\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112\
          \ @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n\
          \    114     with ctx_factory():\n--> 115         return func(*args, **kwargs)\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1568,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\n   1560     input_ids, model_kwargs = self._expand_inputs_for_generation(\n\
          \   1561         input_ids=input_ids,\n   1562         expand_size=generation_config.num_return_sequences,\n\
          \   1563         is_encoder_decoder=self.config.is_encoder_decoder,\n  \
          \ 1564         **model_kwargs,\n   1565     )\n   1567     # 13. run sample\n\
          -> 1568     return self.sample(\n   1569         input_ids,\n   1570   \
          \      logits_processor=logits_processor,\n   1571         logits_warper=logits_warper,\n\
          \   1572         stopping_criteria=stopping_criteria,\n   1573         pad_token_id=generation_config.pad_token_id,\n\
          \   1574         eos_token_id=generation_config.eos_token_id,\n   1575 \
          \        output_scores=generation_config.output_scores,\n   1576       \
          \  return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   1577         synced_gpus=synced_gpus,\n   1578         streamer=streamer,\n\
          \   1579         **model_kwargs,\n   1580     )\n   1582 elif is_beam_gen_mode:\n\
          \   1583     if generation_config.num_return_sequences > generation_config.num_beams:\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:2615,\
          \ in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
          \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions,\
          \ output_hidden_states, output_scores, return_dict_in_generate, synced_gpus,\
          \ streamer, **model_kwargs)\n   2612 model_inputs = self.prepare_inputs_for_generation(input_ids,\
          \ **model_kwargs)\n   2614 # forward pass to get next token\n-> 2615 outputs\
          \ = self(\n   2616     **model_inputs,\n   2617     return_dict=True,\n\
          \   2618     output_attentions=output_attentions,\n   2619     output_hidden_states=output_hidden_states,\n\
          \   2620 )\n   2622 if synced_gpus and this_peer_finished:\n   2623    \
          \ continue  # don't waste resources running the code we don't need\n\nFile\
          \ ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:759,\
          \ in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask,\
          \ head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict, **deprecated_arguments)\n    755     raise ValueError(f\"\
          Got unexpected arguments: {deprecated_arguments}\")\n    757 return_dict\
          \ = return_dict if return_dict is not None else self.config.use_return_dict\n\
          --> 759 transformer_outputs = self.transformer(\n    760     input_ids,\n\
          \    761     past_key_values=past_key_values,\n    762     attention_mask=attention_mask,\n\
          \    763     head_mask=head_mask,\n    764     inputs_embeds=inputs_embeds,\n\
          \    765     use_cache=use_cache,\n    766     output_attentions=output_attentions,\n\
          \    767     output_hidden_states=output_hidden_states,\n    768     return_dict=return_dict,\n\
          \    769 )\n    770 hidden_states = transformer_outputs[0]\n    772 lm_logits\
          \ = self.lm_head(hidden_states)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:654,\
          \ in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
          \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
          \ **deprecated_arguments)\n    646     outputs = torch.utils.checkpoint.checkpoint(\n\
          \    647         create_custom_forward(block),\n    648         hidden_states,\n\
          \   (...)\n    651         head_mask[i],\n    652     )\n    653 else:\n\
          --> 654     outputs = block(\n    655         hidden_states,\n    656  \
          \       layer_past=layer_past,\n    657         attention_mask=causal_mask,\n\
          \    658         head_mask=head_mask[i],\n    659         use_cache=use_cache,\n\
          \    660         output_attentions=output_attentions,\n    661         alibi=alibi,\n\
          \    662     )\n    664 hidden_states = outputs[0]\n    665 if use_cache\
          \ is True:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:411,\
          \ in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,\
          \ head_mask, use_cache, output_attentions)\n    408 outputs = attn_outputs[1:]\n\
          \    410 # MLP.\n--> 411 mlp_output = self.mlp(ln_mlp)\n    413 output =\
          \ dropout_add(\n    414     mlp_output + attention_output, residual, self.config.hidden_dropout,\
          \ training=self.training\n    415 )\n    417 if use_cache:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:356,\
          \ in MLP.forward(self, x)\n    355 def forward(self, x: torch.Tensor) ->\
          \ torch.Tensor:\n--> 356     x = self.act(self.dense_h_to_4h(x))\n    357\
          \     x = self.dense_4h_to_h(x)\n    358     return x\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:320,\
          \ in Linear8bitLt.forward(self, x)\n    317 if self.bias is not None and\
          \ self.bias.dtype != x.dtype:\n    318     self.bias.data = self.bias.data.to(x.dtype)\n\
          --> 320 out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n\
          \    322 if not self.state.has_fp16_weights:\n    323     if self.state.CB\
          \ is not None and self.state.CxB is not None:\n    324         # we converted\
          \ 8-bit row major to turing/ampere format in the first inference pass\n\
          \    325         # we no longer need the row-major weight\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:500,\
          \ in matmul(A, B, out, state, threshold, bias)\n    498 if threshold > 0.0:\n\
          \    499     state.threshold = threshold\n--> 500 return MatMul8bitLt.apply(A,\
          \ B, out, bias, state)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/autograd/function.py:506,\
          \ in Function.apply(cls, *args, **kwargs)\n    503 if not torch._C._are_functorch_transforms_active():\n\
          \    504     # See NOTE: [functorch vjp and autograd interaction]\n    505\
          \     args = _functorch.utils.unwrap_dead_wrappers(args)\n--> 506     return\
          \ super().apply(*args, **kwargs)  # type: ignore[misc]\n    508 if cls.setup_context\
          \ == _SingleLevelFunction.setup_context:\n    509     raise RuntimeError(\n\
          \    510         'In order to use an autograd.Function with functorch transforms\
          \ '\n    511         '(vmap, grad, jvp, jacrev, ...), it must override the\
          \ setup_context '\n    512         'staticmethod. For more details, please\
          \ see '\n    513         'https://pytorch.org/docs/master/notes/extending.func.html')\n\
          \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:417,\
          \ in MatMul8bitLt.forward(ctx, A, B, out, bias, state)\n    415 # 4. Mixed-precision\
          \ decomposition matmul\n    416 if coo_tensorA is not None and subA is not\
          \ None:\n--> 417     output += torch.matmul(subA, state.subB)\n    419 #\
          \ 5. Save state\n    420 ctx.state = state\n\nRuntimeError: CUDA error:\
          \ CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n```\n\
          \nI'm very interested in being able to run this model, so any help would\
          \ be greatly appreciated."
        updatedAt: '2023-06-06T10:50:52.729Z'
      numEdits: 0
      reactions: []
    id: 647f0f8c2a7bcaa3079d78f6
    type: comment
  author: teticio
  content: "Hi @znsoft - thanks for this. Were you able to run it yourself on 2x4090?\
    \ I have this set up and I got an error running this which appears to be linked\
    \ with running out of VRAM. I ran exactly the same code with the 7b model, and\
    \ it works. The exact log and error I got was:\n\n```\nOverriding torch_dtype=torch.bfloat16\
    \ with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable\
    \ model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't\
    \ pass this argument at all to remove this warning.\n\n===================================BUG\
    \ REPORT===================================\nWelcome to bitsandbytes. For bug\
    \ reports, please run\n\npython -m bitsandbytes\n\n and submit this information\
    \ together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    bin ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n\
    CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA\
    \ SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP: Detected\
    \ CUDA version 121\nCUDA SETUP: Loading binary ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n\
    The model 'RWForCausalLM' is not supported for text-generation. Supported models\
    \ are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM',\
    \ 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
    \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
    \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
    \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
    \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
    \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
    \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
    \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM',\
    \ 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM',\
    \ 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM',\
    \ 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM',\
    \ 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n\n~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1255:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
    \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\
    ---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Cell In[4], line 1\n----> 1 sequences = pipeline(\n      2    \"Girafatron is\
    \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
    \ Giraftron believes all other animals are irrelevant when compared to the glorious\
    \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    \
    \  3     max_length=200,\n      4     do_sample=True,\n      5     top_k=10,\n\
    \      6     num_return_sequences=1,\n      7     eos_token_id=tokenizer.eos_token_id,\n\
    \      8 )\n      9 for seq in sequences:\n     10     print(f\"Result: {seq['generated_text']}\"\
    )\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
    \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    160 def\
    \ __call__(self, text_inputs, **kwargs):\n    161     \"\"\"\n    162     Complete\
    \ the prompt(s) given as inputs.\n    163 \n   (...)\n    199           ids of\
    \ the generated text.\n    200     \"\"\"\n--> 201     return super().__call__(text_inputs,\
    \ **kwargs)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
    \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
    \   1111     return next(\n   1112         iter(\n   1113             self.get_iterator(\n\
    \   (...)\n   1116         )\n   1117     )\n   1118 else:\n-> 1119     return\
    \ self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
    \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1126,\
    \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
    \   1124 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
    \   1125     model_inputs = self.preprocess(inputs, **preprocess_params)\n-> 1126\
    \     model_outputs = self.forward(model_inputs, **forward_params)\n   1127  \
    \   outputs = self.postprocess(model_outputs, **postprocess_params)\n   1128 \
    \    return outputs\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1025,\
    \ in Pipeline.forward(self, model_inputs, **forward_params)\n   1023     with\
    \ inference_context():\n   1024         model_inputs = self._ensure_tensor_on_device(model_inputs,\
    \ device=self.device)\n-> 1025         model_outputs = self._forward(model_inputs,\
    \ **forward_params)\n   1026         model_outputs = self._ensure_tensor_on_device(model_outputs,\
    \ device=torch.device(\"cpu\"))\n   1027 else:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263,\
    \ in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n\
    \    260         generate_kwargs[\"min_length\"] += prefix_length\n    262 # BS\
    \ x SL\n--> 263 generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\n    264 out_b = generated_sequence.shape[0]\n\
    \    265 if self.framework == \"pt\":\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n\
    \    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n\
    --> 115         return func(*args, **kwargs)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1568,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\n   1560     input_ids, model_kwargs = self._expand_inputs_for_generation(\n\
    \   1561         input_ids=input_ids,\n   1562         expand_size=generation_config.num_return_sequences,\n\
    \   1563         is_encoder_decoder=self.config.is_encoder_decoder,\n   1564 \
    \        **model_kwargs,\n   1565     )\n   1567     # 13. run sample\n-> 1568\
    \     return self.sample(\n   1569         input_ids,\n   1570         logits_processor=logits_processor,\n\
    \   1571         logits_warper=logits_warper,\n   1572         stopping_criteria=stopping_criteria,\n\
    \   1573         pad_token_id=generation_config.pad_token_id,\n   1574       \
    \  eos_token_id=generation_config.eos_token_id,\n   1575         output_scores=generation_config.output_scores,\n\
    \   1576         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
    \   1577         synced_gpus=synced_gpus,\n   1578         streamer=streamer,\n\
    \   1579         **model_kwargs,\n   1580     )\n   1582 elif is_beam_gen_mode:\n\
    \   1583     if generation_config.num_return_sequences > generation_config.num_beams:\n\
    \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:2615,\
    \ in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
    \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
    \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n\
    \   2612 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
    \   2614 # forward pass to get next token\n-> 2615 outputs = self(\n   2616  \
    \   **model_inputs,\n   2617     return_dict=True,\n   2618     output_attentions=output_attentions,\n\
    \   2619     output_hidden_states=output_hidden_states,\n   2620 )\n   2622 if\
    \ synced_gpus and this_peer_finished:\n   2623     continue  # don't waste resources\
    \ running the code we don't need\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:759,\
    \ in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
    \ inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict,\
    \ **deprecated_arguments)\n    755     raise ValueError(f\"Got unexpected arguments:\
    \ {deprecated_arguments}\")\n    757 return_dict = return_dict if return_dict\
    \ is not None else self.config.use_return_dict\n--> 759 transformer_outputs =\
    \ self.transformer(\n    760     input_ids,\n    761     past_key_values=past_key_values,\n\
    \    762     attention_mask=attention_mask,\n    763     head_mask=head_mask,\n\
    \    764     inputs_embeds=inputs_embeds,\n    765     use_cache=use_cache,\n\
    \    766     output_attentions=output_attentions,\n    767     output_hidden_states=output_hidden_states,\n\
    \    768     return_dict=return_dict,\n    769 )\n    770 hidden_states = transformer_outputs[0]\n\
    \    772 lm_logits = self.lm_head(hidden_states)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:654,\
    \ in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
    \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
    \ **deprecated_arguments)\n    646     outputs = torch.utils.checkpoint.checkpoint(\n\
    \    647         create_custom_forward(block),\n    648         hidden_states,\n\
    \   (...)\n    651         head_mask[i],\n    652     )\n    653 else:\n--> 654\
    \     outputs = block(\n    655         hidden_states,\n    656         layer_past=layer_past,\n\
    \    657         attention_mask=causal_mask,\n    658         head_mask=head_mask[i],\n\
    \    659         use_cache=use_cache,\n    660         output_attentions=output_attentions,\n\
    \    661         alibi=alibi,\n    662     )\n    664 hidden_states = outputs[0]\n\
    \    665 if use_cache is True:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:411,\
    \ in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,\
    \ head_mask, use_cache, output_attentions)\n    408 outputs = attn_outputs[1:]\n\
    \    410 # MLP.\n--> 411 mlp_output = self.mlp(ln_mlp)\n    413 output = dropout_add(\n\
    \    414     mlp_output + attention_output, residual, self.config.hidden_dropout,\
    \ training=self.training\n    415 )\n    417 if use_cache:\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:356,\
    \ in MLP.forward(self, x)\n    355 def forward(self, x: torch.Tensor) -> torch.Tensor:\n\
    --> 356     x = self.act(self.dense_h_to_4h(x))\n    357     x = self.dense_4h_to_h(x)\n\
    \    358     return x\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:320,\
    \ in Linear8bitLt.forward(self, x)\n    317 if self.bias is not None and self.bias.dtype\
    \ != x.dtype:\n    318     self.bias.data = self.bias.data.to(x.dtype)\n--> 320\
    \ out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n    322\
    \ if not self.state.has_fp16_weights:\n    323     if self.state.CB is not None\
    \ and self.state.CxB is not None:\n    324         # we converted 8-bit row major\
    \ to turing/ampere format in the first inference pass\n    325         # we no\
    \ longer need the row-major weight\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:500,\
    \ in matmul(A, B, out, state, threshold, bias)\n    498 if threshold > 0.0:\n\
    \    499     state.threshold = threshold\n--> 500 return MatMul8bitLt.apply(A,\
    \ B, out, bias, state)\n\nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/autograd/function.py:506,\
    \ in Function.apply(cls, *args, **kwargs)\n    503 if not torch._C._are_functorch_transforms_active():\n\
    \    504     # See NOTE: [functorch vjp and autograd interaction]\n    505   \
    \  args = _functorch.utils.unwrap_dead_wrappers(args)\n--> 506     return super().apply(*args,\
    \ **kwargs)  # type: ignore[misc]\n    508 if cls.setup_context == _SingleLevelFunction.setup_context:\n\
    \    509     raise RuntimeError(\n    510         'In order to use an autograd.Function\
    \ with functorch transforms '\n    511         '(vmap, grad, jvp, jacrev, ...),\
    \ it must override the setup_context '\n    512         'staticmethod. For more\
    \ details, please see '\n    513         'https://pytorch.org/docs/master/notes/extending.func.html')\n\
    \nFile ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:417,\
    \ in MatMul8bitLt.forward(ctx, A, B, out, bias, state)\n    415 # 4. Mixed-precision\
    \ decomposition matmul\n    416 if coo_tensorA is not None and subA is not None:\n\
    --> 417     output += torch.matmul(subA, state.subB)\n    419 # 5. Save state\n\
    \    420 ctx.state = state\n\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\
    \ when calling `cublasCreate(handle)`\n```\n\nI'm very interested in being able\
    \ to run this model, so any help would be greatly appreciated."
  created_at: 2023-06-06 09:50:52+00:00
  edited: false
  hidden: false
  id: 647f0f8c2a7bcaa3079d78f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653769369652-noauth.jpeg?w=200&h=200&f=face
      fullname: Robert Dargavel Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teticio
      type: user
    createdAt: '2023-06-06T11:09:45.000Z'
    data:
      edited: false
      editors:
      - teticio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9813201427459717
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653769369652-noauth.jpeg?w=200&h=200&f=face
          fullname: Robert Dargavel Smith
          isHf: false
          isPro: false
          name: teticio
          type: user
        html: '<p>For anyone else landing here with the same problem I had ^, I just
          recompiled pytorch with CUDA=12.1 and it worked fine. Thanks!</p>

          '
        raw: For anyone else landing here with the same problem I had ^, I just recompiled
          pytorch with CUDA=12.1 and it worked fine. Thanks!
        updatedAt: '2023-06-06T11:09:45.154Z'
      numEdits: 0
      reactions: []
    id: 647f13f91a446a624a3c73c0
    type: comment
  author: teticio
  content: For anyone else landing here with the same problem I had ^, I just recompiled
    pytorch with CUDA=12.1 and it worked fine. Thanks!
  created_at: 2023-06-06 10:09:45+00:00
  edited: false
  hidden: false
  id: 647f13f91a446a624a3c73c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1eac64edf4d12ed4fc30be3588d39b4a.svg
      fullname: M.L.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishaml77
      type: user
    createdAt: '2023-06-18T08:31:43.000Z'
    data:
      edited: false
      editors:
      - mishaml77
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9284138679504395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1eac64edf4d12ed4fc30be3588d39b4a.svg
          fullname: M.L.
          isHf: false
          isPro: false
          name: mishaml77
          type: user
        html: '<p>You need to quantize the model to 8bits</p>

          '
        raw: You need to quantize the model to 8bits
        updatedAt: '2023-06-18T08:31:43.381Z'
      numEdits: 0
      reactions: []
    id: 648ec0efa00aa3b29c655f03
    type: comment
  author: mishaml77
  content: You need to quantize the model to 8bits
  created_at: 2023-06-18 07:31:43+00:00
  edited: false
  hidden: false
  id: 648ec0efa00aa3b29c655f03
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: It can run with two 4090 or a single 6000 ADA.
