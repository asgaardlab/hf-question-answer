!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hankcs
conflicting_files: null
created_at: 2023-05-30 02:04:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659043660560-62e2fefda63b58b8eb738cff.jpeg?w=200&h=200&f=face
      fullname: Hankcs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hankcs
      type: user
    createdAt: '2023-05-30T03:04:48.000Z'
    data:
      edited: false
      editors:
      - hankcs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659043660560-62e2fefda63b58b8eb738cff.jpeg?w=200&h=200&f=face
          fullname: Hankcs
          isHf: false
          isPro: false
          name: hankcs
          type: user
        html: '<p>Hi TII researchers,</p>

          <p>Thank you for sharing your great work. After reading your RW paper, I
          got a question regarding your choice of positional encodings. In your paper,
          ALiBi was used to train RW-1B/7B. However, your ultimate models (falcon-7B/40B)
          were using rotary. According to the ALiBi and XPos papers, ALiBi outperforms
          rotary much, especially when applied to context longer than the pretrain
          limit. So, could you explain why you revert ALiBi to rotary in your final
          large-scale pretraining?</p>

          '
        raw: "Hi TII researchers,\r\n\r\nThank you for sharing your great work. After\
          \ reading your RW paper, I got a question regarding your choice of positional\
          \ encodings. In your paper, ALiBi was used to train RW-1B/7B. However, your\
          \ ultimate models (falcon-7B/40B) were using rotary. According to the ALiBi\
          \ and XPos papers, ALiBi outperforms rotary much, especially when applied\
          \ to context longer than the pretrain limit. So, could you explain why you\
          \ revert ALiBi to rotary in your final large-scale pretraining?"
        updatedAt: '2023-05-30T03:04:48.595Z'
      numEdits: 0
      reactions: []
    id: 647567d022b4dec4524c317f
    type: comment
  author: hankcs
  content: "Hi TII researchers,\r\n\r\nThank you for sharing your great work. After\
    \ reading your RW paper, I got a question regarding your choice of positional\
    \ encodings. In your paper, ALiBi was used to train RW-1B/7B. However, your ultimate\
    \ models (falcon-7B/40B) were using rotary. According to the ALiBi and XPos papers,\
    \ ALiBi outperforms rotary much, especially when applied to context longer than\
    \ the pretrain limit. So, could you explain why you revert ALiBi to rotary in\
    \ your final large-scale pretraining?"
  created_at: 2023-05-30 02:04:48+00:00
  edited: false
  hidden: false
  id: 647567d022b4dec4524c317f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:25:07.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;hankcs&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hankcs\">@<span class=\"\
          underline\">hankcs</span></a></span>\n\n\t</span></span>, </p>\n<p>This\
          \ is another one of these somewhat arbitrary decisions :).</p>\n<ul>\n<li>We\
          \ experimented with both rotary and ALiBi early on in the project (that's\
          \ why the RW models use it!);</li>\n<li>We found rotary to consistently\
          \ improve downstream zero-shot performance + autoregressive loss, but it\
          \ was a bit slower than ALiBi;</li>\n<li>However, with a custom Triton kernel\
          \ for FlashAttention+Rotary, we were able to close much of that gap;</li>\n\
          <li>We also did extrapolation experiments, and with a sliding window we\
          \ found Rotary &amp; ALiBi to perform similarly -- however, we did not had\
          \ good long-dependence tasks, so we suspect more work is needed here;</li>\n\
          <li>We experimented with finetuning to longer sequence lengths with Rotary\
          \ and it worked fine.</li>\n</ul>\n<p>Stay tuned, we are actually interested\
          \ in sharing more thoughts and principled experiments about positional embeddings\
          \ in the future.</p>\n"
        raw: "Hey @hankcs, \n\nThis is another one of these somewhat arbitrary decisions\
          \ :).\n\n* We experimented with both rotary and ALiBi early on in the project\
          \ (that's why the RW models use it!);\n* We found rotary to consistently\
          \ improve downstream zero-shot performance + autoregressive loss, but it\
          \ was a bit slower than ALiBi;\n* However, with a custom Triton kernel for\
          \ FlashAttention+Rotary, we were able to close much of that gap;\n* We also\
          \ did extrapolation experiments, and with a sliding window we found Rotary\
          \ & ALiBi to perform similarly -- however, we did not had good long-dependence\
          \ tasks, so we suspect more work is needed here;\n* We experimented with\
          \ finetuning to longer sequence lengths with Rotary and it worked fine.\n\
          \nStay tuned, we are actually interested in sharing more thoughts and principled\
          \ experiments about positional embeddings in the future."
        updatedAt: '2023-05-30T07:25:07.675Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hankcs
      relatedEventId: 6475a4d3e9b57ce0caa193c7
    id: 6475a4d3e9b57ce0caa193c6
    type: comment
  author: FalconLLM
  content: "Hey @hankcs, \n\nThis is another one of these somewhat arbitrary decisions\
    \ :).\n\n* We experimented with both rotary and ALiBi early on in the project\
    \ (that's why the RW models use it!);\n* We found rotary to consistently improve\
    \ downstream zero-shot performance + autoregressive loss, but it was a bit slower\
    \ than ALiBi;\n* However, with a custom Triton kernel for FlashAttention+Rotary,\
    \ we were able to close much of that gap;\n* We also did extrapolation experiments,\
    \ and with a sliding window we found Rotary & ALiBi to perform similarly -- however,\
    \ we did not had good long-dependence tasks, so we suspect more work is needed\
    \ here;\n* We experimented with finetuning to longer sequence lengths with Rotary\
    \ and it worked fine.\n\nStay tuned, we are actually interested in sharing more\
    \ thoughts and principled experiments about positional embeddings in the future."
  created_at: 2023-05-30 06:25:07+00:00
  edited: false
  hidden: false
  id: 6475a4d3e9b57ce0caa193c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:25:07.000Z'
    data:
      status: closed
    id: 6475a4d3e9b57ce0caa193c7
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 06:25:07+00:00
  id: 6475a4d3e9b57ce0caa193c7
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f2e815b15a0f2e3d2c19a27b6e32ddf.svg
      fullname: Taeuk Jang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jangtu052
      type: user
    createdAt: '2023-07-18T07:11:36.000Z'
    data:
      edited: false
      editors:
      - jangtu052
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7419278025627136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f2e815b15a0f2e3d2c19a27b6e32ddf.svg
          fullname: Taeuk Jang
          isHf: false
          isPro: false
          name: jangtu052
          type: user
        html: '<p>Any updates on this?</p>

          '
        raw: Any updates on this?
        updatedAt: '2023-07-18T07:11:36.734Z'
      numEdits: 0
      reactions: []
    id: 64b63b28409950919d9b4c22
    type: comment
  author: jangtu052
  content: Any updates on this?
  created_at: 2023-07-18 06:11:36+00:00
  edited: false
  hidden: false
  id: 64b63b28409950919d9b4c22
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: Choice of positional encodings?
