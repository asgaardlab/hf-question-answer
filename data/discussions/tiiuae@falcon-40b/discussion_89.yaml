!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kartik99
conflicting_files: null
created_at: 2023-07-13 05:27:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
      fullname: Kartik Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kartik99
      type: user
    createdAt: '2023-07-13T06:27:53.000Z'
    data:
      edited: false
      editors:
      - kartik99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.904579222202301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
          fullname: Kartik Gupta
          isHf: false
          isPro: false
          name: kartik99
          type: user
        html: '<p>Running Falcon-7B on A100 GPU, the response time takes around 10-12s
          and 40s for larger answers to generate.<br>How can we increase the output
          speed. Falcon-40B takes around 4-5 mins for a short answer.<br>What are
          the finetuning requirements for the 7B and 40B?</p>

          '
        raw: "Running Falcon-7B on A100 GPU, the response time takes around 10-12s\
          \ and 40s for larger answers to generate. \r\nHow can we increase the output\
          \ speed. Falcon-40B takes around 4-5 mins for a short answer.\r\nWhat are\
          \ the finetuning requirements for the 7B and 40B?"
        updatedAt: '2023-07-13T06:27:53.512Z'
      numEdits: 0
      reactions: []
    id: 64af996946a29208da048b40
    type: comment
  author: kartik99
  content: "Running Falcon-7B on A100 GPU, the response time takes around 10-12s and\
    \ 40s for larger answers to generate. \r\nHow can we increase the output speed.\
    \ Falcon-40B takes around 4-5 mins for a short answer.\r\nWhat are the finetuning\
    \ requirements for the 7B and 40B?"
  created_at: 2023-07-13 05:27:53+00:00
  edited: false
  hidden: false
  id: 64af996946a29208da048b40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-13T12:57:55.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7416010499000549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<pre><code>falcon_print_timings:        load time =  1950.86 ms

          falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23
          ms per token,  4365.33 tokens per second)

          falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96
          ms per token,   337.94 tokens per second)

          falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14
          ms per token,    47.30 tokens per second)

          falcon_print_timings:       total time =  3142.62 ms

          </code></pre>

          <p>As a 7B reference, that''s on a 4090 (closer to H100) but also on 3090
          (that''s A100 speed) the speed is about the same<br>409 tokens prompt and
          90 tokens response take around 3.1 seconds. That''s ~6 bit, It''s 20% slower
          on full precision.<br>Maybe you have a problem on the CPU side, some swapping
          while loading or loading from a hdd etc ?</p>

          <p>Here a 40B reference:</p>

          <pre><code>falcon_print_timings:        load time =  5666.06 ms

          falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23
          ms per token,  4408.47 tokens per second)

          falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06
          ms per token,    99.36 tokens per second)

          falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36
          ms per token,    16.85 tokens per second)

          falcon_print_timings:       total time =  7720.06 ms

          </code></pre>

          '
        raw: '```

          falcon_print_timings:        load time =  1950.86 ms

          falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23
          ms per token,  4365.33 tokens per second)

          falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96
          ms per token,   337.94 tokens per second)

          falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14
          ms per token,    47.30 tokens per second)

          falcon_print_timings:       total time =  3142.62 ms

          ```

          As a 7B reference, that''s on a 4090 (closer to H100) but also on 3090 (that''s
          A100 speed) the speed is about the same

          409 tokens prompt and 90 tokens response take around 3.1 seconds. That''s
          ~6 bit, It''s 20% slower on full precision.

          Maybe you have a problem on the CPU side, some swapping while loading or
          loading from a hdd etc ?


          Here a 40B reference:

          ```

          falcon_print_timings:        load time =  5666.06 ms

          falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23
          ms per token,  4408.47 tokens per second)

          falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06
          ms per token,    99.36 tokens per second)

          falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36
          ms per token,    16.85 tokens per second)

          falcon_print_timings:       total time =  7720.06 ms

          ```'
        updatedAt: '2023-07-13T13:03:37.756Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kartik99
    id: 64aff4d365af747da631effd
    type: comment
  author: cmp-nct
  content: '```

    falcon_print_timings:        load time =  1950.86 ms

    falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23
    ms per token,  4365.33 tokens per second)

    falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96 ms
    per token,   337.94 tokens per second)

    falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14
    ms per token,    47.30 tokens per second)

    falcon_print_timings:       total time =  3142.62 ms

    ```

    As a 7B reference, that''s on a 4090 (closer to H100) but also on 3090 (that''s
    A100 speed) the speed is about the same

    409 tokens prompt and 90 tokens response take around 3.1 seconds. That''s ~6 bit,
    It''s 20% slower on full precision.

    Maybe you have a problem on the CPU side, some swapping while loading or loading
    from a hdd etc ?


    Here a 40B reference:

    ```

    falcon_print_timings:        load time =  5666.06 ms

    falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23
    ms per token,  4408.47 tokens per second)

    falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06 ms
    per token,    99.36 tokens per second)

    falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36
    ms per token,    16.85 tokens per second)

    falcon_print_timings:       total time =  7720.06 ms

    ```'
  created_at: 2023-07-13 11:57:55+00:00
  edited: true
  hidden: false
  id: 64aff4d365af747da631effd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
      fullname: Kartik Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kartik99
      type: user
    createdAt: '2023-07-14T09:03:23.000Z'
    data:
      edited: false
      editors:
      - kartik99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6974092125892639
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
          fullname: Kartik Gupta
          isHf: false
          isPro: false
          name: kartik99
          type: user
        html: '<blockquote>

          <pre><code>falcon_print_timings:        load time =  1950.86 ms

          falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23
          ms per token,  4365.33 tokens per second)

          falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96
          ms per token,   337.94 tokens per second)

          falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14
          ms per token,    47.30 tokens per second)

          falcon_print_timings:       total time =  3142.62 ms

          </code></pre>

          <p>As a 7B reference, that''s on a 4090 (closer to H100) but also on 3090
          (that''s A100 speed) the speed is about the same<br>409 tokens prompt and
          90 tokens response take around 3.1 seconds. That''s ~6 bit, It''s 20% slower
          on full precision.<br>Maybe you have a problem on the CPU side, some swapping
          while loading or loading from a hdd etc ?</p>

          <p>Here a 40B reference:</p>

          <pre><code>falcon_print_timings:        load time =  5666.06 ms

          falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23
          ms per token,  4408.47 tokens per second)

          falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06
          ms per token,    99.36 tokens per second)

          falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36
          ms per token,    16.85 tokens per second)

          falcon_print_timings:       total time =  7720.06 ms

          </code></pre>

          </blockquote>

          <p>for smaller tokens the average speed for the 7B is around 3-4 seconds
          now running on 2 A100 GPUs, but for 40B its still really slow (and failes
          to answer properly), checked the GPU and CPU usage and it seems to be fine<br>Running
          it on 24 vCPUs VM</p>

          '
        raw: "> ```\n> falcon_print_timings:        load time =  1950.86 ms\n> falcon_print_timings:\
          \      sample time =    20.62 ms /    90 runs   (    0.23 ms per token,\
          \  4365.33 tokens per second)\n> falcon_print_timings: batch eval time =\
          \  1210.28 ms /   409 tokens (    2.96 ms per token,   337.94 tokens per\
          \ second)\n> falcon_print_timings:        eval time =  1881.62 ms /    89\
          \ runs   (   21.14 ms per token,    47.30 tokens per second)\n> falcon_print_timings:\
          \       total time =  3142.62 ms\n> ```\n> As a 7B reference, that's on\
          \ a 4090 (closer to H100) but also on 3090 (that's A100 speed) the speed\
          \ is about the same\n> 409 tokens prompt and 90 tokens response take around\
          \ 3.1 seconds. That's ~6 bit, It's 20% slower on full precision.\n> Maybe\
          \ you have a problem on the CPU side, some swapping while loading or loading\
          \ from a hdd etc ?\n> \n> Here a 40B reference:\n> ```\n> falcon_print_timings:\
          \        load time =  5666.06 ms\n> falcon_print_timings:      sample time\
          \ =    13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per\
          \ second)\n> falcon_print_timings: batch eval time =  4116.55 ms /   409\
          \ tokens (   10.06 ms per token,    99.36 tokens per second)\n> falcon_print_timings:\
          \        eval time =  3561.89 ms /    60 runs   (   59.36 ms per token,\
          \    16.85 tokens per second)\n> falcon_print_timings:       total time\
          \ =  7720.06 ms\n> ```\n\nfor smaller tokens the average speed for the 7B\
          \ is around 3-4 seconds now running on 2 A100 GPUs, but for 40B its still\
          \ really slow (and failes to answer properly), checked the GPU and CPU usage\
          \ and it seems to be fine\nRunning it on 24 vCPUs VM\n"
        updatedAt: '2023-07-14T09:03:23.318Z'
      numEdits: 0
      reactions: []
    id: 64b10f5ba8aae48db52e617a
    type: comment
  author: kartik99
  content: "> ```\n> falcon_print_timings:        load time =  1950.86 ms\n> falcon_print_timings:\
    \      sample time =    20.62 ms /    90 runs   (    0.23 ms per token,  4365.33\
    \ tokens per second)\n> falcon_print_timings: batch eval time =  1210.28 ms /\
    \   409 tokens (    2.96 ms per token,   337.94 tokens per second)\n> falcon_print_timings:\
    \        eval time =  1881.62 ms /    89 runs   (   21.14 ms per token,    47.30\
    \ tokens per second)\n> falcon_print_timings:       total time =  3142.62 ms\n\
    > ```\n> As a 7B reference, that's on a 4090 (closer to H100) but also on 3090\
    \ (that's A100 speed) the speed is about the same\n> 409 tokens prompt and 90\
    \ tokens response take around 3.1 seconds. That's ~6 bit, It's 20% slower on full\
    \ precision.\n> Maybe you have a problem on the CPU side, some swapping while\
    \ loading or loading from a hdd etc ?\n> \n> Here a 40B reference:\n> ```\n> falcon_print_timings:\
    \        load time =  5666.06 ms\n> falcon_print_timings:      sample time = \
    \   13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per second)\n\
    > falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06\
    \ ms per token,    99.36 tokens per second)\n> falcon_print_timings:        eval\
    \ time =  3561.89 ms /    60 runs   (   59.36 ms per token,    16.85 tokens per\
    \ second)\n> falcon_print_timings:       total time =  7720.06 ms\n> ```\n\nfor\
    \ smaller tokens the average speed for the 7B is around 3-4 seconds now running\
    \ on 2 A100 GPUs, but for 40B its still really slow (and failes to answer properly),\
    \ checked the GPU and CPU usage and it seems to be fine\nRunning it on 24 vCPUs\
    \ VM\n"
  created_at: 2023-07-14 08:03:23+00:00
  edited: false
  hidden: false
  id: 64b10f5ba8aae48db52e617a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-14T13:04:56.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7878977060317993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <blockquote>

          <pre><code>falcon_print_timings:        load time =  1950.86 ms

          falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23
          ms per token,  4365.33 tokens per second)

          falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96
          ms per token,   337.94 tokens per second)

          falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14
          ms per token,    47.30 tokens per second)

          falcon_print_timings:       total time =  3142.62 ms

          </code></pre>

          <p>As a 7B reference, that''s on a 4090 (closer to H100) but also on 3090
          (that''s A100 speed) the speed is about the same<br>409 tokens prompt and
          90 tokens response take around 3.1 seconds. That''s ~6 bit, It''s 20% slower
          on full precision.<br>Maybe you have a problem on the CPU side, some swapping
          while loading or loading from a hdd etc ?</p>

          <p>Here a 40B reference:</p>

          <pre><code>falcon_print_timings:        load time =  5666.06 ms

          falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23
          ms per token,  4408.47 tokens per second)

          falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06
          ms per token,    99.36 tokens per second)

          falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36
          ms per token,    16.85 tokens per second)

          falcon_print_timings:       total time =  7720.06 ms

          </code></pre>

          </blockquote>

          <p>for smaller tokens the average speed for the 7B is around 3-4 seconds
          now running on 2 A100 GPUs, but for 40B its still really slow (and failes
          to answer properly), checked the GPU and CPU usage and it seems to be fine<br>Running
          it on 24 vCPUs VM</p>

          </blockquote>

          <p>I''m not sure what you mean by average speed, you''d need to measure
          speed similar as I quoted. As in tokens/second generation and tokens/sec
          prompt processing.<br>Try the ggllm.cpp project if you don''t get proper
          speed using python, it''s quite simple to get running and has far more flexibility
          in terms of configuration and quantization.<br>2xA100 would be quite an
          overkill for it, you don''t need that much vram. There is no benefit I''d
          know to inference it at 16 bit precision, you get the same responses at
          6K which is a fraction in size.</p>

          '
        raw: "> > ```\n> > falcon_print_timings:        load time =  1950.86 ms\n\
          > > falcon_print_timings:      sample time =    20.62 ms /    90 runs  \
          \ (    0.23 ms per token,  4365.33 tokens per second)\n> > falcon_print_timings:\
          \ batch eval time =  1210.28 ms /   409 tokens (    2.96 ms per token, \
          \  337.94 tokens per second)\n> > falcon_print_timings:        eval time\
          \ =  1881.62 ms /    89 runs   (   21.14 ms per token,    47.30 tokens per\
          \ second)\n> > falcon_print_timings:       total time =  3142.62 ms\n> >\
          \ ```\n> > As a 7B reference, that's on a 4090 (closer to H100) but also\
          \ on 3090 (that's A100 speed) the speed is about the same\n> > 409 tokens\
          \ prompt and 90 tokens response take around 3.1 seconds. That's ~6 bit,\
          \ It's 20% slower on full precision.\n> > Maybe you have a problem on the\
          \ CPU side, some swapping while loading or loading from a hdd etc ?\n> >\
          \ \n> > Here a 40B reference:\n> > ```\n> > falcon_print_timings:      \
          \  load time =  5666.06 ms\n> > falcon_print_timings:      sample time =\
          \    13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per\
          \ second)\n> > falcon_print_timings: batch eval time =  4116.55 ms /   409\
          \ tokens (   10.06 ms per token,    99.36 tokens per second)\n> > falcon_print_timings:\
          \        eval time =  3561.89 ms /    60 runs   (   59.36 ms per token,\
          \    16.85 tokens per second)\n> > falcon_print_timings:       total time\
          \ =  7720.06 ms\n> > ```\n> \n> for smaller tokens the average speed for\
          \ the 7B is around 3-4 seconds now running on 2 A100 GPUs, but for 40B its\
          \ still really slow (and failes to answer properly), checked the GPU and\
          \ CPU usage and it seems to be fine\n> Running it on 24 vCPUs VM\n\nI'm\
          \ not sure what you mean by average speed, you'd need to measure speed similar\
          \ as I quoted. As in tokens/second generation and tokens/sec prompt processing.\n\
          Try the ggllm.cpp project if you don't get proper speed using python, it's\
          \ quite simple to get running and has far more flexibility in terms of configuration\
          \ and quantization.\n2xA100 would be quite an overkill for it, you don't\
          \ need that much vram. There is no benefit I'd know to inference it at 16\
          \ bit precision, you get the same responses at 6K which is a fraction in\
          \ size."
        updatedAt: '2023-07-14T13:06:26.948Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kartik99
    id: 64b147f826893eb6a6aa0978
    type: comment
  author: cmp-nct
  content: "> > ```\n> > falcon_print_timings:        load time =  1950.86 ms\n> >\
    \ falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23\
    \ ms per token,  4365.33 tokens per second)\n> > falcon_print_timings: batch eval\
    \ time =  1210.28 ms /   409 tokens (    2.96 ms per token,   337.94 tokens per\
    \ second)\n> > falcon_print_timings:        eval time =  1881.62 ms /    89 runs\
    \   (   21.14 ms per token,    47.30 tokens per second)\n> > falcon_print_timings:\
    \       total time =  3142.62 ms\n> > ```\n> > As a 7B reference, that's on a\
    \ 4090 (closer to H100) but also on 3090 (that's A100 speed) the speed is about\
    \ the same\n> > 409 tokens prompt and 90 tokens response take around 3.1 seconds.\
    \ That's ~6 bit, It's 20% slower on full precision.\n> > Maybe you have a problem\
    \ on the CPU side, some swapping while loading or loading from a hdd etc ?\n>\
    \ > \n> > Here a 40B reference:\n> > ```\n> > falcon_print_timings:        load\
    \ time =  5666.06 ms\n> > falcon_print_timings:      sample time =    13.84 ms\
    \ /    61 runs   (    0.23 ms per token,  4408.47 tokens per second)\n> > falcon_print_timings:\
    \ batch eval time =  4116.55 ms /   409 tokens (   10.06 ms per token,    99.36\
    \ tokens per second)\n> > falcon_print_timings:        eval time =  3561.89 ms\
    \ /    60 runs   (   59.36 ms per token,    16.85 tokens per second)\n> > falcon_print_timings:\
    \       total time =  7720.06 ms\n> > ```\n> \n> for smaller tokens the average\
    \ speed for the 7B is around 3-4 seconds now running on 2 A100 GPUs, but for 40B\
    \ its still really slow (and failes to answer properly), checked the GPU and CPU\
    \ usage and it seems to be fine\n> Running it on 24 vCPUs VM\n\nI'm not sure what\
    \ you mean by average speed, you'd need to measure speed similar as I quoted.\
    \ As in tokens/second generation and tokens/sec prompt processing.\nTry the ggllm.cpp\
    \ project if you don't get proper speed using python, it's quite simple to get\
    \ running and has far more flexibility in terms of configuration and quantization.\n\
    2xA100 would be quite an overkill for it, you don't need that much vram. There\
    \ is no benefit I'd know to inference it at 16 bit precision, you get the same\
    \ responses at 6K which is a fraction in size."
  created_at: 2023-07-14 12:04:56+00:00
  edited: true
  hidden: false
  id: 64b147f826893eb6a6aa0978
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
      fullname: Kiran Kamble
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kiranr
      type: user
    createdAt: '2023-07-18T10:22:16.000Z'
    data:
      edited: false
      editors:
      - kiranr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9048791527748108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56ccef10a8426d7160ef3586a771bd63.svg
          fullname: Kiran Kamble
          isHf: false
          isPro: false
          name: kiranr
          type: user
        html: '<p>There is an issue with not using <code>past_key_values</code> in
          the 7 and 40b model code. as mentioned here <a href="https://huggingface.co/tiiuae/falcon-40b/discussions/48">https://huggingface.co/tiiuae/falcon-40b/discussions/48</a>.
          The issue is fixed in this pr in transformers <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/24523">https://github.com/huggingface/transformers/pull/24523</a>.
          you can try using that. </p>

          '
        raw: 'There is an issue with not using `past_key_values` in the 7 and 40b
          model code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48.
          The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523.
          you can try using that. '
        updatedAt: '2023-07-18T10:22:16.076Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kartik99
    id: 64b667d8fec74d048459385d
    type: comment
  author: kiranr
  content: 'There is an issue with not using `past_key_values` in the 7 and 40b model
    code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48.
    The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523.
    you can try using that. '
  created_at: 2023-07-18 09:22:16+00:00
  edited: false
  hidden: false
  id: 64b667d8fec74d048459385d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
      fullname: Kartik Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kartik99
      type: user
    createdAt: '2023-07-18T12:32:27.000Z'
    data:
      edited: false
      editors:
      - kartik99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.903407096862793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ce6c153b8b9f84822a68ba69b89dfbb.svg
          fullname: Kartik Gupta
          isHf: false
          isPro: false
          name: kartik99
          type: user
        html: '<blockquote>

          <p>There is an issue with not using <code>past_key_values</code> in the
          7 and 40b model code. as mentioned here <a href="https://huggingface.co/tiiuae/falcon-40b/discussions/48">https://huggingface.co/tiiuae/falcon-40b/discussions/48</a>.
          The issue is fixed in this pr in transformers <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/24523">https://github.com/huggingface/transformers/pull/24523</a>.
          you can try using that.</p>

          </blockquote>

          <p>Thanks</p>

          '
        raw: '> There is an issue with not using `past_key_values` in the 7 and 40b
          model code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48.
          The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523.
          you can try using that.


          Thanks'
        updatedAt: '2023-07-18T12:32:27.523Z'
      numEdits: 0
      reactions: []
    id: 64b6865b6583d262c6628624
    type: comment
  author: kartik99
  content: '> There is an issue with not using `past_key_values` in the 7 and 40b
    model code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48.
    The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523.
    you can try using that.


    Thanks'
  created_at: 2023-07-18 11:32:27+00:00
  edited: false
  hidden: false
  id: 64b6865b6583d262c6628624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/954a3de26b472369666ad1429d7e80a6.svg
      fullname: Tron Gan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tron2060
      type: user
    createdAt: '2023-08-03T09:46:38.000Z'
    data:
      edited: false
      editors:
      - Tron2060
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523372650146484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/954a3de26b472369666ad1429d7e80a6.svg
          fullname: Tron Gan
          isHf: false
          isPro: false
          name: Tron2060
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kartik99&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kartik99\">@<span class=\"\
          underline\">kartik99</span></a></span>\n\n\t</span></span> I think there\
          \ also has some issue with LayerNorm,  it outputs float32 but the input\
          \ is bfloat16.  It can influence memory consumption and inference speed.</p>\n"
        raw: '@kartik99 I think there also has some issue with LayerNorm,  it outputs
          float32 but the input is bfloat16.  It can influence memory consumption
          and inference speed.'
        updatedAt: '2023-08-03T09:46:38.456Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gptick
    id: 64cb777ebb5d195b991d2cfb
    type: comment
  author: Tron2060
  content: '@kartik99 I think there also has some issue with LayerNorm,  it outputs
    float32 but the input is bfloat16.  It can influence memory consumption and inference
    speed.'
  created_at: 2023-08-03 08:46:38+00:00
  edited: false
  hidden: false
  id: 64cb777ebb5d195b991d2cfb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 89
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Slow response time for 7b and 40b
