!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mkserge
conflicting_files: null
created_at: 2023-07-16 19:16:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
      fullname: Sergey Mkrtchyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mkserge
      type: user
    createdAt: '2023-07-16T20:16:19.000Z'
    data:
      edited: true
      editors:
      - mkserge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9211045503616333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
          fullname: Sergey Mkrtchyan
          isHf: false
          isPro: false
          name: mkserge
          type: user
        html: "<p>Hello,</p>\n<p>It looks like when running SFTTrainer on a falcon\
          \ model for example, the dataset gets pre-processed into a packed dataset\
          \ where it's essentially a dataset of constant length samples. If not packing,\
          \ it trims all the samples to <code>max_seq_length</code> passed to the\
          \ trainer. With this, the supervised fine-tuning is essentially fine-tuning\
          \ on a next token prediction task on the text, which makes sense for causal\
          \ LM.</p>\n<p>However, for instruction type datasets, where you want to\
          \ map user queries to some kind of output, I have a prompt that looks like\
          \ this </p>\n<pre><code>### Instructions\nYou are an assistant that does\
          \ this and that.\n### Examples\nUser: \nAssistant:\n### User\n&lt;input\
          \ query&gt;\n### Assistant\n&lt;output&gt;\n</code></pre>\n<p>I want to\
          \ fine-tune falcon to perform well on this task. And I only care about the\
          \ model produced <code>&lt;output&gt;</code>s from the user queries. I simply\
          \ pre-process my dataset to generate training samples that include the entire\
          \ prompt and all the <code>&lt;input_query&gt;</code>ies and their corresponding\
          \ <code>&lt;output&gt;</code>s.</p>\n<p>With SFTTrainer the model will simply\
          \ be fine-tuning on the entire text, including the instructions and examples.\
          \ This seems weird, since you will be fine-tuning the model over and over\
          \ again on the same prompt, where the actual desired output is only at the\
          \ end (<code>&lt;output&gt;</code>). How do you guys deal with this? Wouldn't\
          \ it make more sense to compute the loss only on the output of the model?\
          \ And is there a way to do this?</p>\n"
        raw: "Hello,\n\nIt looks like when running SFTTrainer on a falcon model for\
          \ example, the dataset gets pre-processed into a packed dataset where it's\
          \ essentially a dataset of constant length samples. If not packing, it trims\
          \ all the samples to `max_seq_length` passed to the trainer. With this,\
          \ the supervised fine-tuning is essentially fine-tuning on a next token\
          \ prediction task on the text, which makes sense for causal LM.\n\nHowever,\
          \ for instruction type datasets, where you want to map user queries to some\
          \ kind of output, I have a prompt that looks like this \n```\n### Instructions\n\
          You are an assistant that does this and that.\n### Examples\nUser: \nAssistant:\n\
          ### User\n<input query>\n### Assistant\n<output>\n```\nI want to fine-tune\
          \ falcon to perform well on this task. And I only care about the model produced\
          \ `<output>`s from the user queries. I simply pre-process my dataset to\
          \ generate training samples that include the entire prompt and all the `<input_query>`ies\
          \ and their corresponding `<output>`s.\n\nWith SFTTrainer the model will\
          \ simply be fine-tuning on the entire text, including the instructions and\
          \ examples. This seems weird, since you will be fine-tuning the model over\
          \ and over again on the same prompt, where the actual desired output is\
          \ only at the end (`<output>`). How do you guys deal with this? Wouldn't\
          \ it make more sense to compute the loss only on the output of the model?\
          \ And is there a way to do this?"
        updatedAt: '2023-07-16T20:17:56.749Z'
      numEdits: 2
      reactions: []
    id: 64b450138d03d1a4e91f879e
    type: comment
  author: mkserge
  content: "Hello,\n\nIt looks like when running SFTTrainer on a falcon model for\
    \ example, the dataset gets pre-processed into a packed dataset where it's essentially\
    \ a dataset of constant length samples. If not packing, it trims all the samples\
    \ to `max_seq_length` passed to the trainer. With this, the supervised fine-tuning\
    \ is essentially fine-tuning on a next token prediction task on the text, which\
    \ makes sense for causal LM.\n\nHowever, for instruction type datasets, where\
    \ you want to map user queries to some kind of output, I have a prompt that looks\
    \ like this \n```\n### Instructions\nYou are an assistant that does this and that.\n\
    ### Examples\nUser: \nAssistant:\n### User\n<input query>\n### Assistant\n<output>\n\
    ```\nI want to fine-tune falcon to perform well on this task. And I only care\
    \ about the model produced `<output>`s from the user queries. I simply pre-process\
    \ my dataset to generate training samples that include the entire prompt and all\
    \ the `<input_query>`ies and their corresponding `<output>`s.\n\nWith SFTTrainer\
    \ the model will simply be fine-tuning on the entire text, including the instructions\
    \ and examples. This seems weird, since you will be fine-tuning the model over\
    \ and over again on the same prompt, where the actual desired output is only at\
    \ the end (`<output>`). How do you guys deal with this? Wouldn't it make more\
    \ sense to compute the loss only on the output of the model? And is there a way\
    \ to do this?"
  created_at: 2023-07-16 19:16:19+00:00
  edited: true
  hidden: false
  id: 64b450138d03d1a4e91f879e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9c700ed5b06a2388a9bb0e93d5beb1.svg
      fullname: mohammad Dabbah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mdabbah
      type: user
    createdAt: '2023-08-09T19:11:35.000Z'
    data:
      edited: false
      editors:
      - mdabbah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5009941458702087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9c700ed5b06a2388a9bb0e93d5beb1.svg
          fullname: mohammad Dabbah
          isHf: false
          isPro: false
          name: mdabbah
          type: user
        html: '<p>checkout </p>

          <p><a href="https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only">https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only</a></p>

          '
        raw: "checkout \n\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only"
        updatedAt: '2023-08-09T19:11:35.601Z'
      numEdits: 0
      reactions: []
    id: 64d3e4e7bcab89854a06f595
    type: comment
  author: mdabbah
  content: "checkout \n\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only"
  created_at: 2023-08-09 18:11:35+00:00
  edited: false
  hidden: false
  id: 64d3e4e7bcab89854a06f595
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 92
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Fine-tune on model response only?
