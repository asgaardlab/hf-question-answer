!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amitgurintecom
conflicting_files: null
created_at: 2023-06-25 18:00:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8031efbb0ea36ac57a298d97cec88d2c.svg
      fullname: Amit Gur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitgurintecom
      type: user
    createdAt: '2023-06-25T19:00:53.000Z'
    data:
      edited: true
      editors:
      - amitgurintecom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5352057218551636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8031efbb0ea36ac57a298d97cec88d2c.svg
          fullname: Amit Gur
          isHf: false
          isPro: false
          name: amitgurintecom
          type: user
        html: '<p>Running the example code in README generate nonsense like this:<br>the
          a- \n in. the. and,,..,\n  to, a the a, the. and - to.\n, in to and\n,,
          the a\n\n,\n a the to and - the-., ,,  </p>

          <p>Tried several prompts, but all gives the same nonsense.<br>Any idea?<br>The
          example code:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import
          transformers<br>import torch</p>

          <p>model = "tiiuae/falcon-40b"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline = transformers.pipeline("text-generation",
          model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True,
          device_map="auto",)<br>sequences = pipeline("Anything here ...", max_length=200,
          do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id,)<br>for
          seq in sequences:<br>    print(f"Result: {seq[''generated_text'']}")</p>

          <p>Running on 4 GPUs (Quadro RTX 8000)<br>conda env:<br>python 3.11.3<br>transformers              4.29.2          py311h06a4308_0<br>pytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch</p>

          '
        raw: "Running the example code in README generate nonsense like this:\nthe\
          \ a- \\n in. the. and,,..,\\n  to, a the a, the. and - to.\\n, in to and\\\
          n,, the a\\n\\n,\\n a the to and - the-., ,,  \n\nTried several prompts,\
          \ but all gives the same nonsense. \nAny idea?\nThe example code:\n\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\
          \ torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          ,)\nsequences = pipeline(\"Anything here ...\", max_length=200, do_sample=True,\
          \ top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id,)\n\
          for seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\
          \nRunning on 4 GPUs (Quadro RTX 8000)\nconda env:\npython 3.11.3\ntransformers\
          \              4.29.2          py311h06a4308_0  \npytorch              \
          \     2.0.1           py3.11_cuda11.8_cudnn8.7.0_0    pytorch\n"
        updatedAt: '2023-06-25T19:01:59.745Z'
      numEdits: 1
      reactions: []
    id: 64988ee56117099581ecc9e7
    type: comment
  author: amitgurintecom
  content: "Running the example code in README generate nonsense like this:\nthe a-\
    \ \\n in. the. and,,..,\\n  to, a the a, the. and - to.\\n, in to and\\n,, the\
    \ a\\n\\n,\\n a the to and - the-., ,,  \n\nTried several prompts, but all gives\
    \ the same nonsense. \nAny idea?\nThe example code:\n\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel\
    \ = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\
    \ torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\",)\n\
    sequences = pipeline(\"Anything here ...\", max_length=200, do_sample=True, top_k=10,\
    \ num_return_sequences=1, eos_token_id=tokenizer.eos_token_id,)\nfor seq in sequences:\n\
    \    print(f\"Result: {seq['generated_text']}\")\n\nRunning on 4 GPUs (Quadro\
    \ RTX 8000)\nconda env:\npython 3.11.3\ntransformers              4.29.2     \
    \     py311h06a4308_0  \npytorch                   2.0.1           py3.11_cuda11.8_cudnn8.7.0_0\
    \    pytorch\n"
  created_at: 2023-06-25 18:00:53+00:00
  edited: true
  hidden: false
  id: 64988ee56117099581ecc9e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8031efbb0ea36ac57a298d97cec88d2c.svg
      fullname: Amit Gur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitgurintecom
      type: user
    createdAt: '2023-06-25T20:10:49.000Z'
    data:
      edited: false
      editors:
      - amitgurintecom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9844173192977905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8031efbb0ea36ac57a298d97cec88d2c.svg
          fullname: Amit Gur
          isHf: false
          isPro: false
          name: amitgurintecom
          type: user
        html: '<p>Problem solved.<br>Tried 4 X A100 GPU and the problem didn''t happen.<br>It
          could be that Quadro-8000 does not support well BF16<br>Or, the model was
          corrupted when downloaded on the first GPU.</p>

          '
        raw: "Problem solved.\nTried 4 X A100 GPU and the problem didn't happen. \n\
          It could be that Quadro-8000 does not support well BF16\nOr, the model was\
          \ corrupted when downloaded on the first GPU.\n"
        updatedAt: '2023-06-25T20:10:49.906Z'
      numEdits: 0
      reactions: []
    id: 64989f4942d585137eb056c2
    type: comment
  author: amitgurintecom
  content: "Problem solved.\nTried 4 X A100 GPU and the problem didn't happen. \n\
    It could be that Quadro-8000 does not support well BF16\nOr, the model was corrupted\
    \ when downloaded on the first GPU.\n"
  created_at: 2023-06-25 19:10:49+00:00
  edited: false
  hidden: false
  id: 64989f4942d585137eb056c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Example code from README output is nonsense
