!!python/object:huggingface_hub.community.DiscussionWithDetails
author: humza-sami
conflicting_files: null
created_at: 2023-07-27 15:09:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
      fullname: Humza Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: humza-sami
      type: user
    createdAt: '2023-07-27T16:09:37.000Z'
    data:
      edited: false
      editors:
      - humza-sami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.914510190486908
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
          fullname: Humza Sami
          isHf: false
          isPro: false
          name: humza-sami
          type: user
        html: '<p>I am currently engaged in training Falcon (LLM) on a chatbot dataset,
          and I would appreciate some guidance on handling variable-length sequences
          within the dataset. The dataset consists of multiple examples of chat messages
          exchanged between user 1 and user 2, totaling around 500 such instances.
          Each example varies in the number of messages it contains, leading to differing
          sequence lengths.<br>Here are two representative data points from the dataset:</p>

          <p>Datapoint 1 = """user 1 : How are you ?\n user 2 : I am good. \n user
          1 : What do you like ? \n user 2 : Apples"""</p>

          <p>Datapoint 2 = """user 1 : How are you ?\nuser 2 : I am good.\n user 1
          : What do you like in fruits?\n user 2 : Oranges \nuser 1 : Great me too\n
          user 2 : But sometimes I like mangoes \nuser 1 : seems intresting \n user
          2 : Yeah"""</p>

          <p>To facilitate the training process, I tokenized the dataset, setting
          a <code>maximum_length</code> of <code>input_ids</code> to 4 tokens, and
          handled overflowed tokens by padding them accordingly.</p>

          <p>Now, my question is: in cases where a chat message contains fewer than
          4 tokens, what is considered a best practice? Should I pad these shorter
          sequences to match the maximum length, or would it be more suitable to keep
          them as they are?</p>

          <p>I would appreciate any insights or suggestions on the most appropriate
          approach for handling variable-length sequences in this context. </p>

          '
        raw: "I am currently engaged in training Falcon (LLM) on a chatbot dataset,\
          \ and I would appreciate some guidance on handling variable-length sequences\
          \ within the dataset. The dataset consists of multiple examples of chat\
          \ messages exchanged between user 1 and user 2, totaling around 500 such\
          \ instances. Each example varies in the number of messages it contains,\
          \ leading to differing sequence lengths.\r\nHere are two representative\
          \ data points from the dataset:\r\n\r\nDatapoint 1 = \"\"\"user 1 : How\
          \ are you ?\\n user 2 : I am good. \\n user 1 : What do you like ? \\n user\
          \ 2 : Apples\"\"\"\r\n\r\nDatapoint 2 = \"\"\"user 1 : How are you ?\\nuser\
          \ 2 : I am good.\\n user 1 : What do you like in fruits?\\n user 2 : Oranges\
          \ \\nuser 1 : Great me too\\n user 2 : But sometimes I like mangoes \\nuser\
          \ 1 : seems intresting \\n user 2 : Yeah\"\"\"\r\n\r\nTo facilitate the\
          \ training process, I tokenized the dataset, setting a `maximum_length`\
          \ of `input_ids` to 4 tokens, and handled overflowed tokens by padding them\
          \ accordingly.\r\n\r\nNow, my question is: in cases where a chat message\
          \ contains fewer than 4 tokens, what is considered a best practice? Should\
          \ I pad these shorter sequences to match the maximum length, or would it\
          \ be more suitable to keep them as they are?\r\n\r\nI would appreciate any\
          \ insights or suggestions on the most appropriate approach for handling\
          \ variable-length sequences in this context. "
        updatedAt: '2023-07-27T16:09:37.974Z'
      numEdits: 0
      reactions: []
    id: 64c296c1ae66cb547e4d57c8
    type: comment
  author: humza-sami
  content: "I am currently engaged in training Falcon (LLM) on a chatbot dataset,\
    \ and I would appreciate some guidance on handling variable-length sequences within\
    \ the dataset. The dataset consists of multiple examples of chat messages exchanged\
    \ between user 1 and user 2, totaling around 500 such instances. Each example\
    \ varies in the number of messages it contains, leading to differing sequence\
    \ lengths.\r\nHere are two representative data points from the dataset:\r\n\r\n\
    Datapoint 1 = \"\"\"user 1 : How are you ?\\n user 2 : I am good. \\n user 1 :\
    \ What do you like ? \\n user 2 : Apples\"\"\"\r\n\r\nDatapoint 2 = \"\"\"user\
    \ 1 : How are you ?\\nuser 2 : I am good.\\n user 1 : What do you like in fruits?\\\
    n user 2 : Oranges \\nuser 1 : Great me too\\n user 2 : But sometimes I like mangoes\
    \ \\nuser 1 : seems intresting \\n user 2 : Yeah\"\"\"\r\n\r\nTo facilitate the\
    \ training process, I tokenized the dataset, setting a `maximum_length` of `input_ids`\
    \ to 4 tokens, and handled overflowed tokens by padding them accordingly.\r\n\r\
    \nNow, my question is: in cases where a chat message contains fewer than 4 tokens,\
    \ what is considered a best practice? Should I pad these shorter sequences to\
    \ match the maximum length, or would it be more suitable to keep them as they\
    \ are?\r\n\r\nI would appreciate any insights or suggestions on the most appropriate\
    \ approach for handling variable-length sequences in this context. "
  created_at: 2023-07-27 15:09:37+00:00
  edited: false
  hidden: false
  id: 64c296c1ae66cb547e4d57c8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 98
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Best Practice for Handling Variable-Length Sequences in Training an LLM Model
  on a Chatbot Dataset
