!!python/object:huggingface_hub.community.DiscussionWithDetails
author: louvivien
conflicting_files: null
created_at: 2023-10-21 19:08:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7926f3f949cabada5e74f6717ef3d76d.svg
      fullname: r
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: louvivien
      type: user
    createdAt: '2023-10-21T20:08:30.000Z'
    data:
      edited: false
      editors:
      - louvivien
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5963467359542847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7926f3f949cabada5e74f6717ef3d76d.svg
          fullname: r
          isHf: false
          isPro: false
          name: louvivien
          type: user
        html: '<p>When i try the code from the readme i get this error:</p>

          <p>''''''<br>vivien@MBP-de-Vivien FalconLLM % python3 falconllm.py</p>

          <p>WARNING: You are currently loading Falcon using legacy code contained
          in the model repository. Falcon has now been fully ported into the Hugging
          Face transformers library. For the most up-to-date and high-performance
          version of the Falcon model code, please update to the latest version of
          transformers and then load the model without the trust_remote_code=True
          argument.</p>

          <p>The argument <code>trust_remote_code</code> is to be used with Auto classes.
          It has no effect here and is ignored.</p>

          <p>Traceback (most recent call last):<br>  File "/Users/vivien/Documents/FalconLLM/falconllm.py",
          line 8, in <br>    pipeline = transformers.pipeline(<br>               ^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 834, in pipeline<br>    framework, model = infer_framework_load_model(<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py",
          line 282, in infer_framework_load_model<br>    raise ValueError(<br>ValueError:
          Could not load model tiiuae/falcon-40b with any of the following classes:
          (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class ''transformers.models.falcon.modeling_falcon.FalconForCausalLM''&gt;).
          See the original errors:<br>while loading with AutoModelForCausalLM, an
          error is thrown:<br>Traceback (most recent call last):<br>  File "/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py",
          line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,
          **kwargs)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py",
          line 560, in from_pretrained<br>    return model_class.from_pretrained(<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py",
          line 3307, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>        ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py",
          line 3428, in _load_pretrained_model<br>    raise ValueError(<br>ValueError:
          The current <code>device_map</code> had weights offloaded to the disk. Please
          provide an <code>offload_folder</code> for them. Alternatively, make sure
          you have <code>safetensors</code> installed if the model you are using offers
          the weights in this format.</p>

          <p>while loading with FalconForCausalLM, an error is thrown:<br>Traceback
          (most recent call last):<br>  File "/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py",
          line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,
          **kwargs)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py",
          line 3307, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>        ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py",
          line 3428, in _load_pretrained_model<br>    raise ValueError(<br>ValueError:
          The current <code>device_map</code> had weights offloaded to the disk. Please
          provide an <code>offload_folder</code> for them. Alternatively, make sure
          you have <code>safetensors</code> installed if the model you are using offers
          the weights in this format.''''''</p>

          <p>i have a 16go ram macbook pro with no graphic card</p>

          <p>do you know what i can do?</p>

          '
        raw: "When i try the code from the readme i get this error:\r\n\r\n'''\r\n\
          vivien@MBP-de-Vivien FalconLLM % python3 falconllm.py\r\n\r\nWARNING: You\
          \ are currently loading Falcon using legacy code contained in the model\
          \ repository. Falcon has now been fully ported into the Hugging Face transformers\
          \ library. For the most up-to-date and high-performance version of the Falcon\
          \ model code, please update to the latest version of transformers and then\
          \ load the model without the trust_remote_code=True argument.\r\n\r\nThe\
          \ argument `trust_remote_code` is to be used with Auto classes. It has no\
          \ effect here and is ignored.\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/Users/vivien/Documents/FalconLLM/falconllm.py\", line 8, in\
          \ <module>\r\n    pipeline = transformers.pipeline(\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 834, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 282, in infer_framework_load_model\r\n    raise ValueError(\r\nValueError:\
          \ Could not load model tiiuae/falcon-40b with any of the following classes:\
          \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.falcon.modeling_falcon.FalconForCausalLM'>).\
          \ See the original errors:\r\nwhile loading with AutoModelForCausalLM, an\
          \ error is thrown:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3428, in _load_pretrained_model\r\n    raise ValueError(\r\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\r\
          \n\r\nwhile loading with FalconForCausalLM, an error is thrown:\r\nTraceback\
          \ (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3428, in _load_pretrained_model\r\n    raise ValueError(\r\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.'''\r\
          \n\r\ni have a 16go ram macbook pro with no graphic card\r\n\r\ndo you know\
          \ what i can do?"
        updatedAt: '2023-10-21T20:08:30.491Z'
      numEdits: 0
      reactions: []
    id: 65342fbe484d775cb0c07642
    type: comment
  author: louvivien
  content: "When i try the code from the readme i get this error:\r\n\r\n'''\r\nvivien@MBP-de-Vivien\
    \ FalconLLM % python3 falconllm.py\r\n\r\nWARNING: You are currently loading Falcon\
    \ using legacy code contained in the model repository. Falcon has now been fully\
    \ ported into the Hugging Face transformers library. For the most up-to-date and\
    \ high-performance version of the Falcon model code, please update to the latest\
    \ version of transformers and then load the model without the trust_remote_code=True\
    \ argument.\r\n\r\nThe argument `trust_remote_code` is to be used with Auto classes.\
    \ It has no effect here and is ignored.\r\n\r\nTraceback (most recent call last):\r\
    \n  File \"/Users/vivien/Documents/FalconLLM/falconllm.py\", line 8, in <module>\r\
    \n    pipeline = transformers.pipeline(\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
    , line 834, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 282, in infer_framework_load_model\r\n    raise ValueError(\r\nValueError:\
    \ Could not load model tiiuae/falcon-40b with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.falcon.modeling_falcon.FalconForCausalLM'>).\
    \ See the original errors:\r\nwhile loading with AutoModelForCausalLM, an error\
    \ is thrown:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n   \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3428, in _load_pretrained_model\r\n    raise ValueError(\r\nValueError:\
    \ The current `device_map` had weights offloaded to the disk. Please provide an\
    \ `offload_folder` for them. Alternatively, make sure you have `safetensors` installed\
    \ if the model you are using offers the weights in this format.\r\n\r\nwhile loading\
    \ with FalconForCausalLM, an error is thrown:\r\nTraceback (most recent call last):\r\
    \n  File \"/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\",\
    \ line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n   \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3428, in _load_pretrained_model\r\n    raise ValueError(\r\nValueError:\
    \ The current `device_map` had weights offloaded to the disk. Please provide an\
    \ `offload_folder` for them. Alternatively, make sure you have `safetensors` installed\
    \ if the model you are using offers the weights in this format.'''\r\n\r\ni have\
    \ a 16go ram macbook pro with no graphic card\r\n\r\ndo you know what i can do?"
  created_at: 2023-10-21 19:08:30+00:00
  edited: false
  hidden: false
  id: 65342fbe484d775cb0c07642
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 108
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: pb when testing the model
