!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SpiridonSunRotator
conflicting_files:
- modelling_RW.py
created_at: 2023-05-28 12:12:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
      fullname: Denis Kuznedelev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiridonSunRotator
      type: user
    createdAt: '2023-05-28T13:12:54.000Z'
    data:
      edited: true
      editors:
      - SpiridonSunRotator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
          fullname: Denis Kuznedelev
          isHf: false
          isPro: false
          name: SpiridonSunRotator
          type: user
        html: "<p>I've observed that loading this model to float16 or bfloat16 lead\
          \ to bug</p>\n<pre><code>modelling_RW.py\", line 289, in forward\n attn_output\
          \ = F.scaled_dot_product_attention(\nRuntimeError: Expected query, key,\
          \ and value to have the same dtype, but got query.dtype: float key.dtype:\
          \ float and value.dtype: c10::Half instead.\n</code></pre>\n<p>I've encountered\
          \ this error when running main script from <a rel=\"nofollow\" href=\"https://github.com/EleutherAI/lm-evaluation-harness\"\
          >lm-eval-harness</a> :</p>\n<pre><code>python main.py \\\n    --model hf-causal\
          \ \\\n    --model_args pretrained=tiiuae/falcon-7b,dtype=float16 \\\n  \
          \  --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n    --batch_size\
          \ 1\n</code></pre>\n<p>This is because the output of <code>cos_sin</code>\
          \ method of <code>RotaryEmbedding</code> class is <code>float32</code>.<br>In\
          \ this commit I propose a simple fix for the model to work successfully\
          \ with half precision.</p>\n"
        raw: "I've observed that loading this model to float16 or bfloat16 lead to\
          \ bug\n```\nmodelling_RW.py\", line 289, in forward\n attn_output = F.scaled_dot_product_attention(\n\
          RuntimeError: Expected query, key, and value to have the same dtype, but\
          \ got query.dtype: float key.dtype: float and value.dtype: c10::Half instead.\n\
          ```\nI've encountered this error when running main script from [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)\
          \ :\n```\npython main.py \\\n    --model hf-causal \\\n    --model_args\
          \ pretrained=tiiuae/falcon-7b,dtype=float16 \\\n    --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge\
          \ \\\n    --batch_size 1\n```\nThis is because the output of `cos_sin` method\
          \ of `RotaryEmbedding` class is `float32`.\nIn this commit I propose a simple\
          \ fix for the model to work successfully with half precision."
        updatedAt: '2023-05-29T07:58:16.929Z'
      numEdits: 1
      reactions: []
    id: 6473535663001a0002c9bc63
    type: comment
  author: SpiridonSunRotator
  content: "I've observed that loading this model to float16 or bfloat16 lead to bug\n\
    ```\nmodelling_RW.py\", line 289, in forward\n attn_output = F.scaled_dot_product_attention(\n\
    RuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype:\
    \ float key.dtype: float and value.dtype: c10::Half instead.\n```\nI've encountered\
    \ this error when running main script from [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)\
    \ :\n```\npython main.py \\\n    --model hf-causal \\\n    --model_args pretrained=tiiuae/falcon-7b,dtype=float16\
    \ \\\n    --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n    --batch_size\
    \ 1\n```\nThis is because the output of `cos_sin` method of `RotaryEmbedding`\
    \ class is `float32`.\nIn this commit I propose a simple fix for the model to\
    \ work successfully with half precision."
  created_at: 2023-05-28 12:12:54+00:00
  edited: true
  hidden: false
  id: 6473535663001a0002c9bc63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
      fullname: Denis Kuznedelev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiridonSunRotator
      type: user
    createdAt: '2023-05-28T13:12:54.000Z'
    data:
      oid: 89d7ce17859b3a60d194f292d578d62b23f63302
      parents:
      - 6e61c89591cc7a3328d4010ceaf701dfec03fd18
      subject: Update modelling_RW.py
    id: '647353560000000000000000'
    type: commit
  author: SpiridonSunRotator
  created_at: 2023-05-28 12:12:54+00:00
  id: '647353560000000000000000'
  oid: 89d7ce17859b3a60d194f292d578d62b23f63302
  summary: Update modelling_RW.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-05-28T15:48:41.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<p>I can confirm this fix works for inference - however, it seems you
          have introduced a duplicate <code>forward()</code> method instead of updating
          the existing one?</p>

          '
        raw: I can confirm this fix works for inference - however, it seems you have
          introduced a duplicate `forward()` method instead of updating the existing
          one?
        updatedAt: '2023-05-28T15:48:41.364Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pcuenq
    id: 647377d96cff2f867205b64c
    type: comment
  author: lewtun
  content: I can confirm this fix works for inference - however, it seems you have
    introduced a duplicate `forward()` method instead of updating the existing one?
  created_at: 2023-05-28 14:48:41+00:00
  edited: false
  hidden: false
  id: 647377d96cff2f867205b64c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
      fullname: Denis Kuznedelev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiridonSunRotator
      type: user
    createdAt: '2023-05-28T19:44:41.000Z'
    data:
      oid: 239b4f8fbcad7af65e1b8689b6dc864fe8754394
      parents:
      - 89d7ce17859b3a60d194f292d578d62b23f63302
      subject: Update modelling_RW.py
    id: 6473af290000000000000000
    type: commit
  author: SpiridonSunRotator
  created_at: 2023-05-28 18:44:41+00:00
  id: 6473af290000000000000000
  oid: 239b4f8fbcad7af65e1b8689b6dc864fe8754394
  summary: Update modelling_RW.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
      fullname: Denis Kuznedelev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiridonSunRotator
      type: user
    createdAt: '2023-05-28T19:47:30.000Z'
    data:
      edited: false
      editors:
      - SpiridonSunRotator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
          fullname: Denis Kuznedelev
          isHf: false
          isPro: false
          name: SpiridonSunRotator
          type: user
        html: '<p>Sorry, actually I wanted is to slightly update forward to output
          the same type given in query and key. Fixed.</p>

          '
        raw: Sorry, actually I wanted is to slightly update forward to output the
          same type given in query and key. Fixed.
        updatedAt: '2023-05-28T19:47:30.329Z'
      numEdits: 0
      reactions: []
    id: 6473afd2352c94a20dda01b6
    type: comment
  author: SpiridonSunRotator
  content: Sorry, actually I wanted is to slightly update forward to output the same
    type given in query and key. Fixed.
  created_at: 2023-05-28 18:47:30+00:00
  edited: false
  hidden: false
  id: 6473afd2352c94a20dda01b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5bd26c0b59efc800107253c2f6c5832f.svg
      fullname: collectiv
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: akashcollectiv
      type: user
    createdAt: '2023-05-28T20:28:32.000Z'
    data:
      edited: true
      editors:
      - akashcollectiv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5bd26c0b59efc800107253c2f6c5832f.svg
          fullname: collectiv
          isHf: false
          isPro: true
          name: akashcollectiv
          type: user
        html: '<p>it worked yesterday for me , clone new one  getting allocation exceed
          issues , while running it on 80GB a100</p>

          <pre><code>return forward_call(*args, **kwargs)

          </code></pre>

          <p>  File "/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py",
          line 93, in forward<br>    return (q * cos) + (rotate_half(q) * sin), (k
          * cos) + (rotate_half(k) * sin)<br>torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity;
          77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>my bad forgot to install<br>pip install xformers :P<br>still same error
          : (</p>

          '
        raw: "it worked yesterday for me , clone new one  getting allocation exceed\
          \ issues , while running it on 80GB a100\n\n\n    return forward_call(*args,\
          \ **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
          , line 93, in forward\n    return (q * cos) + (rotate_half(q) * sin), (k\
          \ * cos) + (rotate_half(k) * sin)\ntorch.cuda.OutOfMemoryError: CUDA out\
          \ of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity;\
          \ 77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total\
          \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          \n\n\nmy bad forgot to install \npip install xformers :P\nstill same error\
          \ : ("
        updatedAt: '2023-05-28T20:58:24.021Z'
      numEdits: 3
      reactions: []
    id: 6473b9706cff2f867209edf5
    type: comment
  author: akashcollectiv
  content: "it worked yesterday for me , clone new one  getting allocation exceed\
    \ issues , while running it on 80GB a100\n\n\n    return forward_call(*args, **kwargs)\n\
    \  File \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
    , line 93, in forward\n    return (q * cos) + (rotate_half(q) * sin), (k * cos)\
    \ + (rotate_half(k) * sin)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated;\
    \ 832.00 KiB free; 78.13 GiB reserved in total by PyTorch) If reserved memory\
    \ is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\n\
    my bad forgot to install \npip install xformers :P\nstill same error : ("
  created_at: 2023-05-28 19:28:32+00:00
  edited: true
  hidden: false
  id: 6473b9706cff2f867209edf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
      fullname: Denis Kuznedelev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiridonSunRotator
      type: user
    createdAt: '2023-05-29T04:29:00.000Z'
    data:
      edited: false
      editors:
      - SpiridonSunRotator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654452258405-noauth.jpeg?w=200&h=200&f=face
          fullname: Denis Kuznedelev
          isHf: false
          isPro: false
          name: SpiridonSunRotator
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akashcollectiv&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/akashcollectiv\"\
          >@<span class=\"underline\">akashcollectiv</span></a></span>\n\n\t</span></span>\
          \  this model occupies almost all A100 capacity if loaded on a single GPU.\
          \ I think there is not enough memory for all but very short sequences.</p>\n"
        raw: '@akashcollectiv  this model occupies almost all A100 capacity if loaded
          on a single GPU. I think there is not enough memory for all but very short
          sequences.'
        updatedAt: '2023-05-29T04:29:00.688Z'
      numEdits: 0
      reactions: []
    id: 64742a0c63001a0002d816e1
    type: comment
  author: SpiridonSunRotator
  content: '@akashcollectiv  this model occupies almost all A100 capacity if loaded
    on a single GPU. I think there is not enough memory for all but very short sequences.'
  created_at: 2023-05-29 03:29:00+00:00
  edited: false
  hidden: false
  id: 64742a0c63001a0002d816e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-05-29T21:12:13.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<blockquote>

          <p>Sorry, actually I wanted is to slightly update forward to output the
          same type given in query and key. Fixed.</p>

          </blockquote>

          <p>Thanks! I can confirm this works as intended and it also enable loading
          the model in 8-bit which is great for inference :)</p>

          '
        raw: '> Sorry, actually I wanted is to slightly update forward to output the
          same type given in query and key. Fixed.


          Thanks! I can confirm this works as intended and it also enable loading
          the model in 8-bit which is great for inference :)'
        updatedAt: '2023-05-29T21:12:13.557Z'
      numEdits: 0
      reactions: []
    id: 6475152d82907acdddf4fced
    type: comment
  author: lewtun
  content: '> Sorry, actually I wanted is to slightly update forward to output the
    same type given in query and key. Fixed.


    Thanks! I can confirm this works as intended and it also enable loading the model
    in 8-bit which is great for inference :)'
  created_at: 2023-05-29 20:12:13+00:00
  edited: false
  hidden: false
  id: 6475152d82907acdddf4fced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:10:39.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>This change will unfortunately change things for bfloat16, due to
          numerical precision. Which is the only dtype we have properly validated
          the performance with, we should expect some degradation of model quality
          in fp16.</p>

          <p>In particular the cos/sin needs to be applied in full precision, as was
          done in the original code.  I believe only adding the q.dtype to the rotary
          forward should be sufficient, I''ll take care of it today.</p>

          '
        raw: 'This change will unfortunately change things for bfloat16, due to numerical
          precision. Which is the only dtype we have properly validated the performance
          with, we should expect some degradation of model quality in fp16.


          In particular the cos/sin needs to be applied in full precision, as was
          done in the original code.  I believe only adding the q.dtype to the rotary
          forward should be sufficient, I''ll take care of it today.'
        updatedAt: '2023-05-30T06:10:39.808Z'
      numEdits: 0
      reactions: []
    id: 6475935f09e7732263319cc3
    type: comment
  author: FalconLLM
  content: 'This change will unfortunately change things for bfloat16, due to numerical
    precision. Which is the only dtype we have properly validated the performance
    with, we should expect some degradation of model quality in fp16.


    In particular the cos/sin needs to be applied in full precision, as was done in
    the original code.  I believe only adding the q.dtype to the rotary forward should
    be sufficient, I''ll take care of it today.'
  created_at: 2023-05-30 05:10:39+00:00
  edited: false
  hidden: false
  id: 6475935f09e7732263319cc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:18:15.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Should be fixed now, though as previously mentioned, inference with
          dtypes other than bfloat16 may incur model degradation.</p>

          '
        raw: Should be fixed now, though as previously mentioned, inference with dtypes
          other than bfloat16 may incur model degradation.
        updatedAt: '2023-05-30T06:18:15.501Z'
      numEdits: 0
      reactions: []
    id: 64759527ab17a37d0b152d9b
    type: comment
  author: FalconLLM
  content: Should be fixed now, though as previously mentioned, inference with dtypes
    other than bfloat16 may incur model degradation.
  created_at: 2023-05-30 05:18:15+00:00
  edited: false
  hidden: false
  id: 64759527ab17a37d0b152d9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:03:11.000Z'
    data:
      status: closed
    id: 64759fafe9b57ce0caa116e7
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 06:03:11+00:00
  id: 64759fafe9b57ce0caa116e7
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 13
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: refs/heads/main
title: Update modelling_RW.py
