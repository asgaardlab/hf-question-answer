!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Toaster496
conflicting_files: null
created_at: 2023-06-06 22:57:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
      fullname: Taj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Toaster496
      type: user
    createdAt: '2023-06-06T23:57:04.000Z'
    data:
      edited: false
      editors:
      - Toaster496
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941249668598175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
          fullname: Taj
          isHf: false
          isPro: false
          name: Toaster496
          type: user
        html: '<p>how much Vram/Hotswap Ram does it take to run Falcon 40b any1 got
          idears??</p>

          '
        raw: how much Vram/Hotswap Ram does it take to run Falcon 40b any1 got idears??
        updatedAt: '2023-06-06T23:57:04.122Z'
      numEdits: 0
      reactions: []
    id: 647fc7d0d0cd8be13e66bdef
    type: comment
  author: Toaster496
  content: how much Vram/Hotswap Ram does it take to run Falcon 40b any1 got idears??
  created_at: 2023-06-06 22:57:04+00:00
  edited: false
  hidden: false
  id: 647fc7d0d0cd8be13e66bdef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b645356e001a5359b5391effa837ccdf.svg
      fullname: Leonardo Apolonio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leoapolonio
      type: user
    createdAt: '2023-06-07T02:43:14.000Z'
    data:
      edited: false
      editors:
      - leoapolonio
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.3318563401699066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b645356e001a5359b5391effa837ccdf.svg
          fullname: Leonardo Apolonio
          isHf: false
          isPro: false
          name: leoapolonio
          type: user
        html: '<p>157G</p>

          '
        raw: 157G
        updatedAt: '2023-06-07T02:43:14.174Z'
      numEdits: 0
      reactions: []
    id: 647feec2cbb8294ed8106102
    type: comment
  author: leoapolonio
  content: 157G
  created_at: 2023-06-07 01:43:14+00:00
  edited: false
  hidden: false
  id: 647feec2cbb8294ed8106102
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
      fullname: Taj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Toaster496
      type: user
    createdAt: '2023-06-07T07:19:57.000Z'
    data:
      edited: false
      editors:
      - Toaster496
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.976455807685852
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
          fullname: Taj
          isHf: false
          isPro: false
          name: Toaster496
          type: user
        html: '<p>? what cards? how is that Vram or RAM?</p>

          '
        raw: '? what cards? how is that Vram or RAM?'
        updatedAt: '2023-06-07T07:19:57.042Z'
      numEdits: 0
      reactions: []
    id: 64802f9d15bde163f4b0f109
    type: comment
  author: Toaster496
  content: '? what cards? how is that Vram or RAM?'
  created_at: 2023-06-07 06:19:57+00:00
  edited: false
  hidden: false
  id: 64802f9d15bde163f4b0f109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
      fullname: serin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serin32
      type: user
    createdAt: '2023-06-07T14:18:13.000Z'
    data:
      edited: true
      editors:
      - serin32
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9694045782089233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e769cd44c0d111892e20285fae33a677.svg
          fullname: serin
          isHf: false
          isPro: false
          name: serin32
          type: user
        html: '<p>Depends on if you want to do inference in 32, 16, 8 or 4 bit, but
          at full 32 bit I think it''s about 80GB of VRAM.<br>Correction: 16 bit is
          80ish GB and 32 bit would be around 160ish GB I believe.  Thanks Mikael110
          was thinking about 16 bit and not 32 when I wrote this.</p>

          '
        raw: 'Depends on if you want to do inference in 32, 16, 8 or 4 bit, but at
          full 32 bit I think it''s about 80GB of VRAM.

          Correction: 16 bit is 80ish GB and 32 bit would be around 160ish GB I believe.  Thanks
          Mikael110 was thinking about 16 bit and not 32 when I wrote this.'
        updatedAt: '2023-06-08T11:42:36.466Z'
      numEdits: 1
      reactions: []
    id: 648091a59aafd41918a95158
    type: comment
  author: serin32
  content: 'Depends on if you want to do inference in 32, 16, 8 or 4 bit, but at full
    32 bit I think it''s about 80GB of VRAM.

    Correction: 16 bit is 80ish GB and 32 bit would be around 160ish GB I believe.  Thanks
    Mikael110 was thinking about 16 bit and not 32 when I wrote this.'
  created_at: 2023-06-07 13:18:13+00:00
  edited: true
  hidden: false
  id: 648091a59aafd41918a95158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-06-08T04:43:14.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9757717847824097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<p>With 8bit loading it consumes ~46GB of VRAM, and with 4bit loading
          it takes ~24GB VRAM. Those numbers exclude OS headroom, so don''t expect
          4bit to fit on actual 24GB cards, and 8bit will be a tight squeeze on 48GB
          cards, you will probably OOM once the context gets even remotely long. I
          can''t give numbers for 16bit and 32bit since they OOM on the A100 80GB
          which I was testing on. But given that even 16bit is too big for the card
          I''m quite confident that 32bit is quite a bit larger than 80GB. Maybe that''s
          the number leoapolonio was referencing? I could definitively see it actually
          being that high for full 32bit inference.</p>

          '
        raw: With 8bit loading it consumes ~46GB of VRAM, and with 4bit loading it
          takes ~24GB VRAM. Those numbers exclude OS headroom, so don't expect 4bit
          to fit on actual 24GB cards, and 8bit will be a tight squeeze on 48GB cards,
          you will probably OOM once the context gets even remotely long. I can't
          give numbers for 16bit and 32bit since they OOM on the A100 80GB which I
          was testing on. But given that even 16bit is too big for the card I'm quite
          confident that 32bit is quite a bit larger than 80GB. Maybe that's the number
          leoapolonio was referencing? I could definitively see it actually being
          that high for full 32bit inference.
        updatedAt: '2023-06-08T05:32:57.252Z'
      numEdits: 5
      reactions: []
    id: 64815c6240facadc557af232
    type: comment
  author: Mikael110
  content: With 8bit loading it consumes ~46GB of VRAM, and with 4bit loading it takes
    ~24GB VRAM. Those numbers exclude OS headroom, so don't expect 4bit to fit on
    actual 24GB cards, and 8bit will be a tight squeeze on 48GB cards, you will probably
    OOM once the context gets even remotely long. I can't give numbers for 16bit and
    32bit since they OOM on the A100 80GB which I was testing on. But given that even
    16bit is too big for the card I'm quite confident that 32bit is quite a bit larger
    than 80GB. Maybe that's the number leoapolonio was referencing? I could definitively
    see it actually being that high for full 32bit inference.
  created_at: 2023-06-08 03:43:14+00:00
  edited: true
  hidden: false
  id: 64815c6240facadc557af232
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
      fullname: Taj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Toaster496
      type: user
    createdAt: '2023-06-08T06:09:06.000Z'
    data:
      edited: false
      editors:
      - Toaster496
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.5090294480323792
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
          fullname: Taj
          isHf: false
          isPro: false
          name: Toaster496
          type: user
        html: '<p>THANK YOU!</p>

          '
        raw: THANK YOU!
        updatedAt: '2023-06-08T06:09:06.164Z'
      numEdits: 0
      reactions: []
    id: 6481708215c5dc5290612d11
    type: comment
  author: Toaster496
  content: THANK YOU!
  created_at: 2023-06-08 05:09:06+00:00
  edited: false
  hidden: false
  id: 6481708215c5dc5290612d11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671801056760-noauth.png?w=200&h=200&f=face
      fullname: Charles Chudant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cchudant
      type: user
    createdAt: '2023-06-08T14:35:33.000Z'
    data:
      edited: false
      editors:
      - cchudant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.863810658454895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671801056760-noauth.png?w=200&h=200&f=face
          fullname: Charles Chudant
          isHf: false
          isPro: false
          name: cchudant
          type: user
        html: '<p>Hi, the model is trained in bfloat16, not float32 - you need 40B
          x 2 byte per param = ~80Go to run it</p>

          '
        raw: Hi, the model is trained in bfloat16, not float32 - you need 40B x 2
          byte per param = ~80Go to run it
        updatedAt: '2023-06-08T14:35:33.446Z'
      numEdits: 0
      reactions: []
    id: 6481e7359021d8071d9015ee
    type: comment
  author: cchudant
  content: Hi, the model is trained in bfloat16, not float32 - you need 40B x 2 byte
    per param = ~80Go to run it
  created_at: 2023-06-08 13:35:33+00:00
  edited: false
  hidden: false
  id: 6481e7359021d8071d9015ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:40:58.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8901394605636597
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>We recommend 80-100GB to run inference on Falcon-40B comfortably.
          </p>

          '
        raw: 'We recommend 80-100GB to run inference on Falcon-40B comfortably. '
        updatedAt: '2023-06-09T14:40:58.691Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648339faa5402eb30b6a8853
    id: 648339faa5402eb30b6a8851
    type: comment
  author: FalconLLM
  content: 'We recommend 80-100GB to run inference on Falcon-40B comfortably. '
  created_at: 2023-06-09 13:40:58+00:00
  edited: false
  hidden: false
  id: 648339faa5402eb30b6a8851
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:40:58.000Z'
    data:
      status: closed
    id: 648339faa5402eb30b6a8853
    type: status-change
  author: FalconLLM
  created_at: 2023-06-09 13:40:58+00:00
  id: 648339faa5402eb30b6a8853
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: how much Vram does it take to run Falcon 40b
