!!python/object:huggingface_hub.community.DiscussionWithDetails
author: uglydumpling
conflicting_files: null
created_at: 2023-06-14 15:27:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0baebcb53d3a3ee421532eb164a9a6.svg
      fullname: Blue
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uglydumpling
      type: user
    createdAt: '2023-06-14T16:27:37.000Z'
    data:
      edited: true
      editors:
      - uglydumpling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5847660899162292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0baebcb53d3a3ee421532eb164a9a6.svg
          fullname: Blue
          isHf: false
          isPro: false
          name: uglydumpling
          type: user
        html: '<p>Keep getting the following error even after using<br>tokenizer =
          AutoTokenizer.from_pretrained("/falcon-40b",return_token_type_ids=False)<br>model
          = AutoModelForCausalLM.from_pretrained(<br>    "/falcon-40b",<br>    trust_remote_code=True,<br>    torch_dtype="auto",<br>    device_map="auto<br>)<br>tokens
          = model.generate(<br>    **input_ids<br>)<br>ValueError: The following <code>model_kwargs</code>
          are not used by the model: [''token_type_ids''] (note: typos in the generate
          arguments will also show up in this list)</p>

          '
        raw: "Keep getting the following error even after using\ntokenizer = AutoTokenizer.from_pretrained(\"\
          /falcon-40b\",return_token_type_ids=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"/falcon-40b\", \n    trust_remote_code=True,\n    torch_dtype=\"\
          auto\",\n    device_map=\"auto\n)\ntokens = model.generate(\n    **input_ids\n\
          )\nValueError: The following `model_kwargs` are not used by the model: ['token_type_ids']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-06-14T17:06:33.836Z'
      numEdits: 3
      reactions: []
    id: 6489ea791441602453986923
    type: comment
  author: uglydumpling
  content: "Keep getting the following error even after using\ntokenizer = AutoTokenizer.from_pretrained(\"\
    /falcon-40b\",return_token_type_ids=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"/falcon-40b\", \n    trust_remote_code=True,\n    torch_dtype=\"auto\"\
    ,\n    device_map=\"auto\n)\ntokens = model.generate(\n    **input_ids\n)\nValueError:\
    \ The following `model_kwargs` are not used by the model: ['token_type_ids'] (note:\
    \ typos in the generate arguments will also show up in this list)"
  created_at: 2023-06-14 15:27:37+00:00
  edited: true
  hidden: false
  id: 6489ea791441602453986923
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 60
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: 'Keep getting ''model_kwargs` are not used by the model: [''token_type_ids''] '
