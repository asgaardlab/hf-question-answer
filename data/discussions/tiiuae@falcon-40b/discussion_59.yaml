!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mikeytrw
conflicting_files: null
created_at: 2023-06-12 18:57:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-06-12T19:57:11.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9379310011863708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>Hi, I find very poor performance during inference compared to Llama
          based models.. Understandably given the param size there should be a difference
          between 30b and 40b, but it''s half the speed or worse when generating tokens.</p>

          <p>I''m running the FP16 version on 2xA100 80GB</p>

          <p>Am I missing something or misconfiguring it?</p>

          '
        raw: "Hi, I find very poor performance during inference compared to Llama\
          \ based models.. Understandably given the param size there should be a difference\
          \ between 30b and 40b, but it's half the speed or worse when generating\
          \ tokens.\r\n\r\nI'm running the FP16 version on 2xA100 80GB\r\n\r\nAm I\
          \ missing something or misconfiguring it?"
        updatedAt: '2023-06-12T19:57:11.681Z'
      numEdits: 0
      reactions: []
    id: 64877897d67acb8ab6e9e272
    type: comment
  author: mikeytrw
  content: "Hi, I find very poor performance during inference compared to Llama based\
    \ models.. Understandably given the param size there should be a difference between\
    \ 30b and 40b, but it's half the speed or worse when generating tokens.\r\n\r\n\
    I'm running the FP16 version on 2xA100 80GB\r\n\r\nAm I missing something or misconfiguring\
    \ it?"
  created_at: 2023-06-12 18:57:11+00:00
  edited: false
  hidden: false
  id: 64877897d67acb8ab6e9e272
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e596a89d174967c591c60de2b723e1ad.svg
      fullname: Mohamed Naji Aboo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NajiAboo
      type: user
    createdAt: '2023-06-15T13:17:24.000Z'
    data:
      edited: false
      editors:
      - NajiAboo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.995074987411499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e596a89d174967c591c60de2b723e1ad.svg
          fullname: Mohamed Naji Aboo
          isHf: false
          isPro: false
          name: NajiAboo
          type: user
        html: '<p>Hi, </p>

          <p> Did you able to figure out it. I am also facing same issue</p>

          '
        raw: "Hi, \n\n Did you able to figure out it. I am also facing same issue"
        updatedAt: '2023-06-15T13:17:24.016Z'
      numEdits: 0
      reactions: []
    id: 648b0f6460292bdb12e37ec2
    type: comment
  author: NajiAboo
  content: "Hi, \n\n Did you able to figure out it. I am also facing same issue"
  created_at: 2023-06-15 12:17:24+00:00
  edited: false
  hidden: false
  id: 648b0f6460292bdb12e37ec2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-06-15T18:58:50.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9716367721557617
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>No, I''m currently running the H2o OpenAssist fine tuned model on
          8xA100 80GB and it''s still sloooooow, well, at least compared to Llama.</p>

          '
        raw: No, I'm currently running the H2o OpenAssist fine tuned model on 8xA100
          80GB and it's still sloooooow, well, at least compared to Llama.
        updatedAt: '2023-06-15T18:58:50.685Z'
      numEdits: 0
      reactions: []
    id: 648b5f6ab2dfa02008f9ff2c
    type: comment
  author: mikeytrw
  content: No, I'm currently running the H2o OpenAssist fine tuned model on 8xA100
    80GB and it's still sloooooow, well, at least compared to Llama.
  created_at: 2023-06-15 17:58:50+00:00
  edited: false
  hidden: false
  id: 648b5f6ab2dfa02008f9ff2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
      fullname: Muhammad Ajmal Siddiqui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajmalsiddiqui
      type: user
    createdAt: '2023-06-15T19:06:17.000Z'
    data:
      edited: true
      editors:
      - ajmalsiddiqui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9190492033958435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
          fullname: Muhammad Ajmal Siddiqui
          isHf: false
          isPro: false
          name: ajmalsiddiqui
          type: user
        html: '<p>Please run inference using safetensor, I would suggest to use hugging
          text generation inference with falcon model. It will be faster. Additionallly,
          number of token will affect the speed. Please try and share your feedback.
          Thanks </p>

          '
        raw: 'Please run inference using safetensor, I would suggest to use hugging
          text generation inference with falcon model. It will be faster. Additionallly,
          number of token will affect the speed. Please try and share your feedback.
          Thanks '
        updatedAt: '2023-06-15T19:06:28.895Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - li-ping
    id: 648b61295f9c5cd0107dbaf6
    type: comment
  author: ajmalsiddiqui
  content: 'Please run inference using safetensor, I would suggest to use hugging
    text generation inference with falcon model. It will be faster. Additionallly,
    number of token will affect the speed. Please try and share your feedback. Thanks '
  created_at: 2023-06-15 18:06:17+00:00
  edited: true
  hidden: false
  id: 648b61295f9c5cd0107dbaf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e596a89d174967c591c60de2b723e1ad.svg
      fullname: Mohamed Naji Aboo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NajiAboo
      type: user
    createdAt: '2023-06-16T10:39:02.000Z'
    data:
      edited: false
      editors:
      - NajiAboo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9370253682136536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e596a89d174967c591c60de2b723e1ad.svg
          fullname: Mohamed Naji Aboo
          isHf: false
          isPro: false
          name: NajiAboo
          type: user
        html: '<p>Thanks for the reply. Can you please share some samples or collab
          or github repo. Thanks for your time.</p>

          '
        raw: Thanks for the reply. Can you please share some samples or collab or
          github repo. Thanks for your time.
        updatedAt: '2023-06-16T10:39:02.203Z'
      numEdits: 0
      reactions: []
    id: 648c3bc6ec10366d8f5604e4
    type: comment
  author: NajiAboo
  content: Thanks for the reply. Can you please share some samples or collab or github
    repo. Thanks for your time.
  created_at: 2023-06-16 09:39:02+00:00
  edited: false
  hidden: false
  id: 648c3bc6ec10366d8f5604e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
      fullname: Muhammad Ajmal Siddiqui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajmalsiddiqui
      type: user
    createdAt: '2023-06-16T11:32:56.000Z'
    data:
      edited: false
      editors:
      - ajmalsiddiqui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7449096441268921
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
          fullname: Muhammad Ajmal Siddiqui
          isHf: false
          isPro: false
          name: ajmalsiddiqui
          type: user
        html: '<p>Please try<br><a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          '
        raw: 'Please try

          https://github.com/huggingface/text-generation-inference'
        updatedAt: '2023-06-16T11:32:56.117Z'
      numEdits: 0
      reactions: []
    id: 648c48682dd7247ce6f8c420
    type: comment
  author: ajmalsiddiqui
  content: 'Please try

    https://github.com/huggingface/text-generation-inference'
  created_at: 2023-06-16 10:32:56+00:00
  edited: false
  hidden: false
  id: 648c48682dd7247ce6f8c420
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
      fullname: Mike Wharton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeytrw
      type: user
    createdAt: '2023-06-17T20:41:28.000Z'
    data:
      edited: false
      editors:
      - mikeytrw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308163523674011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/133104d2b3cf3758268f9f7f7e64ac19.svg
          fullname: Mike Wharton
          isHf: false
          isPro: false
          name: mikeytrw
          type: user
        html: '<p>I don''t want to use text generation interface because I already
          have a server codebase this is part of. Can you explain a bit more about
          using safetensor? Or link to a code example. Thanks</p>

          '
        raw: I don't want to use text generation interface because I already have
          a server codebase this is part of. Can you explain a bit more about using
          safetensor? Or link to a code example. Thanks
        updatedAt: '2023-06-17T20:41:28.350Z'
      numEdits: 0
      reactions: []
    id: 648e1a78385b842618189da0
    type: comment
  author: mikeytrw
  content: I don't want to use text generation interface because I already have a
    server codebase this is part of. Can you explain a bit more about using safetensor?
    Or link to a code example. Thanks
  created_at: 2023-06-17 19:41:28+00:00
  edited: false
  hidden: false
  id: 648e1a78385b842618189da0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
      fullname: Muhammad Ajmal Siddiqui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajmalsiddiqui
      type: user
    createdAt: '2023-06-18T18:14:20.000Z'
    data:
      edited: false
      editors:
      - ajmalsiddiqui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350582957267761
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dda512b380966de00fa19d2499921bea.svg
          fullname: Muhammad Ajmal Siddiqui
          isHf: false
          isPro: false
          name: ajmalsiddiqui
          type: user
        html: '<p>Dear Mike, the same thing i need in my server codebase and could
          not explore  yet the safetensors implenentation that exists in huggging
          face text generation. I will do but at later stage. In your case, i would
          suggest to implement your own using the logic and implementation already
          done in text generation by hugging face.</p>

          '
        raw: Dear Mike, the same thing i need in my server codebase and could not
          explore  yet the safetensors implenentation that exists in huggging face
          text generation. I will do but at later stage. In your case, i would suggest
          to implement your own using the logic and implementation already done in
          text generation by hugging face.
        updatedAt: '2023-06-18T18:14:20.043Z'
      numEdits: 0
      reactions: []
    id: 648f497c8259e7796fd507ce
    type: comment
  author: ajmalsiddiqui
  content: Dear Mike, the same thing i need in my server codebase and could not explore  yet
    the safetensors implenentation that exists in huggging face text generation. I
    will do but at later stage. In your case, i would suggest to implement your own
    using the logic and implementation already done in text generation by hugging
    face.
  created_at: 2023-06-18 17:14:20+00:00
  edited: false
  hidden: false
  id: 648f497c8259e7796fd507ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104ceca27d6adc15caece4669efdd3bb.svg
      fullname: Somesh Fengade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Someshfengde
      type: user
    createdAt: '2023-06-19T13:09:25.000Z'
    data:
      edited: false
      editors:
      - Someshfengde
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210099577903748
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104ceca27d6adc15caece4669efdd3bb.svg
          fullname: Somesh Fengade
          isHf: false
          isPro: false
          name: Someshfengde
          type: user
        html: '<p>I''m also inferencing on the same 2 x A100 80 gb GPUs the inference
          time is high.</p>

          '
        raw: I'm also inferencing on the same 2 x A100 80 gb GPUs the inference time
          is high.
        updatedAt: '2023-06-19T13:09:25.972Z'
      numEdits: 0
      reactions: []
    id: 64905385189ba64b8416bd2f
    type: comment
  author: Someshfengde
  content: I'm also inferencing on the same 2 x A100 80 gb GPUs the inference time
    is high.
  created_at: 2023-06-19 12:09:25+00:00
  edited: false
  hidden: false
  id: 64905385189ba64b8416bd2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9013215e0c5966eb7054f1b5f4a7181f.svg
      fullname: Sekhar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sekharvth
      type: user
    createdAt: '2023-06-21T11:28:18.000Z'
    data:
      edited: false
      editors:
      - sekharvth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9329997897148132
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9013215e0c5966eb7054f1b5f4a7181f.svg
          fullname: Sekhar V
          isHf: false
          isPro: false
          name: sekharvth
          type: user
        html: '<p>Inference time for out of the box falcon models is directly proportional
          to max_new_tokens being generated. This is because of a faulty incorporation
          of the <code>past_key_values</code>  and rotary embeddings , former is used
          to cache the transformer keys and values as each token gets generated so
          that it''s not recomputed at every timestep, latter is responsible for the
          embeddings. There''s also a bug in the causal masking section of the attention
          mechanism being called. All this has been mentioned in <a href="https://huggingface.co/tiiuae/falcon-40b/discussions/48">this</a>
          thread</p>

          '
        raw: Inference time for out of the box falcon models is directly proportional
          to max_new_tokens being generated. This is because of a faulty incorporation
          of the `past_key_values`  and rotary embeddings , former is used to cache
          the transformer keys and values as each token gets generated so that it's
          not recomputed at every timestep, latter is responsible for the embeddings.
          There's also a bug in the causal masking section of the attention mechanism
          being called. All this has been mentioned in [this](https://huggingface.co/tiiuae/falcon-40b/discussions/48)
          thread
        updatedAt: '2023-06-21T11:28:18.213Z'
      numEdits: 0
      reactions: []
    id: 6492ded2345112c38afa8a62
    type: comment
  author: sekharvth
  content: Inference time for out of the box falcon models is directly proportional
    to max_new_tokens being generated. This is because of a faulty incorporation of
    the `past_key_values`  and rotary embeddings , former is used to cache the transformer
    keys and values as each token gets generated so that it's not recomputed at every
    timestep, latter is responsible for the embeddings. There's also a bug in the
    causal masking section of the attention mechanism being called. All this has been
    mentioned in [this](https://huggingface.co/tiiuae/falcon-40b/discussions/48) thread
  created_at: 2023-06-21 10:28:18+00:00
  edited: false
  hidden: false
  id: 6492ded2345112c38afa8a62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
      fullname: Purushottam Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: puru22
      type: user
    createdAt: '2023-07-12T07:15:44.000Z'
    data:
      edited: true
      editors:
      - puru22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9332805275917053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356f54cb44e8ab6c51ca553c742db7e.svg
          fullname: Purushottam Sinha
          isHf: false
          isPro: false
          name: puru22
          type: user
        html: '<p>I have raised a pull request today fixing the slowness. Just change
          the modelling_RW.py with the one in this pull request. Let me know if you
          find any issue with this. Note that pretty much all falcon family models
          involve the same changes for speeding up the generation. As some people
          in this thread mention, its mainly because falcon model is recomputing everything
          from beginning for every next token generation.<br>This is the pull request.
          </p>

          <p><a href="https://huggingface.co/tiiuae/falcon-40b/discussions/85">https://huggingface.co/tiiuae/falcon-40b/discussions/85</a></p>

          '
        raw: "I have raised a pull request today fixing the slowness. Just change\
          \ the modelling_RW.py with the one in this pull request. Let me know if\
          \ you find any issue with this. Note that pretty much all falcon family\
          \ models involve the same changes for speeding up the generation. As some\
          \ people in this thread mention, its mainly because falcon model is recomputing\
          \ everything from beginning for every next token generation. \nThis is the\
          \ pull request. \n\nhttps://huggingface.co/tiiuae/falcon-40b/discussions/85"
        updatedAt: '2023-07-13T18:30:40.205Z'
      numEdits: 1
      reactions: []
    id: 64ae5320179421d320b72b02
    type: comment
  author: puru22
  content: "I have raised a pull request today fixing the slowness. Just change the\
    \ modelling_RW.py with the one in this pull request. Let me know if you find any\
    \ issue with this. Note that pretty much all falcon family models involve the\
    \ same changes for speeding up the generation. As some people in this thread mention,\
    \ its mainly because falcon model is recomputing everything from beginning for\
    \ every next token generation. \nThis is the pull request. \n\nhttps://huggingface.co/tiiuae/falcon-40b/discussions/85"
  created_at: 2023-07-12 06:15:44+00:00
  edited: true
  hidden: false
  id: 64ae5320179421d320b72b02
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 59
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Falcon models slow inference
