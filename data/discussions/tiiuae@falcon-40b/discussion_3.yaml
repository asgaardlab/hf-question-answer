!!python/object:huggingface_hub.community.DiscussionWithDetails
author: catid
conflicting_files: null
created_at: 2023-05-26 19:05:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
      fullname: Chris Taylor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: catid
      type: user
    createdAt: '2023-05-26T20:05:16.000Z'
    data:
      edited: false
      editors:
      - catid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
          fullname: Chris Taylor
          isHf: false
          isPro: false
          name: catid
          type: user
        html: '<p>Used example script with latest pytorch, einops, and transformers
          but it does not work:</p>

          <p>Traceback (most recent call last):<br>  File "/home/catid/sources/supercharger/test_falcon_basic.py",
          line 8, in <br>    pipeline = transformers.pipeline(<br>  File "/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 788, in pipeline<br>    framework, model = infer_framework_load_model(<br>  File
          "/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py",
          line 278, in infer_framework_load_model<br>    raise ValueError(f"Could
          not load model {model} with any of the following classes: {class_tuple}.")<br>ValueError:
          Could not load model tiiuae/falcon-40b with any of the following classes:
          (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          '
        raw: "Used example script with latest pytorch, einops, and transformers but\
          \ it does not work:\r\n\r\nTraceback (most recent call last):\r\n  File\
          \ \"/home/catid/sources/supercharger/test_falcon_basic.py\", line 8, in\
          \ <module>\r\n    pipeline = transformers.pipeline(\r\n  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 788, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
          \n  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 278, in infer_framework_load_model\r\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\r\nValueError: Could not load model tiiuae/falcon-40b with any of the\
          \ following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
        updatedAt: '2023-05-26T20:05:16.729Z'
      numEdits: 0
      reactions:
      - count: 19
        reaction: "\U0001F614"
        users:
        - marcelcastrobr
        - vladimirmoroz
        - vishaal27
        - karrarkazuya
        - Shasaur
        - chrislemke
        - pltrdy
        - franticboy95
        - rajeshradhakrishnan
        - jules-soria
        - shamEiNew
        - patti-j
        - ChinniAjay
        - valentinm8
        - paralin
        - elcolie
        - surya-narayanan
        - LLDDSS
        - MuLing
    id: 647110fcc6b7783c87b84e36
    type: comment
  author: catid
  content: "Used example script with latest pytorch, einops, and transformers but\
    \ it does not work:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/catid/sources/supercharger/test_falcon_basic.py\"\
    , line 8, in <module>\r\n    pipeline = transformers.pipeline(\r\n  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 788, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
    \n  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 278, in infer_framework_load_model\r\n    raise ValueError(f\"Could not\
    \ load model {model} with any of the following classes: {class_tuple}.\")\r\n\
    ValueError: Could not load model tiiuae/falcon-40b with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
  created_at: 2023-05-26 19:05:16+00:00
  edited: false
  hidden: false
  id: 647110fcc6b7783c87b84e36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44ed10bd04cd16b4d5ac7b633fe01f11.svg
      fullname: Matthew Koski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maccam912
      type: user
    createdAt: '2023-05-26T21:01:17.000Z'
    data:
      edited: true
      editors:
      - maccam912
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44ed10bd04cd16b4d5ac7b633fe01f11.svg
          fullname: Matthew Koski
          isHf: false
          isPro: false
          name: maccam912
          type: user
        html: '<p>Possibly related, I get <code>The model ''RWForCausalLM'' is not
          supported for text-generation</code></p>

          <p>I do see that this warning pops up on 7b, which goes on to work fine,
          so might be a misleading warning here, just thought I''d share it.</p>

          '
        raw: 'Possibly related, I get `The model ''RWForCausalLM'' is not supported
          for text-generation`


          I do see that this warning pops up on 7b, which goes on to work fine, so
          might be a misleading warning here, just thought I''d share it.'
        updatedAt: '2023-05-26T21:05:16.813Z'
      numEdits: 1
      reactions: []
    id: 64711e1d9e0a21d009936007
    type: comment
  author: maccam912
  content: 'Possibly related, I get `The model ''RWForCausalLM'' is not supported
    for text-generation`


    I do see that this warning pops up on 7b, which goes on to work fine, so might
    be a misleading warning here, just thought I''d share it.'
  created_at: 2023-05-26 20:01:17+00:00
  edited: true
  hidden: false
  id: 64711e1d9e0a21d009936007
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
      fullname: Leon Lahoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonlahoud
      type: user
    createdAt: '2023-05-26T21:20:51.000Z'
    data:
      edited: false
      editors:
      - leonlahoud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
          fullname: Leon Lahoud
          isHf: false
          isPro: false
          name: leonlahoud
          type: user
        html: '<p>the Model doesn''t work. I get the same error on 40B</p>

          <p>ValueError: Could not load model tiiuae/falcon-40b-instruct with any
          of the following classes: (&lt;class<br>''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class<br>''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''&gt;).</p>

          '
        raw: "the Model doesn't work. I get the same error on 40B\n\nValueError: Could\
          \ not load model tiiuae/falcon-40b-instruct with any of the following classes:\
          \ (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
        updatedAt: '2023-05-26T21:20:51.782Z'
      numEdits: 0
      reactions:
      - count: 10
        reaction: "\U0001F44D"
        users:
        - catid
        - jmugan
        - yuraai
        - karrarkazuya
        - pltrdy
        - YUJIA10
        - jasstionzyf
        - ChinniAjay
        - linuxem
        - sohailsiddiqui
      - count: 3
        reaction: "\U0001F614"
        users:
        - Shasaur
        - pltrdy
        - ChinniAjay
    id: 647122b39e0a21d00993d353
    type: comment
  author: leonlahoud
  content: "the Model doesn't work. I get the same error on 40B\n\nValueError: Could\
    \ not load model tiiuae/falcon-40b-instruct with any of the following classes:\
    \ (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class\
    \ \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
  created_at: 2023-05-26 20:20:51+00:00
  edited: false
  hidden: false
  id: 647122b39e0a21d00993d353
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
      fullname: Chris Taylor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: catid
      type: user
    createdAt: '2023-05-26T21:35:33.000Z'
    data:
      edited: false
      editors:
      - catid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
          fullname: Chris Taylor
          isHf: false
          isPro: false
          name: catid
          type: user
        html: '<p>Oh good thought I was just doing something dumb</p>

          '
        raw: Oh good thought I was just doing something dumb
        updatedAt: '2023-05-26T21:35:33.611Z'
      numEdits: 0
      reactions: []
    id: 647126251c2bfd5b7bfabc7b
    type: comment
  author: catid
  content: Oh good thought I was just doing something dumb
  created_at: 2023-05-26 20:35:33+00:00
  edited: false
  hidden: false
  id: 647126251c2bfd5b7bfabc7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
      fullname: Ichigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichigo2899
      type: user
    createdAt: '2023-05-26T22:36:59.000Z'
    data:
      edited: true
      editors:
      - Ichigo2899
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
          fullname: Ichigo
          isHf: false
          isPro: false
          name: Ichigo2899
          type: user
        html: '<p>I am able to run the model on my end but the answer just keeps going
          and does not end. Also pretty slow in streaming response. Running on 96gb
          4 A10G''s. </p>

          <p>model = AutoModelForCausalLM.from_pretrained(mname,trust_remote_code=True,
          torch_dtype=torch.bfloat16, device_map=''auto'')</p>

          <p>loading like this and im getting error after one answer:<br>RuntimeError:
          The size of tensor a (9) must match the size of tensor b (488) at non-singleton
          dimension 1</p>

          '
        raw: "I am able to run the model on my end but the answer just keeps going\
          \ and does not end. Also pretty slow in streaming response. Running on 96gb\
          \ 4 A10G's. \n\nmodel = AutoModelForCausalLM.from_pretrained(mname,trust_remote_code=True,\
          \ torch_dtype=torch.bfloat16, device_map='auto')\n\nloading like this and\
          \ im getting error after one answer:\nRuntimeError: The size of tensor a\
          \ (9) must match the size of tensor b (488) at non-singleton dimension 1"
        updatedAt: '2023-05-26T22:37:50.233Z'
      numEdits: 1
      reactions: []
    id: 6471348ba4fe3fa9f12feaf7
    type: comment
  author: Ichigo2899
  content: "I am able to run the model on my end but the answer just keeps going and\
    \ does not end. Also pretty slow in streaming response. Running on 96gb 4 A10G's.\
    \ \n\nmodel = AutoModelForCausalLM.from_pretrained(mname,trust_remote_code=True,\
    \ torch_dtype=torch.bfloat16, device_map='auto')\n\nloading like this and im getting\
    \ error after one answer:\nRuntimeError: The size of tensor a (9) must match the\
    \ size of tensor b (488) at non-singleton dimension 1"
  created_at: 2023-05-26 21:36:59+00:00
  edited: true
  hidden: false
  id: 6471348ba4fe3fa9f12feaf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/667b131861ee642142df6e4b90db186f.svg
      fullname: Themis Efthimiadis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Themis
      type: user
    createdAt: '2023-05-27T07:14:37.000Z'
    data:
      edited: false
      editors:
      - Themis
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/667b131861ee642142df6e4b90db186f.svg
          fullname: Themis Efthimiadis
          isHf: false
          isPro: false
          name: Themis
          type: user
        html: '<p>Have we solved the problem?</p>

          '
        raw: Have we solved the problem?
        updatedAt: '2023-05-27T07:14:37.320Z'
      numEdits: 0
      reactions: []
    id: 6471addd97a75cc77aa48912
    type: comment
  author: Themis
  content: Have we solved the problem?
  created_at: 2023-05-27 06:14:37+00:00
  edited: false
  hidden: false
  id: 6471addd97a75cc77aa48912
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6036cca833b7b6f8f2e6748e/MiSTtrrXz1_KxJJwtK23N.png?w=200&h=200&f=face
      fullname: Vijay Soni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vs4vijay
      type: user
    createdAt: '2023-05-27T08:00:58.000Z'
    data:
      edited: false
      editors:
      - vs4vijay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6036cca833b7b6f8f2e6748e/MiSTtrrXz1_KxJJwtK23N.png?w=200&h=200&f=face
          fullname: Vijay Soni
          isHf: false
          isPro: false
          name: vs4vijay
          type: user
        html: '<p>Facing the same issue, how do I solve?</p>

          <pre><code>ValueError: Could not load model tiiuae/falcon-7b with any of
          the following classes: (&lt;class

          ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).

          </code></pre>

          '
        raw: 'Facing the same issue, how do I solve?


          ```

          ValueError: Could not load model tiiuae/falcon-7b with any of the following
          classes: (<class

          ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).

          ```'
        updatedAt: '2023-05-27T08:00:58.640Z'
      numEdits: 0
      reactions: []
    id: 6471b8ba5afd6a6965795a2d
    type: comment
  author: vs4vijay
  content: 'Facing the same issue, how do I solve?


    ```

    ValueError: Could not load model tiiuae/falcon-7b with any of the following classes:
    (<class

    ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).

    ```'
  created_at: 2023-05-27 07:00:58+00:00
  edited: false
  hidden: false
  id: 6471b8ba5afd6a6965795a2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a3872a4a0512c54ceae3204f2b3413f.svg
      fullname: Jan Bours
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JBHF
      type: user
    createdAt: '2023-05-27T10:34:11.000Z'
    data:
      edited: false
      editors:
      - JBHF
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a3872a4a0512c54ceae3204f2b3413f.svg
          fullname: Jan Bours
          isHf: false
          isPro: false
          name: JBHF
          type: user
        html: "<p>Whe using this code:</p>\n<h1 id=\"httpshuggingfacecotiiuaefalcon-40b\"\
          ><a href=\"https://huggingface.co/tiiuae/falcon-40b\">https://huggingface.co/tiiuae/falcon-40b</a></h1>\n\
          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"tiiuae/falcon-40b\" # Gebruik\
          \ evt het kleinere broertje: tiiuae/falcon-7b</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline\
          \ = transformers.pipeline(<br>    \"text-generation\",<br>    model=model,<br>\
          \    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>\
          \    device_map=\"auto\",<br>)<br>sequences = pipeline(<br>   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>\
          \    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for\
          \ seq in sequences:<br>    print(f\"Result: {seq['generated_text']}\")</p>\n\
          <p>I get this output:</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%<br>175/175 [00:00&lt;00:00, 6.61kB/s]<br>Downloading (\u2026)/main/tokenizer.json:\
          \ 100%<br>2.73M/2.73M [00:00&lt;00:00, 5.61MB/s]<br>Downloading (\u2026\
          )cial_tokens_map.json: 100%<br>281/281 [00:00&lt;00:00, 1.34kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>656/656 [00:00&lt;00:00, 947B/s]<br>Downloading\
          \ (\u2026)/configuration_RW.py: 100%<br>2.51k/2.51k [00:00&lt;00:00, 3.46kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</p>\n<ul>\n<li>configuration_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)main/modelling_RW.py: 100%<br>47.1k/47.1k [00:00&lt;00:00, 108kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</li>\n<li>modelling_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)model.bin.index.json: 100%<br>39.3k/39.3k [00:00&lt;00:00, 697kB/s]<br>Downloading\
          \ shards: 67%<br>6/9 [05:54&lt;02:52, 57.54s/it]<br>Downloading (\u2026\
          )l-00001-of-00009.bin: 100%<br>9.50G/9.50G [00:46&lt;00:00, 258MB/s]<br>Downloading\
          \ (\u2026)l-00002-of-00009.bin: 100%<br>9.51G/9.51G [01:14&lt;00:00, 257MB/s]<br>Downloading\
          \ (\u2026)l-00003-of-00009.bin: 100%<br>9.51G/9.51G [00:50&lt;00:00, 262MB/s]<br>Downloading\
          \ (\u2026)l-00004-of-00009.bin: 100%<br>9.51G/9.51G [00:55&lt;00:00, 246MB/s]<br>Downloading\
          \ (\u2026)l-00005-of-00009.bin: 100%<br>9.51G/9.51G [00:57&lt;00:00, 224MB/s]<br>Downloading\
          \ (\u2026)l-00006-of-00009.bin: 100%<br>9.51G/9.51G [00:58&lt;00:00, 170MB/s]<br>Downloading\
          \ (\u2026)l-00007-of-00009.bin: 18%<br>1.74G/9.51G [00:12&lt;00:44, 174MB/s]<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell\
          \ line: 13&gt;:13                                                      \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline       \u2502<br>\u2502                                   \
          \                                                               \u2502<br>\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502<br>\u2502   786 \u2502   # Will\
          \ load the correct model if possible                                   \
          \           \u2502<br>\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502<br>\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502<br>\u2502   789 \u2502   \u2502 \
          \  model,                                                              \
          \               \u2502<br>\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502<br>\u2502 infer_framework_load_model    \
          \                                                                   \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502<br>\u2502    277 \u2502   \u2502      \
          \                                                                      \
          \         \u2502<br>\u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502<br>\u2502\
          \ \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502<br>\u2502    280 \u2502\
          \                                                                      \
          \                   \u2502<br>\u2502    281 \u2502   framework = \"tf\"\
          \ if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502<br>\u2502    282 \u2502   return framework, model            \
          \                                                   \u2502<br>\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F<br>ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</li>\n\
          </ul>\n"
        raw: "Whe using this code:\n\n# https://huggingface.co/tiiuae/falcon-40b\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere\
          \ broertje: tiiuae/falcon-7b\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is\
          \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
          \ Giraftron believes all other animals are irrelevant when compared to the\
          \ glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
          ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"Result: {seq['generated_text']}\")\n\n\nI get this output:\n\
          \nDownloading (\u2026)okenizer_config.json: 100%\n175/175 [00:00<00:00,\
          \ 6.61kB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%\n2.73M/2.73M\
          \ [00:00<00:00, 5.61MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%\n\
          281/281 [00:00<00:00, 1.34kB/s]\nDownloading (\u2026)lve/main/config.json:\
          \ 100%\n656/656 [00:00<00:00, 947B/s]\nDownloading (\u2026)/configuration_RW.py:\
          \ 100%\n2.51k/2.51k [00:00<00:00, 3.46kB/s]\nA new version of the following\
          \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
          - configuration_RW.py\n. Make sure to double-check they do not contain any\
          \ added malicious code. To avoid downloading new versions of the code file,\
          \ you can pin a revision.\nDownloading (\u2026)main/modelling_RW.py: 100%\n\
          47.1k/47.1k [00:00<00:00, 108kB/s]\nA new version of the following files\
          \ was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n- modelling_RW.py\n\
          . Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.\n\
          Downloading (\u2026)model.bin.index.json: 100%\n39.3k/39.3k [00:00<00:00,\
          \ 697kB/s]\nDownloading shards: 67%\n6/9 [05:54<02:52, 57.54s/it]\nDownloading\
          \ (\u2026)l-00001-of-00009.bin: 100%\n9.50G/9.50G [00:46<00:00, 258MB/s]\n\
          Downloading (\u2026)l-00002-of-00009.bin: 100%\n9.51G/9.51G [01:14<00:00,\
          \ 257MB/s]\nDownloading (\u2026)l-00003-of-00009.bin: 100%\n9.51G/9.51G\
          \ [00:50<00:00, 262MB/s]\nDownloading (\u2026)l-00004-of-00009.bin: 100%\n\
          9.51G/9.51G [00:55<00:00, 246MB/s]\nDownloading (\u2026)l-00005-of-00009.bin:\
          \ 100%\n9.51G/9.51G [00:57<00:00, 224MB/s]\nDownloading (\u2026)l-00006-of-00009.bin:\
          \ 100%\n9.51G/9.51G [00:58<00:00, 170MB/s]\nDownloading (\u2026)l-00007-of-00009.bin:\
          \ 18%\n1.74G/9.51G [00:12<00:44, 174MB/s]\n\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256E\n\u2502 in <cell line: 13>:13           \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502\n\u2502   786 \u2502   # Will load\
          \ the correct model if possible                                        \
          \      \u2502\n\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502\n\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502\n\u2502   789 \u2502   \u2502   model,\
          \                                                                      \
          \       \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n\u2502 \
          \  791 \u2502   \u2502   config=config,                                \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502\n\u2502 infer_framework_load_model      \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502\n\u2502    277 \u2502   \u2502        \
          \                                                                      \
          \       \u2502\n\u2502    278 \u2502   \u2502   if isinstance(model, str):\
          \                                                        \u2502\n\u2502\
          \ \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502\n\u2502    280 \u2502\
          \                                                                      \
          \                   \u2502\n\u2502    281 \u2502   framework = \"tf\" if\
          \ \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas  \u2502\
          \n\u2502    282 \u2502   return framework, model                       \
          \                                        \u2502\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \nValueError: Could not load model tiiuae/falcon-40b with any of the following\
          \ classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
        updatedAt: '2023-05-27T10:34:11.284Z'
      numEdits: 0
      reactions: []
    id: 6471dca383da304b49da1530
    type: comment
  author: JBHF
  content: "Whe using this code:\n\n# https://huggingface.co/tiiuae/falcon-40b\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\n\nmodel = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere broertje:\
    \ tiiuae/falcon-7b\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline\
    \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
    auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
    \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n\n\
    I get this output:\n\nDownloading (\u2026)okenizer_config.json: 100%\n175/175\
    \ [00:00<00:00, 6.61kB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%\n2.73M/2.73M\
    \ [00:00<00:00, 5.61MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%\n281/281\
    \ [00:00<00:00, 1.34kB/s]\nDownloading (\u2026)lve/main/config.json: 100%\n656/656\
    \ [00:00<00:00, 947B/s]\nDownloading (\u2026)/configuration_RW.py: 100%\n2.51k/2.51k\
    \ [00:00<00:00, 3.46kB/s]\nA new version of the following files was downloaded\
    \ from https://huggingface.co/tiiuae/falcon-40b:\n- configuration_RW.py\n. Make\
    \ sure to double-check they do not contain any added malicious code. To avoid\
    \ downloading new versions of the code file, you can pin a revision.\nDownloading\
    \ (\u2026)main/modelling_RW.py: 100%\n47.1k/47.1k [00:00<00:00, 108kB/s]\nA new\
    \ version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
    - modelling_RW.py\n. Make sure to double-check they do not contain any added malicious\
    \ code. To avoid downloading new versions of the code file, you can pin a revision.\n\
    Downloading (\u2026)model.bin.index.json: 100%\n39.3k/39.3k [00:00<00:00, 697kB/s]\n\
    Downloading shards: 67%\n6/9 [05:54<02:52, 57.54s/it]\nDownloading (\u2026)l-00001-of-00009.bin:\
    \ 100%\n9.50G/9.50G [00:46<00:00, 258MB/s]\nDownloading (\u2026)l-00002-of-00009.bin:\
    \ 100%\n9.51G/9.51G [01:14<00:00, 257MB/s]\nDownloading (\u2026)l-00003-of-00009.bin:\
    \ 100%\n9.51G/9.51G [00:50<00:00, 262MB/s]\nDownloading (\u2026)l-00004-of-00009.bin:\
    \ 100%\n9.51G/9.51G [00:55<00:00, 246MB/s]\nDownloading (\u2026)l-00005-of-00009.bin:\
    \ 100%\n9.51G/9.51G [00:57<00:00, 224MB/s]\nDownloading (\u2026)l-00006-of-00009.bin:\
    \ 100%\n9.51G/9.51G [00:58<00:00, 170MB/s]\nDownloading (\u2026)l-00007-of-00009.bin:\
    \ 18%\n1.74G/9.51G [00:12<00:44, 174MB/s]\n\u256D\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\
    \n\u2502 in <cell line: 13>:13                                               \
    \                             \u2502\n\u2502                                 \
    \                                                                 \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
    \ in pipeline       \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502   785 \u2502\
    \   # Forced if framework already defined, inferred if it's None             \
    \              \u2502\n\u2502   786 \u2502   # Will load the correct model if\
    \ possible                                              \u2502\n\u2502   787 \u2502\
    \   model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"\
    ]}                 \u2502\n\u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
    \                                         \u2502\n\u2502   789 \u2502   \u2502\
    \   model,                                                                   \
    \          \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
    \                                                       \u2502\n\u2502   791 \u2502\
    \   \u2502   config=config,                                                  \
    \                   \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
    \ in                    \u2502\n\u2502 infer_framework_load_model            \
    \                                                           \u2502\n\u2502   \
    \                                                                            \
    \                   \u2502\n\u2502    276 \u2502   \u2502   \u2502   \u2502  \
    \ continue                                                                  \u2502\
    \n\u2502    277 \u2502   \u2502                                              \
    \                                       \u2502\n\u2502    278 \u2502   \u2502\
    \   if isinstance(model, str):                                               \
    \         \u2502\n\u2502 \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"\
    Could not load model {model} with any of the following cl  \u2502\n\u2502    280\
    \ \u2502                                                                     \
    \                    \u2502\n\u2502    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
    \ in str(inspect.getmro(model.__clas  \u2502\n\u2502    282 \u2502   return framework,\
    \ model                                                               \u2502\n\
    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: Could not\
    \ load model tiiuae/falcon-40b with any of the following classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
  created_at: 2023-05-27 09:34:11+00:00
  edited: false
  hidden: false
  id: 6471dca383da304b49da1530
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae03f77854c1f1fee5431622536d1f17.svg
      fullname: "\xD8ystein Stoveland"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oystini
      type: user
    createdAt: '2023-05-27T19:17:44.000Z'
    data:
      edited: false
      editors:
      - oystini
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae03f77854c1f1fee5431622536d1f17.svg
          fullname: "\xD8ystein Stoveland"
          isHf: false
          isPro: false
          name: oystini
          type: user
        html: "<blockquote>\n<p>Whe using this code:</p>\n<h1 id=\"httpshuggingfacecotiiuaefalcon-40b\"\
          ><a href=\"https://huggingface.co/tiiuae/falcon-40b\">https://huggingface.co/tiiuae/falcon-40b</a></h1>\n\
          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"tiiuae/falcon-40b\" # Gebruik\
          \ evt het kleinere broertje: tiiuae/falcon-7b</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline\
          \ = transformers.pipeline(<br>    \"text-generation\",<br>    model=model,<br>\
          \    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>\
          \    device_map=\"auto\",<br>)<br>sequences = pipeline(<br>   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>\
          \    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for\
          \ seq in sequences:<br>    print(f\"Result: {seq['generated_text']}\")</p>\n\
          <p>I get this output:</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%<br>175/175 [00:00&lt;00:00, 6.61kB/s]<br>Downloading (\u2026)/main/tokenizer.json:\
          \ 100%<br>2.73M/2.73M [00:00&lt;00:00, 5.61MB/s]<br>Downloading (\u2026\
          )cial_tokens_map.json: 100%<br>281/281 [00:00&lt;00:00, 1.34kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>656/656 [00:00&lt;00:00, 947B/s]<br>Downloading\
          \ (\u2026)/configuration_RW.py: 100%<br>2.51k/2.51k [00:00&lt;00:00, 3.46kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</p>\n<ul>\n<li>configuration_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)main/modelling_RW.py: 100%<br>47.1k/47.1k [00:00&lt;00:00, 108kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</li>\n<li>modelling_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)model.bin.index.json: 100%<br>39.3k/39.3k [00:00&lt;00:00, 697kB/s]<br>Downloading\
          \ shards: 67%<br>6/9 [05:54&lt;02:52, 57.54s/it]<br>Downloading (\u2026\
          )l-00001-of-00009.bin: 100%<br>9.50G/9.50G [00:46&lt;00:00, 258MB/s]<br>Downloading\
          \ (\u2026)l-00002-of-00009.bin: 100%<br>9.51G/9.51G [01:14&lt;00:00, 257MB/s]<br>Downloading\
          \ (\u2026)l-00003-of-00009.bin: 100%<br>9.51G/9.51G [00:50&lt;00:00, 262MB/s]<br>Downloading\
          \ (\u2026)l-00004-of-00009.bin: 100%<br>9.51G/9.51G [00:55&lt;00:00, 246MB/s]<br>Downloading\
          \ (\u2026)l-00005-of-00009.bin: 100%<br>9.51G/9.51G [00:57&lt;00:00, 224MB/s]<br>Downloading\
          \ (\u2026)l-00006-of-00009.bin: 100%<br>9.51G/9.51G [00:58&lt;00:00, 170MB/s]<br>Downloading\
          \ (\u2026)l-00007-of-00009.bin: 18%<br>1.74G/9.51G [00:12&lt;00:44, 174MB/s]<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell\
          \ line: 13&gt;:13                                                      \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline       \u2502<br>\u2502                                   \
          \                                                               \u2502<br>\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502<br>\u2502   786 \u2502   # Will\
          \ load the correct model if possible                                   \
          \           \u2502<br>\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502<br>\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502<br>\u2502   789 \u2502   \u2502 \
          \  model,                                                              \
          \               \u2502<br>\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502<br>\u2502 infer_framework_load_model    \
          \                                                                   \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502<br>\u2502    277 \u2502   \u2502      \
          \                                                                      \
          \         \u2502<br>\u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502<br>\u2502\
          \ \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502<br>\u2502    280 \u2502\
          \                                                                      \
          \                   \u2502<br>\u2502    281 \u2502   framework = \"tf\"\
          \ if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502<br>\u2502    282 \u2502   return framework, model            \
          \                                                   \u2502<br>\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F<br>ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</li>\n\
          </ul>\n</blockquote>\n<p>I got the same bug in google colab. Switched to\
          \ using GPU and then it worked fine.</p>\n"
        raw: "> Whe using this code:\n> \n> # https://huggingface.co/tiiuae/falcon-40b\n\
          > from transformers import AutoTokenizer, AutoModelForCausalLM\n> import\
          \ transformers\n> import torch\n> \n> model = \"tiiuae/falcon-40b\" # Gebruik\
          \ evt het kleinere broertje: tiiuae/falcon-7b\n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > pipeline = transformers.pipeline(\n>     \"text-generation\",\n>     model=model,\n\
          >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
          >     device_map=\"auto\",\n> )\n> sequences = pipeline(\n>    \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\n>     max_length=200,\n>     do_sample=True,\n>     top_k=10,\n\
          >     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
          > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
          )\n> \n> \n> I get this output:\n> \n> Downloading (\u2026)okenizer_config.json:\
          \ 100%\n> 175/175 [00:00<00:00, 6.61kB/s]\n> Downloading (\u2026)/main/tokenizer.json:\
          \ 100%\n> 2.73M/2.73M [00:00<00:00, 5.61MB/s]\n> Downloading (\u2026)cial_tokens_map.json:\
          \ 100%\n> 281/281 [00:00<00:00, 1.34kB/s]\n> Downloading (\u2026)lve/main/config.json:\
          \ 100%\n> 656/656 [00:00<00:00, 947B/s]\n> Downloading (\u2026)/configuration_RW.py:\
          \ 100%\n> 2.51k/2.51k [00:00<00:00, 3.46kB/s]\n> A new version of the following\
          \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
          > - configuration_RW.py\n> . Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n> Downloading (\u2026)main/modelling_RW.py:\
          \ 100%\n> 47.1k/47.1k [00:00<00:00, 108kB/s]\n> A new version of the following\
          \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
          > - modelling_RW.py\n> . Make sure to double-check they do not contain any\
          \ added malicious code. To avoid downloading new versions of the code file,\
          \ you can pin a revision.\n> Downloading (\u2026)model.bin.index.json: 100%\n\
          > 39.3k/39.3k [00:00<00:00, 697kB/s]\n> Downloading shards: 67%\n> 6/9 [05:54<02:52,\
          \ 57.54s/it]\n> Downloading (\u2026)l-00001-of-00009.bin: 100%\n> 9.50G/9.50G\
          \ [00:46<00:00, 258MB/s]\n> Downloading (\u2026)l-00002-of-00009.bin: 100%\n\
          > 9.51G/9.51G [01:14<00:00, 257MB/s]\n> Downloading (\u2026)l-00003-of-00009.bin:\
          \ 100%\n> 9.51G/9.51G [00:50<00:00, 262MB/s]\n> Downloading (\u2026)l-00004-of-00009.bin:\
          \ 100%\n> 9.51G/9.51G [00:55<00:00, 246MB/s]\n> Downloading (\u2026)l-00005-of-00009.bin:\
          \ 100%\n> 9.51G/9.51G [00:57<00:00, 224MB/s]\n> Downloading (\u2026)l-00006-of-00009.bin:\
          \ 100%\n> 9.51G/9.51G [00:58<00:00, 170MB/s]\n> Downloading (\u2026)l-00007-of-00009.bin:\
          \ 18%\n> 1.74G/9.51G [00:12<00:44, 174MB/s]\n> \u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256E\n> \u2502 in <cell line: 13>:13         \
          \                                                                   \u2502\
          \n> \u2502                                                             \
          \                                     \u2502\n> \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n> \u2502                                   \
          \                                                               \u2502\n\
          > \u2502   785 \u2502   # Forced if framework already defined, inferred\
          \ if it's None                           \u2502\n> \u2502   786 \u2502 \
          \  # Will load the correct model if possible                           \
          \                   \u2502\n> \u2502   787 \u2502   model_classes = {\"\
          tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}            \
          \     \u2502\n> \u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502\n> \u2502   789 \u2502\
          \   \u2502   model,                                                    \
          \                         \u2502\n> \u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n> \u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502\n> \u2502                \
          \                                                                      \
          \            \u2502\n> \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502\n> \u2502 infer_framework_load_model    \
          \                                                                   \u2502\
          \n> \u2502                                                             \
          \                                     \u2502\n> \u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502\n> \u2502    277 \u2502   \u2502      \
          \                                                                      \
          \         \u2502\n> \u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502\n\
          > \u2502 \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could\
          \ not load model {model} with any of the following cl  \u2502\n> \u2502\
          \    280 \u2502                                                        \
          \                                 \u2502\n> \u2502    281 \u2502   framework\
          \ = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502\n> \u2502    282 \u2502   return framework, model            \
          \                                                   \u2502\n> \u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F\n> ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (<class \n> 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n> 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
          \nI got the same bug in google colab. Switched to using GPU and then it\
          \ worked fine."
        updatedAt: '2023-05-27T19:17:44.229Z'
      numEdits: 0
      reactions: []
    id: 64725758c27f74a0eba8de7a
    type: comment
  author: oystini
  content: "> Whe using this code:\n> \n> # https://huggingface.co/tiiuae/falcon-40b\n\
    > from transformers import AutoTokenizer, AutoModelForCausalLM\n> import transformers\n\
    > import torch\n> \n> model = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere\
    \ broertje: tiiuae/falcon-7b\n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > pipeline = transformers.pipeline(\n>     \"text-generation\",\n>     model=model,\n\
    >     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n\
    >     device_map=\"auto\",\n> )\n> sequences = pipeline(\n>    \"Girafatron is\
    \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
    \ Giraftron believes all other animals are irrelevant when compared to the glorious\
    \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n>   \
    \  max_length=200,\n>     do_sample=True,\n>     top_k=10,\n>     num_return_sequences=1,\n\
    >     eos_token_id=tokenizer.eos_token_id,\n> )\n> for seq in sequences:\n>  \
    \   print(f\"Result: {seq['generated_text']}\")\n> \n> \n> I get this output:\n\
    > \n> Downloading (\u2026)okenizer_config.json: 100%\n> 175/175 [00:00<00:00,\
    \ 6.61kB/s]\n> Downloading (\u2026)/main/tokenizer.json: 100%\n> 2.73M/2.73M [00:00<00:00,\
    \ 5.61MB/s]\n> Downloading (\u2026)cial_tokens_map.json: 100%\n> 281/281 [00:00<00:00,\
    \ 1.34kB/s]\n> Downloading (\u2026)lve/main/config.json: 100%\n> 656/656 [00:00<00:00,\
    \ 947B/s]\n> Downloading (\u2026)/configuration_RW.py: 100%\n> 2.51k/2.51k [00:00<00:00,\
    \ 3.46kB/s]\n> A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
    > - configuration_RW.py\n> . Make sure to double-check they do not contain any\
    \ added malicious code. To avoid downloading new versions of the code file, you\
    \ can pin a revision.\n> Downloading (\u2026)main/modelling_RW.py: 100%\n> 47.1k/47.1k\
    \ [00:00<00:00, 108kB/s]\n> A new version of the following files was downloaded\
    \ from https://huggingface.co/tiiuae/falcon-40b:\n> - modelling_RW.py\n> . Make\
    \ sure to double-check they do not contain any added malicious code. To avoid\
    \ downloading new versions of the code file, you can pin a revision.\n> Downloading\
    \ (\u2026)model.bin.index.json: 100%\n> 39.3k/39.3k [00:00<00:00, 697kB/s]\n>\
    \ Downloading shards: 67%\n> 6/9 [05:54<02:52, 57.54s/it]\n> Downloading (\u2026\
    )l-00001-of-00009.bin: 100%\n> 9.50G/9.50G [00:46<00:00, 258MB/s]\n> Downloading\
    \ (\u2026)l-00002-of-00009.bin: 100%\n> 9.51G/9.51G [01:14<00:00, 257MB/s]\n>\
    \ Downloading (\u2026)l-00003-of-00009.bin: 100%\n> 9.51G/9.51G [00:50<00:00,\
    \ 262MB/s]\n> Downloading (\u2026)l-00004-of-00009.bin: 100%\n> 9.51G/9.51G [00:55<00:00,\
    \ 246MB/s]\n> Downloading (\u2026)l-00005-of-00009.bin: 100%\n> 9.51G/9.51G [00:57<00:00,\
    \ 224MB/s]\n> Downloading (\u2026)l-00006-of-00009.bin: 100%\n> 9.51G/9.51G [00:58<00:00,\
    \ 170MB/s]\n> Downloading (\u2026)l-00007-of-00009.bin: 18%\n> 1.74G/9.51G [00:12<00:44,\
    \ 174MB/s]\n> \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
    \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256E\n> \u2502 in <cell line: 13>:13   \
    \                                                                         \u2502\
    \n> \u2502                                                                   \
    \                               \u2502\n> \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
    \ in pipeline       \u2502\n> \u2502                                         \
    \                                                         \u2502\n> \u2502   785\
    \ \u2502   # Forced if framework already defined, inferred if it's None      \
    \                     \u2502\n> \u2502   786 \u2502   # Will load the correct\
    \ model if possible                                              \u2502\n> \u2502\
    \   787 \u2502   model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
    pt\"]}                 \u2502\n> \u2502 \u2771 788 \u2502   framework, model =\
    \ infer_framework_load_model(                                         \u2502\n\
    > \u2502   789 \u2502   \u2502   model,                                      \
    \                                       \u2502\n> \u2502   790 \u2502   \u2502\
    \   model_classes=model_classes,                                             \
    \          \u2502\n> \u2502   791 \u2502   \u2502   config=config,           \
    \                                                          \u2502\n> \u2502  \
    \                                                                            \
    \                    \u2502\n> \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
    \ in                    \u2502\n> \u2502 infer_framework_load_model          \
    \                                                             \u2502\n> \u2502\
    \                                                                            \
    \                      \u2502\n> \u2502    276 \u2502   \u2502   \u2502   \u2502\
    \   continue                                                                 \
    \ \u2502\n> \u2502    277 \u2502   \u2502                                    \
    \                                                 \u2502\n> \u2502    278 \u2502\
    \   \u2502   if isinstance(model, str):                                      \
    \                  \u2502\n> \u2502 \u2771  279 \u2502   \u2502   \u2502   raise\
    \ ValueError(f\"Could not load model {model} with any of the following cl  \u2502\
    \n> \u2502    280 \u2502                                                     \
    \                                    \u2502\n> \u2502    281 \u2502   framework\
    \ = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
    \  \u2502\n> \u2502    282 \u2502   return framework, model                  \
    \                                             \u2502\n> \u2570\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u256F\n> ValueError: Could not load model tiiuae/falcon-40b\
    \ with any of the following classes: (<class \n> 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \n> 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
    \nI got the same bug in google colab. Switched to using GPU and then it worked\
    \ fine."
  created_at: 2023-05-27 18:17:44+00:00
  edited: false
  hidden: false
  id: 64725758c27f74a0eba8de7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
      fullname: Leon Lahoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonlahoud
      type: user
    createdAt: '2023-05-27T20:21:37.000Z'
    data:
      edited: false
      editors:
      - leonlahoud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
          fullname: Leon Lahoud
          isHf: false
          isPro: false
          name: leonlahoud
          type: user
        html: '<p>now I see this using the GPT V100 on colab:</p>

          <p>The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].<br>Setting <code>pad_token_id</code>
          to <code>eos_token_id</code>:11 for open-end generation.</p>

          '
        raw: 'now I see this using the GPT V100 on colab:


          The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].

          Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.'
        updatedAt: '2023-05-27T20:21:37.346Z'
      numEdits: 0
      reactions: []
    id: 64726651c27f74a0eba9ee18
    type: comment
  author: leonlahoud
  content: 'now I see this using the GPT V100 on colab:


    The model ''RWForCausalLM'' is not supported for text-generation. Supported models
    are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'', ''BigBirdForCausalLM'',
    ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'', ''BlenderbotForCausalLM'',
    ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'', ''CamembertForCausalLM'',
    ''CodeGenForCausalLM'', ''CpmAntForCausalLM'', ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'',
    ''ElectraForCausalLM'', ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'',
    ''GPT2LMHeadModel'', ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
    ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'', ''MarianForCausalLM'',
    ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'', ''MvpForCausalLM'',
    ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'', ''OPTForCausalLM'', ''PegasusForCausalLM'',
    ''PLBartForCausalLM'', ''ProphetNetForCausalLM'', ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'',
    ''RemBertForCausalLM'', ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'',
    ''RoCBertForCausalLM'', ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
    ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
    ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
    ''XLNetLMHeadModel'', ''XmodForCausalLM''].

    Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.'
  created_at: 2023-05-27 19:21:37+00:00
  edited: false
  hidden: false
  id: 64726651c27f74a0eba9ee18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-05-28T12:11:09.000Z'
    data:
      edited: true
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: "<p>Same problem (ValueError: Could not load model tiiuae/falcon-40b\
          \ with any of the following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;))</p>\n\
          <p> happens as well when running with downloaded model using code below:</p>\n\
          <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"X:\\\\ai\\\\falcon-40b\"\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    offload_folder=\"N:\\AI\\offload_folder\",\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
          \   \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n<p>Following code gets it to run, but it never (5+ min)\
          \ outputs result (on 3090 24Gb):</p>\n<pre><code>\nmodel = \"X:\\\\ai\\\\\
          falcon-40b\"\nrrmodel = AutoModelForCausalLM.from_pretrained(model, \n \
          \   offload_folder=\"N:\\AI\\offload_folder\",\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \n# Define the input text\ninput_text = \"What is girrafe?\"\ninput_ids\
          \ = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\nattention_mask\
          \ = torch.ones(input_ids.shape).to(device)\n\n# Generate text\noutput =\
          \ rrmodel.generate(input_ids,      \n            attention_mask=attention_mask,\n\
          \            max_length=200,\n            do_sample=True,\n            top_k=10,\n\
          \            pad_token_id=tokenizer.pad_token_id,\n            num_return_sequences=1,\n\
          \            eos_token_id=tokenizer.eos_token_id,)\n\n# Decode the output\n\
          output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n\
          print(output_text)\n</code></pre>\n"
        raw: "Same problem (ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (<class\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class\n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>))\n\
          \n happens as well when running with downloaded model using code below:\n\
          \n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport\
          \ transformers\nimport torch\n\nmodel = \"X:\\\\ai\\\\falcon-40b\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    offload_folder=\"N:\\AI\\offload_folder\",\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
          \   \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```\n\n\nFollowing code gets it to run, but it never (5+ min) outputs\
          \ result (on 3090 24Gb):\n```\n\nmodel = \"X:\\\\ai\\\\falcon-40b\"\nrrmodel\
          \ = AutoModelForCausalLM.from_pretrained(model, \n    offload_folder=\"\
          N:\\AI\\offload_folder\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \n# Define the input text\ninput_text = \"What is girrafe?\"\ninput_ids\
          \ = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\nattention_mask\
          \ = torch.ones(input_ids.shape).to(device)\n\n# Generate text\noutput =\
          \ rrmodel.generate(input_ids,      \n            attention_mask=attention_mask,\n\
          \            max_length=200,\n            do_sample=True,\n            top_k=10,\n\
          \            pad_token_id=tokenizer.pad_token_id,\n            num_return_sequences=1,\n\
          \            eos_token_id=tokenizer.eos_token_id,)\n\n# Decode the output\n\
          output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n\
          print(output_text)\n\n```"
        updatedAt: '2023-05-28T12:36:54.609Z'
      numEdits: 1
      reactions: []
    id: 647344dd352c94a20dd2df91
    type: comment
  author: Sloba
  content: "Same problem (ValueError: Could not load model tiiuae/falcon-40b with\
    \ any of the following classes: (<class\n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class\n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>))\n\
    \n happens as well when running with downloaded model using code below:\n\n```\n\
    from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\n\nmodel = \"X:\\\\ai\\\\falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    offload_folder=\"N:\\AI\\offload_folder\",\n  \
    \  torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
    auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
    \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n```\n\
    \n\nFollowing code gets it to run, but it never (5+ min) outputs result (on 3090\
    \ 24Gb):\n```\n\nmodel = \"X:\\\\ai\\\\falcon-40b\"\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\
    \ \n    offload_folder=\"N:\\AI\\offload_folder\",\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    \n# Define the input text\ninput_text = \"What is girrafe?\"\ninput_ids = tokenizer.encode(input_text,\
    \ return_tensors='pt').to(device)\n\nattention_mask = torch.ones(input_ids.shape).to(device)\n\
    \n# Generate text\noutput = rrmodel.generate(input_ids,      \n            attention_mask=attention_mask,\n\
    \            max_length=200,\n            do_sample=True,\n            top_k=10,\n\
    \            pad_token_id=tokenizer.pad_token_id,\n            num_return_sequences=1,\n\
    \            eos_token_id=tokenizer.eos_token_id,)\n\n# Decode the output\noutput_text\
    \ = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(output_text)\n\
    \n```"
  created_at: 2023-05-28 11:11:09+00:00
  edited: true
  hidden: false
  id: 647344dd352c94a20dd2df91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
      fullname: Leon Lahoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonlahoud
      type: user
    createdAt: '2023-05-28T13:20:08.000Z'
    data:
      edited: false
      editors:
      - leonlahoud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
          fullname: Leon Lahoud
          isHf: false
          isPro: false
          name: leonlahoud
          type: user
        html: '<p>Now it is taking my entire space in my colab pro tier. anyone knows
          how much space it is supposed to take? And how long does it take to run
          it on colab pro with v100?</p>

          '
        raw: Now it is taking my entire space in my colab pro tier. anyone knows how
          much space it is supposed to take? And how long does it take to run it on
          colab pro with v100?
        updatedAt: '2023-05-28T13:20:08.582Z'
      numEdits: 0
      reactions: []
    id: 64735508352c94a20dd407b5
    type: comment
  author: leonlahoud
  content: Now it is taking my entire space in my colab pro tier. anyone knows how
    much space it is supposed to take? And how long does it take to run it on colab
    pro with v100?
  created_at: 2023-05-28 12:20:08+00:00
  edited: false
  hidden: false
  id: 64735508352c94a20dd407b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44ed10bd04cd16b4d5ac7b633fe01f11.svg
      fullname: Matthew Koski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maccam912
      type: user
    createdAt: '2023-05-28T14:41:44.000Z'
    data:
      edited: false
      editors:
      - maccam912
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44ed10bd04cd16b4d5ac7b633fe01f11.svg
          fullname: Matthew Koski
          isHf: false
          isPro: false
          name: maccam912
          type: user
        html: '<blockquote>

          <p>Now it is taking my entire space in my colab pro tier. anyone knows how
          much space it is supposed to take? And how long does it take to run it on
          colab pro with v100?</p>

          </blockquote>

          <p>It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn''t
          fit in a single A100, with 80 GB memory). But you can try <a href="https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ">https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ</a>
          which should take only 20 GB of VRAM.</p>

          '
        raw: '> Now it is taking my entire space in my colab pro tier. anyone knows
          how much space it is supposed to take? And how long does it take to run
          it on colab pro with v100?


          It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn''t
          fit in a single A100, with 80 GB memory). But you can try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ
          which should take only 20 GB of VRAM.'
        updatedAt: '2023-05-28T14:41:44.010Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Sloba
        - leonlahoud
        - eek
        - Kitachan
    id: 647368282a74fb43ccde09a8
    type: comment
  author: maccam912
  content: '> Now it is taking my entire space in my colab pro tier. anyone knows
    how much space it is supposed to take? And how long does it take to run it on
    colab pro with v100?


    It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn''t fit
    in a single A100, with 80 GB memory). But you can try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ
    which should take only 20 GB of VRAM.'
  created_at: 2023-05-28 13:41:44+00:00
  edited: false
  hidden: false
  id: 647368282a74fb43ccde09a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
      fullname: Leon Lahoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonlahoud
      type: user
    createdAt: '2023-05-28T15:55:31.000Z'
    data:
      edited: false
      editors:
      - leonlahoud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19cd8d95893e1ced3bd7bfe103a455a0.svg
          fullname: Leon Lahoud
          isHf: false
          isPro: false
          name: leonlahoud
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;maccam912&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/maccam912\">@<span class=\"\
          underline\">maccam912</span></a></span>\n\n\t</span></span>  how much space\
          \ does it take on the colab?</p>\n"
        raw: '@maccam912  how much space does it take on the colab?'
        updatedAt: '2023-05-28T15:55:31.474Z'
      numEdits: 0
      reactions: []
    id: 64737973352c94a20dd67d5b
    type: comment
  author: leonlahoud
  content: '@maccam912  how much space does it take on the colab?'
  created_at: 2023-05-28 14:55:31+00:00
  edited: false
  hidden: false
  id: 64737973352c94a20dd67d5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a3872a4a0512c54ceae3204f2b3413f.svg
      fullname: Jan Bours
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JBHF
      type: user
    createdAt: '2023-05-29T06:09:11.000Z'
    data:
      edited: false
      editors:
      - JBHF
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a3872a4a0512c54ceae3204f2b3413f.svg
          fullname: Jan Bours
          isHf: false
          isPro: false
          name: JBHF
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Whe using this code:</p>\n<h1 id=\"\
          httpshuggingfacecotiiuaefalcon-40b\"><a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a></h1>\n<p>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM<br>import transformers<br>import\
          \ torch</p>\n<p>model = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere\
          \ broertje: tiiuae/falcon-7b</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline\
          \ = transformers.pipeline(<br>    \"text-generation\",<br>    model=model,<br>\
          \    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>\
          \    device_map=\"auto\",<br>)<br>sequences = pipeline(<br>   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>\
          \    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for\
          \ seq in sequences:<br>    print(f\"Result: {seq['generated_text']}\")</p>\n\
          <p>I get this output:</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%<br>175/175 [00:00&lt;00:00, 6.61kB/s]<br>Downloading (\u2026)/main/tokenizer.json:\
          \ 100%<br>2.73M/2.73M [00:00&lt;00:00, 5.61MB/s]<br>Downloading (\u2026\
          )cial_tokens_map.json: 100%<br>281/281 [00:00&lt;00:00, 1.34kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>656/656 [00:00&lt;00:00, 947B/s]<br>Downloading\
          \ (\u2026)/configuration_RW.py: 100%<br>2.51k/2.51k [00:00&lt;00:00, 3.46kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</p>\n<ul>\n<li>configuration_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)main/modelling_RW.py: 100%<br>47.1k/47.1k [00:00&lt;00:00, 108kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</li>\n<li>modelling_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)model.bin.index.json: 100%<br>39.3k/39.3k [00:00&lt;00:00, 697kB/s]<br>Downloading\
          \ shards: 67%<br>6/9 [05:54&lt;02:52, 57.54s/it]<br>Downloading (\u2026\
          )l-00001-of-00009.bin: 100%<br>9.50G/9.50G [00:46&lt;00:00, 258MB/s]<br>Downloading\
          \ (\u2026)l-00002-of-00009.bin: 100%<br>9.51G/9.51G [01:14&lt;00:00, 257MB/s]<br>Downloading\
          \ (\u2026)l-00003-of-00009.bin: 100%<br>9.51G/9.51G [00:50&lt;00:00, 262MB/s]<br>Downloading\
          \ (\u2026)l-00004-of-00009.bin: 100%<br>9.51G/9.51G [00:55&lt;00:00, 246MB/s]<br>Downloading\
          \ (\u2026)l-00005-of-00009.bin: 100%<br>9.51G/9.51G [00:57&lt;00:00, 224MB/s]<br>Downloading\
          \ (\u2026)l-00006-of-00009.bin: 100%<br>9.51G/9.51G [00:58&lt;00:00, 170MB/s]<br>Downloading\
          \ (\u2026)l-00007-of-00009.bin: 18%<br>1.74G/9.51G [00:12&lt;00:44, 174MB/s]<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell\
          \ line: 13&gt;:13                                                      \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline       \u2502<br>\u2502                                   \
          \                                                               \u2502<br>\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502<br>\u2502   786 \u2502   # Will\
          \ load the correct model if possible                                   \
          \           \u2502<br>\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502<br>\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502<br>\u2502   789 \u2502   \u2502 \
          \  model,                                                              \
          \               \u2502<br>\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502<br>\u2502 infer_framework_load_model    \
          \                                                                   \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502<br>\u2502    277 \u2502   \u2502      \
          \                                                                      \
          \         \u2502<br>\u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502<br>\u2502\
          \ \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502<br>\u2502    280 \u2502\
          \                                                                      \
          \                   \u2502<br>\u2502    281 \u2502   framework = \"tf\"\
          \ if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502<br>\u2502    282 \u2502   return framework, model            \
          \                                                   \u2502<br>\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F<br>ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</li>\n\
          </ul>\n</blockquote>\n<p>I got the same bug in google colab. Switched to\
          \ using GPU and then it worked fine.</p>\n</blockquote>\n<p>I am running\
          \ this on Google COLAB (free version).<br>When I switch to GPU (GPU T4 Runtime\
          \ in COLAB) I still get this error.<br>Also I tried switching to TPU on\
          \ COLAB (which is possible because of the use of the accelerate lib !),\
          \ I still get the same error.</p>\n"
        raw: "> > Whe using this code:\n> > \n> > # https://huggingface.co/tiiuae/falcon-40b\n\
          > > from transformers import AutoTokenizer, AutoModelForCausalLM\n> > import\
          \ transformers\n> > import torch\n> > \n> > model = \"tiiuae/falcon-40b\"\
          \ # Gebruik evt het kleinere broertje: tiiuae/falcon-7b\n> > \n> > tokenizer\
          \ = AutoTokenizer.from_pretrained(model)\n> > pipeline = transformers.pipeline(\n\
          > >     \"text-generation\",\n> >     model=model,\n> >     tokenizer=tokenizer,\n\
          > >     torch_dtype=torch.bfloat16,\n> >     trust_remote_code=True,\n>\
          \ >     device_map=\"auto\",\n> > )\n> > sequences = pipeline(\n> >    \"\
          Girafatron is obsessed with giraffes, the most glorious animal on the face\
          \ of this Earth. Giraftron believes all other animals are irrelevant when\
          \ compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\n> >     max_length=200,\n> >     do_sample=True,\n> > \
          \    top_k=10,\n> >     num_return_sequences=1,\n> >     eos_token_id=tokenizer.eos_token_id,\n\
          > > )\n> > for seq in sequences:\n> >     print(f\"Result: {seq['generated_text']}\"\
          )\n> > \n> > \n> > I get this output:\n> > \n> > Downloading (\u2026)okenizer_config.json:\
          \ 100%\n> > 175/175 [00:00<00:00, 6.61kB/s]\n> > Downloading (\u2026)/main/tokenizer.json:\
          \ 100%\n> > 2.73M/2.73M [00:00<00:00, 5.61MB/s]\n> > Downloading (\u2026\
          )cial_tokens_map.json: 100%\n> > 281/281 [00:00<00:00, 1.34kB/s]\n> > Downloading\
          \ (\u2026)lve/main/config.json: 100%\n> > 656/656 [00:00<00:00, 947B/s]\n\
          > > Downloading (\u2026)/configuration_RW.py: 100%\n> > 2.51k/2.51k [00:00<00:00,\
          \ 3.46kB/s]\n> > A new version of the following files was downloaded from\
          \ https://huggingface.co/tiiuae/falcon-40b:\n> > - configuration_RW.py\n\
          > > . Make sure to double-check they do not contain any added malicious\
          \ code. To avoid downloading new versions of the code file, you can pin\
          \ a revision.\n> > Downloading (\u2026)main/modelling_RW.py: 100%\n> > 47.1k/47.1k\
          \ [00:00<00:00, 108kB/s]\n> > A new version of the following files was downloaded\
          \ from https://huggingface.co/tiiuae/falcon-40b:\n> > - modelling_RW.py\n\
          > > . Make sure to double-check they do not contain any added malicious\
          \ code. To avoid downloading new versions of the code file, you can pin\
          \ a revision.\n> > Downloading (\u2026)model.bin.index.json: 100%\n> > 39.3k/39.3k\
          \ [00:00<00:00, 697kB/s]\n> > Downloading shards: 67%\n> > 6/9 [05:54<02:52,\
          \ 57.54s/it]\n> > Downloading (\u2026)l-00001-of-00009.bin: 100%\n> > 9.50G/9.50G\
          \ [00:46<00:00, 258MB/s]\n> > Downloading (\u2026)l-00002-of-00009.bin:\
          \ 100%\n> > 9.51G/9.51G [01:14<00:00, 257MB/s]\n> > Downloading (\u2026\
          )l-00003-of-00009.bin: 100%\n> > 9.51G/9.51G [00:50<00:00, 262MB/s]\n> >\
          \ Downloading (\u2026)l-00004-of-00009.bin: 100%\n> > 9.51G/9.51G [00:55<00:00,\
          \ 246MB/s]\n> > Downloading (\u2026)l-00005-of-00009.bin: 100%\n> > 9.51G/9.51G\
          \ [00:57<00:00, 224MB/s]\n> > Downloading (\u2026)l-00006-of-00009.bin:\
          \ 100%\n> > 9.51G/9.51G [00:58<00:00, 170MB/s]\n> > Downloading (\u2026\
          )l-00007-of-00009.bin: 18%\n> > 1.74G/9.51G [00:12<00:44, 174MB/s]\n> >\
          \ \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n> >\
          \ \u2502 in <cell line: 13>:13                                         \
          \                                   \u2502\n> > \u2502                 \
          \                                                                      \
          \           \u2502\n> > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n> > \u2502                                 \
          \                                                                 \u2502\
          \n> > \u2502   785 \u2502   # Forced if framework already defined, inferred\
          \ if it's None                           \u2502\n> > \u2502   786 \u2502\
          \   # Will load the correct model if possible                          \
          \                    \u2502\n> > \u2502   787 \u2502   model_classes = {\"\
          tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}            \
          \     \u2502\n> > \u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502\n> > \u2502   789 \u2502\
          \   \u2502   model,                                                    \
          \                         \u2502\n> > \u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n> > \u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502\n> > \u2502              \
          \                                                                      \
          \              \u2502\n> > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502\n> > \u2502 infer_framework_load_model  \
          \                                                                     \u2502\
          \n> > \u2502                                                           \
          \                                       \u2502\n> > \u2502    276 \u2502\
          \   \u2502   \u2502   \u2502   continue                                \
          \                                  \u2502\n> > \u2502    277 \u2502   \u2502\
          \                                                                      \
          \               \u2502\n> > \u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502\n\
          > > \u2502 \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could\
          \ not load model {model} with any of the following cl  \u2502\n> > \u2502\
          \    280 \u2502                                                        \
          \                                 \u2502\n> > \u2502    281 \u2502   framework\
          \ = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502\n> > \u2502    282 \u2502   return framework, model          \
          \                                                     \u2502\n> > \u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F\n> > ValueError: Could not load model tiiuae/falcon-40b\
          \ with any of the following classes: (<class \n> > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n> > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
          > \n> I got the same bug in google colab. Switched to using GPU and then\
          \ it worked fine.\n\nI am running this on Google COLAB (free version).\n\
          When I switch to GPU (GPU T4 Runtime in COLAB) I still get this error.\n\
          Also I tried switching to TPU on COLAB (which is possible because of the\
          \ use of the accelerate lib !), I still get the same error."
        updatedAt: '2023-05-29T06:09:11.778Z'
      numEdits: 0
      reactions: []
    id: 647441875ada8510bc393063
    type: comment
  author: JBHF
  content: "> > Whe using this code:\n> > \n> > # https://huggingface.co/tiiuae/falcon-40b\n\
    > > from transformers import AutoTokenizer, AutoModelForCausalLM\n> > import transformers\n\
    > > import torch\n> > \n> > model = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere\
    \ broertje: tiiuae/falcon-7b\n> > \n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > > pipeline = transformers.pipeline(\n> >     \"text-generation\",\n> >     model=model,\n\
    > >     tokenizer=tokenizer,\n> >     torch_dtype=torch.bfloat16,\n> >     trust_remote_code=True,\n\
    > >     device_map=\"auto\",\n> > )\n> > sequences = pipeline(\n> >    \"Girafatron\
    \ is obsessed with giraffes, the most glorious animal on the face of this Earth.\
    \ Giraftron believes all other animals are irrelevant when compared to the glorious\
    \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n> > \
    \    max_length=200,\n> >     do_sample=True,\n> >     top_k=10,\n> >     num_return_sequences=1,\n\
    > >     eos_token_id=tokenizer.eos_token_id,\n> > )\n> > for seq in sequences:\n\
    > >     print(f\"Result: {seq['generated_text']}\")\n> > \n> > \n> > I get this\
    \ output:\n> > \n> > Downloading (\u2026)okenizer_config.json: 100%\n> > 175/175\
    \ [00:00<00:00, 6.61kB/s]\n> > Downloading (\u2026)/main/tokenizer.json: 100%\n\
    > > 2.73M/2.73M [00:00<00:00, 5.61MB/s]\n> > Downloading (\u2026)cial_tokens_map.json:\
    \ 100%\n> > 281/281 [00:00<00:00, 1.34kB/s]\n> > Downloading (\u2026)lve/main/config.json:\
    \ 100%\n> > 656/656 [00:00<00:00, 947B/s]\n> > Downloading (\u2026)/configuration_RW.py:\
    \ 100%\n> > 2.51k/2.51k [00:00<00:00, 3.46kB/s]\n> > A new version of the following\
    \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n> > - configuration_RW.py\n\
    > > . Make sure to double-check they do not contain any added malicious code.\
    \ To avoid downloading new versions of the code file, you can pin a revision.\n\
    > > Downloading (\u2026)main/modelling_RW.py: 100%\n> > 47.1k/47.1k [00:00<00:00,\
    \ 108kB/s]\n> > A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
    > > - modelling_RW.py\n> > . Make sure to double-check they do not contain any\
    \ added malicious code. To avoid downloading new versions of the code file, you\
    \ can pin a revision.\n> > Downloading (\u2026)model.bin.index.json: 100%\n> >\
    \ 39.3k/39.3k [00:00<00:00, 697kB/s]\n> > Downloading shards: 67%\n> > 6/9 [05:54<02:52,\
    \ 57.54s/it]\n> > Downloading (\u2026)l-00001-of-00009.bin: 100%\n> > 9.50G/9.50G\
    \ [00:46<00:00, 258MB/s]\n> > Downloading (\u2026)l-00002-of-00009.bin: 100%\n\
    > > 9.51G/9.51G [01:14<00:00, 257MB/s]\n> > Downloading (\u2026)l-00003-of-00009.bin:\
    \ 100%\n> > 9.51G/9.51G [00:50<00:00, 262MB/s]\n> > Downloading (\u2026)l-00004-of-00009.bin:\
    \ 100%\n> > 9.51G/9.51G [00:55<00:00, 246MB/s]\n> > Downloading (\u2026)l-00005-of-00009.bin:\
    \ 100%\n> > 9.51G/9.51G [00:57<00:00, 224MB/s]\n> > Downloading (\u2026)l-00006-of-00009.bin:\
    \ 100%\n> > 9.51G/9.51G [00:58<00:00, 170MB/s]\n> > Downloading (\u2026)l-00007-of-00009.bin:\
    \ 18%\n> > 1.74G/9.51G [00:12<00:44, 174MB/s]\n> > \u256D\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\
    \n> > \u2502 in <cell line: 13>:13                                           \
    \                                 \u2502\n> > \u2502                         \
    \                                                                         \u2502\
    \n> > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
    \ in pipeline       \u2502\n> > \u2502                                       \
    \                                                           \u2502\n> > \u2502\
    \   785 \u2502   # Forced if framework already defined, inferred if it's None\
    \                           \u2502\n> > \u2502   786 \u2502   # Will load the\
    \ correct model if possible                                              \u2502\
    \n> > \u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"tf\"], \"\
    pt\": targeted_task[\"pt\"]}                 \u2502\n> > \u2502 \u2771 788 \u2502\
    \   framework, model = infer_framework_load_model(                           \
    \              \u2502\n> > \u2502   789 \u2502   \u2502   model,             \
    \                                                                \u2502\n> > \u2502\
    \   790 \u2502   \u2502   model_classes=model_classes,                       \
    \                                \u2502\n> > \u2502   791 \u2502   \u2502   config=config,\
    \                                                                     \u2502\n\
    > > \u2502                                                                   \
    \                               \u2502\n> > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
    \ in                    \u2502\n> > \u2502 infer_framework_load_model        \
    \                                                               \u2502\n> > \u2502\
    \                                                                            \
    \                      \u2502\n> > \u2502    276 \u2502   \u2502   \u2502   \u2502\
    \   continue                                                                 \
    \ \u2502\n> > \u2502    277 \u2502   \u2502                                  \
    \                                                   \u2502\n> > \u2502    278\
    \ \u2502   \u2502   if isinstance(model, str):                               \
    \                         \u2502\n> > \u2502 \u2771  279 \u2502   \u2502   \u2502\
    \   raise ValueError(f\"Could not load model {model} with any of the following\
    \ cl  \u2502\n> > \u2502    280 \u2502                                       \
    \                                                  \u2502\n> > \u2502    281 \u2502\
    \   framework = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
    \  \u2502\n> > \u2502    282 \u2502   return framework, model                \
    \                                               \u2502\n> > \u2570\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256F\n> > ValueError: Could not load model tiiuae/falcon-40b\
    \ with any of the following classes: (<class \n> > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \n> > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
    > \n> I got the same bug in google colab. Switched to using GPU and then it worked\
    \ fine.\n\nI am running this on Google COLAB (free version).\nWhen I switch to\
    \ GPU (GPU T4 Runtime in COLAB) I still get this error.\nAlso I tried switching\
    \ to TPU on COLAB (which is possible because of the use of the accelerate lib\
    \ !), I still get the same error."
  created_at: 2023-05-29 05:09:11+00:00
  edited: false
  hidden: false
  id: 647441875ada8510bc393063
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
      fullname: Air Table
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: airtable
      type: user
    createdAt: '2023-05-30T09:44:43.000Z'
    data:
      edited: false
      editors:
      - airtable
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62ec05a0857bc02b8d060e2c71b8eb53.svg
          fullname: Air Table
          isHf: false
          isPro: false
          name: airtable
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Now it is taking my entire space in\
          \ my colab pro tier. anyone knows how much space it is supposed to take?\
          \ And how long does it take to run it on colab pro with v100?</p>\n</blockquote>\n\
          <p>It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn't\
          \ fit in a single A100, with 80 GB memory). But you can try <a href=\"https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ\"\
          >https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ</a> which should\
          \ take only 20 GB of VRAM.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;leonlahoud&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/leonlahoud\"\
          >@<span class=\"underline\">leonlahoud</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;maccam912&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/maccam912\">@<span class=\"\
          underline\">maccam912</span></a></span>\n\n\t</span></span> Falcon-40B doesn't\
          \ load on 1xNvidia H100 GPU with 80 GB VRAM, Falcon-7B works though I don't\
          \ like answer repetition.</p>\n"
        raw: "> > Now it is taking my entire space in my colab pro tier. anyone knows\
          \ how much space it is supposed to take? And how long does it take to run\
          \ it on colab pro with v100?\n> \n> It will take 80 GB of VRAM or so, plus\
          \ some extra for overhead (it doesn't fit in a single A100, with 80 GB memory).\
          \ But you can try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ\
          \ which should take only 20 GB of VRAM.\n\n@leonlahoud @maccam912 Falcon-40B\
          \ doesn't load on 1xNvidia H100 GPU with 80 GB VRAM, Falcon-7B works though\
          \ I don't like answer repetition."
        updatedAt: '2023-05-30T09:44:43.602Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - leonlahoud
    id: 6475c58be9b57ce0caa512ef
    type: comment
  author: airtable
  content: "> > Now it is taking my entire space in my colab pro tier. anyone knows\
    \ how much space it is supposed to take? And how long does it take to run it on\
    \ colab pro with v100?\n> \n> It will take 80 GB of VRAM or so, plus some extra\
    \ for overhead (it doesn't fit in a single A100, with 80 GB memory). But you can\
    \ try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ which should take\
    \ only 20 GB of VRAM.\n\n@leonlahoud @maccam912 Falcon-40B doesn't load on 1xNvidia\
    \ H100 GPU with 80 GB VRAM, Falcon-7B works though I don't like answer repetition."
  created_at: 2023-05-30 08:44:43+00:00
  edited: false
  hidden: false
  id: 6475c58be9b57ce0caa512ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f2745982455e61cc5fb1d/Pi7Y2p5R2Ul5tMGClX5vy.jpeg?w=200&h=200&f=face
      fullname: Radu-Sebastian Amarie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eek
      type: user
    createdAt: '2023-05-30T13:31:56.000Z'
    data:
      edited: false
      editors:
      - eek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630f2745982455e61cc5fb1d/Pi7Y2p5R2Ul5tMGClX5vy.jpeg?w=200&h=200&f=face
          fullname: Radu-Sebastian Amarie
          isHf: false
          isPro: false
          name: eek
          type: user
        html: '<p>I also have issues with running Falcon-40B on 1xH100 GPU with 80GB
          of VRAM using 8-bit quantization. It fails with <code>Exception: cublasLt
          ran into an error!</code> tried with both everything built for CUDA 11.8
          and CUDA 12.1, and still fails, even though bitsandbytes says everything
          is ok.</p>

          '
        raw: 'I also have issues with running Falcon-40B on 1xH100 GPU with 80GB of
          VRAM using 8-bit quantization. It fails with `Exception: cublasLt ran into
          an error!` tried with both everything built for CUDA 11.8 and CUDA 12.1,
          and still fails, even though bitsandbytes says everything is ok.'
        updatedAt: '2023-05-30T13:31:56.106Z'
      numEdits: 0
      reactions: []
    id: 6475facc09e77322633ab68c
    type: comment
  author: eek
  content: 'I also have issues with running Falcon-40B on 1xH100 GPU with 80GB of
    VRAM using 8-bit quantization. It fails with `Exception: cublasLt ran into an
    error!` tried with both everything built for CUDA 11.8 and CUDA 12.1, and still
    fails, even though bitsandbytes says everything is ok.'
  created_at: 2023-05-30 12:31:56+00:00
  edited: false
  hidden: false
  id: 6475facc09e77322633ab68c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c8b2cdef3599eaf4a345e1ac486d964.svg
      fullname: Sumit Agrawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumitagrawal
      type: user
    createdAt: '2023-05-30T18:41:05.000Z'
    data:
      edited: false
      editors:
      - sumitagrawal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6c8b2cdef3599eaf4a345e1ac486d964.svg
          fullname: Sumit Agrawal
          isHf: false
          isPro: false
          name: sumitagrawal
          type: user
        html: "<p>Same error at my end also.</p>\n<pre><code> % python falcon-demo.py\n\
          Explicitly passing a `revision` is encouraged when loading a configuration\
          \ with custom code to ensure no malicious code has been contributed in a\
          \ newer revision.\nExplicitly passing a `revision` is encouraged when loading\
          \ a configuration with custom code to ensure no malicious code has been\
          \ contributed in a newer revision.\nExplicitly passing a `revision` is encouraged\
          \ when loading a model with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.\nDownloading (\u2026)l-00007-of-00009.bin:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 9.51G/9.51G [20:08&lt;00:00, 7.87MB/s]\n\
          Downloading (\u2026)l-00008-of-00009.bin: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 9.51G/9.51G [18:28&lt;00:00, 8.58MB/s]\nDownloading (\u2026)l-00009-of-00009.bin:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 7.58G/7.58G [18:49&lt;00:00, 6.71MB/s]\n\
          Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [57:32&lt;00:00, 383.65s/it]\n\
          Traceback (most recent call last):\n  File \"/LLM/falcon-demo.py\", line\
          \ 10, in &lt;module&gt;\n    pipeline = transformers.pipeline(\n  File \"\
          /Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,).\n\
          </code></pre>\n<p>Reran it, this is the result again.</p>\n<pre><code>%\
          \ python falcon-demo.py\nExplicitly passing a `revision` is encouraged when\
          \ loading a configuration with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.\nExplicitly passing a `revision`\
          \ is encouraged when loading a configuration with custom code to ensure\
          \ no malicious code has been contributed in a newer revision.\nExplicitly\
          \ passing a `revision` is encouraged when loading a model with custom code\
          \ to ensure no malicious code has been contributed in a newer revision.\n\
          Traceback (most recent call last):\n  File \"/LLM/falcon-demo.py\", line\
          \ 10, in &lt;module&gt;\n    pipeline = transformers.pipeline(\n  File \"\
          /opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,).\n\
          </code></pre>\n"
        raw: "Same error at my end also.\n```\n % python falcon-demo.py\nExplicitly\
          \ passing a `revision` is encouraged when loading a configuration with custom\
          \ code to ensure no malicious code has been contributed in a newer revision.\n\
          Explicitly passing a `revision` is encouraged when loading a configuration\
          \ with custom code to ensure no malicious code has been contributed in a\
          \ newer revision.\nExplicitly passing a `revision` is encouraged when loading\
          \ a model with custom code to ensure no malicious code has been contributed\
          \ in a newer revision.\nDownloading (\u2026)l-00007-of-00009.bin: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 9.51G/9.51G [20:08<00:00, 7.87MB/s]\nDownloading\
          \ (\u2026)l-00008-of-00009.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.51G/9.51G\
          \ [18:28<00:00, 8.58MB/s]\nDownloading (\u2026)l-00009-of-00009.bin: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 7.58G/7.58G [18:49<00:00, 6.71MB/s]\nDownloading\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [57:32<00:00, 383.65s/it]\nTraceback\
          \ (most recent call last):\n  File \"/LLM/falcon-demo.py\", line 10, in\
          \ <module>\n    pipeline = transformers.pipeline(\n  File \"/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          ```\nReran it, this is the result again.\n```\n% python falcon-demo.py\n\
          Explicitly passing a `revision` is encouraged when loading a configuration\
          \ with custom code to ensure no malicious code has been contributed in a\
          \ newer revision.\nExplicitly passing a `revision` is encouraged when loading\
          \ a configuration with custom code to ensure no malicious code has been\
          \ contributed in a newer revision.\nExplicitly passing a `revision` is encouraged\
          \ when loading a model with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.\nTraceback (most recent call last):\n\
          \  File \"/LLM/falcon-demo.py\", line 10, in <module>\n    pipeline = transformers.pipeline(\n\
          \  File \"/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
          , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          ```"
        updatedAt: '2023-05-30T18:41:05.152Z'
      numEdits: 0
      reactions: []
    id: 6476434157108da176fc5d95
    type: comment
  author: sumitagrawal
  content: "Same error at my end also.\n```\n % python falcon-demo.py\nExplicitly\
    \ passing a `revision` is encouraged when loading a configuration with custom\
    \ code to ensure no malicious code has been contributed in a newer revision.\n\
    Explicitly passing a `revision` is encouraged when loading a configuration with\
    \ custom code to ensure no malicious code has been contributed in a newer revision.\n\
    Explicitly passing a `revision` is encouraged when loading a model with custom\
    \ code to ensure no malicious code has been contributed in a newer revision.\n\
    Downloading (\u2026)l-00007-of-00009.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 9.51G/9.51G [20:08<00:00, 7.87MB/s]\nDownloading\
    \ (\u2026)l-00008-of-00009.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 9.51G/9.51G [18:28<00:00, 8.58MB/s]\nDownloading (\u2026\
    )l-00009-of-00009.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 7.58G/7.58G [18:49<00:00, 6.71MB/s]\nDownloading shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [57:32<00:00, 383.65s/it]\nTraceback\
    \ (most recent call last):\n  File \"/LLM/falcon-demo.py\", line 10, in <module>\n\
    \    pipeline = transformers.pipeline(\n  File \"/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
    , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \  File \"/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
    , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could not load\
    \ model {model} with any of the following classes: {class_tuple}.\")\nValueError:\
    \ Could not load model tiiuae/falcon-40b-instruct with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
    ```\nReran it, this is the result again.\n```\n% python falcon-demo.py\nExplicitly\
    \ passing a `revision` is encouraged when loading a configuration with custom\
    \ code to ensure no malicious code has been contributed in a newer revision.\n\
    Explicitly passing a `revision` is encouraged when loading a configuration with\
    \ custom code to ensure no malicious code has been contributed in a newer revision.\n\
    Explicitly passing a `revision` is encouraged when loading a model with custom\
    \ code to ensure no malicious code has been contributed in a newer revision.\n\
    Traceback (most recent call last):\n  File \"/LLM/falcon-demo.py\", line 10, in\
    \ <module>\n    pipeline = transformers.pipeline(\n  File \"/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py\"\
    , line 779, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \  File \"/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
    , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could not load\
    \ model {model} with any of the following classes: {class_tuple}.\")\nValueError:\
    \ Could not load model tiiuae/falcon-40b-instruct with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
    ```"
  created_at: 2023-05-30 17:41:05+00:00
  edited: false
  hidden: false
  id: 6476434157108da176fc5d95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5417956515b46bd61d78aa659013a479.svg
      fullname: Alex Wall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexwall77
      type: user
    createdAt: '2023-05-31T01:36:31.000Z'
    data:
      edited: false
      editors:
      - alexwall77
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5417956515b46bd61d78aa659013a479.svg
          fullname: Alex Wall
          isHf: false
          isPro: false
          name: alexwall77
          type: user
        html: "<p>I can't run this on my machine because I don't have the hardware,\
          \ but I was able to get past the above errors by adjusting the code as follows,\
          \ specifically the model = line:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\", trust_remote_code=True)\n\
          \n\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is\
          \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
          \ Giraftron believes all other animals are irrelevant when compared to the\
          \ glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
          ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"Result: {seq['generated_text']}\")\n</code></pre>\n"
        raw: "I can't run this on my machine because I don't have the hardware, but\
          \ I was able to get past the above errors by adjusting the code as follows,\
          \ specifically the model = line:\n```\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b\", trust_remote_code=True)\n\n\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```"
        updatedAt: '2023-05-31T01:36:31.189Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - afafeaw
        - datajohngraham
        - ArunAMenon
        - chengxuncc
        - patti-j
        - sdanzo
        - Mohcine7
    id: 6476a49fcfe9d995bf440d66
    type: comment
  author: alexwall77
  content: "I can't run this on my machine because I don't have the hardware, but\
    \ I was able to get past the above errors by adjusting the code as follows, specifically\
    \ the model = line:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import transformers\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-40b\", trust_remote_code=True)\n\n\n\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
    \   \"Girafatron is obsessed with giraffes, the most glorious animal on the face\
    \ of this Earth. Giraftron believes all other animals are irrelevant when compared\
    \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
    ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
    \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"\
    Result: {seq['generated_text']}\")\n```"
  created_at: 2023-05-31 00:36:31+00:00
  edited: false
  hidden: false
  id: 6476a49fcfe9d995bf440d66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
      fullname: KitasanBlack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kitachan
      type: user
    createdAt: '2023-05-31T02:57:27.000Z'
    data:
      edited: false
      editors:
      - Kitachan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
          fullname: KitasanBlack
          isHf: false
          isPro: false
          name: Kitachan
          type: user
        html: '<p>Does anyone know how to move the .cache file generated by the model
          data download from C drive to another drive? My C drive doesn''t have that
          much space =)</p>

          '
        raw: Does anyone know how to move the .cache file generated by the model data
          download from C drive to another drive? My C drive doesn't have that much
          space =)
        updatedAt: '2023-05-31T02:57:27.865Z'
      numEdits: 0
      reactions: []
    id: 6476b797a4ebdbfb13860e3a
    type: comment
  author: Kitachan
  content: Does anyone know how to move the .cache file generated by the model data
    download from C drive to another drive? My C drive doesn't have that much space
    =)
  created_at: 2023-05-31 01:57:27+00:00
  edited: false
  hidden: false
  id: 6476b797a4ebdbfb13860e3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5417956515b46bd61d78aa659013a479.svg
      fullname: Alex Wall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexwall77
      type: user
    createdAt: '2023-05-31T03:03:35.000Z'
    data:
      edited: true
      editors:
      - alexwall77
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5417956515b46bd61d78aa659013a479.svg
          fullname: Alex Wall
          isHf: false
          isPro: false
          name: alexwall77
          type: user
        html: '<p>Possibly by adding this to your python script that runs the transformer
          </p>

          <pre><code>cache_dir = "/path/to/new/cache_directory"

          CacheConfig().cache_dir = cache_dir

          </code></pre>

          '
        raw: "Possibly by adding this to your python script that runs the transformer\
          \ \n```\ncache_dir = \"/path/to/new/cache_directory\"\nCacheConfig().cache_dir\
          \ = cache_dir\n```"
        updatedAt: '2023-05-31T03:04:11.237Z'
      numEdits: 2
      reactions: []
    id: 6476b9070214fec3e76cdd9a
    type: comment
  author: alexwall77
  content: "Possibly by adding this to your python script that runs the transformer\
    \ \n```\ncache_dir = \"/path/to/new/cache_directory\"\nCacheConfig().cache_dir\
    \ = cache_dir\n```"
  created_at: 2023-05-31 02:03:35+00:00
  edited: true
  hidden: false
  id: 6476b9070214fec3e76cdd9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
      fullname: KitasanBlack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kitachan
      type: user
    createdAt: '2023-05-31T03:10:42.000Z'
    data:
      edited: false
      editors:
      - Kitachan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
          fullname: KitasanBlack
          isHf: false
          isPro: false
          name: Kitachan
          type: user
        html: '<blockquote>

          <p>Possibly by adding this to your python script that runs the transformer
          </p>

          <pre><code>cache_dir = "/path/to/new/cache_directory"

          CacheConfig().cache_dir = cache_dir

          </code></pre>

          <p>I tried doing this but it doesn''t seem to work</p>

          </blockquote>

          '
        raw: "> Possibly by adding this to your python script that runs the transformer\
          \ \n> ```\n> cache_dir = \"/path/to/new/cache_directory\"\n> CacheConfig().cache_dir\
          \ = cache_dir\n> ```\nI tried doing this but it doesn't seem to work"
        updatedAt: '2023-05-31T03:10:42.993Z'
      numEdits: 0
      reactions: []
    id: 6476bab226ec66674c6edea3
    type: comment
  author: Kitachan
  content: "> Possibly by adding this to your python script that runs the transformer\
    \ \n> ```\n> cache_dir = \"/path/to/new/cache_directory\"\n> CacheConfig().cache_dir\
    \ = cache_dir\n> ```\nI tried doing this but it doesn't seem to work"
  created_at: 2023-05-31 02:10:42+00:00
  edited: false
  hidden: false
  id: 6476bab226ec66674c6edea3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-31T04:03:56.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Kitachan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Kitachan\">@<span class=\"\
          underline\">Kitachan</span></a></span>\n\n\t</span></span> You should be\
          \ able to set environment variables <code>HUGGINGFACE_HUB_CACHE</code> or\
          \ <code>HF_HOME</code> to where you want the cache to be. </p>\n<p>See <a\
          \ href=\"https://huggingface.co/docs/huggingface_hub/guides/manage-cache\"\
          >https://huggingface.co/docs/huggingface_hub/guides/manage-cache</a></p>\n"
        raw: "@Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE`\
          \ or `HF_HOME` to where you want the cache to be. \n\nSee https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
        updatedAt: '2023-05-31T04:03:56.732Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kitachan
    id: 6476c72ca4ebdbfb1387444d
    type: comment
  author: FalconLLM
  content: "@Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE`\
    \ or `HF_HOME` to where you want the cache to be. \n\nSee https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
  created_at: 2023-05-31 03:03:56+00:00
  edited: false
  hidden: false
  id: 6476c72ca4ebdbfb1387444d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
      fullname: KitasanBlack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kitachan
      type: user
    createdAt: '2023-05-31T06:28:09.000Z'
    data:
      edited: false
      editors:
      - Kitachan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
          fullname: KitasanBlack
          isHf: false
          isPro: false
          name: Kitachan
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Kitachan&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Kitachan\"\
          >@<span class=\"underline\">Kitachan</span></a></span>\n\n\t</span></span>\
          \ You should be able to set environment variables <code>HUGGINGFACE_HUB_CACHE</code>\
          \ or <code>HF_HOME</code> to where you want the cache to be. </p>\n<p>See\
          \ <a href=\"https://huggingface.co/docs/huggingface_hub/guides/manage-cache\"\
          >https://huggingface.co/docs/huggingface_hub/guides/manage-cache</a></p>\n\
          </blockquote>\n<p>This did work for my problem, thanks</p>\n"
        raw: "> @Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE`\
          \ or `HF_HOME` to where you want the cache to be. \n> \n> See https://huggingface.co/docs/huggingface_hub/guides/manage-cache\n\
          \nThis did work for my problem, thanks"
        updatedAt: '2023-05-31T06:28:09.508Z'
      numEdits: 0
      reactions: []
    id: 6476e8f940c99df876fcd133
    type: comment
  author: Kitachan
  content: "> @Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE`\
    \ or `HF_HOME` to where you want the cache to be. \n> \n> See https://huggingface.co/docs/huggingface_hub/guides/manage-cache\n\
    \nThis did work for my problem, thanks"
  created_at: 2023-05-31 05:28:09+00:00
  edited: false
  hidden: false
  id: 6476e8f940c99df876fcd133
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-31T14:27:42.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>I am having the same issue for Falcon 40b instruct. See the error
          below. Any solution?</p>

          <p>ValueError                                Traceback (most recent call
          last)<br>Cell In[2], line 8<br>      5 model = "tiiuae/falcon-40b-instruct"<br>      7
          tokenizer = AutoTokenizer.from_pretrained(model)<br>----&gt; 8 pipeline
          = transformers.pipeline(<br>      9     "text-generation",<br>     10     model=model,<br>     11     tokenizer=tokenizer,<br>     12     torch_dtype=torch.bfloat16,<br>     13     trust_remote_code=True,<br>     14     device_map="auto",<br>     15
          )</p>

          <p>File ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py:788,
          in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,
          framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,
          trust_remote_code, model_kwargs, pipeline_class, **kwargs)<br>    784 #
          Infer the framework from the model<br>    785 # Forced if framework already
          defined, inferred if it''s None<br>    786 # Will load the correct model
          if possible<br>    787 model_classes = {"tf": targeted_task["tf"], "pt":
          targeted_task["pt"]}<br>--&gt; 788 framework, model = infer_framework_load_model(<br>    789     model,<br>    790     model_classes=model_classes,<br>    791     config=config,<br>    792     framework=framework,<br>    793     task=task,<br>    794     **hub_kwargs,<br>    795     **model_kwargs,<br>    796
          )<br>    798 model_config = model.config<br>    799 hub_kwargs["_commit_hash"]
          = model.config._commit_hash</p>

          <p>File ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/base.py:279,
          in infer_framework_load_model(model, config, model_classes, task, framework,
          **model_kwargs)<br>    276             continue<br>    278     if isinstance(model,
          str):<br>--&gt; 279         raise ValueError(f"Could not load model {model}
          with any of the following classes: {class_tuple}.")<br>    281 framework
          = "tf" if "keras.engine.training.Model" in str(inspect.getmro(model.<strong>class</strong>))
          else "pt"<br>    282 return framework, model</p>

          <p>ValueError: Could not load model tiiuae/falcon-40b-instruct with any
          of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          '
        raw: "I am having the same issue for Falcon 40b instruct. See the error below.\
          \ Any solution?\n\nValueError                                Traceback (most\
          \ recent call last)\nCell In[2], line 8\n      5 model = \"tiiuae/falcon-40b-instruct\"\
          \n      7 tokenizer = AutoTokenizer.from_pretrained(model)\n----> 8 pipeline\
          \ = transformers.pipeline(\n      9     \"text-generation\",\n     10  \
          \   model=model,\n     11     tokenizer=tokenizer,\n     12     torch_dtype=torch.bfloat16,\n\
          \     13     trust_remote_code=True,\n     14     device_map=\"auto\",\n\
          \     15 )\n\nFile ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/__init__.py:788,\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\n    784 #\
          \ Infer the framework from the model\n    785 # Forced if framework already\
          \ defined, inferred if it's None\n    786 # Will load the correct model\
          \ if possible\n    787 model_classes = {\"tf\": targeted_task[\"tf\"], \"\
          pt\": targeted_task[\"pt\"]}\n--> 788 framework, model = infer_framework_load_model(\n\
          \    789     model,\n    790     model_classes=model_classes,\n    791 \
          \    config=config,\n    792     framework=framework,\n    793     task=task,\n\
          \    794     **hub_kwargs,\n    795     **model_kwargs,\n    796 )\n   \
          \ 798 model_config = model.config\n    799 hub_kwargs[\"_commit_hash\"]\
          \ = model.config._commit_hash\n\nFile ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/base.py:279,\
          \ in infer_framework_load_model(model, config, model_classes, task, framework,\
          \ **model_kwargs)\n    276             continue\n    278     if isinstance(model,\
          \ str):\n--> 279         raise ValueError(f\"Could not load model {model}\
          \ with any of the following classes: {class_tuple}.\")\n    281 framework\
          \ = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__class__))\
          \ else \"pt\"\n    282 return framework, model\n\nValueError: Could not\
          \ load model tiiuae/falcon-40b-instruct with any of the following classes:\
          \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
        updatedAt: '2023-05-31T14:27:42.733Z'
      numEdits: 0
      reactions: []
    id: 6477595ef911e9e76c61c5b8
    type: comment
  author: phdykd
  content: "I am having the same issue for Falcon 40b instruct. See the error below.\
    \ Any solution?\n\nValueError                                Traceback (most recent\
    \ call last)\nCell In[2], line 8\n      5 model = \"tiiuae/falcon-40b-instruct\"\
    \n      7 tokenizer = AutoTokenizer.from_pretrained(model)\n----> 8 pipeline =\
    \ transformers.pipeline(\n      9     \"text-generation\",\n     10     model=model,\n\
    \     11     tokenizer=tokenizer,\n     12     torch_dtype=torch.bfloat16,\n \
    \    13     trust_remote_code=True,\n     14     device_map=\"auto\",\n     15\
    \ )\n\nFile ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/__init__.py:788,\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
    \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\n    784 # Infer\
    \ the framework from the model\n    785 # Forced if framework already defined,\
    \ inferred if it's None\n    786 # Will load the correct model if possible\n \
    \   787 model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
    pt\"]}\n--> 788 framework, model = infer_framework_load_model(\n    789     model,\n\
    \    790     model_classes=model_classes,\n    791     config=config,\n    792\
    \     framework=framework,\n    793     task=task,\n    794     **hub_kwargs,\n\
    \    795     **model_kwargs,\n    796 )\n    798 model_config = model.config\n\
    \    799 hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n\nFile ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/base.py:279,\
    \ in infer_framework_load_model(model, config, model_classes, task, framework,\
    \ **model_kwargs)\n    276             continue\n    278     if isinstance(model,\
    \ str):\n--> 279         raise ValueError(f\"Could not load model {model} with\
    \ any of the following classes: {class_tuple}.\")\n    281 framework = \"tf\"\
    \ if \"keras.engine.training.Model\" in str(inspect.getmro(model.__class__)) else\
    \ \"pt\"\n    282 return framework, model\n\nValueError: Could not load model\
    \ tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
  created_at: 2023-05-31 13:27:42+00:00
  edited: false
  hidden: false
  id: 6477595ef911e9e76c61c5b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
      fullname: KitasanBlack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kitachan
      type: user
    createdAt: '2023-05-31T15:00:13.000Z'
    data:
      edited: false
      editors:
      - Kitachan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
          fullname: KitasanBlack
          isHf: false
          isPro: false
          name: Kitachan
          type: user
        html: '<p>hummm I was wondering how much VM I needed to run it, and my computer
          didn''t seem to have enough RAM =( Maybe I should try google colab, but
          it feels cumbersome to have to re-download the model every time</p>

          '
        raw: hummm I was wondering how much VM I needed to run it, and my computer
          didn't seem to have enough RAM =( Maybe I should try google colab, but it
          feels cumbersome to have to re-download the model every time
        updatedAt: '2023-05-31T15:00:13.919Z'
      numEdits: 0
      reactions: []
    id: 647760fdbb7681ad67065436
    type: comment
  author: Kitachan
  content: hummm I was wondering how much VM I needed to run it, and my computer didn't
    seem to have enough RAM =( Maybe I should try google colab, but it feels cumbersome
    to have to re-download the model every time
  created_at: 2023-05-31 14:00:13+00:00
  edited: false
  hidden: false
  id: 647760fdbb7681ad67065436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-31T15:05:07.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was
          downloading .bin, but then stopped and gave the error mentioned above. Still
          waiting someone to answer it.</p>

          '
        raw: I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading
          .bin, but then stopped and gave the error mentioned above. Still waiting
          someone to answer it.
        updatedAt: '2023-05-31T15:05:07.441Z'
      numEdits: 0
      reactions: []
    id: 64776223f911e9e76c62a0c4
    type: comment
  author: phdykd
  content: I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading
    .bin, but then stopped and gave the error mentioned above. Still waiting someone
    to answer it.
  created_at: 2023-05-31 14:05:07+00:00
  edited: false
  hidden: false
  id: 64776223f911e9e76c62a0c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71726db2fdfea9a1c107eb16928517d.svg
      fullname: Phil Mui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philmui
      type: user
    createdAt: '2023-06-01T01:53:26.000Z'
    data:
      edited: false
      editors:
      - philmui
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71726db2fdfea9a1c107eb16928517d.svg
          fullname: Phil Mui
          isHf: false
          isPro: false
          name: philmui
          type: user
        html: '<p>M2max with 96GB -- definitely not a machine / hardware issue!</p>

          <p>Same error as many of you:</p>

          <blockquote>

          <p>raise ValueError(f"Could not load model {model} with any of the following
          classes: {class_tuple}.")<br>ValueError: Could not load model tiiuae/falcon-40b-instruct
          with any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          </blockquote>

          '
        raw: 'M2max with 96GB -- definitely not a machine / hardware issue!


          Same error as many of you:


          > raise ValueError(f"Could not load model {model} with any of the following
          classes: {class_tuple}.")

          ValueError: Could not load model tiiuae/falcon-40b-instruct with any of
          the following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
        updatedAt: '2023-06-01T01:53:26.235Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - jmugan
        - yuraai
    id: 6477fa16f911e9e76c6ec962
    type: comment
  author: philmui
  content: 'M2max with 96GB -- definitely not a machine / hardware issue!


    Same error as many of you:


    > raise ValueError(f"Could not load model {model} with any of the following classes:
    {class_tuple}.")

    ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following
    classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
  created_at: 2023-06-01 00:53:26+00:00
  edited: false
  hidden: false
  id: 6477fa16f911e9e76c6ec962
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3de3fb7d36f23996cedff24a16e73cfe.svg
      fullname: AJ Shreim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ashreim
      type: user
    createdAt: '2023-06-01T05:38:59.000Z'
    data:
      edited: false
      editors:
      - ashreim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3de3fb7d36f23996cedff24a16e73cfe.svg
          fullname: AJ Shreim
          isHf: false
          isPro: false
          name: ashreim
          type: user
        html: '<blockquote>

          <p>Does anyone know how to move the .cache file generated by the model data
          download from C drive to another drive? My C drive doesn''t have that much
          space =)</p>

          </blockquote>

          <p>move your cache to new directory<br>and use this:<br>import os<br>os.environ[''HF_HOME'']
          = '':\/HuggingFace''</p>

          '
        raw: "> Does anyone know how to move the .cache file generated by the model\
          \ data download from C drive to another drive? My C drive doesn't have that\
          \ much space =)\n\nmove your cache to new directory  \nand use this:\nimport\
          \ os\nos.environ['HF_HOME'] = '<drive>:\\\\<xxxxxxxxx>/HuggingFace'"
        updatedAt: '2023-06-01T05:38:59.065Z'
      numEdits: 0
      reactions: []
    id: 64782ef3bb7681ad6716ca8e
    type: comment
  author: ashreim
  content: "> Does anyone know how to move the .cache file generated by the model\
    \ data download from C drive to another drive? My C drive doesn't have that much\
    \ space =)\n\nmove your cache to new directory  \nand use this:\nimport os\nos.environ['HF_HOME']\
    \ = '<drive>:\\\\<xxxxxxxxx>/HuggingFace'"
  created_at: 2023-06-01 04:38:59+00:00
  edited: false
  hidden: false
  id: 64782ef3bb7681ad6716ca8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
      fullname: KitasanBlack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kitachan
      type: user
    createdAt: '2023-06-01T07:26:27.000Z'
    data:
      edited: false
      editors:
      - Kitachan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476b71002fc644c810b589e/Q-JwFpZEFjH-0LhFM6XuK.png?w=200&h=200&f=face
          fullname: KitasanBlack
          isHf: false
          isPro: false
          name: Kitachan
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Does anyone know how to move the .cache file generated by the model data
          download from C drive to another drive? My C drive doesn''t have that much
          space =)</p>

          </blockquote>

          <p>move your cache to new directory<br>and use this:<br>import os<br>os.environ[''HF_HOME'']
          = '':\/HuggingFace''</p>

          </blockquote>

          <p>It really works, thx =)</p>

          '
        raw: "> > Does anyone know how to move the .cache file generated by the model\
          \ data download from C drive to another drive? My C drive doesn't have that\
          \ much space =)\n> \n> move your cache to new directory  \n> and use this:\n\
          > import os\n> os.environ['HF_HOME'] = '<drive>:\\\\<xxxxxxxxx>/HuggingFace'\n\
          \nIt really works, thx =)"
        updatedAt: '2023-06-01T07:26:27.310Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ashreim
    id: 64784823ad83f3939b45cad2
    type: comment
  author: Kitachan
  content: "> > Does anyone know how to move the .cache file generated by the model\
    \ data download from C drive to another drive? My C drive doesn't have that much\
    \ space =)\n> \n> move your cache to new directory  \n> and use this:\n> import\
    \ os\n> os.environ['HF_HOME'] = '<drive>:\\\\<xxxxxxxxx>/HuggingFace'\n\nIt really\
    \ works, thx =)"
  created_at: 2023-06-01 06:26:27+00:00
  edited: false
  hidden: false
  id: 64784823ad83f3939b45cad2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
      fullname: Juraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joorei
      type: user
    createdAt: '2023-06-01T08:53:22.000Z'
    data:
      edited: false
      editors:
      - joorei
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
          fullname: Juraj
          isHf: false
          isPro: false
          name: joorei
          type: user
        html: '<p>Same here. :(</p>

          '
        raw: Same here. :(
        updatedAt: '2023-06-01T08:53:22.141Z'
      numEdits: 0
      reactions: []
    id: 64785c82ad83f3939b47d807
    type: comment
  author: joorei
  content: Same here. :(
  created_at: 2023-06-01 07:53:22+00:00
  edited: false
  hidden: false
  id: 64785c82ad83f3939b47d807
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64665600e03f25ce60f7c82d/5zkgFlk92A7nVxI7WF8NC.png?w=200&h=200&f=face
      fullname: Richard Christopher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alephpt
      type: user
    createdAt: '2023-06-01T15:03:19.000Z'
    data:
      edited: true
      editors:
      - alephpt
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64665600e03f25ce60f7c82d/5zkgFlk92A7nVxI7WF8NC.png?w=200&h=200&f=face
          fullname: Richard Christopher
          isHf: false
          isPro: false
          name: alephpt
          type: user
        html: '<p>Was this problem fixed?</p>

          <p>I have a Ryzen 9 7900X 12 core, and 64GB of Ram with 2TB of disk space..
          everything downloaded but doesn''t run and gets the same error as OP.</p>

          '
        raw: 'Was this problem fixed?


          I have a Ryzen 9 7900X 12 core, and 64GB of Ram with 2TB of disk space..
          everything downloaded but doesn''t run and gets the same error as OP.'
        updatedAt: '2023-06-01T15:09:29.453Z'
      numEdits: 2
      reactions: []
    id: 6478b337ad83f3939b500799
    type: comment
  author: alephpt
  content: 'Was this problem fixed?


    I have a Ryzen 9 7900X 12 core, and 64GB of Ram with 2TB of disk space.. everything
    downloaded but doesn''t run and gets the same error as OP.'
  created_at: 2023-06-01 14:03:19+00:00
  edited: true
  hidden: false
  id: 6478b337ad83f3939b500799
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-06-01T15:05:04.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was
          downloading .bin, but then stopped and gave the error mentioned above. Still
          waiting someone to answer it. </p>

          <p>It has not been resolved yet...</p>

          '
        raw: "I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading\
          \ .bin, but then stopped and gave the error mentioned above. Still waiting\
          \ someone to answer it. \n\nIt has not been resolved yet..."
        updatedAt: '2023-06-01T15:05:04.261Z'
      numEdits: 0
      reactions: []
    id: 6478b3a0159a889d0027fe5d
    type: comment
  author: phdykd
  content: "I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading\
    \ .bin, but then stopped and gave the error mentioned above. Still waiting someone\
    \ to answer it. \n\nIt has not been resolved yet..."
  created_at: 2023-06-01 14:05:04+00:00
  edited: false
  hidden: false
  id: 6478b3a0159a889d0027fe5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-06-01T16:53:23.000Z'
    data:
      edited: false
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: '<p>On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch
          2.0.0 (Ubuntu 20.04) 20230530 it works. ~5$/hour. storage should be 100gb.</p>

          <p>Here is hopefully complete steps from 0 to running Falcon-40 (it was
          tested on Falcon-40-instruct):<br>On AWS g5.12xlarge with 96Gb of VRAM and
          Deep Learning AMI GPU PyTorch 2.0.0 (Ubuntu 20.04) 20230530 it worked using
          following code:</p>

          <ol>

          <li>Assuming you know how to create new EC2 instance and set it up. Lets
          say you name your EC2 instance "GpuTest".</li>

          <li>Open port 8888 toward your EC instance (i.e. "GpuTest")(this is how
          you will access the Jupyter notebook).</li>

          <li>You need to get PEM key. Watch out for chmod and file privileges, it
          may complain. If complain, I think "chmod 400 your_key.pem" will fix it
          (group and others should be no access).</li>

          <li>ssh -i "name of the pem key" <a rel="nofollow" href="mailto:ubuntu@dns-address-of-your-ec2-instance.com">ubuntu@dns-address-of-your-ec2-instance.com</a>
          (please use the correct one).</li>

          <li>conda init</li>

          <li>"restart bash" or "ctrl-c" or "exit" and reenter instance (using step
          4.)</li>

          <li>conda activate pytorch</li>

          <li>jupyter notebook --no-browser</li>

          <li>open second terminal and execute: ssh -i "name of the pem key" -L localhost:8888:localhost:8888
          <a rel="nofollow" href="mailto:ubuntu@dns-address-of-your-ec2-instance.com">ubuntu@dns-address-of-your-ec2-instance.com</a>
          (this creates SSH tunnel from your computer to running instance). It will
          not work without this.</li>

          <li>At this point you should be able to go to your local browser and type:
          <a rel="nofollow" href="http://127.0.0.1:8888">http://127.0.0.1:8888</a>
          (or <a rel="nofollow" href="http://localhost:8888">http://localhost:8888</a>)
          and connect to jupyter (it will ask for token, this token should be visible
          as a result of step 8.) on "GpuTest" instance.</li>

          <li>Now comes the fun part:</li>

          <li>pip install transformers</li>

          <li>pip install einops</li>

          <li>pip install accelerate</li>

          <li>pip install xformers</li>

          </ol>

          <p>At this point provided sample worked perfectly.</p>

          <p>There will be a big business in AI cards with 100Gb on them.</p>

          '
        raw: 'On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch
          2.0.0 (Ubuntu 20.04) 20230530 it works. ~5$/hour. storage should be 100gb.


          Here is hopefully complete steps from 0 to running Falcon-40 (it was tested
          on Falcon-40-instruct):

          On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch 2.0.0
          (Ubuntu 20.04) 20230530 it worked using following code:

          1. Assuming you know how to create new EC2 instance and set it up. Lets
          say you name your EC2 instance "GpuTest".

          2. Open port 8888 toward your EC instance (i.e. "GpuTest")(this is how you
          will access the Jupyter notebook).

          3. You need to get PEM key. Watch out for chmod and file privileges, it
          may complain. If complain, I think "chmod 400 your_key.pem" will fix it
          (group and others should be no access).

          4. ssh -i "name of the pem key" ubuntu@dns-address-of-your-ec2-instance.com
          (please use the correct one).

          5. conda init

          6. "restart bash" or "ctrl-c" or "exit" and reenter instance (using step
          4.)

          7. conda activate pytorch

          8. jupyter notebook --no-browser

          9. open second terminal and execute: ssh -i "name of the pem key" -L localhost:8888:localhost:8888
          ubuntu@dns-address-of-your-ec2-instance.com (this creates SSH tunnel from
          your computer to running instance). It will not work without this.

          10. At this point you should be able to go to your local browser and type:
          http://127.0.0.1:8888 (or http://localhost:8888) and connect to jupyter
          (it will ask for token, this token should be visible as a result of step
          8.) on "GpuTest" instance.

          11. Now comes the fun part:

          12. pip install transformers

          13. pip install einops

          14. pip install accelerate

          15. pip install xformers


          At this point provided sample worked perfectly.


          There will be a big business in AI cards with 100Gb on them.'
        updatedAt: '2023-06-01T16:53:23.223Z'
      numEdits: 0
      reactions: []
    id: 6478cd0342b1805ae2abee16
    type: comment
  author: Sloba
  content: 'On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch
    2.0.0 (Ubuntu 20.04) 20230530 it works. ~5$/hour. storage should be 100gb.


    Here is hopefully complete steps from 0 to running Falcon-40 (it was tested on
    Falcon-40-instruct):

    On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch 2.0.0 (Ubuntu
    20.04) 20230530 it worked using following code:

    1. Assuming you know how to create new EC2 instance and set it up. Lets say you
    name your EC2 instance "GpuTest".

    2. Open port 8888 toward your EC instance (i.e. "GpuTest")(this is how you will
    access the Jupyter notebook).

    3. You need to get PEM key. Watch out for chmod and file privileges, it may complain.
    If complain, I think "chmod 400 your_key.pem" will fix it (group and others should
    be no access).

    4. ssh -i "name of the pem key" ubuntu@dns-address-of-your-ec2-instance.com (please
    use the correct one).

    5. conda init

    6. "restart bash" or "ctrl-c" or "exit" and reenter instance (using step 4.)

    7. conda activate pytorch

    8. jupyter notebook --no-browser

    9. open second terminal and execute: ssh -i "name of the pem key" -L localhost:8888:localhost:8888
    ubuntu@dns-address-of-your-ec2-instance.com (this creates SSH tunnel from your
    computer to running instance). It will not work without this.

    10. At this point you should be able to go to your local browser and type: http://127.0.0.1:8888
    (or http://localhost:8888) and connect to jupyter (it will ask for token, this
    token should be visible as a result of step 8.) on "GpuTest" instance.

    11. Now comes the fun part:

    12. pip install transformers

    13. pip install einops

    14. pip install accelerate

    15. pip install xformers


    At this point provided sample worked perfectly.


    There will be a big business in AI cards with 100Gb on them.'
  created_at: 2023-06-01 15:53:23+00:00
  edited: false
  hidden: false
  id: 6478cd0342b1805ae2abee16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-06-01T17:09:03.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sloba\">@<span class=\"\
          underline\">Sloba</span></a></span>\n\n\t</span></span> ,<br>I am not using\
          \ AWS, but M2 max chip MacBook. Do you have any recommendation for MacOS\
          \ systems that does not have NVIDIA, but has mps (CPU AND GPU as platform\
          \ computer)?<br>Thanks</p>\n"
        raw: 'Hi @Sloba ,

          I am not using AWS, but M2 max chip MacBook. Do you have any recommendation
          for MacOS systems that does not have NVIDIA, but has mps (CPU AND GPU as
          platform computer)?

          Thanks'
        updatedAt: '2023-06-01T17:09:03.396Z'
      numEdits: 0
      reactions: []
    id: 6478d0afdbf97e0b5cc461c6
    type: comment
  author: phdykd
  content: 'Hi @Sloba ,

    I am not using AWS, but M2 max chip MacBook. Do you have any recommendation for
    MacOS systems that does not have NVIDIA, but has mps (CPU AND GPU as platform
    computer)?

    Thanks'
  created_at: 2023-06-01 16:09:03+00:00
  edited: false
  hidden: false
  id: 6478d0afdbf97e0b5cc461c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-06-01T17:19:00.000Z'
    data:
      edited: false
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;phdykd&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/phdykd\">@<span class=\"\
          underline\">phdykd</span></a></span>\n\n\t</span></span> , unfortunately\
          \ no. I think quantization will be needed due to size.</p>\n"
        raw: Hi @phdykd , unfortunately no. I think quantization will be needed due
          to size.
        updatedAt: '2023-06-01T17:19:00.828Z'
      numEdits: 0
      reactions: []
    id: 6478d304c68a021fbba0fd58
    type: comment
  author: Sloba
  content: Hi @phdykd , unfortunately no. I think quantization will be needed due
    to size.
  created_at: 2023-06-01 16:19:00+00:00
  edited: false
  hidden: false
  id: 6478d304c68a021fbba0fd58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dbe3728ee7dd462b63891eab23c356a.svg
      fullname: Liang Yi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chnlyi
      type: user
    createdAt: '2023-06-02T02:02:54.000Z'
    data:
      edited: false
      editors:
      - chnlyi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dbe3728ee7dd462b63891eab23c356a.svg
          fullname: Liang Yi
          isHf: false
          isPro: false
          name: chnlyi
          type: user
        html: "<blockquote>\n<p>I can't run this on my machine because I don't have\
          \ the hardware, but I was able to get past the above errors by adjusting\
          \ the code as follows, specifically the model = line:</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\"\
          , trust_remote_code=True)\n\n\n\npipeline = transformers.pipeline(\n   \
          \ \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n   \
          \ torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n</blockquote>\n<p>I tried your method and the code worked.\
          \ However, same as you my hardware could not support (only 8gb GPU).<br>Here\
          \ is my modification:</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\n\
          \    \"tiiuae/falcon-40b-instruct\", \n    trust_remote_code=True,\n   \
          \ )\ntokenizer = AutoTokenizer.from_pretrained(model)\n</code></pre>\n"
        raw: "> I can't run this on my machine because I don't have the hardware,\
          \ but I was able to get past the above errors by adjusting the code as follows,\
          \ specifically the model = line:\n> ```\n> from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n> import transformers\n> import torch\n> \n> model\
          \ = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\", trust_remote_code=True)\n\
          > \n> \n> \n> pipeline = transformers.pipeline(\n>     \"text-generation\"\
          ,\n>     model=model,\n>     tokenizer=tokenizer,\n>     torch_dtype=torch.bfloat16,\n\
          >     trust_remote_code=True,\n>     device_map=\"auto\",\n> )\n> sequences\
          \ = pipeline(\n>    \"Girafatron is obsessed with giraffes, the most glorious\
          \ animal on the face of this Earth. Giraftron believes all other animals\
          \ are irrelevant when compared to the glorious majesty of the giraffe.\\\
          nDaniel: Hello, Girafatron!\\nGirafatron:\",\n>     max_length=200,\n> \
          \    do_sample=True,\n>     top_k=10,\n>     num_return_sequences=1,\n>\
          \     eos_token_id=tokenizer.eos_token_id,\n> )\n> for seq in sequences:\n\
          >     print(f\"Result: {seq['generated_text']}\")\n> ```\n\nI tried your\
          \ method and the code worked. However, same as you my hardware could not\
          \ support (only 8gb GPU).\nHere is my modification:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"tiiuae/falcon-40b-instruct\", \n    trust_remote_code=True,\n   \
          \ )\ntokenizer = AutoTokenizer.from_pretrained(model)\n```"
        updatedAt: '2023-06-02T02:02:54.762Z'
      numEdits: 0
      reactions: []
    id: 64794dceba447930a6fd15e8
    type: comment
  author: chnlyi
  content: "> I can't run this on my machine because I don't have the hardware, but\
    \ I was able to get past the above errors by adjusting the code as follows, specifically\
    \ the model = line:\n> ```\n> from transformers import AutoTokenizer, AutoModelForCausalLM\n\
    > import transformers\n> import torch\n> \n> model = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-40b\", trust_remote_code=True)\n> \n> \n> \n> pipeline = transformers.pipeline(\n\
    >     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n>\
    \     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n>     device_map=\"\
    auto\",\n> )\n> sequences = pipeline(\n>    \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\n>     max_length=200,\n>     do_sample=True,\n\
    >     top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
    > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
    )\n> ```\n\nI tried your method and the code worked. However, same as you my hardware\
    \ could not support (only 8gb GPU).\nHere is my modification:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"tiiuae/falcon-40b-instruct\", \n    trust_remote_code=True,\n    )\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\n```"
  created_at: 2023-06-02 01:02:54+00:00
  edited: false
  hidden: false
  id: 64794dceba447930a6fd15e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
      fullname: M. Lemoyne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mlemoyne
      type: user
    createdAt: '2023-06-04T01:50:41.000Z'
    data:
      edited: false
      editors:
      - Mlemoyne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7636299133300781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
          fullname: M. Lemoyne
          isHf: false
          isPro: false
          name: Mlemoyne
          type: user
        html: '<p>To those who have questions not related to this, please go start
          another discussion. Here it is to discuss ValueError: Could not load model
          tiiuae/falcon-40b with any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).
          Anyone knows how to solve this problem?</p>

          '
        raw: 'To those who have questions not related to this, please go start another
          discussion. Here it is to discuss ValueError: Could not load model tiiuae/falcon-40b
          with any of the following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).
          Anyone knows how to solve this problem?'
        updatedAt: '2023-06-04T01:50:41.899Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - mahmoudajawad
        - airtable
        - sehadnassim
        - arof
        - natewaddoups
        - lucas0
    id: 647bedf1e8b7333058b5dad4
    type: comment
  author: Mlemoyne
  content: 'To those who have questions not related to this, please go start another
    discussion. Here it is to discuss ValueError: Could not load model tiiuae/falcon-40b
    with any of the following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).
    Anyone knows how to solve this problem?'
  created_at: 2023-06-04 00:50:41+00:00
  edited: false
  hidden: false
  id: 647bedf1e8b7333058b5dad4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a40812f3c7b5e15383dd5591e0715f9f.svg
      fullname: Chris Hare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chrishare
      type: user
    createdAt: '2023-06-04T07:40:08.000Z'
    data:
      edited: false
      editors:
      - chrishare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8568916320800781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a40812f3c7b5e15383dd5591e0715f9f.svg
          fullname: Chris Hare
          isHf: false
          isPro: false
          name: chrishare
          type: user
        html: '<p>Newbie here, so take this with a grain of salt. In my case, I was
          running out of GPU memory. Remember, even ''small'' 7B models can require
          14Gb memory during inference without optimization (e.g. quantization) and
          16bit floats - so it''s real easy to max out even modern GPUs. To confirm
          that is the case for you, you want to track memory usage during training
          (or inference). There are a bunch of easy ways to do that and rule that
          in or out that depend on your setup:</p>

          <pre><code># Baseline GPU memory

          nvidia-smi

          nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

          # Watch GPU memory interactively

          watch nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

          nvidia-smi -l 1

          # Use HF trainer callbacks to integration with WanDB etc - https://huggingface.co/transformers/v3.4.0/main_classes/callback.html#available-callbacks

          # Use Colab''s Runtime &gt; Manage sessions UI to watch what the usage is,
          interactively

          </code></pre>

          <p>There could be other reasons to hit this error, I am sure - for instance,
          I tried 7B on a TPU and that seemed to fail as well. But experimenting with
          memory usage was how I got through this on a 16Gb GPU (Colab T4 Tesla) for
          a 7B model.</p>

          '
        raw: 'Newbie here, so take this with a grain of salt. In my case, I was running
          out of GPU memory. Remember, even ''small'' 7B models can require 14Gb memory
          during inference without optimization (e.g. quantization) and 16bit floats
          - so it''s real easy to max out even modern GPUs. To confirm that is the
          case for you, you want to track memory usage during training (or inference).
          There are a bunch of easy ways to do that and rule that in or out that depend
          on your setup:


          ```

          # Baseline GPU memory

          nvidia-smi

          nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

          # Watch GPU memory interactively

          watch nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

          nvidia-smi -l 1

          # Use HF trainer callbacks to integration with WanDB etc - https://huggingface.co/transformers/v3.4.0/main_classes/callback.html#available-callbacks

          # Use Colab''s Runtime > Manage sessions UI to watch what the usage is,
          interactively

          ```


          There could be other reasons to hit this error, I am sure - for instance,
          I tried 7B on a TPU and that seemed to fail as well. But experimenting with
          memory usage was how I got through this on a 16Gb GPU (Colab T4 Tesla) for
          a 7B model.'
        updatedAt: '2023-06-04T07:40:08.598Z'
      numEdits: 0
      reactions: []
    id: 647c3fd860dfe0f35d48fa9f
    type: comment
  author: chrishare
  content: 'Newbie here, so take this with a grain of salt. In my case, I was running
    out of GPU memory. Remember, even ''small'' 7B models can require 14Gb memory
    during inference without optimization (e.g. quantization) and 16bit floats - so
    it''s real easy to max out even modern GPUs. To confirm that is the case for you,
    you want to track memory usage during training (or inference). There are a bunch
    of easy ways to do that and rule that in or out that depend on your setup:


    ```

    # Baseline GPU memory

    nvidia-smi

    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

    # Watch GPU memory interactively

    watch nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu

    nvidia-smi -l 1

    # Use HF trainer callbacks to integration with WanDB etc - https://huggingface.co/transformers/v3.4.0/main_classes/callback.html#available-callbacks

    # Use Colab''s Runtime > Manage sessions UI to watch what the usage is, interactively

    ```


    There could be other reasons to hit this error, I am sure - for instance, I tried
    7B on a TPU and that seemed to fail as well. But experimenting with memory usage
    was how I got through this on a 16Gb GPU (Colab T4 Tesla) for a 7B model.'
  created_at: 2023-06-04 06:40:08+00:00
  edited: false
  hidden: false
  id: 647c3fd860dfe0f35d48fa9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efc82ec2e17837c0d0528332324a3dfb.svg
      fullname: Mahmoud Abduljawad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahmoudajawad
      type: user
    createdAt: '2023-06-05T07:41:35.000Z'
    data:
      edited: false
      editors:
      - mahmoudajawad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9863936901092529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efc82ec2e17837c0d0528332324a3dfb.svg
          fullname: Mahmoud Abduljawad
          isHf: false
          isPro: false
          name: mahmoudajawad
          type: user
        html: "<p>Agree <span data-props=\"{&quot;user&quot;:&quot;Mlemoyne&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Mlemoyne\"\
          >@<span class=\"underline\">Mlemoyne</span></a></span>\n\n\t</span></span>,\
          \ this topic has pivoted.<br>I'm having the same issue. I'm running this\
          \ on SageMaker but doesn't seem to work. I'll consider giving Lambda Labs\
          \ a shot later on to see whether this error is related or not.</p>\n"
        raw: "Agree @Mlemoyne, this topic has pivoted. \nI'm having the same issue.\
          \ I'm running this on SageMaker but doesn't seem to work. I'll consider\
          \ giving Lambda Labs a shot later on to see whether this error is related\
          \ or not."
        updatedAt: '2023-06-05T07:41:35.368Z'
      numEdits: 0
      reactions: []
    id: 647d91aff14eafc3b4452972
    type: comment
  author: mahmoudajawad
  content: "Agree @Mlemoyne, this topic has pivoted. \nI'm having the same issue.\
    \ I'm running this on SageMaker but doesn't seem to work. I'll consider giving\
    \ Lambda Labs a shot later on to see whether this error is related or not."
  created_at: 2023-06-05 06:41:35+00:00
  edited: false
  hidden: false
  id: 647d91aff14eafc3b4452972
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8fc721e0ffb2f369c4dde75d10120f5.svg
      fullname: Fran Abellan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cw-franabellan
      type: user
    createdAt: '2023-06-05T20:04:02.000Z'
    data:
      edited: false
      editors:
      - cw-franabellan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6031425595283508
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8fc721e0ffb2f369c4dde75d10120f5.svg
          fullname: Fran Abellan
          isHf: false
          isPro: false
          name: cw-franabellan
          type: user
        html: '<p>Just for the record, the same happens with model falcon-7b-instruct
          in Sagemaker:</p>

          <p>ValueError: Could not load model tiiuae/falcon-7b-instruct with any of
          the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          '
        raw: 'Just for the record, the same happens with model falcon-7b-instruct
          in Sagemaker:


          ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the
          following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
        updatedAt: '2023-06-05T20:04:02.777Z'
      numEdits: 0
      reactions: []
    id: 647e3fb210b7a3b1570feb44
    type: comment
  author: cw-franabellan
  content: 'Just for the record, the same happens with model falcon-7b-instruct in
    Sagemaker:


    ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following
    classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
  created_at: 2023-06-05 19:04:02+00:00
  edited: false
  hidden: false
  id: 647e3fb210b7a3b1570feb44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea4d139056694db8c76b7914fd2a6f94.svg
      fullname: Robin Platte
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nolyzlel
      type: user
    createdAt: '2023-06-07T10:13:50.000Z'
    data:
      edited: false
      editors:
      - Nolyzlel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6680048108100891
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea4d139056694db8c76b7914fd2a6f94.svg
          fullname: Robin Platte
          isHf: false
          isPro: false
          name: Nolyzlel
          type: user
        html: '<p>Can someone give a final solution to the error which is </p>

          <p>ValueError: Could not load model tiiuae/falcon-7b-instruct with any of
          the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          <p>Thanks :'''')</p>

          '
        raw: "Can someone give a final solution to the error which is \n\nValueError:\
          \ Could not load model tiiuae/falcon-7b-instruct with any of the following\
          \ classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          \nThanks :'')"
        updatedAt: '2023-06-07T10:13:50.755Z'
      numEdits: 0
      reactions: []
    id: 6480585ee1421e205fd4c4c9
    type: comment
  author: Nolyzlel
  content: "Can someone give a final solution to the error which is \n\nValueError:\
    \ Could not load model tiiuae/falcon-7b-instruct with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\n\
    Thanks :'')"
  created_at: 2023-06-07 09:13:50+00:00
  edited: false
  hidden: false
  id: 6480585ee1421e205fd4c4c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72f054a769299ca08fae49258913e27e.svg
      fullname: Jim Ahlstrand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 4639-94d6
      type: user
    createdAt: '2023-06-07T13:26:11.000Z'
    data:
      edited: false
      editors:
      - 4639-94d6
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720935821533203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72f054a769299ca08fae49258913e27e.svg
          fullname: Jim Ahlstrand
          isHf: false
          isPro: false
          name: 4639-94d6
          type: user
        html: '<p>I tried to run the 7B model on CPU only and got this message, removing
          device_map="auto" from the pipeline seemed to solve it.</p>

          '
        raw: I tried to run the 7B model on CPU only and got this message, removing
          device_map="auto" from the pipeline seemed to solve it.
        updatedAt: '2023-06-07T13:26:11.436Z'
      numEdits: 0
      reactions: []
    id: 64808573bb25a636c9db1511
    type: comment
  author: 4639-94d6
  content: I tried to run the 7B model on CPU only and got this message, removing
    device_map="auto" from the pipeline seemed to solve it.
  created_at: 2023-06-07 12:26:11+00:00
  edited: false
  hidden: false
  id: 64808573bb25a636c9db1511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59f3eeba7d9db5eef2dbc66764e38fd0.svg
      fullname: Sehad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sehadnassim
      type: user
    createdAt: '2023-06-07T14:12:04.000Z'
    data:
      edited: true
      editors:
      - sehadnassim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507995247840881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59f3eeba7d9db5eef2dbc66764e38fd0.svg
          fullname: Sehad
          isHf: false
          isPro: false
          name: sehadnassim
          type: user
        html: '<blockquote>

          <p>I tried to run the 7B model on CPU only and got this message, removing
          device_map="auto" from the pipeline seemed to solve it.<br>This removes
          the error but the process gets killed after a while. Do you know what are
          the minimum CPU requirements ?</p>

          </blockquote>

          '
        raw: '> I tried to run the 7B model on CPU only and got this message, removing
          device_map="auto" from the pipeline seemed to solve it.

          This removes the error but the process gets killed after a while. Do you
          know what are the minimum CPU requirements ?'
        updatedAt: '2023-06-07T14:12:30.355Z'
      numEdits: 2
      reactions: []
    id: 64809034e1421e205fd8f047
    type: comment
  author: sehadnassim
  content: '> I tried to run the 7B model on CPU only and got this message, removing
    device_map="auto" from the pipeline seemed to solve it.

    This removes the error but the process gets killed after a while. Do you know
    what are the minimum CPU requirements ?'
  created_at: 2023-06-07 13:12:04+00:00
  edited: true
  hidden: false
  id: 64809034e1421e205fd8f047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d0b01cf7b55e2abbd963b4b8614e573.svg
      fullname: Lawrence Algocat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lawrence-algocat
      type: user
    createdAt: '2023-06-07T16:18:07.000Z'
    data:
      edited: false
      editors:
      - lawrence-algocat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9310075640678406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d0b01cf7b55e2abbd963b4b8614e573.svg
          fullname: Lawrence Algocat
          isHf: false
          isPro: false
          name: lawrence-algocat
          type: user
        html: '<p>removing device_map="auto" worked for me, the model loads on CPU.<br>still
          though, my colab VM crashes because of lack of RAM...</p>

          '
        raw: 'removing device_map="auto" worked for me, the model loads on CPU.

          still though, my colab VM crashes because of lack of RAM...'
        updatedAt: '2023-06-07T16:18:07.443Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nielsole
    id: 6480adbfbb25a636c9de26b0
    type: comment
  author: lawrence-algocat
  content: 'removing device_map="auto" worked for me, the model loads on CPU.

    still though, my colab VM crashes because of lack of RAM...'
  created_at: 2023-06-07 15:18:07+00:00
  edited: false
  hidden: false
  id: 6480adbfbb25a636c9de26b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72f054a769299ca08fae49258913e27e.svg
      fullname: Jim Ahlstrand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 4639-94d6
      type: user
    createdAt: '2023-06-08T07:45:39.000Z'
    data:
      edited: true
      editors:
      - 4639-94d6
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575435519218445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72f054a769299ca08fae49258913e27e.svg
          fullname: Jim Ahlstrand
          isHf: false
          isPro: false
          name: 4639-94d6
          type: user
        html: '<p>You are correct, I was a bit too quick with my answer, I do not
          recommend removing device_map due to the amount of ram it requires (it crashed
          for me as well after 20Gb allocated ram). Instead i found a solution by
          simply upgrading torch to 2.0.1, xformers to 0.0.20 and accelerate to 0.20.1<br>Now
          it runs fine for me with device_map=auto</p>

          '
        raw: 'You are correct, I was a bit too quick with my answer, I do not recommend
          removing device_map due to the amount of ram it requires (it crashed for
          me as well after 20Gb allocated ram). Instead i found a solution by simply
          upgrading torch to 2.0.1, xformers to 0.0.20 and accelerate to 0.20.1

          Now it runs fine for me with device_map=auto'
        updatedAt: '2023-06-08T08:31:45.096Z'
      numEdits: 1
      reactions: []
    id: 648187236f283a7468603c0d
    type: comment
  author: 4639-94d6
  content: 'You are correct, I was a bit too quick with my answer, I do not recommend
    removing device_map due to the amount of ram it requires (it crashed for me as
    well after 20Gb allocated ram). Instead i found a solution by simply upgrading
    torch to 2.0.1, xformers to 0.0.20 and accelerate to 0.20.1

    Now it runs fine for me with device_map=auto'
  created_at: 2023-06-08 06:45:39+00:00
  edited: true
  hidden: false
  id: 648187236f283a7468603c0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2f05d82fcbaf897f7f4247a89e88ce27.svg
      fullname: Carlos Mario Toro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosmtoro
      type: user
    createdAt: '2023-06-08T20:56:59.000Z'
    data:
      edited: false
      editors:
      - carlosmtoro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5385844111442566
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2f05d82fcbaf897f7f4247a89e88ce27.svg
          fullname: Carlos Mario Toro
          isHf: false
          isPro: false
          name: carlosmtoro
          type: user
        html: "<p>Hello, I have a computer with 64 GB of ram, two Xeon processors\
          \ with 32 cores each. I did NOT enable graphics card (it is very small),\
          \ I downloaded the falcon-40b model files and installed in Dockerfile:</p>\n\
          <p>ENV TRANSFORMERS_CACHE=/app/transformercache</p>\n<p>RUN mkdir -p $TRANSFORMERS_CACHE</p>\n\
          <p>RUN pip3 install transformers</p>\n<p>RUN pip3 install --pre torch --index-url\
          \ <a rel=\"nofollow\" href=\"https://download.pytorch.org/whl/nightly/cpu\"\
          >https://download.pytorch.org/whl/nightly/cpu</a></p>\n<p>RUN pip3 install\
          \ einops<br>RUN pip3 install accelerate<br>RUN pip3 install xformers</p>\n\
          <p>When trying to test the script:</p>\n<p>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM<br>import transformers<br>import torch</p>\n<h1 id=\"\
          this-instruction-loads-the-pre-trained-model\">This instruction loads the\
          \ pre-trained model</h1>\n<p>model=\"./model/falcon-40b\"</p>\n<p>rmodel\
          \ = AutoModelForCausalLM.from_pretrained(<br>         models,<br>      \
          \   offload_folder='/app/model/falcon-40b',<br>         trust_remote_code=True,<br>\
          \         device_map='auto',<br>         torch_dtype=torch.float16,<br>\
          \         low_cpu_mem_usage=True,<br>         chunk_size_feed_forward=512000,<br>\
          \         cache_dir = \"./transformercache\"<br>)</p>\n<p>Just exit python:</p>\n\
          <blockquote>\n<blockquote>\n<blockquote>\n<p>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM<br>import transformers<br>import torch<br>model=\"\
          ./model/falcon-40b\"<br>rmodel = AutoModelForCausalLM.from_pretrained(<br>...\
          \ models,<br>...offload_folder='/app/model/falcon-40b',<br>...trust_remote_code=True,<br>...\
          \ device_map='auto',<br>...torch_dtype=torch.float16,<br>... low_cpu_mem_usage=True,<br>...\
          \ chunk_size_feed_forward=512000,<br>... cache_dir = \"./transformercache\"\
          <br>... )<br>Loading checkpoint shards: 67%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 | 6/9 [01:04&lt;00:32,\
          \ 10.75s/it] Killed<br>root@75b049882bd4:/app#</p>\n</blockquote>\n</blockquote>\n\
          </blockquote>\n<p>I guess it's because of memory, does anyone have any idea\
          \ what I can do?</p>\n"
        raw: "Hello, I have a computer with 64 GB of ram, two Xeon processors with\
          \ 32 cores each. I did NOT enable graphics card (it is very small), I downloaded\
          \ the falcon-40b model files and installed in Dockerfile:\n\nENV TRANSFORMERS_CACHE=/app/transformercache\n\
          \nRUN mkdir -p $TRANSFORMERS_CACHE\n\nRUN pip3 install transformers\n\n\
          RUN pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu\n\
          \nRUN pip3 install einops\nRUN pip3 install accelerate\nRUN pip3 install\
          \ xformers\n\nWhen trying to test the script:\n\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\
          \n# This instruction loads the pre-trained model\nmodel=\"./model/falcon-40b\"\
          \n\nrmodel = AutoModelForCausalLM.from_pretrained(\n         models,\n \
          \        offload_folder='/app/model/falcon-40b',\n         trust_remote_code=True,\n\
          \         device_map='auto',\n         torch_dtype=torch.float16,\n    \
          \     low_cpu_mem_usage=True,\n         chunk_size_feed_forward=512000,\n\
          \         cache_dir = \"./transformercache\"\n)\n\nJust exit python:\n\n\
          >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n>>> import\
          \ transformers\n>>> import torch\n>>> model=\"./model/falcon-40b\"\n>>>\
          \ rmodel = AutoModelForCausalLM.from_pretrained(\n... models,\n...offload_folder='/app/model/falcon-40b',\n\
          ...trust_remote_code=True,\n... device_map='auto',\n...torch_dtype=torch.float16,\n\
          ... low_cpu_mem_usage=True,\n... chunk_size_feed_forward=512000,\n... cache_dir\
          \ = \"./transformercache\"\n... )\nLoading checkpoint shards: 67%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\
          \u2588\u2588 | 6/9 [01:04<00:32, 10.75s/it] Killed\nroot@75b049882bd4:/app#\n\
          \nI guess it's because of memory, does anyone have any idea what I can do?"
        updatedAt: '2023-06-08T20:56:59.226Z'
      numEdits: 0
      reactions: []
    id: 6482409b10cd9ffea8a25d6e
    type: comment
  author: carlosmtoro
  content: "Hello, I have a computer with 64 GB of ram, two Xeon processors with 32\
    \ cores each. I did NOT enable graphics card (it is very small), I downloaded\
    \ the falcon-40b model files and installed in Dockerfile:\n\nENV TRANSFORMERS_CACHE=/app/transformercache\n\
    \nRUN mkdir -p $TRANSFORMERS_CACHE\n\nRUN pip3 install transformers\n\nRUN pip3\
    \ install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu\n\
    \nRUN pip3 install einops\nRUN pip3 install accelerate\nRUN pip3 install xformers\n\
    \nWhen trying to test the script:\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import transformers\nimport torch\n\n# This instruction loads the pre-trained\
    \ model\nmodel=\"./model/falcon-40b\"\n\nrmodel = AutoModelForCausalLM.from_pretrained(\n\
    \         models,\n         offload_folder='/app/model/falcon-40b',\n        \
    \ trust_remote_code=True,\n         device_map='auto',\n         torch_dtype=torch.float16,\n\
    \         low_cpu_mem_usage=True,\n         chunk_size_feed_forward=512000,\n\
    \         cache_dir = \"./transformercache\"\n)\n\nJust exit python:\n\n>>> from\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\n>>> import transformers\n\
    >>> import torch\n>>> model=\"./model/falcon-40b\"\n>>> rmodel = AutoModelForCausalLM.from_pretrained(\n\
    ... models,\n...offload_folder='/app/model/falcon-40b',\n...trust_remote_code=True,\n\
    ... device_map='auto',\n...torch_dtype=torch.float16,\n... low_cpu_mem_usage=True,\n\
    ... chunk_size_feed_forward=512000,\n... cache_dir = \"./transformercache\"\n\
    ... )\nLoading checkpoint shards: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\
    \u2588\u2588 | 6/9 [01:04<00:32, 10.75s/it] Killed\nroot@75b049882bd4:/app#\n\n\
    I guess it's because of memory, does anyone have any idea what I can do?"
  created_at: 2023-06-08 19:56:59+00:00
  edited: false
  hidden: false
  id: 6482409b10cd9ffea8a25d6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:23:16.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9204760193824768
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>We recommend having at least 80-100GB to fit the 40B model comfortably.
          </p>

          <p>If you do not have that much memory available, you can have a look at
          <a rel="nofollow" href="https://github.com/rmihaylov/falcontune">FalconTune</a>
          to run the model in 4-bit, or at this <a href="https://huggingface.co/blog/falcon">blogpost</a>
          from HuggingFace.</p>

          '
        raw: "We recommend having at least 80-100GB to fit the 40B model comfortably.\
          \ \n\nIf you do not have that much memory available, you can have a look\
          \ at [FalconTune](https://github.com/rmihaylov/falcontune) to run the model\
          \ in 4-bit, or at this [blogpost](https://huggingface.co/blog/falcon) from\
          \ HuggingFace."
        updatedAt: '2023-06-09T14:23:16.389Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - carlosmtoro
        - alephpt
      - count: 2
        reaction: "\U0001F44D"
        users:
        - carlosmtoro
        - alephpt
    id: 648335d4823496a7c311c7a8
    type: comment
  author: FalconLLM
  content: "We recommend having at least 80-100GB to fit the 40B model comfortably.\
    \ \n\nIf you do not have that much memory available, you can have a look at [FalconTune](https://github.com/rmihaylov/falcontune)\
    \ to run the model in 4-bit, or at this [blogpost](https://huggingface.co/blog/falcon)\
    \ from HuggingFace."
  created_at: 2023-06-09 13:23:16+00:00
  edited: false
  hidden: false
  id: 648335d4823496a7c311c7a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59f3eeba7d9db5eef2dbc66764e38fd0.svg
      fullname: Sehad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sehadnassim
      type: user
    createdAt: '2023-06-12T20:41:05.000Z'
    data:
      edited: false
      editors:
      - sehadnassim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16116133332252502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59f3eeba7d9db5eef2dbc66764e38fd0.svg
          fullname: Sehad
          isHf: false
          isPro: false
          name: sehadnassim
          type: user
        html: "<p>still getting this error for 7b : \u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256E<br>\u2502                                     \
          \                                                             \u2502<br>\u2502\
          \ /home/sehadn1/falcon7b-modif.py:14 in                                \
          \                    \u2502<br>\u2502                                  \
          \                                                                \u2502\
          <br>\u2502   11 model = \"tiiuae/falcon-7b-instruct\"                  \
          \                                       \u2502<br>\u2502   12          \
          \                                                                      \
          \             \u2502<br>\u2502   13 tokenizer = AutoTokenizer.from_pretrained(model)\
          \                                            \u2502<br>\u2502 \u2771 14\
          \ pipeline = transformers.pipeline(                                    \
          \                       \u2502<br>\u2502   15 \u2502   \"text-generation\"\
          ,                                                                      \u2502\
          <br>\u2502   16 \u2502   model=model,                                  \
          \                                          \u2502<br>\u2502   17 \u2502\
          \   tokenizer=tokenizer,                                               \
          \                     \u2502<br>\u2502 /home/sehadn1/transformers/src/transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline                \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502   785 \u2502   # Forced if framework already defined,\
          \ inferred if it's None                           \u2502<br>\u2502   786\
          \ \u2502   # Will load the correct model if possible                   \
          \                           \u2502<br>\u2502   787 \u2502   model_classes\
          \ = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}     \
          \            \u2502<br>\u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502<br>\u2502   789 \u2502\
          \   \u2502   model,                                                    \
          \                         \u2502<br>\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /home/sehadn1/transformers/src/transformers/pipelines/base.py:278\
          \ in infer_framework_load_model  \u2502<br>\u2502                      \
          \                                                                      \
          \      \u2502<br>\u2502    275 \u2502   \u2502   \u2502   \u2502   continue\
          \                                                                  \u2502\
          <br>\u2502    276 \u2502   \u2502                                      \
          \                                               \u2502<br>\u2502    277\
          \ \u2502   \u2502   if isinstance(model, str):                         \
          \                               \u2502<br>\u2502 \u2771  278 \u2502   \u2502\
          \   \u2502   raise ValueError(f\"Could not load model {model} with any of\
          \ the following cl  \u2502<br>\u2502    279 \u2502                     \
          \                                                                    \u2502\
          <br>\u2502    280 \u2502   framework = infer_framework(model.<strong>class</strong>)\
          \                                          \u2502<br>\u2502    281 \u2502\
          \   return framework, model                                            \
          \                   \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>ValueError: Could\
          \ not load model tiiuae/falcon-7b-instruct with any of the following classes:\
          \ (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</p>\n"
        raw: "still getting this error for 7b : \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502                                             \
          \                                                     \u2502\n\u2502 /home/sehadn1/falcon7b-modif.py:14\
          \ in <module>                                                   \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502   11 model = \"tiiuae/falcon-7b-instruct\"\
          \                                                         \u2502\n\u2502\
          \   12                                                                 \
          \                            \u2502\n\u2502   13 tokenizer = AutoTokenizer.from_pretrained(model)\
          \                                            \u2502\n\u2502 \u2771 14 pipeline\
          \ = transformers.pipeline(                                             \
          \              \u2502\n\u2502   15 \u2502   \"text-generation\",       \
          \                                                               \u2502\n\
          \u2502   16 \u2502   model=model,                                      \
          \                                      \u2502\n\u2502   17 \u2502   tokenizer=tokenizer,\
          \                                                                    \u2502\
          \n\u2502 /home/sehadn1/transformers/src/transformers/pipelines/__init__.py:788\
          \ in pipeline                \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   785 \u2502   # Forced if framework already defined, inferred\
          \ if it's None                           \u2502\n\u2502   786 \u2502   #\
          \ Will load the correct model if possible                              \
          \                \u2502\n\u2502   787 \u2502   model_classes = {\"tf\":\
          \ targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}                \
          \ \u2502\n\u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502\n\u2502   789 \u2502  \
          \ \u2502   model,                                                      \
          \                       \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n\u2502 \
          \  791 \u2502   \u2502   config=config,                                \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 /home/sehadn1/transformers/src/transformers/pipelines/base.py:278\
          \ in infer_framework_load_model  \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502    275 \u2502   \u2502   \u2502   \u2502   continue\
          \                                                                  \u2502\
          \n\u2502    276 \u2502   \u2502                                        \
          \                                             \u2502\n\u2502    277 \u2502\
          \   \u2502   if isinstance(model, str):                                \
          \                        \u2502\n\u2502 \u2771  278 \u2502   \u2502   \u2502\
          \   raise ValueError(f\"Could not load model {model} with any of the following\
          \ cl  \u2502\n\u2502    279 \u2502                                     \
          \                                                    \u2502\n\u2502    280\
          \ \u2502   framework = infer_framework(model.__class__)                \
          \                          \u2502\n\u2502    281 \u2502   return framework,\
          \ model                                                               \u2502\
          \n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\nValueError: Could not load model tiiuae/falcon-7b-instruct\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
        updatedAt: '2023-06-12T20:41:05.887Z'
      numEdits: 0
      reactions: []
    id: 648782e15da1a27689d9842c
    type: comment
  author: sehadnassim
  content: "still getting this error for 7b : \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
    \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502   \
    \                                                                            \
    \                   \u2502\n\u2502 /home/sehadn1/falcon7b-modif.py:14 in <module>\
    \                                                   \u2502\n\u2502           \
    \                                                                            \
    \           \u2502\n\u2502   11 model = \"tiiuae/falcon-7b-instruct\"        \
    \                                                 \u2502\n\u2502   12        \
    \                                                                            \
    \         \u2502\n\u2502   13 tokenizer = AutoTokenizer.from_pretrained(model)\
    \                                            \u2502\n\u2502 \u2771 14 pipeline\
    \ = transformers.pipeline(                                                   \
    \        \u2502\n\u2502   15 \u2502   \"text-generation\",                   \
    \                                                   \u2502\n\u2502   16 \u2502\
    \   model=model,                                                             \
    \               \u2502\n\u2502   17 \u2502   tokenizer=tokenizer,            \
    \                                                        \u2502\n\u2502 /home/sehadn1/transformers/src/transformers/pipelines/__init__.py:788\
    \ in pipeline                \u2502\n\u2502                                  \
    \                                                                \u2502\n\u2502\
    \   785 \u2502   # Forced if framework already defined, inferred if it's None\
    \                           \u2502\n\u2502   786 \u2502   # Will load the correct\
    \ model if possible                                              \u2502\n\u2502\
    \   787 \u2502   model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
    pt\"]}                 \u2502\n\u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
    \                                         \u2502\n\u2502   789 \u2502   \u2502\
    \   model,                                                                   \
    \          \u2502\n\u2502   790 \u2502   \u2502   model_classes=model_classes,\
    \                                                       \u2502\n\u2502   791 \u2502\
    \   \u2502   config=config,                                                  \
    \                   \u2502\n\u2502                                           \
    \                                                       \u2502\n\u2502 /home/sehadn1/transformers/src/transformers/pipelines/base.py:278\
    \ in infer_framework_load_model  \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \    275 \u2502   \u2502   \u2502   \u2502   continue                        \
    \                                          \u2502\n\u2502    276 \u2502   \u2502\
    \                                                                            \
    \         \u2502\n\u2502    277 \u2502   \u2502   if isinstance(model, str): \
    \                                                       \u2502\n\u2502 \u2771\
    \  278 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load model {model}\
    \ with any of the following cl  \u2502\n\u2502    279 \u2502                 \
    \                                                                        \u2502\
    \n\u2502    280 \u2502   framework = infer_framework(model.__class__)        \
    \                                  \u2502\n\u2502    281 \u2502   return framework,\
    \ model                                                               \u2502\n\
    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nValueError: Could not\
    \ load model tiiuae/falcon-7b-instruct with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class \n'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
  created_at: 2023-06-12 19:41:05+00:00
  edited: false
  hidden: false
  id: 648782e15da1a27689d9842c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
      fullname: yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucky
      type: user
    createdAt: '2023-06-14T03:07:24.000Z'
    data:
      edited: false
      editors:
      - lucky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9359268546104431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
          fullname: yu
          isHf: false
          isPro: false
          name: lucky
          type: user
        html: '<blockquote>

          <p>Possibly related, I get <code>The model ''RWForCausalLM'' is not supported
          for text-generation</code></p>

          <p>I do see that this warning pops up on 7b, which goes on to work fine,
          so might be a misleading warning here, just thought I''d share it.</p>

          </blockquote>

          '
        raw: "> Possibly related, I get `The model 'RWForCausalLM' is not supported\
          \ for text-generation`\n> \n> I do see that this warning pops up on 7b,\
          \ which goes on to work fine, so might be a misleading warning here, just\
          \ thought I'd share it.\n\n"
        updatedAt: '2023-06-14T03:07:24.270Z'
      numEdits: 0
      reactions: []
    id: 64892eec35f2eb53106be276
    type: comment
  author: lucky
  content: "> Possibly related, I get `The model 'RWForCausalLM' is not supported\
    \ for text-generation`\n> \n> I do see that this warning pops up on 7b, which\
    \ goes on to work fine, so might be a misleading warning here, just thought I'd\
    \ share it.\n\n"
  created_at: 2023-06-14 02:07:24+00:00
  edited: false
  hidden: false
  id: 64892eec35f2eb53106be276
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
      fullname: yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucky
      type: user
    createdAt: '2023-06-14T03:08:07.000Z'
    data:
      edited: false
      editors:
      - lucky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6782655715942383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
          fullname: yu
          isHf: false
          isPro: false
          name: lucky
          type: user
        html: '<p>pip install transformers<br>pip install einops<br>pip install accelerate<br>pip
          install xformers</p>

          <p>if you pip this package , it maybe ok</p>

          '
        raw: 'pip install transformers

          pip install einops

          pip install accelerate

          pip install xformers


          if you pip this package , it maybe ok'
        updatedAt: '2023-06-14T03:08:07.590Z'
      numEdits: 0
      reactions: []
    id: 64892f1783c649c931e2df1f
    type: comment
  author: lucky
  content: 'pip install transformers

    pip install einops

    pip install accelerate

    pip install xformers


    if you pip this package , it maybe ok'
  created_at: 2023-06-14 02:08:07+00:00
  edited: false
  hidden: false
  id: 64892f1783c649c931e2df1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
      fullname: yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucky
      type: user
    createdAt: '2023-06-14T03:10:48.000Z'
    data:
      edited: false
      editors:
      - lucky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5651922821998596
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/753ccd16db987543ff0a7b4e41fa7b46.svg
          fullname: yu
          isHf: false
          isPro: false
          name: lucky
          type: user
        html: '<p>pip install transformers<br>pip install einops<br>pip install accelerate<br>pip
          install xformers</p>

          <p>if you pip this package , it maybe ok , the problem of  " ValueError:
          Could not load model tiiuae/falcon-7b-instruct with any of the following
          classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,
          &lt;class<br>''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''&gt;"    maybe
          solved</p>

          '
        raw: 'pip install transformers

          pip install einops

          pip install accelerate

          pip install xformers


          if you pip this package , it maybe ok , the problem of  " ValueError: Could
          not load model tiiuae/falcon-7b-instruct with any of the following classes:
          (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,
          <class

          ''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''>"    maybe
          solved

          '
        updatedAt: '2023-06-14T03:10:48.182Z'
      numEdits: 0
      reactions: []
    id: 64892fb87bef7ca8046e6233
    type: comment
  author: lucky
  content: 'pip install transformers

    pip install einops

    pip install accelerate

    pip install xformers


    if you pip this package , it maybe ok , the problem of  " ValueError: Could not
    load model tiiuae/falcon-7b-instruct with any of the following classes: (<class
    ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>, <class

    ''transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM''>"    maybe
    solved

    '
  created_at: 2023-06-14 02:10:48+00:00
  edited: false
  hidden: false
  id: 64892fb87bef7ca8046e6233
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97ff33715cd753679df5e97719c3dd13.svg
      fullname: Tomas Koctur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tommykoctur
      type: user
    createdAt: '2023-06-14T14:07:20.000Z'
    data:
      edited: false
      editors:
      - tommykoctur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5792080760002136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97ff33715cd753679df5e97719c3dd13.svg
          fullname: Tomas Koctur
          isHf: false
          isPro: false
          name: tommykoctur
          type: user
        html: '<p>I am loading model to A6000 GPU with 48GB ram. with torch.int8 .
          I am getting the same error.:<br>ValueError: Could not load model tiiuae/falcon-40b-instruct
          with any of the following classes: (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          '
        raw: 'I am loading model to A6000 GPU with 48GB ram. with torch.int8 . I am
          getting the same error.:

          ValueError: Could not load model tiiuae/falcon-40b-instruct with any of
          the following classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
        updatedAt: '2023-06-14T14:07:20.110Z'
      numEdits: 0
      reactions: []
    id: 6489c9984344b9f97dac83f7
    type: comment
  author: tommykoctur
  content: 'I am loading model to A6000 GPU with 48GB ram. with torch.int8 . I am
    getting the same error.:

    ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following
    classes: (<class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''>,).'
  created_at: 2023-06-14 13:07:20+00:00
  edited: false
  hidden: false
  id: 6489c9984344b9f97dac83f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-06-14T19:42:22.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6262856125831604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>Kernel Restarting<br>The kernel for Desktop/LLM/Falcon/Fl.ipynb
          appears to have died. It will restart automatically.<br>it does not work
          on M2 Apple MacBook.</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import
          transformers<br>import torch</p>

          <h1 id="model--tiiuaefalcon-40b">model = "tiiuae/falcon-40b"</h1>

          <p>model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b", trust_remote_code=True)</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    #
          torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    # device_map="auto",<br>)<br>sequences
          = pipeline(<br>   "Girafatron is obsessed with giraffes, the most glorious
          animal on the face of this Earth. Giraftron believes all other animals are
          irrelevant when compared to the glorious majesty of the giraffe.\nDaniel:
          Hello, Girafatron!\nGirafatron:",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for
          seq in sequences:<br>    print(f"Result: {seq[''generated_text'']}")</p>

          '
        raw: "Kernel Restarting\nThe kernel for Desktop/LLM/Falcon/Fl.ipynb appears\
          \ to have died. It will restart automatically.\nit does not work on M2 Apple\
          \ MacBook.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\n# model = \"tiiuae/falcon-40b\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\", trust_remote_code=True)\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    # torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    # device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )"
        updatedAt: '2023-06-14T19:42:22.566Z'
      numEdits: 0
      reactions: []
    id: 648a181e304a0e50d2a5df4b
    type: comment
  author: phdykd
  content: "Kernel Restarting\nThe kernel for Desktop/LLM/Falcon/Fl.ipynb appears\
    \ to have died. It will restart automatically.\nit does not work on M2 Apple MacBook.\n\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\n\n# model = \"tiiuae/falcon-40b\"\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-40b\", trust_remote_code=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    # torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    # device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed\
    \ with giraffes, the most glorious animal on the face of this Earth. Giraftron\
    \ believes all other animals are irrelevant when compared to the glorious majesty\
    \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
    \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")"
  created_at: 2023-06-14 18:42:22+00:00
  edited: false
  hidden: false
  id: 648a181e304a0e50d2a5df4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1edcd0edf763e5dd977a0e802232c013.svg
      fullname: Patti Jorgensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patti-j
      type: user
    createdAt: '2023-06-18T22:44:03.000Z'
    data:
      edited: false
      editors:
      - patti-j
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5744695663452148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1edcd0edf763e5dd977a0e802232c013.svg
          fullname: Patti Jorgensen
          isHf: false
          isPro: false
          name: patti-j
          type: user
        html: "<p>I was able to get past the AutoModelForCausalLM error in falcon-7b-instruct\
          \ by using the line <span data-props=\"{&quot;user&quot;:&quot;alexwall77&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alexwall77\"\
          >@<span class=\"underline\">alexwall77</span></a></span>\n\n\t</span></span>\
          \ provided below: </p>\n<p>model = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b\", trust_remote_code=True)</p>\n<p>Thank you, Alex!</p>\n\
          <blockquote>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b\", trust_remote_code=True)\n\n\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n</blockquote>\n"
        raw: "I was able to get past the AutoModelForCausalLM error in falcon-7b-instruct\
          \ by using the line @alexwall77 provided below: \n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b\", trust_remote_code=True)\n\nThank you, Alex!\n\n> ```\n\
          > from transformers import AutoTokenizer, AutoModelForCausalLM\n> import\
          \ transformers\n> import torch\n> \n> model = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b\", trust_remote_code=True)\n> \n> \n> \n> pipeline = transformers.pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n\
          >     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n>    \
          \ device_map=\"auto\",\n> )\n> sequences = pipeline(\n>    \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\n>     max_length=200,\n>     do_sample=True,\n>     top_k=10,\n\
          >     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
          > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
          )\n> ```\n\n"
        updatedAt: '2023-06-18T22:44:03.889Z'
      numEdits: 0
      reactions: []
    id: 648f88b376e8bad260c504cb
    type: comment
  author: patti-j
  content: "I was able to get past the AutoModelForCausalLM error in falcon-7b-instruct\
    \ by using the line @alexwall77 provided below: \n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-40b\", trust_remote_code=True)\n\nThank you, Alex!\n\n> ```\n> from\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\n> import transformers\n\
    > import torch\n> \n> model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\"\
    , trust_remote_code=True)\n> \n> \n> \n> pipeline = transformers.pipeline(\n>\
    \     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n>\
    \     torch_dtype=torch.bfloat16,\n>     trust_remote_code=True,\n>     device_map=\"\
    auto\",\n> )\n> sequences = pipeline(\n>    \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\n>     max_length=200,\n>     do_sample=True,\n\
    >     top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
    > )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
    )\n> ```\n\n"
  created_at: 2023-06-18 21:44:03+00:00
  edited: false
  hidden: false
  id: 648f88b376e8bad260c504cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa85c93760f96e64a17a9ede2f75ebff.svg
      fullname: Li Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andcircle
      type: user
    createdAt: '2023-06-19T20:02:13.000Z'
    data:
      edited: false
      editors:
      - Andcircle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4765270948410034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa85c93760f96e64a17a9ede2f75ebff.svg
          fullname: Li Zhang
          isHf: false
          isPro: false
          name: Andcircle
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>Whe using this code:</p>\n\
          <h1 id=\"httpshuggingfacecotiiuaefalcon-40b\"><a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a></h1>\n<p>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM<br>import transformers<br>import\
          \ torch</p>\n<p>model = \"tiiuae/falcon-40b\" # Gebruik evt het kleinere\
          \ broertje: tiiuae/falcon-7b</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline\
          \ = transformers.pipeline(<br>    \"text-generation\",<br>    model=model,<br>\
          \    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>\
          \    device_map=\"auto\",<br>)<br>sequences = pipeline(<br>   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>\
          \    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for\
          \ seq in sequences:<br>    print(f\"Result: {seq['generated_text']}\")</p>\n\
          <p>I get this output:</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%<br>175/175 [00:00&lt;00:00, 6.61kB/s]<br>Downloading (\u2026)/main/tokenizer.json:\
          \ 100%<br>2.73M/2.73M [00:00&lt;00:00, 5.61MB/s]<br>Downloading (\u2026\
          )cial_tokens_map.json: 100%<br>281/281 [00:00&lt;00:00, 1.34kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>656/656 [00:00&lt;00:00, 947B/s]<br>Downloading\
          \ (\u2026)/configuration_RW.py: 100%<br>2.51k/2.51k [00:00&lt;00:00, 3.46kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</p>\n<ul>\n<li>configuration_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)main/modelling_RW.py: 100%<br>47.1k/47.1k [00:00&lt;00:00, 108kB/s]<br>A\
          \ new version of the following files was downloaded from <a href=\"https://huggingface.co/tiiuae/falcon-40b\"\
          >https://huggingface.co/tiiuae/falcon-40b</a>:</li>\n<li>modelling_RW.py<br>.\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.<br>Downloading\
          \ (\u2026)model.bin.index.json: 100%<br>39.3k/39.3k [00:00&lt;00:00, 697kB/s]<br>Downloading\
          \ shards: 67%<br>6/9 [05:54&lt;02:52, 57.54s/it]<br>Downloading (\u2026\
          )l-00001-of-00009.bin: 100%<br>9.50G/9.50G [00:46&lt;00:00, 258MB/s]<br>Downloading\
          \ (\u2026)l-00002-of-00009.bin: 100%<br>9.51G/9.51G [01:14&lt;00:00, 257MB/s]<br>Downloading\
          \ (\u2026)l-00003-of-00009.bin: 100%<br>9.51G/9.51G [00:50&lt;00:00, 262MB/s]<br>Downloading\
          \ (\u2026)l-00004-of-00009.bin: 100%<br>9.51G/9.51G [00:55&lt;00:00, 246MB/s]<br>Downloading\
          \ (\u2026)l-00005-of-00009.bin: 100%<br>9.51G/9.51G [00:57&lt;00:00, 224MB/s]<br>Downloading\
          \ (\u2026)l-00006-of-00009.bin: 100%<br>9.51G/9.51G [00:58&lt;00:00, 170MB/s]<br>Downloading\
          \ (\u2026)l-00007-of-00009.bin: 18%<br>1.74G/9.51G [00:12&lt;00:44, 174MB/s]<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell\
          \ line: 13&gt;:13                                                      \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/<strong>init</strong>.py:788\
          \ in pipeline       \u2502<br>\u2502                                   \
          \                                                               \u2502<br>\u2502\
          \   785 \u2502   # Forced if framework already defined, inferred if it's\
          \ None                           \u2502<br>\u2502   786 \u2502   # Will\
          \ load the correct model if possible                                   \
          \           \u2502<br>\u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}                 \u2502<br>\u2502 \u2771\
          \ 788 \u2502   framework, model = infer_framework_load_model(          \
          \                               \u2502<br>\u2502   789 \u2502   \u2502 \
          \  model,                                                              \
          \               \u2502<br>\u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502<br>\u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502<br>\u2502 infer_framework_load_model    \
          \                                                                   \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502    276 \u2502   \u2502\
          \   \u2502   \u2502   continue                                         \
          \                         \u2502<br>\u2502    277 \u2502   \u2502      \
          \                                                                      \
          \         \u2502<br>\u2502    278 \u2502   \u2502   if isinstance(model,\
          \ str):                                                        \u2502<br>\u2502\
          \ \u2771  279 \u2502   \u2502   \u2502   raise ValueError(f\"Could not load\
          \ model {model} with any of the following cl  \u2502<br>\u2502    280 \u2502\
          \                                                                      \
          \                   \u2502<br>\u2502    281 \u2502   framework = \"tf\"\
          \ if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
          \  \u2502<br>\u2502    282 \u2502   return framework, model            \
          \                                                   \u2502<br>\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F<br>ValueError: Could not load model tiiuae/falcon-40b with\
          \ any of the following classes: (&lt;class<br>'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class<br>'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</li>\n\
          </ul>\n</blockquote>\n<p>I got the same bug in google colab. Switched to\
          \ using GPU and then it worked fine.</p>\n</blockquote>\n<p>I am running\
          \ this on Google COLAB (free version).<br>When I switch to GPU (GPU T4 Runtime\
          \ in COLAB) I still get this error.<br>Also I tried switching to TPU on\
          \ COLAB (which is possible because of the use of the accelerate lib !),\
          \ I still get the same error.</p>\n</blockquote>\n<p>How did you solve the\
          \ problem? I run it on an EC2 with 8 A100, also got the same problem.</p>\n"
        raw: "> > > Whe using this code:\n> > > \n> > > # https://huggingface.co/tiiuae/falcon-40b\n\
          > > > from transformers import AutoTokenizer, AutoModelForCausalLM\n> >\
          \ > import transformers\n> > > import torch\n> > > \n> > > model = \"tiiuae/falcon-40b\"\
          \ # Gebruik evt het kleinere broertje: tiiuae/falcon-7b\n> > > \n> > > tokenizer\
          \ = AutoTokenizer.from_pretrained(model)\n> > > pipeline = transformers.pipeline(\n\
          > > >     \"text-generation\",\n> > >     model=model,\n> > >     tokenizer=tokenizer,\n\
          > > >     torch_dtype=torch.bfloat16,\n> > >     trust_remote_code=True,\n\
          > > >     device_map=\"auto\",\n> > > )\n> > > sequences = pipeline(\n>\
          \ > >    \"Girafatron is obsessed with giraffes, the most glorious animal\
          \ on the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n> > >     max_length=200,\n> > >     do_sample=True,\n\
          > > >     top_k=10,\n> > >     num_return_sequences=1,\n> > >     eos_token_id=tokenizer.eos_token_id,\n\
          > > > )\n> > > for seq in sequences:\n> > >     print(f\"Result: {seq['generated_text']}\"\
          )\n> > > \n> > > \n> > > I get this output:\n> > > \n> > > Downloading (\u2026\
          )okenizer_config.json: 100%\n> > > 175/175 [00:00<00:00, 6.61kB/s]\n> >\
          \ > Downloading (\u2026)/main/tokenizer.json: 100%\n> > > 2.73M/2.73M [00:00<00:00,\
          \ 5.61MB/s]\n> > > Downloading (\u2026)cial_tokens_map.json: 100%\n> > >\
          \ 281/281 [00:00<00:00, 1.34kB/s]\n> > > Downloading (\u2026)lve/main/config.json:\
          \ 100%\n> > > 656/656 [00:00<00:00, 947B/s]\n> > > Downloading (\u2026)/configuration_RW.py:\
          \ 100%\n> > > 2.51k/2.51k [00:00<00:00, 3.46kB/s]\n> > > A new version of\
          \ the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
          > > > - configuration_RW.py\n> > > . Make sure to double-check they do not\
          \ contain any added malicious code. To avoid downloading new versions of\
          \ the code file, you can pin a revision.\n> > > Downloading (\u2026)main/modelling_RW.py:\
          \ 100%\n> > > 47.1k/47.1k [00:00<00:00, 108kB/s]\n> > > A new version of\
          \ the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n\
          > > > - modelling_RW.py\n> > > . Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n> > > Downloading (\u2026)model.bin.index.json:\
          \ 100%\n> > > 39.3k/39.3k [00:00<00:00, 697kB/s]\n> > > Downloading shards:\
          \ 67%\n> > > 6/9 [05:54<02:52, 57.54s/it]\n> > > Downloading (\u2026)l-00001-of-00009.bin:\
          \ 100%\n> > > 9.50G/9.50G [00:46<00:00, 258MB/s]\n> > > Downloading (\u2026\
          )l-00002-of-00009.bin: 100%\n> > > 9.51G/9.51G [01:14<00:00, 257MB/s]\n\
          > > > Downloading (\u2026)l-00003-of-00009.bin: 100%\n> > > 9.51G/9.51G\
          \ [00:50<00:00, 262MB/s]\n> > > Downloading (\u2026)l-00004-of-00009.bin:\
          \ 100%\n> > > 9.51G/9.51G [00:55<00:00, 246MB/s]\n> > > Downloading (\u2026\
          )l-00005-of-00009.bin: 100%\n> > > 9.51G/9.51G [00:57<00:00, 224MB/s]\n\
          > > > Downloading (\u2026)l-00006-of-00009.bin: 100%\n> > > 9.51G/9.51G\
          \ [00:58<00:00, 170MB/s]\n> > > Downloading (\u2026)l-00007-of-00009.bin:\
          \ 18%\n> > > 1.74G/9.51G [00:12<00:44, 174MB/s]\n> > > \u256D\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u256E\n> > > \u2502 in <cell line:\
          \ 13>:13                                                               \
          \             \u2502\n> > > \u2502                                     \
          \                                                             \u2502\n>\
          \ > > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
          \ in pipeline       \u2502\n> > > \u2502                               \
          \                                                                   \u2502\
          \n> > > \u2502   785 \u2502   # Forced if framework already defined, inferred\
          \ if it's None                           \u2502\n> > > \u2502   786 \u2502\
          \   # Will load the correct model if possible                          \
          \                    \u2502\n> > > \u2502   787 \u2502   model_classes =\
          \ {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}       \
          \          \u2502\n> > > \u2502 \u2771 788 \u2502   framework, model = infer_framework_load_model(\
          \                                         \u2502\n> > > \u2502   789 \u2502\
          \   \u2502   model,                                                    \
          \                         \u2502\n> > > \u2502   790 \u2502   \u2502   model_classes=model_classes,\
          \                                                       \u2502\n> > > \u2502\
          \   791 \u2502   \u2502   config=config,                               \
          \                                      \u2502\n> > > \u2502            \
          \                                                                      \
          \                \u2502\n> > > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279\
          \ in                    \u2502\n> > > \u2502 infer_framework_load_model\
          \                                                                      \
          \ \u2502\n> > > \u2502                                                 \
          \                                                 \u2502\n> > > \u2502 \
          \   276 \u2502   \u2502   \u2502   \u2502   continue                   \
          \                                               \u2502\n> > > \u2502   \
          \ 277 \u2502   \u2502                                                  \
          \                                   \u2502\n> > > \u2502    278 \u2502 \
          \  \u2502   if isinstance(model, str):                                 \
          \                       \u2502\n> > > \u2502 \u2771  279 \u2502   \u2502\
          \   \u2502   raise ValueError(f\"Could not load model {model} with any of\
          \ the following cl  \u2502\n> > > \u2502    280 \u2502                 \
          \                                                                      \
          \  \u2502\n> > > \u2502    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\"\
          \ in str(inspect.getmro(model.__clas  \u2502\n> > > \u2502    282 \u2502\
          \   return framework, model                                            \
          \                   \u2502\n> > > \u2570\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\n> > > ValueError:\
          \ Could not load model tiiuae/falcon-40b with any of the following classes:\
          \ (<class \n> > > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n> > > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
          > > \n> > I got the same bug in google colab. Switched to using GPU and\
          \ then it worked fine.\n> \n> I am running this on Google COLAB (free version).\n\
          > When I switch to GPU (GPU T4 Runtime in COLAB) I still get this error.\n\
          > Also I tried switching to TPU on COLAB (which is possible because of the\
          \ use of the accelerate lib !), I still get the same error.\n\nHow did you\
          \ solve the problem? I run it on an EC2 with 8 A100, also got the same problem."
        updatedAt: '2023-06-19T20:02:13.901Z'
      numEdits: 0
      reactions: []
    id: 6490b44571ead918ea9fcef7
    type: comment
  author: Andcircle
  content: "> > > Whe using this code:\n> > > \n> > > # https://huggingface.co/tiiuae/falcon-40b\n\
    > > > from transformers import AutoTokenizer, AutoModelForCausalLM\n> > > import\
    \ transformers\n> > > import torch\n> > > \n> > > model = \"tiiuae/falcon-40b\"\
    \ # Gebruik evt het kleinere broertje: tiiuae/falcon-7b\n> > > \n> > > tokenizer\
    \ = AutoTokenizer.from_pretrained(model)\n> > > pipeline = transformers.pipeline(\n\
    > > >     \"text-generation\",\n> > >     model=model,\n> > >     tokenizer=tokenizer,\n\
    > > >     torch_dtype=torch.bfloat16,\n> > >     trust_remote_code=True,\n> >\
    \ >     device_map=\"auto\",\n> > > )\n> > > sequences = pipeline(\n> > >    \"\
    Girafatron is obsessed with giraffes, the most glorious animal on the face of\
    \ this Earth. Giraftron believes all other animals are irrelevant when compared\
    \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
    ,\n> > >     max_length=200,\n> > >     do_sample=True,\n> > >     top_k=10,\n\
    > > >     num_return_sequences=1,\n> > >     eos_token_id=tokenizer.eos_token_id,\n\
    > > > )\n> > > for seq in sequences:\n> > >     print(f\"Result: {seq['generated_text']}\"\
    )\n> > > \n> > > \n> > > I get this output:\n> > > \n> > > Downloading (\u2026\
    )okenizer_config.json: 100%\n> > > 175/175 [00:00<00:00, 6.61kB/s]\n> > > Downloading\
    \ (\u2026)/main/tokenizer.json: 100%\n> > > 2.73M/2.73M [00:00<00:00, 5.61MB/s]\n\
    > > > Downloading (\u2026)cial_tokens_map.json: 100%\n> > > 281/281 [00:00<00:00,\
    \ 1.34kB/s]\n> > > Downloading (\u2026)lve/main/config.json: 100%\n> > > 656/656\
    \ [00:00<00:00, 947B/s]\n> > > Downloading (\u2026)/configuration_RW.py: 100%\n\
    > > > 2.51k/2.51k [00:00<00:00, 3.46kB/s]\n> > > A new version of the following\
    \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n> > > -\
    \ configuration_RW.py\n> > > . Make sure to double-check they do not contain any\
    \ added malicious code. To avoid downloading new versions of the code file, you\
    \ can pin a revision.\n> > > Downloading (\u2026)main/modelling_RW.py: 100%\n\
    > > > 47.1k/47.1k [00:00<00:00, 108kB/s]\n> > > A new version of the following\
    \ files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n> > > -\
    \ modelling_RW.py\n> > > . Make sure to double-check they do not contain any added\
    \ malicious code. To avoid downloading new versions of the code file, you can\
    \ pin a revision.\n> > > Downloading (\u2026)model.bin.index.json: 100%\n> > >\
    \ 39.3k/39.3k [00:00<00:00, 697kB/s]\n> > > Downloading shards: 67%\n> > > 6/9\
    \ [05:54<02:52, 57.54s/it]\n> > > Downloading (\u2026)l-00001-of-00009.bin: 100%\n\
    > > > 9.50G/9.50G [00:46<00:00, 258MB/s]\n> > > Downloading (\u2026)l-00002-of-00009.bin:\
    \ 100%\n> > > 9.51G/9.51G [01:14<00:00, 257MB/s]\n> > > Downloading (\u2026)l-00003-of-00009.bin:\
    \ 100%\n> > > 9.51G/9.51G [00:50<00:00, 262MB/s]\n> > > Downloading (\u2026)l-00004-of-00009.bin:\
    \ 100%\n> > > 9.51G/9.51G [00:55<00:00, 246MB/s]\n> > > Downloading (\u2026)l-00005-of-00009.bin:\
    \ 100%\n> > > 9.51G/9.51G [00:57<00:00, 224MB/s]\n> > > Downloading (\u2026)l-00006-of-00009.bin:\
    \ 100%\n> > > 9.51G/9.51G [00:58<00:00, 170MB/s]\n> > > Downloading (\u2026)l-00007-of-00009.bin:\
    \ 18%\n> > > 1.74G/9.51G [00:12<00:44, 174MB/s]\n> > > \u256D\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u256E\n> > > \u2502 in <cell line: 13>:13                                   \
    \                                         \u2502\n> > > \u2502               \
    \                                                                            \
    \       \u2502\n> > > \u2502 /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788\
    \ in pipeline       \u2502\n> > > \u2502                                     \
    \                                                             \u2502\n> > > \u2502\
    \   785 \u2502   # Forced if framework already defined, inferred if it's None\
    \                           \u2502\n> > > \u2502   786 \u2502   # Will load the\
    \ correct model if possible                                              \u2502\
    \n> > > \u2502   787 \u2502   model_classes = {\"tf\": targeted_task[\"tf\"],\
    \ \"pt\": targeted_task[\"pt\"]}                 \u2502\n> > > \u2502 \u2771 788\
    \ \u2502   framework, model = infer_framework_load_model(                    \
    \                     \u2502\n> > > \u2502   789 \u2502   \u2502   model,    \
    \                                                                         \u2502\
    \n> > > \u2502   790 \u2502   \u2502   model_classes=model_classes,          \
    \                                             \u2502\n> > > \u2502   791 \u2502\
    \   \u2502   config=config,                                                  \
    \                   \u2502\n> > > \u2502                                     \
    \                                                             \u2502\n> > > \u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279 in\
    \                    \u2502\n> > > \u2502 infer_framework_load_model         \
    \                                                              \u2502\n> > > \u2502\
    \                                                                            \
    \                      \u2502\n> > > \u2502    276 \u2502   \u2502   \u2502  \
    \ \u2502   continue                                                          \
    \        \u2502\n> > > \u2502    277 \u2502   \u2502                         \
    \                                                            \u2502\n> > > \u2502\
    \    278 \u2502   \u2502   if isinstance(model, str):                        \
    \                                \u2502\n> > > \u2502 \u2771  279 \u2502   \u2502\
    \   \u2502   raise ValueError(f\"Could not load model {model} with any of the\
    \ following cl  \u2502\n> > > \u2502    280 \u2502                           \
    \                                                              \u2502\n> > > \u2502\
    \    281 \u2502   framework = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__clas\
    \  \u2502\n> > > \u2502    282 \u2502   return framework, model              \
    \                                                 \u2502\n> > > \u2570\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256F\n> > > ValueError: Could not load model tiiuae/falcon-40b\
    \ with any of the following classes: (<class \n> > > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \n> > > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
    > > \n> > I got the same bug in google colab. Switched to using GPU and then it\
    \ worked fine.\n> \n> I am running this on Google COLAB (free version).\n> When\
    \ I switch to GPU (GPU T4 Runtime in COLAB) I still get this error.\n> Also I\
    \ tried switching to TPU on COLAB (which is possible because of the use of the\
    \ accelerate lib !), I still get the same error.\n\nHow did you solve the problem?\
    \ I run it on an EC2 with 8 A100, also got the same problem."
  created_at: 2023-06-19 19:02:13+00:00
  edited: false
  hidden: false
  id: 6490b44571ead918ea9fcef7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79494b8e0651dabc122271fb267f6a40.svg
      fullname: Sumedh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumedh7-11
      type: user
    createdAt: '2023-07-02T06:22:43.000Z'
    data:
      edited: false
      editors:
      - sumedh7-11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7757824063301086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79494b8e0651dabc122271fb267f6a40.svg
          fullname: Sumedh
          isHf: false
          isPro: false
          name: sumedh7-11
          type: user
        html: '<p>I am still getting the same error. Unable to load Falcon 40b instruct
          or Falcon 40b. This is the error<br>ValueError: Could not load model tiiuae/falcon-40b
          with any of the following classes: (&lt;class<br>''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          <p>Also, I have enough space in RAM. It could be an issue with text generation.
          Any help on this.</p>

          '
        raw: "I am still getting the same error. Unable to load Falcon 40b instruct\
          \ or Falcon 40b. This is the error\nValueError: Could not load model tiiuae/falcon-40b\
          \ with any of the following classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
          \nAlso, I have enough space in RAM. It could be an issue with text generation.\
          \ Any help on this."
        updatedAt: '2023-07-02T06:22:43.029Z'
      numEdits: 0
      reactions: []
    id: 64a117b3189a9985ad4d03f3
    type: comment
  author: sumedh7-11
  content: "I am still getting the same error. Unable to load Falcon 40b instruct\
    \ or Falcon 40b. This is the error\nValueError: Could not load model tiiuae/falcon-40b\
    \ with any of the following classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\n\
    \nAlso, I have enough space in RAM. It could be an issue with text generation.\
    \ Any help on this."
  created_at: 2023-07-02 05:22:43+00:00
  edited: false
  hidden: false
  id: 64a117b3189a9985ad4d03f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8684df49e4ccce3dd6503f6ae55cd8c2.svg
      fullname: Robert Franklin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RobeFranWBA
      type: user
    createdAt: '2023-07-19T10:35:58.000Z'
    data:
      edited: true
      editors:
      - RobeFranWBA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8957958221435547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8684df49e4ccce3dd6503f6ae55cd8c2.svg
          fullname: Robert Franklin
          isHf: false
          isPro: false
          name: RobeFranWBA
          type: user
        html: "<p>I found the solution to this.</p>\n<p>I had to create a folder to\
          \ offload the existing weights to to get it to work though which i named\
          \ <code>\"device_map_weights\"</code>.</p>\n<pre><code class=\"language-from\"\
          >import transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(\n    model,\n    device_map=\"\
          auto\",\n    trust_remote_code=True,\n    offload_folder=\"device_map_weights\"\
          \n    )\nmodel = AutoModelForCausalLM.from_pretrained(\n    model,\n   \
          \ device_map=\"auto\",\n    trust_remote_code=True,\n    offload_folder=\"\
          device_map_weights\"\n    )\n\npipeline = transformers.pipeline(\n    \"\
          text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
          \   \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n"
        raw: "I found the solution to this.\n\nI had to create a folder to offload\
          \ the existing weights to to get it to work though which i named `\"device_map_weights\"\
          `.\n\n```from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(\n    model,\n    device_map=\"\
          auto\",\n    trust_remote_code=True,\n    offload_folder=\"device_map_weights\"\
          \n    )\nmodel = AutoModelForCausalLM.from_pretrained(\n    model,\n   \
          \ device_map=\"auto\",\n    trust_remote_code=True,\n    offload_folder=\"\
          device_map_weights\"\n    )\n\npipeline = transformers.pipeline(\n    \"\
          text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
          \   \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```"
        updatedAt: '2023-07-19T10:54:24.570Z'
      numEdits: 1
      reactions: []
    id: 64b7bc8e62b2914fd571ef89
    type: comment
  author: RobeFranWBA
  content: "I found the solution to this.\n\nI had to create a folder to offload the\
    \ existing weights to to get it to work though which i named `\"device_map_weights\"\
    `.\n\n```from transformers import AutoTokenizer, AutoModelForCausalLM\nimport\
    \ transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\n    model,\n    device_map=\"auto\",\n   \
    \ trust_remote_code=True,\n    offload_folder=\"device_map_weights\"\n    )\n\
    model = AutoModelForCausalLM.from_pretrained(\n    model,\n    device_map=\"auto\"\
    ,\n    trust_remote_code=True,\n    offload_folder=\"device_map_weights\"\n  \
    \  )\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed\
    \ with giraffes, the most glorious animal on the face of this Earth. Giraftron\
    \ believes all other animals are irrelevant when compared to the glorious majesty\
    \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
    \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n```"
  created_at: 2023-07-19 09:35:58+00:00
  edited: true
  hidden: false
  id: 64b7bc8e62b2914fd571ef89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
      fullname: Surya Narayanan Hari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: surya-narayanan
      type: user
    createdAt: '2023-08-23T20:37:15.000Z'
    data:
      edited: false
      editors:
      - surya-narayanan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9845303297042847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/317df1c0b62410524f26d177b30f0fa1.svg
          fullname: Surya Narayanan Hari
          isHf: false
          isPro: false
          name: surya-narayanan
          type: user
        html: '<p>have we solved the original issue? </p>

          '
        raw: 'have we solved the original issue? '
        updatedAt: '2023-08-23T20:37:15.239Z'
      numEdits: 0
      reactions: []
    id: 64e66dfb737235902309f699
    type: comment
  author: surya-narayanan
  content: 'have we solved the original issue? '
  created_at: 2023-08-23 19:37:15+00:00
  edited: false
  hidden: false
  id: 64e66dfb737235902309f699
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: '[Bug] Does not work'
