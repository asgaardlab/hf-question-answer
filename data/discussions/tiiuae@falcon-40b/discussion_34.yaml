!!python/object:huggingface_hub.community.DiscussionWithDetails
author: utensil
conflicting_files: null
created_at: 2023-06-01 05:27:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a30fc4114856bc3ba729c1ed95e54d9b.svg
      fullname: Utensil Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: utensil
      type: user
    createdAt: '2023-06-01T06:27:06.000Z'
    data:
      edited: true
      editors:
      - utensil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a30fc4114856bc3ba729c1ed95e54d9b.svg
          fullname: Utensil Song
          isHf: false
          isPro: false
          name: utensil
          type: user
        html: '<p>I''m trying to figure out whether falcon is using Flash attention
          (it is per its model card), but I found no related code in the repo such
          as <code>from flash_attn.flash_attention import FlashMHA</code>etc. Am I
          missing something?</p>

          '
        raw: I'm trying to figure out whether falcon is using Flash attention (it
          is per its model card), but I found no related code in the repo such as
          `from flash_attn.flash_attention import FlashMHA`etc. Am I missing something?
        updatedAt: '2023-06-01T06:37:04.121Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - lightningRalf
        - GaussianMixture
    id: 64783a3a256b62e2198e3a38
    type: comment
  author: utensil
  content: I'm trying to figure out whether falcon is using Flash attention (it is
    per its model card), but I found no related code in the repo such as `from flash_attn.flash_attention
    import FlashMHA`etc. Am I missing something?
  created_at: 2023-06-01 05:27:06+00:00
  edited: true
  hidden: false
  id: 64783a3a256b62e2198e3a38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8a658f50443c963df63128990e5db26.svg
      fullname: Alexey Kuntsevich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jezzarax
      type: user
    createdAt: '2023-06-02T10:20:04.000Z'
    data:
      edited: false
      editors:
      - Jezzarax
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8a658f50443c963df63128990e5db26.svg
          fullname: Alexey Kuntsevich
          isHf: false
          isPro: false
          name: Jezzarax
          type: user
        html: '<p>You can find the model code using <a rel="nofollow" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">scaled_dot_product_attention</a>,
          check <a href="https://huggingface.co/tiiuae/falcon-40b/blob/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py#L289">here</a>.
          It is expected to work in flash attention mode, but due to an <a rel="nofollow"
          href="https://github.com/pytorch/pytorch/issues/97514">issue</a> we got
          to wait for PyTorch 2.1 to benefit from flash or memory_efficient attention.</p>

          '
        raw: You can find the model code using [scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html),
          check [here](https://huggingface.co/tiiuae/falcon-40b/blob/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py#L289).
          It is expected to work in flash attention mode, but due to an [issue](https://github.com/pytorch/pytorch/issues/97514)
          we got to wait for PyTorch 2.1 to benefit from flash or memory_efficient
          attention.
        updatedAt: '2023-06-02T10:20:04.833Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - utensil
        - yahma
        - FalconLLM
    id: 6479c2541a2aefceecd4af85
    type: comment
  author: Jezzarax
  content: You can find the model code using [scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html),
    check [here](https://huggingface.co/tiiuae/falcon-40b/blob/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py#L289).
    It is expected to work in flash attention mode, but due to an [issue](https://github.com/pytorch/pytorch/issues/97514)
    we got to wait for PyTorch 2.1 to benefit from flash or memory_efficient attention.
  created_at: 2023-06-02 09:20:04+00:00
  edited: false
  hidden: false
  id: 6479c2541a2aefceecd4af85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a30fc4114856bc3ba729c1ed95e54d9b.svg
      fullname: Utensil Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: utensil
      type: user
    createdAt: '2023-06-02T10:55:54.000Z'
    data:
      edited: false
      editors:
      - utensil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a30fc4114856bc3ba729c1ed95e54d9b.svg
          fullname: Utensil Song
          isHf: false
          isPro: false
          name: utensil
          type: user
        html: '<p>Thank you for the reply and the pointers, and the great work in
          general!</p>

          <p>As for xformer attention mentioned in the issue, my test shows that falcon
          can work with it already and saves ~ 15% VRAM (exact number might vary in
          different setting).</p>

          <p>May I also assume that with pytorch 2.1, falcon will work with better
          transformer (which includes flash attention to my knowledge ) ? Link: <a
          href="https://huggingface.co/docs/optimum/bettertransformer/overview">https://huggingface.co/docs/optimum/bettertransformer/overview</a></p>

          '
        raw: 'Thank you for the reply and the pointers, and the great work in general!


          As for xformer attention mentioned in the issue, my test shows that falcon
          can work with it already and saves ~ 15% VRAM (exact number might vary in
          different setting).


          May I also assume that with pytorch 2.1, falcon will work with better transformer
          (which includes flash attention to my knowledge ) ? Link: https://huggingface.co/docs/optimum/bettertransformer/overview'
        updatedAt: '2023-06-02T10:55:54.762Z'
      numEdits: 0
      reactions: []
    id: 6479cabaf518a860fbc5a41f
    type: comment
  author: utensil
  content: 'Thank you for the reply and the pointers, and the great work in general!


    As for xformer attention mentioned in the issue, my test shows that falcon can
    work with it already and saves ~ 15% VRAM (exact number might vary in different
    setting).


    May I also assume that with pytorch 2.1, falcon will work with better transformer
    (which includes flash attention to my knowledge ) ? Link: https://huggingface.co/docs/optimum/bettertransformer/overview'
  created_at: 2023-06-02 09:55:54+00:00
  edited: false
  hidden: false
  id: 6479cabaf518a860fbc5a41f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: Flash attention
