!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rmihaylov
conflicting_files: null
created_at: 2023-05-31 14:00:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/989a02a63a1d25dcf28b00d6f746e741.svg
      fullname: Rumen Mihaylov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rmihaylov
      type: user
    createdAt: '2023-05-31T15:00:40.000Z'
    data:
      edited: false
      editors:
      - rmihaylov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/989a02a63a1d25dcf28b00d6f746e741.svg
          fullname: Rumen Mihaylov
          isHf: false
          isPro: false
          name: rmihaylov
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/rmihaylov/falcontune">https://github.com/rmihaylov/falcontune</a></p>

          '
        raw: https://github.com/rmihaylov/falcontune
        updatedAt: '2023-05-31T15:00:40.046Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - matthoffner
        - max-fry
        - SamuelAzran
        - vonderoh
        - FalconLLM
        - hamad
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - SamuelAzran
        - Ichsan2895
        - FalconLLM
    id: 64776118f32a4117fd13ceec
    type: comment
  author: rmihaylov
  content: https://github.com/rmihaylov/falcontune
  created_at: 2023-05-31 14:00:40+00:00
  edited: false
  hidden: false
  id: 64776118f32a4117fd13ceec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-04T16:34:41.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9366174936294556
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<p>Excuse me, some question for you..</p>

          <ol>

          <li>What is the different between your <code>falcontune</code> and <code>QLoRA</code>?</li>

          <li>What is the different fine tuning (with the new dataset) in <code>Bitsandbytes</code>+<code>peft</code>
          and your code? Or maybe your script is the simple form of <code>bitsandbytes</code>+<code>peft</code>?</li>

          <li>Can I activate ''nf4'' (normal four bit float) in the <code>GPTQ</code>?</li>

          </ol>

          '
        raw: 'Excuse me, some question for you..

          1. What is the different between your `falcontune` and `QLoRA`?

          2. What is the different fine tuning (with the new dataset) in `Bitsandbytes`+`peft`
          and your code? Or maybe your script is the simple form of `bitsandbytes`+`peft`?

          3. Can I activate ''nf4'' (normal four bit float) in the `GPTQ`?'
        updatedAt: '2023-06-04T16:35:26.358Z'
      numEdits: 2
      reactions: []
    id: 647cbd2160dfe0f35d57067a
    type: comment
  author: Ichsan2895
  content: 'Excuse me, some question for you..

    1. What is the different between your `falcontune` and `QLoRA`?

    2. What is the different fine tuning (with the new dataset) in `Bitsandbytes`+`peft`
    and your code? Or maybe your script is the simple form of `bitsandbytes`+`peft`?

    3. Can I activate ''nf4'' (normal four bit float) in the `GPTQ`?'
  created_at: 2023-06-04 15:34:41+00:00
  edited: true
  hidden: false
  id: 647cbd2160dfe0f35d57067a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:28:06.000Z'
    data:
      pinned: true
    id: 648336f690dec912c983e0d4
    type: pinning-change
  author: FalconLLM
  created_at: 2023-06-09 13:28:06+00:00
  id: 648336f690dec912c983e0d4
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-06-10T09:55:07.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774294495582581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: '<blockquote>

          <p>Excuse me, some question for you..</p>

          </blockquote>

          <p>I join in the questions!</p>

          '
        raw: '> Excuse me, some question for you..


          I join in the questions!'
        updatedAt: '2023-06-10T09:55:07.101Z'
      numEdits: 0
      reactions: []
    id: 6484487b8cf0b32b0b9c0d73
    type: comment
  author: dimaischenko
  content: '> Excuse me, some question for you..


    I join in the questions!'
  created_at: 2023-06-10 08:55:07+00:00
  edited: false
  hidden: false
  id: 6484487b8cf0b32b0b9c0d73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/o8Ulf7Tan062ifN8nXZIN.png?w=200&h=200&f=face
      fullname: jimbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cr00
      type: user
    createdAt: '2023-07-01T01:09:44.000Z'
    data:
      edited: false
      editors:
      - cr00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9827755093574524
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/o8Ulf7Tan062ifN8nXZIN.png?w=200&h=200&f=face
          fullname: jimbo
          isHf: false
          isPro: false
          name: cr00
          type: user
        html: '<p>Doesn''t 40b require like 48Gb of VRAM? also if anyone reads this
          I would be very appreciative for any insight into cost efficient/realistic
          hardware for ML, it seems like the cheapest build is somewhere in the neighborhood
          of $5-6k, and I think I would rather have my own hardware than rely on Amazon/Google/Azure,
          Thanks</p>

          '
        raw: Doesn't 40b require like 48Gb of VRAM? also if anyone reads this I would
          be very appreciative for any insight into cost efficient/realistic hardware
          for ML, it seems like the cheapest build is somewhere in the neighborhood
          of $5-6k, and I think I would rather have my own hardware than rely on Amazon/Google/Azure,
          Thanks
        updatedAt: '2023-07-01T01:09:44.461Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Varadh
    id: 649f7cd82cf6c6a571ecf66a
    type: comment
  author: cr00
  content: Doesn't 40b require like 48Gb of VRAM? also if anyone reads this I would
    be very appreciative for any insight into cost efficient/realistic hardware for
    ML, it seems like the cheapest build is somewhere in the neighborhood of $5-6k,
    and I think I would rather have my own hardware than rely on Amazon/Google/Azure,
    Thanks
  created_at: 2023-07-01 00:09:44+00:00
  edited: false
  hidden: false
  id: 649f7cd82cf6c6a571ecf66a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b81359016a20d568f1d6a498c1c5916.svg
      fullname: Andy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andyecher7
      type: user
    createdAt: '2023-07-12T15:36:17.000Z'
    data:
      edited: true
      editors:
      - andyecher7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7438033223152161
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b81359016a20d568f1d6a498c1c5916.svg
          fullname: Andy
          isHf: false
          isPro: false
          name: andyecher7
          type: user
        html: '<p>Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000
          48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to
          generate 20 tokens (words), in 4bit -it takes 25gb ram and  12 second for
          same 20 tokens - not sure why..</p>

          <p>...<br>bnb_config = BitsAndBytesConfig(<br>   load_in_8bit=True,<br>    bnb_4bit_use_double_quant=False,<br>    bnb_4bit_quant_type="nf4",<br>    bnb_4bit_compute_dtype=torch.bfloat16,<br>)</p>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    PATH,<br>    device_map="auto"<br>    trust_remote_code=True,<br>    quantization_config=bnb_config,<br>)</p>

          '
        raw: "Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000\
          \ 48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to\
          \ generate 20 tokens (words), in 4bit -it takes 25gb ram and  12 second\
          \ for same 20 tokens - not sure why..\n\n...\nbnb_config = BitsAndBytesConfig(\n\
          \   load_in_8bit=True,   \n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    PATH,\n    device_map=\"auto\"\n    trust_remote_code=True,\n    quantization_config=bnb_config,\
          \        \n)\n\n"
        updatedAt: '2023-07-12T15:53:03.090Z'
      numEdits: 3
      reactions: []
    id: 64aec871fcd8c4d4b5dba002
    type: comment
  author: andyecher7
  content: "Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000 48GB\
    \ (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to generate\
    \ 20 tokens (words), in 4bit -it takes 25gb ram and  12 second for same 20 tokens\
    \ - not sure why..\n\n...\nbnb_config = BitsAndBytesConfig(\n   load_in_8bit=True,\
    \   \n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n\
    \    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    PATH,\n    device_map=\"auto\"\n    trust_remote_code=True,\n    quantization_config=bnb_config,\
    \        \n)\n\n"
  created_at: 2023-07-12 14:36:17+00:00
  edited: true
  hidden: false
  id: 64aec871fcd8c4d4b5dba002
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aeb99bfe03303b3af452e6ef29b88b5c.svg
      fullname: wasim maliik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wasiim
      type: user
    createdAt: '2023-07-13T10:48:02.000Z'
    data:
      edited: false
      editors:
      - wasiim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487377405166626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aeb99bfe03303b3af452e6ef29b88b5c.svg
          fullname: wasim maliik
          isHf: false
          isPro: false
          name: wasiim
          type: user
        html: '<p>can anyone help me please<br>i have the text data stored in .txt
          the text data is simple information about a technology<br>i want to fine
          tune the falcon model and the i want to ask the question to the falcon model
          according to that .txt file </p>

          '
        raw: "can anyone help me please \ni have the text data stored in .txt the\
          \ text data is simple information about a technology \ni want to fine tune\
          \ the falcon model and the i want to ask the question to the falcon model\
          \ according to that .txt file "
        updatedAt: '2023-07-13T10:48:02.515Z'
      numEdits: 0
      reactions: []
    id: 64afd662cf60365d699eaa90
    type: comment
  author: wasiim
  content: "can anyone help me please \ni have the text data stored in .txt the text\
    \ data is simple information about a technology \ni want to fine tune the falcon\
    \ model and the i want to ask the question to the falcon model according to that\
    \ .txt file "
  created_at: 2023-07-13 09:48:02+00:00
  edited: false
  hidden: false
  id: 64afd662cf60365d699eaa90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
      fullname: JEREMY D GAMET
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: archonlith
      type: user
    createdAt: '2023-08-15T17:18:18.000Z'
    data:
      edited: false
      editors:
      - archonlith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9147798418998718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
          fullname: JEREMY D GAMET
          isHf: false
          isPro: false
          name: archonlith
          type: user
        html: '<blockquote>

          <p>Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000 48GB
          (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to generate
          20 tokens (words), in 4bit -it takes 25gb ram and  12 second for same 20
          tokens - not sure why..</p>

          </blockquote>

          <p>I would also love to know why it takes so long.</p>

          <p>My main reason, (and I suspect many people''s) main use case for GPT
          alternatives include both open source AND hopefully faster speed. Reducing
          the memory profile but increasing the lag seems like a lateral move.</p>

          '
        raw: "> Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000\
          \ 48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to\
          \ generate 20 tokens (words), in 4bit -it takes 25gb ram and  12 second\
          \ for same 20 tokens - not sure why..\n> \n\nI would also love to know why\
          \ it takes so long.\n\nMy main reason, (and I suspect many people's) main\
          \ use case for GPT alternatives include both open source AND hopefully faster\
          \ speed. Reducing the memory profile but increasing the lag seems like a\
          \ lateral move."
        updatedAt: '2023-08-15T17:18:18.544Z'
      numEdits: 0
      reactions: []
    id: 64dbb35a1d19239f50728d62
    type: comment
  author: archonlith
  content: "> Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000\
    \ 48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to generate\
    \ 20 tokens (words), in 4bit -it takes 25gb ram and  12 second for same 20 tokens\
    \ - not sure why..\n> \n\nI would also love to know why it takes so long.\n\n\
    My main reason, (and I suspect many people's) main use case for GPT alternatives\
    \ include both open source AND hopefully faster speed. Reducing the memory profile\
    \ but increasing the lag seems like a lateral move."
  created_at: 2023-08-15 16:18:18+00:00
  edited: false
  hidden: false
  id: 64dbb35a1d19239f50728d62
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: tiiuae/falcon-40b
repo_type: model
status: open
target_branch: null
title: 'Custom 4-bit Finetuning 5-7 times faster inference than QLora '
