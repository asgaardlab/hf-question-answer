!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MosheBeeri
conflicting_files: null
created_at: 2023-05-29 06:33:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/011ed4e106cc1979f19d8fcac63c2716.svg
      fullname: moshe Beeri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MosheBeeri
      type: user
    createdAt: '2023-05-29T07:33:51.000Z'
    data:
      edited: true
      editors:
      - MosheBeeri
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/011ed4e106cc1979f19d8fcac63c2716.svg
          fullname: moshe Beeri
          isHf: false
          isPro: false
          name: MosheBeeri
          type: user
        html: '<p>It just consumes all available memory<br>51 G Ram, and 16 G GPU
          RAM<br>Any idea besides using machine with more RAM?</p>

          '
        raw: 'It just consumes all available memory

          51 G Ram, and 16 G GPU RAM

          Any idea besides using machine with more RAM?'
        updatedAt: '2023-05-29T07:34:11.577Z'
      numEdits: 1
      reactions: []
    id: 6474555fd815855e4ef379c3
    type: comment
  author: MosheBeeri
  content: 'It just consumes all available memory

    51 G Ram, and 16 G GPU RAM

    Any idea besides using machine with more RAM?'
  created_at: 2023-05-29 06:33:51+00:00
  edited: true
  hidden: false
  id: 6474555fd815855e4ef379c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6406d8f4d684369027164e41/vHRepecrzKtxdm0q52lVd.jpeg?w=200&h=200&f=face
      fullname: sunyuhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuuhan
      type: user
    createdAt: '2023-05-29T07:51:16.000Z'
    data:
      edited: false
      editors:
      - yuuhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6406d8f4d684369027164e41/vHRepecrzKtxdm0q52lVd.jpeg?w=200&h=200&f=face
          fullname: sunyuhan
          isHf: false
          isPro: false
          name: yuuhan
          type: user
        html: '<p>Of course, even 13b model need v100_32g to run, the 40b model must
          need more!</p>

          '
        raw: Of course, even 13b model need v100_32g to run, the 40b model must need
          more!
        updatedAt: '2023-05-29T07:51:16.004Z'
      numEdits: 0
      reactions: []
    id: 64745974d56974d0c0570eca
    type: comment
  author: yuuhan
  content: Of course, even 13b model need v100_32g to run, the 40b model must need
    more!
  created_at: 2023-05-29 06:51:16+00:00
  edited: false
  hidden: false
  id: 64745974d56974d0c0570eca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:09:43.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Model weights alone are ~= 80GB, so fast inference would require
          at least 90-100GB.<br>You can try to see if you can get accelerate with
          cpu offloading to work: <a href="https://huggingface.co/docs/accelerate/package_reference/big_modeling">https://huggingface.co/docs/accelerate/package_reference/big_modeling</a>
          </p>

          <p>The community has also created a 4bit quantised version of the model:
          <a href="https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ">https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ</a>,
          which should only require 20GB for the model weights. </p>

          <p>Otherwise the best bet would be to work with the smaller models: <a href="https://huggingface.co/tiiuae/falcon-7b">https://huggingface.co/tiiuae/falcon-7b</a></p>

          '
        raw: "Model weights alone are ~= 80GB, so fast inference would require at\
          \ least 90-100GB.\nYou can try to see if you can get accelerate with cpu\
          \ offloading to work: https://huggingface.co/docs/accelerate/package_reference/big_modeling\
          \ \n\nThe community has also created a 4bit quantised version of the model:\
          \ https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ, which should\
          \ only require 20GB for the model weights. \n\nOtherwise the best bet would\
          \ be to work with the smaller models: https://huggingface.co/tiiuae/falcon-7b\n"
        updatedAt: '2023-05-30T07:09:43.566Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6475a137e9b57ce0caa136e6
    id: 6475a137e9b57ce0caa136e5
    type: comment
  author: FalconLLM
  content: "Model weights alone are ~= 80GB, so fast inference would require at least\
    \ 90-100GB.\nYou can try to see if you can get accelerate with cpu offloading\
    \ to work: https://huggingface.co/docs/accelerate/package_reference/big_modeling\
    \ \n\nThe community has also created a 4bit quantised version of the model: https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ,\
    \ which should only require 20GB for the model weights. \n\nOtherwise the best\
    \ bet would be to work with the smaller models: https://huggingface.co/tiiuae/falcon-7b\n"
  created_at: 2023-05-30 06:09:43+00:00
  edited: false
  hidden: false
  id: 6475a137e9b57ce0caa136e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:09:43.000Z'
    data:
      status: closed
    id: 6475a137e9b57ce0caa136e6
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 06:09:43+00:00
  id: 6475a137e9b57ce0caa136e6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: Could not run on Colab
