!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alpindale
conflicting_files: null
created_at: 2023-05-27 11:01:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-05-27T12:01:50.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<p>Is this a typo or was there a reasoning behind this decision?</p>

          '
        raw: Is this a typo or was there a reasoning behind this decision?
        updatedAt: '2023-05-27T12:01:50.110Z'
      numEdits: 0
      reactions: []
    id: 6471f12e021b8f492f06e90c
    type: comment
  author: alpindale
  content: Is this a typo or was there a reasoning behind this decision?
  created_at: 2023-05-27 11:01:50+00:00
  edited: false
  hidden: false
  id: 6471f12e021b8f492f06e90c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e5579e089a57e52149b322e2ed22421a.svg
      fullname: Max Fry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: max-fry
      type: user
    createdAt: '2023-05-27T13:09:18.000Z'
    data:
      edited: false
      editors:
      - max-fry
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e5579e089a57e52149b322e2ed22421a.svg
          fullname: Max Fry
          isHf: false
          isPro: false
          name: max-fry
          type: user
        html: '<p>Most likely it''s because training a 40B model is significantly
          more expensive than training a 7B model.</p>

          '
        raw: Most likely it's because training a 40B model is significantly more expensive
          than training a 7B model.
        updatedAt: '2023-05-27T13:09:18.662Z'
      numEdits: 0
      reactions: []
    id: 647200fe83da304b49dd2d1b
    type: comment
  author: max-fry
  content: Most likely it's because training a 40B model is significantly more expensive
    than training a 7B model.
  created_at: 2023-05-27 12:09:18+00:00
  edited: false
  hidden: false
  id: 647200fe83da304b49dd2d1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659043660560-62e2fefda63b58b8eb738cff.jpeg?w=200&h=200&f=face
      fullname: Hankcs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hankcs
      type: user
    createdAt: '2023-05-29T17:07:10.000Z'
    data:
      edited: false
      editors:
      - hankcs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659043660560-62e2fefda63b58b8eb738cff.jpeg?w=200&h=200&f=face
          fullname: Hankcs
          isHf: false
          isPro: false
          name: hankcs
          type: user
        html: '<p>I''m interested in this question too. Looking forward to an official
          explanation from the authors. </p>

          <p>BTW, according to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">leaderboard</a>,
          falcon-7b outperforms mpt-7b by 0.2, which could also be attributed to the
          fact that falcon-7b is only trained on 1T tokens of <em>unrefined</em> web
          data.</p>

          '
        raw: "I'm interested in this question too. Looking forward to an official\
          \ explanation from the authors. \n\nBTW, according to the [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),\
          \ falcon-7b outperforms mpt-7b by 0.2, which could also be attributed to\
          \ the fact that falcon-7b is only trained on 1T tokens of *unrefined* web\
          \ data."
        updatedAt: '2023-05-29T17:07:10.617Z'
      numEdits: 0
      reactions: []
    id: 6474dbbef9e3e0b312f05d01
    type: comment
  author: hankcs
  content: "I'm interested in this question too. Looking forward to an official explanation\
    \ from the authors. \n\nBTW, according to the [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),\
    \ falcon-7b outperforms mpt-7b by 0.2, which could also be attributed to the fact\
    \ that falcon-7b is only trained on 1T tokens of *unrefined* web data."
  created_at: 2023-05-29 16:07:10+00:00
  edited: false
  hidden: false
  id: 6474dbbef9e3e0b312f05d01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:17:40.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Hey! </p>

          <p>This is a purely arbitrary decision :). We iterate a lot on internal
          models, and Falcon-40B was our first serious foray into this scale--so we
          wanted to validate infra, codebase, data, etc. That''s why we stuck to 1T.</p>

          <p>The 7B came later, when we had 384 GPUs unscheduled for two weeks, so
          1.5T was a good match. </p>

          <p>Regarding the different with MPT-7B being smaller, we believe this is
          due to a combination of three factors: (1) we are approaching the limits
          of what can be done with a 7B pretrained model; (2) multiquery with 64 attention
          head size improves inference scalability, but that''s at the cost of some
          task performance; (3) we experimented for the 7B with a very large batch
          size.</p>

          '
        raw: "Hey! \n\nThis is a purely arbitrary decision :). We iterate a lot on\
          \ internal models, and Falcon-40B was our first serious foray into this\
          \ scale--so we wanted to validate infra, codebase, data, etc. That's why\
          \ we stuck to 1T.\n\nThe 7B came later, when we had 384 GPUs unscheduled\
          \ for two weeks, so 1.5T was a good match. \n\nRegarding the different with\
          \ MPT-7B being smaller, we believe this is due to a combination of three\
          \ factors: (1) we are approaching the limits of what can be done with a\
          \ 7B pretrained model; (2) multiquery with 64 attention head size improves\
          \ inference scalability, but that's at the cost of some task performance;\
          \ (3) we experimented for the 7B with a very large batch size."
        updatedAt: '2023-05-30T07:17:40.884Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - hankcs
        - max-fry
      relatedEventId: 6475a314c894b5c9cf6ebff9
    id: 6475a314c894b5c9cf6ebff8
    type: comment
  author: FalconLLM
  content: "Hey! \n\nThis is a purely arbitrary decision :). We iterate a lot on internal\
    \ models, and Falcon-40B was our first serious foray into this scale--so we wanted\
    \ to validate infra, codebase, data, etc. That's why we stuck to 1T.\n\nThe 7B\
    \ came later, when we had 384 GPUs unscheduled for two weeks, so 1.5T was a good\
    \ match. \n\nRegarding the different with MPT-7B being smaller, we believe this\
    \ is due to a combination of three factors: (1) we are approaching the limits\
    \ of what can be done with a 7B pretrained model; (2) multiquery with 64 attention\
    \ head size improves inference scalability, but that's at the cost of some task\
    \ performance; (3) we experimented for the 7B with a very large batch size."
  created_at: 2023-05-30 06:17:40+00:00
  edited: false
  hidden: false
  id: 6475a314c894b5c9cf6ebff8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T07:17:40.000Z'
    data:
      status: closed
    id: 6475a314c894b5c9cf6ebff9
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 06:17:40+00:00
  id: 6475a314c894b5c9cf6ebff9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: tiiuae/falcon-40b
repo_type: model
status: closed
target_branch: null
title: Is the 7B trained on 1.5 trillion tokens, but the *40B* on 1 trillion only?
