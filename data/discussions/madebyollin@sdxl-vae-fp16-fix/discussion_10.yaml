!!python/object:huggingface_hub.community.DiscussionWithDetails
author: darshat
conflicting_files: null
created_at: 2023-08-16 04:27:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
      fullname: DS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darshat
      type: user
    createdAt: '2023-08-16T05:27:33.000Z'
    data:
      edited: false
      editors:
      - darshat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9119998216629028
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
          fullname: DS
          isHf: false
          isPro: false
          name: darshat
          type: user
        html: '<p>Hi,<br>Is it possible to further train this VAE? The scenario is
          to preserve text details in an image and then use it with SDXL. It would
          be great to retrain this vae on images that contain text.<br>Thanks!</p>

          '
        raw: "Hi,\r\nIs it possible to further train this VAE? The scenario is to\
          \ preserve text details in an image and then use it with SDXL. It would\
          \ be great to retrain this vae on images that contain text. \r\nThanks!"
        updatedAt: '2023-08-16T05:27:33.660Z'
      numEdits: 0
      reactions: []
    id: 64dc5e45c38427829de9b092
    type: comment
  author: darshat
  content: "Hi,\r\nIs it possible to further train this VAE? The scenario is to preserve\
    \ text details in an image and then use it with SDXL. It would be great to retrain\
    \ this vae on images that contain text. \r\nThanks!"
  created_at: 2023-08-16 04:27:33+00:00
  edited: false
  hidden: false
  id: 64dc5e45c38427829de9b092
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
      fullname: madebyollin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: madebyollin
      type: user
    createdAt: '2023-08-16T15:20:01.000Z'
    data:
      edited: false
      editors:
      - madebyollin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8315045237541199
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
          fullname: madebyollin
          isHf: false
          isPro: false
          name: madebyollin
          type: user
        html: '<p>Somewhat possible :) but it''s out of scope for this repo.</p>

          <p>SDXL-VAE compresses every 8x8 patch of input (RGB) pixels into 1 (RGBA)
          pixel, so the SDXL-VAE latents can store at most 1/48th the information
          of the original image (which means the SDXL-VAE encoder always has to throw
          away information, and the SDXL-VAE decoder always has to make up new details).</p>

          <p>Given how compressed the latents are, SDXL-VAE actually does an incredibly
          good job at recovering text. SDXL-VAE only struggles when the text is very
          small:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/95MTLH-p4DbDEQcws55Uh.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/95MTLH-p4DbDEQcws55Uh.png"></a></p>

          <p>You can probably get better small-size letterforms by fine-tuning the
          VAE decoder on text images, like you described (I think SGM <a rel="nofollow"
          href="https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml">has
          code for VAE training</a>)... but it won''t fix the fundamental lack of
          information in the latents (small letterforms will still be "made up" during
          decoding), and it also won''t improve the UNet / diffusion process (which
          is the reason that SDXL generates nonsense text even at very large font
          sizes)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/ys-_J5q0Eta2QGxvG_tdn.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/ys-_J5q0Eta2QGxvG_tdn.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/fZuxamIvRg3UZYUC66wju.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/fZuxamIvRg3UZYUC66wju.png"></a></p>

          '
        raw: 'Somewhat possible :) but it''s out of scope for this repo.


          SDXL-VAE compresses every 8x8 patch of input (RGB) pixels into 1 (RGBA)
          pixel, so the SDXL-VAE latents can store at most 1/48th the information
          of the original image (which means the SDXL-VAE encoder always has to throw
          away information, and the SDXL-VAE decoder always has to make up new details).


          Given how compressed the latents are, SDXL-VAE actually does an incredibly
          good job at recovering text. SDXL-VAE only struggles when the text is very
          small:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/95MTLH-p4DbDEQcws55Uh.png)


          You can probably get better small-size letterforms by fine-tuning the VAE
          decoder on text images, like you described (I think SGM [has code for VAE
          training](https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml))...
          but it won''t fix the fundamental lack of information in the latents (small
          letterforms will still be "made up" during decoding), and it also won''t
          improve the UNet / diffusion process (which is the reason that SDXL generates
          nonsense text even at very large font sizes)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/ys-_J5q0Eta2QGxvG_tdn.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/fZuxamIvRg3UZYUC66wju.png)'
        updatedAt: '2023-08-16T15:20:01.752Z'
      numEdits: 0
      reactions: []
    id: 64dce921c797b87132e5b0e2
    type: comment
  author: madebyollin
  content: 'Somewhat possible :) but it''s out of scope for this repo.


    SDXL-VAE compresses every 8x8 patch of input (RGB) pixels into 1 (RGBA) pixel,
    so the SDXL-VAE latents can store at most 1/48th the information of the original
    image (which means the SDXL-VAE encoder always has to throw away information,
    and the SDXL-VAE decoder always has to make up new details).


    Given how compressed the latents are, SDXL-VAE actually does an incredibly good
    job at recovering text. SDXL-VAE only struggles when the text is very small:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/95MTLH-p4DbDEQcws55Uh.png)


    You can probably get better small-size letterforms by fine-tuning the VAE decoder
    on text images, like you described (I think SGM [has code for VAE training](https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml))...
    but it won''t fix the fundamental lack of information in the latents (small letterforms
    will still be "made up" during decoding), and it also won''t improve the UNet
    / diffusion process (which is the reason that SDXL generates nonsense text even
    at very large font sizes)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/ys-_J5q0Eta2QGxvG_tdn.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/fZuxamIvRg3UZYUC66wju.png)'
  created_at: 2023-08-16 14:20:01+00:00
  edited: false
  hidden: false
  id: 64dce921c797b87132e5b0e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
      fullname: DS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darshat
      type: user
    createdAt: '2023-08-17T03:11:36.000Z'
    data:
      edited: false
      editors:
      - darshat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7326465845108032
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
          fullname: DS
          isHf: false
          isPro: false
          name: darshat
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;madebyollin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/madebyollin\"\
          >@<span class=\"underline\">madebyollin</span></a></span>\n\n\t</span></span>\
          \ for the comprehensive reply! I will try out the sgm link</p>\n"
        raw: Thanks @madebyollin for the comprehensive reply! I will try out the sgm
          link
        updatedAt: '2023-08-17T03:11:36.674Z'
      numEdits: 0
      reactions: []
    id: 64dd8fe8cd2dc3dc38080bdb
    type: comment
  author: darshat
  content: Thanks @madebyollin for the comprehensive reply! I will try out the sgm
    link
  created_at: 2023-08-17 02:11:36+00:00
  edited: false
  hidden: false
  id: 64dd8fe8cd2dc3dc38080bdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
      fullname: DS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darshat
      type: user
    createdAt: '2023-08-26T06:53:17.000Z'
    data:
      edited: false
      editors:
      - darshat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985046148300171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
          fullname: DS
          isHf: false
          isPro: false
          name: darshat
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;madebyollin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/madebyollin\"\
          >@<span class=\"underline\">madebyollin</span></a></span>\n\n\t</span></span>,\
          \ I'm able to use the sgm scripts to train kl-f4 vae. But the config of\
          \ a vae in sgm repo differs from the config used here for sdxl-vae (autoencoderKL\
          \ class also different). </p>\n<p>The sgm scripts are useful as they define\
          \ the lpips and loss types. Can you share how did you retrain - did you\
          \ use the sgm repo scripts but somehow find a mapping to the sdxl vae class\
          \ and config?</p>\n"
        raw: "Hi @madebyollin, I'm able to use the sgm scripts to train kl-f4 vae.\
          \ But the config of a vae in sgm repo differs from the config used here\
          \ for sdxl-vae (autoencoderKL class also different). \n\nThe sgm scripts\
          \ are useful as they define the lpips and loss types. Can you share how\
          \ did you retrain - did you use the sgm repo scripts but somehow find a\
          \ mapping to the sdxl vae class and config?"
        updatedAt: '2023-08-26T06:53:17.301Z'
      numEdits: 0
      reactions: []
    id: 64e9a15d4b996ef709b5c076
    type: comment
  author: darshat
  content: "Hi @madebyollin, I'm able to use the sgm scripts to train kl-f4 vae. But\
    \ the config of a vae in sgm repo differs from the config used here for sdxl-vae\
    \ (autoencoderKL class also different). \n\nThe sgm scripts are useful as they\
    \ define the lpips and loss types. Can you share how did you retrain - did you\
    \ use the sgm repo scripts but somehow find a mapping to the sdxl vae class and\
    \ config?"
  created_at: 2023-08-26 05:53:17+00:00
  edited: false
  hidden: false
  id: 64e9a15d4b996ef709b5c076
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
      fullname: madebyollin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: madebyollin
      type: user
    createdAt: '2023-08-26T15:03:34.000Z'
    data:
      edited: false
      editors:
      - madebyollin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6730614304542542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
          fullname: madebyollin
          isHf: false
          isPro: false
          name: madebyollin
          type: user
        html: '<p>I trained <code>sdxl-vae-fp16-fix</code> using the SGM <a rel="nofollow"
          href="https://github.com/Stability-AI/generative-models/blob/main/sgm/models/autoencoder.py#L282">AutoencoderKL</a>
          model class (+ my own training notebook), and my trained <a href="https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors">SGM-compatible</a>
          <code>sdxl-vae-fp16-fix</code> weights were <a rel="nofollow" href="https://github.com/huggingface/diffusers/blob/main/scripts/convert_vae_pt_to_diffusers.py">converted</a>
          to diffusers format post-hoc. </p>

          <p>I haven''t personally used Stability''s VAE training code, but their
          <a rel="nofollow" href="https://github.com/Stability-AI/generative-models/blob/main/configs/inference/sd_xl_base.yaml#L80">SDXL
          inference config</a> includes all the VAE settings. You can probably copy
          those VAE settings into the <code>kl-f4</code> <a rel="nofollow" href="https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml#L22">training
          config</a> to get a <code>kl-f8</code> config that works with the <a href="https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors">SGM-compatible</a>
          <code>sdxl-vae-fp16-fix</code> weights.</p>

          '
        raw: "I trained `sdxl-vae-fp16-fix` using the SGM [AutoencoderKL](https://github.com/Stability-AI/generative-models/blob/main/sgm/models/autoencoder.py#L282)\
          \ model class (+ my own training notebook), and my trained [SGM-compatible](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors)\
          \ `sdxl-vae-fp16-fix` weights were [converted](https://github.com/huggingface/diffusers/blob/main/scripts/convert_vae_pt_to_diffusers.py)\
          \ to diffusers format post-hoc. \n\nI haven't personally used Stability's\
          \ VAE training code, but their [SDXL inference config](https://github.com/Stability-AI/generative-models/blob/main/configs/inference/sd_xl_base.yaml#L80)\
          \ includes all the VAE settings. You can probably copy those VAE settings\
          \ into the `kl-f4` [training config](https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml#L22)\
          \ to get a `kl-f8` config that works with the [SGM-compatible](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors)\
          \ `sdxl-vae-fp16-fix` weights."
        updatedAt: '2023-08-26T15:03:34.104Z'
      numEdits: 0
      reactions: []
    id: 64ea14465f3e15f98121aea0
    type: comment
  author: madebyollin
  content: "I trained `sdxl-vae-fp16-fix` using the SGM [AutoencoderKL](https://github.com/Stability-AI/generative-models/blob/main/sgm/models/autoencoder.py#L282)\
    \ model class (+ my own training notebook), and my trained [SGM-compatible](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors)\
    \ `sdxl-vae-fp16-fix` weights were [converted](https://github.com/huggingface/diffusers/blob/main/scripts/convert_vae_pt_to_diffusers.py)\
    \ to diffusers format post-hoc. \n\nI haven't personally used Stability's VAE\
    \ training code, but their [SDXL inference config](https://github.com/Stability-AI/generative-models/blob/main/configs/inference/sd_xl_base.yaml#L80)\
    \ includes all the VAE settings. You can probably copy those VAE settings into\
    \ the `kl-f4` [training config](https://github.com/Stability-AI/generative-models/blob/main/configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml#L22)\
    \ to get a `kl-f8` config that works with the [SGM-compatible](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl_vae.safetensors)\
    \ `sdxl-vae-fp16-fix` weights."
  created_at: 2023-08-26 14:03:34+00:00
  edited: false
  hidden: false
  id: 64ea14465f3e15f98121aea0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f6745c3859cfa6863f6db57307399cd.svg
      fullname: '47'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MkJojo
      type: user
    createdAt: '2023-09-21T15:02:23.000Z'
    data:
      edited: false
      editors:
      - MkJojo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176225662231445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f6745c3859cfa6863f6db57307399cd.svg
          fullname: '47'
          isHf: false
          isPro: false
          name: MkJojo
          type: user
        html: '<p>Please create a repo for that sdxl vae finetuning notebook! Basically
          there is no guide anywhere on how to create a training loop for training
          the sdxl vae. And also what is the dataset have you used for the finetuning
          process. I also need to how you have managed to visualize the parameter
          of the vae in a safetensors file. did you use the state_dict? Also how to
          visualize a latent image without decoding it? man you are really awesome!</p>

          '
        raw: 'Please create a repo for that sdxl vae finetuning notebook! Basically
          there is no guide anywhere on how to create a training loop for training
          the sdxl vae. And also what is the dataset have you used for the finetuning
          process. I also need to how you have managed to visualize the parameter
          of the vae in a safetensors file. did you use the state_dict? Also how to
          visualize a latent image without decoding it? man you are really awesome!

          '
        updatedAt: '2023-09-21T15:02:23.967Z'
      numEdits: 0
      reactions: []
    id: 650c5aff5b868581f92e8fd0
    type: comment
  author: MkJojo
  content: 'Please create a repo for that sdxl vae finetuning notebook! Basically
    there is no guide anywhere on how to create a training loop for training the sdxl
    vae. And also what is the dataset have you used for the finetuning process. I
    also need to how you have managed to visualize the parameter of the vae in a safetensors
    file. did you use the state_dict? Also how to visualize a latent image without
    decoding it? man you are really awesome!

    '
  created_at: 2023-09-21 14:02:23+00:00
  edited: false
  hidden: false
  id: 650c5aff5b868581f92e8fd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
      fullname: madebyollin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: madebyollin
      type: user
    createdAt: '2023-09-23T15:58:31.000Z'
    data:
      edited: false
      editors:
      - madebyollin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7349470853805542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
          fullname: madebyollin
          isHf: false
          isPro: false
          name: madebyollin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MkJojo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MkJojo\">@<span class=\"\
          underline\">MkJojo</span></a></span>\n\n\t</span></span></p>\n<blockquote>\n\
          <p> how you have managed to visualize the parameter of the vae in a safetensors\
          \ file</p>\n</blockquote>\n<p><a rel=\"nofollow\" href=\"https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797\"\
          >Here's</a> the script I've been using to compare my fp16 VAE weights to\
          \ the original VAE weights.</p>\n<blockquote>\n<p>how to visualize a latent\
          \ image without decoding it</p>\n</blockquote>\n<p><a rel=\"nofollow\" href=\"\
          https://gist.github.com/madebyollin/b7eeb57ce56b47051b4fd8ac4f647947\">Here's</a>\
          \ the notebook I used for the comparison image above.</p>\n"
        raw: '@MkJojo


          >  how you have managed to visualize the parameter of the vae in a safetensors
          file


          [Here''s](https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797)
          the script I''ve been using to compare my fp16 VAE weights to the original
          VAE weights.


          > how to visualize a latent image without decoding it


          [Here''s](https://gist.github.com/madebyollin/b7eeb57ce56b47051b4fd8ac4f647947)
          the notebook I used for the comparison image above.'
        updatedAt: '2023-09-23T15:58:31.395Z'
      numEdits: 0
      reactions: []
    id: 650f0b27dcfa9d24c51722b1
    type: comment
  author: madebyollin
  content: '@MkJojo


    >  how you have managed to visualize the parameter of the vae in a safetensors
    file


    [Here''s](https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797)
    the script I''ve been using to compare my fp16 VAE weights to the original VAE
    weights.


    > how to visualize a latent image without decoding it


    [Here''s](https://gist.github.com/madebyollin/b7eeb57ce56b47051b4fd8ac4f647947)
    the notebook I used for the comparison image above.'
  created_at: 2023-09-23 14:58:31+00:00
  edited: false
  hidden: false
  id: 650f0b27dcfa9d24c51722b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
      fullname: DS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darshat
      type: user
    createdAt: '2023-09-24T03:03:28.000Z'
    data:
      edited: false
      editors:
      - darshat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9460803866386414
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5084d2aa4518537b26dca05ce3087b5b.svg
          fullname: DS
          isHf: false
          isPro: false
          name: darshat
          type: user
        html: "<p>I tried the sgm pointer for training but wasnt successful. Posted\
          \ to the sgm [<a rel=\"nofollow\" href=\"https://github.com/Stability-AI/generative-models/issues/121%5D\"\
          >https://github.com/Stability-AI/generative-models/issues/121]</a> issues\
          \ also for help, but no reply so far. Would be very helpful to know how\
          \ you trained it <span data-props=\"{&quot;user&quot;:&quot;madebyollin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/madebyollin\"\
          >@<span class=\"underline\">madebyollin</span></a></span>\n\n\t</span></span>\
          \ .</p>\n"
        raw: I tried the sgm pointer for training but wasnt successful. Posted to
          the sgm [https://github.com/Stability-AI/generative-models/issues/121] issues
          also for help, but no reply so far. Would be very helpful to know how you
          trained it @madebyollin .
        updatedAt: '2023-09-24T03:03:28.284Z'
      numEdits: 0
      reactions: []
    id: 650fa7001765bd51f4c1a283
    type: comment
  author: darshat
  content: I tried the sgm pointer for training but wasnt successful. Posted to the
    sgm [https://github.com/Stability-AI/generative-models/issues/121] issues also
    for help, but no reply so far. Would be very helpful to know how you trained it
    @madebyollin .
  created_at: 2023-09-24 02:03:28+00:00
  edited: false
  hidden: false
  id: 650fa7001765bd51f4c1a283
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: madebyollin/sdxl-vae-fp16-fix
repo_type: model
status: open
target_branch: null
title: retrain vae further
