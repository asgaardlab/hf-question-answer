!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gkorepanov
conflicting_files: null
created_at: 2023-07-11 11:01:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/009140b589810890daa6b0fdf2805da2.svg
      fullname: George Korepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gkorepanov
      type: user
    createdAt: '2023-07-11T12:01:33.000Z'
    data:
      edited: false
      editors:
      - gkorepanov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9378398060798645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/009140b589810890daa6b0fdf2805da2.svg
          fullname: George Korepanov
          isHf: false
          isPro: false
          name: gkorepanov
          type: user
        html: '<p>Hi, can you share the details of how you have fixed the VAE in fp16?</p>

          '
        raw: Hi, can you share the details of how you have fixed the VAE in fp16?
        updatedAt: '2023-07-11T12:01:33.064Z'
      numEdits: 0
      reactions: []
    id: 64ad449dcfb9ae8971eaa04a
    type: comment
  author: gkorepanov
  content: Hi, can you share the details of how you have fixed the VAE in fp16?
  created_at: 2023-07-11 11:01:33+00:00
  edited: false
  hidden: false
  id: 64ad449dcfb9ae8971eaa04a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
      fullname: madebyollin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: madebyollin
      type: user
    createdAt: '2023-07-11T15:36:53.000Z'
    data:
      edited: false
      editors:
      - madebyollin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3882947564125061
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
          fullname: madebyollin
          isHf: false
          isPro: false
          name: madebyollin
          type: user
        html: '<p>Added a section <a href="https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/README.md#details">https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/README.md#details</a></p>

          '
        raw: Added a section https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/README.md#details
        updatedAt: '2023-07-11T15:36:53.117Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gkorepanov
    id: 64ad7715616d3eb361548d53
    type: comment
  author: madebyollin
  content: Added a section https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/README.md#details
  created_at: 2023-07-11 14:36:53+00:00
  edited: false
  hidden: false
  id: 64ad7715616d3eb361548d53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/009140b589810890daa6b0fdf2805da2.svg
      fullname: George Korepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gkorepanov
      type: user
    createdAt: '2023-07-11T17:19:34.000Z'
    data:
      edited: false
      editors:
      - gkorepanov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9690839648246765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/009140b589810890daa6b0fdf2805da2.svg
          fullname: George Korepanov
          isHf: false
          isPro: false
          name: gkorepanov
          type: user
        html: '<p>Thank you, this is a nice work!</p>

          '
        raw: Thank you, this is a nice work!
        updatedAt: '2023-07-11T17:19:34.122Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - madebyollin
      relatedEventId: 64ad8f2650c5936418b83902
    id: 64ad8f2650c5936418b838fd
    type: comment
  author: gkorepanov
  content: Thank you, this is a nice work!
  created_at: 2023-07-11 16:19:34+00:00
  edited: false
  hidden: false
  id: 64ad8f2650c5936418b838fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/009140b589810890daa6b0fdf2805da2.svg
      fullname: George Korepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gkorepanov
      type: user
    createdAt: '2023-07-11T17:19:34.000Z'
    data:
      status: closed
    id: 64ad8f2650c5936418b83902
    type: status-change
  author: gkorepanov
  created_at: 2023-07-11 16:19:34+00:00
  id: 64ad8f2650c5936418b83902
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6dab514b878dda251d33a46d0c0b3477.svg
      fullname: Jakub Sztandera
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kubuxu
      type: user
    createdAt: '2023-07-27T15:27:22.000Z'
    data:
      edited: false
      editors:
      - Kubuxu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9836323857307434
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6dab514b878dda251d33a46d0c0b3477.svg
          fullname: Jakub Sztandera
          isHf: false
          isPro: false
          name: Kubuxu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;madebyollin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/madebyollin\"\
          >@<span class=\"underline\">madebyollin</span></a></span>\n\n\t</span></span>\
          \ would you be willing to share the code (however nice or not it might be)\
          \ of how you did it? I'm interested in learning of how something like this\
          \ is done.</p>\n"
        raw: '@madebyollin would you be willing to share the code (however nice or
          not it might be) of how you did it? I''m interested in learning of how something
          like this is done.'
        updatedAt: '2023-07-27T15:27:22.449Z'
      numEdits: 0
      reactions: []
    id: 64c28cda5176b28ce99ce663
    type: comment
  author: Kubuxu
  content: '@madebyollin would you be willing to share the code (however nice or not
    it might be) of how you did it? I''m interested in learning of how something like
    this is done.'
  created_at: 2023-07-27 14:27:22+00:00
  edited: false
  hidden: false
  id: 64c28cda5176b28ce99ce663
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
      fullname: madebyollin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: madebyollin
      type: user
    createdAt: '2023-07-30T06:18:34.000Z'
    data:
      edited: false
      editors:
      - madebyollin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8360996842384338
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/eWxGBQdUjL3xkk-I_XKwm.png?w=200&h=200&f=face
          fullname: madebyollin
          isHf: false
          isPro: false
          name: madebyollin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Kubuxu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Kubuxu\">@<span class=\"\
          underline\">Kubuxu</span></a></span>\n\n\t</span></span> No code, sorry,\
          \ too messy (+too much of it changed during training).</p>\n<p>Some notes\
          \ on fine-tuning process:</p>\n<ul>\n<li><p>I mostly trained in <code>bfloat16</code>to\
          \ avoid OOM</p>\n</li>\n<li><p>I watched activation-map magnitudes + output\
          \ deltas on a <a rel=\"nofollow\" href=\"https://en.wikipedia.org/wiki/Shiba_Inu#/media/File:Redshibaurajiro.jpg\"\
          >test image</a> and manually rebalanced the match-original-output and make-activation-maps-smaller\
          \ losses occasionally. <a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/iVq4QV3RFV6sI4GNlX78T.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/iVq4QV3RFV6sI4GNlX78T.png\"\
          ></a> <a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/6GQ7WFbiDfDX-ryuW0r1B.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/6GQ7WFbiDfDX-ryuW0r1B.png\"\
          ></a></p>\n</li>\n<li><p>I froze the weight matrices and only fine-tuned\
          \ biases / normalization layers / a single scaler for each weight matrix\
          \ (screenshot for decoder).<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/d3SZbVli5OauTwV7u3OxS.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/d3SZbVli5OauTwV7u3OxS.png\"\
          ></a></p>\n</li>\n</ul>\n<p>Some speculation on what might have happened\
          \ to the original VAE:</p>\n<ul>\n<li>I think Stability zero-initialized\
          \ final convs in <a rel=\"nofollow\" href=\"https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/openaimodel.py#L290\"\
          >UNet resblocks</a>, but not <a rel=\"nofollow\" href=\"https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/model.py#L115\"\
          >VAE resblocks</a>, which I think leads to variance increasing with VAE\
          \ depth (per FixUp / ReZero papers)</li>\n<li>I think Stability initialized\
          \ up / down convs with too-big weights (default PyTorch initialization assumes\
          \ a ReLU-like nonlinearity afterwards, but these convs have no nonlinearity),\
          \ which I think will increase variance after each of these convs (per He\
          \ initialization paper)</li>\n<li>If you <a rel=\"nofollow\" href=\"https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797\"\
          >compare</a> the original / fixed weights, resblock final convs and up /\
          \ down convs are mostly what shrunk, which seems like weak evidence in favor\
          \ of these weights being too large initially. <a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/53Vv0qAUg8VON6_NRznsd.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/53Vv0qAUg8VON6_NRznsd.png\"\
          ></a></li>\n</ul>\n<p>That is all speculation though - I don't thoroughly\
          \ understand the issue yet. I just threw some code together and happened\
          \ to get it working :)</p>\n"
        raw: '@Kubuxu No code, sorry, too messy (+too much of it changed during training).


          Some notes on fine-tuning process:


          * I mostly trained in `bfloat16`to avoid OOM


          * I watched activation-map magnitudes + output deltas on a [test image](https://en.wikipedia.org/wiki/Shiba_Inu#/media/File:Redshibaurajiro.jpg)
          and manually rebalanced the match-original-output and make-activation-maps-smaller
          losses occasionally. ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/iVq4QV3RFV6sI4GNlX78T.png)
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/6GQ7WFbiDfDX-ryuW0r1B.png)


          * I froze the weight matrices and only fine-tuned biases / normalization
          layers / a single scaler for each weight matrix (screenshot for decoder).

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/d3SZbVli5OauTwV7u3OxS.png)


          Some speculation on what might have happened to the original VAE:


          * I think Stability zero-initialized final convs in [UNet resblocks](https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/openaimodel.py#L290),
          but not [VAE resblocks](https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/model.py#L115),
          which I think leads to variance increasing with VAE depth (per FixUp / ReZero
          papers)

          *  I think Stability initialized up / down convs with too-big weights (default
          PyTorch initialization assumes a ReLU-like nonlinearity afterwards, but
          these convs have no nonlinearity), which I think will increase variance
          after each of these convs (per He initialization paper)

          * If you [compare](https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797)
          the original / fixed weights, resblock final convs and up / down convs are
          mostly what shrunk, which seems like weak evidence in favor of these weights
          being too large initially. ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/53Vv0qAUg8VON6_NRznsd.png)


          That is all speculation though - I don''t thoroughly understand the issue
          yet. I just threw some code together and happened to get it working :)'
        updatedAt: '2023-07-30T06:18:34.805Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Kubuxu
    id: 64c600bac097c6c2b3bb10b5
    type: comment
  author: madebyollin
  content: '@Kubuxu No code, sorry, too messy (+too much of it changed during training).


    Some notes on fine-tuning process:


    * I mostly trained in `bfloat16`to avoid OOM


    * I watched activation-map magnitudes + output deltas on a [test image](https://en.wikipedia.org/wiki/Shiba_Inu#/media/File:Redshibaurajiro.jpg)
    and manually rebalanced the match-original-output and make-activation-maps-smaller
    losses occasionally. ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/iVq4QV3RFV6sI4GNlX78T.png)
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/6GQ7WFbiDfDX-ryuW0r1B.png)


    * I froze the weight matrices and only fine-tuned biases / normalization layers
    / a single scaler for each weight matrix (screenshot for decoder).

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/d3SZbVli5OauTwV7u3OxS.png)


    Some speculation on what might have happened to the original VAE:


    * I think Stability zero-initialized final convs in [UNet resblocks](https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/openaimodel.py#L290),
    but not [VAE resblocks](https://github.com/Stability-AI/generative-models/blob/45c443b316737a4ab6e40413d7794a7f5657c19f/sgm/modules/diffusionmodules/model.py#L115),
    which I think leads to variance increasing with VAE depth (per FixUp / ReZero
    papers)

    *  I think Stability initialized up / down convs with too-big weights (default
    PyTorch initialization assumes a ReLU-like nonlinearity afterwards, but these
    convs have no nonlinearity), which I think will increase variance after each of
    these convs (per He initialization paper)

    * If you [compare](https://gist.github.com/madebyollin/034afe6670fc03966d075912cbccf797)
    the original / fixed weights, resblock final convs and up / down convs are mostly
    what shrunk, which seems like weak evidence in favor of these weights being too
    large initially. ![image.png](https://cdn-uploads.huggingface.co/production/uploads/630447d40547362a22a969a2/53Vv0qAUg8VON6_NRznsd.png)


    That is all speculation though - I don''t thoroughly understand the issue yet.
    I just threw some code together and happened to get it working :)'
  created_at: 2023-07-30 05:18:34+00:00
  edited: false
  hidden: false
  id: 64c600bac097c6c2b3bb10b5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: madebyollin/sdxl-vae-fp16-fix
repo_type: model
status: closed
target_branch: null
title: How is this achieved?
