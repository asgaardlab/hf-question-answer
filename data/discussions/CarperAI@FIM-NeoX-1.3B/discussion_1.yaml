!!python/object:huggingface_hub.community.DiscussionWithDetails
author: narphorium
conflicting_files: null
created_at: 2022-10-08 00:36:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35f92bea6fe3f214abf971e28bc36e5b.svg
      fullname: Shawn Simister
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: narphorium
      type: user
    createdAt: '2022-10-08T01:36:52.000Z'
    data:
      edited: true
      editors:
      - narphorium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35f92bea6fe3f214abf971e28bc36e5b.svg
          fullname: Shawn Simister
          isHf: false
          isPro: false
          name: narphorium
          type: user
        html: '<p>According to the model card, the way to do infilling is to pass
          in the input as :</p>

          <p><code>&lt;SUF&gt; {some text following cursor} &lt;PRE&gt; {some prelude
          text here} &lt;MID&gt;</code></p>

          <p>In the example code, the special token IDs are specified as:</p>

          <p> <code>&lt;SUF&gt;</code> = 50253<br><code>&lt;PRE&gt;</code> = 50254<br><code>&lt;MID&gt;</code>
          = 50255</p>

          <p>However, when I generate completions using those tokens I haven''t been
          able to get any accurate results. For example:</p>

          <pre><code class="language-python">prefix = <span class="hljs-string">"def
          top_k(values):\n"</span>

          suffix = <span class="hljs-string">"  return results"</span>

          </code></pre>

          <p>... infills as:</p>

          <pre><code class="language-python"><span class="hljs-keyword">def</span>
          <span class="hljs-title function_">top_k</span>(<span class="hljs-params">values</span>):

          <span class="hljs-keyword">return</span> results.count(values  <span class="hljs-keyword">return</span>
          results

          </code></pre>

          <p>This looks like the suffix is being ignored and the model is just completing
          after the prefix. </p>

          <p>When I decode the special tokens back to text I get:</p>

          <p>50253 = <code>'' Outcomes''</code><br>50254 = 24 spaces<br>50255 = 23
          spaces</p>

          <p>So I''m wondering if those are really the correct tokens to separate
          the FIM inputs?</p>

          '
        raw: "According to the model card, the way to do infilling is to pass in the\
          \ input as :\n\n`<SUF> {some text following cursor} <PRE> {some prelude\
          \ text here} <MID>`\n\nIn the example code, the special token IDs are specified\
          \ as:\n\n `<SUF>` = 50253\n`<PRE>` = 50254\n`<MID>` = 50255\n\nHowever,\
          \ when I generate completions using those tokens I haven't been able to\
          \ get any accurate results. For example:\n\n```python\nprefix = \"def top_k(values):\\\
          n\"\nsuffix = \"  return results\"\n```\n\n... infills as:\n\n```python\n\
          def top_k(values):\nreturn results.count(values  return results\n```\n\n\
          This looks like the suffix is being ignored and the model is just completing\
          \ after the prefix. \n\nWhen I decode the special tokens back to text I\
          \ get:\n\n50253 = `' Outcomes'`\n50254 = 24 spaces\n50255 = 23 spaces\n\n\
          So I'm wondering if those are really the correct tokens to separate the\
          \ FIM inputs?"
        updatedAt: '2022-10-08T01:42:07.680Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sradc
    id: 6340d434b78ed99eab0370ee
    type: comment
  author: narphorium
  content: "According to the model card, the way to do infilling is to pass in the\
    \ input as :\n\n`<SUF> {some text following cursor} <PRE> {some prelude text here}\
    \ <MID>`\n\nIn the example code, the special token IDs are specified as:\n\n `<SUF>`\
    \ = 50253\n`<PRE>` = 50254\n`<MID>` = 50255\n\nHowever, when I generate completions\
    \ using those tokens I haven't been able to get any accurate results. For example:\n\
    \n```python\nprefix = \"def top_k(values):\\n\"\nsuffix = \"  return results\"\
    \n```\n\n... infills as:\n\n```python\ndef top_k(values):\nreturn results.count(values\
    \  return results\n```\n\nThis looks like the suffix is being ignored and the\
    \ model is just completing after the prefix. \n\nWhen I decode the special tokens\
    \ back to text I get:\n\n50253 = `' Outcomes'`\n50254 = 24 spaces\n50255 = 23\
    \ spaces\n\nSo I'm wondering if those are really the correct tokens to separate\
    \ the FIM inputs?"
  created_at: 2022-10-08 00:36:52+00:00
  edited: true
  hidden: false
  id: 6340d434b78ed99eab0370ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670406191557-625079fe5bf543dbd26048f8.jpeg?w=200&h=200&f=face
      fullname: Florian Jenett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fjenett
      type: user
    createdAt: '2022-10-08T11:06:20.000Z'
    data:
      edited: false
      editors:
      - fjenett
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670406191557-625079fe5bf543dbd26048f8.jpeg?w=200&h=200&f=face
          fullname: Florian Jenett
          isHf: false
          isPro: false
          name: fjenett
          type: user
        html: '<p>+1</p>

          '
        raw: '+1'
        updatedAt: '2022-10-08T11:06:20.195Z'
      numEdits: 0
      reactions: []
    id: 634159ace5071b0c5ccaacd7
    type: comment
  author: fjenett
  content: '+1'
  created_at: 2022-10-08 10:06:20+00:00
  edited: false
  hidden: false
  id: 634159ace5071b0c5ccaacd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669665010552-62895a0215aeee85756062c4.jpeg?w=200&h=200&f=face
      fullname: Hailey Schoelkopf
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hails
      type: user
    createdAt: '2022-10-08T12:18:39.000Z'
    data:
      edited: false
      editors:
      - hails
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669665010552-62895a0215aeee85756062c4.jpeg?w=200&h=200&f=face
          fullname: Hailey Schoelkopf
          isHf: false
          isPro: false
          name: hails
          type: user
        html: '<p>thanks for bringing this to our attention! Looking into this and
          will get back to you asap.</p>

          '
        raw: thanks for bringing this to our attention! Looking into this and will
          get back to you asap.
        updatedAt: '2022-10-08T12:18:39.686Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - narphorium
        - sradc
        - BigSalmon
    id: 63416a9fc5565a4b8d433c05
    type: comment
  author: hails
  content: thanks for bringing this to our attention! Looking into this and will get
    back to you asap.
  created_at: 2022-10-08 11:18:39+00:00
  edited: false
  hidden: false
  id: 63416a9fc5565a4b8d433c05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665155571839-60d35a0307da9c17c7270909.jpeg?w=200&h=200&f=face
      fullname: Louis Castricato
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: LouisCastricato
      type: user
    createdAt: '2022-10-08T14:52:57.000Z'
    data:
      edited: false
      editors:
      - LouisCastricato
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665155571839-60d35a0307da9c17c7270909.jpeg?w=200&h=200&f=face
          fullname: Louis Castricato
          isHf: false
          isPro: false
          name: LouisCastricato
          type: user
        html: '<p>Thank you for raising this concern. It seems like it''s an issue
          with the tokenizer. Unfortunately all of our engineers are OOO for the long
          weekend, we should have a patch out Tuesday or Wednesday. Thanks.</p>

          '
        raw: Thank you for raising this concern. It seems like it's an issue with
          the tokenizer. Unfortunately all of our engineers are OOO for the long weekend,
          we should have a patch out Tuesday or Wednesday. Thanks.
        updatedAt: '2022-10-08T14:52:57.128Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - narphorium
        - sradc
        - BigSalmon
    id: 63418ec9c5565a4b8d44e3b6
    type: comment
  author: LouisCastricato
  content: Thank you for raising this concern. It seems like it's an issue with the
    tokenizer. Unfortunately all of our engineers are OOO for the long weekend, we
    should have a patch out Tuesday or Wednesday. Thanks.
  created_at: 2022-10-08 13:52:57+00:00
  edited: false
  hidden: false
  id: 63418ec9c5565a4b8d44e3b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669665010552-62895a0215aeee85756062c4.jpeg?w=200&h=200&f=face
      fullname: Hailey Schoelkopf
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hails
      type: user
    createdAt: '2022-10-11T14:04:37.000Z'
    data:
      edited: false
      editors:
      - hails
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669665010552-62895a0215aeee85756062c4.jpeg?w=200&h=200&f=face
          fullname: Hailey Schoelkopf
          isHf: false
          isPro: false
          name: hails
          type: user
        html: "<p>There was an issue where the sentinel <code>&lt;|SUF|&gt;</code>,\
          \ <code>&lt;|PRE|&gt;</code>, and <code>&lt;|MID|&gt;</code> tokens were\
          \ not the correct ids in the uploaded tokenizer and model card! Please try\
          \ clearing the Huggingface cache and redownloading the model :))</p>\n<p>This\
          \ is what I get, attempting to try out open-ended generation on a simple\
          \ code function </p>\n<pre><code>def score(x,y) -&gt; int:\n    \"\"\"\n\
          \    \n</code></pre>\n<p>and also infilling with </p>\n<pre><code>def score(x,y)\
          \ -&gt; int:\n    \"\"\"\n    &lt;|MID|&gt; (infill here)\n    \"\"\"\n\n\
          \    score = x + y\n    return score\n</code></pre>\n<pre><code>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          CarperAI/FIM-NeoX-1.3B\")\ntok = AutoTokenizer.from_pretrained(\"CarperAI/\n\
          \n# infilling demo\nprefix = 'def score(x, y) -&gt; int:\\n\"\"\"\\n'\n\
          suffix = '\"\"\"\\n\\n    score = x + y\\n    return score'\n\n model_input\
          \ = [50277, *tok(suffix)[\"input_ids\"], 50278, *tok(prefix)[\"input_ids\"\
          ], 50279]\n output = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0),\
          \ max_length=40)[0])\n\nprint(output)\n</code></pre>\n<blockquote>\n<p>\
          \ '&lt;|SUF|&gt;\"\"\"\\n\\n    score = x + y\\n    return score&lt;|PRE|&gt;def\
          \ score(x, y) -&gt; int:\\n\"\"\"\\n&lt;|MID|&gt;    score(x, y) -&gt; int\\\
          n&lt;|endoftext|&gt;'</p>\n</blockquote>\n<pre><code>from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# non-infilling demo\n\
          prefix = 'def score(x, y) -&gt; int:\\n\"\"\"\\n'\nmodel_input = [*tok(prefix)[\"\
          input_ids\"]]\noutput = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0),\
          \ max_length=100)[0])\nprint(output)\n</code></pre>\n<blockquote>\n<p>'def\
          \ score(x, y) -&gt; int:\\n\"\"\"\\n    Return the score of the given point.\\\
          n    \"\"\"\\n    return sum(x * y for x, y in zip(x_list, y_list))\\n\\\
          ndef get_point_score(x, y) -&gt; int:\\n    \"\"\"\\n    Return the score\
          \ of the given point.\\n    \"\"\"\\n    return sum(x * y for x, y in zip(x_list,\
          \ y'</p>\n</blockquote>\n<p>Hope this helps! I will also update the model\
          \ card with this example :)</p>\n"
        raw: "There was an issue where the sentinel `<|SUF|>`, `<|PRE|>`, and `<|MID|>`\
          \ tokens were not the correct ids in the uploaded tokenizer and model card!\
          \ Please try clearing the Huggingface cache and redownloading the model\
          \ :))\n\nThis is what I get, attempting to try out open-ended generation\
          \ on a simple code function \n```\ndef score(x,y) -> int:\n    \"\"\"\n\
          \    \n```\n\nand also infilling with \n\n```\ndef score(x,y) -> int:\n\
          \    \"\"\"\n    <|MID|> (infill here)\n    \"\"\"\n\n    score = x + y\n\
          \    return score\n```\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"CarperAI/FIM-NeoX-1.3B\"\
          )\ntok = AutoTokenizer.from_pretrained(\"CarperAI/\n\n# infilling demo\n\
          prefix = 'def score(x, y) -> int:\\n\"\"\"\\n'\nsuffix = '\"\"\"\\n\\n \
          \   score = x + y\\n    return score'\n\n model_input = [50277, *tok(suffix)[\"\
          input_ids\"], 50278, *tok(prefix)[\"input_ids\"], 50279]\n output = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0),\
          \ max_length=40)[0])\n\nprint(output)\n```\n>  '<|SUF|>\"\"\"\\n\\n    score\
          \ = x + y\\n    return score<|PRE|>def score(x, y) -> int:\\n\"\"\"\\n<|MID|>\
          \    score(x, y) -> int\\n<|endoftext|>'\n\n```\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# non-infilling demo\n\
          prefix = 'def score(x, y) -> int:\\n\"\"\"\\n'\nmodel_input = [*tok(prefix)[\"\
          input_ids\"]]\noutput = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0),\
          \ max_length=100)[0])\nprint(output)\n```\n> 'def score(x, y) -> int:\\\
          n\"\"\"\\n    Return the score of the given point.\\n    \"\"\"\\n    return\
          \ sum(x * y for x, y in zip(x_list, y_list))\\n\\ndef get_point_score(x,\
          \ y) -> int:\\n    \"\"\"\\n    Return the score of the given point.\\n\
          \    \"\"\"\\n    return sum(x * y for x, y in zip(x_list, y'\n\nHope this\
          \ helps! I will also update the model card with this example :)"
        updatedAt: '2022-10-11T14:04:37.184Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F917"
        users:
        - narphorium
        - BigSalmon
        - machine-learnoooooooor
        - ncoop57
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - BigSalmon
        - LouisCastricato
    id: 634577f5ac1cb29fb2a65a96
    type: comment
  author: hails
  content: "There was an issue where the sentinel `<|SUF|>`, `<|PRE|>`, and `<|MID|>`\
    \ tokens were not the correct ids in the uploaded tokenizer and model card! Please\
    \ try clearing the Huggingface cache and redownloading the model :))\n\nThis is\
    \ what I get, attempting to try out open-ended generation on a simple code function\
    \ \n```\ndef score(x,y) -> int:\n    \"\"\"\n    \n```\n\nand also infilling with\
    \ \n\n```\ndef score(x,y) -> int:\n    \"\"\"\n    <|MID|> (infill here)\n   \
    \ \"\"\"\n\n    score = x + y\n    return score\n```\n\n```\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    CarperAI/FIM-NeoX-1.3B\")\ntok = AutoTokenizer.from_pretrained(\"CarperAI/\n\n\
    # infilling demo\nprefix = 'def score(x, y) -> int:\\n\"\"\"\\n'\nsuffix = '\"\
    \"\"\\n\\n    score = x + y\\n    return score'\n\n model_input = [50277, *tok(suffix)[\"\
    input_ids\"], 50278, *tok(prefix)[\"input_ids\"], 50279]\n output = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0),\
    \ max_length=40)[0])\n\nprint(output)\n```\n>  '<|SUF|>\"\"\"\\n\\n    score =\
    \ x + y\\n    return score<|PRE|>def score(x, y) -> int:\\n\"\"\"\\n<|MID|>  \
    \  score(x, y) -> int\\n<|endoftext|>'\n\n```\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\nimport torch\n\n# non-infilling demo\nprefix = 'def score(x,\
    \ y) -> int:\\n\"\"\"\\n'\nmodel_input = [*tok(prefix)[\"input_ids\"]]\noutput\
    \ = tok.decode(model.generate(torch.IntTensor(model_input).unsqueeze(0), max_length=100)[0])\n\
    print(output)\n```\n> 'def score(x, y) -> int:\\n\"\"\"\\n    Return the score\
    \ of the given point.\\n    \"\"\"\\n    return sum(x * y for x, y in zip(x_list,\
    \ y_list))\\n\\ndef get_point_score(x, y) -> int:\\n    \"\"\"\\n    Return the\
    \ score of the given point.\\n    \"\"\"\\n    return sum(x * y for x, y in zip(x_list,\
    \ y'\n\nHope this helps! I will also update the model card with this example :)"
  created_at: 2022-10-11 13:04:37+00:00
  edited: false
  hidden: false
  id: 634577f5ac1cb29fb2a65a96
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: CarperAI/FIM-NeoX-1.3B
repo_type: model
status: open
target_branch: null
title: Unable to get accurate infilling
