!!python/object:huggingface_hub.community.DiscussionWithDetails
author: blackwingedkite
conflicting_files: null
created_at: 2023-08-11 05:05:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4630c497160b26d65dcc77b1c57b7e2b.svg
      fullname: "\u67EF\u5BA5\u573B"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blackwingedkite
      type: user
    createdAt: '2023-08-11T06:05:01.000Z'
    data:
      edited: false
      editors:
      - blackwingedkite
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9406496286392212
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4630c497160b26d65dcc77b1c57b7e2b.svg
          fullname: "\u67EF\u5BA5\u573B"
          isHf: false
          isPro: false
          name: blackwingedkite
          type: user
        html: '<p>Sorry, I am an user from Taiwan, and I am really excited to see
          the bloom3b-zh-instruct have released. However, when I would like to try
          the model on Hosted inference API, here is the error code:</p>

          <p>Can''t load tokenizer using from_pretrained, please update its configuration:
          Can''t load tokenizer for ''ckip-joint/bloom-3b-zh-instruct''. If you were
          trying to load it from ''<a href="https://huggingface.co/models''">https://huggingface.co/models''</a>,
          make sure you don''t have a local directory with the same name. Otherwise,
          make sure ''ckip-joint/bloom-3b-zh-instruct'' is the correct path to a directory
          containing all relevant files for a BloomTokenizerFast tokenizer.</p>

          <p>I would like to figure out what happened and if it can be resolved, since
          I can''t download the model to my computer since I have only 8GB RAM, thanks.</p>

          '
        raw: "Sorry, I am an user from Taiwan, and I am really excited to see the\
          \ bloom3b-zh-instruct have released. However, when I would like to try the\
          \ model on Hosted inference API, here is the error code:\r\n\r\nCan't load\
          \ tokenizer using from_pretrained, please update its configuration: Can't\
          \ load tokenizer for 'ckip-joint/bloom-3b-zh-instruct'. If you were trying\
          \ to load it from 'https://huggingface.co/models', make sure you don't have\
          \ a local directory with the same name. Otherwise, make sure 'ckip-joint/bloom-3b-zh-instruct'\
          \ is the correct path to a directory containing all relevant files for a\
          \ BloomTokenizerFast tokenizer.\r\n\r\nI would like to figure out what happened\
          \ and if it can be resolved, since I can't download the model to my computer\
          \ since I have only 8GB RAM, thanks."
        updatedAt: '2023-08-11T06:05:01.670Z'
      numEdits: 0
      reactions: []
    id: 64d5cf8d0d87929689e5172c
    type: comment
  author: blackwingedkite
  content: "Sorry, I am an user from Taiwan, and I am really excited to see the bloom3b-zh-instruct\
    \ have released. However, when I would like to try the model on Hosted inference\
    \ API, here is the error code:\r\n\r\nCan't load tokenizer using from_pretrained,\
    \ please update its configuration: Can't load tokenizer for 'ckip-joint/bloom-3b-zh-instruct'.\
    \ If you were trying to load it from 'https://huggingface.co/models', make sure\
    \ you don't have a local directory with the same name. Otherwise, make sure 'ckip-joint/bloom-3b-zh-instruct'\
    \ is the correct path to a directory containing all relevant files for a BloomTokenizerFast\
    \ tokenizer.\r\n\r\nI would like to figure out what happened and if it can be\
    \ resolved, since I can't download the model to my computer since I have only\
    \ 8GB RAM, thanks."
  created_at: 2023-08-11 05:05:01+00:00
  edited: false
  hidden: false
  id: 64d5cf8d0d87929689e5172c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6705c8ab5c96da55643fae68eab276a5.svg
      fullname: wenhaochan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tarklanse
      type: user
    createdAt: '2023-09-11T06:26:16.000Z'
    data:
      edited: false
      editors:
      - Tarklanse
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.2398288995027542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6705c8ab5c96da55643fae68eab276a5.svg
          fullname: wenhaochan
          isHf: false
          isPro: false
          name: Tarklanse
          type: user
        html: "<p>Hello,\u56E0\u70BA\u5C08\u6848\u5E95\u4E0B\u6C92\u6709\u63D0\u4F9B\
          tokenizer\uFF0C\u6211\u5617\u8A66\u76F4\u63A5\u62FFbloom-3b-zh\u7684tokenizer\u57F7\
          \u884C\u662F\u80FD\u5920\u7522\u5B57\u7684<br>\u4F60\u53EF\u4EE5\u5617\u8A66\
          \u57F7\u884C\u4EE5\u4E0B\u7A0B\u5F0F\u78BC\uFF0C\u53BB\u8B80\u53D6\u6A21\
          \u578B\u4E26\u7522\u751F\u6587\u5B57</p>\n<p>from transformers import AutoModelForCausalLM<br>from\
          \ transformers import AutoTokenizer</p>\n<p>#\u8A9E\u8A00\u6A21\u578B\u7684\
          \u8DEF\u5F91<br>model_path=\"/content/bloom-3b-zh-instruct\"<br>#\u8B80\u53D6\
          \u8A9E\u8A00\u6A21\u578B<br>model = AutoModelForCausalLM.from_pretrained(model_path\
          \ ,device_map= \"auto\", local_files_only=True)<br>#\u8B80\u53D6\u8A9E\u8A00\
          \u6A21\u578B\u5206\u8A5E\u5668<br>tokenizer = AutoTokenizer.from_pretrained(\"\
          ckip-joint/bloom-3b-zh\")<br>#\u6E96\u5099\u8F38\u5165<br>model_inputs =\
          \ tokenizer([\"\u4F60\u597D\uFF0C\u6211\u662F\"], return_tensors=\"pt\"\
          ).to(\"cuda\")<br>#\u6A21\u578B\u7522\u51FA<br>generated_ids = model.generate(**model_inputs,\
          \ max_new_tokens=20)<br>#\u89E3\u6790\u6A21\u578B\u7522\u51FA<br>tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)[0]</p>\n<p>\u7D50\u679C\u793A\u610F:<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63744314b2484d1af704cc28/qjbTtBKqXE2E7BLwGmLSw.png\"\
          ><img alt=\"bloom3B.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63744314b2484d1af704cc28/qjbTtBKqXE2E7BLwGmLSw.png\"\
          ></a></p>\n<p>\u5E0C\u671B\u9019\u5C0D\u4F60\u6709\u5E6B\u52A9</p>\n"
        raw: "Hello,\u56E0\u70BA\u5C08\u6848\u5E95\u4E0B\u6C92\u6709\u63D0\u4F9Btokenizer\uFF0C\
          \u6211\u5617\u8A66\u76F4\u63A5\u62FFbloom-3b-zh\u7684tokenizer\u57F7\u884C\
          \u662F\u80FD\u5920\u7522\u5B57\u7684\n\u4F60\u53EF\u4EE5\u5617\u8A66\u57F7\
          \u884C\u4EE5\u4E0B\u7A0B\u5F0F\u78BC\uFF0C\u53BB\u8B80\u53D6\u6A21\u578B\
          \u4E26\u7522\u751F\u6587\u5B57\n\nfrom transformers import AutoModelForCausalLM\n\
          from transformers import AutoTokenizer\n\n#\u8A9E\u8A00\u6A21\u578B\u7684\
          \u8DEF\u5F91\nmodel_path=\"/content/bloom-3b-zh-instruct\"\n#\u8B80\u53D6\
          \u8A9E\u8A00\u6A21\u578B\nmodel = AutoModelForCausalLM.from_pretrained(model_path\
          \ ,device_map= \"auto\", local_files_only=True)\n#\u8B80\u53D6\u8A9E\u8A00\
          \u6A21\u578B\u5206\u8A5E\u5668\ntokenizer = AutoTokenizer.from_pretrained(\"\
          ckip-joint/bloom-3b-zh\")\n#\u6E96\u5099\u8F38\u5165\nmodel_inputs = tokenizer([\"\
          \u4F60\u597D\uFF0C\u6211\u662F\"], return_tensors=\"pt\").to(\"cuda\")\n\
          #\u6A21\u578B\u7522\u51FA\ngenerated_ids = model.generate(**model_inputs,\
          \ max_new_tokens=20)\n#\u89E3\u6790\u6A21\u578B\u7522\u51FA\ntokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)[0]\n\n\u7D50\u679C\u793A\u610F:\n![bloom3B.PNG](https://cdn-uploads.huggingface.co/production/uploads/63744314b2484d1af704cc28/qjbTtBKqXE2E7BLwGmLSw.png)\n\
          \n\u5E0C\u671B\u9019\u5C0D\u4F60\u6709\u5E6B\u52A9"
        updatedAt: '2023-09-11T06:26:16.911Z'
      numEdits: 0
      reactions: []
    id: 64feb3082a6d4e23be54d6c0
    type: comment
  author: Tarklanse
  content: "Hello,\u56E0\u70BA\u5C08\u6848\u5E95\u4E0B\u6C92\u6709\u63D0\u4F9Btokenizer\uFF0C\
    \u6211\u5617\u8A66\u76F4\u63A5\u62FFbloom-3b-zh\u7684tokenizer\u57F7\u884C\u662F\
    \u80FD\u5920\u7522\u5B57\u7684\n\u4F60\u53EF\u4EE5\u5617\u8A66\u57F7\u884C\u4EE5\
    \u4E0B\u7A0B\u5F0F\u78BC\uFF0C\u53BB\u8B80\u53D6\u6A21\u578B\u4E26\u7522\u751F\
    \u6587\u5B57\n\nfrom transformers import AutoModelForCausalLM\nfrom transformers\
    \ import AutoTokenizer\n\n#\u8A9E\u8A00\u6A21\u578B\u7684\u8DEF\u5F91\nmodel_path=\"\
    /content/bloom-3b-zh-instruct\"\n#\u8B80\u53D6\u8A9E\u8A00\u6A21\u578B\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model_path ,device_map= \"auto\", local_files_only=True)\n\
    #\u8B80\u53D6\u8A9E\u8A00\u6A21\u578B\u5206\u8A5E\u5668\ntokenizer = AutoTokenizer.from_pretrained(\"\
    ckip-joint/bloom-3b-zh\")\n#\u6E96\u5099\u8F38\u5165\nmodel_inputs = tokenizer([\"\
    \u4F60\u597D\uFF0C\u6211\u662F\"], return_tensors=\"pt\").to(\"cuda\")\n#\u6A21\
    \u578B\u7522\u51FA\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=20)\n\
    #\u89E3\u6790\u6A21\u578B\u7522\u51FA\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
    \n\u7D50\u679C\u793A\u610F:\n![bloom3B.PNG](https://cdn-uploads.huggingface.co/production/uploads/63744314b2484d1af704cc28/qjbTtBKqXE2E7BLwGmLSw.png)\n\
    \n\u5E0C\u671B\u9019\u5C0D\u4F60\u6709\u5E6B\u52A9"
  created_at: 2023-09-11 05:26:16+00:00
  edited: false
  hidden: false
  id: 64feb3082a6d4e23be54d6c0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: ckip-joint/bloom-3b-zh-instruct
repo_type: model
status: open
target_branch: null
title: Hosted inference API
