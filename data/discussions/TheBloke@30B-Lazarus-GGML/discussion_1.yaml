!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chenxiangyi10
conflicting_files: null
created_at: 2023-06-15 18:55:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
      fullname: Paul CHEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxiangyi10
      type: user
    createdAt: '2023-06-15T19:55:51.000Z'
    data:
      edited: false
      editors:
      - chenxiangyi10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7402817606925964
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
          fullname: Paul CHEN
          isHf: false
          isPro: false
          name: chenxiangyi10
          type: user
        html: '<p>Thank you for providing this great model.</p>

          <p>In a <a href="https://huggingface.co/TheBloke/30B-Lazarus-GGML#provided-files">table</a>
          of the model card, it gives the max ram required. If I offload the model
          to GPU, will it require the same amount of VRAM?</p>

          <p>And does the "max ram required" mean the ram required to use the context
          length (2048) fully?</p>

          '
        raw: "Thank you for providing this great model.\r\n\r\nIn a [table](https://huggingface.co/TheBloke/30B-Lazarus-GGML#provided-files)\
          \ of the model card, it gives the max ram required. If I offload the model\
          \ to GPU, will it require the same amount of VRAM?\r\n\r\nAnd does the \"\
          max ram required\" mean the ram required to use the context length (2048)\
          \ fully?\r\n"
        updatedAt: '2023-06-15T19:55:51.118Z'
      numEdits: 0
      reactions: []
    id: 648b6cc7769aeaccf46b3d0e
    type: comment
  author: chenxiangyi10
  content: "Thank you for providing this great model.\r\n\r\nIn a [table](https://huggingface.co/TheBloke/30B-Lazarus-GGML#provided-files)\
    \ of the model card, it gives the max ram required. If I offload the model to\
    \ GPU, will it require the same amount of VRAM?\r\n\r\nAnd does the \"max ram\
    \ required\" mean the ram required to use the context length (2048) fully?\r\n"
  created_at: 2023-06-15 18:55:51+00:00
  edited: false
  hidden: false
  id: 648b6cc7769aeaccf46b3d0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-15T20:01:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9583261609077454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome.</p>

          <p>If you offload to GPU, yes it will need roughly as much VRAM as you see
          there in the RAM table.  In other words if it says it needs 18GB RAM and
          you offload all layers to VRAM, it will need somewhere 18GB VRAM. Actually
          probably a bit more than that, especially if you''re using the latest llama.cpp
          code which now has full GPU acceleration.  </p>

          <p>If you do offload to VRAM, you won''t then need as much RAM.  That is
          why it says "max RAM required", because that''s the amount of RAM that''s
          needed if you don''t offload to GPU at all.  If you fully offloaded to GPU,
          it would only need about 3GB RAM.  If you offloaded half the layers, then
          it would need half the RAM, and the other half in VRAM.  And so on.</p>

          <p>With llama.cpp/GGML, RAM usage does not really change as context gets
          longer. So these RAM figures are for 2048 context, but also 500 context.  It
          grows a little bit at the start, but it''s not like GPTQ where VRAM usage
          keeps growing as context increases.</p>

          '
        raw: "You're welcome.\n\nIf you offload to GPU, yes it will need roughly as\
          \ much VRAM as you see there in the RAM table.  In other words if it says\
          \ it needs 18GB RAM and you offload all layers to VRAM, it will need somewhere\
          \ 18GB VRAM. Actually probably a bit more than that, especially if you're\
          \ using the latest llama.cpp code which now has full GPU acceleration. \
          \ \n\nIf you do offload to VRAM, you won't then need as much RAM.  That\
          \ is why it says \"max RAM required\", because that's the amount of RAM\
          \ that's needed if you don't offload to GPU at all.  If you fully offloaded\
          \ to GPU, it would only need about 3GB RAM.  If you offloaded half the layers,\
          \ then it would need half the RAM, and the other half in VRAM.  And so on.\n\
          \nWith llama.cpp/GGML, RAM usage does not really change as context gets\
          \ longer. So these RAM figures are for 2048 context, but also 500 context.\
          \  It grows a little bit at the start, but it's not like GPTQ where VRAM\
          \ usage keeps growing as context increases."
        updatedAt: '2023-06-15T20:01:18.278Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - chenxiangyi10
        - phi0112358
        - TravelingMan
        - mikeee
    id: 648b6e0e769aeaccf46b698f
    type: comment
  author: TheBloke
  content: "You're welcome.\n\nIf you offload to GPU, yes it will need roughly as\
    \ much VRAM as you see there in the RAM table.  In other words if it says it needs\
    \ 18GB RAM and you offload all layers to VRAM, it will need somewhere 18GB VRAM.\
    \ Actually probably a bit more than that, especially if you're using the latest\
    \ llama.cpp code which now has full GPU acceleration.  \n\nIf you do offload to\
    \ VRAM, you won't then need as much RAM.  That is why it says \"max RAM required\"\
    , because that's the amount of RAM that's needed if you don't offload to GPU at\
    \ all.  If you fully offloaded to GPU, it would only need about 3GB RAM.  If you\
    \ offloaded half the layers, then it would need half the RAM, and the other half\
    \ in VRAM.  And so on.\n\nWith llama.cpp/GGML, RAM usage does not really change\
    \ as context gets longer. So these RAM figures are for 2048 context, but also\
    \ 500 context.  It grows a little bit at the start, but it's not like GPTQ where\
    \ VRAM usage keeps growing as context increases."
  created_at: 2023-06-15 19:01:18+00:00
  edited: false
  hidden: false
  id: 648b6e0e769aeaccf46b698f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
      fullname: Paul CHEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxiangyi10
      type: user
    createdAt: '2023-06-15T21:12:44.000Z'
    data:
      edited: false
      editors:
      - chenxiangyi10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598373770713806
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
          fullname: Paul CHEN
          isHf: false
          isPro: false
          name: chenxiangyi10
          type: user
        html: '<blockquote>

          <p>You''re welcome.</p>

          <p>If you offload to GPU, yes it will need roughly as much VRAM as you see
          there in the RAM table.  In other words if it says it needs 18GB RAM and
          you offload all layers to VRAM, it will need somewhere 18GB VRAM. Actually
          probably a bit more than that, especially if you''re using the latest llama.cpp
          code which now has full GPU acceleration.  </p>

          <p>If you do offload to VRAM, you won''t then need as much RAM.  That is
          why it says "max RAM required", because that''s the amount of RAM that''s
          needed if you don''t offload to GPU at all.  If you fully offloaded to GPU,
          it would only need about 3GB RAM.  If you offloaded half the layers, then
          it would need half the RAM, and the other half in VRAM.  And so on.</p>

          <p>With llama.cpp/GGML, RAM usage does not really change as context gets
          longer. So these RAM figures are for 2048 context, but also 500 context.  It
          grows a little bit at the start, but it''s not like GPTQ where VRAM usage
          keeps growing as context increases.</p>

          </blockquote>

          <p>Thank you so much. It is very interesting that "it  is not like GPTQ
          where VRAM usage keeps growing as context increases."</p>

          '
        raw: "> You're welcome.\n> \n> If you offload to GPU, yes it will need roughly\
          \ as much VRAM as you see there in the RAM table.  In other words if it\
          \ says it needs 18GB RAM and you offload all layers to VRAM, it will need\
          \ somewhere 18GB VRAM. Actually probably a bit more than that, especially\
          \ if you're using the latest llama.cpp code which now has full GPU acceleration.\
          \  \n> \n> If you do offload to VRAM, you won't then need as much RAM. \
          \ That is why it says \"max RAM required\", because that's the amount of\
          \ RAM that's needed if you don't offload to GPU at all.  If you fully offloaded\
          \ to GPU, it would only need about 3GB RAM.  If you offloaded half the layers,\
          \ then it would need half the RAM, and the other half in VRAM.  And so on.\n\
          > \n> With llama.cpp/GGML, RAM usage does not really change as context gets\
          \ longer. So these RAM figures are for 2048 context, but also 500 context.\
          \  It grows a little bit at the start, but it's not like GPTQ where VRAM\
          \ usage keeps growing as context increases.\n\nThank you so much. It is\
          \ very interesting that \"it  is not like GPTQ where VRAM usage keeps growing\
          \ as context increases.\""
        updatedAt: '2023-06-15T21:12:44.196Z'
      numEdits: 0
      reactions: []
    id: 648b7ecce9baeaafdaa1d35f
    type: comment
  author: chenxiangyi10
  content: "> You're welcome.\n> \n> If you offload to GPU, yes it will need roughly\
    \ as much VRAM as you see there in the RAM table.  In other words if it says it\
    \ needs 18GB RAM and you offload all layers to VRAM, it will need somewhere 18GB\
    \ VRAM. Actually probably a bit more than that, especially if you're using the\
    \ latest llama.cpp code which now has full GPU acceleration.  \n> \n> If you do\
    \ offload to VRAM, you won't then need as much RAM.  That is why it says \"max\
    \ RAM required\", because that's the amount of RAM that's needed if you don't\
    \ offload to GPU at all.  If you fully offloaded to GPU, it would only need about\
    \ 3GB RAM.  If you offloaded half the layers, then it would need half the RAM,\
    \ and the other half in VRAM.  And so on.\n> \n> With llama.cpp/GGML, RAM usage\
    \ does not really change as context gets longer. So these RAM figures are for\
    \ 2048 context, but also 500 context.  It grows a little bit at the start, but\
    \ it's not like GPTQ where VRAM usage keeps growing as context increases.\n\n\
    Thank you so much. It is very interesting that \"it  is not like GPTQ where VRAM\
    \ usage keeps growing as context increases.\""
  created_at: 2023-06-15 20:12:44+00:00
  edited: false
  hidden: false
  id: 648b7ecce9baeaafdaa1d35f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/30B-Lazarus-GGML
repo_type: model
status: open
target_branch: null
title: RAM and VRAM utilization
