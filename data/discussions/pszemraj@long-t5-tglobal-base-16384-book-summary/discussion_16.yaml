!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alexpengxiao
conflicting_files: null
created_at: 2022-12-16 11:59:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670826183416-noauth.jpeg?w=200&h=200&f=face
      fullname: alex peng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexpengxiao
      type: user
    createdAt: '2022-12-16T11:59:38.000Z'
    data:
      edited: false
      editors:
      - alexpengxiao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670826183416-noauth.jpeg?w=200&h=200&f=face
          fullname: alex peng
          isHf: false
          isPro: false
          name: alexpengxiao
          type: user
        html: '<p>I tried to deploy the model as an endpoint on aws sagemaker, following
          the instructions(as shown in the screenshot).<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1671191757433-6396c8feb53e01e16f8ee442.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1671191757433-6396c8feb53e01e16f8ee442.png"></a><br>but
          when I call the endpoint for prediction, error occurred: "<br>ModelError:
          An error occurred (ModelError) when calling the InvokeEndpoint operation:
          Received client error (400) from primary with message "{<br>  "code": 400,<br>  "type":
          "InternalServerException",<br>  "message": "\u0027longt5\u0027"<br>}<br>".
          See <a rel="nofollow" href="https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-12-16-11-54-24-597">https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-12-16-11-54-24-597</a>
          in account *** for more information.<br>"<br>in the cloudwatch, it shows
          "<br>2022-12-16T11:24:30,534 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise KeyError(key)<br>2022-12-16T11:24:30,534
          [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          - KeyError: ''longt5''<br>2022-12-16T11:24:30,535 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -<br>2022-12-16T11:24:30,535 [INFO
          ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          - During handling of the above exception, another exception occurred:<br>2022-12-16T11:24:30,535
          [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          -<br>2022-12-16T11:24:30,536 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):<br>2022-12-16T11:24:30,536
          [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          -   File "/opt/conda/lib/python3.8/site-packages/mms/service.py", line 108,
          in predict<br>2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch,
          self.context)<br>2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py",
          line 243, in handle<br>2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e),
          400)<br>2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException:
          ''longt5'' : 400<br>"</p>

          '
        raw: "I tried to deploy the model as an endpoint on aws sagemaker, following\
          \ the instructions(as shown in the screenshot). \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1671191757433-6396c8feb53e01e16f8ee442.png)\r\
          \nbut when I call the endpoint for prediction, error occurred: \"\r\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\r\n  \"code\"\
          : 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\\\
          u0027longt5\\u0027\"\r\n}\r\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-12-16-11-54-24-597\
          \ in account *** for more information.\r\n\"\r\nin the cloudwatch, it shows\
          \ \"\r\n2022-12-16T11:24:30,534 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise KeyError(key)\r\n\
          2022-12-16T11:24:30,534 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - KeyError: 'longt5'\r\n2022-12-16T11:24:30,535 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \r\n2022-12-16T11:24:30,535\
          \ [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - During handling of the above exception, another exception occurred:\r\
          \n2022-12-16T11:24:30,535 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - \r\n2022-12-16T11:24:30,536 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call\
          \ last):\r\n2022-12-16T11:24:30,536 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\"\
          , line 108, in predict\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch,\
          \ self.context)\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\"\
          , line 243, in handle\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e),\
          \ 400)\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException:\
          \ 'longt5' : 400\r\n\"\r\n"
        updatedAt: '2022-12-16T11:59:38.156Z'
      numEdits: 0
      reactions: []
    id: 639c5daa1c597f893b6a8923
    type: comment
  author: alexpengxiao
  content: "I tried to deploy the model as an endpoint on aws sagemaker, following\
    \ the instructions(as shown in the screenshot). \r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1671191757433-6396c8feb53e01e16f8ee442.png)\r\
    \nbut when I call the endpoint for prediction, error occurred: \"\r\nModelError:\
    \ An error occurred (ModelError) when calling the InvokeEndpoint operation: Received\
    \ client error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"\
    type\": \"InternalServerException\",\r\n  \"message\": \"\\u0027longt5\\u0027\"\
    \r\n}\r\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-12-16-11-54-24-597\
    \ in account *** for more information.\r\n\"\r\nin the cloudwatch, it shows \"\
    \r\n2022-12-16T11:24:30,534 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ -     raise KeyError(key)\r\n2022-12-16T11:24:30,534 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: 'longt5'\r\n2022-12-16T11:24:30,535\
    \ [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - \r\n2022-12-16T11:24:30,535 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - During handling of the above exception, another exception occurred:\r\n2022-12-16T11:24:30,535\
    \ [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - \r\n2022-12-16T11:24:30,536 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - Traceback (most recent call last):\r\n2022-12-16T11:24:30,536 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\"\
    , line 108, in predict\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch,\
    \ self.context)\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\"\
    , line 243, in handle\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e),\
    \ 400)\r\n2022-12-16T11:24:30,538 [INFO ] W-pszemraj__long-t5-tglobal-2-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException:\
    \ 'longt5' : 400\r\n\"\r\n"
  created_at: 2022-12-16 11:59:38+00:00
  edited: false
  hidden: false
  id: 639c5daa1c597f893b6a8923
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-20T03:54:42.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Thank you for bringing this issue to my attention. Sorry for the
          inconvenience you have experienced while using the model. There may be an
          issue with the deployment of the model on AWS Sagemaker. I recommend that
          you raise this issue with both Hugging Face and AWS, as it may be a problem
          with the integration between the two platforms.</p>

          <p>To ensure that this issue is adequately addressed, I recommend you create
          an issue on the official Hugging Face repository (<a rel="nofollow" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>)
          to report any issues related to the use of the model on AWS Sagemaker. The
          team at Hugging Face will be able to provide further assistance and guidance
          in resolving this issue.</p>

          <p>If they state something needs to be updated with the config/etc. to enable
          this integration, please feel free to reopen this issue :)</p>

          '
        raw: 'Thank you for bringing this issue to my attention. Sorry for the inconvenience
          you have experienced while using the model. There may be an issue with the
          deployment of the model on AWS Sagemaker. I recommend that you raise this
          issue with both Hugging Face and AWS, as it may be a problem with the integration
          between the two platforms.


          To ensure that this issue is adequately addressed, I recommend you create
          an issue on the official Hugging Face repository (https://github.com/huggingface/transformers)
          to report any issues related to the use of the model on AWS Sagemaker. The
          team at Hugging Face will be able to provide further assistance and guidance
          in resolving this issue.


          If they state something needs to be updated with the config/etc. to enable
          this integration, please feel free to reopen this issue :)'
        updatedAt: '2022-12-20T03:54:42.610Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63a132022fabbbb899a21e3c
    id: 63a132022fabbbb899a21e3b
    type: comment
  author: pszemraj
  content: 'Thank you for bringing this issue to my attention. Sorry for the inconvenience
    you have experienced while using the model. There may be an issue with the deployment
    of the model on AWS Sagemaker. I recommend that you raise this issue with both
    Hugging Face and AWS, as it may be a problem with the integration between the
    two platforms.


    To ensure that this issue is adequately addressed, I recommend you create an issue
    on the official Hugging Face repository (https://github.com/huggingface/transformers)
    to report any issues related to the use of the model on AWS Sagemaker. The team
    at Hugging Face will be able to provide further assistance and guidance in resolving
    this issue.


    If they state something needs to be updated with the config/etc. to enable this
    integration, please feel free to reopen this issue :)'
  created_at: 2022-12-20 03:54:42+00:00
  edited: false
  hidden: false
  id: 63a132022fabbbb899a21e3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-12-20T03:54:42.000Z'
    data:
      status: closed
    id: 63a132022fabbbb899a21e3c
    type: status-change
  author: pszemraj
  created_at: 2022-12-20 03:54:42+00:00
  id: 63a132022fabbbb899a21e3c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: pszemraj/long-t5-tglobal-base-16384-book-summary
repo_type: model
status: closed
target_branch: null
title: error when calling endpoint after deployed on aws sagemaker
