!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JESUSCOLIN
conflicting_files: null
created_at: 2023-12-29 04:43:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8802105b250c222f9538fe243a0eb47.svg
      fullname: JESUS ALEJANDRO COLIN VILCHIS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JESUSCOLIN
      type: user
    createdAt: '2023-12-29T04:43:39.000Z'
    data:
      edited: false
      editors:
      - JESUSCOLIN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7421273589134216
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8802105b250c222f9538fe243a0eb47.svg
          fullname: JESUS ALEJANDRO COLIN VILCHIS
          isHf: false
          isPro: false
          name: JESUSCOLIN
          type: user
        html: "<p>Hi, I spend some time testing the onnx model. Can someone identify\
          \ the issue, the are not clear instructions and I am new on this.</p>\n\
          <p>from onnxruntime import InferenceSession<br>import numpy as np<br>from\
          \ transformers import AutoTokenizer</p>\n<p>session = InferenceSession(\"\
          model.onnx\")<br>long_text =\" A typical feed-forward neural field algorithm.\
          \ Spatiotemporal coordinates are fed into a neural network that predicts\
          \ values in the reconstructed domain. Then, this domain is mapped to the\
          \ sensor domain where sensor measurements are available as supervision.\
          \ Class and Section Problems Addressed Generalization (Section 2) Inverse\
          \ problems, ill-posed problems, editability; symmetries. Hybrid Representations\
          \ (Section 3) Computation &amp; memory efficiency, representation capacity,\
          \ editability: Forward Maps (Section 4) Inverse problems Network Architecture\
          \ (Section 5) Spectral bias, integration &amp; derivatives. Manipulating\
          \ Neural Fields (Section 6) Edit ability, constraints, regularization. Table\
          \ 2: The five classes of techniques in the neural field toolbox each addresses\
          \ problems that arise in learning, inference, and control. (Section 3).\
          \ We can supervise reconstruction via differentiable forward maps that transform\
          \ Or project our domain (e.g, 3D reconstruction via 2D images; Section 4)\
          \ With appropriate network architecture choices, we can overcome neural\
          \ network spectral biases (blurriness) and efficiently compute derivatives\
          \ and integrals (Section 5). Finally, we can manipulate neural fields to\
          \ add constraints and regularizations, and to achieve editable representations\
          \ (Section 6). Collectively, these classes constitute a 'toolbox' of techniques\
          \ to help solve problems with neural fields There are three components in\
          \ a conditional neural field: (1) An encoder or inference function \u20AC\
          \ that outputs the conditioning latent variable 2 given an observation 0\
          \ E(0) =2. 2 is typically a low-dimensional vector, and is often referred\
          \ to aS a latent code Or feature code_ (2) A mapping function 4 between\
          \ Z and neural field parameters O: Y(z) = O; (3) The neural field itself\
          \ $. The encoder \u20AC finds the most probable z given the observations\
          \ O: argmaxz P(2/0). The decoder maximizes the inverse conditional probability\
          \ to find the most probable 0 given Z: arg- max P(Olz). We discuss different\
          \ encoding schemes with different optimality guarantees (Section 2.1.1),\
          \ both global and local conditioning (Section 2.1.2), and different mapping\
          \ functions Y (Section 2.1.3) 2. Generalization Suppose we wish to estimate\
          \ a plausible 3D surface shape given a partial or noisy point cloud. We\
          \ need a suitable prior over the sur- face in its reconstruction domain\
          \ to generalize to the partial observations. A neural network expresses\
          \ a prior via the function space of its architecture and parameters 0, and\
          \ generalization is influenced by the inductive bias of this function space\
          \ (Section 5).\"<br>tokenizer = AutoTokenizer.from_pretrained(\"t5-base\"\
          )</p>\n<p>input_dict = tokenizer(long_text, return_tensors=\"np\", padding=\"\
          max_length\", truncation=True, max_length=3)<br>input_ids = input_dict[\"\
          input_ids\"]<br>attention_mask = input_dict[\"attention_mask\"]<br>decoder_input_ids\
          \ = np.zeros_like(input_ids)  # Puede variar seg\xFAn tus necesidades</p>\n\
          <p>input_feed = {<br>    \"input_ids\": input_ids.astype(np.int64),<br>\
          \    \"attention_mask\": attention_mask.astype(np.int64),<br>    \"decoder_input_ids\"\
          : decoder_input_ids.astype(np.int64)<br>}</p>\n<p>output_names = [\"logits\"\
          ]  # Not sure about this output_names</p>\n<p>outputs = session.run(output_names,\
          \ input_feed)</p>\n<p>summary_ids = outputs[0]<br>summary_text = tokenizer.decode(summary_ids[0],\
          \ skip_special_tokens=True)</p>\n<p>print(summary_text)</p>\n"
        raw: "Hi, I spend some time testing the onnx model. Can someone identify the\
          \ issue, the are not clear instructions and I am new on this.\r\n\r\nfrom\
          \ onnxruntime import InferenceSession\r\nimport numpy as np\r\nfrom transformers\
          \ import AutoTokenizer\r\n\r\nsession = InferenceSession(\"model.onnx\"\
          )\r\nlong_text =\" A typical feed-forward neural field algorithm. Spatiotemporal\
          \ coordinates are fed into a neural network that predicts values in the\
          \ reconstructed domain. Then, this domain is mapped to the sensor domain\
          \ where sensor measurements are available as supervision. Class and Section\
          \ Problems Addressed Generalization (Section 2) Inverse problems, ill-posed\
          \ problems, editability; symmetries. Hybrid Representations (Section 3)\
          \ Computation & memory efficiency, representation capacity, editability:\
          \ Forward Maps (Section 4) Inverse problems Network Architecture (Section\
          \ 5) Spectral bias, integration & derivatives. Manipulating Neural Fields\
          \ (Section 6) Edit ability, constraints, regularization. Table 2: The five\
          \ classes of techniques in the neural field toolbox each addresses problems\
          \ that arise in learning, inference, and control. (Section 3). We can supervise\
          \ reconstruction via differentiable forward maps that transform Or project\
          \ our domain (e.g, 3D reconstruction via 2D images; Section 4) With appropriate\
          \ network architecture choices, we can overcome neural network spectral\
          \ biases (blurriness) and efficiently compute derivatives and integrals\
          \ (Section 5). Finally, we can manipulate neural fields to add constraints\
          \ and regularizations, and to achieve editable representations (Section\
          \ 6). Collectively, these classes constitute a 'toolbox' of techniques to\
          \ help solve problems with neural fields There are three components in a\
          \ conditional neural field: (1) An encoder or inference function \u20AC\
          \ that outputs the conditioning latent variable 2 given an observation 0\
          \ E(0) =2. 2 is typically a low-dimensional vector, and is often referred\
          \ to aS a latent code Or feature code_ (2) A mapping function 4 between\
          \ Z and neural field parameters O: Y(z) = O; (3) The neural field itself\
          \ $. The encoder \u20AC finds the most probable z given the observations\
          \ O: argmaxz P(2/0). The decoder maximizes the inverse conditional probability\
          \ to find the most probable 0 given Z: arg- max P(Olz). We discuss different\
          \ encoding schemes with different optimality guarantees (Section 2.1.1),\
          \ both global and local conditioning (Section 2.1.2), and different mapping\
          \ functions Y (Section 2.1.3) 2. Generalization Suppose we wish to estimate\
          \ a plausible 3D surface shape given a partial or noisy point cloud. We\
          \ need a suitable prior over the sur- face in its reconstruction domain\
          \ to generalize to the partial observations. A neural network expresses\
          \ a prior via the function space of its architecture and parameters 0, and\
          \ generalization is influenced by the inductive bias of this function space\
          \ (Section 5).\"\r\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\"\
          )\r\n\r\ninput_dict = tokenizer(long_text, return_tensors=\"np\", padding=\"\
          max_length\", truncation=True, max_length=3)\r\ninput_ids = input_dict[\"\
          input_ids\"]\r\nattention_mask = input_dict[\"attention_mask\"]\r\ndecoder_input_ids\
          \ = np.zeros_like(input_ids)  # Puede variar seg\xFAn tus necesidades\r\n\
          \r\n\r\ninput_feed = {\r\n    \"input_ids\": input_ids.astype(np.int64),\r\
          \n    \"attention_mask\": attention_mask.astype(np.int64),\r\n    \"decoder_input_ids\"\
          : decoder_input_ids.astype(np.int64)\r\n}\r\n\r\noutput_names = [\"logits\"\
          ]  # Not sure about this output_names\r\n\r\noutputs = session.run(output_names,\
          \ input_feed)\r\n\r\nsummary_ids = outputs[0]\r\nsummary_text = tokenizer.decode(summary_ids[0],\
          \ skip_special_tokens=True)\r\n\r\nprint(summary_text)"
        updatedAt: '2023-12-29T04:43:39.510Z'
      numEdits: 0
      reactions: []
    id: 658e4e7bf0152a21fc97fafe
    type: comment
  author: JESUSCOLIN
  content: "Hi, I spend some time testing the onnx model. Can someone identify the\
    \ issue, the are not clear instructions and I am new on this.\r\n\r\nfrom onnxruntime\
    \ import InferenceSession\r\nimport numpy as np\r\nfrom transformers import AutoTokenizer\r\
    \n\r\nsession = InferenceSession(\"model.onnx\")\r\nlong_text =\" A typical feed-forward\
    \ neural field algorithm. Spatiotemporal coordinates are fed into a neural network\
    \ that predicts values in the reconstructed domain. Then, this domain is mapped\
    \ to the sensor domain where sensor measurements are available as supervision.\
    \ Class and Section Problems Addressed Generalization (Section 2) Inverse problems,\
    \ ill-posed problems, editability; symmetries. Hybrid Representations (Section\
    \ 3) Computation & memory efficiency, representation capacity, editability: Forward\
    \ Maps (Section 4) Inverse problems Network Architecture (Section 5) Spectral\
    \ bias, integration & derivatives. Manipulating Neural Fields (Section 6) Edit\
    \ ability, constraints, regularization. Table 2: The five classes of techniques\
    \ in the neural field toolbox each addresses problems that arise in learning,\
    \ inference, and control. (Section 3). We can supervise reconstruction via differentiable\
    \ forward maps that transform Or project our domain (e.g, 3D reconstruction via\
    \ 2D images; Section 4) With appropriate network architecture choices, we can\
    \ overcome neural network spectral biases (blurriness) and efficiently compute\
    \ derivatives and integrals (Section 5). Finally, we can manipulate neural fields\
    \ to add constraints and regularizations, and to achieve editable representations\
    \ (Section 6). Collectively, these classes constitute a 'toolbox' of techniques\
    \ to help solve problems with neural fields There are three components in a conditional\
    \ neural field: (1) An encoder or inference function \u20AC that outputs the conditioning\
    \ latent variable 2 given an observation 0 E(0) =2. 2 is typically a low-dimensional\
    \ vector, and is often referred to aS a latent code Or feature code_ (2) A mapping\
    \ function 4 between Z and neural field parameters O: Y(z) = O; (3) The neural\
    \ field itself $. The encoder \u20AC finds the most probable z given the observations\
    \ O: argmaxz P(2/0). The decoder maximizes the inverse conditional probability\
    \ to find the most probable 0 given Z: arg- max P(Olz). We discuss different encoding\
    \ schemes with different optimality guarantees (Section 2.1.1), both global and\
    \ local conditioning (Section 2.1.2), and different mapping functions Y (Section\
    \ 2.1.3) 2. Generalization Suppose we wish to estimate a plausible 3D surface\
    \ shape given a partial or noisy point cloud. We need a suitable prior over the\
    \ sur- face in its reconstruction domain to generalize to the partial observations.\
    \ A neural network expresses a prior via the function space of its architecture\
    \ and parameters 0, and generalization is influenced by the inductive bias of\
    \ this function space (Section 5).\"\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    t5-base\")\r\n\r\ninput_dict = tokenizer(long_text, return_tensors=\"np\", padding=\"\
    max_length\", truncation=True, max_length=3)\r\ninput_ids = input_dict[\"input_ids\"\
    ]\r\nattention_mask = input_dict[\"attention_mask\"]\r\ndecoder_input_ids = np.zeros_like(input_ids)\
    \  # Puede variar seg\xFAn tus necesidades\r\n\r\n\r\ninput_feed = {\r\n    \"\
    input_ids\": input_ids.astype(np.int64),\r\n    \"attention_mask\": attention_mask.astype(np.int64),\r\
    \n    \"decoder_input_ids\": decoder_input_ids.astype(np.int64)\r\n}\r\n\r\noutput_names\
    \ = [\"logits\"]  # Not sure about this output_names\r\n\r\noutputs = session.run(output_names,\
    \ input_feed)\r\n\r\nsummary_ids = outputs[0]\r\nsummary_text = tokenizer.decode(summary_ids[0],\
    \ skip_special_tokens=True)\r\n\r\nprint(summary_text)"
  created_at: 2023-12-29 04:43:39+00:00
  edited: false
  hidden: false
  id: 658e4e7bf0152a21fc97fafe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b8802105b250c222f9538fe243a0eb47.svg
      fullname: JESUS ALEJANDRO COLIN VILCHIS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JESUSCOLIN
      type: user
    createdAt: '2023-12-30T02:34:51.000Z'
    data:
      status: closed
    id: 658f81cb315340de5f1bea43
    type: status-change
  author: JESUSCOLIN
  created_at: 2023-12-30 02:34:51+00:00
  id: 658f81cb315340de5f1bea43
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8802105b250c222f9538fe243a0eb47.svg
      fullname: JESUS ALEJANDRO COLIN VILCHIS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JESUSCOLIN
      type: user
    createdAt: '2023-12-30T02:36:22.000Z'
    data:
      edited: false
      editors:
      - JESUSCOLIN
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16045841574668884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8802105b250c222f9538fe243a0eb47.svg
          fullname: JESUS ALEJANDRO COLIN VILCHIS
          isHf: false
          isPro: false
          name: JESUSCOLIN
          type: user
        html: '<p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM<br>tokenizer
          = AutoTokenizer.from_pretrained(''./'')<br>model = AutoModelForSeq2SeqLM.from_pretrained(''./'')<br>def
          generate_text(input_text):<br>  input_ids = tokenizer.encode(input_text,
          return_tensors=''pt'')<br>  output_ids = model.generate(input_ids)<br>  output_text
          = tokenizer.decode(output_ids[0], skip_special_tokens=True)<br>  return
          output_text<br>long_text =" ...."<br>print(generate_text(long_text))</p>

          <p>FYI</p>

          '
        raw: "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer\
          \ = AutoTokenizer.from_pretrained('./')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('./')\n\
          def generate_text(input_text):\n  input_ids = tokenizer.encode(input_text,\
          \ return_tensors='pt')\n  output_ids = model.generate(input_ids)\n  output_text\
          \ = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n  return\
          \ output_text\nlong_text =\" ....\"\nprint(generate_text(long_text))\n\n\
          FYI"
        updatedAt: '2023-12-30T02:36:22.694Z'
      numEdits: 0
      reactions: []
    id: 658f8226de82e1ef7b04a2d1
    type: comment
  author: JESUSCOLIN
  content: "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer\
    \ = AutoTokenizer.from_pretrained('./')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('./')\n\
    def generate_text(input_text):\n  input_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
    \  output_ids = model.generate(input_ids)\n  output_text = tokenizer.decode(output_ids[0],\
    \ skip_special_tokens=True)\n  return output_text\nlong_text =\" ....\"\nprint(generate_text(long_text))\n\
    \nFYI"
  created_at: 2023-12-30 02:36:22+00:00
  edited: false
  hidden: false
  id: 658f8226de82e1ef7b04a2d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-12-30T03:58:50.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9321901202201843
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>hi! thanks for reaching out. I just finetuned the model and posted
          it, if you''re looking for support on how how use a model with optimum,
          I would recommend checking out <a rel="nofollow" href="https://discuss.huggingface.co/">the
          forums</a>. If something is not working with optimum, you can create an
          issue in <a rel="nofollow" href="https://github.com/huggingface/optimum">the
          repo</a></p>

          <p>if something works with <strong>most other</strong> T5/long-T5 models
          and does not with this one, then that is something that should be here :)</p>

          '
        raw: 'hi! thanks for reaching out. I just finetuned the model and posted it,
          if you''re looking for support on how how use a model with optimum, I would
          recommend checking out [the forums](https://discuss.huggingface.co/). If
          something is not working with optimum, you can create an issue in [the repo](https://github.com/huggingface/optimum)


          if something works with **most other** T5/long-T5 models and does not with
          this one, then that is something that should be here :)'
        updatedAt: '2023-12-30T03:58:50.170Z'
      numEdits: 0
      reactions: []
    id: 658f957a5b7553ca5c58cc37
    type: comment
  author: pszemraj
  content: 'hi! thanks for reaching out. I just finetuned the model and posted it,
    if you''re looking for support on how how use a model with optimum, I would recommend
    checking out [the forums](https://discuss.huggingface.co/). If something is not
    working with optimum, you can create an issue in [the repo](https://github.com/huggingface/optimum)


    if something works with **most other** T5/long-T5 models and does not with this
    one, then that is something that should be here :)'
  created_at: 2023-12-30 03:58:50+00:00
  edited: false
  hidden: false
  id: 658f957a5b7553ca5c58cc37
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: pszemraj/long-t5-tglobal-base-16384-book-summary
repo_type: model
status: closed
target_branch: null
title: Not able to test it.
