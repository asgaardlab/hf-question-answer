!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fanwave-rr
conflicting_files: null
created_at: 2023-10-20 12:13:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58823fd76e53dede1967361e62aa6d7d.svg
      fullname: fanwave-rr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fanwave-rr
      type: user
    createdAt: '2023-10-20T13:13:19.000Z'
    data:
      edited: false
      editors:
      - fanwave-rr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5002151131629944
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58823fd76e53dede1967361e62aa6d7d.svg
          fullname: fanwave-rr
          isHf: false
          isPro: false
          name: fanwave-rr
          type: user
        html: "<p>Model seems to load fine (see image) but upon inference, only outputs\
          \ 1 word like \"I\" or \"Hey\". Maybe an issue with spaces?</p>\n<h2 id=\"\
          error-message-below\">Error message below:</h2>\n<p>Loaded the model via\
          \ Transformers<br>2023-10-20 13:02:59 INFO:Loading TheBloke_Falcon-180B-Chat-GPTQ...<br>2023-10-20\
          \ 13:02:59 INFO:Loading with ExLlama kernel disabled.<br>Output generated\
          \ in 0.83 seconds (0.00 tokens/s, 0 tokens, context 851, seed 1729916028)<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 10/10 [00:17&lt;00:00,  1.76s/it]<br>2023-10-20 13:03:33 INFO:Loaded\
          \ the model in 34.52 seconds.<br>100.64.0.22 - - [20/Oct/2023 13:03:40]\
          \ \"POST /api/v1/generate HTTP/1.1\" 200 -<br>Traceback (most recent call\
          \ last):<br>  File \"/workspace/text-generation-webui/modules/callbacks.py\"\
          , line 56, in gentask<br>    ret = self.mfunc(callback=_callback, *args,\
          \ **self.kwargs)<br>  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 351, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1648, in generate<br>    return self.sample(<br>  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2730, in sample<br>    outputs = self(<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 1031, in forward<br>    transformer_outputs = self.transformer(<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 928, in forward<br>    outputs = block(<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 553, in forward<br>    attn_outputs = self.self_attention(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 386, in forward<br>    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
          , line 208, in forward<br>    self.autogptq_cuda.vecquant4matmul(x.float(),\
          \ self.qweight, out, self.scales.float(), self.qzeros, self.g_idx)<br>RuntimeError:\
          \ Unrecognized tensor type ID: AutocastCUDA</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6530f67b753d5411b7eb62be/8UoVFRbvh6LihnNifSzZ6.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6530f67b753d5411b7eb62be/8UoVFRbvh6LihnNifSzZ6.png\"\
          ></a></p>\n"
        raw: "Model seems to load fine (see image) but upon inference, only outputs\
          \ 1 word like \"I\" or \"Hey\". Maybe an issue with spaces?\r\n\r\nError\
          \ message below:\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\
          \nLoaded the model via Transformers \r\n2023-10-20 13:02:59 INFO:Loading\
          \ TheBloke_Falcon-180B-Chat-GPTQ...\r\n2023-10-20 13:02:59 INFO:Loading\
          \ with ExLlama kernel disabled.\r\nOutput generated in 0.83 seconds (0.00\
          \ tokens/s, 0 tokens, context 851, seed 1729916028)\r\nLoading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 10/10 [00:17<00:00,  1.76s/it]\r\n2023-10-20 13:03:33 INFO:Loaded the\
          \ model in 34.52 seconds.\r\n100.64.0.22 - - [20/Oct/2023 13:03:40] \"POST\
          \ /api/v1/generate HTTP/1.1\" 200 -\r\nTraceback (most recent call last):\r\
          \n  File \"/workspace/text-generation-webui/modules/callbacks.py\", line\
          \ 56, in gentask\r\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\r\
          \n  File \"/workspace/text-generation-webui/modules/text_generation.py\"\
          , line 351, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1648, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2730, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 1031, in forward\r\n    transformer_outputs = self.transformer(\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 928, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 553, in forward\r\n    attn_outputs = self.self_attention(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
          , line 386, in forward\r\n    fused_qkv = self.query_key_value(hidden_states)\
          \  # [batch_size, seq_length, 3 x hidden_size]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
          , line 208, in forward\r\n    self.autogptq_cuda.vecquant4matmul(x.float(),\
          \ self.qweight, out, self.scales.float(), self.qzeros, self.g_idx)\r\nRuntimeError:\
          \ Unrecognized tensor type ID: AutocastCUDA\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6530f67b753d5411b7eb62be/8UoVFRbvh6LihnNifSzZ6.png)\r\
          \n"
        updatedAt: '2023-10-20T13:13:19.423Z'
      numEdits: 0
      reactions: []
    id: 65327cefc7530aa27f693f72
    type: comment
  author: fanwave-rr
  content: "Model seems to load fine (see image) but upon inference, only outputs\
    \ 1 word like \"I\" or \"Hey\". Maybe an issue with spaces?\r\n\r\nError message\
    \ below:\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\
    \nLoaded the model via Transformers \r\n2023-10-20 13:02:59 INFO:Loading TheBloke_Falcon-180B-Chat-GPTQ...\r\
    \n2023-10-20 13:02:59 INFO:Loading with ExLlama kernel disabled.\r\nOutput generated\
    \ in 0.83 seconds (0.00 tokens/s, 0 tokens, context 851, seed 1729916028)\r\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 10/10 [00:17<00:00,  1.76s/it]\r\n2023-10-20 13:03:33 INFO:Loaded\
    \ the model in 34.52 seconds.\r\n100.64.0.22 - - [20/Oct/2023 13:03:40] \"POST\
    \ /api/v1/generate HTTP/1.1\" 200 -\r\nTraceback (most recent call last):\r\n\
    \  File \"/workspace/text-generation-webui/modules/callbacks.py\", line 56, in\
    \ gentask\r\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\r\n\
    \  File \"/workspace/text-generation-webui/modules/text_generation.py\", line\
    \ 351, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line\
    \ 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 1648, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 2730, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
    , line 1031, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
    \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
    , line 928, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
    , line 553, in forward\r\n    attn_outputs = self.self_attention(\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,\
    \ in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
    \ 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\
    \n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/falcon/modeling_falcon.py\"\
    , line 386, in forward\r\n    fused_qkv = self.query_key_value(hidden_states)\
    \  # [batch_size, seq_length, 3 x hidden_size]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\"\
    , line 208, in forward\r\n    self.autogptq_cuda.vecquant4matmul(x.float(), self.qweight,\
    \ out, self.scales.float(), self.qzeros, self.g_idx)\r\nRuntimeError: Unrecognized\
    \ tensor type ID: AutocastCUDA\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6530f67b753d5411b7eb62be/8UoVFRbvh6LihnNifSzZ6.png)\r\
    \n"
  created_at: 2023-10-20 12:13:19+00:00
  edited: false
  hidden: false
  id: 65327cefc7530aa27f693f72
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Falcon-180B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Model outputs 1 word or letter ("I", "Hey")
