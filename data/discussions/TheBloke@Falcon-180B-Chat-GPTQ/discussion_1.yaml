!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shafiqalibhai
conflicting_files: null
created_at: 2023-09-06 16:58:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dce3b48af089e214318c97456c87d6e0.svg
      fullname: shafiq alibhai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shafiqalibhai
      type: user
    createdAt: '2023-09-06T17:58:48.000Z'
    data:
      edited: false
      editors:
      - shafiqalibhai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7853398323059082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dce3b48af089e214318c97456c87d6e0.svg
          fullname: shafiq alibhai
          isHf: false
          isPro: false
          name: shafiqalibhai
          type: user
        html: '<p>Your speed. {{slow clapping}}</p>

          '
        raw: Your speed. {{slow clapping}}
        updatedAt: '2023-09-06T17:58:48.754Z'
      numEdits: 0
      reactions: []
    id: 64f8bdd8ceabf1e6fc61ceba
    type: comment
  author: shafiqalibhai
  content: Your speed. {{slow clapping}}
  created_at: 2023-09-06 16:58:48+00:00
  edited: false
  hidden: false
  id: 64f8bdd8ceabf1e6fc61ceba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-09-06T18:07:05.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7903383374214172
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>180B? Ouch. Gonna be a big one.</p>

          '
        raw: 180B? Ouch. Gonna be a big one.
        updatedAt: '2023-09-06T18:07:05.931Z'
      numEdits: 0
      reactions: []
    id: 64f8bfc937d3c6eb54dd3cfc
    type: comment
  author: Nurb432
  content: 180B? Ouch. Gonna be a big one.
  created_at: 2023-09-06 17:07:05+00:00
  edited: false
  hidden: false
  id: 64f8bfc937d3c6eb54dd3cfc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-06T21:22:12.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9164721369743347
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>Imagine how good it can be with 180B.</p>

          '
        raw: Imagine how good it can be with 180B.
        updatedAt: '2023-09-06T21:22:12.594Z'
      numEdits: 0
      reactions: []
    id: 64f8ed848a234f114e3b22dd
    type: comment
  author: Flanua
  content: Imagine how good it can be with 180B.
  created_at: 2023-09-06 20:22:12+00:00
  edited: false
  hidden: false
  id: 64f8ed848a234f114e3b22dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63e5fbcf980214f063b3cd9620c5eff2.svg
      fullname: Pourfard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pourfard
      type: user
    createdAt: '2023-09-06T23:36:04.000Z'
    data:
      edited: false
      editors:
      - Pourfard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778357744216919
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63e5fbcf980214f063b3cd9620c5eff2.svg
          fullname: Pourfard
          isHf: false
          isPro: false
          name: Pourfard
          type: user
        html: '<p>I hope I could run it on 4 3090.</p>

          '
        raw: I hope I could run it on 4 3090.
        updatedAt: '2023-09-06T23:36:04.139Z'
      numEdits: 0
      reactions: []
    id: 64f90ce4766ff9f3d2b6900d
    type: comment
  author: Pourfard
  content: I hope I could run it on 4 3090.
  created_at: 2023-09-06 22:36:04+00:00
  edited: false
  hidden: false
  id: 64f90ce4766ff9f3d2b6900d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-09-06T23:54:49.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8354203701019287
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>how good?<br>So far falcon 40B is worse than 13B llama2 .... so
          180b maybe get level of 34b llama2 or a bit better .....</p>

          <p>edit</p>

          <p>HA!<br>I was right<br><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></p>

          <p>falcon 180b has level llama2 34b ....</p>

          '
        raw: "how good?\nSo far falcon 40B is worse than 13B llama2 .... so 180b maybe\
          \ get level of 34b llama2 or a bit better .....\n\n\nedit\n\nHA!\nI was\
          \ right \nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n\
          \nfalcon 180b has level llama2 34b ...."
        updatedAt: '2023-09-06T23:58:07.444Z'
      numEdits: 2
      reactions: []
    id: 64f9114905961fa127a1d861
    type: comment
  author: mirek190
  content: "how good?\nSo far falcon 40B is worse than 13B llama2 .... so 180b maybe\
    \ get level of 34b llama2 or a bit better .....\n\n\nedit\n\nHA!\nI was right\
    \ \nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n\nfalcon\
    \ 180b has level llama2 34b ...."
  created_at: 2023-09-06 22:54:49+00:00
  edited: true
  hidden: false
  id: 64f9114905961fa127a1d861
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1588403060688-noauth.png?w=200&h=200&f=face
      fullname: PenutChen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: penut85420
      type: user
    createdAt: '2023-09-07T01:25:27.000Z'
    data:
      edited: false
      editors:
      - penut85420
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9452317953109741
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1588403060688-noauth.png?w=200&h=200&f=face
          fullname: PenutChen
          isHf: false
          isPro: false
          name: penut85420
          type: user
        html: '<p>Wow, how did you all manage to complete it so incredibly quickly?!!!!!!!!!!!!!!!!!!!</p>

          '
        raw: Wow, how did you all manage to complete it so incredibly quickly?!!!!!!!!!!!!!!!!!!!
        updatedAt: '2023-09-07T01:25:27.808Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Yhyu13
    id: 64f92687e7584abc62723ceb
    type: comment
  author: penut85420
  content: Wow, how did you all manage to complete it so incredibly quickly?!!!!!!!!!!!!!!!!!!!
  created_at: 2023-09-07 00:25:27+00:00
  edited: false
  hidden: false
  id: 64f92687e7584abc62723ceb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-07T09:38:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9799627065658569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;penut85420&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/penut85420\">@<span class=\"\
          underline\">penut85420</span></a></span>\n\n\t</span></span> I only uploaded\
          \ it a couple of hours ago, so it took me 24 hours. Far too long :)</p>\n\
          <p>Had some problems overnight; I forgot the files would be &gt;50GB so\
          \ they failed to upload until I manually split them this morning.</p>\n"
        raw: '@penut85420 I only uploaded it a couple of hours ago, so it took me
          24 hours. Far too long :)


          Had some problems overnight; I forgot the files would be >50GB so they failed
          to upload until I manually split them this morning.


          '
        updatedAt: '2023-09-07T09:38:52.404Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - mirek190
        - Flanua
        - penut85420
        - nps798
    id: 64f99a2cfd190e10c8301e5d
    type: comment
  author: TheBloke
  content: '@penut85420 I only uploaded it a couple of hours ago, so it took me 24
    hours. Far too long :)


    Had some problems overnight; I forgot the files would be >50GB so they failed
    to upload until I manually split them this morning.


    '
  created_at: 2023-09-07 08:38:52+00:00
  edited: false
  hidden: false
  id: 64f99a2cfd190e10c8301e5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-07T16:56:10.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9671335816383362
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <p>I hope I could run it on 4 3090.</p>

          </blockquote>

          <p>Haha it''s impossible. You need like 300+ GB of ram for that. :))</p>

          <p>Corrections about ram:<br>You will need at least 400GB of memory to swiftly
          run inference with Falcon-180B.</p>

          '
        raw: "> I hope I could run it on 4 3090.\n\nHaha it's impossible. You need\
          \ like 300+ GB of ram for that. :))\n\nCorrections about ram: \nYou will\
          \ need at least 400GB of memory to swiftly run inference with Falcon-180B."
        updatedAt: '2023-09-07T17:05:03.908Z'
      numEdits: 1
      reactions: []
    id: 64fa00aadc465697353d05a0
    type: comment
  author: Flanua
  content: "> I hope I could run it on 4 3090.\n\nHaha it's impossible. You need like\
    \ 300+ GB of ram for that. :))\n\nCorrections about ram: \nYou will need at least\
    \ 400GB of memory to swiftly run inference with Falcon-180B."
  created_at: 2023-09-07 15:56:10+00:00
  edited: true
  hidden: false
  id: 64fa00aadc465697353d05a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-07T16:59:15.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8745844960212708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <p>how good?<br>So far falcon 40B is worse than 13B llama2 .... so 180b
          maybe get level of 34b llama2 or a bit better .....</p>

          <p>edit</p>

          <p>HA!<br>I was right<br><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></p>

          <p>falcon 180b has level llama2 34b ....</p>

          </blockquote>

          <p>It''s hard to train 180B model to unleash it''s potential. A lot of resourced
          are required though.</p>

          '
        raw: "> how good?\n> So far falcon 40B is worse than 13B llama2 .... so 180b\
          \ maybe get level of 34b llama2 or a bit better .....\n> \n> \n> edit\n\
          > \n> HA!\n> I was right \n> https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n\
          > \n> falcon 180b has level llama2 34b ....\n\nIt's hard to train 180B model\
          \ to unleash it's potential. A lot of resourced are required though."
        updatedAt: '2023-09-07T17:04:48.631Z'
      numEdits: 2
      reactions: []
    id: 64fa0163b961d0d12c5cbd7a
    type: comment
  author: Flanua
  content: "> how good?\n> So far falcon 40B is worse than 13B llama2 .... so 180b\
    \ maybe get level of 34b llama2 or a bit better .....\n> \n> \n> edit\n> \n> HA!\n\
    > I was right \n> https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n\
    > \n> falcon 180b has level llama2 34b ....\n\nIt's hard to train 180B model to\
    \ unleash it's potential. A lot of resourced are required though."
  created_at: 2023-09-07 15:59:15+00:00
  edited: true
  hidden: false
  id: 64fa0163b961d0d12c5cbd7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-07T17:01:12.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.921882152557373
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>No it should work - you only need enough RAM to load each piece\
          \ into RAM before it goes to VRAM.  And the model is now sharded (split\
          \ into multiple smaller files).</p>\n<p>Each piece is only 10GB, so in theory\
          \ you only need 10GB RAM + whatever overhead there is.</p>\n<p>As for 4\
          \ x 24GB - that won't be enough to load the 4-bit, but should be enough\
          \ to load the 3-bit.</p>\n<p>Give it a try <span data-props=\"{&quot;user&quot;:&quot;Pourfard&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Pourfard\"\
          >@<span class=\"underline\">Pourfard</span></a></span>\n\n\t</span></span>\
          \ and let us know!  So far I've only tested it on 2 x A100 80GB and 6 x\
          \ L40 48GB.</p>\n<p>Note: the model has to be loaded with Transformers directly\
          \ (not AutoGPTQ), or Text Generation Inference.  Loading with AutoGPTQ,\
          \ or clients that use AutoGPTQ, won't currently work due to the sharding.\
          \  If you're using text-generation-webui, it should work using the Transformers\
          \ loader, though I've not tested that yet myself.</p>\n"
        raw: 'No it should work - you only need enough RAM to load each piece into
          RAM before it goes to VRAM.  And the model is now sharded (split into multiple
          smaller files).


          Each piece is only 10GB, so in theory you only need 10GB RAM + whatever
          overhead there is.


          As for 4 x 24GB - that won''t be enough to load the 4-bit, but should be
          enough to load the 3-bit.


          Give it a try @Pourfard and let us know!  So far I''ve only tested it on
          2 x A100 80GB and 6 x L40 48GB.


          Note: the model has to be loaded with Transformers directly (not AutoGPTQ),
          or Text Generation Inference.  Loading with AutoGPTQ, or clients that use
          AutoGPTQ, won''t currently work due to the sharding.  If you''re using text-generation-webui,
          it should work using the Transformers loader, though I''ve not tested that
          yet myself.'
        updatedAt: '2023-09-07T17:02:03.022Z'
      numEdits: 1
      reactions: []
    id: 64fa01d852992431a6b362b3
    type: comment
  author: TheBloke
  content: 'No it should work - you only need enough RAM to load each piece into RAM
    before it goes to VRAM.  And the model is now sharded (split into multiple smaller
    files).


    Each piece is only 10GB, so in theory you only need 10GB RAM + whatever overhead
    there is.


    As for 4 x 24GB - that won''t be enough to load the 4-bit, but should be enough
    to load the 3-bit.


    Give it a try @Pourfard and let us know!  So far I''ve only tested it on 2 x A100
    80GB and 6 x L40 48GB.


    Note: the model has to be loaded with Transformers directly (not AutoGPTQ), or
    Text Generation Inference.  Loading with AutoGPTQ, or clients that use AutoGPTQ,
    won''t currently work due to the sharding.  If you''re using text-generation-webui,
    it should work using the Transformers loader, though I''ve not tested that yet
    myself.'
  created_at: 2023-09-07 16:01:12+00:00
  edited: true
  hidden: false
  id: 64fa01d852992431a6b362b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-07T17:09:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801066517829895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>It''s hard to train 180B model to unleash it''s potential. A lot of resourced
          are required though.</p>

          </blockquote>

          <p>Sequence length of 2048 is also disappointing.  And I don''t think RoPE
          scaling works with Falcon yet (though I might be wrong - haven''t checked
          if that was added in Transformers 4.33.0)</p>

          '
        raw: '> It''s hard to train 180B model to unleash it''s potential. A lot of
          resourced are required though.


          Sequence length of 2048 is also disappointing.  And I don''t think RoPE
          scaling works with Falcon yet (though I might be wrong - haven''t checked
          if that was added in Transformers 4.33.0)'
        updatedAt: '2023-09-07T17:09:13.536Z'
      numEdits: 0
      reactions: []
    id: 64fa03b98c21ebb3db48b84b
    type: comment
  author: TheBloke
  content: '> It''s hard to train 180B model to unleash it''s potential. A lot of
    resourced are required though.


    Sequence length of 2048 is also disappointing.  And I don''t think RoPE scaling
    works with Falcon yet (though I might be wrong - haven''t checked if that was
    added in Transformers 4.33.0)'
  created_at: 2023-09-07 16:09:13+00:00
  edited: false
  hidden: false
  id: 64fa03b98c21ebb3db48b84b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-09-07T20:41:56.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9876298904418945
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>falcon was promising few a months ago  ( maybe will be in the future
          ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily
          beats 180b! model.<br>Meta said they are going to release llma3  in near
          future the should be as powerful as GPT4 sooo  .... not mention they also
          said even started working on llama4 as well ...</p>

          '
        raw: "falcon was promising few a months ago  ( maybe will be in the future\
          \ ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily\
          \ beats 180b! model.  \nMeta said they are going to release llma3  in near\
          \ future the should be as powerful as GPT4 sooo  .... not mention they also\
          \ said even started working on llama4 as well ..."
        updatedAt: '2023-09-07T20:41:56.799Z'
      numEdits: 0
      reactions: []
    id: 64fa3594010f41e43507a256
    type: comment
  author: mirek190
  content: "falcon was promising few a months ago  ( maybe will be in the future )\
    \ now seems obsolete comparing to llama2 variations ... 70b llama2 easily beats\
    \ 180b! model.  \nMeta said they are going to release llma3  in near future the\
    \ should be as powerful as GPT4 sooo  .... not mention they also said even started\
    \ working on llama4 as well ..."
  created_at: 2023-09-07 19:41:56+00:00
  edited: false
  hidden: false
  id: 64fa3594010f41e43507a256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-07T21:00:06.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798343181610107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <p>falcon was promising few a months ago  ( maybe will be in the future
          ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily
          beats 180b! model.<br>Meta said they are going to release llma3  in near
          future the should be as powerful as GPT4 sooo  .... not mention they also
          said even started working on llama4 as well ...</p>

          </blockquote>

          <p>Base Llama2 was already beaten by this 180B model. 70b llama2 variations
          are fine tuned models and this 180B Falcon model is barebone model when
          it gets more trained and fine tuned you will see how capable it actually
          is.</p>

          '
        raw: "> falcon was promising few a months ago  ( maybe will be in the future\
          \ ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily\
          \ beats 180b! model.  \n> Meta said they are going to release llma3  in\
          \ near future the should be as powerful as GPT4 sooo  .... not mention they\
          \ also said even started working on llama4 as well ...\n\nBase Llama2 was\
          \ already beaten by this 180B model. 70b llama2 variations are fine tuned\
          \ models and this 180B Falcon model is barebone model when it gets more\
          \ trained and fine tuned you will see how capable it actually is."
        updatedAt: '2023-09-07T21:00:06.180Z'
      numEdits: 0
      reactions: []
    id: 64fa39d6b60e2c9ddde96231
    type: comment
  author: Flanua
  content: "> falcon was promising few a months ago  ( maybe will be in the future\
    \ ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily beats\
    \ 180b! model.  \n> Meta said they are going to release llma3  in near future\
    \ the should be as powerful as GPT4 sooo  .... not mention they also said even\
    \ started working on llama4 as well ...\n\nBase Llama2 was already beaten by this\
    \ 180B model. 70b llama2 variations are fine tuned models and this 180B Falcon\
    \ model is barebone model when it gets more trained and fine tuned you will see\
    \ how capable it actually is."
  created_at: 2023-09-07 20:00:06+00:00
  edited: false
  hidden: false
  id: 64fa39d6b60e2c9ddde96231
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
      fullname: Tea Lover
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tea-lover-418
      type: user
    createdAt: '2023-09-08T08:14:18.000Z'
    data:
      edited: false
      editors:
      - tea-lover-418
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789724946022034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
          fullname: Tea Lover
          isHf: false
          isPro: false
          name: tea-lover-418
          type: user
        html: '<blockquote>

          <blockquote>

          <p>falcon was promising few a months ago  ( maybe will be in the future
          ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily
          beats 180b! model.<br>Meta said they are going to release llma3  in near
          future the should be as powerful as GPT4 sooo  .... not mention they also
          said even started working on llama4 as well ...</p>

          </blockquote>

          <p>Base Llama2 was already beaten by this 180B model. 70b llama2 variations
          are fine tuned models and this 180B Falcon model is barebone model when
          it gets more trained and fine tuned you will see how capable it actually
          is.</p>

          </blockquote>

          <p>But how would you run it. You need insane amounts of RAM to even run
          this model, let alone fine tune it.</p>

          '
        raw: "> > falcon was promising few a months ago  ( maybe will be in the future\
          \ ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily\
          \ beats 180b! model.  \n> > Meta said they are going to release llma3  in\
          \ near future the should be as powerful as GPT4 sooo  .... not mention they\
          \ also said even started working on llama4 as well ...\n> \n> Base Llama2\
          \ was already beaten by this 180B model. 70b llama2 variations are fine\
          \ tuned models and this 180B Falcon model is barebone model when it gets\
          \ more trained and fine tuned you will see how capable it actually is.\n\
          \nBut how would you run it. You need insane amounts of RAM to even run this\
          \ model, let alone fine tune it."
        updatedAt: '2023-09-08T08:14:18.331Z'
      numEdits: 0
      reactions: []
    id: 64fad7da0e486522f80f4c4f
    type: comment
  author: tea-lover-418
  content: "> > falcon was promising few a months ago  ( maybe will be in the future\
    \ ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily beats\
    \ 180b! model.  \n> > Meta said they are going to release llma3  in near future\
    \ the should be as powerful as GPT4 sooo  .... not mention they also said even\
    \ started working on llama4 as well ...\n> \n> Base Llama2 was already beaten\
    \ by this 180B model. 70b llama2 variations are fine tuned models and this 180B\
    \ Falcon model is barebone model when it gets more trained and fine tuned you\
    \ will see how capable it actually is.\n\nBut how would you run it. You need insane\
    \ amounts of RAM to even run this model, let alone fine tune it."
  created_at: 2023-09-08 07:14:18+00:00
  edited: false
  hidden: false
  id: 64fad7da0e486522f80f4c4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-08T16:45:17.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9758232831954956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>falcon was promising few a months ago  ( maybe will be in the future
          ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily
          beats 180b! model.<br>Meta said they are going to release llma3  in near
          future the should be as powerful as GPT4 sooo  .... not mention they also
          said even started working on llama4 as well ...</p>

          </blockquote>

          <p>Base Llama2 was already beaten by this 180B model. 70b llama2 variations
          are fine tuned models and this 180B Falcon model is barebone model when
          it gets more trained and fine tuned you will see how capable it actually
          is.</p>

          </blockquote>

          <p>But how would you run it. You need insane amounts of RAM to even run
          this model, let alone fine tune it.</p>

          </blockquote>

          <p>I will just use hp proliant dl360 gen10 server with 400GB of ram. I don''t
          have that much ram yet but I will add it in the near future. Also we can
          use a swap file to increase the amount of ram available but with extremely
          slow performance though.</p>

          '
        raw: " \n> > > falcon was promising few a months ago  ( maybe will be in the\
          \ future ) now seems obsolete comparing to llama2 variations ... 70b llama2\
          \ easily beats 180b! model.  \n> > > Meta said they are going to release\
          \ llma3  in near future the should be as powerful as GPT4 sooo  .... not\
          \ mention they also said even started working on llama4 as well ...\n> >\
          \ \n> > Base Llama2 was already beaten by this 180B model. 70b llama2 variations\
          \ are fine tuned models and this 180B Falcon model is barebone model when\
          \ it gets more trained and fine tuned you will see how capable it actually\
          \ is.\n> \n> But how would you run it. You need insane amounts of RAM to\
          \ even run this model, let alone fine tune it.\n\nI will just use hp proliant\
          \ dl360 gen10 server with 400GB of ram. I don't have that much ram yet but\
          \ I will add it in the near future. Also we can use a swap file to increase\
          \ the amount of ram available but with extremely slow performance though."
        updatedAt: '2023-09-09T18:21:24.956Z'
      numEdits: 2
      reactions: []
    id: 64fb4f9db961d0d12c87f3e5
    type: comment
  author: Flanua
  content: " \n> > > falcon was promising few a months ago  ( maybe will be in the\
    \ future ) now seems obsolete comparing to llama2 variations ... 70b llama2 easily\
    \ beats 180b! model.  \n> > > Meta said they are going to release llma3  in near\
    \ future the should be as powerful as GPT4 sooo  .... not mention they also said\
    \ even started working on llama4 as well ...\n> > \n> > Base Llama2 was already\
    \ beaten by this 180B model. 70b llama2 variations are fine tuned models and this\
    \ 180B Falcon model is barebone model when it gets more trained and fine tuned\
    \ you will see how capable it actually is.\n> \n> But how would you run it. You\
    \ need insane amounts of RAM to even run this model, let alone fine tune it.\n\
    \nI will just use hp proliant dl360 gen10 server with 400GB of ram. I don't have\
    \ that much ram yet but I will add it in the near future. Also we can use a swap\
    \ file to increase the amount of ram available but with extremely slow performance\
    \ though."
  created_at: 2023-09-08 15:45:17+00:00
  edited: true
  hidden: false
  id: 64fb4f9db961d0d12c87f3e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16fd1b7ffe17bc559f1206a75a24d0e6.svg
      fullname: Alina
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: linavelz
      type: user
    createdAt: '2023-09-15T18:57:31.000Z'
    data:
      edited: false
      editors:
      - linavelz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9921047687530518
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16fd1b7ffe17bc559f1206a75a24d0e6.svg
          fullname: Alina
          isHf: false
          isPro: false
          name: linavelz
          type: user
        html: '<p>What was your Tk/s on the 180B  on 2 A100''s?</p>

          '
        raw: What was your Tk/s on the 180B  on 2 A100's?
        updatedAt: '2023-09-15T18:57:31.903Z'
      numEdits: 0
      reactions: []
    id: 6504a91b1dab4860088e475a
    type: comment
  author: linavelz
  content: What was your Tk/s on the 180B  on 2 A100's?
  created_at: 2023-09-15 17:57:31+00:00
  edited: false
  hidden: false
  id: 6504a91b1dab4860088e475a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Falcon-180B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Brilliant!
