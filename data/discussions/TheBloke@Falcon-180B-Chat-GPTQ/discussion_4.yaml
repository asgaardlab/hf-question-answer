!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nsegev
conflicting_files: null
created_at: 2023-09-16 18:15:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
      fullname: Noam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nsegev
      type: user
    createdAt: '2023-09-16T19:15:34.000Z'
    data:
      edited: false
      editors:
      - nsegev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7461432218551636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
          fullname: Noam
          isHf: false
          isPro: false
          name: nsegev
          type: user
        html: '<p>Hi,</p>

          <p>I know the full Falcon-180b runs in Sagemaker on a p4de.24xlarge instance
          (8 * A100-80GB).<br>An 8-bit variant runs in Sagemaker on a p4d.24xlarge
          instance (8 * A100-40GB).<br>I''m trying to see if the 4bit GPTQ variant
          will work on a g5.48xlarge instance (8 * A10-24GB).</p>

          <p>I''m using HuggingFace TGI, any ideas why I''m seeing the following error:
          "NotImplementedError: Tensor Parallelism is not implemented for 14 not divisible
          by 8".</p>

          <p>Thanks</p>

          '
        raw: "Hi,\r\n\r\nI know the full Falcon-180b runs in Sagemaker on a p4de.24xlarge\
          \ instance (8 * A100-80GB).\r\nAn 8-bit variant runs in Sagemaker on a p4d.24xlarge\
          \ instance (8 * A100-40GB).\r\nI'm trying to see if the 4bit GPTQ variant\
          \ will work on a g5.48xlarge instance (8 * A10-24GB).\r\n\r\nI'm using HuggingFace\
          \ TGI, any ideas why I'm seeing the following error: \"NotImplementedError:\
          \ Tensor Parallelism is not implemented for 14 not divisible by 8\".\r\n\
          \r\nThanks"
        updatedAt: '2023-09-16T19:15:34.211Z'
      numEdits: 0
      reactions: []
    id: 6505fed6c58de848bb62f028
    type: comment
  author: nsegev
  content: "Hi,\r\n\r\nI know the full Falcon-180b runs in Sagemaker on a p4de.24xlarge\
    \ instance (8 * A100-80GB).\r\nAn 8-bit variant runs in Sagemaker on a p4d.24xlarge\
    \ instance (8 * A100-40GB).\r\nI'm trying to see if the 4bit GPTQ variant will\
    \ work on a g5.48xlarge instance (8 * A10-24GB).\r\n\r\nI'm using HuggingFace\
    \ TGI, any ideas why I'm seeing the following error: \"NotImplementedError: Tensor\
    \ Parallelism is not implemented for 14 not divisible by 8\".\r\n\r\nThanks"
  created_at: 2023-09-16 18:15:34+00:00
  edited: false
  hidden: false
  id: 6505fed6c58de848bb62f028
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
      fullname: Noam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nsegev
      type: user
    createdAt: '2023-10-01T17:26:06.000Z'
    data:
      edited: false
      editors:
      - nsegev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9382544159889221
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
          fullname: Noam
          isHf: false
          isPro: false
          name: nsegev
          type: user
        html: '<p>With the release of TGI 1.1.0 it is possible to load this model
          to a g5.48x on AWS.<br>But there seems to be a memory error when the model
          loads and tries to prefill.<br>Error repeats for both 4-bit and 3-bit versions,
          which is odd</p>

          '
        raw: 'With the release of TGI 1.1.0 it is possible to load this model to a
          g5.48x on AWS.

          But there seems to be a memory error when the model loads and tries to prefill.

          Error repeats for both 4-bit and 3-bit versions, which is odd'
        updatedAt: '2023-10-01T17:26:06.919Z'
      numEdits: 0
      reactions: []
    id: 6519abaeff197684a541d2c3
    type: comment
  author: nsegev
  content: 'With the release of TGI 1.1.0 it is possible to load this model to a g5.48x
    on AWS.

    But there seems to be a memory error when the model loads and tries to prefill.

    Error repeats for both 4-bit and 3-bit versions, which is odd'
  created_at: 2023-10-01 16:26:06+00:00
  edited: false
  hidden: false
  id: 6519abaeff197684a541d2c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a56f6cdf73a57feb9a7e160c5600fd0.svg
      fullname: Kyle Ulrich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ulrichkr
      type: user
    createdAt: '2023-10-20T03:31:06.000Z'
    data:
      edited: false
      editors:
      - ulrichkr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5073046088218689
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a56f6cdf73a57feb9a7e160c5600fd0.svg
          fullname: Kyle Ulrich
          isHf: false
          isPro: false
          name: ulrichkr
          type: user
        html: "<p>Has anyone managed to deploy via TGI 1.1.0 on g5.48xlarge? I am\
          \ seeing the same CUDA illegal memory access error during prefill:</p>\n\
          <pre><code>#033[2m2023-10-20T03:26:32.395175Z#033[0m #033[32m INFO#033[0m\
          \ #033[2mtext_generation_router#033[0m#033[2m:#033[0m #033[2mrouter/src/main.rs#033[0m#033[2m:#033[0m#033[2m213:#033[0m\
          \ Warming up model\n#033[2m2023-10-20T03:26:34.571922Z#033[0m #033[31mERROR#033[0m\
          \ #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Warmup encountered\
          \ an error.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 672, in warmup\n    _, batch = self.generate_token(batch)\n  File\
          \ \"/opt/conda/lib/python3.9/contextlib.py\", line 79, in inner\n    return\
          \ func(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 753, in generate_token\n    raise e\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 750, in generate_token\n    out = self.forward(batch)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 717, in forward\n    return self.model.forward(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 643, in forward\n    hidden_states = self.transformer(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 603, in forward\n    hidden_states, residual = layer(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 521, in forward\n    attn_output = self.self_attention(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 353, in forward\n    return self.dense(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 349, in forward\n    out = QuantLinearFunction.apply(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/autograd/function.py\", line\
          \ 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\"\
          , line 106, in decorate_fwd\n    return fwd(*args, **kwargs)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 244, in forward\n    output = matmul248(input, qweight, scales, qzeros,\
          \ g_idx, bits, maxq)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 216, in matmul248\n    matmul_248_kernel[grid](\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 110, in run\n    timings = {\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 111, in &lt;dictcomp&gt;\n    config: self._bench(*args, config=config,\
          \ **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 90, in _bench\n    return triton.testing.do_bench(\n  File \"/opt/conda/lib/python3.9/site-packages/triton/testing.py\"\
          , line 144, in do_bench\n    torch.cuda.synchronize()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py\"\
          , line 688, in synchronize\n    return torch._C._cuda_synchronize()\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nThe\
          \ above exception was the direct cause of the following exception:\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\"\
          , line 159, in invoke_intercept_method\n    return await self.intercept(\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\n    return await response\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\n    raise error\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\n    return await behavior(request_or_iterator,\
          \ context)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 72, in Warmup\n    max_supported_total_tokens = self.model.warmup(batch)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 674, in warmup\n    raise RuntimeError(\nRuntimeError: Not enough\
          \ memory to handle 4096 prefill tokens. You need to decrease `--max-batch-prefill-tokens`\n\
          #033[2m2023-10-20T03:26:34.573224Z#033[0m #033[31mERROR#033[0m #033[1mwarmup#033[0m#033[1m{#033[0m#033[3mmax_input_length#033[0m#033[2m=#033[0m1024\
          \ #033[3mmax_prefill_tokens#033[0m#033[2m=#033[0m4096#033[1m}#033[0m#033[2m:#033[0m#033[1mwarmup#033[0m#033[2m:#033[0m\
          \ #033[2mtext_generation_client#033[0m#033[2m:#033[0m #033[2mrouter/client/src/lib.rs#033[0m#033[2m:#033[0m#033[2m33:#033[0m\
          \ Server error: Unexpected &lt;class 'RuntimeError'&gt;: CUDA error: an\
          \ illegal memory access was encountered\nCUDA kernel errors might be asynchronously\
          \ reported at some other API call, so the stacktrace below might be incorrect.\n\
          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\n</code></pre>\n<p>Decreasing max-batch-prefill-tokens\
          \ does not help resolve this error.</p>\n"
        raw: "Has anyone managed to deploy via TGI 1.1.0 on g5.48xlarge? I am seeing\
          \ the same CUDA illegal memory access error during prefill:\n```\n#033[2m2023-10-20T03:26:32.395175Z#033[0m\
          \ #033[32m INFO#033[0m #033[2mtext_generation_router#033[0m#033[2m:#033[0m\
          \ #033[2mrouter/src/main.rs#033[0m#033[2m:#033[0m#033[2m213:#033[0m Warming\
          \ up model\n#033[2m2023-10-20T03:26:34.571922Z#033[0m #033[31mERROR#033[0m\
          \ #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Warmup encountered\
          \ an error.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 672, in warmup\n    _, batch = self.generate_token(batch)\n  File\
          \ \"/opt/conda/lib/python3.9/contextlib.py\", line 79, in inner\n    return\
          \ func(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 753, in generate_token\n    raise e\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 750, in generate_token\n    out = self.forward(batch)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 717, in forward\n    return self.model.forward(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 643, in forward\n    hidden_states = self.transformer(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 603, in forward\n    hidden_states, residual = layer(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 521, in forward\n    attn_output = self.self_attention(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line\
          \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
          , line 353, in forward\n    return self.dense(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 349, in forward\n    out = QuantLinearFunction.apply(\n  File \"\
          /opt/conda/lib/python3.9/site-packages/torch/autograd/function.py\", line\
          \ 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\"\
          , line 106, in decorate_fwd\n    return fwd(*args, **kwargs)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 244, in forward\n    output = matmul248(input, qweight, scales, qzeros,\
          \ g_idx, bits, maxq)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
          , line 216, in matmul248\n    matmul_248_kernel[grid](\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 110, in run\n    timings = {\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 111, in <dictcomp>\n    config: self._bench(*args, config=config,\
          \ **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
          , line 90, in _bench\n    return triton.testing.do_bench(\n  File \"/opt/conda/lib/python3.9/site-packages/triton/testing.py\"\
          , line 144, in do_bench\n    torch.cuda.synchronize()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py\"\
          , line 688, in synchronize\n    return torch._C._cuda_synchronize()\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nThe\
          \ above exception was the direct cause of the following exception:\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\"\
          , line 159, in invoke_intercept_method\n    return await self.intercept(\n\
          > File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\n    return await response\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\n    raise error\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\n    return await behavior(request_or_iterator,\
          \ context)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 72, in Warmup\n    max_supported_total_tokens = self.model.warmup(batch)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 674, in warmup\n    raise RuntimeError(\nRuntimeError: Not enough\
          \ memory to handle 4096 prefill tokens. You need to decrease `--max-batch-prefill-tokens`\n\
          #033[2m2023-10-20T03:26:34.573224Z#033[0m #033[31mERROR#033[0m #033[1mwarmup#033[0m#033[1m{#033[0m#033[3mmax_input_length#033[0m#033[2m=#033[0m1024\
          \ #033[3mmax_prefill_tokens#033[0m#033[2m=#033[0m4096#033[1m}#033[0m#033[2m:#033[0m#033[1mwarmup#033[0m#033[2m:#033[0m\
          \ #033[2mtext_generation_client#033[0m#033[2m:#033[0m #033[2mrouter/client/src/lib.rs#033[0m#033[2m:#033[0m#033[2m33:#033[0m\
          \ Server error: Unexpected <class 'RuntimeError'>: CUDA error: an illegal\
          \ memory access was encountered\nCUDA kernel errors might be asynchronously\
          \ reported at some other API call, so the stacktrace below might be incorrect.\n\
          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\n```\nDecreasing max-batch-prefill-tokens\
          \ does not help resolve this error."
        updatedAt: '2023-10-20T03:31:06.468Z'
      numEdits: 0
      reactions: []
    id: 6531f47a69fffcfe1554f306
    type: comment
  author: ulrichkr
  content: "Has anyone managed to deploy via TGI 1.1.0 on g5.48xlarge? I am seeing\
    \ the same CUDA illegal memory access error during prefill:\n```\n#033[2m2023-10-20T03:26:32.395175Z#033[0m\
    \ #033[32m INFO#033[0m #033[2mtext_generation_router#033[0m#033[2m:#033[0m #033[2mrouter/src/main.rs#033[0m#033[2m:#033[0m#033[2m213:#033[0m\
    \ Warming up model\n#033[2m2023-10-20T03:26:34.571922Z#033[0m #033[31mERROR#033[0m\
    \ #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Method Warmup encountered\
    \ an error.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 672, in warmup\n    _, batch = self.generate_token(batch)\n  File \"/opt/conda/lib/python3.9/contextlib.py\"\
    , line 79, in inner\n    return func(*args, **kwds)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 753, in generate_token\n    raise e\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 750, in generate_token\n    out = self.forward(batch)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 717, in forward\n    return self.model.forward(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 643, in forward\n    hidden_states = self.transformer(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 603, in forward\n    hidden_states, residual = layer(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 521, in forward\n    attn_output = self.self_attention(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_rw_modeling.py\"\
    , line 353, in forward\n    return self.dense(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
    , line 349, in forward\n    out = QuantLinearFunction.apply(\n  File \"/opt/conda/lib/python3.9/site-packages/torch/autograd/function.py\"\
    , line 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\"\
    , line 106, in decorate_fwd\n    return fwd(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
    , line 244, in forward\n    output = matmul248(input, qweight, scales, qzeros,\
    \ g_idx, bits, maxq)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/quant_linear.py\"\
    , line 216, in matmul248\n    matmul_248_kernel[grid](\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
    , line 110, in run\n    timings = {\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
    , line 111, in <dictcomp>\n    config: self._bench(*args, config=config, **kwargs)\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/custom_autotune.py\"\
    , line 90, in _bench\n    return triton.testing.do_bench(\n  File \"/opt/conda/lib/python3.9/site-packages/triton/testing.py\"\
    , line 144, in do_bench\n    torch.cuda.synchronize()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py\"\
    , line 688, in synchronize\n    return torch._C._cuda_synchronize()\nRuntimeError:\
    \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors might\
    \ be asynchronously reported at some other API call, so the stacktrace below might\
    \ be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile\
    \ with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nThe above exception\
    \ was the direct cause of the following exception:\nTraceback (most recent call\
    \ last):\n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n\
    \    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n\
    \    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in\
    \ invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line\
    \ 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/grpc_interceptor/server.py\", line\
    \ 159, in invoke_intercept_method\n    return await self.intercept(\n> File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/interceptor.py\"\
    , line 21, in intercept\n    return await response\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 82, in _unary_interceptor\n    raise error\n  File \"/opt/conda/lib/python3.9/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 73, in _unary_interceptor\n    return await behavior(request_or_iterator,\
    \ context)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 72, in Warmup\n    max_supported_total_tokens = self.model.warmup(batch)\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 674, in warmup\n    raise RuntimeError(\nRuntimeError: Not enough memory\
    \ to handle 4096 prefill tokens. You need to decrease `--max-batch-prefill-tokens`\n\
    #033[2m2023-10-20T03:26:34.573224Z#033[0m #033[31mERROR#033[0m #033[1mwarmup#033[0m#033[1m{#033[0m#033[3mmax_input_length#033[0m#033[2m=#033[0m1024\
    \ #033[3mmax_prefill_tokens#033[0m#033[2m=#033[0m4096#033[1m}#033[0m#033[2m:#033[0m#033[1mwarmup#033[0m#033[2m:#033[0m\
    \ #033[2mtext_generation_client#033[0m#033[2m:#033[0m #033[2mrouter/client/src/lib.rs#033[0m#033[2m:#033[0m#033[2m33:#033[0m\
    \ Server error: Unexpected <class 'RuntimeError'>: CUDA error: an illegal memory\
    \ access was encountered\nCUDA kernel errors might be asynchronously reported\
    \ at some other API call, so the stacktrace below might be incorrect.\nFor debugging\
    \ consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\n```\nDecreasing max-batch-prefill-tokens\
    \ does not help resolve this error."
  created_at: 2023-10-20 02:31:06+00:00
  edited: false
  hidden: false
  id: 6531f47a69fffcfe1554f306
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-10-20T13:02:37.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518749117851257
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p>It seems that you need to adjust the <code>max-batch-prefill-tokens</code>
          value since you ran out of memory on the instance. </p>

          '
        raw: 'It seems that you need to adjust the `max-batch-prefill-tokens` value
          since you ran out of memory on the instance. '
        updatedAt: '2023-10-20T13:02:37.101Z'
      numEdits: 0
      reactions: []
    id: 65327a6dd434308ba40d3d9a
    type: comment
  author: philschmid
  content: 'It seems that you need to adjust the `max-batch-prefill-tokens` value
    since you ran out of memory on the instance. '
  created_at: 2023-10-20 12:02:37+00:00
  edited: false
  hidden: false
  id: 65327a6dd434308ba40d3d9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d497e9e74568639fddd140f03f00f6f6.svg
      fullname: dbt20
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dbt20
      type: user
    createdAt: '2023-11-22T13:42:19.000Z'
    data:
      edited: false
      editors:
      - dbt20
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8371090888977051
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d497e9e74568639fddd140f03f00f6f6.svg
          fullname: dbt20
          isHf: false
          isPro: false
          name: dbt20
          type: user
        html: '<p>Any update on this? I''m still getting the same error on the g5.48xl
          with TGI 1.1.0 (8x24 GB VRAM) with the GPTQ version of falcon 180.<br>I
          tried down to 100 max prefill tokens and I still get "You need to decrease
          <code>--max-batch-prefill-tokens</code>"<br>How to estimate the extra memory
          requirement needed after the model is loaded ?</p>

          '
        raw: "Any update on this? I'm still getting the same error on the g5.48xl\
          \ with TGI 1.1.0 (8x24 GB VRAM) with the GPTQ version of falcon 180. \n\
          I tried down to 100 max prefill tokens and I still get \"You need to decrease\
          \ `--max-batch-prefill-tokens`\"\nHow to estimate the extra memory requirement\
          \ needed after the model is loaded ?"
        updatedAt: '2023-11-22T13:42:19.921Z'
      numEdits: 0
      reactions: []
    id: 655e053bca52f87505fda798
    type: comment
  author: dbt20
  content: "Any update on this? I'm still getting the same error on the g5.48xl with\
    \ TGI 1.1.0 (8x24 GB VRAM) with the GPTQ version of falcon 180. \nI tried down\
    \ to 100 max prefill tokens and I still get \"You need to decrease `--max-batch-prefill-tokens`\"\
    \nHow to estimate the extra memory requirement needed after the model is loaded\
    \ ?"
  created_at: 2023-11-22 13:42:19+00:00
  edited: false
  hidden: false
  id: 655e053bca52f87505fda798
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
      fullname: Noam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nsegev
      type: user
    createdAt: '2023-11-22T13:53:41.000Z'
    data:
      edited: false
      editors:
      - nsegev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6029059886932373
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1914f88b666d45b226db5b20908a312f.svg
          fullname: Noam
          isHf: false
          isPro: false
          name: nsegev
          type: user
        html: '<p>I''ll be honest, I couldn''t get the GPTQ version working on TGI
          1.1.0, but 1.1.0 does support bitsandbytes-nf4 which did work for me on
          g5.48xl<br>My configuration is:</p>

          <p>config = {<br>        ''SM_NUM_GPUS'': json.dumps(8),<br>        ''MAX_TOTAL_TOKENS'':
          json.dumps(2048 + 512),<br>        ''MAX_INPUT_LENGTH'': json.dumps(2048),<br>        ''HUGGING_FACE_HUB_TOKEN'':
          HUGGING_FACE_HUB_TOKEN,<br>        ''HF_MODEL_ID'': ''tiiuae/falcon-180B-chat'',<br>        ''HF_MODEL_QUANTIZE'':
          ''bitsandbytes-nf4'',<br>    }<br>f_model = HuggingFaceModel(role=SAGE_ROLE,
          image_uri=LLM_CONTAINER, env=config)<br>predictor = hf_model.deploy(initial_instance_count=1,
          instance_type=''ml.g5.48xlarge'',<br>                                 container_startup_health_check_timeout=900)</p>

          <p>I think they had a problem with TGI 1.1.0 (<a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/issues/1000">https://github.com/huggingface/text-generation-inference/issues/1000</a>),
          you could try 1.1.1 and see if resolved</p>

          '
        raw: "I'll be honest, I couldn't get the GPTQ version working on TGI 1.1.0,\
          \ but 1.1.0 does support bitsandbytes-nf4 which did work for me on g5.48xl\n\
          My configuration is:\n\nconfig = {\n        'SM_NUM_GPUS': json.dumps(8),\n\
          \        'MAX_TOTAL_TOKENS': json.dumps(2048 + 512),\n        'MAX_INPUT_LENGTH':\
          \ json.dumps(2048),\n        'HUGGING_FACE_HUB_TOKEN': HUGGING_FACE_HUB_TOKEN,\n\
          \        'HF_MODEL_ID': 'tiiuae/falcon-180B-chat',\n        'HF_MODEL_QUANTIZE':\
          \ 'bitsandbytes-nf4',\n    }\nf_model = HuggingFaceModel(role=SAGE_ROLE,\
          \ image_uri=LLM_CONTAINER, env=config)\npredictor = hf_model.deploy(initial_instance_count=1,\
          \ instance_type='ml.g5.48xlarge',\n                                 container_startup_health_check_timeout=900)\n\
          \n\nI think they had a problem with TGI 1.1.0 (https://github.com/huggingface/text-generation-inference/issues/1000),\
          \ you could try 1.1.1 and see if resolved"
        updatedAt: '2023-11-22T13:53:41.492Z'
      numEdits: 0
      reactions: []
    id: 655e07e58c2d4379a718162a
    type: comment
  author: nsegev
  content: "I'll be honest, I couldn't get the GPTQ version working on TGI 1.1.0,\
    \ but 1.1.0 does support bitsandbytes-nf4 which did work for me on g5.48xl\nMy\
    \ configuration is:\n\nconfig = {\n        'SM_NUM_GPUS': json.dumps(8),\n   \
    \     'MAX_TOTAL_TOKENS': json.dumps(2048 + 512),\n        'MAX_INPUT_LENGTH':\
    \ json.dumps(2048),\n        'HUGGING_FACE_HUB_TOKEN': HUGGING_FACE_HUB_TOKEN,\n\
    \        'HF_MODEL_ID': 'tiiuae/falcon-180B-chat',\n        'HF_MODEL_QUANTIZE':\
    \ 'bitsandbytes-nf4',\n    }\nf_model = HuggingFaceModel(role=SAGE_ROLE, image_uri=LLM_CONTAINER,\
    \ env=config)\npredictor = hf_model.deploy(initial_instance_count=1, instance_type='ml.g5.48xlarge',\n\
    \                                 container_startup_health_check_timeout=900)\n\
    \n\nI think they had a problem with TGI 1.1.0 (https://github.com/huggingface/text-generation-inference/issues/1000),\
    \ you could try 1.1.1 and see if resolved"
  created_at: 2023-11-22 13:53:41+00:00
  edited: false
  hidden: false
  id: 655e07e58c2d4379a718162a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Falcon-180B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Can run on SageMaker g5 instance?
