!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jalbarracin
conflicting_files: null
created_at: 2023-03-28 13:24:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
      fullname: JAVIER ALBARRACIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jalbarracin
      type: user
    createdAt: '2023-03-28T14:24:29.000Z'
    data:
      edited: false
      editors:
      - jalbarracin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
          fullname: JAVIER ALBARRACIN
          isHf: false
          isPro: true
          name: jalbarracin
          type: user
        html: "<p>While running the colab (using the 'bertin-project/bertin-alpaca-lora-7b'\
          \ model) I am getting this error:</p>\n<p>The model 'PeftModelForCausalLM'\
          \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
          \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM',\
          \ 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LLaMAForCausalLM', 'MarianForCausalLM',\
          \ 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].<br>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E<br>\u2502 in :5                                     \
          \                                               \u2502<br>\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F</p>\n"
        raw: "While running the colab (using the 'bertin-project/bertin-alpaca-lora-7b'\
          \ model) I am getting this error:\r\n\r\nThe model 'PeftModelForCausalLM'\
          \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
          \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM',\
          \ 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LLaMAForCausalLM', 'MarianForCausalLM',\
          \ 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\r\n\u2502 in <module>:5                             \
          \                                                       \u2502\r\n\u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F"
        updatedAt: '2023-03-28T14:24:29.116Z'
      numEdits: 0
      reactions: []
    id: 6422f89de5c1e06babcee2e6
    type: comment
  author: jalbarracin
  content: "While running the colab (using the 'bertin-project/bertin-alpaca-lora-7b'\
    \ model) I am getting this error:\r\n\r\nThe model 'PeftModelForCausalLM' is not\
    \ supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
    \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
    \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel',\
    \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
    \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM',\
    \ 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LLaMAForCausalLM', 'MarianForCausalLM',\
    \ 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel',\
    \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
    \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM',\
    \ 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM',\
    \ 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM',\
    \ 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
    \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\n\u256D\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\
    \r\n\u2502 in <module>:5                                                     \
    \                               \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u256F"
  created_at: 2023-03-28 13:24:29+00:00
  edited: false
  hidden: false
  id: 6422f89de5c1e06babcee2e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
      fullname: Javier de la Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: versae
      type: user
    createdAt: '2023-03-28T14:49:58.000Z'
    data:
      edited: false
      editors:
      - versae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593016943046-noauth.jpeg?w=200&h=200&f=face
          fullname: Javier de la Rosa
          isHf: false
          isPro: false
          name: versae
          type: user
        html: "<p>That's right! <code>PeftModelForCausalLM</code> is not supported\
          \ yet in Transformers pipelines. So you have two options:</p>\n<ol>\n<li>Consolidate\
          \ the model by merging the adapter into the LLaMA weights.</li>\n<li>Use\
          \ the model's <code>generate()</code> method:</li>\n</ol>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> GenerationConfig\n\n<span class=\"\
          hljs-comment\"># Load the model</span>\nmodel = ...\n\n<span class=\"hljs-comment\"\
          ># Generate prompts from Alpaca template</span>\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">generate_prompt</span>(<span\
          \ class=\"hljs-params\">instruction, <span class=\"hljs-built_in\">input</span>=<span\
          \ class=\"hljs-literal\">None</span></span>):\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">input</span>:\n        <span class=\"\
          hljs-keyword\">return</span> <span class=\"hljs-string\">f\"\"\"Below is\
          \ an instruction that describes a task, paired with an input that provides\
          \ further context. Write a response that appropriately completes the request.\
          \  # noqa: E501</span>\n<span class=\"hljs-string\">### Instruction:</span>\n\
          <span class=\"hljs-string\"><span class=\"hljs-subst\">{instruction}</span></span>\n\
          <span class=\"hljs-string\">### Input:</span>\n<span class=\"hljs-string\"\
          ><span class=\"hljs-subst\">{<span class=\"hljs-built_in\">input</span>}</span></span>\n\
          <span class=\"hljs-string\">### Response:</span>\n<span class=\"hljs-string\"\
          >\"\"\"</span>\n    <span class=\"hljs-keyword\">else</span>:\n        <span\
          \ class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">f\"\"\
          \"Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.  # noqa: E501</span>\n<span class=\"hljs-string\"\
          >### Instruction:</span>\n<span class=\"hljs-string\"><span class=\"hljs-subst\"\
          >{instruction}</span></span>\n<span class=\"hljs-string\">### Response:</span>\n\
          <span class=\"hljs-string\">\"\"\"</span>\n\n<span class=\"hljs-comment\"\
          >#&nbsp;Generate responses</span>\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">generate</span>(<span class=\"hljs-params\"\
          >instruction, <span class=\"hljs-built_in\">input</span>=<span class=\"\
          hljs-literal\">None</span></span>):\n    prompt = generate_prompt(instruction,\
          \ <span class=\"hljs-built_in\">input</span>)\n    inputs = tokenizer(prompt,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n    input_ids\
          \ = inputs[<span class=\"hljs-string\">\"input_ids\"</span>].cuda()\n  \
          \  generation_output = model.generate(\n        input_ids=input_ids,\n \
          \       generation_config=GenerationConfig(temperature=<span class=\"hljs-number\"\
          >0.2</span>, top_p=<span class=\"hljs-number\">0.75</span>, num_beams=<span\
          \ class=\"hljs-number\">4</span>),\n        return_dict_in_generate=<span\
          \ class=\"hljs-literal\">True</span>,\n        output_scores=<span class=\"\
          hljs-literal\">True</span>,\n        max_new_tokens=<span class=\"hljs-number\"\
          >256</span>\n    )\n    <span class=\"hljs-keyword\">for</span> seq <span\
          \ class=\"hljs-keyword\">in</span> generation_output.sequences:\n      \
          \  output = tokenizer.decode(seq)\n        <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Respuesta:\"</span>, output.split(<span\
          \ class=\"hljs-string\">\"### Response:\"</span>)[<span class=\"hljs-number\"\
          >1</span>].strip())\n</code></pre>\n"
        raw: "That's right! `PeftModelForCausalLM` is not supported yet in Transformers\
          \ pipelines. So you have two options:\n1. Consolidate the model by merging\
          \ the adapter into the LLaMA weights.\n2. Use the model's `generate()` method:\n\
          ```python\nfrom transformers import GenerationConfig\n\n# Load the model\n\
          model = ...\n\n# Generate prompts from Alpaca template\ndef generate_prompt(instruction,\
          \ input=None):\n    if input:\n        return f\"\"\"Below is an instruction\
          \ that describes a task, paired with an input that provides further context.\
          \ Write a response that appropriately completes the request.  # noqa: E501\n\
          ### Instruction:\n{instruction}\n### Input:\n{input}\n### Response:\n\"\"\
          \"\n    else:\n        return f\"\"\"Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.  #\
          \ noqa: E501\n### Instruction:\n{instruction}\n### Response:\n\"\"\"\n\n\
          #\_Generate responses\ndef generate(instruction, input=None):\n    prompt\
          \ = generate_prompt(instruction, input)\n    inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n\
          \    generation_output = model.generate(\n        input_ids=input_ids,\n\
          \        generation_config=GenerationConfig(temperature=0.2, top_p=0.75,\
          \ num_beams=4),\n        return_dict_in_generate=True,\n        output_scores=True,\n\
          \        max_new_tokens=256\n    )\n    for seq in generation_output.sequences:\n\
          \        output = tokenizer.decode(seq)\n        print(\"Respuesta:\", output.split(\"\
          ### Response:\")[1].strip())\n```"
        updatedAt: '2023-03-28T14:49:58.249Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - dfrank
        - sidnb13
        - arpan-shrivastava-06
        - mini97
        - BrainSlugs83
        - dhruvmullick
    id: 6422fe963e21deadbe077906
    type: comment
  author: versae
  content: "That's right! `PeftModelForCausalLM` is not supported yet in Transformers\
    \ pipelines. So you have two options:\n1. Consolidate the model by merging the\
    \ adapter into the LLaMA weights.\n2. Use the model's `generate()` method:\n```python\n\
    from transformers import GenerationConfig\n\n# Load the model\nmodel = ...\n\n\
    # Generate prompts from Alpaca template\ndef generate_prompt(instruction, input=None):\n\
    \    if input:\n        return f\"\"\"Below is an instruction that describes a\
    \ task, paired with an input that provides further context. Write a response that\
    \ appropriately completes the request.  # noqa: E501\n### Instruction:\n{instruction}\n\
    ### Input:\n{input}\n### Response:\n\"\"\"\n    else:\n        return f\"\"\"\
    Below is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.  # noqa: E501\n### Instruction:\n{instruction}\n### Response:\n\
    \"\"\"\n\n#\_Generate responses\ndef generate(instruction, input=None):\n    prompt\
    \ = generate_prompt(instruction, input)\n    inputs = tokenizer(prompt, return_tensors=\"\
    pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n    generation_output = model.generate(\n\
    \        input_ids=input_ids,\n        generation_config=GenerationConfig(temperature=0.2,\
    \ top_p=0.75, num_beams=4),\n        return_dict_in_generate=True,\n        output_scores=True,\n\
    \        max_new_tokens=256\n    )\n    for seq in generation_output.sequences:\n\
    \        output = tokenizer.decode(seq)\n        print(\"Respuesta:\", output.split(\"\
    ### Response:\")[1].strip())\n```"
  created_at: 2023-03-28 13:49:58+00:00
  edited: false
  hidden: false
  id: 6422fe963e21deadbe077906
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
      fullname: JAVIER ALBARRACIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jalbarracin
      type: user
    createdAt: '2023-03-28T19:13:15.000Z'
    data:
      edited: false
      editors:
      - jalbarracin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
          fullname: JAVIER ALBARRACIN
          isHf: false
          isPro: true
          name: jalbarracin
          type: user
        html: '<p>Thank you for your fast answer :) I am going to test this asap.
          Thank you.</p>

          '
        raw: Thank you for your fast answer :) I am going to test this asap. Thank
          you.
        updatedAt: '2023-03-28T19:13:15.961Z'
      numEdits: 0
      reactions: []
    id: 64233c4bb0e466cdd989ef99
    type: comment
  author: jalbarracin
  content: Thank you for your fast answer :) I am going to test this asap. Thank you.
  created_at: 2023-03-28 18:13:15+00:00
  edited: false
  hidden: false
  id: 64233c4bb0e466cdd989ef99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
      fullname: JAVIER ALBARRACIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jalbarracin
      type: user
    createdAt: '2023-03-28T19:40:38.000Z'
    data:
      status: closed
    id: 642342b6b25ca6af13dfb658
    type: status-change
  author: jalbarracin
  created_at: 2023-03-28 18:40:38+00:00
  id: 642342b6b25ca6af13dfb658
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
      fullname: JAVIER ALBARRACIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jalbarracin
      type: user
    createdAt: '2023-03-28T19:42:14.000Z'
    data:
      edited: false
      editors:
      - jalbarracin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630a02ece81e1dea2cea6db9/wSbkuUdkalTNDQpi28y6Q.jpeg?w=200&h=200&f=face
          fullname: JAVIER ALBARRACIN
          isHf: false
          isPro: true
          name: jalbarracin
          type: user
        html: '<p>worked by changing "def generate(" with "def generate_prompt("</p>

          '
        raw: worked by changing "def generate(" with "def generate_prompt("
        updatedAt: '2023-03-28T19:42:14.995Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - versae
        - thucdangvan020999
    id: 64234316ccb78df81e830085
    type: comment
  author: jalbarracin
  content: worked by changing "def generate(" with "def generate_prompt("
  created_at: 2023-03-28 18:42:14+00:00
  edited: false
  hidden: false
  id: 64234316ccb78df81e830085
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bertin-project/bertin-alpaca-lora-7b
repo_type: model
status: closed
target_branch: null
title: 'AttributeError: ''TextGenerationPipeline'' object has no attribute ''generate'''
