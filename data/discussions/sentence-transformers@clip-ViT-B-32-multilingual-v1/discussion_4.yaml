!!python/object:huggingface_hub.community.DiscussionWithDetails
author: canavar
conflicting_files: null
created_at: 2023-11-08 14:34:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
      fullname: abdulkadir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: canavar
      type: user
    createdAt: '2023-11-08T14:34:01.000Z'
    data:
      edited: false
      editors:
      - canavar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5548291206359863
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
          fullname: abdulkadir
          isHf: false
          isPro: false
          name: canavar
          type: user
        html: "<p>running <code>optimum-cli export onnx -m sentence-transformers/clip-ViT-B-32-multilingual-v1\
          \ \\ --task feature-extraction models/clip_vit_multilingual_onnx</code></p>\n\
          <p>with following output;</p>\n<pre><code>huggingface/tokenizers: The current\
          \ process just got forked, after parallelism has already been used. Disabling\
          \ parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\
          \    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly\
          \ set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nFramework\
          \ not specified. Using pt to export to ONNX.\nUsing the export variant default.\
          \ Available variants are:\n    - default: The default ONNX variant.\nUsing\
          \ framework PyTorch: 2.1.0\n[...transformers/models/distilbert/modeling_distilbert.py:223):\
          \ TracerWarning: torch.tensor results are registered as constants in the\
          \ trace. You can safely ignore this warning if you use this function to\
          \ create tensors out of constant variables that would be the same every\
          \ time you call this function. In any other case, this might cause the trace\
          \ to be incorrect.\n  mask, torch.tensor(torch.finfo(scores.dtype).min)\n\
          Post-processing the exported models...\nDeduplicating shared (tied) weights...\n\
          Validating ONNX model models/clip_vit_multilingual_onnx/model.onnx...\n\
          \    -[\u2713] ONNX model output names match reference model (last_hidden_state)\n\
          \    - Validating ONNX Model output \"last_hidden_state\":\n        -[\u2713\
          ] (2, 16, 768) matches (2, 16, 768)\n        -[\u2713] all values close\
          \ (atol: 0.0001)\nThe ONNX export succeeded and the exported model was saved\
          \ at: models/clip_vit_multilingual_onnx\n</code></pre>\n<p>importing the\
          \ code and running it;</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer\n<span class=\"hljs-keyword\">from</span> optimum.onnxruntime\
          \ <span class=\"hljs-keyword\">import</span> ORTModelForFeatureExtraction\n\
          \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"models/clip_vit_multilingual_onnx\"</span>)\nmodel = ORTModelForFeatureExtraction.from_pretrained(<span\
          \ class=\"hljs-string\">\"models/clip_vit_multilingual_onnx\"</span>)\n\n\
          inputs = tokenizer(<span class=\"hljs-string\">\"What am I using?\"</span>,\
          \ <span class=\"hljs-string\">\"Using DistilBERT with ONNX Runtime!\"</span>,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\noutputs = model(**inputs)\n\
          last_hidden_state = outputs.last_hidden_state\n<span class=\"hljs-built_in\"\
          >list</span>(last_hidden_state.shape)\n</code></pre>\n<p>gives the output\
          \ slightly(export output: (2, 16, 768), actual: [1, 21, 768])  confirming\
          \ the export command output:</p>\n<pre><code>[1, 21, 768]\n</code></pre>\n\
          <p>If I use the same model from sentence-transformers:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> sentence_transformers\
          \ <span class=\"hljs-keyword\">import</span> SentenceTransformer, util\n\
          <span class=\"hljs-keyword\">import</span> torch\n\ntext_model = SentenceTransformer(<span\
          \ class=\"hljs-string\">'sentence-transformers/clip-ViT-B-32-multilingual-v1'</span>)\n\
          texts = [\n    <span class=\"hljs-string\">\"What am I using?\"</span>,\
          \ <span class=\"hljs-string\">\"Using DistilBERT with ONNX Runtime!\"</span>,\
          \ <span class=\"hljs-comment\"># Spanish: a beach with palm trees</span>\n\
          ]\ntext_embeddings = text_model.encode(texts)\n<span class=\"hljs-built_in\"\
          >print</span>(text_embeddings.shape)\n</code></pre>\n<p>output: </p>\n<pre><code>(2,\
          \ 512)\n</code></pre>\n<p>as documented in: <a href=\"https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1\"\
          >https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1</a>;</p>\n\
          <pre><code>SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128,\
          \ 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1):\
          \ Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False,\
          \ 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens':\
          \ False})\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias':\
          \ False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n)\n\
          </code></pre>\n<p>From model card, sentence-transformers/clip-ViT-B-32-multilingual-v1\
          \ pytorch model has pooling and dense layers, which converts the output\
          \ format from dim 768 to 512(on dense layer) from modules.json.</p>\n<p>In\
          \ case of exporting from optimum-cli, apparently it ignores pooling and\
          \ dense part, resulting an output shape of [1, 21, 768].</p>\n<p>Looking\
          \ at the optimum library, I can't seem to find any information about pooling\
          \ and dense layer inclusion. optimum-cli will not obviously check 1_pooling\
          \ or 2_dense folders at <a href=\"https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1/tree/main\"\
          >https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1/tree/main</a>\
          \ next to pytorch_model.bin</p>\n<p>I am looking for a way to combine 1_pooling\
          \ and 2_dense layers into exported onnx. I tried to create a new model including\
          \ all the layers, Transformer layer coming from pretrained model, and pooling\
          \ and dense layer newly created;</p>\n<pre><code>word_embedding_model =\
          \ AutoModel.from_pretrained(\"sentence-transformers/clip-ViT-B-32-multilingual-v1\"\
          )\npooling_model = models.Pooling(768, \n                              \
          \ pooling_mode_mean_tokens=True, \n                               pooling_mode_cls_token=False,\
          \ \n                               pooling_mode_max_tokens=False,\n    \
          \                           pooling_mode_mean_sqrt_len_tokens=False)\ndense_model\
          \ = models.Dense(in_features=768, \n                           out_features=512,\n\
          \                           bias=False, \n                           activation_function=nn.Identity())\n\
          \nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model,\
          \ dense_model])\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/clip-ViT-B-32-multilingual-v1\"\
          )\n\ninputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX\
          \ Runtime!\", return_tensors=\"pt\")\n\ninput_ids = inputs['input_ids']\n\
          input_names = list(inputs.keys())\n\nprint(input_names)\n# ['input_ids',\
          \ 'attention_mask']\n\n# forward \noutputs = model(input_ids)\n</code></pre>\n\
          <p>when <code>outputs = model(input_ids)</code> run, I get the following\
          \ error;</p>\n<pre><code>---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          download_optimize_model.ipynb Cell 30 line 3\n     28 print(input_names)\n\
          \     30 # Export the model\n---&gt; 31 outputs = model(input_ids)\n   \
          \  32 last_hidden_state = outputs.last_hidden_state\n     34 # Export the\
          \ model\n\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517\
          \ else:\n-&gt; 1518     return self._call_impl(*args, **kwargs)\n\nFile\
          \ /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1523 # this function,\
          \ and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1526        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1527  \
          \   return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result\
          \ = None\n\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/container.py:215,\
          \ in Sequential.forward(self, input)\n...\n--&gt; 401     return inner_dict[k]\n\
          \    402 else:\n    403     return self.to_tuple()[k] \n\n\nKeyError: 'token_embeddings'\n\
          </code></pre>\n<p>The question is, can I combine 3 layers into 1 and export\
          \ that single model consists of tranformer, pooling and dense layers?</p>\n"
        raw: "running ```optimum-cli export onnx -m sentence-transformers/clip-ViT-B-32-multilingual-v1\
          \ \\\r\n--task feature-extraction models/clip_vit_multilingual_onnx```\r\
          \n\r\nwith following output;\r\n\r\n```\r\nhuggingface/tokenizers: The current\
          \ process just got forked, after parallelism has already been used. Disabling\
          \ parallelism to avoid deadlocks...\r\nTo disable this warning, you can\
          \ either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\
          \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true\
          \ | false)\r\nFramework not specified. Using pt to export to ONNX.\r\nUsing\
          \ the export variant default. Available variants are:\r\n    - default:\
          \ The default ONNX variant.\r\nUsing framework PyTorch: 2.1.0\r\n[...transformers/models/distilbert/modeling_distilbert.py:223):\
          \ TracerWarning: torch.tensor results are registered as constants in the\
          \ trace. You can safely ignore this warning if you use this function to\
          \ create tensors out of constant variables that would be the same every\
          \ time you call this function. In any other case, this might cause the trace\
          \ to be incorrect.\r\n  mask, torch.tensor(torch.finfo(scores.dtype).min)\r\
          \nPost-processing the exported models...\r\nDeduplicating shared (tied)\
          \ weights...\r\nValidating ONNX model models/clip_vit_multilingual_onnx/model.onnx...\r\
          \n\t-[\u2713] ONNX model output names match reference model (last_hidden_state)\r\
          \n\t- Validating ONNX Model output \"last_hidden_state\":\r\n\t\t-[\u2713\
          ] (2, 16, 768) matches (2, 16, 768)\r\n\t\t-[\u2713] all values close (atol:\
          \ 0.0001)\r\nThe ONNX export succeeded and the exported model was saved\
          \ at: models/clip_vit_multilingual_onnx\r\n```\r\n\r\nimporting the code\
          \ and running it;\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\
          \nfrom optimum.onnxruntime import ORTModelForFeatureExtraction\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"models/clip_vit_multilingual_onnx\"\
          )\r\nmodel = ORTModelForFeatureExtraction.from_pretrained(\"models/clip_vit_multilingual_onnx\"\
          )\r\n\r\ninputs = tokenizer(\"What am I using?\", \"Using DistilBERT with\
          \ ONNX Runtime!\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\
          \nlast_hidden_state = outputs.last_hidden_state\r\nlist(last_hidden_state.shape)\r\
          \n```\r\n\r\ngives the output slightly(export output: (2, 16, 768), actual:\
          \ [1, 21, 768])  confirming the export command output:\r\n\r\n```\r\n[1,\
          \ 21, 768]\r\n```\r\n\r\nIf I use the same model from sentence-transformers:\r\
          \n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer,\
          \ util\r\nimport torch\r\n\r\ntext_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\r\
          \ntexts = [\r\n    \"What am I using?\", \"Using DistilBERT with ONNX Runtime!\"\
          , # Spanish: a beach with palm trees\r\n]\r\ntext_embeddings = text_model.encode(texts)\r\
          \nprint(text_embeddings.shape)\r\n```\r\n\r\noutput: \r\n\r\n```\r\n(2,\
          \ 512)\r\n```\r\n\r\nas documented in: https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1;\r\
          \n\r\n```\r\nSentenceTransformer(\r\n  (0): Transformer({'max_seq_length':\
          \ 128, 'do_lower_case': False}) with Transformer model: DistilBertModel\
          \ \r\n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token':\
          \ False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False,\
          \ 'pooling_mode_mean_sqrt_len_tokens': False})\r\n  (2): Dense({'in_features':\
          \ 768, 'out_features': 512, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\r\
          \n)\r\n```\r\nFrom model card, sentence-transformers/clip-ViT-B-32-multilingual-v1\
          \ pytorch model has pooling and dense layers, which converts the output\
          \ format from dim 768 to 512(on dense layer) from modules.json.\r\n\r\n\
          In case of exporting from optimum-cli, apparently it ignores pooling and\
          \ dense part, resulting an output shape of [1, 21, 768].\r\n\r\nLooking\
          \ at the optimum library, I can't seem to find any information about pooling\
          \ and dense layer inclusion. optimum-cli will not obviously check 1_pooling\
          \ or 2_dense folders at https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1/tree/main\
          \ next to pytorch_model.bin\r\n\r\nI am looking for a way to combine 1_pooling\
          \ and 2_dense layers into exported onnx. I tried to create a new model including\
          \ all the layers, Transformer layer coming from pretrained model, and pooling\
          \ and dense layer newly created;\r\n\r\n```\r\nword_embedding_model = AutoModel.from_pretrained(\"\
          sentence-transformers/clip-ViT-B-32-multilingual-v1\")\r\npooling_model\
          \ = models.Pooling(768, \r\n                               pooling_mode_mean_tokens=True,\
          \ \r\n                               pooling_mode_cls_token=False, \r\n\
          \                               pooling_mode_max_tokens=False,\r\n     \
          \                          pooling_mode_mean_sqrt_len_tokens=False)\r\n\
          dense_model = models.Dense(in_features=768, \r\n                       \
          \    out_features=512,\r\n                           bias=False, \r\n  \
          \                         activation_function=nn.Identity())\r\n\r\nmodel\
          \ = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/clip-ViT-B-32-multilingual-v1\"\
          )\r\n\r\ninputs = tokenizer(\"What am I using?\", \"Using DistilBERT with\
          \ ONNX Runtime!\", return_tensors=\"pt\")\r\n\r\ninput_ids = inputs['input_ids']\r\
          \ninput_names = list(inputs.keys())\r\n\r\nprint(input_names)\r\n# ['input_ids',\
          \ 'attention_mask']\r\n\r\n# forward \r\noutputs = model(input_ids)\r\n\
          ```\r\n\r\nwhen `outputs = model(input_ids)` run, I get the following error;\r\
          \n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\ndownload_optimize_model.ipynb Cell 30 line 3\r\n     28 print(input_names)\r\
          \n     30 # Export the model\r\n---> 31 outputs = model(input_ids)\r\n \
          \    32 last_hidden_state = outputs.last_hidden_state\r\n     34 # Export\
          \ the model\r\n\r\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n  \
          \ 1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\
          \nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\r\n   1523 # this\
          \ function, and just call forward.\r\n   1524 if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\
          \n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\
          \n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n \
          \  1530     result = None\r\n\r\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/container.py:215,\
          \ in Sequential.forward(self, input)\r\n...\r\n--> 401     return inner_dict[k]\r\
          \n    402 else:\r\n    403     return self.to_tuple()[k] \r\n\r\n\r\nKeyError:\
          \ 'token_embeddings'\r\n```\r\n\r\nThe question is, can I combine 3 layers\
          \ into 1 and export that single model consists of tranformer, pooling and\
          \ dense layers?\r\n"
        updatedAt: '2023-11-08T14:34:01.471Z'
      numEdits: 0
      reactions: []
    id: 654b9c5997f41c6d5f76c1aa
    type: comment
  author: canavar
  content: "running ```optimum-cli export onnx -m sentence-transformers/clip-ViT-B-32-multilingual-v1\
    \ \\\r\n--task feature-extraction models/clip_vit_multilingual_onnx```\r\n\r\n\
    with following output;\r\n\r\n```\r\nhuggingface/tokenizers: The current process\
    \ just got forked, after parallelism has already been used. Disabling parallelism\
    \ to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid\
    \ using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment\
    \ variable TOKENIZERS_PARALLELISM=(true | false)\r\nFramework not specified. Using\
    \ pt to export to ONNX.\r\nUsing the export variant default. Available variants\
    \ are:\r\n    - default: The default ONNX variant.\r\nUsing framework PyTorch:\
    \ 2.1.0\r\n[...transformers/models/distilbert/modeling_distilbert.py:223): TracerWarning:\
    \ torch.tensor results are registered as constants in the trace. You can safely\
    \ ignore this warning if you use this function to create tensors out of constant\
    \ variables that would be the same every time you call this function. In any other\
    \ case, this might cause the trace to be incorrect.\r\n  mask, torch.tensor(torch.finfo(scores.dtype).min)\r\
    \nPost-processing the exported models...\r\nDeduplicating shared (tied) weights...\r\
    \nValidating ONNX model models/clip_vit_multilingual_onnx/model.onnx...\r\n\t\
    -[\u2713] ONNX model output names match reference model (last_hidden_state)\r\n\
    \t- Validating ONNX Model output \"last_hidden_state\":\r\n\t\t-[\u2713] (2, 16,\
    \ 768) matches (2, 16, 768)\r\n\t\t-[\u2713] all values close (atol: 0.0001)\r\
    \nThe ONNX export succeeded and the exported model was saved at: models/clip_vit_multilingual_onnx\r\
    \n```\r\n\r\nimporting the code and running it;\r\n\r\n```python\r\nfrom transformers\
    \ import AutoTokenizer\r\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"models/clip_vit_multilingual_onnx\"\
    )\r\nmodel = ORTModelForFeatureExtraction.from_pretrained(\"models/clip_vit_multilingual_onnx\"\
    )\r\n\r\ninputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX\
    \ Runtime!\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\nlast_hidden_state\
    \ = outputs.last_hidden_state\r\nlist(last_hidden_state.shape)\r\n```\r\n\r\n\
    gives the output slightly(export output: (2, 16, 768), actual: [1, 21, 768]) \
    \ confirming the export command output:\r\n\r\n```\r\n[1, 21, 768]\r\n```\r\n\r\
    \nIf I use the same model from sentence-transformers:\r\n\r\n```python\r\nfrom\
    \ sentence_transformers import SentenceTransformer, util\r\nimport torch\r\n\r\
    \ntext_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\r\
    \ntexts = [\r\n    \"What am I using?\", \"Using DistilBERT with ONNX Runtime!\"\
    , # Spanish: a beach with palm trees\r\n]\r\ntext_embeddings = text_model.encode(texts)\r\
    \nprint(text_embeddings.shape)\r\n```\r\n\r\noutput: \r\n\r\n```\r\n(2, 512)\r\
    \n```\r\n\r\nas documented in: https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1;\r\
    \n\r\n```\r\nSentenceTransformer(\r\n  (0): Transformer({'max_seq_length': 128,\
    \ 'do_lower_case': False}) with Transformer model: DistilBertModel \r\n  (1):\
    \ Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens':\
    \ True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens':\
    \ False})\r\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': False,\
    \ 'activation_function': 'torch.nn.modules.linear.Identity'})\r\n)\r\n```\r\n\
    From model card, sentence-transformers/clip-ViT-B-32-multilingual-v1 pytorch model\
    \ has pooling and dense layers, which converts the output format from dim 768\
    \ to 512(on dense layer) from modules.json.\r\n\r\nIn case of exporting from optimum-cli,\
    \ apparently it ignores pooling and dense part, resulting an output shape of [1,\
    \ 21, 768].\r\n\r\nLooking at the optimum library, I can't seem to find any information\
    \ about pooling and dense layer inclusion. optimum-cli will not obviously check\
    \ 1_pooling or 2_dense folders at https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1/tree/main\
    \ next to pytorch_model.bin\r\n\r\nI am looking for a way to combine 1_pooling\
    \ and 2_dense layers into exported onnx. I tried to create a new model including\
    \ all the layers, Transformer layer coming from pretrained model, and pooling\
    \ and dense layer newly created;\r\n\r\n```\r\nword_embedding_model = AutoModel.from_pretrained(\"\
    sentence-transformers/clip-ViT-B-32-multilingual-v1\")\r\npooling_model = models.Pooling(768,\
    \ \r\n                               pooling_mode_mean_tokens=True, \r\n     \
    \                          pooling_mode_cls_token=False, \r\n                \
    \               pooling_mode_max_tokens=False,\r\n                           \
    \    pooling_mode_mean_sqrt_len_tokens=False)\r\ndense_model = models.Dense(in_features=768,\
    \ \r\n                           out_features=512,\r\n                       \
    \    bias=False, \r\n                           activation_function=nn.Identity())\r\
    \n\r\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model,\
    \ dense_model])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/clip-ViT-B-32-multilingual-v1\"\
    )\r\n\r\ninputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX\
    \ Runtime!\", return_tensors=\"pt\")\r\n\r\ninput_ids = inputs['input_ids']\r\n\
    input_names = list(inputs.keys())\r\n\r\nprint(input_names)\r\n# ['input_ids',\
    \ 'attention_mask']\r\n\r\n# forward \r\noutputs = model(input_ids)\r\n```\r\n\
    \r\nwhen `outputs = model(input_ids)` run, I get the following error;\r\n\r\n\
    ```\r\n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \ndownload_optimize_model.ipynb Cell 30 line 3\r\n     28 print(input_names)\r\
    \n     30 # Export the model\r\n---> 31 outputs = model(input_ids)\r\n     32\
    \ last_hidden_state = outputs.last_hidden_state\r\n     34 # Export the model\r\
    \n\r\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1518,\
    \ in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args,\
    \ **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args,\
    \ **kwargs)\r\n\r\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1527,\
    \ in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1523 # this function,\
    \ and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\
    \n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530\
    \     result = None\r\n\r\nFile /opt/conda/envs/embed/lib/python3.11/site-packages/torch/nn/modules/container.py:215,\
    \ in Sequential.forward(self, input)\r\n...\r\n--> 401     return inner_dict[k]\r\
    \n    402 else:\r\n    403     return self.to_tuple()[k] \r\n\r\n\r\nKeyError:\
    \ 'token_embeddings'\r\n```\r\n\r\nThe question is, can I combine 3 layers into\
    \ 1 and export that single model consists of tranformer, pooling and dense layers?\r\
    \n"
  created_at: 2023-11-08 14:34:01+00:00
  edited: false
  hidden: false
  id: 654b9c5997f41c6d5f76c1aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
      fullname: abdulkadir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: canavar
      type: user
    createdAt: '2023-11-22T21:58:21.000Z'
    data:
      edited: true
      editors:
      - canavar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5737240314483643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
          fullname: abdulkadir
          isHf: false
          isPro: false
          name: canavar
          type: user
        html: "<p>OK, finally I got it working. hf optimum onnx export can export\
          \ this model with (0) Transformer and (1) Pooling. But it can not extend\
          \ with provided dense layer.  What I have done  is, I created a model that\
          \ combines 3 layers as follows;</p>\n<h3 id=\"combinedmodel\">CombinedModel</h3>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-keyword\">from</span>\
          \ sentence_transformers <span class=\"hljs-keyword\">import</span> SentenceTransformer\n\
          <span class=\"hljs-keyword\">from</span> sentence_transformers <span class=\"\
          hljs-keyword\">import</span> models\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">import</span> torch.nn <span class=\"\
          hljs-keyword\">as</span> nn\n<span class=\"hljs-keyword\">import</span>\
          \ onnx\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"\
          hljs-keyword\">as</span> np\n\n<span class=\"hljs-keyword\">class</span>\
          \ <span class=\"hljs-title class_\">CombinedModel</span>(nn.Module):\n \
          \   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >__init__</span>(<span class=\"hljs-params\">self, transformer_model, dense_model</span>):\n\
          \        <span class=\"hljs-built_in\">super</span>(CombinedModel, self).__init__()\n\
          \        self.transformer = transformer_model\n        self.dense = dense_model\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >forward</span>(<span class=\"hljs-params\">self, input_ids, attention_mask</span>):\n\
          \        outputs = self.transformer({<span class=\"hljs-string\">'input_ids'</span>:\
          \ input_ids, <span class=\"hljs-string\">'attention_mask'</span>: attention_mask})\n\
          \        token_embeddings = outputs[<span class=\"hljs-string\">'token_embeddings'</span>]\n\
          \        dense_output = self.dense({<span class=\"hljs-string\">'sentence_embedding'</span>:\
          \ token_embeddings})\n        dense_output_tensor = dense_output[<span class=\"\
          hljs-string\">'sentence_embedding'</span>]\n        \n        <span class=\"\
          hljs-comment\">### this was important for me. it took me a bit to figure\
          \ out that original model takes the mean of dense output</span>\n      \
          \  mean_output = torch.mean(dense_output_tensor, dim=<span class=\"hljs-number\"\
          >1</span>)\n        flattened_output = mean_output.squeeze(<span class=\"\
          hljs-number\">0</span>)\n        <span class=\"hljs-keyword\">return</span>\
          \ flattened_output\n</code></pre>\n<h3 id=\"combine-dense-with-original-model\"\
          >Combine dense with original model</h3>\n<pre><code class=\"language-python\"\
          >transformer_model = SentenceTransformer(<span class=\"hljs-string\">'clip-ViT-B-32-multilingual-v1'</span>,\
          \ cache_folder=<span class=\"hljs-string\">'model_pytorch'</span>)\ntokenizer\
          \ = transformer_model.tokenizer\n\n<span class=\"hljs-comment\">### this\
          \ is from dense model configuration</span>\ndense_model = models.Dense(\n\
          \    in_features=<span class=\"hljs-number\">768</span>,\n    out_features=<span\
          \ class=\"hljs-number\">512</span>,\n    bias=<span class=\"hljs-literal\"\
          >False</span>,\n    activation_function= nn.Identity()\n)\n\n<span class=\"\
          hljs-comment\">### load the weights from dense model binary</span>\nstate_dict\
          \ = torch.load(<span class=\"hljs-string\">'model_pytorch/sentence-transformers_clip-ViT-B-32-multilingual-v1/2_Dense/pytorch_model.bin'</span>)\n\
          dense_model.load_state_dict(state_dict)\n\nmodel = CombinedModel(transformer_model,\
          \ dense_model)\n</code></pre>\n<h3 id=\"export-combined-model-to-onnx\"\
          >Export combined model to onnx</h3>\n<pre><code class=\"language-python\"\
          >model.<span class=\"hljs-built_in\">eval</span>()\n\ninput_text = <span\
          \ class=\"hljs-string\">\"This is a multi-lingual version of the OpenAI\
          \ CLIP-ViT-B32 model. You can map text (in 50+ languages) and images to\
          \ a common dense vector space such that images and the matching texts are\
          \ close.\"</span>\n\ninputs = tokenizer(input_text, padding=<span class=\"\
          hljs-string\">'longest'</span>, truncation=<span class=\"hljs-literal\"\
          >True</span>, max_length=<span class=\"hljs-number\">128</span>, return_tensors=<span\
          \ class=\"hljs-string\">'pt'</span>)\ninput_ids = inputs[<span class=\"\
          hljs-string\">'input_ids'</span>]\nattention_mask = inputs[<span class=\"\
          hljs-string\">'attention_mask'</span>]\n\n<span class=\"hljs-comment\">#\
          \ Export the model</span>\ntorch.onnx.export(model,               <span\
          \ class=\"hljs-comment\"># model being run</span>\n                  (input_ids,\
          \ attention_mask), <span class=\"hljs-comment\"># model input (or a tuple\
          \ for multiple inputs)</span>\n                  <span class=\"hljs-string\"\
          >\"combined_model.onnx\"</span>, <span class=\"hljs-comment\"># where to\
          \ save the model (can be a file or file-like object)</span>\n          \
          \        export_params=<span class=\"hljs-literal\">True</span>,       \
          \ <span class=\"hljs-comment\"># store the trained parameter weights inside\
          \ the model file</span>\n                  opset_version=<span class=\"\
          hljs-number\">17</span>,          <span class=\"hljs-comment\"># the ONNX\
          \ version to export the model to</span>\n                  do_constant_folding=<span\
          \ class=\"hljs-literal\">True</span>,  <span class=\"hljs-comment\"># whether\
          \ to execute constant folding for optimization</span>\n                \
          \  input_names = [<span class=\"hljs-string\">'input_ids'</span>, <span\
          \ class=\"hljs-string\">'attention_mask'</span>],   <span class=\"hljs-comment\"\
          ># the model's input names</span>\n                  output_names = [<span\
          \ class=\"hljs-string\">'output'</span>], <span class=\"hljs-comment\">#\
          \ the model's output names</span>\n                  dynamic_axes={<span\
          \ class=\"hljs-string\">'input_ids'</span>: {<span class=\"hljs-number\"\
          >0</span> : <span class=\"hljs-string\">'batch_size'</span>, <span class=\"\
          hljs-number\">1</span>: <span class=\"hljs-string\">'seq_length'</span>},\
          \    <span class=\"hljs-comment\"># variable length axes</span>\n      \
          \                          <span class=\"hljs-string\">'attention_mask'</span>:\
          \ {<span class=\"hljs-number\">0</span> : <span class=\"hljs-string\">'batch_size'</span>,\
          \ <span class=\"hljs-number\">1</span>: <span class=\"hljs-string\">'seq_length'</span>},\n\
          \                                <span class=\"hljs-string\">'output'</span>\
          \ : {<span class=\"hljs-number\">0</span> : <span class=\"hljs-string\"\
          >'batch_size'</span>}})\n\nonnx.checker.check_model(<span class=\"hljs-string\"\
          >\"combined_model.onnx\"</span>)\ncomdined_model = onnx.load(<span class=\"\
          hljs-string\">\"combined_model.onnx\"</span>)\n</code></pre>\n<h3 id=\"\
          compare-both-original-and-onnx-model-output\">Compare both original and\
          \ onnx model output;</h3>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span>\
          \ numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\"\
          >import</span> onnxruntime <span class=\"hljs-keyword\">as</span> ort\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\n\nmodel = SentenceTransformer(<span class=\"\
          hljs-string\">'sentence-transformers/clip-ViT-B-32-multilingual-v1'</span>)\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">'sentence-transformers/clip-ViT-B-32-multilingual-v1'</span>)\n\
          \n<span class=\"hljs-comment\"># Prepare the input</span>\ntext = <span\
          \ class=\"hljs-string\">\"This is an example sentence.\"</span>\ninputs\
          \ = tokenizer(text, padding=<span class=\"hljs-string\">'longest'</span>,\
          \ truncation=<span class=\"hljs-literal\">True</span>, max_length=<span\
          \ class=\"hljs-number\">128</span>, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>)\n\n<span class=\"hljs-comment\"># Run the PyTorch model</span>\n\
          pytorch_output =  model.encode(text, convert_to_tensor=<span class=\"hljs-literal\"\
          >True</span>, device=<span class=\"hljs-string\">'cpu'</span>)\n\n<span\
          \ class=\"hljs-comment\"># Convert the inputs to numpy arrays for the ONNX\
          \ model</span>\ninputs_onnx = {name: tensor.numpy() <span class=\"hljs-keyword\"\
          >for</span> name, tensor <span class=\"hljs-keyword\">in</span> inputs.items()}\n\
          \n<span class=\"hljs-comment\"># Run the ONNX model</span>\nsess = ort.InferenceSession(<span\
          \ class=\"hljs-string\">\"combined_model.onnx\"</span>)\nonnx_output = sess.run(<span\
          \ class=\"hljs-literal\">None</span>, inputs_onnx)\n\n<span class=\"hljs-comment\"\
          ># Compare the outputs</span>\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"Are the outputs close?\"</span>, np.allclose(pytorch_output.detach().numpy(),\
          \ onnx_output[<span class=\"hljs-number\">0</span>], atol=<span class=\"\
          hljs-number\">1e-6</span>))\n\n<span class=\"hljs-comment\"># Calculate\
          \ the differences between the outputs</span>\ndifferences = pytorch_output.detach().numpy()\
          \ - onnx_output[<span class=\"hljs-number\">0</span>]\n\n<span class=\"\
          hljs-comment\"># Print the standard deviation of the differences</span>\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Standard deviation of the differences:\"</span>, np.std(differences))\n\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          pytorch_output size:\"</span>, pytorch_output.size())\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"onnx_output size:\"</span>,\
          \ onnx_output[<span class=\"hljs-number\">0</span>].shape)\n</code></pre>\n\
          <h3 id=\"output\">Output:</h3>\n<pre><code>Are the outputs close? True\n\
          Standard deviation of the differences: 1.6167593e-07\npytorch_output size:\
          \ torch.Size([512])\nonnx_output size: (512,)\n</code></pre>\n<p>I would\
          \ really like to contribute the onnx model, novices like me can use the\
          \ onnx version easily.</p>\n"
        raw: "OK, finally I got it working. hf optimum onnx export can export this\
          \ model with (0) Transformer and (1) Pooling. But it can not extend with\
          \ provided dense layer.  What I have done  is, I created a model that combines\
          \ 3 layers as follows;\n\n### CombinedModel\n\n```py\nfrom sentence_transformers\
          \ import SentenceTransformer\nfrom sentence_transformers import models\n\
          import torch\nimport torch.nn as nn\nimport onnx\nimport numpy as np\n\n\
          class CombinedModel(nn.Module):\n    def __init__(self, transformer_model,\
          \ dense_model):\n        super(CombinedModel, self).__init__()\n       \
          \ self.transformer = transformer_model\n        self.dense = dense_model\n\
          \n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer({'input_ids':\
          \ input_ids, 'attention_mask': attention_mask})\n        token_embeddings\
          \ = outputs['token_embeddings']\n        dense_output = self.dense({'sentence_embedding':\
          \ token_embeddings})\n        dense_output_tensor = dense_output['sentence_embedding']\n\
          \        \n        ### this was important for me. it took me a bit to figure\
          \ out that original model takes the mean of dense output\n        mean_output\
          \ = torch.mean(dense_output_tensor, dim=1)\n        flattened_output = mean_output.squeeze(0)\n\
          \        return flattened_output\n```\n\n\n### Combine dense with original\
          \ model\n```python\ntransformer_model = SentenceTransformer('clip-ViT-B-32-multilingual-v1',\
          \ cache_folder='model_pytorch')\ntokenizer = transformer_model.tokenizer\n\
          \n### this is from dense model configuration\ndense_model = models.Dense(\n\
          \    in_features=768,\n    out_features=512,\n    bias=False,\n    activation_function=\
          \ nn.Identity()\n)\n\n### load the weights from dense model binary\nstate_dict\
          \ = torch.load('model_pytorch/sentence-transformers_clip-ViT-B-32-multilingual-v1/2_Dense/pytorch_model.bin')\n\
          dense_model.load_state_dict(state_dict)\n\nmodel = CombinedModel(transformer_model,\
          \ dense_model)\n```\n### Export combined model to onnx\n```python\nmodel.eval()\n\
          \ninput_text = \"This is a multi-lingual version of the OpenAI CLIP-ViT-B32\
          \ model. You can map text (in 50+ languages) and images to a common dense\
          \ vector space such that images and the matching texts are close.\"\n\n\
          inputs = tokenizer(input_text, padding='longest', truncation=True, max_length=128,\
          \ return_tensors='pt')\ninput_ids = inputs['input_ids']\nattention_mask\
          \ = inputs['attention_mask']\n\n# Export the model\ntorch.onnx.export(model,\
          \               # model being run\n                  (input_ids, attention_mask),\
          \ # model input (or a tuple for multiple inputs)\n                  \"combined_model.onnx\"\
          , # where to save the model (can be a file or file-like object)\n      \
          \            export_params=True,        # store the trained parameter weights\
          \ inside the model file\n                  opset_version=17,          #\
          \ the ONNX version to export the model to\n                  do_constant_folding=True,\
          \  # whether to execute constant folding for optimization\n            \
          \      input_names = ['input_ids', 'attention_mask'],   # the model's input\
          \ names\n                  output_names = ['output'], # the model's output\
          \ names\n                  dynamic_axes={'input_ids': {0 : 'batch_size',\
          \ 1: 'seq_length'},    # variable length axes\n                        \
          \        'attention_mask': {0 : 'batch_size', 1: 'seq_length'},\n      \
          \                          'output' : {0 : 'batch_size'}})\n\nonnx.checker.check_model(\"\
          combined_model.onnx\")\ncomdined_model = onnx.load(\"combined_model.onnx\"\
          )\n```\n### Compare both original and onnx model output;\n```python\nimport\
          \ torch\nimport numpy as np\nimport onnxruntime as ort\nfrom transformers\
          \ import AutoTokenizer\n\nmodel = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n\
          tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n\
          \n# Prepare the input\ntext = \"This is an example sentence.\"\ninputs =\
          \ tokenizer(text, padding='longest', truncation=True, max_length=128, return_tensors='pt')\n\
          \n# Run the PyTorch model\npytorch_output =  model.encode(text, convert_to_tensor=True,\
          \ device='cpu')\n\n# Convert the inputs to numpy arrays for the ONNX model\n\
          inputs_onnx = {name: tensor.numpy() for name, tensor in inputs.items()}\n\
          \n# Run the ONNX model\nsess = ort.InferenceSession(\"combined_model.onnx\"\
          )\nonnx_output = sess.run(None, inputs_onnx)\n\n# Compare the outputs\n\
          print(\"Are the outputs close?\", np.allclose(pytorch_output.detach().numpy(),\
          \ onnx_output[0], atol=1e-6))\n\n# Calculate the differences between the\
          \ outputs\ndifferences = pytorch_output.detach().numpy() - onnx_output[0]\n\
          \n# Print the standard deviation of the differences\nprint(\"Standard deviation\
          \ of the differences:\", np.std(differences))\n\nprint(\"pytorch_output\
          \ size:\", pytorch_output.size())\nprint(\"onnx_output size:\", onnx_output[0].shape)\n\
          ```\n\n### Output:\n\n```\nAre the outputs close? True\nStandard deviation\
          \ of the differences: 1.6167593e-07\npytorch_output size: torch.Size([512])\n\
          onnx_output size: (512,)\n```\n\n\nI would really like to contribute the\
          \ onnx model, novices like me can use the onnx version easily."
        updatedAt: '2023-11-22T22:31:38.183Z'
      numEdits: 1
      reactions: []
    id: 655e797d0ec0e16185269119
    type: comment
  author: canavar
  content: "OK, finally I got it working. hf optimum onnx export can export this model\
    \ with (0) Transformer and (1) Pooling. But it can not extend with provided dense\
    \ layer.  What I have done  is, I created a model that combines 3 layers as follows;\n\
    \n### CombinedModel\n\n```py\nfrom sentence_transformers import SentenceTransformer\n\
    from sentence_transformers import models\nimport torch\nimport torch.nn as nn\n\
    import onnx\nimport numpy as np\n\nclass CombinedModel(nn.Module):\n    def __init__(self,\
    \ transformer_model, dense_model):\n        super(CombinedModel, self).__init__()\n\
    \        self.transformer = transformer_model\n        self.dense = dense_model\n\
    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer({'input_ids':\
    \ input_ids, 'attention_mask': attention_mask})\n        token_embeddings = outputs['token_embeddings']\n\
    \        dense_output = self.dense({'sentence_embedding': token_embeddings})\n\
    \        dense_output_tensor = dense_output['sentence_embedding']\n        \n\
    \        ### this was important for me. it took me a bit to figure out that original\
    \ model takes the mean of dense output\n        mean_output = torch.mean(dense_output_tensor,\
    \ dim=1)\n        flattened_output = mean_output.squeeze(0)\n        return flattened_output\n\
    ```\n\n\n### Combine dense with original model\n```python\ntransformer_model =\
    \ SentenceTransformer('clip-ViT-B-32-multilingual-v1', cache_folder='model_pytorch')\n\
    tokenizer = transformer_model.tokenizer\n\n### this is from dense model configuration\n\
    dense_model = models.Dense(\n    in_features=768,\n    out_features=512,\n   \
    \ bias=False,\n    activation_function= nn.Identity()\n)\n\n### load the weights\
    \ from dense model binary\nstate_dict = torch.load('model_pytorch/sentence-transformers_clip-ViT-B-32-multilingual-v1/2_Dense/pytorch_model.bin')\n\
    dense_model.load_state_dict(state_dict)\n\nmodel = CombinedModel(transformer_model,\
    \ dense_model)\n```\n### Export combined model to onnx\n```python\nmodel.eval()\n\
    \ninput_text = \"This is a multi-lingual version of the OpenAI CLIP-ViT-B32 model.\
    \ You can map text (in 50+ languages) and images to a common dense vector space\
    \ such that images and the matching texts are close.\"\n\ninputs = tokenizer(input_text,\
    \ padding='longest', truncation=True, max_length=128, return_tensors='pt')\ninput_ids\
    \ = inputs['input_ids']\nattention_mask = inputs['attention_mask']\n\n# Export\
    \ the model\ntorch.onnx.export(model,               # model being run\n      \
    \            (input_ids, attention_mask), # model input (or a tuple for multiple\
    \ inputs)\n                  \"combined_model.onnx\", # where to save the model\
    \ (can be a file or file-like object)\n                  export_params=True, \
    \       # store the trained parameter weights inside the model file\n        \
    \          opset_version=17,          # the ONNX version to export the model to\n\
    \                  do_constant_folding=True,  # whether to execute constant folding\
    \ for optimization\n                  input_names = ['input_ids', 'attention_mask'],\
    \   # the model's input names\n                  output_names = ['output'], #\
    \ the model's output names\n                  dynamic_axes={'input_ids': {0 :\
    \ 'batch_size', 1: 'seq_length'},    # variable length axes\n                \
    \                'attention_mask': {0 : 'batch_size', 1: 'seq_length'},\n    \
    \                            'output' : {0 : 'batch_size'}})\n\nonnx.checker.check_model(\"\
    combined_model.onnx\")\ncomdined_model = onnx.load(\"combined_model.onnx\")\n\
    ```\n### Compare both original and onnx model output;\n```python\nimport torch\n\
    import numpy as np\nimport onnxruntime as ort\nfrom transformers import AutoTokenizer\n\
    \nmodel = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n\
    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n\
    \n# Prepare the input\ntext = \"This is an example sentence.\"\ninputs = tokenizer(text,\
    \ padding='longest', truncation=True, max_length=128, return_tensors='pt')\n\n\
    # Run the PyTorch model\npytorch_output =  model.encode(text, convert_to_tensor=True,\
    \ device='cpu')\n\n# Convert the inputs to numpy arrays for the ONNX model\ninputs_onnx\
    \ = {name: tensor.numpy() for name, tensor in inputs.items()}\n\n# Run the ONNX\
    \ model\nsess = ort.InferenceSession(\"combined_model.onnx\")\nonnx_output = sess.run(None,\
    \ inputs_onnx)\n\n# Compare the outputs\nprint(\"Are the outputs close?\", np.allclose(pytorch_output.detach().numpy(),\
    \ onnx_output[0], atol=1e-6))\n\n# Calculate the differences between the outputs\n\
    differences = pytorch_output.detach().numpy() - onnx_output[0]\n\n# Print the\
    \ standard deviation of the differences\nprint(\"Standard deviation of the differences:\"\
    , np.std(differences))\n\nprint(\"pytorch_output size:\", pytorch_output.size())\n\
    print(\"onnx_output size:\", onnx_output[0].shape)\n```\n\n### Output:\n\n```\n\
    Are the outputs close? True\nStandard deviation of the differences: 1.6167593e-07\n\
    pytorch_output size: torch.Size([512])\nonnx_output size: (512,)\n```\n\n\nI would\
    \ really like to contribute the onnx model, novices like me can use the onnx version\
    \ easily."
  created_at: 2023-11-22 21:58:21+00:00
  edited: true
  hidden: false
  id: 655e797d0ec0e16185269119
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
      fullname: abdulkadir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: canavar
      type: user
    createdAt: '2023-12-05T13:23:01.000Z'
    data:
      edited: false
      editors:
      - canavar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8163502812385559
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b349b8902a7e761197a0690299f7875b.svg
          fullname: abdulkadir
          isHf: false
          isPro: false
          name: canavar
          type: user
        html: '<p>Anyone in the void?</p>

          '
        raw: Anyone in the void?
        updatedAt: '2023-12-05T13:23:01.284Z'
      numEdits: 0
      reactions: []
    id: 656f2435709a7c73d28ca743
    type: comment
  author: canavar
  content: Anyone in the void?
  created_at: 2023-12-05 13:23:01+00:00
  edited: false
  hidden: false
  id: 656f2435709a7c73d28ca743
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: sentence-transformers/clip-ViT-B-32-multilingual-v1
repo_type: model
status: open
target_branch: null
title: Exporting model with optimum, but optimum does not take pooling and dense layers
  into account.
