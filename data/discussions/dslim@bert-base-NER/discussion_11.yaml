!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amitsinghkec
conflicting_files: null
created_at: 2023-06-03 17:49:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd77c467bf27bd15b81742c2440b6dcb.svg
      fullname: Amit Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitsinghkec
      type: user
    createdAt: '2023-06-03T18:49:36.000Z'
    data:
      edited: false
      editors:
      - amitsinghkec
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7471595406532288
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd77c467bf27bd15b81742c2440b6dcb.svg
          fullname: Amit Singh
          isHf: false
          isPro: false
          name: amitsinghkec
          type: user
        html: "<p>I have fine-tuned a BERT NER model to my dataset. The base model\
          \ that I am fine-tuning is \u201Cdslim/bert-base-NER\u201D. I have been\
          \ successfully able to train the model using the following script as refrence:\
          \ <a rel=\"nofollow\" href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax\"\
          >https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax</a></p>\n\
          <p>The code which does the prediction:</p>\n<p>from transformers import\
          \ pipeline, BertTokenizer</p>\n<p>tokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER',\
          \ return_offsets_mapping=True, is_split_into_words=True)<br>model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')</p>\n\
          <p>pipe = pipeline(task=\"ner\", model=model.to(\"cpu\"), tokenizer=tokenizer,\
          \ grouped_entities=True)<br>pipe(\"this is a Abc Corp. Ltd\")<br>The prediction\
          \ form the base model contained the start and end position of the word in\
          \ the original text like:</p>\n<p>{\u2018entity_group\u2019: \u2018ORG\u2019\
          , \u2018score\u2019: 0.9992545247077942, \u2018word\u2019: \u2018A\u2019\
          , \u2018start\u2019: 10, \u2018end\u2019: 11}<br>{\u2018entity_group\u2019\
          : \u2018ORG\u2019, \u2018score\u2019: 0.998507097363472, \u2018word\u2019\
          : \u2018##bc Corp Ltd\u2019, \u2018start\u2019: 11, \u2018end\u2019: 22}<br>While\
          \ the prediction from the re-trained model is:</p>\n<p>{\u2018entity_group\u2019\
          : \u2018ORG\u2019, \u2018score\u2019: 0.747031033039093, \u2018word\u2019\
          : \u2018##7\u2019, \u2018start\u2019: None, \u2018end\u2019: None},<br>{\u2018\
          entity_group\u2019: \u2018ORG\u2019, \u2018score\u2019: 0.9055356582005819,\
          \ \u2018word\u2019: \u2018Games , Inc\u2019, \u2018start\u2019: None, \u2018\
          end\u2019: None}<br>I am passing the position ids to the model during the\
          \ training process. I looked at the model training parameters but, could\
          \ not find a way to pass start and end position of the words to model training\
          \ process. I have the start and end position of the tokenized words.</p>\n"
        raw: "I have fine-tuned a BERT NER model to my dataset. The base model that\
          \ I am fine-tuning is \u201Cdslim/bert-base-NER\u201D. I have been successfully\
          \ able to train the model using the following script as refrence: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax\r\
          \n\r\nThe code which does the prediction:\r\n\r\nfrom transformers import\
          \ pipeline, BertTokenizer\r\n\r\ntokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER',\
          \ return_offsets_mapping=True, is_split_into_words=True)\r\nmodel = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')\r\
          \n\r\npipe = pipeline(task=\"ner\", model=model.to(\"cpu\"), tokenizer=tokenizer,\
          \ grouped_entities=True)\r\npipe(\"this is a Abc Corp. Ltd\")\r\nThe prediction\
          \ form the base model contained the start and end position of the word in\
          \ the original text like:\r\n\r\n{\u2018entity_group\u2019: \u2018ORG\u2019\
          , \u2018score\u2019: 0.9992545247077942, \u2018word\u2019: \u2018A\u2019\
          , \u2018start\u2019: 10, \u2018end\u2019: 11}\r\n{\u2018entity_group\u2019\
          : \u2018ORG\u2019, \u2018score\u2019: 0.998507097363472, \u2018word\u2019\
          : \u2018##bc Corp Ltd\u2019, \u2018start\u2019: 11, \u2018end\u2019: 22}\r\
          \nWhile the prediction from the re-trained model is:\r\n\r\n{\u2018entity_group\u2019\
          : \u2018ORG\u2019, \u2018score\u2019: 0.747031033039093, \u2018word\u2019\
          : \u2018##7\u2019, \u2018start\u2019: None, \u2018end\u2019: None},\r\n\
          {\u2018entity_group\u2019: \u2018ORG\u2019, \u2018score\u2019: 0.9055356582005819,\
          \ \u2018word\u2019: \u2018Games , Inc\u2019, \u2018start\u2019: None, \u2018\
          end\u2019: None}\r\nI am passing the position ids to the model during the\
          \ training process. I looked at the model training parameters but, could\
          \ not find a way to pass start and end position of the words to model training\
          \ process. I have the start and end position of the tokenized words.\r\n\
          \r\n"
        updatedAt: '2023-06-03T18:49:36.606Z'
      numEdits: 0
      reactions: []
    id: 647b8b406a79fbf5e997c6c8
    type: comment
  author: amitsinghkec
  content: "I have fine-tuned a BERT NER model to my dataset. The base model that\
    \ I am fine-tuning is \u201Cdslim/bert-base-NER\u201D. I have been successfully\
    \ able to train the model using the following script as refrence: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=zPDla1mmZiax\r\
    \n\r\nThe code which does the prediction:\r\n\r\nfrom transformers import pipeline,\
    \ BertTokenizer\r\n\r\ntokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER',\
    \ return_offsets_mapping=True, is_split_into_words=True)\r\nmodel = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')\r\
    \n\r\npipe = pipeline(task=\"ner\", model=model.to(\"cpu\"), tokenizer=tokenizer,\
    \ grouped_entities=True)\r\npipe(\"this is a Abc Corp. Ltd\")\r\nThe prediction\
    \ form the base model contained the start and end position of the word in the\
    \ original text like:\r\n\r\n{\u2018entity_group\u2019: \u2018ORG\u2019, \u2018\
    score\u2019: 0.9992545247077942, \u2018word\u2019: \u2018A\u2019, \u2018start\u2019\
    : 10, \u2018end\u2019: 11}\r\n{\u2018entity_group\u2019: \u2018ORG\u2019, \u2018\
    score\u2019: 0.998507097363472, \u2018word\u2019: \u2018##bc Corp Ltd\u2019, \u2018\
    start\u2019: 11, \u2018end\u2019: 22}\r\nWhile the prediction from the re-trained\
    \ model is:\r\n\r\n{\u2018entity_group\u2019: \u2018ORG\u2019, \u2018score\u2019\
    : 0.747031033039093, \u2018word\u2019: \u2018##7\u2019, \u2018start\u2019: None,\
    \ \u2018end\u2019: None},\r\n{\u2018entity_group\u2019: \u2018ORG\u2019, \u2018\
    score\u2019: 0.9055356582005819, \u2018word\u2019: \u2018Games , Inc\u2019, \u2018\
    start\u2019: None, \u2018end\u2019: None}\r\nI am passing the position ids to\
    \ the model during the training process. I looked at the model training parameters\
    \ but, could not find a way to pass start and end position of the words to model\
    \ training process. I have the start and end position of the tokenized words.\r\
    \n\r\n"
  created_at: 2023-06-03 17:49:36+00:00
  edited: false
  hidden: false
  id: 647b8b406a79fbf5e997c6c8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: dslim/bert-base-NER
repo_type: model
status: open
target_branch: null
title: Fine-tuning the model losses the start and end position of the word in the
  predicted output
