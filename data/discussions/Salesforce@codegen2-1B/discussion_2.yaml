!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muelletm
conflicting_files: null
created_at: 2023-05-23 07:23:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-23T08:23:47.000Z'
    data:
      edited: false
      editors:
      - muelletm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
          fullname: "Thomas M\xFCller"
          isHf: false
          isPro: false
          name: muelletm
          type: user
        html: "<p>Running the example from the README i get this:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForCausalLM\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          Salesforce/codegen2-1B\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"Salesforce/codegen2-1B\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, revision=<span class=\"hljs-string\"\
          >\"main\"</span>)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">format</span>(<span class=\"hljs-params\">prefix,\
          \ suffix</span>):\n  <span class=\"hljs-keyword\">return</span> prefix +\
          \ <span class=\"hljs-string\">\"&lt;mask_1&gt;\"</span> + suffix + <span\
          \ class=\"hljs-string\">\"&lt;|endoftext|&gt;\"</span> + <span class=\"\
          hljs-string\">\"&lt;sep&gt;\"</span> + <span class=\"hljs-string\">\"&lt;mask_1&gt;\"\
          </span>\n\n\nprefix = <span class=\"hljs-string\">\"def hello_world():\\\
          n    \"</span>\nsuffix = <span class=\"hljs-string\">\"    return name\"\
          </span>\ntext = <span class=\"hljs-built_in\">format</span>(prefix, suffix)\n\
          input_ids = tokenizer(text, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>).input_ids\ngenerated_ids = model.generate(input_ids, max_length=<span\
          \ class=\"hljs-number\">128</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(generated_ids[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >False</span>)[<span class=\"hljs-built_in\">len</span>(text):])\n</code></pre>\n\
          <pre><code>def hello_world&lt;eom&gt;&lt;|endoftext|&gt;&lt;|python|&gt;#\n\
          </code></pre>\n<p>I also tested with the 7B model the output was something\
          \ like <code>return \"Hello World\"</code> which is also not ideal given\
          \ that the following line is <code>return name</code>.</p>\n<p>I am wondering\
          \ if these models are not good at infilling or if there is maybe a problem\
          \ with the prompt construction.</p>\n<p>Here is a colab to reproduce:<br><a\
          \ rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1UZquOlGviRlV5xByenbs-A1GGAi_YTbs\"\
          >https://colab.research.google.com/drive/1UZquOlGviRlV5xByenbs-A1GGAi_YTbs</a></p>\n\
          <p>Cheers!</p>\n"
        raw: "Running the example from the README i get this:\r\n\r\n```python\r\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B\")\r\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen2-1B\", trust_remote_code=True,\
          \ revision=\"main\")\r\n\r\n\r\ndef format(prefix, suffix):\r\n  return\
          \ prefix + \"<mask_1>\" + suffix + \"<|endoftext|>\" + \"<sep>\" + \"<mask_1>\"\
          \r\n\r\n\r\nprefix = \"def hello_world():\\n    \"\r\nsuffix = \"    return\
          \ name\"\r\ntext = format(prefix, suffix)\r\ninput_ids = tokenizer(text,\
          \ return_tensors=\"pt\").input_ids\r\ngenerated_ids = model.generate(input_ids,\
          \ max_length=128)\r\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=False)[len(text):])\r\
          \n```\r\n\r\n```\r\ndef hello_world<eom><|endoftext|><|python|>#\r\n```\r\
          \n\r\nI also tested with the 7B model the output was something like `return\
          \ \"Hello World\"` which is also not ideal given that the following line\
          \ is `return name`.\r\n\r\nI am wondering if these models are not good at\
          \ infilling or if there is maybe a problem with the prompt construction.\r\
          \n\r\nHere is a colab to reproduce:\r\nhttps://colab.research.google.com/drive/1UZquOlGviRlV5xByenbs-A1GGAi_YTbs\r\
          \n\r\nCheers!"
        updatedAt: '2023-05-23T08:23:47.762Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - LynRo
        - chonghao-cc
        - ludwik-trammer
        - devymex
        - mommi84
        - aincvy
        - Libro
    id: 646c7813342160d8bb468da4
    type: comment
  author: muelletm
  content: "Running the example from the README i get this:\r\n\r\n```python\r\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    Salesforce/codegen2-1B\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen2-1B\"\
    , trust_remote_code=True, revision=\"main\")\r\n\r\n\r\ndef format(prefix, suffix):\r\
    \n  return prefix + \"<mask_1>\" + suffix + \"<|endoftext|>\" + \"<sep>\" + \"\
    <mask_1>\"\r\n\r\n\r\nprefix = \"def hello_world():\\n    \"\r\nsuffix = \"  \
    \  return name\"\r\ntext = format(prefix, suffix)\r\ninput_ids = tokenizer(text,\
    \ return_tensors=\"pt\").input_ids\r\ngenerated_ids = model.generate(input_ids,\
    \ max_length=128)\r\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=False)[len(text):])\r\
    \n```\r\n\r\n```\r\ndef hello_world<eom><|endoftext|><|python|>#\r\n```\r\n\r\n\
    I also tested with the 7B model the output was something like `return \"Hello\
    \ World\"` which is also not ideal given that the following line is `return name`.\r\
    \n\r\nI am wondering if these models are not good at infilling or if there is\
    \ maybe a problem with the prompt construction.\r\n\r\nHere is a colab to reproduce:\r\
    \nhttps://colab.research.google.com/drive/1UZquOlGviRlV5xByenbs-A1GGAi_YTbs\r\n\
    \r\nCheers!"
  created_at: 2023-05-23 07:23:47+00:00
  edited: false
  hidden: false
  id: 646c7813342160d8bb468da4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/codegen2-1B
repo_type: model
status: open
target_branch: null
title: Infill example broken?
