!!python/object:huggingface_hub.community.DiscussionWithDetails
author: edmond
conflicting_files: null
created_at: 2022-06-19 18:35:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-06-19T19:35:14.000Z'
    data:
      edited: true
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: '<p>I have an out of memory error during a training iteration despite
          : </p>

          <ul>

          <li>my V100 GPU has 32Gb of ram</li>

          <li>I used "with torch.cuda.amp.autocast():" </li>

          <li>I used torch.cuda.empty_cache(). I have no memory leak since I printed
          memory usage after each batch, and it was constant</li>

          <li>my sequence (batch of size 1) is not supposed to be the longest (ie.
          longer sequences didnt cause the error during training)</li>

          </ul>

          <p>It says this : "ERROR root CUDA out of memory. Tried to allocate 64.00
          MiB (GPU 0; 31.75 GiB total capacity; 30.22 GiB already allocated; 61.50
          MiB free; 30.33 GiB reserved in total by PyTorch) If reserved memory is
          &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF."</p>

          <p>Isnt 1b3 supposed to fit easily in a V100 ?</p>

          '
        raw: "I have an out of memory error during a training iteration despite :\
          \ \n- my V100 GPU has 32Gb of ram\n- I used \"with torch.cuda.amp.autocast():\"\
          \ \n- I used torch.cuda.empty_cache(). I have no memory leak since I printed\
          \ memory usage after each batch, and it was constant\n- my sequence (batch\
          \ of size 1) is not supposed to be the longest (ie. longer sequences didnt\
          \ cause the error during training)\n\nIt says this : \"ERROR root CUDA out\
          \ of memory. Tried to allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity;\
          \ 30.22 GiB already allocated; 61.50 MiB free; 30.33 GiB reserved in total\
          \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF.\"\
          \n\nIsnt 1b3 supposed to fit easily in a V100 ?"
        updatedAt: '2022-06-19T19:45:40.277Z'
      numEdits: 1
      reactions: []
    id: 62af7a72457691d789ce4bb5
    type: comment
  author: edmond
  content: "I have an out of memory error during a training iteration despite : \n\
    - my V100 GPU has 32Gb of ram\n- I used \"with torch.cuda.amp.autocast():\" \n\
    - I used torch.cuda.empty_cache(). I have no memory leak since I printed memory\
    \ usage after each batch, and it was constant\n- my sequence (batch of size 1)\
    \ is not supposed to be the longest (ie. longer sequences didnt cause the error\
    \ during training)\n\nIt says this : \"ERROR root CUDA out of memory. Tried to\
    \ allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity; 30.22 GiB already allocated;\
    \ 61.50 MiB free; 30.33 GiB reserved in total by PyTorch) If reserved memory is\
    \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See\
    \ documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF.\"\n\nIsnt 1b3\
    \ supposed to fit easily in a V100 ?"
  created_at: 2022-06-19 18:35:14+00:00
  edited: true
  hidden: false
  id: 62af7a72457691d789ce4bb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-06-19T22:20:14.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;edmond&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/edmond\">@<span class=\"\
          underline\">edmond</span></a></span>\n\n\t</span></span> !<br>Thank you\
          \ very much for your question. This is weird indeed, what I suspect as a\
          \ first sight is that you are loading everything in fp32 (by default). The\
          \ weights in the Hub are stored in fp16, they are ~3.4Gb in fp16 and would\
          \ be 2x higher in fp32 I think. This can rapidly blow up the GPU ram together\
          \ with the optimizer state.<br>Can you try to load the weights + optimizer\
          \ state directly in fp16 like the following: <code>AutoModelForCausalLM.from_pretrained(\"\
          bigscience/bloom-1b3\", torch_dtype=\"auto\")</code> ? Let me know if this\
          \ works!</p>\n"
        raw: 'Hi @edmond !

          Thank you very much for your question. This is weird indeed, what I suspect
          as a first sight is that you are loading everything in fp32 (by default).
          The weights in the Hub are stored in fp16, they are ~3.4Gb in fp16 and would
          be 2x higher in fp32 I think. This can rapidly blow up the GPU ram together
          with the optimizer state.

          Can you try to load the weights + optimizer state directly in fp16 like
          the following: `AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b3",
          torch_dtype="auto")` ? Let me know if this works!'
        updatedAt: '2022-06-19T22:20:14.763Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - edmond
    id: 62afa11e8098f0e9deeda8d7
    type: comment
  author: ybelkada
  content: 'Hi @edmond !

    Thank you very much for your question. This is weird indeed, what I suspect as
    a first sight is that you are loading everything in fp32 (by default). The weights
    in the Hub are stored in fp16, they are ~3.4Gb in fp16 and would be 2x higher
    in fp32 I think. This can rapidly blow up the GPU ram together with the optimizer
    state.

    Can you try to load the weights + optimizer state directly in fp16 like the following:
    `AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b3", torch_dtype="auto")`
    ? Let me know if this works!'
  created_at: 2022-06-19 21:20:14+00:00
  edited: false
  hidden: false
  id: 62afa11e8098f0e9deeda8d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-06-19T22:40:45.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Would you be able to share a minimal example? I think this should
          work with a V100 32Gb (including the optimizer states)</p>

          '
        raw: Would you be able to share a minimal example? I think this should work
          with a V100 32Gb (including the optimizer states)
        updatedAt: '2022-06-19T22:40:45.805Z'
      numEdits: 0
      reactions: []
    id: 62afa5edde48438b95be6b96
    type: comment
  author: TimeRobber
  content: Would you be able to share a minimal example? I think this should work
    with a V100 32Gb (including the optimizer states)
  created_at: 2022-06-19 21:40:45+00:00
  edited: false
  hidden: false
  id: 62afa5edde48438b95be6b96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-06-20T09:37:23.000Z'
    data:
      edited: true
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: "<p>Hi, thanks for the quick answer, AutoModelWithLMHead.from_pretrained(\"\
          bigscience/bloom-1b3\", torch_dtype=\"auto\") leads to ERROR root Attempting\
          \ to unscale FP16 gradients.</p>\n<p>Here is a minimal example :<br>\"\"\
          \"<br>import sys, torch, time, logging<br>import pandas as pd<br>import\
          \ numpy as np<br>from transformers import AutoModelWithLMHead, AutoTokenizer<br>from\
          \ sklearn.model_selection import KFold</p>\n<p>df = pd.DataFrame()<br>df['tokens']\
          \ = [[i+100 for i in range(200)] for _ in range(10000)]</p>\n<p>with torch.cuda.amp.autocast():<br>\
          \    tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b3\"\
          )<br>    model = AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-1b3\"\
          )</p>\n<pre><code>df['tokens'] = df['tokens'].map(lambda x: x + [tokenizer.eos_token_id])\n\
          \nscaler = torch.cuda.amp.GradScaler()\n\ntorch.manual_seed(12)\nmodel.opt\
          \ = torch.optim.NAdam(model.parameters(), lr=0.000007)\ndevice = 'cuda'\n\
          model.to(device)\n\nkf = KFold(n_splits=len(df), shuffle=True)\nfor batch_nb,\
          \ (_, batch_indexes) in enumerate(kf.split(df)):\n    batch = df.iloc[batch_indexes]\n\
          \    batch = np.array(batch['tokens'].values[0])\n    batch = torch.from_numpy(batch)[None,\
          \ :].to(model.device)\n    batch = batch[:, :1024]\n    print(batch.shape)\n\
          \n    pred = model.forward(batch[:, :-1]).logits[0]\n    loss = torch.nn.functional.cross_entropy(pred,\
          \ batch[0, 1:])\n\n    model.opt.zero_grad()\n    scaler.scale(loss).backward()\n\
          \    scaler.step(model.opt)\n    scaler.update()\n</code></pre>\n<p>\"\"\
          \"</p>\n<p>It returns:<br>\"\"\"<br>/workspace/.miniconda3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:969:\
          \ FutureWarning: The class <code>AutoModelWithLMHead</code> is deprecated\
          \ and will be removed in a future version. Please use <code>AutoModelForCausalLM</code>\
          \ for causal language models, <code>AutoModelForMaskedLM</code> for masked\
          \ language models and <code>AutoModelForSeq2SeqLM</code> for encoder-decoder\
          \ models.<br>  warnings.warn(<br>torch.Size([1, 201])<br>torch.Size([1,\
          \ 201])<br>torch.Size([1, 201])<br>torch.Size([1, 201])<br>torch.Size([1,\
          \ 201])</p>\n<hr>\n<p>RuntimeError                              Traceback\
          \ (most recent call last)<br>Input In [1], in &lt;cell line: 10&gt;()<br>\
          \     34 model.opt.zero_grad()<br>     35 scaler.scale(loss).backward()<br>---&gt;\
          \ 36 scaler.step(model.opt)<br>     37 scaler.update()</p>\n<p>File ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:338,\
          \ in GradScaler.step(self, optimizer, *args, **kwargs)<br>    334     self.unscale_(optimizer)<br>\
          \    336 assert len(optimizer_state[\"found_inf_per_device\"]) &gt; 0, \"\
          No inf checks were recorded for this optimizer.\"<br>--&gt; 338 retval =\
          \ self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)<br>\
          \    340 optimizer_state[\"stage\"] = OptState.STEPPED<br>    342 return\
          \ retval</p>\n<p>File ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:285,\
          \ in GradScaler._maybe_opt_step(self, optimizer, optimizer_state, *args,\
          \ **kwargs)<br>    283 retval = None<br>    284 if not sum(v.item() for\
          \ v in optimizer_state[\"found_inf_per_device\"].values()):<br>--&gt; 285\
          \     retval = optimizer.step(*args, **kwargs)<br>    286 return retval</p>\n\
          <p>File ~/.miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88,\
          \ in Optimizer._hook_for_profile..profile_hook_step..wrapper(*args, **kwargs)<br>\
          \     86 profile_name = \"Optimizer.step#{}.step\".format(obj.<strong>class</strong>.<strong>name</strong>)<br>\
          \     87 with torch.autograd.profiler.record_function(profile_name):<br>---&gt;\
          \ 88     return func(*args, **kwargs)</p>\n<p>File ~/.miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28,\
          \ in _DecoratorContextManager.<strong>call</strong>..decorate_context(*args,\
          \ **kwargs)<br>     25 @functools.wraps(func)<br>     26 def decorate_context(*args,\
          \ **kwargs):<br>     27     with self.<strong>class</strong>():<br>---&gt;\
          \ 28         return func(*args, **kwargs)</p>\n<p>File ~/.miniconda3/lib/python3.9/site-packages/torch/optim/nadam.py:119,\
          \ in NAdam.step(self, closure)<br>    116         # record the step after\
          \ step update<br>    117         state_steps.append(state['step'])<br>--&gt;\
          \ 119 F.nadam(params_with_grad,<br>    120         grads,<br>    121   \
          \      exp_avgs,<br>    122         exp_avg_sqs,<br>    123         mu_products,<br>\
          \    124         state_steps,<br>    125         beta1=beta1,<br>    126\
          \         beta2=beta2,<br>    127         lr=group['lr'],<br>    128   \
          \      weight_decay=group['weight_decay'],<br>    129         momentum_decay=group['momentum_decay'],<br>\
          \    130         eps=group['eps'])<br>    132 # update mu_product<br>  \
          \  133 for p, mu_product in zip(params_with_grad, mu_products):</p>\n<p>File\
          \ ~/.miniconda3/lib/python3.9/site-packages/torch/optim/<em>functional.py:402,\
          \ in nadam(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps,\
          \ beta1, beta2, lr, weight_decay, momentum_decay, eps)<br>    399 exp_avg.mul</em>(beta1).add_(grad,\
          \ alpha=1 - beta1)<br>    400 exp_avg_sq.mul_(beta2).addcmul_(grad, grad,\
          \ value=1 - beta2)<br>--&gt; 402 denom = exp_avg_sq.div(bias_correction2).sqrt().add_(eps)<br>\
          \    403 param.addcdiv_(grad, denom, value=-lr * (1. - mu) / (1. - mu_product))<br>\
          \    404 param.addcdiv_(exp_avg, denom, value=-lr * mu_next / (1. - mu_product_next))</p>\n\
          <p>RuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0;\
          \ 31.75 GiB total capacity; 28.97 GiB already allocated; 999.50 MiB free;\
          \ 29.42 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;\
          \ allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>\"\
          \"\"</p>\n"
        raw: "Hi, thanks for the quick answer, AutoModelWithLMHead.from_pretrained(\"\
          bigscience/bloom-1b3\", torch_dtype=\"auto\") leads to ERROR root Attempting\
          \ to unscale FP16 gradients.\n\nHere is a minimal example :\n\"\"\"\nimport\
          \ sys, torch, time, logging\nimport pandas as pd\nimport numpy as np\nfrom\
          \ transformers import AutoModelWithLMHead, AutoTokenizer\nfrom sklearn.model_selection\
          \ import KFold\n\ndf = pd.DataFrame()\ndf['tokens'] = [[i+100 for i in range(200)]\
          \ for _ in range(10000)]\n\nwith torch.cuda.amp.autocast():\n    tokenizer\
          \ = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b3\")\n    model =\
          \ AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-1b3\")\n\n    df['tokens']\
          \ = df['tokens'].map(lambda x: x + [tokenizer.eos_token_id])\n\n    scaler\
          \ = torch.cuda.amp.GradScaler()\n\n    torch.manual_seed(12)\n    model.opt\
          \ = torch.optim.NAdam(model.parameters(), lr=0.000007)\n    device = 'cuda'\n\
          \    model.to(device)\n\n    kf = KFold(n_splits=len(df), shuffle=True)\n\
          \    for batch_nb, (_, batch_indexes) in enumerate(kf.split(df)):\n    \
          \    batch = df.iloc[batch_indexes]\n        batch = np.array(batch['tokens'].values[0])\n\
          \        batch = torch.from_numpy(batch)[None, :].to(model.device)\n   \
          \     batch = batch[:, :1024]\n        print(batch.shape)\n\n        pred\
          \ = model.forward(batch[:, :-1]).logits[0]\n        loss = torch.nn.functional.cross_entropy(pred,\
          \ batch[0, 1:])\n\n        model.opt.zero_grad()\n        scaler.scale(loss).backward()\n\
          \        scaler.step(model.opt)\n        scaler.update()\n\"\"\"\n\nIt returns:\n\
          \"\"\"\n/workspace/.miniconda3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:969:\
          \ FutureWarning: The class `AutoModelWithLMHead` is deprecated and will\
          \ be removed in a future version. Please use `AutoModelForCausalLM` for\
          \ causal language models, `AutoModelForMaskedLM` for masked language models\
          \ and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n\
          torch.Size([1, 201])\ntorch.Size([1, 201])\ntorch.Size([1, 201])\ntorch.Size([1,\
          \ 201])\ntorch.Size([1, 201])\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Input In [1], in <cell line: 10>()\n     34 model.opt.zero_grad()\n    \
          \ 35 scaler.scale(loss).backward()\n---> 36 scaler.step(model.opt)\n   \
          \  37 scaler.update()\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:338,\
          \ in GradScaler.step(self, optimizer, *args, **kwargs)\n    334     self.unscale_(optimizer)\n\
          \    336 assert len(optimizer_state[\"found_inf_per_device\"]) > 0, \"No\
          \ inf checks were recorded for this optimizer.\"\n--> 338 retval = self._maybe_opt_step(optimizer,\
          \ optimizer_state, *args, **kwargs)\n    340 optimizer_state[\"stage\"]\
          \ = OptState.STEPPED\n    342 return retval\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:285,\
          \ in GradScaler._maybe_opt_step(self, optimizer, optimizer_state, *args,\
          \ **kwargs)\n    283 retval = None\n    284 if not sum(v.item() for v in\
          \ optimizer_state[\"found_inf_per_device\"].values()):\n--> 285     retval\
          \ = optimizer.step(*args, **kwargs)\n    286 return retval\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88,\
          \ in Optimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper(*args,\
          \ **kwargs)\n     86 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\n\
          \     87 with torch.autograd.profiler.record_function(profile_name):\n--->\
          \ 88     return func(*args, **kwargs)\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28,\
          \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args,\
          \ **kwargs)\n     25 @functools.wraps(func)\n     26 def decorate_context(*args,\
          \ **kwargs):\n     27     with self.__class__():\n---> 28         return\
          \ func(*args, **kwargs)\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/nadam.py:119,\
          \ in NAdam.step(self, closure)\n    116         # record the step after\
          \ step update\n    117         state_steps.append(state['step'])\n--> 119\
          \ F.nadam(params_with_grad,\n    120         grads,\n    121         exp_avgs,\n\
          \    122         exp_avg_sqs,\n    123         mu_products,\n    124   \
          \      state_steps,\n    125         beta1=beta1,\n    126         beta2=beta2,\n\
          \    127         lr=group['lr'],\n    128         weight_decay=group['weight_decay'],\n\
          \    129         momentum_decay=group['momentum_decay'],\n    130      \
          \   eps=group['eps'])\n    132 # update mu_product\n    133 for p, mu_product\
          \ in zip(params_with_grad, mu_products):\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py:402,\
          \ in nadam(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps,\
          \ beta1, beta2, lr, weight_decay, momentum_decay, eps)\n    399 exp_avg.mul_(beta1).add_(grad,\
          \ alpha=1 - beta1)\n    400 exp_avg_sq.mul_(beta2).addcmul_(grad, grad,\
          \ value=1 - beta2)\n--> 402 denom = exp_avg_sq.div(bias_correction2).sqrt().add_(eps)\n\
          \    403 param.addcdiv_(grad, denom, value=-lr * (1. - mu) / (1. - mu_product))\n\
          \    404 param.addcdiv_(exp_avg, denom, value=-lr * mu_next / (1. - mu_product_next))\n\
          \nRuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 31.75\
          \ GiB total capacity; 28.97 GiB already allocated; 999.50 MiB free; 29.42\
          \ GiB reserved in total by PyTorch) If reserved memory is >> allocated memory\
          \ try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\"\"\""
        updatedAt: '2022-06-20T09:38:36.080Z'
      numEdits: 2
      reactions: []
    id: 62b03fd3744b9a896b9bb17a
    type: comment
  author: edmond
  content: "Hi, thanks for the quick answer, AutoModelWithLMHead.from_pretrained(\"\
    bigscience/bloom-1b3\", torch_dtype=\"auto\") leads to ERROR root Attempting to\
    \ unscale FP16 gradients.\n\nHere is a minimal example :\n\"\"\"\nimport sys,\
    \ torch, time, logging\nimport pandas as pd\nimport numpy as np\nfrom transformers\
    \ import AutoModelWithLMHead, AutoTokenizer\nfrom sklearn.model_selection import\
    \ KFold\n\ndf = pd.DataFrame()\ndf['tokens'] = [[i+100 for i in range(200)] for\
    \ _ in range(10000)]\n\nwith torch.cuda.amp.autocast():\n    tokenizer = AutoTokenizer.from_pretrained(\"\
    bigscience/bloom-1b3\")\n    model = AutoModelWithLMHead.from_pretrained(\"bigscience/bloom-1b3\"\
    )\n\n    df['tokens'] = df['tokens'].map(lambda x: x + [tokenizer.eos_token_id])\n\
    \n    scaler = torch.cuda.amp.GradScaler()\n\n    torch.manual_seed(12)\n    model.opt\
    \ = torch.optim.NAdam(model.parameters(), lr=0.000007)\n    device = 'cuda'\n\
    \    model.to(device)\n\n    kf = KFold(n_splits=len(df), shuffle=True)\n    for\
    \ batch_nb, (_, batch_indexes) in enumerate(kf.split(df)):\n        batch = df.iloc[batch_indexes]\n\
    \        batch = np.array(batch['tokens'].values[0])\n        batch = torch.from_numpy(batch)[None,\
    \ :].to(model.device)\n        batch = batch[:, :1024]\n        print(batch.shape)\n\
    \n        pred = model.forward(batch[:, :-1]).logits[0]\n        loss = torch.nn.functional.cross_entropy(pred,\
    \ batch[0, 1:])\n\n        model.opt.zero_grad()\n        scaler.scale(loss).backward()\n\
    \        scaler.step(model.opt)\n        scaler.update()\n\"\"\"\n\nIt returns:\n\
    \"\"\"\n/workspace/.miniconda3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:969:\
    \ FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed\
    \ in a future version. Please use `AutoModelForCausalLM` for causal language models,\
    \ `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM`\
    \ for encoder-decoder models.\n  warnings.warn(\ntorch.Size([1, 201])\ntorch.Size([1,\
    \ 201])\ntorch.Size([1, 201])\ntorch.Size([1, 201])\ntorch.Size([1, 201])\n---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Input In [1], in <cell line: 10>()\n     34 model.opt.zero_grad()\n     35 scaler.scale(loss).backward()\n\
    ---> 36 scaler.step(model.opt)\n     37 scaler.update()\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:338,\
    \ in GradScaler.step(self, optimizer, *args, **kwargs)\n    334     self.unscale_(optimizer)\n\
    \    336 assert len(optimizer_state[\"found_inf_per_device\"]) > 0, \"No inf checks\
    \ were recorded for this optimizer.\"\n--> 338 retval = self._maybe_opt_step(optimizer,\
    \ optimizer_state, *args, **kwargs)\n    340 optimizer_state[\"stage\"] = OptState.STEPPED\n\
    \    342 return retval\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:285,\
    \ in GradScaler._maybe_opt_step(self, optimizer, optimizer_state, *args, **kwargs)\n\
    \    283 retval = None\n    284 if not sum(v.item() for v in optimizer_state[\"\
    found_inf_per_device\"].values()):\n--> 285     retval = optimizer.step(*args,\
    \ **kwargs)\n    286 return retval\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:88,\
    \ in Optimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper(*args,\
    \ **kwargs)\n     86 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\n\
    \     87 with torch.autograd.profiler.record_function(profile_name):\n---> 88\
    \     return func(*args, **kwargs)\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28,\
    \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)\n\
    \     25 @functools.wraps(func)\n     26 def decorate_context(*args, **kwargs):\n\
    \     27     with self.__class__():\n---> 28         return func(*args, **kwargs)\n\
    \nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/nadam.py:119, in\
    \ NAdam.step(self, closure)\n    116         # record the step after step update\n\
    \    117         state_steps.append(state['step'])\n--> 119 F.nadam(params_with_grad,\n\
    \    120         grads,\n    121         exp_avgs,\n    122         exp_avg_sqs,\n\
    \    123         mu_products,\n    124         state_steps,\n    125         beta1=beta1,\n\
    \    126         beta2=beta2,\n    127         lr=group['lr'],\n    128      \
    \   weight_decay=group['weight_decay'],\n    129         momentum_decay=group['momentum_decay'],\n\
    \    130         eps=group['eps'])\n    132 # update mu_product\n    133 for p,\
    \ mu_product in zip(params_with_grad, mu_products):\n\nFile ~/.miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py:402,\
    \ in nadam(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps, beta1,\
    \ beta2, lr, weight_decay, momentum_decay, eps)\n    399 exp_avg.mul_(beta1).add_(grad,\
    \ alpha=1 - beta1)\n    400 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1\
    \ - beta2)\n--> 402 denom = exp_avg_sq.div(bias_correction2).sqrt().add_(eps)\n\
    \    403 param.addcdiv_(grad, denom, value=-lr * (1. - mu) / (1. - mu_product))\n\
    \    404 param.addcdiv_(exp_avg, denom, value=-lr * mu_next / (1. - mu_product_next))\n\
    \nRuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 31.75 GiB\
    \ total capacity; 28.97 GiB already allocated; 999.50 MiB free; 29.42 GiB reserved\
    \ in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    \"\"\""
  created_at: 2022-06-20 08:37:23+00:00
  edited: true
  hidden: false
  id: 62b03fd3744b9a896b9bb17a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-06-20T09:38:11.000Z'
    data:
      status: closed
    id: 62b0400353d878042faa52bd
    type: status-change
  author: edmond
  created_at: 2022-06-20 08:38:11+00:00
  id: 62b0400353d878042faa52bd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-06-20T09:38:17.000Z'
    data:
      status: open
    id: 62b04009744b9a896b9bb645
    type: status-change
  author: edmond
  created_at: 2022-06-20 08:38:17+00:00
  id: 62b04009744b9a896b9bb645
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-06-29T19:40:38.000Z'
    data:
      edited: true
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: '<p>Anyone ?<br>I used Adam but it still doesnt fit, only SGD does or
          if I freeze many layers, is that normal ? :(</p>

          '
        raw: 'Anyone ?

          I used Adam but it still doesnt fit, only SGD does or if I freeze many layers,
          is that normal ? :('
        updatedAt: '2022-06-29T20:32:33.257Z'
      numEdits: 4
      reactions: []
    id: 62bcaab6e81dfd65cceeed39
    type: comment
  author: edmond
  content: 'Anyone ?

    I used Adam but it still doesnt fit, only SGD does or if I freeze many layers,
    is that normal ? :('
  created_at: 2022-06-29 18:40:38+00:00
  edited: true
  hidden: false
  id: 62bcaab6e81dfd65cceeed39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-06-29T23:08:39.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Hey! Sorry I''m currently busy on another project. I''ll try your
          minimal script as soon as I have some bandwidth. I''ll have to double check
          a few things:</p>

          <ul>

          <li>is 1b3 the total amount of parameters with or without vocabulary? Vocab
          is actually quite big in our case but I don''t think it should really tip
          the scale on v100 32g.</li>

          <li>i would suggest removing <code>auto</code> as it''s for larger models
          that require cpu offloading, but you should easily fit the weights inside
          a single device.</li>

          <li>can you just load weights + optimizer and check memory footprint? I
          want to understand if the issue is actually loading the model or running
          it?</li>

          </ul>

          <p>Sorry for the very late reply.</p>

          '
        raw: "Hey! Sorry I'm currently busy on another project. I'll try your minimal\
          \ script as soon as I have some bandwidth. I'll have to double check a few\
          \ things:\n - is 1b3 the total amount of parameters with or without vocabulary?\
          \ Vocab is actually quite big in our case but I don't think it should really\
          \ tip the scale on v100 32g.\n - i would suggest removing `auto` as it's\
          \ for larger models that require cpu offloading, but you should easily fit\
          \ the weights inside a single device.\n - can you just load weights + optimizer\
          \ and check memory footprint? I want to understand if the issue is actually\
          \ loading the model or running it?\n\nSorry for the very late reply."
        updatedAt: '2022-06-29T23:08:39.946Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - edmond
    id: 62bcdb778d752f69a96e36c4
    type: comment
  author: TimeRobber
  content: "Hey! Sorry I'm currently busy on another project. I'll try your minimal\
    \ script as soon as I have some bandwidth. I'll have to double check a few things:\n\
    \ - is 1b3 the total amount of parameters with or without vocabulary? Vocab is\
    \ actually quite big in our case but I don't think it should really tip the scale\
    \ on v100 32g.\n - i would suggest removing `auto` as it's for larger models that\
    \ require cpu offloading, but you should easily fit the weights inside a single\
    \ device.\n - can you just load weights + optimizer and check memory footprint?\
    \ I want to understand if the issue is actually loading the model or running it?\n\
    \nSorry for the very late reply."
  created_at: 2022-06-29 22:08:39+00:00
  edited: false
  hidden: false
  id: 62bcdb778d752f69a96e36c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-06-29T23:20:27.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Also can you check if removing GradScaler helps?</p>

          '
        raw: Also can you check if removing GradScaler helps?
        updatedAt: '2022-06-29T23:20:27.271Z'
      numEdits: 0
      reactions: []
    id: 62bcde3b7553b8a6ac52fad9
    type: comment
  author: TimeRobber
  content: Also can you check if removing GradScaler helps?
  created_at: 2022-06-29 22:20:27+00:00
  edited: false
  hidden: false
  id: 62bcde3b7553b8a6ac52fad9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2022-07-01T16:56:29.000Z'
    data:
      edited: false
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: "<p>Hi, no worries for the late, I just thought you guys forgot me,\
          \ I will be a bit more patient next time ^^.</p>\n<ul>\n<li><p>About the\
          \ number of parameters I am not sure but I tried this :<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/1656692971491-62af665424488e6adfa9b8e2.png\"\
          ><img alt=\"Capture d\u2019e\u0301cran 2022-07-01 a\u0300 6.28.11 PM.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/1656692971491-62af665424488e6adfa9b8e2.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1656692976595-62af665424488e6adfa9b8e2.png\"\
          ><img alt=\"Capture d\u2019e\u0301cran 2022-07-01 a\u0300 6.27.59 PM.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/1656692976595-62af665424488e6adfa9b8e2.png\"\
          ></a></p>\n</li>\n<li><p>I deleted auto and the model runs if I use SGD\
          \ or more frozen layers by doing :<br>\"<br>  for name, param in model.named_parameters():<br>\
          \  param.requires_grad = True if 'ln' in name.lower() or 'norm' in <br>\
          \                        name.lower() or 'wpe' in name.lower() or <br> \
          \                       'wte' in name.lower() or <br>                  \
          \      'position_embeddings' in name.lower() or <br>                   \
          \     'pos_drop' in name.lower() else False<br>\"</p>\n</li>\n<li><p>Sure,\
          \ about the footprint I am following <a rel=\"nofollow\" href=\"https://discuss.pytorch.org/t/how-to-check-the-gpu-memory-being-used/131220?u=edmondj\"\
          >https://discuss.pytorch.org/t/how-to-check-the-gpu-memory-being-used/131220?u=edmondj</a>\
          \ and running this before having the exception being raised :<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1656693969752-62af665424488e6adfa9b8e2.png\"\
          ><img alt=\"Capture d\u2019e\u0301cran 2022-07-01 a\u0300 6.45.34 PM.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/1656693969752-62af665424488e6adfa9b8e2.png\"\
          ></a></p>\n</li>\n<li><p>Without the GradScaler its even worse, not even\
          \ one backprop step succeeds, it crashes at the first one (but I think it\
          \ is a bit random since when I got also rid of with torch.cuda.amp.autocast():\
          \ after that, and it crashed at the 2nd iteration).</p>\n</li>\n</ul>\n\
          <p>Thanks !</p>\n"
        raw: "Hi, no worries for the late, I just thought you guys forgot me, I will\
          \ be a bit more patient next time ^^.\n\n- About the number of parameters\
          \ I am not sure but I tried this :\n![Capture d\u2019e\u0301cran 2022-07-01\
          \ a\u0300 6.28.11 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656692971491-62af665424488e6adfa9b8e2.png)\n\
          ![Capture d\u2019e\u0301cran 2022-07-01 a\u0300 6.27.59 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656692976595-62af665424488e6adfa9b8e2.png)\n\
          \n- I deleted auto and the model runs if I use SGD or more frozen layers\
          \ by doing :\n\"\n    for name, param in model.named_parameters():\n   \
          \     param.requires_grad = True if 'ln' in name.lower() or 'norm' in \\\
          \n                              name.lower() or 'wpe' in name.lower() or\
          \ \\\n                              'wte' in name.lower() or \\\n      \
          \                        'position_embeddings' in name.lower() or \\\n \
          \                             'pos_drop' in name.lower() else False\n\"\n\
          \n- Sure, about the footprint I am following https://discuss.pytorch.org/t/how-to-check-the-gpu-memory-being-used/131220?u=edmondj\
          \ and running this before having the exception being raised :\n![Capture\
          \ d\u2019e\u0301cran 2022-07-01 a\u0300 6.45.34 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656693969752-62af665424488e6adfa9b8e2.png)\n\
          \n- Without the GradScaler its even worse, not even one backprop step succeeds,\
          \ it crashes at the first one (but I think it is a bit random since when\
          \ I got also rid of with torch.cuda.amp.autocast(): after that, and it crashed\
          \ at the 2nd iteration).\n\nThanks !"
        updatedAt: '2022-07-01T16:56:29.533Z'
      numEdits: 0
      reactions: []
    id: 62bf273d34afe69e1a1922fb
    type: comment
  author: edmond
  content: "Hi, no worries for the late, I just thought you guys forgot me, I will\
    \ be a bit more patient next time ^^.\n\n- About the number of parameters I am\
    \ not sure but I tried this :\n![Capture d\u2019e\u0301cran 2022-07-01 a\u0300\
    \ 6.28.11 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656692971491-62af665424488e6adfa9b8e2.png)\n\
    ![Capture d\u2019e\u0301cran 2022-07-01 a\u0300 6.27.59 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656692976595-62af665424488e6adfa9b8e2.png)\n\
    \n- I deleted auto and the model runs if I use SGD or more frozen layers by doing\
    \ :\n\"\n    for name, param in model.named_parameters():\n        param.requires_grad\
    \ = True if 'ln' in name.lower() or 'norm' in \\\n                           \
    \   name.lower() or 'wpe' in name.lower() or \\\n                            \
    \  'wte' in name.lower() or \\\n                              'position_embeddings'\
    \ in name.lower() or \\\n                              'pos_drop' in name.lower()\
    \ else False\n\"\n\n- Sure, about the footprint I am following https://discuss.pytorch.org/t/how-to-check-the-gpu-memory-being-used/131220?u=edmondj\
    \ and running this before having the exception being raised :\n![Capture d\u2019\
    e\u0301cran 2022-07-01 a\u0300 6.45.34 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1656693969752-62af665424488e6adfa9b8e2.png)\n\
    \n- Without the GradScaler its even worse, not even one backprop step succeeds,\
    \ it crashes at the first one (but I think it is a bit random since when I got\
    \ also rid of with torch.cuda.amp.autocast(): after that, and it crashed at the\
    \ 2nd iteration).\n\nThanks !"
  created_at: 2022-07-01 15:56:29+00:00
  edited: false
  hidden: false
  id: 62bf273d34afe69e1a1922fb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: bigscience/bloom-1b7
repo_type: model
status: open
target_branch: null
title: ERROR root CUDA out of memory
