!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gihong99
conflicting_files: null
created_at: 2023-12-21 01:59:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8907b81c1f3acb833f26bc6e295144ec.svg
      fullname: Lee Gi Hong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gihong99
      type: user
    createdAt: '2023-12-21T01:59:46.000Z'
    data:
      edited: false
      editors:
      - gihong99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9134989380836487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8907b81c1f3acb833f26bc6e295144ec.svg
          fullname: Lee Gi Hong
          isHf: false
          isPro: false
          name: gihong99
          type: user
        html: '<p>I''m a student who learns and touches the LLM model. Can I get the
          information on the number of data and the learning code separately?</p>

          '
        raw: I'm a student who learns and touches the LLM model. Can I get the information
          on the number of data and the learning code separately?
        updatedAt: '2023-12-21T01:59:46.833Z'
      numEdits: 0
      reactions: []
    id: 65839c1240e42901d649b1d5
    type: comment
  author: gihong99
  content: I'm a student who learns and touches the LLM model. Can I get the information
    on the number of data and the learning code separately?
  created_at: 2023-12-21 01:59:46+00:00
  edited: false
  hidden: false
  id: 65839c1240e42901d649b1d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
      fullname: Saofiq
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Sao10K
      type: user
    createdAt: '2023-12-21T04:03:50.000Z'
    data:
      edited: false
      editors:
      - Sao10K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6641064286231995
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
          fullname: Saofiq
          isHf: false
          isPro: false
          name: Sao10K
          type: user
        html: '<p>Hi, what exactly do you mean for the learning code?</p>

          <p>If it is hyperparameters, this is it:</p>

          <p>qLoRA finetune.<br>2 Epochs -  Batch size of 1 on 4x 4090s.<br>Learning
          Rate - 0.00035<br>Scheduler - Cosine<br>Optimizer - adamw_bnb_8bit<br>LoRA
          rank - 16 | LoRA alpha - 32<br>lora_target_modules - [  - gate_proj  - down_proj  -
          up_proj  - q_proj - v_proj  - k_proj  - o_proj]</p>

          <p>For the dataset itself, that is private. Number of samples would be based
          on the model card, so 10% is 5.2K, total 52K. I listed the composition there.</p>

          '
        raw: 'Hi, what exactly do you mean for the learning code?


          If it is hyperparameters, this is it:


          qLoRA finetune.

          2 Epochs -  Batch size of 1 on 4x 4090s.

          Learning Rate - 0.00035

          Scheduler - Cosine

          Optimizer - adamw_bnb_8bit

          LoRA rank - 16 | LoRA alpha - 32

          lora_target_modules - [  - gate_proj  - down_proj  - up_proj  - q_proj -
          v_proj  - k_proj  - o_proj]


          For the dataset itself, that is private. Number of samples would be based
          on the model card, so 10% is 5.2K, total 52K. I listed the composition there.

          '
        updatedAt: '2023-12-21T04:03:50.013Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 6583b926c327dc81ff0f4e95
    type: comment
  author: Sao10K
  content: 'Hi, what exactly do you mean for the learning code?


    If it is hyperparameters, this is it:


    qLoRA finetune.

    2 Epochs -  Batch size of 1 on 4x 4090s.

    Learning Rate - 0.00035

    Scheduler - Cosine

    Optimizer - adamw_bnb_8bit

    LoRA rank - 16 | LoRA alpha - 32

    lora_target_modules - [  - gate_proj  - down_proj  - up_proj  - q_proj - v_proj  -
    k_proj  - o_proj]


    For the dataset itself, that is private. Number of samples would be based on the
    model card, so 10% is 5.2K, total 52K. I listed the composition there.

    '
  created_at: 2023-12-21 04:03:50+00:00
  edited: false
  hidden: false
  id: 6583b926c327dc81ff0f4e95
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Sao10K/Frostwind-10.7B-v1
repo_type: model
status: open
target_branch: null
title: Provide learning code
