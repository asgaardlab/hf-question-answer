!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bryanjmo
conflicting_files: null
created_at: 2023-08-16 15:23:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/864fe2463922f82febb4f0bf19a3e255.svg
      fullname: Bryan Moore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bryanjmo
      type: user
    createdAt: '2023-08-16T16:23:07.000Z'
    data:
      edited: false
      editors:
      - bryanjmo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9668580293655396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/864fe2463922f82febb4f0bf19a3e255.svg
          fullname: Bryan Moore
          isHf: false
          isPro: false
          name: bryanjmo
          type: user
        html: '<p>I am missing how the functions are currently able to handle the
          gates being stochastic during training and deterministic during testing.
          In the original repo this is handled during forward() with z = self.sample_z(input.size(0),
          sample=self.training). But in this L0_Regularizer version it seems that
          sample is always set to True. Am I missing something?</p>

          '
        raw: I am missing how the functions are currently able to handle the gates
          being stochastic during training and deterministic during testing. In the
          original repo this is handled during forward() with z = self.sample_z(input.size(0),
          sample=self.training). But in this L0_Regularizer version it seems that
          sample is always set to True. Am I missing something?
        updatedAt: '2023-08-16T16:23:07.229Z'
      numEdits: 0
      reactions: []
    id: 64dcf7ebd0062f988c8b1b11
    type: comment
  author: bryanjmo
  content: I am missing how the functions are currently able to handle the gates being
    stochastic during training and deterministic during testing. In the original repo
    this is handled during forward() with z = self.sample_z(input.size(0), sample=self.training).
    But in this L0_Regularizer version it seems that sample is always set to True.
    Am I missing something?
  created_at: 2023-08-16 15:23:07+00:00
  edited: false
  hidden: false
  id: 64dcf7ebd0062f988c8b1b11
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chaaz992/L0_Regularizer
repo_type: model
status: open
target_branch: null
title: Handles stochastic gates during training and deterministic during validation/testing?
