!!python/object:huggingface_hub.community.DiscussionWithDetails
author: giuliogalvan
conflicting_files: null
created_at: 2023-06-29 07:37:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
      fullname: GIulioGalvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giuliogalvan
      type: user
    createdAt: '2023-06-29T08:37:58.000Z'
    data:
      edited: false
      editors:
      - giuliogalvan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8369277715682983
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
          fullname: GIulioGalvan
          isHf: false
          isPro: false
          name: giuliogalvan
          type: user
        html: '<p>inference endpoint does not work. Getting sharding error.</p>

          '
        raw: inference endpoint does not work. Getting sharding error.
        updatedAt: '2023-06-29T08:37:58.512Z'
      numEdits: 0
      reactions: []
    id: 649d42e6e41931df0edb8757
    type: comment
  author: giuliogalvan
  content: inference endpoint does not work. Getting sharding error.
  created_at: 2023-06-29 07:37:58+00:00
  edited: false
  hidden: false
  id: 649d42e6e41931df0edb8757
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
      fullname: GIulioGalvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giuliogalvan
      type: user
    createdAt: '2023-06-30T08:21:58.000Z'
    data:
      edited: false
      editors:
      - giuliogalvan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4801918864250183
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
          fullname: GIulioGalvan
          isHf: false
          isPro: false
          name: giuliogalvan
          type: user
        html: '<p>updated to the newest revision but still cannot use endpoints. I
          attach the log:</p>

          <p>cj6v6 2023-06-30T08:19:33.903Z {"timestamp":"2023-06-30T08:19:33.903685Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n File "/opt/conda/bin/text-generation-server",
          line 8, in \n sys.exit(app())\n File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in __call__\n return get_command(self)(*args, **kwargs)\n File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1130, in __call__\n
          return self.main(*args, **kwargs)\n File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 778, in main\n return _main(\n File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main\n rv = self.invoke(ctx)\n File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1657, in invoke\n return _process_result(sub_ctx.command.invoke(sub_ctx))\n
          File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1404,
          in invoke\n return ctx.invoke(self.callback, **ctx.params)\n File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 760, in invoke\n return __callback(*args, **kwargs)\n File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 683, in wrapper\n return callback(**use_params) # type: ignore\n File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 67, in serve\n server.serve(model_id, revision, sharded, quantize,
          trust_remote_code, uds_path)\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded,
          quantize, trust_remote_code))\n File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run\n return loop.run_until_complete(main)\n File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete\n self.run_forever()\n File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever\n self._run_once()\n File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once\n handle._run()\n File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run\n self._context.run(self._callback, *self._args)\n&gt;
          File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 124, in serve_inner\n model = get_model(model_id, revision, sharded,
          quantize, trust_remote_code)\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 246, in get_model\n return llama_cls(\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py",
          line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n File
          "/usr/src/transformers/src/transformers/tokenization_utils_base.py", line
          1812, in from_pretrained\n return cls._from_pretrained(\n File "/usr/src/transformers/src/transformers/tokenization_utils_base.py",
          line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n
          File "/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py",
          line 96, in __init__\n self.sp_model.Load(vocab_file)\n File "/opt/conda/lib/python3.9/site-packages/sentencepiece/<strong>init</strong>.py",
          line 905, in Load\n return self.LoadFromFile(model_file)\n File "/opt/conda/lib/python3.9/site-packages/sentencepiece/<strong>init</strong>.py",
          line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)\nTypeError: not a string\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}<br>cj6v6
          2023-06-30T08:19:34.351Z Error: ShardCannotStart<br>cj6v6 2023-06-30T08:19:34.351Z
          {"timestamp":"2023-06-30T08:19:34.351030Z","level":"ERROR","fields":{"message":"Shard
          0 failed to start:\nThe tokenizer class you load from this checkpoint is
          not the same type as the class this function is called from. It may result
          in unexpected tokenization. \nThe tokenizer class you load from this checkpoint
          is ''XgenTokenizer''. \nThe class this function is called from is ''LlamaTokenizer''.\nTraceback
          (most recent call last):\n\n File "/opt/conda/bin/text-generation-server",
          line 8, in \n sys.exit(app())\n\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 67, in serve\n server.serve(model_id, revision, sharded, quantize,
          trust_remote_code, uds_path)\n\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded,
          quantize, trust_remote_code))\n\n File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run\n return loop.run_until_complete(main)\n\n File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 647, in run_until_complete\n return future.result()\n\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 124, in serve_inner\n model = get_model(model_id, revision, sharded,
          quantize, trust_remote_code)\n\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 246, in get_model\n return llama_cls(\n\n File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py",
          line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n\n File
          "/usr/src/transformers/src/transformers/tokenization_utils_base.py", line
          1812, in from_pretrained\n return cls._from_pretrained(\n\n File "/usr/src/transformers/src/transformers/tokenization_utils_base.py",
          line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n\n
          File "/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py",
          line 96, in __init__\n self.sp_model.Load(vocab_file)\n\n File "/opt/conda/lib/python3.9/site-packages/sentencepiece/<strong>init</strong>.py",
          line 905, in Load\n return self.LoadFromFile(model_file)\n\n File "/opt/conda/lib/python3.9/site-packages/sentencepiece/<strong>init</strong>.py",
          line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)\n\nTypeError: not a string\n\n"},"target":"text_generation_launcher"}<br>cj6v6
          2023-06-30T08:19:34.351Z {"timestamp":"2023-06-30T08:19:34.351077Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}</p>

          '
        raw: 'updated to the newest revision but still cannot use endpoints. I attach
          the log:


          cj6v6 2023-06-30T08:19:33.903Z {"timestamp":"2023-06-30T08:19:33.903685Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n File \"/opt/conda/bin/text-generation-server\",
          line 8, in <module>\n sys.exit(app())\n File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n return get_command(self)(*args, **kwargs)\n File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130, in
          __call__\n return self.main(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n return _main(\n File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n rv = self.invoke(ctx)\n File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1657, in invoke\n return _process_result(sub_ctx.command.invoke(sub_ctx))\n
          File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,
          in invoke\n return ctx.invoke(self.callback, **ctx.params)\n File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 760, in invoke\n return __callback(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n return callback(**use_params) # type: ignore\n File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 67, in serve\n server.serve(model_id, revision, sharded, quantize,
          trust_remote_code, uds_path)\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded,
          quantize, trust_remote_code))\n File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n return loop.run_until_complete(main)\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n self.run_forever()\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n self._run_once()\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n handle._run()\n File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n self._context.run(self._callback, *self._args)\n> File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 124, in serve_inner\n model = get_model(model_id, revision, sharded,
          quantize, trust_remote_code)\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 246, in get_model\n return llama_cls(\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\",
          line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n File
          \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line
          1812, in from_pretrained\n return cls._from_pretrained(\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
          line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n
          File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\",
          line 96, in __init__\n self.sp_model.Load(vocab_file)\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
          line 905, in Load\n return self.LoadFromFile(model_file)\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
          line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)\nTypeError: not a string\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          cj6v6 2023-06-30T08:19:34.351Z Error: ShardCannotStart

          cj6v6 2023-06-30T08:19:34.351Z {"timestamp":"2023-06-30T08:19:34.351030Z","level":"ERROR","fields":{"message":"Shard
          0 failed to start:\nThe tokenizer class you load from this checkpoint is
          not the same type as the class this function is called from. It may result
          in unexpected tokenization. \nThe tokenizer class you load from this checkpoint
          is ''XgenTokenizer''. \nThe class this function is called from is ''LlamaTokenizer''.\nTraceback
          (most recent call last):\n\n File \"/opt/conda/bin/text-generation-server\",
          line 8, in <module>\n sys.exit(app())\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 67, in serve\n server.serve(model_id, revision, sharded, quantize,
          trust_remote_code, uds_path)\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded,
          quantize, trust_remote_code))\n\n File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n return loop.run_until_complete(main)\n\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n return future.result()\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 124, in serve_inner\n model = get_model(model_id, revision, sharded,
          quantize, trust_remote_code)\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 246, in get_model\n return llama_cls(\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\",
          line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n\n File
          \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line
          1812, in from_pretrained\n return cls._from_pretrained(\n\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
          line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n\n
          File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\",
          line 96, in __init__\n self.sp_model.Load(vocab_file)\n\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
          line 905, in Load\n return self.LoadFromFile(model_file)\n\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
          line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)\n\nTypeError: not a string\n\n"},"target":"text_generation_launcher"}

          cj6v6 2023-06-30T08:19:34.351Z {"timestamp":"2023-06-30T08:19:34.351077Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}'
        updatedAt: '2023-06-30T08:21:58.174Z'
      numEdits: 0
      reactions: []
    id: 649e90a6d07383fba8d8368a
    type: comment
  author: giuliogalvan
  content: 'updated to the newest revision but still cannot use endpoints. I attach
    the log:


    cj6v6 2023-06-30T08:19:33.903Z {"timestamp":"2023-06-30T08:19:33.903685Z","level":"ERROR","fields":{"message":"Error
    when initializing model\nTraceback (most recent call last):\n File \"/opt/conda/bin/text-generation-server\",
    line 8, in <module>\n sys.exit(app())\n File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
    line 311, in __call__\n return get_command(self)(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 1130, in __call__\n return self.main(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
    line 778, in main\n return _main(\n File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
    line 216, in _main\n rv = self.invoke(ctx)\n File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 1657, in invoke\n return _process_result(sub_ctx.command.invoke(sub_ctx))\n
    File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n
    return ctx.invoke(self.callback, **ctx.params)\n File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 760, in invoke\n return __callback(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
    line 683, in wrapper\n return callback(**use_params) # type: ignore\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
    line 67, in serve\n server.serve(model_id, revision, sharded, quantize, trust_remote_code,
    uds_path)\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded, quantize,
    trust_remote_code))\n File \"/opt/conda/lib/python3.9/asyncio/runners.py\", line
    44, in run\n return loop.run_until_complete(main)\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 634, in run_until_complete\n self.run_forever()\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 601, in run_forever\n self._run_once()\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 1905, in _run_once\n handle._run()\n File \"/opt/conda/lib/python3.9/asyncio/events.py\",
    line 80, in _run\n self._context.run(self._callback, *self._args)\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 124, in serve_inner\n model = get_model(model_id, revision, sharded, quantize,
    trust_remote_code)\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
    line 246, in get_model\n return llama_cls(\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\",
    line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
    line 1812, in from_pretrained\n return cls._from_pretrained(\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
    line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n
    File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\",
    line 96, in __init__\n self.sp_model.Load(vocab_file)\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
    line 905, in Load\n return self.LoadFromFile(model_file)\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
    line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
    arg)\nTypeError: not a string\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

    cj6v6 2023-06-30T08:19:34.351Z Error: ShardCannotStart

    cj6v6 2023-06-30T08:19:34.351Z {"timestamp":"2023-06-30T08:19:34.351030Z","level":"ERROR","fields":{"message":"Shard
    0 failed to start:\nThe tokenizer class you load from this checkpoint is not the
    same type as the class this function is called from. It may result in unexpected
    tokenization. \nThe tokenizer class you load from this checkpoint is ''XgenTokenizer''.
    \nThe class this function is called from is ''LlamaTokenizer''.\nTraceback (most
    recent call last):\n\n File \"/opt/conda/bin/text-generation-server\", line 8,
    in <module>\n sys.exit(app())\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
    line 67, in serve\n server.serve(model_id, revision, sharded, quantize, trust_remote_code,
    uds_path)\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 155, in serve\n asyncio.run(serve_inner(model_id, revision, sharded, quantize,
    trust_remote_code))\n\n File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
    line 44, in run\n return loop.run_until_complete(main)\n\n File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 647, in run_until_complete\n return future.result()\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 124, in serve_inner\n model = get_model(model_id, revision, sharded, quantize,
    trust_remote_code)\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
    line 246, in get_model\n return llama_cls(\n\n File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\",
    line 44, in __init__\n tokenizer = LlamaTokenizer.from_pretrained(\n\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
    line 1812, in from_pretrained\n return cls._from_pretrained(\n\n File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\",
    line 1975, in _from_pretrained\n tokenizer = cls(*init_inputs, **init_kwargs)\n\n
    File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\",
    line 96, in __init__\n self.sp_model.Load(vocab_file)\n\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
    line 905, in Load\n return self.LoadFromFile(model_file)\n\n File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",
    line 310, in LoadFromFile\n return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
    arg)\n\nTypeError: not a string\n\n"},"target":"text_generation_launcher"}

    cj6v6 2023-06-30T08:19:34.351Z {"timestamp":"2023-06-30T08:19:34.351077Z","level":"INFO","fields":{"message":"Shutting
    down shards"},"target":"text_generation_launcher"}'
  created_at: 2023-06-30 07:21:58+00:00
  edited: false
  hidden: false
  id: 649e90a6d07383fba8d8368a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/xgen-7b-8k-inst
repo_type: model
status: open
target_branch: null
title: inference endpoint error
