!!python/object:huggingface_hub.community.DiscussionWithDetails
author: naman-trilogy
conflicting_files: null
created_at: 2023-07-07 14:29:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2427886248c9e0d2787535fa83b1a41e.svg
      fullname: 'Naman '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: naman-trilogy
      type: user
    createdAt: '2023-07-07T15:29:15.000Z'
    data:
      edited: false
      editors:
      - naman-trilogy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.940669059753418
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2427886248c9e0d2787535fa83b1a41e.svg
          fullname: 'Naman '
          isHf: false
          isPro: false
          name: naman-trilogy
          type: user
        html: '<p>Is it recommended to extend pre-training (using plain text) of such
          an instruct model? </p>

          <p>I''ve tried this approach with Llama-7b but then the instruction tuning
          using public datasets wasn''t as good as these models. So now I''m thinking
          of going the other way around to utilise the powerful instruction following
          capabilities of such models on my local enterprise data.</p>

          '
        raw: "Is it recommended to extend pre-training (using plain text) of such\
          \ an instruct model? \r\n\r\nI've tried this approach with Llama-7b but\
          \ then the instruction tuning using public datasets wasn't as good as these\
          \ models. So now I'm thinking of going the other way around to utilise the\
          \ powerful instruction following capabilities of such models on my local\
          \ enterprise data."
        updatedAt: '2023-07-07T15:29:15.441Z'
      numEdits: 0
      reactions: []
    id: 64a82f4bed54ffdf614b0159
    type: comment
  author: naman-trilogy
  content: "Is it recommended to extend pre-training (using plain text) of such an\
    \ instruct model? \r\n\r\nI've tried this approach with Llama-7b but then the\
    \ instruction tuning using public datasets wasn't as good as these models. So\
    \ now I'm thinking of going the other way around to utilise the powerful instruction\
    \ following capabilities of such models on my local enterprise data."
  created_at: 2023-07-07 14:29:15+00:00
  edited: false
  hidden: false
  id: 64a82f4bed54ffdf614b0159
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: Salesforce/xgen-7b-8k-inst
repo_type: model
status: open
target_branch: null
title: Extended pre-training (using plain text) of such an instruct model?
