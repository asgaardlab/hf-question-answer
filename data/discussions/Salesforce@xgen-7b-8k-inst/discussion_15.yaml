!!python/object:huggingface_hub.community.DiscussionWithDetails
author: StevenLyp
conflicting_files: null
created_at: 2023-10-24 06:55:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc4b65e944632b832dfd396882501873.svg
      fullname: StevenLyp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StevenLyp
      type: user
    createdAt: '2023-10-24T07:55:58.000Z'
    data:
      edited: false
      editors:
      - StevenLyp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23394368588924408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc4b65e944632b832dfd396882501873.svg
          fullname: StevenLyp
          isHf: false
          isPro: false
          name: StevenLyp
          type: user
        html: "<p>class XGen(LLM):<br>    max_token: int = 8192<br>    temperature:\
          \ float = 0.8<br>    top_p = 0.9<br>    tokenizer: object = None<br>   \
          \ model: object = None</p>\n<pre><code>def __init__(self):\n    super().__init__()\n\
          \    \n@property\ndef _llm_type(self) -&gt; str:\n    return \"XGen\"\n\
          \        \ndef load_model(self, llm_device=\"gpu\", model_name_or_path=None):\n\
          \    self.model, self.tokenizer = load_model(model_name_or_path, 'cuda',\
          \  1)  # \u9009\u62E9\u4F7F\u7528\u8BAD\u7EC3\u7684GPU)\n    print('xgen\
          \ model load finished')\n\ndef _call(self,prompt:str, stop: Optional[List[str]]\
          \ = None):\n    input_ids = self.tokenizer([prompt]).input_ids\n    \n \
          \   output_ids = self.model.generate(\n                torch.as_tensor(input_ids).to('cuda'),\
          \ temperature=self.temperature,max_new_tokens=self.max_token\n         \
          \       ) \n    output_ids = output_ids[0][len(input_ids[0]) :]\n    response\
          \ = self.tokenizer.decode(\n        output_ids, skip_special_tokens=True,\
          \ spaces_between_special_tokens=False\n    )\n    \n    return response.replace('\\\
          \\', '').split('\\n')[0]\n</code></pre>\n<p>os.environ[\"CUDA_VISIBLE_DEVICES\"\
          ] = use_gpu(0.1)<br>import logging</p>\n<p>model_path = r\"/data/package/lyp/model/xgen_7b_8k_inst\"\
          <br>llm = XGen()<br>llm.load_model(model_name_or_path = model_path)</p>\n\
          <p>Error mssage\uFF1A<br>Loading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 3/3 [00:08&lt;00:00,  2.88s/it]<br>Traceback (most recent call last):<br>\
          \  File \"/data/package/lyp/model/xgen_7b_8k_inst/test.py\", line 74, in\
          \ <br>    llm.load_model(model_name_or_path = model_path)<br>  File \"/data/package/lyp/model/xgen_7b_8k_inst/test.py\"\
          , line 52, in load_model<br>    self.model, self.tokenizer = load_model(model_name_or_path,\
          \ 'cuda',  1)  # \u9009\u62E9\u4F7F\u7528\u8BAD\u7EC3\u7684GPU)<br>  File\
          \ \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
          , line 288, in load_model<br>    model, tokenizer = adapter.load_model(model_path,\
          \ kwargs)<br>  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
          , line 1201, in load_model<br>    tokenizer = AutoTokenizer.from_pretrained(<br>\
          \  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 738, in from_pretrained<br>    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)<br>  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2045, in from_pretrained<br>    return cls._from_pretrained(<br>\
          \  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2256, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 137, in <strong>init</strong><br>    super().<strong>init</strong>(<br>\
          \  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
          , line 366, in <strong>init</strong><br>    self._add_tokens(self.all_special_tokens_extended,\
          \ special_tokens=True)<br>  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
          , line 462, in _add_tokens<br>    current_vocab = self.get_vocab().copy()<br>\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 154, in get_vocab<br>    vocab = {self.encoder.decode_single_token_bytes(i):\
          \ i for i in range(self.vocab_size)}<br>  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 150, in vocab_size<br>    return self.encoder.n_vocab<br>AttributeError:\
          \ 'XgenTokenizer' object has no attribute 'encoder'</p>\n"
        raw: "class XGen(LLM):\r\n    max_token: int = 8192\r\n    temperature: float\
          \ = 0.8\r\n    top_p = 0.9\r\n    tokenizer: object = None\r\n    model:\
          \ object = None\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\
          \n        \r\n    @property\r\n    def _llm_type(self) -> str:\r\n     \
          \   return \"XGen\"\r\n            \r\n    def load_model(self, llm_device=\"\
          gpu\", model_name_or_path=None):\r\n        self.model, self.tokenizer =\
          \ load_model(model_name_or_path, 'cuda',  1)  # \u9009\u62E9\u4F7F\u7528\
          \u8BAD\u7EC3\u7684GPU)\r\n        print('xgen model load finished')\r\n\r\
          \n    def _call(self,prompt:str, stop: Optional[List[str]] = None):\r\n\
          \        input_ids = self.tokenizer([prompt]).input_ids\r\n        \r\n\
          \        output_ids = self.model.generate(\r\n                    torch.as_tensor(input_ids).to('cuda'),\
          \ temperature=self.temperature,max_new_tokens=self.max_token\r\n       \
          \             ) \r\n        output_ids = output_ids[0][len(input_ids[0])\
          \ :]\r\n        response = self.tokenizer.decode(\r\n            output_ids,\
          \ skip_special_tokens=True, spaces_between_special_tokens=False\r\n    \
          \    )\r\n        \r\n        return response.replace('\\\\', '').split('\\\
          n')[0]\r\n\r\n    \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = use_gpu(0.1)\r\
          \nimport logging\r\n\r\nmodel_path = r\"/data/package/lyp/model/xgen_7b_8k_inst\"\
          \r\nllm = XGen()\r\nllm.load_model(model_name_or_path = model_path)\r\n\r\
          \n\r\nError mssage\uFF1A\r\nLoading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 3/3 [00:08<00:00,  2.88s/it]\r\nTraceback (most recent call\
          \ last):\r\n  File \"/data/package/lyp/model/xgen_7b_8k_inst/test.py\",\
          \ line 74, in <module>\r\n    llm.load_model(model_name_or_path = model_path)\r\
          \n  File \"/data/package/lyp/model/xgen_7b_8k_inst/test.py\", line 52, in\
          \ load_model\r\n    self.model, self.tokenizer = load_model(model_name_or_path,\
          \ 'cuda',  1)  # \u9009\u62E9\u4F7F\u7528\u8BAD\u7EC3\u7684GPU)\r\n  File\
          \ \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
          , line 288, in load_model\r\n    model, tokenizer = adapter.load_model(model_path,\
          \ kwargs)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
          , line 1201, in load_model\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\
          \n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 738, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2045, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2256, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 137, in __init__\r\n    super().__init__(\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
          , line 366, in __init__\r\n    self._add_tokens(self.all_special_tokens_extended,\
          \ special_tokens=True)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
          , line 462, in _add_tokens\r\n    current_vocab = self.get_vocab().copy()\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 154, in get_vocab\r\n    vocab = {self.encoder.decode_single_token_bytes(i):\
          \ i for i in range(self.vocab_size)}\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
          , line 150, in vocab_size\r\n    return self.encoder.n_vocab\r\nAttributeError:\
          \ 'XgenTokenizer' object has no attribute 'encoder'"
        updatedAt: '2023-10-24T07:55:58.444Z'
      numEdits: 0
      reactions: []
    id: 6537788e605a07338d09121c
    type: comment
  author: StevenLyp
  content: "class XGen(LLM):\r\n    max_token: int = 8192\r\n    temperature: float\
    \ = 0.8\r\n    top_p = 0.9\r\n    tokenizer: object = None\r\n    model: object\
    \ = None\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n\
    \        \r\n    @property\r\n    def _llm_type(self) -> str:\r\n        return\
    \ \"XGen\"\r\n            \r\n    def load_model(self, llm_device=\"gpu\", model_name_or_path=None):\r\
    \n        self.model, self.tokenizer = load_model(model_name_or_path, 'cuda',\
    \  1)  # \u9009\u62E9\u4F7F\u7528\u8BAD\u7EC3\u7684GPU)\r\n        print('xgen\
    \ model load finished')\r\n\r\n    def _call(self,prompt:str, stop: Optional[List[str]]\
    \ = None):\r\n        input_ids = self.tokenizer([prompt]).input_ids\r\n     \
    \   \r\n        output_ids = self.model.generate(\r\n                    torch.as_tensor(input_ids).to('cuda'),\
    \ temperature=self.temperature,max_new_tokens=self.max_token\r\n             \
    \       ) \r\n        output_ids = output_ids[0][len(input_ids[0]) :]\r\n    \
    \    response = self.tokenizer.decode(\r\n            output_ids, skip_special_tokens=True,\
    \ spaces_between_special_tokens=False\r\n        )\r\n        \r\n        return\
    \ response.replace('\\\\', '').split('\\n')[0]\r\n\r\n    \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
    ] = use_gpu(0.1)\r\nimport logging\r\n\r\nmodel_path = r\"/data/package/lyp/model/xgen_7b_8k_inst\"\
    \r\nllm = XGen()\r\nllm.load_model(model_name_or_path = model_path)\r\n\r\n\r\n\
    Error mssage\uFF1A\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
    \ [00:08<00:00,  2.88s/it]\r\nTraceback (most recent call last):\r\n  File \"\
    /data/package/lyp/model/xgen_7b_8k_inst/test.py\", line 74, in <module>\r\n  \
    \  llm.load_model(model_name_or_path = model_path)\r\n  File \"/data/package/lyp/model/xgen_7b_8k_inst/test.py\"\
    , line 52, in load_model\r\n    self.model, self.tokenizer = load_model(model_name_or_path,\
    \ 'cuda',  1)  # \u9009\u62E9\u4F7F\u7528\u8BAD\u7EC3\u7684GPU)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
    , line 288, in load_model\r\n    model, tokenizer = adapter.load_model(model_path,\
    \ kwargs)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/fastchat/model/model_adapter.py\"\
    , line 1201, in load_model\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\
    \n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 738, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2045, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2256, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
    , line 137, in __init__\r\n    super().__init__(\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
    , line 366, in __init__\r\n    self._add_tokens(self.all_special_tokens_extended,\
    \ special_tokens=True)\r\n  File \"/root/anaconda3/envs/fastchat/lib/python3.9/site-packages/transformers/tokenization_utils.py\"\
    , line 462, in _add_tokens\r\n    current_vocab = self.get_vocab().copy()\r\n\
    \  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
    , line 154, in get_vocab\r\n    vocab = {self.encoder.decode_single_token_bytes(i):\
    \ i for i in range(self.vocab_size)}\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/xgen_7b_8k_inst/tokenization_xgen.py\"\
    , line 150, in vocab_size\r\n    return self.encoder.n_vocab\r\nAttributeError:\
    \ 'XgenTokenizer' object has no attribute 'encoder'"
  created_at: 2023-10-24 06:55:58+00:00
  edited: false
  hidden: false
  id: 6537788e605a07338d09121c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
      fullname: Tian Xie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tianxie-sf
      type: user
    createdAt: '2023-10-24T17:39:29.000Z'
    data:
      edited: false
      editors:
      - tianxie-sf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8172281980514526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24574113bfe000250196e935e4f7cde2.svg
          fullname: Tian Xie
          isHf: false
          isPro: false
          name: tianxie-sf
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;StevenLyp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/StevenLyp\"\
          >@<span class=\"underline\">StevenLyp</span></a></span>\n\n\t</span></span>,\
          \ please clear the hf cache, and retry.</p>\n"
        raw: Hi @StevenLyp, please clear the hf cache, and retry.
        updatedAt: '2023-10-24T17:39:29.387Z'
      numEdits: 0
      reactions: []
    id: 65380151c8b0cccf1f227202
    type: comment
  author: tianxie-sf
  content: Hi @StevenLyp, please clear the hf cache, and retry.
  created_at: 2023-10-24 16:39:29+00:00
  edited: false
  hidden: false
  id: 65380151c8b0cccf1f227202
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: Salesforce/xgen-7b-8k-inst
repo_type: model
status: open
target_branch: null
title: 'AttributeError: ''XgenTokenizer'' object has no attribute ''encoder'''
