!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2023-12-21 16:09:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-21T16:09:36.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9409924745559692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Have you considered training this on the Yi 200K base instead of
          the 4K model?</p>

          <p>Seems like this would be much better for storytelling, especially since
          your dataset naturally contains very long passages.</p>

          <p>And FYI long context training is quite doable on a single A100,  or even
          a 48GB GPU, especially if you use UnSloth to train. It doesn''t have to
          be near 200K to get good long context performance. </p>

          '
        raw: 'Have you considered training this on the Yi 200K base instead of the
          4K model?


          Seems like this would be much better for storytelling, especially since
          your dataset naturally contains very long passages.


          And FYI long context training is quite doable on a single A100,  or even
          a 48GB GPU, especially if you use UnSloth to train. It doesn''t have to
          be near 200K to get good long context performance. '
        updatedAt: '2023-12-21T16:10:40.543Z'
      numEdits: 2
      reactions: []
    id: 65846340cce29a06216a5a2b
    type: comment
  author: brucethemoose
  content: 'Have you considered training this on the Yi 200K base instead of the 4K
    model?


    Seems like this would be much better for storytelling, especially since your dataset
    naturally contains very long passages.


    And FYI long context training is quite doable on a single A100,  or even a 48GB
    GPU, especially if you use UnSloth to train. It doesn''t have to be near 200K
    to get good long context performance. '
  created_at: 2023-12-21 16:09:36+00:00
  edited: true
  hidden: false
  id: 65846340cce29a06216a5a2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
      fullname: triad party
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TriadParty
      type: user
    createdAt: '2023-12-22T05:50:41.000Z'
    data:
      edited: false
      editors:
      - TriadParty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750992059707642
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
          fullname: triad party
          isHf: false
          isPro: false
          name: TriadParty
          type: user
        html: '<p>Well, to be honest, oom is the main enemy stopping me from doing
          qlora on the 200k version. I just took a look at UnSloth, and I feel that
          this is indeed a good project that I need.So, I know what I want to do next.</p>

          '
        raw: Well, to be honest, oom is the main enemy stopping me from doing qlora
          on the 200k version. I just took a look at UnSloth, and I feel that this
          is indeed a good project that I need.So, I know what I want to do next.
        updatedAt: '2023-12-22T05:50:41.662Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 658523b1bd67a029295fc11d
    type: comment
  author: TriadParty
  content: Well, to be honest, oom is the main enemy stopping me from doing qlora
    on the 200k version. I just took a look at UnSloth, and I feel that this is indeed
    a good project that I need.So, I know what I want to do next.
  created_at: 2023-12-22 05:50:41+00:00
  edited: false
  hidden: false
  id: 658523b1bd67a029295fc11d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-22T07:21:34.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9019536972045898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Again, you can just change the context size in the config and train
          on whatever context size you can fit, then set it back, and the model will
          still largely retain 200K performance.</p>

          '
        raw: Again, you can just change the context size in the config and train on
          whatever context size you can fit, then set it back, and the model will
          still largely retain 200K performance.
        updatedAt: '2023-12-22T07:21:34.831Z'
      numEdits: 0
      reactions: []
    id: 658538fef5358611c0965606
    type: comment
  author: brucethemoose
  content: Again, you can just change the context size in the config and train on
    whatever context size you can fit, then set it back, and the model will still
    largely retain 200K performance.
  created_at: 2023-12-22 07:21:34+00:00
  edited: false
  hidden: false
  id: 658538fef5358611c0965606
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TriadParty/Deepsword-34B-Base
repo_type: model
status: open
target_branch: null
title: 200K Version?
