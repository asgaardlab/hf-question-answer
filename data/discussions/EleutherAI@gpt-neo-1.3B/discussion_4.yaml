!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Qwinpin
conflicting_files: null
created_at: 2023-02-14 10:32:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1332acefcabdf0f151e5de46ab64a6c9.svg
      fullname: Q W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Qwinpin
      type: user
    createdAt: '2023-02-14T10:32:10.000Z'
    data:
      edited: false
      editors:
      - Qwinpin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1332acefcabdf0f151e5de46ab64a6c9.svg
          fullname: Q W
          isHf: false
          isPro: false
          name: Qwinpin
          type: user
        html: '<p>Using GPT-Neo-1.3B from transformers interface I noticed that second
          (but not subsequent) invocation of the inference results in additional memory
          allocation.<br>I use <code>with torch.no_grad()</code> context manager and
          don''t understand why GPU memory consumption increasing after the first
          call of the model even with the same input data, so input tensors have the
          same size in all dimensions.</p>

          '
        raw: "Using GPT-Neo-1.3B from transformers interface I noticed that second\
          \ (but not subsequent) invocation of the inference results in additional\
          \ memory allocation.\r\nI use ```with torch.no_grad()``` context manager\
          \ and don't understand why GPU memory consumption increasing after the first\
          \ call of the model even with the same input data, so input tensors have\
          \ the same size in all dimensions."
        updatedAt: '2023-02-14T10:32:10.594Z'
      numEdits: 0
      reactions: []
    id: 63eb632a13a3eb9b0dc6b9db
    type: comment
  author: Qwinpin
  content: "Using GPT-Neo-1.3B from transformers interface I noticed that second (but\
    \ not subsequent) invocation of the inference results in additional memory allocation.\r\
    \nI use ```with torch.no_grad()``` context manager and don't understand why GPU\
    \ memory consumption increasing after the first call of the model even with the\
    \ same input data, so input tensors have the same size in all dimensions."
  created_at: 2023-02-14 10:32:10+00:00
  edited: false
  hidden: false
  id: 63eb632a13a3eb9b0dc6b9db
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: EleutherAI/gpt-neo-1.3B
repo_type: model
status: open
target_branch: null
title: GPU memory consumption
