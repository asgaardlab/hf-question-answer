!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2024-01-21 13:29:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2024-01-21T13:29:47.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578536748886108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>HI,</p>

          <p>I am not sure if you have tried transformer inference or not, but it
          seems internlm2 would not work properly under bitsandbytes 4 bit quantization.
          It will constantly spit out self Q&amp;As without stopping. </p>

          <p>Also accerlerate QLoRA would also not work on internlm2 with error on
          some tensor has no grad.</p>

          <p>float16 would work in all cases.</p>

          <p>It is not a big deal since internlm2 mainly support lmdeploy framework
          with its own 4 bit quantization instead of transformer.</p>

          <p>Thanks!</p>

          '
        raw: "HI,\r\n\r\nI am not sure if you have tried transformer inference or\
          \ not, but it seems internlm2 would not work properly under bitsandbytes\
          \ 4 bit quantization. It will constantly spit out self Q&As without stopping.\
          \ \r\n\r\nAlso accerlerate QLoRA would also not work on internlm2 with error\
          \ on some tensor has no grad.\r\n\r\nfloat16 would work in all cases.\r\n\
          \r\nIt is not a big deal since internlm2 mainly support lmdeploy framework\
          \ with its own 4 bit quantization instead of transformer.\r\n\r\nThanks!"
        updatedAt: '2024-01-21T13:29:47.528Z'
      numEdits: 0
      reactions: []
    id: 65ad1c4bae773a8112b18ace
    type: comment
  author: Yhyu13
  content: "HI,\r\n\r\nI am not sure if you have tried transformer inference or not,\
    \ but it seems internlm2 would not work properly under bitsandbytes 4 bit quantization.\
    \ It will constantly spit out self Q&As without stopping. \r\n\r\nAlso accerlerate\
    \ QLoRA would also not work on internlm2 with error on some tensor has no grad.\r\
    \n\r\nfloat16 would work in all cases.\r\n\r\nIt is not a big deal since internlm2\
    \ mainly support lmdeploy framework with its own 4 bit quantization instead of\
    \ transformer.\r\n\r\nThanks!"
  created_at: 2024-01-21 13:29:47+00:00
  edited: false
  hidden: false
  id: 65ad1c4bae773a8112b18ace
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18958b8406d1ce492b54c1c839f18c54.svg
      fullname: Wenwei Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ZwwWayne
      type: user
    createdAt: '2024-01-24T08:07:32.000Z'
    data:
      edited: false
      editors:
      - ZwwWayne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9249570369720459
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18958b8406d1ce492b54c1c839f18c54.svg
          fullname: Wenwei Zhang
          isHf: false
          isPro: false
          name: ZwwWayne
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> ,<br>Could you\
          \ please refer to this doc <a rel=\"nofollow\" href=\"https://github.com/InternLM/InternLM/pull/636\"\
          >https://github.com/InternLM/InternLM/pull/636</a> and see if it solves\
          \ your issue?</p>\n"
        raw: 'Hi @Yhyu13 ,

          Could you please refer to this doc https://github.com/InternLM/InternLM/pull/636
          and see if it solves your issue?'
        updatedAt: '2024-01-24T08:07:32.404Z'
      numEdits: 0
      reactions: []
    id: 65b0c5441dbd85fd0c3310c1
    type: comment
  author: ZwwWayne
  content: 'Hi @Yhyu13 ,

    Could you please refer to this doc https://github.com/InternLM/InternLM/pull/636
    and see if it solves your issue?'
  created_at: 2024-01-24 08:07:32+00:00
  edited: false
  hidden: false
  id: 65b0c5441dbd85fd0c3310c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: internlm/internlm2-chat-7b
repo_type: model
status: open
target_branch: null
title: Transformer bitsandbytes 4 bit quant does not work well. QLoRA also fails
