!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepgoyal19
conflicting_files: null
created_at: 2023-05-25 20:04:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
      fullname: Deepanshu Goyal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepgoyal19
      type: user
    createdAt: '2023-05-25T21:04:45.000Z'
    data:
      edited: false
      editors:
      - deepgoyal19
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
          fullname: Deepanshu Goyal
          isHf: false
          isPro: false
          name: deepgoyal19
          type: user
        html: '<p>I built my own lora model and a base model. I have connected my
          lora model with my base model but still i''m not getting the desired results.
          The images generated by my lora models comes out be as same as the images
          generated by my base model. Can you please help?</p>

          <p><a href="https://huggingface.co/deepgoyal19/lora15">https://huggingface.co/deepgoyal19/lora15</a></p>

          <p><a href="https://huggingface.co/deepgoyal19/mysd1">https://huggingface.co/deepgoyal19/mysd1</a></p>

          '
        raw: "I built my own lora model and a base model. I have connected my lora\
          \ model with my base model but still i'm not getting the desired results.\
          \ The images generated by my lora models comes out be as same as the images\
          \ generated by my base model. Can you please help?\r\n\r\nhttps://huggingface.co/deepgoyal19/lora15\r\
          \n\r\nhttps://huggingface.co/deepgoyal19/mysd1"
        updatedAt: '2023-05-25T21:04:45.496Z'
      numEdits: 0
      reactions: []
    id: 646fcd6d850a938d6c5507f3
    type: comment
  author: deepgoyal19
  content: "I built my own lora model and a base model. I have connected my lora model\
    \ with my base model but still i'm not getting the desired results. The images\
    \ generated by my lora models comes out be as same as the images generated by\
    \ my base model. Can you please help?\r\n\r\nhttps://huggingface.co/deepgoyal19/lora15\r\
    \n\r\nhttps://huggingface.co/deepgoyal19/mysd1"
  created_at: 2023-05-25 20:04:45+00:00
  edited: false
  hidden: false
  id: 646fcd6d850a938d6c5507f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68bf1dd49dd92e724e8eab341ea61df8.svg
      fullname: Song
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: miluELK
      type: user
    createdAt: '2023-05-31T09:59:39.000Z'
    data:
      edited: false
      editors:
      - miluELK
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68bf1dd49dd92e724e8eab341ea61df8.svg
          fullname: Song
          isHf: false
          isPro: false
          name: miluELK
          type: user
        html: '<p>I''m so sorry I haven''t check my community recently.I''ve just
          use your models to generate 2 images in the same parameters,your work is
          very effective.The image of LoRA looks better.Perhaps there is a error in
          your generation code,here''s my code:</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span>
          DiffusionPipeline


          pipeline = DiffusionPipeline.from_pretrained(<span class="hljs-string">"deepgoyal19/mysd1"</span>)

          pipeline.to(<span class="hljs-string">"cuda"</span>)

          generator = torch.Generator(<span class="hljs-string">"cuda"</span>).manual_seed(<span
          class="hljs-number">2000</span>)

          prompt = <span class="hljs-string">"cute puppy"</span>

          image = pipeline(prompt, generator=generator, num_inference_steps=<span
          class="hljs-number">50</span>, guidance_scale=<span class="hljs-number">7.5</span>).images[<span
          class="hljs-number">0</span>]

          image

          </code></pre>

          <p>sd1<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6DqotPdnRbo21xLl8qmIV.png"><img
          alt="puppy.png" src="https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6DqotPdnRbo21xLl8qmIV.png"></a></p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline

          <span class="hljs-keyword">import</span> torch


          pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">"deepgoyal19/mysd1"</span>,torch_dtype=torch.float16)

          model_path = <span class="hljs-string">"deepgoyal19/lora15"</span>

          pipe.unet.load_attn_procs(model_path)

          pipe.to(<span class="hljs-string">"cuda"</span>)

          prompt = <span class="hljs-string">"cute puppy"</span>

          generator = torch.Generator(<span class="hljs-string">"cuda"</span>).manual_seed(<span
          class="hljs-number">2000</span>)

          image = pipe(prompt, generator=generator, num_inference_steps=<span class="hljs-number">50</span>,
          guidance_scale=<span class="hljs-number">7.5</span>).images[<span class="hljs-number">0</span>]

          image

          </code></pre>

          <p>lora<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6aVL8XqFJzqYq1nlEqwA8.png"><img
          alt="puppy_lora .png" src="https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6aVL8XqFJzqYq1nlEqwA8.png"></a></p>

          '
        raw: 'I''m so sorry I haven''t check my community recently.I''ve just use
          your models to generate 2 images in the same parameters,your work is very
          effective.The image of LoRA looks better.Perhaps there is a error in your
          generation code,here''s my code:


          ```python

          import torch

          from diffusers import DiffusionPipeline


          pipeline = DiffusionPipeline.from_pretrained("deepgoyal19/mysd1")

          pipeline.to("cuda")

          generator = torch.Generator("cuda").manual_seed(2000)

          prompt = "cute puppy"

          image = pipeline(prompt, generator=generator, num_inference_steps=50, guidance_scale=7.5).images[0]

          image

          ```

          sd1

          ![puppy.png](https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6DqotPdnRbo21xLl8qmIV.png)

          ```python

          from diffusers import StableDiffusionPipeline

          import torch


          pipe = StableDiffusionPipeline.from_pretrained("deepgoyal19/mysd1",torch_dtype=torch.float16)

          model_path = "deepgoyal19/lora15"

          pipe.unet.load_attn_procs(model_path)

          pipe.to("cuda")

          prompt = "cute puppy"

          generator = torch.Generator("cuda").manual_seed(2000)

          image = pipe(prompt, generator=generator, num_inference_steps=50, guidance_scale=7.5).images[0]

          image

          ```

          lora

          ![puppy_lora .png](https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6aVL8XqFJzqYq1nlEqwA8.png)'
        updatedAt: '2023-05-31T09:59:39.284Z'
      numEdits: 0
      reactions: []
    id: 64771a8b40c99df87601f76c
    type: comment
  author: miluELK
  content: 'I''m so sorry I haven''t check my community recently.I''ve just use your
    models to generate 2 images in the same parameters,your work is very effective.The
    image of LoRA looks better.Perhaps there is a error in your generation code,here''s
    my code:


    ```python

    import torch

    from diffusers import DiffusionPipeline


    pipeline = DiffusionPipeline.from_pretrained("deepgoyal19/mysd1")

    pipeline.to("cuda")

    generator = torch.Generator("cuda").manual_seed(2000)

    prompt = "cute puppy"

    image = pipeline(prompt, generator=generator, num_inference_steps=50, guidance_scale=7.5).images[0]

    image

    ```

    sd1

    ![puppy.png](https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6DqotPdnRbo21xLl8qmIV.png)

    ```python

    from diffusers import StableDiffusionPipeline

    import torch


    pipe = StableDiffusionPipeline.from_pretrained("deepgoyal19/mysd1",torch_dtype=torch.float16)

    model_path = "deepgoyal19/lora15"

    pipe.unet.load_attn_procs(model_path)

    pipe.to("cuda")

    prompt = "cute puppy"

    generator = torch.Generator("cuda").manual_seed(2000)

    image = pipe(prompt, generator=generator, num_inference_steps=50, guidance_scale=7.5).images[0]

    image

    ```

    lora

    ![puppy_lora .png](https://cdn-uploads.huggingface.co/production/uploads/6437ccb270cb1a21df86c9ea/6aVL8XqFJzqYq1nlEqwA8.png)'
  created_at: 2023-05-31 08:59:39+00:00
  edited: false
  hidden: false
  id: 64771a8b40c99df87601f76c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
      fullname: Deepanshu Goyal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepgoyal19
      type: user
    createdAt: '2023-05-31T10:15:26.000Z'
    data:
      edited: true
      editors:
      - deepgoyal19
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
          fullname: Deepanshu Goyal
          isHf: false
          isPro: false
          name: deepgoyal19
          type: user
        html: '<p>Thankyou for your reply.</p>

          <p>As you have seen my LORA generated good results but it this is not happening
          on Hugging face inference api.(<a href="https://huggingface.co/deepgoyal19/lora15">https://huggingface.co/deepgoyal19/lora15</a>)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6409877b3e6afa3d8dbc5505/pXPZHrcevC4qW-gC0TY8-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6409877b3e6afa3d8dbc5505/pXPZHrcevC4qW-gC0TY8-.png"></a></p>

          <p>Do you know why?</p>

          '
        raw: 'Thankyou for your reply.


          As you have seen my LORA generated good results but it this is not happening
          on Hugging face inference api.(https://huggingface.co/deepgoyal19/lora15)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6409877b3e6afa3d8dbc5505/pXPZHrcevC4qW-gC0TY8-.png)


          Do you know why?'
        updatedAt: '2023-05-31T10:17:38.524Z'
      numEdits: 2
      reactions: []
    id: 64771e3e906bb0203e508a7a
    type: comment
  author: deepgoyal19
  content: 'Thankyou for your reply.


    As you have seen my LORA generated good results but it this is not happening on
    Hugging face inference api.(https://huggingface.co/deepgoyal19/lora15)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6409877b3e6afa3d8dbc5505/pXPZHrcevC4qW-gC0TY8-.png)


    Do you know why?'
  created_at: 2023-05-31 09:15:26+00:00
  edited: true
  hidden: false
  id: 64771e3e906bb0203e508a7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/68bf1dd49dd92e724e8eab341ea61df8.svg
      fullname: Song
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: miluELK
      type: user
    createdAt: '2023-05-31T14:21:57.000Z'
    data:
      edited: false
      editors:
      - miluELK
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/68bf1dd49dd92e724e8eab341ea61df8.svg
          fullname: Song
          isHf: false
          isPro: false
          name: miluELK
          type: user
        html: "<p>This situation is normal,you will find more imformation in <a rel=\"\
          nofollow\" href=\"https://api-inference.huggingface.co/models/deepgoyal19/lora15\"\
          >https://api-inference.huggingface.co/models/deepgoyal19/lora15</a>,the\
          \ JSON shows that the argument \"id: deepgoyal19/lora15\" and \"base_model:\
          \ deepgoyal19/mysd1\",if you use this model id in the pipeline like the\
          \ following code:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> diffusers <span class=\"hljs-keyword\">import</span>\
          \ DiffusionPipeline\npipeline = DiffusionPipeline.from_pretrained(<span\
          \ class=\"hljs-string\">\"deepgoyal19/lora15\"</span>)\n</code></pre>\n\
          <p>you will find this error:</p>\n<pre><code class=\"language-python\">\
          \ HTTPError: <span class=\"hljs-number\">404</span> Client Error: Not Found\
          \ <span class=\"hljs-keyword\">for</span> url: \nEntry Not Found <span class=\"\
          hljs-keyword\">for</span> url: https://huggingface.co/deepgoyal19/lora15/resolve/main/model_index.json.\n\
          </code></pre>\n<p>because there is no model_index.json or other files for\
          \ pipeline,so it can't find such files.<br>I guess  in order to avoid this\
          \ error while using API,the programmer of HuggingFace just use the base_model\
          \ id for pipeline,the API is using model \"deepgoyal19/mysd1\".</p>\n<p>In\
          \ order to solve this problem, I have two idea:</p>\n<ul>\n<li><ol>\n<li>maybe\
          \ add the model \"deepgoyal19/mysd1\" files into \"deepgoyal19/lora15\"\
          \ repository will solve this problem ,I'm not sure ,you should backups these\
          \ two repositories if you want to take a try.</li>\n</ol>\n</li>\n<li><ol\
          \ start=\"2\">\n<li>the HF API is unreliable,I think they use \"Gradio\"\
          \ and \"FastApi\" lib to bulid it.Maybe custom a API in your Space is a\
          \ better choice,you could find a introduction in this page <a rel=\"nofollow\"\
          \ href=\"https://gradio.app/quickstart/\">Quickstart</a></li>\n</ol>\n</li>\n\
          </ul>\n<p>I hope it will be help.</p>\n"
        raw: "This situation is normal,you will find more imformation in [https://api-inference.huggingface.co/models/deepgoyal19/lora15](https://api-inference.huggingface.co/models/deepgoyal19/lora15),the\
          \ JSON shows that the argument \"id: deepgoyal19/lora15\" and \"base_model:\
          \ deepgoyal19/mysd1\",if you use this model id in the pipeline like the\
          \ following code:\n```python\nfrom diffusers import DiffusionPipeline\n\
          pipeline = DiffusionPipeline.from_pretrained(\"deepgoyal19/lora15\")\n```\n\
          you will find this error:\n```python\n HTTPError: 404 Client Error: Not\
          \ Found for url: \nEntry Not Found for url: https://huggingface.co/deepgoyal19/lora15/resolve/main/model_index.json.\n\
          ```\n\nbecause there is no model_index.json or other files for pipeline,so\
          \ it can't find such files.\nI guess  in order to avoid this error while\
          \ using API,the programmer of HuggingFace just use the base_model id for\
          \ pipeline,the API is using model \"deepgoyal19/mysd1\".\n\nIn order to\
          \ solve this problem, I have two idea:\n- 1) maybe add the model \"deepgoyal19/mysd1\"\
          \ files into \"deepgoyal19/lora15\" repository will solve this problem ,I'm\
          \ not sure ,you should backups these two repositories if you want to take\
          \ a try.\n- 2) the HF API is unreliable,I think they use \"Gradio\" and\
          \ \"FastApi\" lib to bulid it.Maybe custom a API in your Space is a better\
          \ choice,you could find a introduction in this page [Quickstart](https://gradio.app/quickstart/)\n\
          \nI hope it will be help."
        updatedAt: '2023-05-31T14:21:57.660Z'
      numEdits: 0
      reactions: []
    id: 6477580504aa03da2abb7b01
    type: comment
  author: miluELK
  content: "This situation is normal,you will find more imformation in [https://api-inference.huggingface.co/models/deepgoyal19/lora15](https://api-inference.huggingface.co/models/deepgoyal19/lora15),the\
    \ JSON shows that the argument \"id: deepgoyal19/lora15\" and \"base_model: deepgoyal19/mysd1\"\
    ,if you use this model id in the pipeline like the following code:\n```python\n\
    from diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from_pretrained(\"\
    deepgoyal19/lora15\")\n```\nyou will find this error:\n```python\n HTTPError:\
    \ 404 Client Error: Not Found for url: \nEntry Not Found for url: https://huggingface.co/deepgoyal19/lora15/resolve/main/model_index.json.\n\
    ```\n\nbecause there is no model_index.json or other files for pipeline,so it\
    \ can't find such files.\nI guess  in order to avoid this error while using API,the\
    \ programmer of HuggingFace just use the base_model id for pipeline,the API is\
    \ using model \"deepgoyal19/mysd1\".\n\nIn order to solve this problem, I have\
    \ two idea:\n- 1) maybe add the model \"deepgoyal19/mysd1\" files into \"deepgoyal19/lora15\"\
    \ repository will solve this problem ,I'm not sure ,you should backups these two\
    \ repositories if you want to take a try.\n- 2) the HF API is unreliable,I think\
    \ they use \"Gradio\" and \"FastApi\" lib to bulid it.Maybe custom a API in your\
    \ Space is a better choice,you could find a introduction in this page [Quickstart](https://gradio.app/quickstart/)\n\
    \nI hope it will be help."
  created_at: 2023-05-31 13:21:57+00:00
  edited: false
  hidden: false
  id: 6477580504aa03da2abb7b01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
      fullname: Deepanshu Goyal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepgoyal19
      type: user
    createdAt: '2023-05-31T16:48:19.000Z'
    data:
      edited: false
      editors:
      - deepgoyal19
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6c6c4a6792463f01823723b2bc9dff0.svg
          fullname: Deepanshu Goyal
          isHf: false
          isPro: false
          name: deepgoyal19
          type: user
        html: '<p>I tried your idea no.1 and it''s not working.<br>I know that I can
          make my custom space but HF API is really fast so I did want to do this
          with that.</p>

          <p>I am still not able to figure out why only my model is not working. I
          can see everyone''s LORA model working on hub.</p>

          <p>Thankyou again!</p>

          '
        raw: 'I tried your idea no.1 and it''s not working.

          I know that I can make my custom space but HF API is really fast so I did
          want to do this with that.


          I am still not able to figure out why only my model is not working. I can
          see everyone''s LORA model working on hub.


          Thankyou again!'
        updatedAt: '2023-05-31T16:48:19.677Z'
      numEdits: 0
      reactions: []
    id: 64777a5304aa03da2abec2f0
    type: comment
  author: deepgoyal19
  content: 'I tried your idea no.1 and it''s not working.

    I know that I can make my custom space but HF API is really fast so I did want
    to do this with that.


    I am still not able to figure out why only my model is not working. I can see
    everyone''s LORA model working on hub.


    Thankyou again!'
  created_at: 2023-05-31 15:48:19+00:00
  edited: false
  hidden: false
  id: 64777a5304aa03da2abec2f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: miluELK/Taiyi-sd-pokemon-LoRA-zh-512-v2
repo_type: model
status: open
target_branch: null
title: How to built a lora model
