!!python/object:huggingface_hub.community.DiscussionWithDetails
author: stormchaser
conflicting_files: null
created_at: 2023-08-20 18:09:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
      fullname: abubakar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stormchaser
      type: user
    createdAt: '2023-08-20T19:09:49.000Z'
    data:
      edited: false
      editors:
      - stormchaser
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9771798849105835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
          fullname: abubakar
          isHf: false
          isPro: false
          name: stormchaser
          type: user
        html: '<p>maybe a noob question but will it work with llama.cpp? will download
          this tomorrow and try.</p>

          '
        raw: maybe a noob question but will it work with llama.cpp? will download
          this tomorrow and try.
        updatedAt: '2023-08-20T19:09:49.736Z'
      numEdits: 0
      reactions: []
    id: 64e264fdc21dd0c666d320bb
    type: comment
  author: stormchaser
  content: maybe a noob question but will it work with llama.cpp? will download this
    tomorrow and try.
  created_at: 2023-08-20 18:09:49+00:00
  edited: false
  hidden: false
  id: 64e264fdc21dd0c666d320bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620109777572-5ec0c827ed25d76864d553f5.png?w=200&h=200&f=face
      fullname: reshinth.adith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: reshinthadith
      type: user
    createdAt: '2023-08-23T06:39:42.000Z'
    data:
      edited: false
      editors:
      - reshinthadith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438852071762085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620109777572-5ec0c827ed25d76864d553f5.png?w=200&h=200&f=face
          fullname: reshinth.adith
          isHf: false
          isPro: false
          name: reshinthadith
          type: user
        html: '<p>I don''t think so, you''ll have to integrate this model with <code>llama.cpp</code>.  But
          this model is compatible with <a rel="nofollow" href="https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox">GGML</a>
          which is an alternative to this.</p>

          '
        raw: I don't think so, you'll have to integrate this model with `llama.cpp`.  But
          this model is compatible with [GGML](https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox)
          which is an alternative to this.
        updatedAt: '2023-08-23T06:39:42.839Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64e5a9aea9a5eabaa6fadd25
    id: 64e5a9aea9a5eabaa6fadd22
    type: comment
  author: reshinthadith
  content: I don't think so, you'll have to integrate this model with `llama.cpp`.  But
    this model is compatible with [GGML](https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox)
    which is an alternative to this.
  created_at: 2023-08-23 05:39:42+00:00
  edited: false
  hidden: false
  id: 64e5a9aea9a5eabaa6fadd22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620109777572-5ec0c827ed25d76864d553f5.png?w=200&h=200&f=face
      fullname: reshinth.adith
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: reshinthadith
      type: user
    createdAt: '2023-08-23T06:39:42.000Z'
    data:
      status: closed
    id: 64e5a9aea9a5eabaa6fadd25
    type: status-change
  author: reshinthadith
  created_at: 2023-08-23 05:39:42+00:00
  id: 64e5a9aea9a5eabaa6fadd25
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
      fullname: abubakar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stormchaser
      type: user
    createdAt: '2023-08-23T09:43:17.000Z'
    data:
      edited: false
      editors:
      - stormchaser
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8875459432601929
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
          fullname: abubakar
          isHf: false
          isPro: false
          name: stormchaser
          type: user
        html: '<p>yes I tried and it does, i do run models with llama.cpp like the
          llama 2 7B chat ggml, but I what I dont get is if this is compatible with
          ggml then why wont it run. And I would really appreciate if someone can
          guide on how to convert this to llama.cpp compatible format on my laptop.</p>

          '
        raw: yes I tried and it does, i do run models with llama.cpp like the llama
          2 7B chat ggml, but I what I dont get is if this is compatible with ggml
          then why wont it run. And I would really appreciate if someone can guide
          on how to convert this to llama.cpp compatible format on my laptop.
        updatedAt: '2023-08-23T09:43:17.511Z'
      numEdits: 0
      reactions: []
    id: 64e5d4b50dc1e9ef6b4746bb
    type: comment
  author: stormchaser
  content: yes I tried and it does, i do run models with llama.cpp like the llama
    2 7B chat ggml, but I what I dont get is if this is compatible with ggml then
    why wont it run. And I would really appreciate if someone can guide on how to
    convert this to llama.cpp compatible format on my laptop.
  created_at: 2023-08-23 08:43:17+00:00
  edited: false
  hidden: false
  id: 64e5d4b50dc1e9ef6b4746bb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: stabilityai/stablecode-completion-alpha-3b
repo_type: model
status: closed
target_branch: null
title: will it work with llama.cpp?
