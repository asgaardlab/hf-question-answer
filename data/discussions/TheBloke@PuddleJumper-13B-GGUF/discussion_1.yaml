!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-08-23 21:36:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-23T22:36:48.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981482207775116
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>First guff ...nice</p>

          <p>Why did you added old q5?<br>q5 Is even worse than q5k_s.</p>

          '
        raw: "First guff ...nice\r\n\r\nWhy did you added old q5?   \r\nq5 Is even\
          \ worse than q5k_s."
        updatedAt: '2023-08-23T22:36:48.065Z'
      numEdits: 0
      reactions: []
    id: 64e68a00032d13c348c0c199
    type: comment
  author: mirek190
  content: "First guff ...nice\r\n\r\nWhy did you added old q5?   \r\nq5 Is even worse\
    \ than q5k_s."
  created_at: 2023-08-23 21:36:48+00:00
  edited: false
  hidden: false
  id: 64e68a00032d13c348c0c199
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T22:38:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778949022293091
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Someone asked me to, saying it performs better (in terms of speed)
          than the new k-quants on their old hardware</p>

          '
        raw: Someone asked me to, saying it performs better (in terms of speed) than
          the new k-quants on their old hardware
        updatedAt: '2023-08-23T22:38:17.689Z'
      numEdits: 0
      reactions: []
    id: 64e68a59d82128b8d575efc7
    type: comment
  author: TheBloke
  content: Someone asked me to, saying it performs better (in terms of speed) than
    the new k-quants on their old hardware
  created_at: 2023-08-23 21:38:17+00:00
  edited: false
  hidden: false
  id: 64e68a59d82128b8d575efc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-23T22:40:54.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9768844246864319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>ok ... just inform ...  even q4k_m has lower perplexity than old
          q5... and q4k_m is much faster than old q5.</p>

          '
        raw: ok ... just inform ...  even q4k_m has lower perplexity than old q5...
          and q4k_m is much faster than old q5.
        updatedAt: '2023-08-23T22:40:54.162Z'
      numEdits: 0
      reactions: []
    id: 64e68af6032d13c348c0e383
    type: comment
  author: mirek190
  content: ok ... just inform ...  even q4k_m has lower perplexity than old q5...
    and q4k_m is much faster than old q5.
  created_at: 2023-08-23 21:40:54+00:00
  edited: false
  hidden: false
  id: 64e68af6032d13c348c0e383
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T22:41:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6359180212020874
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/2707#issuecomment-1690316359">https://github.com/ggerganov/llama.cpp/pull/2707#issuecomment-1690316359</a></p>

          '
        raw: https://github.com/ggerganov/llama.cpp/pull/2707#issuecomment-1690316359
        updatedAt: '2023-08-23T22:41:40.529Z'
      numEdits: 0
      reactions: []
    id: 64e68b24a6bab0d55915d1d7
    type: comment
  author: TheBloke
  content: https://github.com/ggerganov/llama.cpp/pull/2707#issuecomment-1690316359
  created_at: 2023-08-23 21:41:40+00:00
  edited: false
  hidden: false
  id: 64e68b24a6bab0d55915d1d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-23T22:48:56.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9930844902992249
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I see .. he was comparing old Q5 to Q5_K_S but not to Q4_K_M ....
          which is faster than old Q5 and has lower perplexity.  </p>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1508">https://github.com/ggerganov/llama.cpp/pull/1508</a></p>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1684">https://github.com/ggerganov/llama.cpp/pull/1684</a></p>

          '
        raw: "I see .. he was comparing old Q5 to Q5_K_S but not to Q4_K_M .... which\
          \ is faster than old Q5 and has lower perplexity.  \n\n\nhttps://github.com/ggerganov/llama.cpp/pull/1508\n\
          \nhttps://github.com/ggerganov/llama.cpp/pull/1684"
        updatedAt: '2023-08-23T22:55:06.393Z'
      numEdits: 1
      reactions: []
    id: 64e68cd8372a6d52391869f6
    type: comment
  author: mirek190
  content: "I see .. he was comparing old Q5 to Q5_K_S but not to Q4_K_M .... which\
    \ is faster than old Q5 and has lower perplexity.  \n\n\nhttps://github.com/ggerganov/llama.cpp/pull/1508\n\
    \nhttps://github.com/ggerganov/llama.cpp/pull/1684"
  created_at: 2023-08-23 21:48:56+00:00
  edited: true
  hidden: false
  id: 64e68cd8372a6d52391869f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b13c51206b87161e63ccd7b544b92f3e.svg
      fullname: BASPI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sAMBASPO
      type: user
    createdAt: '2023-08-24T18:48:32.000Z'
    data:
      edited: true
      editors:
      - sAMBASPO
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9449191689491272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b13c51206b87161e63ccd7b544b92f3e.svg
          fullname: BASPI
          isHf: false
          isPro: false
          name: sAMBASPO
          type: user
        html: '<p>just a silly question from a newbie probably best suited for reddit:i
          am following the development of new compressed formats since the beginning
          mostly ggml formats because i have only cpu+ram power and i totally embrace
          them..but with these new integrated metadata do you think it will be possible
          for this new gguf format to be retrocompatible with older versions?The absence
          of retrocompatibility in ggml versions  were because lama.cpp wasn''t able
          to recognize the older versions or was a deliberate choice?I often return
          using older vicuna versions for nostalgia valour (they were the first to
          surprise me so much) and i switch between lama.cpp versions.Will you convert
          Gguf versions of  other previous models in your list  with good score on
          the leaderboard or you will go from now on with just the new ones?. Thanks
          in advance for your work</p>

          '
        raw: just a silly question from a newbie probably best suited for reddit:i
          am following the development of new compressed formats since the beginning
          mostly ggml formats because i have only cpu+ram power and i totally embrace
          them..but with these new integrated metadata do you think it will be possible
          for this new gguf format to be retrocompatible with older versions?The absence
          of retrocompatibility in ggml versions  were because lama.cpp wasn't able
          to recognize the older versions or was a deliberate choice?I often return
          using older vicuna versions for nostalgia valour (they were the first to
          surprise me so much) and i switch between lama.cpp versions.Will you convert
          Gguf versions of  other previous models in your list  with good score on
          the leaderboard or you will go from now on with just the new ones?. Thanks
          in advance for your work
        updatedAt: '2023-08-24T18:58:15.757Z'
      numEdits: 1
      reactions: []
    id: 64e7a6003b9c1e300e195c19
    type: comment
  author: sAMBASPO
  content: just a silly question from a newbie probably best suited for reddit:i am
    following the development of new compressed formats since the beginning mostly
    ggml formats because i have only cpu+ram power and i totally embrace them..but
    with these new integrated metadata do you think it will be possible for this new
    gguf format to be retrocompatible with older versions?The absence of retrocompatibility
    in ggml versions  were because lama.cpp wasn't able to recognize the older versions
    or was a deliberate choice?I often return using older vicuna versions for nostalgia
    valour (they were the first to surprise me so much) and i switch between lama.cpp
    versions.Will you convert Gguf versions of  other previous models in your list  with
    good score on the leaderboard or you will go from now on with just the new ones?.
    Thanks in advance for your work
  created_at: 2023-08-24 17:48:32+00:00
  edited: true
  hidden: false
  id: 64e7a6003b9c1e300e195c19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-24T20:36:39.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517600536346436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>You can always convert older models to the newer ones by itself.<br>I
          do not see a problem.<br>Apart from that if I know The Bloke will convert
          all his older models to the new gguf format anyway. </p>

          <p>Apart from that ... what a problem to keep old llama.exe version ( it
          is one file of size around 660 KB ) .</p>

          '
        raw: "\nYou can always convert older models to the newer ones by itself. \n\
          I do not see a problem.\nApart from that if I know The Bloke will convert\
          \ all his older models to the new gguf format anyway. \n\nApart from that\
          \ ... what a problem to keep old llama.exe version ( it is one file of size\
          \ around 660 KB ) ."
        updatedAt: '2023-08-24T20:38:35.521Z'
      numEdits: 1
      reactions: []
    id: 64e7bf5731f7cea634d85dd0
    type: comment
  author: mirek190
  content: "\nYou can always convert older models to the newer ones by itself. \n\
    I do not see a problem.\nApart from that if I know The Bloke will convert all\
    \ his older models to the new gguf format anyway. \n\nApart from that ... what\
    \ a problem to keep old llama.exe version ( it is one file of size around 660\
    \ KB ) ."
  created_at: 2023-08-24 19:36:39+00:00
  edited: true
  hidden: false
  id: 64e7bf5731f7cea634d85dd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-08-24T20:39:33.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331533312797546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<blockquote>

          <p>You can always convert older models to the newer ones by itself.<br>I
          do not see a problem.<br>Apart from that if I know The Bloke will convert
          all his older models to the new gguf format anyway.</p>

          </blockquote>

          <p>Any guidance on that conversion? Just now starting to read on the new
          format.   I do hope its far better, else its silly to be inventing new formats
          just for fun.</p>

          '
        raw: "> You can always convert older models to the newer ones by itself. \n\
          > I do not see a problem.\n> Apart from that if I know The Bloke will convert\
          \ all his older models to the new gguf format anyway.\n\nAny guidance on\
          \ that conversion? Just now starting to read on the new format.   I do hope\
          \ its far better, else its silly to be inventing new formats just for fun."
        updatedAt: '2023-08-24T20:39:33.883Z'
      numEdits: 0
      reactions: []
    id: 64e7c00579630927bfe5de02
    type: comment
  author: Nurb432
  content: "> You can always convert older models to the newer ones by itself. \n\
    > I do not see a problem.\n> Apart from that if I know The Bloke will convert\
    \ all his older models to the new gguf format anyway.\n\nAny guidance on that\
    \ conversion? Just now starting to read on the new format.   I do hope its far\
    \ better, else its silly to be inventing new formats just for fun."
  created_at: 2023-08-24 19:39:33+00:00
  edited: false
  hidden: false
  id: 64e7c00579630927bfe5de02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-24T22:54:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9521206617355347
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>llama.cpp not providing backwards compatibility is a deliberate
          choice. They prefer to keep moving forward, and not spend time maintaining
          older code.  This does make it somewhat harder for users.</p>

          <p>Other clients/libraries do provide backwards compatibility. KoboldCpp
          for example supports GGUF (as of today), GGML v3 (the most recent version,
          the one we''ve been using since May), and older versions of GGML as well.  The
          developer of that aims to provide ongoing support for as wide a range of
          files for as long as possible.</p>

          <p>Yes I will be adding GGUFs for older repos soon, hopefully starting tomorrow.</p>

          '
        raw: 'llama.cpp not providing backwards compatibility is a deliberate choice.
          They prefer to keep moving forward, and not spend time maintaining older
          code.  This does make it somewhat harder for users.


          Other clients/libraries do provide backwards compatibility. KoboldCpp for
          example supports GGUF (as of today), GGML v3 (the most recent version, the
          one we''ve been using since May), and older versions of GGML as well.  The
          developer of that aims to provide ongoing support for as wide a range of
          files for as long as possible.


          Yes I will be adding GGUFs for older repos soon, hopefully starting tomorrow.'
        updatedAt: '2023-08-24T22:54:30.035Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Nurb432
        - mirek190
    id: 64e7dfa6ba5868ced8ee5459
    type: comment
  author: TheBloke
  content: 'llama.cpp not providing backwards compatibility is a deliberate choice.
    They prefer to keep moving forward, and not spend time maintaining older code.  This
    does make it somewhat harder for users.


    Other clients/libraries do provide backwards compatibility. KoboldCpp for example
    supports GGUF (as of today), GGML v3 (the most recent version, the one we''ve
    been using since May), and older versions of GGML as well.  The developer of that
    aims to provide ongoing support for as wide a range of files for as long as possible.


    Yes I will be adding GGUFs for older repos soon, hopefully starting tomorrow.'
  created_at: 2023-08-24 21:54:30+00:00
  edited: false
  hidden: false
  id: 64e7dfa6ba5868ced8ee5459
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
      fullname: Tea Lover
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tea-lover-418
      type: user
    createdAt: '2023-08-25T16:19:26.000Z'
    data:
      edited: true
      editors:
      - tea-lover-418
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9591037631034851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbc9431b4b7a98b1d986e2787b36c853.svg
          fullname: Tea Lover
          isHf: false
          isPro: false
          name: tea-lover-418
          type: user
        html: '<p>I''m not able to download it using the textgen webui, it just says
          "done" immediately after downloading everything but the large files. Are
          other people experiencing this? Probably an issue on their end since I have
          it on all GGUF repos, just curious if I''m not alone.</p>

          <p>Edit: I guess it''s intentional. They say GGML is not supported either
          for direct download but that does work, which is what got my confused. </p>

          '
        raw: 'I''m not able to download it using the textgen webui, it just says "done"
          immediately after downloading everything but the large files. Are other
          people experiencing this? Probably an issue on their end since I have it
          on all GGUF repos, just curious if I''m not alone.


          Edit: I guess it''s intentional. They say GGML is not supported either for
          direct download but that does work, which is what got my confused. '
        updatedAt: '2023-08-25T16:25:44.854Z'
      numEdits: 3
      reactions: []
    id: 64e8d48eb96ff0e17509b24c
    type: comment
  author: tea-lover-418
  content: 'I''m not able to download it using the textgen webui, it just says "done"
    immediately after downloading everything but the large files. Are other people
    experiencing this? Probably an issue on their end since I have it on all GGUF
    repos, just curious if I''m not alone.


    Edit: I guess it''s intentional. They say GGML is not supported either for direct
    download but that does work, which is what got my confused. '
  created_at: 2023-08-25 15:19:26+00:00
  edited: true
  hidden: false
  id: 64e8d48eb96ff0e17509b24c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T16:35:01.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9767870306968689
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it''s intentional - text-gen-ui doesn''t support GGUF files
          yet.  I think it''s looking for filenames ending in <code>.bin</code> or
          with ''ggml'' in their name, and not finding any here. </p>

          <p>I have a GGML repo for this model too, you should use that until text-gen-ui
          supports GGUF, which I imagine it will in a few days.</p>

          '
        raw: "Yeah it's intentional - text-gen-ui doesn't support GGUF files yet.\
          \  I think it's looking for filenames ending in `.bin` or with 'ggml' in\
          \ their name, and not finding any here. \n\nI have a GGML repo for this\
          \ model too, you should use that until text-gen-ui supports GGUF, which\
          \ I imagine it will in a few days."
        updatedAt: '2023-08-25T16:35:25.893Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - tea-lover-418
        - Nurb432
    id: 64e8d8358c523cced2e625f5
    type: comment
  author: TheBloke
  content: "Yeah it's intentional - text-gen-ui doesn't support GGUF files yet.  I\
    \ think it's looking for filenames ending in `.bin` or with 'ggml' in their name,\
    \ and not finding any here. \n\nI have a GGML repo for this model too, you should\
    \ use that until text-gen-ui supports GGUF, which I imagine it will in a few days."
  created_at: 2023-08-25 15:35:01+00:00
  edited: true
  hidden: false
  id: 64e8d8358c523cced2e625f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/PuddleJumper-13B-GGUF
repo_type: model
status: open
target_branch: null
title: 'first gguf ...nice '
