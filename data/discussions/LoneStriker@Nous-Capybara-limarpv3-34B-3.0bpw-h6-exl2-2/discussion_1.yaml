!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eramax
conflicting_files: null
created_at: 2023-12-16 07:06:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T07:06:41.000Z'
    data:
      edited: true
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6283681988716125
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>I thought I might be able to load this model on a colab T4 GPU instance\
          \ since its size is less than 15GB of VRAM, but I don't know why it couldn't\
          \ fit and I got this crash</p>\n<pre><code>!python examples/chat.py -m /content/model\
          \ -mode llama\n\n -- Model: /content/model\n -- Options: ['rope_scale 1.0',\
          \ 'rope_alpha 1.0']\n -- Loading model...\n -- Loading tokenizer...\nTraceback\
          \ (most recent call last):\n  File \"/content/exllamav2/examples/chat.py\"\
          , line 126, in &lt;module&gt;\n    cache = ExLlamaV2Cache(model, lazy =\
          \ not model.loaded)\n  File \"/content/exllamav2/exllamav2/cache.py\", line\
          \ 133, in __init__\n    self.create_state_tensors(copy_from, lazy)\n  File\
          \ \"/content/exllamav2/exllamav2/cache.py\", line 45, in create_state_tensors\n\
          \    p_key_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads,\
          \ self.head_dim, dtype = self.dtype, device = self.model.cache_map[i]).contiguous()\n\
          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00\
          \ MiB. GPU 0 has a total capacty of 14.75 GiB of which 209.06 MiB is free.\
          \ Process 270593 has 14.54 GiB memory in use. Of the allocated memory 14.32\
          \ GiB is allocated by PyTorch, and 111.15 MiB is reserved by PyTorch but\
          \ unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          </code></pre>\n<p>I am using the experimental branch of exllamav2.<br>I\
          \ would like to understand the VRAM requirements for each quantization since\
          \ I think it requires much more than the model size.</p>\n<p>Best,</p>\n"
        raw: "I thought I might be able to load this model on a colab T4 GPU instance\
          \ since its size is less than 15GB of VRAM, but I don't know why it couldn't\
          \ fit and I got this crash\n```\n!python examples/chat.py -m /content/model\
          \ -mode llama\n\n -- Model: /content/model\n -- Options: ['rope_scale 1.0',\
          \ 'rope_alpha 1.0']\n -- Loading model...\n -- Loading tokenizer...\nTraceback\
          \ (most recent call last):\n  File \"/content/exllamav2/examples/chat.py\"\
          , line 126, in <module>\n    cache = ExLlamaV2Cache(model, lazy = not model.loaded)\n\
          \  File \"/content/exllamav2/exllamav2/cache.py\", line 133, in __init__\n\
          \    self.create_state_tensors(copy_from, lazy)\n  File \"/content/exllamav2/exllamav2/cache.py\"\
          , line 45, in create_state_tensors\n    p_key_states = torch.zeros(self.batch_size,\
          \ self.max_seq_len, self.num_key_value_heads, self.head_dim, dtype = self.dtype,\
          \ device = self.model.cache_map[i]).contiguous()\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty\
          \ of 14.75 GiB of which 209.06 MiB is free. Process 270593 has 14.54 GiB\
          \ memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch,\
          \ and 111.15 MiB is reserved by PyTorch but unallocated. If reserved but\
          \ unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          ```\n\nI am using the experimental branch of exllamav2.\nI would like to\
          \ understand the VRAM requirements for each quantization since I think it\
          \ requires much more than the model size.\n\nBest,\n\n"
        updatedAt: '2023-12-16T08:43:52.968Z'
      numEdits: 1
      reactions: []
    id: 657d4c81a575d54a1e433c3c
    type: comment
  author: eramax
  content: "I thought I might be able to load this model on a colab T4 GPU instance\
    \ since its size is less than 15GB of VRAM, but I don't know why it couldn't fit\
    \ and I got this crash\n```\n!python examples/chat.py -m /content/model -mode\
    \ llama\n\n -- Model: /content/model\n -- Options: ['rope_scale 1.0', 'rope_alpha\
    \ 1.0']\n -- Loading model...\n -- Loading tokenizer...\nTraceback (most recent\
    \ call last):\n  File \"/content/exllamav2/examples/chat.py\", line 126, in <module>\n\
    \    cache = ExLlamaV2Cache(model, lazy = not model.loaded)\n  File \"/content/exllamav2/exllamav2/cache.py\"\
    , line 133, in __init__\n    self.create_state_tensors(copy_from, lazy)\n  File\
    \ \"/content/exllamav2/exllamav2/cache.py\", line 45, in create_state_tensors\n\
    \    p_key_states = torch.zeros(self.batch_size, self.max_seq_len, self.num_key_value_heads,\
    \ self.head_dim, dtype = self.dtype, device = self.model.cache_map[i]).contiguous()\n\
    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB.\
    \ GPU 0 has a total capacty of 14.75 GiB of which 209.06 MiB is free. Process\
    \ 270593 has 14.54 GiB memory in use. Of the allocated memory 14.32 GiB is allocated\
    \ by PyTorch, and 111.15 MiB is reserved by PyTorch but unallocated. If reserved\
    \ but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n```\n\n\
    I am using the experimental branch of exllamav2.\nI would like to understand the\
    \ VRAM requirements for each quantization since I think it requires much more\
    \ than the model size.\n\nBest,\n\n"
  created_at: 2023-12-16 07:06:41+00:00
  edited: true
  hidden: false
  id: 657d4c81a575d54a1e433c3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T13:29:29.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461849927902222
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>You need to account for the context length. Try restricting that
          to a very low number first to see if it will load. Otherwise, you''re going
          to need a lower bpw model or more VRAM.</p>

          '
        raw: You need to account for the context length. Try restricting that to a
          very low number first to see if it will load. Otherwise, you're going to
          need a lower bpw model or more VRAM.
        updatedAt: '2023-12-16T13:29:29.106Z'
      numEdits: 0
      reactions: []
    id: 657da639504da7f6f3acb43a
    type: comment
  author: LoneStriker
  content: You need to account for the context length. Try restricting that to a very
    low number first to see if it will load. Otherwise, you're going to need a lower
    bpw model or more VRAM.
  created_at: 2023-12-16 13:29:29+00:00
  edited: false
  hidden: false
  id: 657da639504da7f6f3acb43a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T13:34:50.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7286130785942078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ ., could you please give me the arrguments I should use for the command\
          \ <code>!python examples/chat.py -m /content/model -mode llama</code> to\
          \ make this model can fit for the 15 GB T4 VRAM?</p>\n<p>Regards,</p>\n"
        raw: '@LoneStriker ., could you please give me the arrguments I should use
          for the command `!python examples/chat.py -m /content/model -mode llama`
          to make this model can fit for the 15 GB T4 VRAM?


          Regards,'
        updatedAt: '2023-12-16T13:34:50.257Z'
      numEdits: 0
      reactions: []
    id: 657da77a7929709128e40b46
    type: comment
  author: eramax
  content: '@LoneStriker ., could you please give me the arrguments I should use for
    the command `!python examples/chat.py -m /content/model -mode llama` to make this
    model can fit for the 15 GB T4 VRAM?


    Regards,'
  created_at: 2023-12-16 13:34:50+00:00
  edited: false
  hidden: false
  id: 657da77a7929709128e40b46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T14:00:18.000Z'
    data:
      edited: true
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9448933005332947
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>One sec, sorry was answering a different question.</p>

          '
        raw: One sec, sorry was answering a different question.
        updatedAt: '2023-12-16T14:00:57.284Z'
      numEdits: 1
      reactions: []
    id: 657dad72db7adc7f733e80cb
    type: comment
  author: LoneStriker
  content: One sec, sorry was answering a different question.
  created_at: 2023-12-16 14:00:18+00:00
  edited: true
  hidden: false
  id: 657dad72db7adc7f733e80cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T14:05:02.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41869375109672546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>Try using the <code>-resc</code> and <code>-maxr</code> options\
          \ and setting them lower than the 250 and 1000 defaults.  There's not a\
          \ whole lot of room to scale down though, so you might need  a lower bpw\
          \ model.</p>\n<p>You can see the full options here:</p>\n<pre><code>$ python\
          \ ./examples/chat.py --help\nusage: chat.py [-h] [-dm DRAFT_MODEL_DIR] [-nds]\
          \ [-modes] [-mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}]\n\
          \               [-un USERNAME] [-bn BOTNAME] [-sp SYSTEM_PROMPT] [-temp\
          \ TEMPERATURE] [-topk TOP_K] [-topp TOP_P] [-typical TYPICAL]\n        \
          \       [-repp REPETITION_PENALTY] [-maxr MAX_RESPONSE_TOKENS] [-resc RESPONSE_CHUNK]\
          \ [-ncf] [-c8] [-pt] [-amnesia]\n               [-m MODEL_DIR] [-gs GPU_SPLIT]\
          \ [-l LENGTH] [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-nfa] [-lm]\n\nSimple Llama2\
          \ chat example for ExLlamaV2\n\noptions:\n  -h, --help            show this\
          \ help message and exit\n  -dm DRAFT_MODEL_DIR, --draft_model_dir DRAFT_MODEL_DIR\n\
          \                        Path to draft model directory\n  -nds, --no_draft_scale\n\
          \                        If draft model has smaller context size than model,\
          \ don't apply alpha (NTK) scaling to extend it\n  -modes, --modes      \
          \ List available modes and exit.\n  -mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar},\
          \ --mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}\n\
          \                        Chat mode. Use llama for Llama 1/2 chat finetunes.\n\
          \  -un USERNAME, --username USERNAME\n                        Username when\
          \ using raw chat mode\n  -bn BOTNAME, --botname BOTNAME\n              \
          \          Bot name when using raw chat mode\n  -sp SYSTEM_PROMPT, --system_prompt\
          \ SYSTEM_PROMPT\n                        Use custom system prompt\n  -temp\
          \ TEMPERATURE, --temperature TEMPERATURE\n                        Sampler\
          \ temperature, default = 0.95 (1 to disable)\n  -topk TOP_K, --top_k TOP_K\n\
          \                        Sampler top-K, default = 50 (0 to disable)\n  -topp\
          \ TOP_P, --top_p TOP_P\n                        Sampler top-P, default =\
          \ 0.8 (0 to disable)\n  -typical TYPICAL, --typical TYPICAL\n          \
          \              Sampler typical threshold, default = 0.0 (0 to disable)\n\
          \  -repp REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n \
          \                       Sampler repetition penalty, default = 1.05 (1 to\
          \ disable)\n  -maxr MAX_RESPONSE_TOKENS, --max_response_tokens MAX_RESPONSE_TOKENS\n\
          \                        Max tokens per response, default = 1000\n  -resc\
          \ RESPONSE_CHUNK, --response_chunk RESPONSE_CHUNK\n                    \
          \    Space to reserve in context for reply, default = 250\n  -ncf, --no_code_formatting\n\
          \                        Disable code formatting/syntax highlighting\n \
          \ -c8, --cache_8bit     Use 8-bit cache\n  -pt, --print_timings  Output\
          \ timings after each prompt\n  -amnesia, --amnesia   Forget context after\
          \ every response\n  -m MODEL_DIR, --model_dir MODEL_DIR\n              \
          \          Path to model directory\n  -gs GPU_SPLIT, --gpu_split GPU_SPLIT\n\
          \                        \"auto\", or VRAM allocation per GPU in GB\n  -l\
          \ LENGTH, --length LENGTH\n                        Maximum sequence length\n\
          \  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n                        RoPE\
          \ scaling factor\n  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n          \
          \              RoPE alpha value (NTK)\n  -nfa, --no_flash_attn\n       \
          \                 Disable Flash Attention\n  -lm, --low_mem        Enable\
          \ VRAM optimizations, potentially trading off speed\n</code></pre>\n"
        raw: "Try using the `-resc` and `-maxr` options and setting them lower than\
          \ the 250 and 1000 defaults.  There's not a whole lot of room to scale down\
          \ though, so you might need  a lower bpw model.\n\nYou can see the full\
          \ options here:\n```\n$ python ./examples/chat.py --help\nusage: chat.py\
          \ [-h] [-dm DRAFT_MODEL_DIR] [-nds] [-modes] [-mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}]\n\
          \               [-un USERNAME] [-bn BOTNAME] [-sp SYSTEM_PROMPT] [-temp\
          \ TEMPERATURE] [-topk TOP_K] [-topp TOP_P] [-typical TYPICAL]\n        \
          \       [-repp REPETITION_PENALTY] [-maxr MAX_RESPONSE_TOKENS] [-resc RESPONSE_CHUNK]\
          \ [-ncf] [-c8] [-pt] [-amnesia]\n               [-m MODEL_DIR] [-gs GPU_SPLIT]\
          \ [-l LENGTH] [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-nfa] [-lm]\n\nSimple Llama2\
          \ chat example for ExLlamaV2\n\noptions:\n  -h, --help            show this\
          \ help message and exit\n  -dm DRAFT_MODEL_DIR, --draft_model_dir DRAFT_MODEL_DIR\n\
          \                        Path to draft model directory\n  -nds, --no_draft_scale\n\
          \                        If draft model has smaller context size than model,\
          \ don't apply alpha (NTK) scaling to extend it\n  -modes, --modes      \
          \ List available modes and exit.\n  -mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar},\
          \ --mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}\n\
          \                        Chat mode. Use llama for Llama 1/2 chat finetunes.\n\
          \  -un USERNAME, --username USERNAME\n                        Username when\
          \ using raw chat mode\n  -bn BOTNAME, --botname BOTNAME\n              \
          \          Bot name when using raw chat mode\n  -sp SYSTEM_PROMPT, --system_prompt\
          \ SYSTEM_PROMPT\n                        Use custom system prompt\n  -temp\
          \ TEMPERATURE, --temperature TEMPERATURE\n                        Sampler\
          \ temperature, default = 0.95 (1 to disable)\n  -topk TOP_K, --top_k TOP_K\n\
          \                        Sampler top-K, default = 50 (0 to disable)\n  -topp\
          \ TOP_P, --top_p TOP_P\n                        Sampler top-P, default =\
          \ 0.8 (0 to disable)\n  -typical TYPICAL, --typical TYPICAL\n          \
          \              Sampler typical threshold, default = 0.0 (0 to disable)\n\
          \  -repp REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n \
          \                       Sampler repetition penalty, default = 1.05 (1 to\
          \ disable)\n  -maxr MAX_RESPONSE_TOKENS, --max_response_tokens MAX_RESPONSE_TOKENS\n\
          \                        Max tokens per response, default = 1000\n  -resc\
          \ RESPONSE_CHUNK, --response_chunk RESPONSE_CHUNK\n                    \
          \    Space to reserve in context for reply, default = 250\n  -ncf, --no_code_formatting\n\
          \                        Disable code formatting/syntax highlighting\n \
          \ -c8, --cache_8bit     Use 8-bit cache\n  -pt, --print_timings  Output\
          \ timings after each prompt\n  -amnesia, --amnesia   Forget context after\
          \ every response\n  -m MODEL_DIR, --model_dir MODEL_DIR\n              \
          \          Path to model directory\n  -gs GPU_SPLIT, --gpu_split GPU_SPLIT\n\
          \                        \"auto\", or VRAM allocation per GPU in GB\n  -l\
          \ LENGTH, --length LENGTH\n                        Maximum sequence length\n\
          \  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n                        RoPE\
          \ scaling factor\n  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n          \
          \              RoPE alpha value (NTK)\n  -nfa, --no_flash_attn\n       \
          \                 Disable Flash Attention\n  -lm, --low_mem        Enable\
          \ VRAM optimizations, potentially trading off speed\n\n```"
        updatedAt: '2023-12-16T14:05:02.959Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eramax
    id: 657dae8e416635415ff03a34
    type: comment
  author: LoneStriker
  content: "Try using the `-resc` and `-maxr` options and setting them lower than\
    \ the 250 and 1000 defaults.  There's not a whole lot of room to scale down though,\
    \ so you might need  a lower bpw model.\n\nYou can see the full options here:\n\
    ```\n$ python ./examples/chat.py --help\nusage: chat.py [-h] [-dm DRAFT_MODEL_DIR]\
    \ [-nds] [-modes] [-mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}]\n\
    \               [-un USERNAME] [-bn BOTNAME] [-sp SYSTEM_PROMPT] [-temp TEMPERATURE]\
    \ [-topk TOP_K] [-topp TOP_P] [-typical TYPICAL]\n               [-repp REPETITION_PENALTY]\
    \ [-maxr MAX_RESPONSE_TOKENS] [-resc RESPONSE_CHUNK] [-ncf] [-c8] [-pt] [-amnesia]\n\
    \               [-m MODEL_DIR] [-gs GPU_SPLIT] [-l LENGTH] [-rs ROPE_SCALE] [-ra\
    \ ROPE_ALPHA] [-nfa] [-lm]\n\nSimple Llama2 chat example for ExLlamaV2\n\noptions:\n\
    \  -h, --help            show this help message and exit\n  -dm DRAFT_MODEL_DIR,\
    \ --draft_model_dir DRAFT_MODEL_DIR\n                        Path to draft model\
    \ directory\n  -nds, --no_draft_scale\n                        If draft model\
    \ has smaller context size than model, don't apply alpha (NTK) scaling to extend\
    \ it\n  -modes, --modes       List available modes and exit.\n  -mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar},\
    \ --mode {raw,llama,codellama,chatml,tinyllama,zephyr,deepseek,solar}\n      \
    \                  Chat mode. Use llama for Llama 1/2 chat finetunes.\n  -un USERNAME,\
    \ --username USERNAME\n                        Username when using raw chat mode\n\
    \  -bn BOTNAME, --botname BOTNAME\n                        Bot name when using\
    \ raw chat mode\n  -sp SYSTEM_PROMPT, --system_prompt SYSTEM_PROMPT\n        \
    \                Use custom system prompt\n  -temp TEMPERATURE, --temperature\
    \ TEMPERATURE\n                        Sampler temperature, default = 0.95 (1\
    \ to disable)\n  -topk TOP_K, --top_k TOP_K\n                        Sampler top-K,\
    \ default = 50 (0 to disable)\n  -topp TOP_P, --top_p TOP_P\n                \
    \        Sampler top-P, default = 0.8 (0 to disable)\n  -typical TYPICAL, --typical\
    \ TYPICAL\n                        Sampler typical threshold, default = 0.0 (0\
    \ to disable)\n  -repp REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n\
    \                        Sampler repetition penalty, default = 1.05 (1 to disable)\n\
    \  -maxr MAX_RESPONSE_TOKENS, --max_response_tokens MAX_RESPONSE_TOKENS\n    \
    \                    Max tokens per response, default = 1000\n  -resc RESPONSE_CHUNK,\
    \ --response_chunk RESPONSE_CHUNK\n                        Space to reserve in\
    \ context for reply, default = 250\n  -ncf, --no_code_formatting\n           \
    \             Disable code formatting/syntax highlighting\n  -c8, --cache_8bit\
    \     Use 8-bit cache\n  -pt, --print_timings  Output timings after each prompt\n\
    \  -amnesia, --amnesia   Forget context after every response\n  -m MODEL_DIR,\
    \ --model_dir MODEL_DIR\n                        Path to model directory\n  -gs\
    \ GPU_SPLIT, --gpu_split GPU_SPLIT\n                        \"auto\", or VRAM\
    \ allocation per GPU in GB\n  -l LENGTH, --length LENGTH\n                   \
    \     Maximum sequence length\n  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n   \
    \                     RoPE scaling factor\n  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n\
    \                        RoPE alpha value (NTK)\n  -nfa, --no_flash_attn\n   \
    \                     Disable Flash Attention\n  -lm, --low_mem        Enable\
    \ VRAM optimizations, potentially trading off speed\n\n```"
  created_at: 2023-12-16 14:05:02+00:00
  edited: false
  hidden: false
  id: 657dae8e416635415ff03a34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T15:19:23.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6243422627449036
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ ., could you please give me the arrguments I should use for the command\
          \ <code>!python examples/chat.py -m /content/model -mode llama</code> to\
          \ make this model can fit for the 15 GB T4 VRAM?</p>\n<p>Regards,</p>\n\
          </blockquote>\n<p>Try this 2.8bpw model:<br><a href=\"https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.8bpw-h6-exl2-2\"\
          >https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.8bpw-h6-exl2-2</a></p>\n"
        raw: "> @LoneStriker ., could you please give me the arrguments I should use\
          \ for the command `!python examples/chat.py -m /content/model -mode llama`\
          \ to make this model can fit for the 15 GB T4 VRAM?\n> \n> Regards,\n\n\
          Try this 2.8bpw model:\nhttps://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.8bpw-h6-exl2-2"
        updatedAt: '2023-12-16T15:19:23.450Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - eramax
    id: 657dbffb8888ccb894ce96a8
    type: comment
  author: LoneStriker
  content: "> @LoneStriker ., could you please give me the arrguments I should use\
    \ for the command `!python examples/chat.py -m /content/model -mode llama` to\
    \ make this model can fit for the 15 GB T4 VRAM?\n> \n> Regards,\n\nTry this 2.8bpw\
    \ model:\nhttps://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.8bpw-h6-exl2-2"
  created_at: 2023-12-16 15:19:23+00:00
  edited: false
  hidden: false
  id: 657dbffb8888ccb894ce96a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T15:24:07.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6716790199279785
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>Also you can try these options:</p>\n<pre><code>  -c8, --cache_8bit\
          \     Use 8-bit cache\n  -lm, --low_mem        Enable VRAM optimizations,\
          \ potentially trading off speed\n</code></pre>\n"
        raw: "Also you can try these options:\n```\n  -c8, --cache_8bit     Use 8-bit\
          \ cache\n  -lm, --low_mem        Enable VRAM optimizations, potentially\
          \ trading off speed\n```"
        updatedAt: '2023-12-16T15:24:07.700Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eramax
    id: 657dc1173c65a5be2a05493b
    type: comment
  author: LoneStriker
  content: "Also you can try these options:\n```\n  -c8, --cache_8bit     Use 8-bit\
    \ cache\n  -lm, --low_mem        Enable VRAM optimizations, potentially trading\
    \ off speed\n```"
  created_at: 2023-12-16 15:24:07+00:00
  edited: false
  hidden: false
  id: 657dc1173c65a5be2a05493b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T16:17:08.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7584241032600403
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ , I tried to tune the arguments but still couldn't run it<br><a rel=\"\
          nofollow\" href=\"https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a\"\
          >https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a</a></p>\n"
        raw: "Thanks @LoneStriker , I tried to tune the arguments but still couldn't\
          \ run it \nhttps://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a"
        updatedAt: '2023-12-16T16:17:08.058Z'
      numEdits: 0
      reactions: []
    id: 657dcd84f4f72f2c4c160613
    type: comment
  author: eramax
  content: "Thanks @LoneStriker , I tried to tune the arguments but still couldn't\
    \ run it \nhttps://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a"
  created_at: 2023-12-16 16:17:08+00:00
  edited: false
  hidden: false
  id: 657dcd84f4f72f2c4c160613
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T17:34:39.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6114620566368103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Last attempt, you can try to load this 2.4bpw model:<br><a href="https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2">https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2</a></p>

          '
        raw: 'Last attempt, you can try to load this 2.4bpw model:

          https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2'
        updatedAt: '2023-12-16T17:34:39.586Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - eramax
    id: 657ddfafa2ef321675d9f3ca
    type: comment
  author: LoneStriker
  content: 'Last attempt, you can try to load this 2.4bpw model:

    https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2'
  created_at: 2023-12-16 17:34:39+00:00
  edited: false
  hidden: false
  id: 657ddfafa2ef321675d9f3ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T18:08:08.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9290185570716858
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>,\
          \ same issue couldn't run because of vram.<br><a rel=\"nofollow\" href=\"\
          https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a\">https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a</a></p>\n\
          <p>The key is that in order to determine the effectiveness of quantization\
          \ with respect to vram size, speed, and perplexity, we must have a thorough\
          \ grasp of the vram needs for each quantized model to be able to run.</p>\n"
        raw: 'Thanks @LoneStriker, same issue couldn''t run because of vram.

          https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a


          The key is that in order to determine the effectiveness of quantization
          with respect to vram size, speed, and perplexity, we must have a thorough
          grasp of the vram needs for each quantized model to be able to run.



          '
        updatedAt: '2023-12-16T18:08:08.948Z'
      numEdits: 0
      reactions: []
    id: 657de788f5eacd4bdadf89bf
    type: comment
  author: eramax
  content: 'Thanks @LoneStriker, same issue couldn''t run because of vram.

    https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a


    The key is that in order to determine the effectiveness of quantization with respect
    to vram size, speed, and perplexity, we must have a thorough grasp of the vram
    needs for each quantized model to be able to run.



    '
  created_at: 2023-12-16 18:08:08+00:00
  edited: false
  hidden: false
  id: 657de788f5eacd4bdadf89bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T18:19:14.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5784112811088562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>Looks like it's the <code>-l</code> option that you're missing.\
          \  You should be able to load one of the bigger models like 2.8 or 3.0.\
          \  Loading the 2.4bpw model, I'm using 12384MiB memory.</p>\n<pre><code>$\
          \ python examples/chat.py -m /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\
          \ --mode llama -l 2048\n -- Model: /models/models/hf/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
          \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n --\
          \ Loading model...\n -- Loading tokenizer...\n -- Prompt format: llama\n\
          \ -- System prompt:\n\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature.\n\nUser:\n</code></pre>\n<pre><code>$ nvidia-smi\n\
          Sat Dec 16 12:16:52 2023\n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |     \
          \             Off |\n|  0%   45C    P8              38W / 450W |  12384MiB\
          \ / 24564MiB |      0%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          </code></pre>\n"
        raw: "Looks like it's the `-l` option that you're missing.  You should be\
          \ able to load one of the bigger models like 2.8 or 3.0.  Loading the 2.4bpw\
          \ model, I'm using 12384MiB memory.\n\n```\n$ python examples/chat.py -m\
          \ /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2 --mode llama -l 2048\n\
          \ -- Model: /models/models/hf/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
          \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n --\
          \ Loading model...\n -- Loading tokenizer...\n -- Prompt format: llama\n\
          \ -- System prompt:\n\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature.\n\nUser:\n```\n\n```\n$ nvidia-smi\nSat Dec 16\
          \ 12:16:52 2023\n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |     \
          \             Off |\n|  0%   45C    P8              38W / 450W |  12384MiB\
          \ / 24564MiB |      0%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          ```\n\n"
        updatedAt: '2023-12-16T18:19:14.630Z'
      numEdits: 0
      reactions: []
    id: 657dea223687559a675dc194
    type: comment
  author: LoneStriker
  content: "Looks like it's the `-l` option that you're missing.  You should be able\
    \ to load one of the bigger models like 2.8 or 3.0.  Loading the 2.4bpw model,\
    \ I'm using 12384MiB memory.\n\n```\n$ python examples/chat.py -m /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\
    \ --mode llama -l 2048\n -- Model: /models/models/hf/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
    \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading\
    \ model...\n -- Loading tokenizer...\n -- Prompt format: llama\n -- System prompt:\n\
    \nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe.  Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature.\n\nUser:\n\
    ```\n\n```\n$ nvidia-smi\nSat Dec 16 12:16:52 2023\n+---------------------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version:\
    \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr.\
    \ ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util\
    \  Compute M. |\n|                                         |                 \
    \     |               MIG M. |\n|=========================================+======================+======================|\n\
    |   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |           \
    \       Off |\n|  0%   45C    P8              38W / 450W |  12384MiB / 24564MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
    ```\n\n"
  created_at: 2023-12-16 18:19:14+00:00
  edited: false
  hidden: false
  id: 657dea223687559a675dc194
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T18:58:33.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9554902911186218
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>Now it worked, Thanks for help, the model unfortunately does not
          generates any outputs maybe the prompt, I will try another one.</p>

          '
        raw: 'Now it worked, Thanks for help, the model unfortunately does not generates
          any outputs maybe the prompt, I will try another one.

          '
        updatedAt: '2023-12-16T18:58:33.666Z'
      numEdits: 0
      reactions: []
    id: 657df3591e3e9c41a494b7c9
    type: comment
  author: eramax
  content: 'Now it worked, Thanks for help, the model unfortunately does not generates
    any outputs maybe the prompt, I will try another one.

    '
  created_at: 2023-12-16 18:58:33+00:00
  edited: false
  hidden: false
  id: 657df3591e3e9c41a494b7c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-16T19:23:25.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8111388087272644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>I tried different models, all gave been loaded successfully by your\
          \ recommendation of using length (I think the length is the context size)<br>The\
          \ models some of them didn't make any outputs like</p>\n<pre><code> -- Model:\
          \ /content/model\n -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha\
          \ 1.0']\n -- Loading model...\n -- Loading tokenizer...\n -- Prompt format:\
          \ llama\n -- System prompt:\n\nYou are a helpful, respectful and honest\
          \ assistant. Always answer as helpfully as possible, while being safe. \
          \ Your answers should not include any harmful, unethical, racist, sexist,\
          \ toxic, dangerous, or illegal content. Please ensure that your responses\
          \ are socially unbiased and positive in nature.\n\nUser: who are u\n\n\n\
          \nUser: hello\n\n\n\nUser: \n^C\n</code></pre>\n<p>Others gave me an error\
          \ <code> Response exceeded 2000 tokens and was cut short</code> I adjacted\
          \ the <code>maxr</code> from 1000 to 2000 and even that it didn't give me\
          \ any outputs</p>\n<pre><code> -- Model: /content/model\n -- Options: ['length:\
          \ 4096', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading model...\n --\
          \ Loading tokenizer...\n -- Prompt format: deepseek\n -- System prompt:\n\
          \nYou are an AI programming assistant, utilizing the Deepseek Coder model,\
          \ developed by Deepseek Company, and you only answer questions related to\
          \ computer science. For politically sensitive questions, security and privacy\
          \ issues, and other non-computer science questions, you will refuse to answer.\n\
          \nUser: who are u\n\n\n !! Response exceeded 2000 tokens and was cut short.\n\
          \nUser: \n</code></pre>\n<p>When I used <code>--low_mem</code> I got this\
          \ error </p>\n<pre><code> -- Model: /content/model\n -- Options: ['length:\
          \ 2048', 'rope_scale 1.0', 'rope_alpha 1.0', 'low_mem']\n -- Loading model...\n\
          \ -- Loading tokenizer...\n -- Prompt format: llama\n -- System prompt:\n\
          \nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\n\
          \nUser: who are u\n\nTraceback (most recent call last):\n  File \"/content/exllamav2/examples/chat.py\"\
          , line 242, in &lt;module&gt;\n    generator.begin_stream(active_context,\
          \ settings)\n  File \"/content/exllamav2/exllamav2/generator/streaming.py\"\
          , line 88, in begin_stream\n    self._gen_begin_reuse(input_ids, gen_settings)\n\
          \  File \"/content/exllamav2/exllamav2/generator/streaming.py\", line 267,\
          \ in _gen_begin_reuse\n    self._gen_begin(in_tokens, gen_settings)\n  File\
          \ \"/content/exllamav2/exllamav2/generator/streaming.py\", line 253, in\
          \ _gen_begin\n    self.model.forward(self.sequence_ids[:, :-1], self.cache,\
          \ preprocess_only = True, loras = self.active_loras)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/content/exllamav2/exllamav2/model.py\", line 589, in forward\n    r,\
          \ ls = self._forward(input_ids = input_ids[:, chunk_begin : chunk_end],\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/content/exllamav2/exllamav2/model.py\", line 665, in _forward\n   \
          \ x = module.forward(x, cache = cache, attn_mask = attn_mask, past_len =\
          \ past_len, loras = loras, position_offsets = position_offsets)\n  File\
          \ \"/content/exllamav2/exllamav2/attn.py\", line 497, in forward\n    pass_loras,\n\
          UnboundLocalError: local variable 'pass_loras' referenced before assignment\n\
          </code></pre>\n<p>I should mentioned that before I didn't use any length\
          \ size and I had an issue that no logner happens after I used specified\
          \ the length size which was the <code>chat.py</code> script takes around\
          \ 90 to 120 seconds doing nothing (I think it calls sleep method as i was\
          \ shown in colab and no activites on the cpu or the gpu at this period)\
          \ and after that sleep time, it starts printing and loading the model, this\
          \ issue only when I didn't specifiy the length size.</p>\n<p>Best,</p>\n"
        raw: "I tried different models, all gave been loaded successfully by your\
          \ recommendation of using length (I think the length is the context size)\n\
          The models some of them didn't make any outputs like\n```\n -- Model: /content/model\n\
          \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n --\
          \ Loading model...\n -- Loading tokenizer...\n -- Prompt format: llama\n\
          \ -- System prompt:\n\nYou are a helpful, respectful and honest assistant.\
          \ Always answer as helpfully as possible, while being safe.  Your answers\
          \ should not include any harmful, unethical, racist, sexist, toxic, dangerous,\
          \ or illegal content. Please ensure that your responses are socially unbiased\
          \ and positive in nature.\n\nUser: who are u\n\n\n\nUser: hello\n\n\n\n\
          User: \n^C\n```\n\nOthers gave me an error ` Response exceeded 2000 tokens\
          \ and was cut short` I adjacted the `maxr` from 1000 to 2000 and even that\
          \ it didn't give me any outputs\n```\n -- Model: /content/model\n -- Options:\
          \ ['length: 4096', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading model...\n\
          \ -- Loading tokenizer...\n -- Prompt format: deepseek\n -- System prompt:\n\
          \nYou are an AI programming assistant, utilizing the Deepseek Coder model,\
          \ developed by Deepseek Company, and you only answer questions related to\
          \ computer science. For politically sensitive questions, security and privacy\
          \ issues, and other non-computer science questions, you will refuse to answer.\n\
          \nUser: who are u\n\n\n !! Response exceeded 2000 tokens and was cut short.\n\
          \nUser: \n\n```\nWhen I used `--low_mem` I got this error \n```\n -- Model:\
          \ /content/model\n -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha\
          \ 1.0', 'low_mem']\n -- Loading model...\n -- Loading tokenizer...\n --\
          \ Prompt format: llama\n -- System prompt:\n\nYou are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe.  Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature.\n\nUser: who are\
          \ u\n\nTraceback (most recent call last):\n  File \"/content/exllamav2/examples/chat.py\"\
          , line 242, in <module>\n    generator.begin_stream(active_context, settings)\n\
          \  File \"/content/exllamav2/exllamav2/generator/streaming.py\", line 88,\
          \ in begin_stream\n    self._gen_begin_reuse(input_ids, gen_settings)\n\
          \  File \"/content/exllamav2/exllamav2/generator/streaming.py\", line 267,\
          \ in _gen_begin_reuse\n    self._gen_begin(in_tokens, gen_settings)\n  File\
          \ \"/content/exllamav2/exllamav2/generator/streaming.py\", line 253, in\
          \ _gen_begin\n    self.model.forward(self.sequence_ids[:, :-1], self.cache,\
          \ preprocess_only = True, loras = self.active_loras)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/content/exllamav2/exllamav2/model.py\", line 589, in forward\n    r,\
          \ ls = self._forward(input_ids = input_ids[:, chunk_begin : chunk_end],\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/content/exllamav2/exllamav2/model.py\", line 665, in _forward\n   \
          \ x = module.forward(x, cache = cache, attn_mask = attn_mask, past_len =\
          \ past_len, loras = loras, position_offsets = position_offsets)\n  File\
          \ \"/content/exllamav2/exllamav2/attn.py\", line 497, in forward\n    pass_loras,\n\
          UnboundLocalError: local variable 'pass_loras' referenced before assignment\n\
          ```\n\nI should mentioned that before I didn't use any length size and I\
          \ had an issue that no logner happens after I used specified the length\
          \ size which was the `chat.py` script takes around 90 to 120 seconds doing\
          \ nothing (I think it calls sleep method as i was shown in colab and no\
          \ activites on the cpu or the gpu at this period) and after that sleep time,\
          \ it starts printing and loading the model, this issue only when I didn't\
          \ specifiy the length size.\n\nBest,"
        updatedAt: '2023-12-16T19:23:25.719Z'
      numEdits: 0
      reactions: []
    id: 657df92d1e2087032428e48d
    type: comment
  author: eramax
  content: "I tried different models, all gave been loaded successfully by your recommendation\
    \ of using length (I think the length is the context size)\nThe models some of\
    \ them didn't make any outputs like\n```\n -- Model: /content/model\n -- Options:\
    \ ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading model...\n\
    \ -- Loading tokenizer...\n -- Prompt format: llama\n -- System prompt:\n\nYou\
    \ are a helpful, respectful and honest assistant. Always answer as helpfully as\
    \ possible, while being safe.  Your answers should not include any harmful, unethical,\
    \ racist, sexist, toxic, dangerous, or illegal content. Please ensure that your\
    \ responses are socially unbiased and positive in nature.\n\nUser: who are u\n\
    \n\n\nUser: hello\n\n\n\nUser: \n^C\n```\n\nOthers gave me an error ` Response\
    \ exceeded 2000 tokens and was cut short` I adjacted the `maxr` from 1000 to 2000\
    \ and even that it didn't give me any outputs\n```\n -- Model: /content/model\n\
    \ -- Options: ['length: 4096', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading\
    \ model...\n -- Loading tokenizer...\n -- Prompt format: deepseek\n -- System\
    \ prompt:\n\nYou are an AI programming assistant, utilizing the Deepseek Coder\
    \ model, developed by Deepseek Company, and you only answer questions related\
    \ to computer science. For politically sensitive questions, security and privacy\
    \ issues, and other non-computer science questions, you will refuse to answer.\n\
    \nUser: who are u\n\n\n !! Response exceeded 2000 tokens and was cut short.\n\n\
    User: \n\n```\nWhen I used `--low_mem` I got this error \n```\n -- Model: /content/model\n\
    \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0', 'low_mem']\n\
    \ -- Loading model...\n -- Loading tokenizer...\n -- Prompt format: llama\n --\
    \ System prompt:\n\nYou are a helpful, respectful and honest assistant. Always\
    \ answer as helpfully as possible, while being safe.  Your answers should not\
    \ include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal\
    \ content. Please ensure that your responses are socially unbiased and positive\
    \ in nature.\n\nUser: who are u\n\nTraceback (most recent call last):\n  File\
    \ \"/content/exllamav2/examples/chat.py\", line 242, in <module>\n    generator.begin_stream(active_context,\
    \ settings)\n  File \"/content/exllamav2/exllamav2/generator/streaming.py\", line\
    \ 88, in begin_stream\n    self._gen_begin_reuse(input_ids, gen_settings)\n  File\
    \ \"/content/exllamav2/exllamav2/generator/streaming.py\", line 267, in _gen_begin_reuse\n\
    \    self._gen_begin(in_tokens, gen_settings)\n  File \"/content/exllamav2/exllamav2/generator/streaming.py\"\
    , line 253, in _gen_begin\n    self.model.forward(self.sequence_ids[:, :-1], self.cache,\
    \ preprocess_only = True, loras = self.active_loras)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/content/exllamav2/exllamav2/model.py\"\
    , line 589, in forward\n    r, ls = self._forward(input_ids = input_ids[:, chunk_begin\
    \ : chunk_end],\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/content/exllamav2/exllamav2/model.py\"\
    , line 665, in _forward\n    x = module.forward(x, cache = cache, attn_mask =\
    \ attn_mask, past_len = past_len, loras = loras, position_offsets = position_offsets)\n\
    \  File \"/content/exllamav2/exllamav2/attn.py\", line 497, in forward\n    pass_loras,\n\
    UnboundLocalError: local variable 'pass_loras' referenced before assignment\n\
    ```\n\nI should mentioned that before I didn't use any length size and I had an\
    \ issue that no logner happens after I used specified the length size which was\
    \ the `chat.py` script takes around 90 to 120 seconds doing nothing (I think it\
    \ calls sleep method as i was shown in colab and no activites on the cpu or the\
    \ gpu at this period) and after that sleep time, it starts printing and loading\
    \ the model, this issue only when I didn't specifiy the length size.\n\nBest,"
  created_at: 2023-12-16 19:23:25+00:00
  edited: false
  hidden: false
  id: 657df92d1e2087032428e48d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T19:36:10.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9573370814323425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>It's the prompt format most likely.  Try raw mode, seems to generate\
          \ text for me:</p>\n<pre><code>$ python examples/chat.py -m /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\
          \ -l 2048 --mode raw\n -- Model: /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
          \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n --\
          \ Loading model...\n -- Loading tokenizer...\n -- Prompt format: raw\n --\
          \ System prompt:\n\nThis is a conversation between a helpful AI assistant\
          \ named Chatbort and a user.\n\nUser: Write a story about llamas\n\nChatbort:\
          \ Once upon a time in a land far, far away, there was a village known as\
          \ Llama-topia. The villagers were an eclectic mix of animals, all of whom\
          \ coexisted harmoniously. Among them were the kind-hearted llamas. These\
          \ gentle creatures had become the heart and soul of the village, their wool\
          \ providing warmth for everyone during the harsh winter months.\n\nOne day,\
          \ a young llama named Lupita noticed that some of her fellow villagers were\
          \ shivering from the cold. She decided to organize a group of volunteers\
          \ to gather more wool. They worked tirelessly, shearing the soft wool from\
          \ their bodies and spinning it into yarn. The yarn was then knitted into\
          \ beautiful blankets by another group of volunteers.\n\nAs days passed,\
          \ the village became warmer and warmer, with every llama in the village\
          \ enjoying the warmth provided by the blankets. Lupita and her team felt\
          \ overjoyed at the sight of their work bringing comfort to those who needed\
          \ it most. Their spirits lifted even more when they saw how much happier\
          \ and healthier everyone seemed to be due to their efforts.\n\nFrom that\
          \ day forward, Lupita and her fellow llamas continued their work, ensuring\
          \ that no one in Llama-topia ever had to endure the biting cold again. Their\
          \ selfless actions created a lasting bond between all villagers, proving\
          \ that when we work together, anything can be achieved.&lt;/s&gt;\n</code></pre>\n"
        raw: "It's the prompt format most likely.  Try raw mode, seems to generate\
          \ text for me:\n\n```\n$ python examples/chat.py -m /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\
          \ -l 2048 --mode raw\n -- Model: /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
          \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n --\
          \ Loading model...\n -- Loading tokenizer...\n -- Prompt format: raw\n --\
          \ System prompt:\n\nThis is a conversation between a helpful AI assistant\
          \ named Chatbort and a user.\n\nUser: Write a story about llamas\n\nChatbort:\
          \ Once upon a time in a land far, far away, there was a village known as\
          \ Llama-topia. The villagers were an eclectic mix of animals, all of whom\
          \ coexisted harmoniously. Among them were the kind-hearted llamas. These\
          \ gentle creatures had become the heart and soul of the village, their wool\
          \ providing warmth for everyone during the harsh winter months.\n\nOne day,\
          \ a young llama named Lupita noticed that some of her fellow villagers were\
          \ shivering from the cold. She decided to organize a group of volunteers\
          \ to gather more wool. They worked tirelessly, shearing the soft wool from\
          \ their bodies and spinning it into yarn. The yarn was then knitted into\
          \ beautiful blankets by another group of volunteers.\n\nAs days passed,\
          \ the village became warmer and warmer, with every llama in the village\
          \ enjoying the warmth provided by the blankets. Lupita and her team felt\
          \ overjoyed at the sight of their work bringing comfort to those who needed\
          \ it most. Their spirits lifted even more when they saw how much happier\
          \ and healthier everyone seemed to be due to their efforts.\n\nFrom that\
          \ day forward, Lupita and her fellow llamas continued their work, ensuring\
          \ that no one in Llama-topia ever had to endure the biting cold again. Their\
          \ selfless actions created a lasting bond between all villagers, proving\
          \ that when we work together, anything can be achieved.</s>\n\n```"
        updatedAt: '2023-12-16T19:36:10.278Z'
      numEdits: 0
      reactions: []
    id: 657dfc2a647c0211e79189e9
    type: comment
  author: LoneStriker
  content: "It's the prompt format most likely.  Try raw mode, seems to generate text\
    \ for me:\n\n```\n$ python examples/chat.py -m /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\
    \ -l 2048 --mode raw\n -- Model: /models/Nous-Capybara-limarpv3-34B-2.4bpw-h6-exl2-2\n\
    \ -- Options: ['length: 2048', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading\
    \ model...\n -- Loading tokenizer...\n -- Prompt format: raw\n -- System prompt:\n\
    \nThis is a conversation between a helpful AI assistant named Chatbort and a user.\n\
    \nUser: Write a story about llamas\n\nChatbort: Once upon a time in a land far,\
    \ far away, there was a village known as Llama-topia. The villagers were an eclectic\
    \ mix of animals, all of whom coexisted harmoniously. Among them were the kind-hearted\
    \ llamas. These gentle creatures had become the heart and soul of the village,\
    \ their wool providing warmth for everyone during the harsh winter months.\n\n\
    One day, a young llama named Lupita noticed that some of her fellow villagers\
    \ were shivering from the cold. She decided to organize a group of volunteers\
    \ to gather more wool. They worked tirelessly, shearing the soft wool from their\
    \ bodies and spinning it into yarn. The yarn was then knitted into beautiful blankets\
    \ by another group of volunteers.\n\nAs days passed, the village became warmer\
    \ and warmer, with every llama in the village enjoying the warmth provided by\
    \ the blankets. Lupita and her team felt overjoyed at the sight of their work\
    \ bringing comfort to those who needed it most. Their spirits lifted even more\
    \ when they saw how much happier and healthier everyone seemed to be due to their\
    \ efforts.\n\nFrom that day forward, Lupita and her fellow llamas continued their\
    \ work, ensuring that no one in Llama-topia ever had to endure the biting cold\
    \ again. Their selfless actions created a lasting bond between all villagers,\
    \ proving that when we work together, anything can be achieved.</s>\n\n```"
  created_at: 2023-12-16 19:36:10+00:00
  edited: false
  hidden: false
  id: 657dfc2a647c0211e79189e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-17T10:12:05.000Z'
    data:
      edited: true
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9211741089820862
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>It worked successfully, thank you,  I even was able to run <code>LoneStriker/Toppy-Mix-4x7B-4.0bpw-h6-exl2-2</code>
          smoothly and fast without any issues  <a rel="nofollow" href="https://gist.github.com/eramax/485513cd4b1c5d1698fa89ff817b705f">https://gist.github.com/eramax/485513cd4b1c5d1698fa89ff817b705f</a><br><a
          rel="nofollow" href="https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a">https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a

          </a></p>

          <p>maybe you can add the ipynb to the examples or the tutorial links so
          people can get benefit of it.<br>The links are publicly accessible.</p>

          <p>Next thing we could improve:</p>

          <ol>

          <li>fix the delay in loading the model in the first run, it occurs as well
          when i specify the length :(</li>

          <li>the output in the jupyter notebook has an issue  which is the <code>CR
          and LF control characters</code> don''t show correctly (check the links
          above)</li>

          <li>some models I have tried out and didn''t generates any outputs ( i haven''t
          tried all of them one more time, but I think even if I don''t provide the
          correct prompt format, the model should generate an output and some models
          gave me this error <code> !! Response exceeded 2000 tokens and was cut short.</code>
          without any outputs</li>

          <li>enable offloading layers to the cpu or offloading the tokenizer will
          be very helpful</li>

          </ol>

          <p>Finally the model size is much better than gguf, the speed not measured
          yet for comparason<br>Best,</p>

          '
        raw: "It worked successfully, thank you,  I even was able to run `LoneStriker/Toppy-Mix-4x7B-4.0bpw-h6-exl2-2`\
          \ smoothly and fast without any issues  https://gist.github.com/eramax/485513cd4b1c5d1698fa89ff817b705f\n\
          [https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a\n](https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a)\n\
          \nmaybe you can add the ipynb to the examples or the tutorial links so people\
          \ can get benefit of it.\nThe links are publicly accessible.\n\nNext thing\
          \ we could improve:\n1. fix the delay in loading the model in the first\
          \ run, it occurs as well when i specify the length :(\n2. the output in\
          \ the jupyter notebook has an issue  which is the `CR and LF control characters`\
          \ don't show correctly (check the links above)\n3. some models I have tried\
          \ out and didn't generates any outputs ( i haven't tried all of them one\
          \ more time, but I think even if I don't provide the correct prompt format,\
          \ the model should generate an output and some models gave me this error\
          \ ` !! Response exceeded 2000 tokens and was cut short.` without any outputs\n\
          4. enable offloading layers to the cpu or offloading the tokenizer will\
          \ be very helpful \n\nFinally the model size is much better than gguf, the\
          \ speed not measured yet for comparason\nBest,\n"
        updatedAt: '2023-12-17T10:20:24.390Z'
      numEdits: 4
      reactions: []
    id: 657ec97542fc53e18b5c114a
    type: comment
  author: eramax
  content: "It worked successfully, thank you,  I even was able to run `LoneStriker/Toppy-Mix-4x7B-4.0bpw-h6-exl2-2`\
    \ smoothly and fast without any issues  https://gist.github.com/eramax/485513cd4b1c5d1698fa89ff817b705f\n\
    [https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a\n](https://gist.github.com/eramax/732ab060b8c3adddd12a2ad6f0741d5a)\n\
    \nmaybe you can add the ipynb to the examples or the tutorial links so people\
    \ can get benefit of it.\nThe links are publicly accessible.\n\nNext thing we\
    \ could improve:\n1. fix the delay in loading the model in the first run, it occurs\
    \ as well when i specify the length :(\n2. the output in the jupyter notebook\
    \ has an issue  which is the `CR and LF control characters` don't show correctly\
    \ (check the links above)\n3. some models I have tried out and didn't generates\
    \ any outputs ( i haven't tried all of them one more time, but I think even if\
    \ I don't provide the correct prompt format, the model should generate an output\
    \ and some models gave me this error ` !! Response exceeded 2000 tokens and was\
    \ cut short.` without any outputs\n4. enable offloading layers to the cpu or offloading\
    \ the tokenizer will be very helpful \n\nFinally the model size is much better\
    \ than gguf, the speed not measured yet for comparason\nBest,\n"
  created_at: 2023-12-17 10:12:05+00:00
  edited: true
  hidden: false
  id: 657ec97542fc53e18b5c114a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Nous-Capybara-limarpv3-34B-3.0bpw-h6-exl2-2
repo_type: model
status: open
target_branch: null
title: Couldn't run in colab
