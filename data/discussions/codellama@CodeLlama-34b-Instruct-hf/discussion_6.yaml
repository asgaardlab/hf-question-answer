!!python/object:huggingface_hub.community.DiscussionWithDetails
author: borzunov
conflicting_files: null
created_at: 2023-08-25 22:18:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-08-25T23:18:50.000Z'
    data:
      edited: true
      editors:
      - borzunov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8664072155952454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/philschmid\"\
          >@<span class=\"underline\">philschmid</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;osanseviero&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/osanseviero\">@<span\
          \ class=\"underline\">osanseviero</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Thanks for all your work on making this model available on the Model\
          \ Hub!</p>\n<p>I have an issue while using the model - quite often, the\
          \ model starts to generate <code>\\n</code> indefinitely instead of generating\
          \ <code>&lt;/s&gt;</code> and stopping.</p>\n<p>This is using the standard\
          \ generation params (<code>temperature=0.2, top_p=0.95</code>) with the\
          \ prompt format and the example prompt suggested in the official repository:</p>\n\
          <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nA chat between a curious\
          \ human and an artificial intelligence assistant. The assistant gives helpful,\
          \ detailed, and polite answers to the user's questions.\n&lt;&lt;/SYS&gt;&gt;\n\
          \nIn Bash, how do I list all text files in the current directory (excluding\
          \ subdirectories) that have been modified in the last month? [/INST]\n</code></pre>\n\
          <p>This issue was also reported on GitHub: <a rel=\"nofollow\" href=\"https://github.com/facebookresearch/codellama/issues/26\"\
          >https://github.com/facebookresearch/codellama/issues/26</a></p>\n"
        raw: 'Hi @philschmid @osanseviero,


          Thanks for all your work on making this model available on the Model Hub!


          I have an issue while using the model - quite often, the model starts to
          generate `\n` indefinitely instead of generating `</s>` and stopping.


          This is using the standard generation params (`temperature=0.2, top_p=0.95`)
          with the prompt format and the example prompt suggested in the official
          repository:


          ```

          <s>[INST] <<SYS>>

          A chat between a curious human and an artificial intelligence assistant.
          The assistant gives helpful, detailed, and polite answers to the user''s
          questions.

          <</SYS>>


          In Bash, how do I list all text files in the current directory (excluding
          subdirectories) that have been modified in the last month? [/INST]

          ```


          This issue was also reported on GitHub: https://github.com/facebookresearch/codellama/issues/26'
        updatedAt: '2023-08-26T01:59:12.228Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - zzhqinxiao
        - Vinhnguyenx
    id: 64e936da662874dbc9ba8507
    type: comment
  author: borzunov
  content: 'Hi @philschmid @osanseviero,


    Thanks for all your work on making this model available on the Model Hub!


    I have an issue while using the model - quite often, the model starts to generate
    `\n` indefinitely instead of generating `</s>` and stopping.


    This is using the standard generation params (`temperature=0.2, top_p=0.95`) with
    the prompt format and the example prompt suggested in the official repository:


    ```

    <s>[INST] <<SYS>>

    A chat between a curious human and an artificial intelligence assistant. The assistant
    gives helpful, detailed, and polite answers to the user''s questions.

    <</SYS>>


    In Bash, how do I list all text files in the current directory (excluding subdirectories)
    that have been modified in the last month? [/INST]

    ```


    This issue was also reported on GitHub: https://github.com/facebookresearch/codellama/issues/26'
  created_at: 2023-08-25 22:18:50+00:00
  edited: true
  hidden: false
  id: 64e936da662874dbc9ba8507
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-08-25T23:21:09.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-26T01:58:58.548Z'
      numEdits: 1
      reactions: []
    id: 64e937654a2e1742115289f5
    type: comment
  author: borzunov
  content: This comment has been hidden
  created_at: 2023-08-25 22:21:09+00:00
  edited: true
  hidden: true
  id: 64e937654a2e1742115289f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21b902318508544c08f4d925368cfe4a.svg
      fullname: Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vinhnguyenx
      type: user
    createdAt: '2023-08-31T04:39:23.000Z'
    data:
      edited: true
      editors:
      - Vinhnguyenx
      hidden: false
      identifiedLanguage:
        language: nl
        probability: 0.4369029700756073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21b902318508544c08f4d925368cfe4a.svg
          fullname: Nguyen
          isHf: false
          isPro: false
          name: Vinhnguyenx
          type: user
        html: '<p>+1. I''m observing that the model doesn''t seem to follow instructions
          and generates poor answers when served with HF TGI 1.0.2. </p>

          '
        raw: '+1. I''m observing that the model doesn''t seem to follow instructions
          and generates poor answers when served with HF TGI 1.0.2. '
        updatedAt: '2023-08-31T07:30:41.955Z'
      numEdits: 5
      reactions: []
    id: 64f0197bbae933568b81162c
    type: comment
  author: Vinhnguyenx
  content: '+1. I''m observing that the model doesn''t seem to follow instructions
    and generates poor answers when served with HF TGI 1.0.2. '
  created_at: 2023-08-31 03:39:23+00:00
  edited: true
  hidden: false
  id: 64f0197bbae933568b81162c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: codellama/CodeLlama-34b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Model pads response with newlines up to max_length
