!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jiajia100
conflicting_files: null
created_at: 2023-10-06 08:36:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4de15cd88468bc7be4c4e3380d49a661.svg
      fullname: cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jiajia100
      type: user
    createdAt: '2023-10-06T09:36:01.000Z'
    data:
      edited: false
      editors:
      - jiajia100
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5644683837890625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4de15cd88468bc7be4c4e3380d49a661.svg
          fullname: cheng
          isHf: false
          isPro: false
          name: jiajia100
          type: user
        html: "<p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>\
          \        tokenizer = AutoTokenizer.from_pretrained(<br>            model_path,\
          \ use_fast=self.use_fast_tokenizer, revision=revision<br>        )<br> \
          \       model = AutoModelForCausalLM.from_pretrained(<br>            model_path,<br>\
          \            low_cpu_mem_usage=True,<br>            **from_pretrained_kwargs,<br>\
          \        )</p>\n<p>log:</p>\n<p>Loading checkpoint shards:   0%|       \
          \                                                                      \
          \                                             | 0/7 [00:00&lt;?, ?it/s]<br>Loading\
          \ checkpoint shards:  14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258E                 \
          \                                                                      \
          \          | 1/7 [00:09&lt;00:56,  9.38s/it]<br>Loading checkpoint shards:\
          \  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C           \
          \                                                                      |\
          \ 2/7 [00:18&lt;00:45,  9.16s/it]<br>Loading checkpoint shards:  29%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C                       \
          \                                                          | 2/7 [00:18&lt;00:45,\
          \  9.19s/it]<br>2023-10-06 17:09:12,662 | ERROR | stderr |<br>2023-10-06\
          \ 17:09:12,662 | ERROR | stderr | Traceback (most recent call last):<br>2023-10-06\
          \ 17:09:12,662 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 484, in load_state_dict<br>2023-10-06 17:09:12,663 | ERROR | stderr\
          \ |     return torch.load(checkpoint_file, map_location=map_location)<br>2023-10-06\
          \ 17:09:12,663 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 815, in load<br>2023-10-06 17:09:12,663 | ERROR | stderr |     return\
          \ _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)<br>2023-10-06\
          \ 17:09:12,664 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 1018, in _legacy_load<br>2023-10-06 17:09:12,664 | ERROR | stderr\
          \ |     return legacy_load(f)<br>2023-10-06 17:09:12,664 | ERROR | stderr\
          \ |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 904, in legacy_load<br>2023-10-06 17:09:12,665 | ERROR | stderr |\
          \     tar.extract('storages', path=tmpdir)<br>2023-10-06 17:09:12,665 |\
          \ ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\"\
          , line 2091, in extract<br>2023-10-06 17:09:12,665 | ERROR | stderr |  \
          \   tarinfo = self.getmember(member)<br>2023-10-06 17:09:12,665 | ERROR\
          \ | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\",\
          \ line 1813, in getmember<br>2023-10-06 17:09:12,666 | ERROR | stderr |\
          \     raise KeyError(\"filename %r not found\" % name)<br>2023-10-06 17:09:12,666\
          \ | ERROR | stderr | KeyError: \"filename 'storages' not found\"<br>2023-10-06\
          \ 17:09:12,666 | ERROR | stderr |<br>2023-10-06 17:09:12,666 | ERROR | stderr\
          \ | The above exception was the direct cause of the following exception:<br>2023-10-06\
          \ 17:09:12,666 | ERROR | stderr |<br>2023-10-06 17:09:12,666 | ERROR | stderr\
          \ | Traceback (most recent call last):<br>2023-10-06 17:09:12,667 | ERROR\
          \ | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 495, in load_state_dict<br>2023-10-06 17:09:12,667 | ERROR | stderr\
          \ |     raise ValueError(<br>2023-10-06 17:09:12,667 | ERROR | stderr |\
          \ ValueError: Unable to locate the file models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin\
          \ which is necessary to load this pretrained model. Make sure you have saved\
          \ the model properly.<br>2023-10-06 17:09:12,667 | ERROR | stderr |<br>2023-10-06\
          \ 17:09:12,667 | ERROR | stderr | During handling of the above exception,\
          \ another exception occurred:<br>2023-10-06 17:09:12,667 | ERROR | stderr\
          \ |<br>2023-10-06 17:09:12,668 | ERROR | stderr | Traceback (most recent\
          \ call last):<br>2023-10-06 17:09:12,668 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/runpy.py\"\
          , line 196, in _run_module_as_main<br>2023-10-06 17:09:12,668 | ERROR |\
          \ stderr |     return _run_code(code, main_globals, None,<br>2023-10-06\
          \ 17:09:12,668 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/runpy.py\"\
          , line 86, in _run_code<br>2023-10-06 17:09:12,668 | ERROR | stderr |  \
          \   exec(code, run_globals)<br>2023-10-06 17:09:12,668 | ERROR | stderr\
          \ |   File \"/home/fastchat/serve/model_worker.py\", line 467, in <br>2023-10-06\
          \ 17:09:12,669 | ERROR | stderr |     worker = ModelWorker(<br>2023-10-06\
          \ 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/serve/model_worker.py\"\
          , line 210, in <strong>init</strong><br>2023-10-06 17:09:12,669 | ERROR\
          \ | stderr |     self.model, self.tokenizer = load_model(<br>2023-10-06\
          \ 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/model/model_adapter.py\"\
          , line 264, in load_model<br>2023-10-06 17:09:12,669 | ERROR | stderr |\
          \     model, tokenizer = adapter.load_model(model_path, kwargs)<br>2023-10-06\
          \ 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/model/model_adapter.py\"\
          , line 1280, in load_model<br>2023-10-06 17:09:12,670 | ERROR | stderr |\
          \     model = AutoModelForCausalLM.from_pretrained(<br>2023-10-06 17:09:12,670\
          \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 565, in from_pretrained<br>2023-10-06 17:09:12,670 | ERROR | stderr\
          \ |     return model_class.from_pretrained(<br>2023-10-06 17:09:12,670 |\
          \ ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained<br>2023-10-06 17:09:12,671 | ERROR | stderr\
          \ |     ) = cls._load_pretrained_model(<br>2023-10-06 17:09:12,671 | ERROR\
          \ | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3681, in _load_pretrained_model<br>2023-10-06 17:09:12,672 | ERROR\
          \ | stderr |     state_dict = load_state_dict(shard_file)<br>2023-10-06\
          \ 17:09:12,672 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 500, in load_state_dict<br>2023-10-06 17:09:12,673 | ERROR | stderr\
          \ |     raise OSError(<br>2023-10-06 17:09:12,673 | ERROR | stderr | OSError:\
          \ Unable to load weights from pytorch checkpoint file for 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'\
          \ at 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'.\
          \ If you tried to load a PyTorch model from a TF 2.0 checkpoint, please\
          \ set from_tf=True.</p>\n<p>evns:<br>transformers              4.34<br>accelerate\
          \                    0.23.0</p>\n"
        raw: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n   \
          \     tokenizer = AutoTokenizer.from_pretrained(\r\n            model_path,\
          \ use_fast=self.use_fast_tokenizer, revision=revision\r\n        )\r\n \
          \       model = AutoModelForCausalLM.from_pretrained(\r\n            model_path,\r\
          \n            low_cpu_mem_usage=True,\r\n            **from_pretrained_kwargs,\r\
          \n        )\r\n\r\n\r\nlog:\r\n\r\nLoading checkpoint shards:   0%|    \
          \                                                                      \
          \                                                | 0/7 [00:00<?, ?it/s]\r\
          \nLoading checkpoint shards:  14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258E           \
          \                                                                      \
          \                | 1/7 [00:09<00:56,  9.38s/it]\r\nLoading checkpoint shards:\
          \  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C           \
          \                                                                      |\
          \ 2/7 [00:18<00:45,  9.16s/it]\r\nLoading checkpoint shards:  29%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C                       \
          \                                                          | 2/7 [00:18<00:45,\
          \  9.19s/it]\r\n2023-10-06 17:09:12,662 | ERROR | stderr | \r\n2023-10-06\
          \ 17:09:12,662 | ERROR | stderr | Traceback (most recent call last):\r\n\
          2023-10-06 17:09:12,662 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 484, in load_state_dict\r\n2023-10-06 17:09:12,663 | ERROR | stderr\
          \ |     return torch.load(checkpoint_file, map_location=map_location)\r\n\
          2023-10-06 17:09:12,663 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 815, in load\r\n2023-10-06 17:09:12,663 | ERROR | stderr |     return\
          \ _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\
          \n2023-10-06 17:09:12,664 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 1018, in _legacy_load\r\n2023-10-06 17:09:12,664 | ERROR | stderr\
          \ |     return legacy_load(f)\r\n2023-10-06 17:09:12,664 | ERROR | stderr\
          \ |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 904, in legacy_load\r\n2023-10-06 17:09:12,665 | ERROR | stderr |\
          \     tar.extract('storages', path=tmpdir)\r\n2023-10-06 17:09:12,665 |\
          \ ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\"\
          , line 2091, in extract\r\n2023-10-06 17:09:12,665 | ERROR | stderr |  \
          \   tarinfo = self.getmember(member)\r\n2023-10-06 17:09:12,665 | ERROR\
          \ | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\",\
          \ line 1813, in getmember\r\n2023-10-06 17:09:12,666 | ERROR | stderr |\
          \     raise KeyError(\"filename %r not found\" % name)\r\n2023-10-06 17:09:12,666\
          \ | ERROR | stderr | KeyError: \"filename 'storages' not found\"\r\n2023-10-06\
          \ 17:09:12,666 | ERROR | stderr | \r\n2023-10-06 17:09:12,666 | ERROR |\
          \ stderr | The above exception was the direct cause of the following exception:\r\
          \n2023-10-06 17:09:12,666 | ERROR | stderr | \r\n2023-10-06 17:09:12,666\
          \ | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 17:09:12,667\
          \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 495, in load_state_dict\r\n2023-10-06 17:09:12,667 | ERROR | stderr\
          \ |     raise ValueError(\r\n2023-10-06 17:09:12,667 | ERROR | stderr |\
          \ ValueError: Unable to locate the file models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin\
          \ which is necessary to load this pretrained model. Make sure you have saved\
          \ the model properly.\r\n2023-10-06 17:09:12,667 | ERROR | stderr | \r\n\
          2023-10-06 17:09:12,667 | ERROR | stderr | During handling of the above\
          \ exception, another exception occurred:\r\n2023-10-06 17:09:12,667 | ERROR\
          \ | stderr | \r\n2023-10-06 17:09:12,668 | ERROR | stderr | Traceback (most\
          \ recent call last):\r\n2023-10-06 17:09:12,668 | ERROR | stderr |   File\
          \ \"/opt/miniconda3/envs/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\
          \n2023-10-06 17:09:12,668 | ERROR | stderr |     return _run_code(code,\
          \ main_globals, None,\r\n2023-10-06 17:09:12,668 | ERROR | stderr |   File\
          \ \"/opt/miniconda3/envs/lib/python3.10/runpy.py\", line 86, in _run_code\r\
          \n2023-10-06 17:09:12,668 | ERROR | stderr |     exec(code, run_globals)\r\
          \n2023-10-06 17:09:12,668 | ERROR | stderr |   File \"/home/fastchat/serve/model_worker.py\"\
          , line 467, in <module>\r\n2023-10-06 17:09:12,669 | ERROR | stderr |  \
          \   worker = ModelWorker(\r\n2023-10-06 17:09:12,669 | ERROR | stderr |\
          \   File \"/home/fastchat/serve/model_worker.py\", line 210, in __init__\r\
          \n2023-10-06 17:09:12,669 | ERROR | stderr |     self.model, self.tokenizer\
          \ = load_model(\r\n2023-10-06 17:09:12,669 | ERROR | stderr |   File \"\
          /home/fastchat/model/model_adapter.py\", line 264, in load_model\r\n2023-10-06\
          \ 17:09:12,669 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path,\
          \ kwargs)\r\n2023-10-06 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/model/model_adapter.py\"\
          , line 1280, in load_model\r\n2023-10-06 17:09:12,670 | ERROR | stderr |\
          \     model = AutoModelForCausalLM.from_pretrained(\r\n2023-10-06 17:09:12,670\
          \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 565, in from_pretrained\r\n2023-10-06 17:09:12,670 | ERROR | stderr\
          \ |     return model_class.from_pretrained(\r\n2023-10-06 17:09:12,670 |\
          \ ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\r\n2023-10-06 17:09:12,671 | ERROR | stderr\
          \ |     ) = cls._load_pretrained_model(\r\n2023-10-06 17:09:12,671 | ERROR\
          \ | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3681, in _load_pretrained_model\r\n2023-10-06 17:09:12,672 | ERROR\
          \ | stderr |     state_dict = load_state_dict(shard_file)\r\n2023-10-06\
          \ 17:09:12,672 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 500, in load_state_dict\r\n2023-10-06 17:09:12,673 | ERROR | stderr\
          \ |     raise OSError(\r\n2023-10-06 17:09:12,673 | ERROR | stderr | OSError:\
          \ Unable to load weights from pytorch checkpoint file for 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'\
          \ at 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'.\
          \ If you tried to load a PyTorch model from a TF 2.0 checkpoint, please\
          \ set from_tf=True.\r\n\r\nevns:\r\ntransformers              4.34\r\naccelerate\
          \                    0.23.0"
        updatedAt: '2023-10-06T09:36:01.511Z'
      numEdits: 0
      reactions: []
    id: 651fd501ad9603918747bd24
    type: comment
  author: jiajia100
  content: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n     \
    \   tokenizer = AutoTokenizer.from_pretrained(\r\n            model_path, use_fast=self.use_fast_tokenizer,\
    \ revision=revision\r\n        )\r\n        model = AutoModelForCausalLM.from_pretrained(\r\
    \n            model_path,\r\n            low_cpu_mem_usage=True,\r\n         \
    \   **from_pretrained_kwargs,\r\n        )\r\n\r\n\r\nlog:\r\n\r\nLoading checkpoint\
    \ shards:   0%|                                                              \
    \                                                            | 0/7 [00:00<?, ?it/s]\r\
    \nLoading checkpoint shards:  14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258E                       \
    \                                                                          | 1/7\
    \ [00:09<00:56,  9.38s/it]\r\nLoading checkpoint shards:  29%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u258C                                                     \
    \                            | 2/7 [00:18<00:45,  9.16s/it]\r\nLoading checkpoint\
    \ shards:  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C                       \
    \                                                          | 2/7 [00:18<00:45,\
    \  9.19s/it]\r\n2023-10-06 17:09:12,662 | ERROR | stderr | \r\n2023-10-06 17:09:12,662\
    \ | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06 17:09:12,662\
    \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 484, in load_state_dict\r\n2023-10-06 17:09:12,663 | ERROR | stderr | \
    \    return torch.load(checkpoint_file, map_location=map_location)\r\n2023-10-06\
    \ 17:09:12,663 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 815, in load\r\n2023-10-06 17:09:12,663 | ERROR | stderr |     return _legacy_load(opened_file,\
    \ map_location, pickle_module, **pickle_load_args)\r\n2023-10-06 17:09:12,664\
    \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 1018, in _legacy_load\r\n2023-10-06 17:09:12,664 | ERROR | stderr |   \
    \  return legacy_load(f)\r\n2023-10-06 17:09:12,664 | ERROR | stderr |   File\
    \ \"/opt/miniconda3/envs/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 904, in legacy_load\r\n2023-10-06 17:09:12,665 | ERROR | stderr |     tar.extract('storages',\
    \ path=tmpdir)\r\n2023-10-06 17:09:12,665 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\"\
    , line 2091, in extract\r\n2023-10-06 17:09:12,665 | ERROR | stderr |     tarinfo\
    \ = self.getmember(member)\r\n2023-10-06 17:09:12,665 | ERROR | stderr |   File\
    \ \"/opt/miniconda3/envs/lib/python3.10/tarfile.py\", line 1813, in getmember\r\
    \n2023-10-06 17:09:12,666 | ERROR | stderr |     raise KeyError(\"filename %r\
    \ not found\" % name)\r\n2023-10-06 17:09:12,666 | ERROR | stderr | KeyError:\
    \ \"filename 'storages' not found\"\r\n2023-10-06 17:09:12,666 | ERROR | stderr\
    \ | \r\n2023-10-06 17:09:12,666 | ERROR | stderr | The above exception was the\
    \ direct cause of the following exception:\r\n2023-10-06 17:09:12,666 | ERROR\
    \ | stderr | \r\n2023-10-06 17:09:12,666 | ERROR | stderr | Traceback (most recent\
    \ call last):\r\n2023-10-06 17:09:12,667 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 495, in load_state_dict\r\n2023-10-06 17:09:12,667 | ERROR | stderr | \
    \    raise ValueError(\r\n2023-10-06 17:09:12,667 | ERROR | stderr | ValueError:\
    \ Unable to locate the file models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin\
    \ which is necessary to load this pretrained model. Make sure you have saved the\
    \ model properly.\r\n2023-10-06 17:09:12,667 | ERROR | stderr | \r\n2023-10-06\
    \ 17:09:12,667 | ERROR | stderr | During handling of the above exception, another\
    \ exception occurred:\r\n2023-10-06 17:09:12,667 | ERROR | stderr | \r\n2023-10-06\
    \ 17:09:12,668 | ERROR | stderr | Traceback (most recent call last):\r\n2023-10-06\
    \ 17:09:12,668 | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/runpy.py\"\
    , line 196, in _run_module_as_main\r\n2023-10-06 17:09:12,668 | ERROR | stderr\
    \ |     return _run_code(code, main_globals, None,\r\n2023-10-06 17:09:12,668\
    \ | ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/runpy.py\",\
    \ line 86, in _run_code\r\n2023-10-06 17:09:12,668 | ERROR | stderr |     exec(code,\
    \ run_globals)\r\n2023-10-06 17:09:12,668 | ERROR | stderr |   File \"/home/fastchat/serve/model_worker.py\"\
    , line 467, in <module>\r\n2023-10-06 17:09:12,669 | ERROR | stderr |     worker\
    \ = ModelWorker(\r\n2023-10-06 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/serve/model_worker.py\"\
    , line 210, in __init__\r\n2023-10-06 17:09:12,669 | ERROR | stderr |     self.model,\
    \ self.tokenizer = load_model(\r\n2023-10-06 17:09:12,669 | ERROR | stderr | \
    \  File \"/home/fastchat/model/model_adapter.py\", line 264, in load_model\r\n\
    2023-10-06 17:09:12,669 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path,\
    \ kwargs)\r\n2023-10-06 17:09:12,669 | ERROR | stderr |   File \"/home/fastchat/model/model_adapter.py\"\
    , line 1280, in load_model\r\n2023-10-06 17:09:12,670 | ERROR | stderr |     model\
    \ = AutoModelForCausalLM.from_pretrained(\r\n2023-10-06 17:09:12,670 | ERROR |\
    \ stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 565, in from_pretrained\r\n2023-10-06 17:09:12,670 | ERROR | stderr | \
    \    return model_class.from_pretrained(\r\n2023-10-06 17:09:12,670 | ERROR |\
    \ stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3307, in from_pretrained\r\n2023-10-06 17:09:12,671 | ERROR | stderr |\
    \     ) = cls._load_pretrained_model(\r\n2023-10-06 17:09:12,671 | ERROR | stderr\
    \ |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3681, in _load_pretrained_model\r\n2023-10-06 17:09:12,672 | ERROR | stderr\
    \ |     state_dict = load_state_dict(shard_file)\r\n2023-10-06 17:09:12,672 |\
    \ ERROR | stderr |   File \"/opt/miniconda3/envs/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 500, in load_state_dict\r\n2023-10-06 17:09:12,673 | ERROR | stderr | \
    \    raise OSError(\r\n2023-10-06 17:09:12,673 | ERROR | stderr | OSError: Unable\
    \ to load weights from pytorch checkpoint file for 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'\
    \ at 'models/CodeLlama-34b-Instruct-hf/pytorch_model-00003-of-00007.bin'. If you\
    \ tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
    \n\r\nevns:\r\ntransformers              4.34\r\naccelerate                  \
    \  0.23.0"
  created_at: 2023-10-06 08:36:01+00:00
  edited: false
  hidden: false
  id: 651fd501ad9603918747bd24
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: codellama/CodeLlama-34b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: 'KeyError: "filename ''storages'' not found"'
