!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mlschmidt366
conflicting_files: null
created_at: 2023-09-19 10:43:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7418030c0ae6250a2cef370c21402d51.svg
      fullname: Marvin Schmidt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mlschmidt366
      type: user
    createdAt: '2023-09-19T11:43:20.000Z'
    data:
      edited: false
      editors:
      - mlschmidt366
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8103792667388916
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7418030c0ae6250a2cef370c21402d51.svg
          fullname: Marvin Schmidt
          isHf: false
          isPro: false
          name: mlschmidt366
          type: user
        html: '<p>Hi,</p>

          <p>I am trying to use HF''s inference API to interact with the model from
          a gradio app. For larger inputs, I receive a validation error: "Input validation
          error: <code>inputs</code> tokens + <code>max_new_tokens</code> must be
          &lt;= 8192". Is this a limitation on this HF implementation or am I using
          the inference API wrong? From the blog post I read that CodeLlama should
          support up to 100k tokens in the input. How to achieve that with this model?</p>

          '
        raw: "Hi,\r\n\r\nI am trying to use HF's inference API to interact with the\
          \ model from a gradio app. For larger inputs, I receive a validation error:\
          \ \"Input validation error: `inputs` tokens + `max_new_tokens` must be <=\
          \ 8192\". Is this a limitation on this HF implementation or am I using the\
          \ inference API wrong? From the blog post I read that CodeLlama should support\
          \ up to 100k tokens in the input. How to achieve that with this model?"
        updatedAt: '2023-09-19T11:43:20.557Z'
      numEdits: 0
      reactions: []
    id: 65098958ce83a0c12a87aa4f
    type: comment
  author: mlschmidt366
  content: "Hi,\r\n\r\nI am trying to use HF's inference API to interact with the\
    \ model from a gradio app. For larger inputs, I receive a validation error: \"\
    Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8192\".\
    \ Is this a limitation on this HF implementation or am I using the inference API\
    \ wrong? From the blog post I read that CodeLlama should support up to 100k tokens\
    \ in the input. How to achieve that with this model?"
  created_at: 2023-09-19 10:43:20+00:00
  edited: false
  hidden: false
  id: 65098958ce83a0c12a87aa4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635897a55ddda96618737311/WmTCqW-DysgzfOrUjsjY9.jpeg?w=200&h=200&f=face
      fullname: Douglas Purdy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cancelself
      type: user
    createdAt: '2023-10-03T00:32:53.000Z'
    data:
      edited: false
      editors:
      - cancelself
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16420507431030273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635897a55ddda96618737311/WmTCqW-DysgzfOrUjsjY9.jpeg?w=200&h=200&f=face
          fullname: Douglas Purdy
          isHf: false
          isPro: false
          name: cancelself
          type: user
        html: '<p>You have to extend the context window using ROPE.</p>

          <pre><code>text-generation-launcher     --model-id $MODEL_ID     --rope-scaling
          dynamic     --max-input-length 16384     --max-total-tokens 32768     --max-batch-prefill-tokens
          16384     --hostname 0.0.0.0     --port 3000

          </code></pre>

          '
        raw: 'You have to extend the context window using ROPE.

          ```

          text-generation-launcher     --model-id $MODEL_ID     --rope-scaling dynamic     --max-input-length
          16384     --max-total-tokens 32768     --max-batch-prefill-tokens 16384     --hostname
          0.0.0.0     --port 3000

          ```'
        updatedAt: '2023-10-03T00:32:53.909Z'
      numEdits: 0
      reactions: []
    id: 651b6135ca84a54d9ce0342e
    type: comment
  author: cancelself
  content: 'You have to extend the context window using ROPE.

    ```

    text-generation-launcher     --model-id $MODEL_ID     --rope-scaling dynamic     --max-input-length
    16384     --max-total-tokens 32768     --max-batch-prefill-tokens 16384     --hostname
    0.0.0.0     --port 3000

    ```'
  created_at: 2023-10-02 23:32:53+00:00
  edited: false
  hidden: false
  id: 651b6135ca84a54d9ce0342e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cddcfcb36b2496175226784663cb04f3.svg
      fullname: Mitchell Spencer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mitchhs12
      type: user
    createdAt: '2023-10-03T17:54:55.000Z'
    data:
      edited: false
      editors:
      - mitchhs12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8501205444335938
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cddcfcb36b2496175226784663cb04f3.svg
          fullname: Mitchell Spencer
          isHf: false
          isPro: false
          name: mitchhs12
          type: user
        html: '<blockquote>

          <p>Hi,</p>

          <p>I am trying to use HF''s inference API to interact with the model from
          a gradio app. For larger inputs, I receive a validation error: "Input validation
          error: <code>inputs</code> tokens + <code>max_new_tokens</code> must be
          &lt;= 8192". Is this a limitation on this HF implementation or am I using
          the inference API wrong? From the blog post I read that CodeLlama should
          support up to 100k tokens in the input. How to achieve that with this model?</p>

          </blockquote>

          <p>I am also having this problem, am trying to use Langchain.</p>

          '
        raw: "> Hi,\n> \n> I am trying to use HF's inference API to interact with\
          \ the model from a gradio app. For larger inputs, I receive a validation\
          \ error: \"Input validation error: `inputs` tokens + `max_new_tokens` must\
          \ be <= 8192\". Is this a limitation on this HF implementation or am I using\
          \ the inference API wrong? From the blog post I read that CodeLlama should\
          \ support up to 100k tokens in the input. How to achieve that with this\
          \ model?\n\nI am also having this problem, am trying to use Langchain."
        updatedAt: '2023-10-03T17:54:55.542Z'
      numEdits: 0
      reactions: []
    id: 651c556fc243af5d76121935
    type: comment
  author: mitchhs12
  content: "> Hi,\n> \n> I am trying to use HF's inference API to interact with the\
    \ model from a gradio app. For larger inputs, I receive a validation error: \"\
    Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8192\".\
    \ Is this a limitation on this HF implementation or am I using the inference API\
    \ wrong? From the blog post I read that CodeLlama should support up to 100k tokens\
    \ in the input. How to achieve that with this model?\n\nI am also having this\
    \ problem, am trying to use Langchain."
  created_at: 2023-10-03 16:54:55+00:00
  edited: false
  hidden: false
  id: 651c556fc243af5d76121935
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ccee6620b0696f7cf5d4706623b1b09.svg
      fullname: T Welch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: twelch2
      type: user
    createdAt: '2023-10-20T04:23:45.000Z'
    data:
      edited: false
      editors:
      - twelch2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445408582687378
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ccee6620b0696f7cf5d4706623b1b09.svg
          fullname: T Welch
          isHf: false
          isPro: false
          name: twelch2
          type: user
        html: '<p>I''m having the same issue. Anybody have any insight? Is this  configurable,
          or is it a hard limit through the Inference API model?</p>

          '
        raw: I'm having the same issue. Anybody have any insight? Is this  configurable,
          or is it a hard limit through the Inference API model?
        updatedAt: '2023-10-20T04:23:45.423Z'
      numEdits: 0
      reactions: []
    id: 653200d1ace0c1c5f0ec3730
    type: comment
  author: twelch2
  content: I'm having the same issue. Anybody have any insight? Is this  configurable,
    or is it a hard limit through the Inference API model?
  created_at: 2023-10-20 03:23:45+00:00
  edited: false
  hidden: false
  id: 653200d1ace0c1c5f0ec3730
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: codellama/CodeLlama-34b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Inference API doesn't seem to support 100k context window
