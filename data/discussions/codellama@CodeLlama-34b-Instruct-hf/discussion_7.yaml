!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Varunk29
conflicting_files: null
created_at: 2023-08-26 13:59:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f2632df040b1b55c59f3a38c5ebeff2.svg
      fullname: Varun Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Varunk29
      type: user
    createdAt: '2023-08-26T14:59:13.000Z'
    data:
      edited: false
      editors:
      - Varunk29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9667457342147827
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f2632df040b1b55c59f3a38c5ebeff2.svg
          fullname: Varun Kumar
          isHf: false
          isPro: false
          name: Varunk29
          type: user
        html: '<p>I tried using 4 * 24 GB inference was very slow, Can you suggest
          the right gpu to run it on for fast inference</p>

          '
        raw: I tried using 4 * 24 GB inference was very slow, Can you suggest the
          right gpu to run it on for fast inference
        updatedAt: '2023-08-26T14:59:13.955Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mintisan
    id: 64ea1341213a0415bd0d0001
    type: comment
  author: Varunk29
  content: I tried using 4 * 24 GB inference was very slow, Can you suggest the right
    gpu to run it on for fast inference
  created_at: 2023-08-26 13:59:13+00:00
  edited: false
  hidden: false
  id: 64ea1341213a0415bd0d0001
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5be15bbe96f71f311a6c7b576ea4275c.svg
      fullname: Richard Gong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gongy
      type: user
    createdAt: '2023-08-26T16:20:52.000Z'
    data:
      edited: false
      editors:
      - gongy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8654083013534546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5be15bbe96f71f311a6c7b576ea4275c.svg
          fullname: Richard Gong
          isHf: false
          isPro: false
          name: gongy
          type: user
        html: '<p>I''m having success running it on a 80GB A100, generating about
          22 tokens/s (with up to around 10 concurrent requests). Seems to be working
          after bumping the latest vLLM and TGI versions.</p>

          <p>P.S. For GPU access I''m using Modal (disclaimer: I work at Modal) -
          there are a couple of examples (<a rel="nofollow" href="https://modal.com/docs/guide/ex/text_generation_inference">TGI</a>,
          <a rel="nofollow" href="https://modal.com/docs/guide/ex/vllm_inference">vLLM</a>)
          there for how to run this quickly.</p>

          '
        raw: 'I''m having success running it on a 80GB A100, generating about 22 tokens/s
          (with up to around 10 concurrent requests). Seems to be working after bumping
          the latest vLLM and TGI versions.


          P.S. For GPU access I''m using Modal (disclaimer: I work at Modal) - there
          are a couple of examples ([TGI](https://modal.com/docs/guide/ex/text_generation_inference),
          [vLLM](https://modal.com/docs/guide/ex/vllm_inference)) there for how to
          run this quickly.'
        updatedAt: '2023-08-26T16:20:52.946Z'
      numEdits: 0
      reactions: []
    id: 64ea2664d014af2062e19a63
    type: comment
  author: gongy
  content: 'I''m having success running it on a 80GB A100, generating about 22 tokens/s
    (with up to around 10 concurrent requests). Seems to be working after bumping
    the latest vLLM and TGI versions.


    P.S. For GPU access I''m using Modal (disclaimer: I work at Modal) - there are
    a couple of examples ([TGI](https://modal.com/docs/guide/ex/text_generation_inference),
    [vLLM](https://modal.com/docs/guide/ex/vllm_inference)) there for how to run this
    quickly.'
  created_at: 2023-08-26 15:20:52+00:00
  edited: false
  hidden: false
  id: 64ea2664d014af2062e19a63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667506924586-noauth.jpeg?w=200&h=200&f=face
      fullname: dieselbaby
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dieselbaby
      type: user
    createdAt: '2023-09-06T13:23:21.000Z'
    data:
      edited: false
      editors:
      - dieselbaby
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8769503831863403
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667506924586-noauth.jpeg?w=200&h=200&f=face
          fullname: dieselbaby
          isHf: false
          isPro: false
          name: dieselbaby
          type: user
        html: '<blockquote>

          <p>I''m having success running it on a 80GB A100, generating about 22 tokens/s
          (with up to around 10 concurrent requests). Seems to be working after bumping
          the latest vLLM and TGI versions.</p>

          <p>P.S. For GPU access I''m using Modal (disclaimer: I work at Modal) -
          there are a couple of examples (<a rel="nofollow" href="https://modal.com/docs/guide/ex/text_generation_inference">TGI</a>,
          <a rel="nofollow" href="https://modal.com/docs/guide/ex/vllm_inference">vLLM</a>)
          there for how to run this quickly.</p>

          </blockquote>

          <p>Thanks for the suggestion, quite helpful :)</p>

          '
        raw: "> I'm having success running it on a 80GB A100, generating about 22\
          \ tokens/s (with up to around 10 concurrent requests). Seems to be working\
          \ after bumping the latest vLLM and TGI versions.\n> \n> P.S. For GPU access\
          \ I'm using Modal (disclaimer: I work at Modal) - there are a couple of\
          \ examples ([TGI](https://modal.com/docs/guide/ex/text_generation_inference),\
          \ [vLLM](https://modal.com/docs/guide/ex/vllm_inference)) there for how\
          \ to run this quickly.\n\nThanks for the suggestion, quite helpful :)"
        updatedAt: '2023-09-06T13:23:21.970Z'
      numEdits: 0
      reactions: []
    id: 64f87d496389380c77833483
    type: comment
  author: dieselbaby
  content: "> I'm having success running it on a 80GB A100, generating about 22 tokens/s\
    \ (with up to around 10 concurrent requests). Seems to be working after bumping\
    \ the latest vLLM and TGI versions.\n> \n> P.S. For GPU access I'm using Modal\
    \ (disclaimer: I work at Modal) - there are a couple of examples ([TGI](https://modal.com/docs/guide/ex/text_generation_inference),\
    \ [vLLM](https://modal.com/docs/guide/ex/vllm_inference)) there for how to run\
    \ this quickly.\n\nThanks for the suggestion, quite helpful :)"
  created_at: 2023-09-06 12:23:21+00:00
  edited: false
  hidden: false
  id: 64f87d496389380c77833483
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-08T14:41:53.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9302111864089966
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Varunk29&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Varunk29\">@<span class=\"\
          underline\">Varunk29</span></a></span>\n\n\t</span></span> Are generate()\
          \ encode() functions from tokenizer and model thread safe?</p>\n<p>I also\
          \ want to concurrent inferences (from multiple threads on same model object)\
          \ but not sure if they are thread safe?</p>\n"
        raw: '@Varunk29 Are generate() encode() functions from tokenizer and model
          thread safe?


          I also want to concurrent inferences (from multiple threads on same model
          object) but not sure if they are thread safe?'
        updatedAt: '2023-09-08T14:41:53.969Z'
      numEdits: 0
      reactions: []
    id: 64fb32b1fa6446542264e557
    type: comment
  author: vermanic
  content: '@Varunk29 Are generate() encode() functions from tokenizer and model thread
    safe?


    I also want to concurrent inferences (from multiple threads on same model object)
    but not sure if they are thread safe?'
  created_at: 2023-09-08 13:41:53+00:00
  edited: false
  hidden: false
  id: 64fb32b1fa6446542264e557
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39c2366409150eaaddcebbb8b5fce878.svg
      fullname: "\u738B\u6668\u5EB7"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangchenkang2023
      type: user
    createdAt: '2023-09-14T07:00:43.000Z'
    data:
      edited: true
      editors:
      - wangchenkang2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9352243542671204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39c2366409150eaaddcebbb8b5fce878.svg
          fullname: "\u738B\u6668\u5EB7"
          isHf: false
          isPro: false
          name: wangchenkang2023
          type: user
        html: '<p>i tried running 4*v100(32G) inference was very slow, One inference
          takes 6 minutes.input token len 1700</p>

          '
        raw: i tried running 4*v100(32G) inference was very slow, One inference takes
          6 minutes.input token len 1700
        updatedAt: '2023-09-14T07:01:08.515Z'
      numEdits: 1
      reactions: []
    id: 6502af9b8e46888d673025df
    type: comment
  author: wangchenkang2023
  content: i tried running 4*v100(32G) inference was very slow, One inference takes
    6 minutes.input token len 1700
  created_at: 2023-09-14 06:00:43+00:00
  edited: true
  hidden: false
  id: 6502af9b8e46888d673025df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: codellama/CodeLlama-34b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: What is right GPU to run this
