!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nxnhjrjtbjfzhrovwl
conflicting_files: null
created_at: 2023-07-03 21:16:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14366c6dcc941238852438e6ffa8bf4e.svg
      fullname: nxnhjrjtbjfzhrovwl@nthrl.com
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nxnhjrjtbjfzhrovwl
      type: user
    createdAt: '2023-07-03T22:16:11.000Z'
    data:
      edited: false
      editors:
      - nxnhjrjtbjfzhrovwl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5238324999809265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14366c6dcc941238852438e6ffa8bf4e.svg
          fullname: nxnhjrjtbjfzhrovwl@nthrl.com
          isHf: false
          isPro: false
          name: nxnhjrjtbjfzhrovwl
          type: user
        html: '<p>any chance for 65b?</p>

          '
        raw: any chance for 65b?
        updatedAt: '2023-07-03T22:16:11.218Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - alexandrost
    id: 64a348ab0d5af4b1620c240d
    type: comment
  author: nxnhjrjtbjfzhrovwl
  content: any chance for 65b?
  created_at: 2023-07-03 21:16:11+00:00
  edited: false
  hidden: false
  id: 64a348ab0d5af4b1620c240d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-07-03T22:39:37.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597722887992859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>I''d like to. VRAM is the challenge given the sequence lengths in
          this dataset. I''ll see what I can do. </p>

          '
        raw: 'I''d like to. VRAM is the challenge given the sequence lengths in this
          dataset. I''ll see what I can do. '
        updatedAt: '2023-07-03T22:39:37.628Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Renegadesoffun
        - alexandrost
    id: 64a34e2913cb9732edbf5bb8
    type: comment
  author: bhenrym14
  content: 'I''d like to. VRAM is the challenge given the sequence lengths in this
    dataset. I''ll see what I can do. '
  created_at: 2023-07-03 21:39:37+00:00
  edited: false
  hidden: false
  id: 64a34e2913cb9732edbf5bb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
      fullname: Alexandros Triantafyllidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexandrost
      type: user
    createdAt: '2023-07-14T19:42:51.000Z'
    data:
      edited: false
      editors:
      - alexandrost
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.961166262626648
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
          fullname: Alexandros Triantafyllidis
          isHf: false
          isPro: false
          name: alexandrost
          type: user
        html: '<p>+1 here! Would love to see a 65b version. I have high hopes for
          it. It could be a strong contender.<br>Another idea: a 65b version with
          4k context. I wonder how that would stack up against GPT-3.5-Turbo (which
          also has a 4k-long context). Perhaps 4k (a middle ground) in combination
          with 65b params could be the sweet-spot!</p>

          '
        raw: '+1 here! Would love to see a 65b version. I have high hopes for it.
          It could be a strong contender.

          Another idea: a 65b version with 4k context. I wonder how that would stack
          up against GPT-3.5-Turbo (which also has a 4k-long context). Perhaps 4k
          (a middle ground) in combination with 65b params could be the sweet-spot!'
        updatedAt: '2023-07-14T19:42:51.560Z'
      numEdits: 0
      reactions: []
    id: 64b1a53b30274153f4c07a3d
    type: comment
  author: alexandrost
  content: '+1 here! Would love to see a 65b version. I have high hopes for it. It
    could be a strong contender.

    Another idea: a 65b version with 4k context. I wonder how that would stack up
    against GPT-3.5-Turbo (which also has a 4k-long context). Perhaps 4k (a middle
    ground) in combination with 65b params could be the sweet-spot!'
  created_at: 2023-07-14 18:42:51+00:00
  edited: false
  hidden: false
  id: 64b1a53b30274153f4c07a3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-07-14T20:16:12.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9252312183380127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>Actually, Ycros went ahead and did this! I haven''t experimented
          with it personally, but he basically took my code verbatim and trained a
          65b. See his repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder</p>

          <p>I have done several experiments since building this model that show substantial
          improvements. The model I uploaded yesterday incorporates a long sequence
          pretraining phase and attempts to extend to 16k tokens. Despite the longer
          context, it outperforms this model even at shorter context lengths. <a href="https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ">https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ</a></p>

          <p>I''ll likely train a 65b parameter model incorporating all these improvements
          soon. I like the idea of only extending to 4k, as it may be possible to
          do so with minimal damage to short context performance, and on the hardware
          I have.</p>

          '
        raw: 'Actually, Ycros went ahead and did this! I haven''t experimented with
          it personally, but he basically took my code verbatim and trained a 65b.
          See his repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder


          I have done several experiments since building this model that show substantial
          improvements. The model I uploaded yesterday incorporates a long sequence
          pretraining phase and attempts to extend to 16k tokens. Despite the longer
          context, it outperforms this model even at shorter context lengths. https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ


          I''ll likely train a 65b parameter model incorporating all these improvements
          soon. I like the idea of only extending to 4k, as it may be possible to
          do so with minimal damage to short context performance, and on the hardware
          I have.'
        updatedAt: '2023-07-14T20:16:12.428Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - alexandrost
    id: 64b1ad0ca17e4a05199a0a3b
    type: comment
  author: bhenrym14
  content: 'Actually, Ycros went ahead and did this! I haven''t experimented with
    it personally, but he basically took my code verbatim and trained a 65b. See his
    repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder


    I have done several experiments since building this model that show substantial
    improvements. The model I uploaded yesterday incorporates a long sequence pretraining
    phase and attempts to extend to 16k tokens. Despite the longer context, it outperforms
    this model even at shorter context lengths. https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ


    I''ll likely train a 65b parameter model incorporating all these improvements
    soon. I like the idea of only extending to 4k, as it may be possible to do so
    with minimal damage to short context performance, and on the hardware I have.'
  created_at: 2023-07-14 19:16:12+00:00
  edited: false
  hidden: false
  id: 64b1ad0ca17e4a05199a0a3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
      fullname: Alexandros Triantafyllidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexandrost
      type: user
    createdAt: '2023-07-14T21:10:47.000Z'
    data:
      edited: true
      editors:
      - alexandrost
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9392976760864258
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
          fullname: Alexandros Triantafyllidis
          isHf: false
          isPro: false
          name: alexandrost
          type: user
        html: '<blockquote>

          <p>Actually, Ycros went ahead and did this! I haven''t experimented with
          it personally, but he basically took my code verbatim and trained a 65b.
          See his repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder</p>

          <p>I have done several experiments since building this model that show substantial
          improvements. The model I uploaded yesterday incorporates a long sequence
          pretraining phase and attempts to extend to 16k tokens. Despite the longer
          context, it outperforms this model even at shorter context lengths. <a href="https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ">https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ</a></p>

          <p>I''ll likely train a 65b parameter model incorporating all these improvements
          soon. I like the idea of only extending to 4k, as it may be possible to
          do so with minimal damage to short context performance, and on the hardware
          I have.</p>

          </blockquote>

          <p>Thanks! I have already tried Ycros'' model but unfortunately it didn''t
          work in Oobabooga (neither ExLLama nor AutoGPTQ - unlike your 33b versions
          - my experiments were done on 2 x A6000s).<br>What type of hardware would
          you require to do the 65b 4k training vs the 65b 8k training?</p>

          '
        raw: "> Actually, Ycros went ahead and did this! I haven't experimented with\
          \ it personally, but he basically took my code verbatim and trained a 65b.\
          \ See his repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder\n\
          > \n> I have done several experiments since building this model that show\
          \ substantial improvements. The model I uploaded yesterday incorporates\
          \ a long sequence pretraining phase and attempts to extend to 16k tokens.\
          \ Despite the longer context, it outperforms this model even at shorter\
          \ context lengths. https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ\n\
          > \n> I'll likely train a 65b parameter model incorporating all these improvements\
          \ soon. I like the idea of only extending to 4k, as it may be possible to\
          \ do so with minimal damage to short context performance, and on the hardware\
          \ I have.\n\nThanks! I have already tried Ycros' model but unfortunately\
          \ it didn't work in Oobabooga (neither ExLLama nor AutoGPTQ - unlike your\
          \ 33b versions - my experiments were done on 2 x A6000s).\nWhat type of\
          \ hardware would you require to do the 65b 4k training vs the 65b 8k training?"
        updatedAt: '2023-07-14T21:10:58.242Z'
      numEdits: 1
      reactions: []
    id: 64b1b9d7ce290f8e9a70ae8c
    type: comment
  author: alexandrost
  content: "> Actually, Ycros went ahead and did this! I haven't experimented with\
    \ it personally, but he basically took my code verbatim and trained a 65b. See\
    \ his repo  ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder\n> \n> I\
    \ have done several experiments since building this model that show substantial\
    \ improvements. The model I uploaded yesterday incorporates a long sequence pretraining\
    \ phase and attempts to extend to 16k tokens. Despite the longer context, it outperforms\
    \ this model even at shorter context lengths. https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GPTQ\n\
    > \n> I'll likely train a 65b parameter model incorporating all these improvements\
    \ soon. I like the idea of only extending to 4k, as it may be possible to do so\
    \ with minimal damage to short context performance, and on the hardware I have.\n\
    \nThanks! I have already tried Ycros' model but unfortunately it didn't work in\
    \ Oobabooga (neither ExLLama nor AutoGPTQ - unlike your 33b versions - my experiments\
    \ were done on 2 x A6000s).\nWhat type of hardware would you require to do the\
    \ 65b 4k training vs the 65b 8k training?"
  created_at: 2023-07-14 20:10:47+00:00
  edited: true
  hidden: false
  id: 64b1b9d7ce290f8e9a70ae8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-07-14T21:57:59.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9779888987541199
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>I''ve heard of issues from others as well. Weird. </p>

          <p>The different hardware requirements are entirely down to the long sequence
          pre-training. I have a single RTX 6000 Ada (48gb) and it''s enough to pretrain
          65b to 2700 tokens. Based on my 33b experiments I think this is almost certainly
          enough for a 4k context extension. 8k might leave more on the table. I don''t
          know. A proper 8k pretrain would likely require at least another 24gb of
          VRAM; probably much more. Maybe 2x A6000 could do it but there''s certainly
          no guarantee.</p>

          <p>How far beyond 4k are you able to go with ExLLama and only one of your
          A6000s?</p>

          '
        raw: "I've heard of issues from others as well. Weird. \n\nThe different hardware\
          \ requirements are entirely down to the long sequence pre-training. I have\
          \ a single RTX 6000 Ada (48gb) and it's enough to pretrain 65b to 2700 tokens.\
          \ Based on my 33b experiments I think this is almost certainly enough for\
          \ a 4k context extension. 8k might leave more on the table. I don't know.\
          \ A proper 8k pretrain would likely require at least another 24gb of VRAM;\
          \ probably much more. Maybe 2x A6000 could do it but there's certainly no\
          \ guarantee.\n\nHow far beyond 4k are you able to go with ExLLama and only\
          \ one of your A6000s?\n"
        updatedAt: '2023-07-14T21:57:59.685Z'
      numEdits: 0
      reactions: []
    id: 64b1c4e79826b935a2b3707a
    type: comment
  author: bhenrym14
  content: "I've heard of issues from others as well. Weird. \n\nThe different hardware\
    \ requirements are entirely down to the long sequence pre-training. I have a single\
    \ RTX 6000 Ada (48gb) and it's enough to pretrain 65b to 2700 tokens. Based on\
    \ my 33b experiments I think this is almost certainly enough for a 4k context\
    \ extension. 8k might leave more on the table. I don't know. A proper 8k pretrain\
    \ would likely require at least another 24gb of VRAM; probably much more. Maybe\
    \ 2x A6000 could do it but there's certainly no guarantee.\n\nHow far beyond 4k\
    \ are you able to go with ExLLama and only one of your A6000s?\n"
  created_at: 2023-07-14 20:57:59+00:00
  edited: false
  hidden: false
  id: 64b1c4e79826b935a2b3707a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
      fullname: Alexandros Triantafyllidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexandrost
      type: user
    createdAt: '2023-07-14T22:38:35.000Z'
    data:
      edited: false
      editors:
      - alexandrost
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9474329352378845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
          fullname: Alexandros Triantafyllidis
          isHf: false
          isPro: false
          name: alexandrost
          type: user
        html: '<blockquote>

          <p>I''ve heard of issues from others as well. Weird. </p>

          <p>The different hardware requirements are entirely down to the long sequence
          pre-training. I have a single RTX 6000 Ada (48gb) and it''s enough to pretrain
          65b to 2700 tokens. Based on my 33b experiments I think this is almost certainly
          enough for a 4k context extension. 8k might leave more on the table. I don''t
          know. A proper 8k pretrain would likely require at least another 24gb of
          VRAM; probably much more. Maybe 2x A6000 could do it but there''s certainly
          no guarantee.</p>

          <p>How far beyond 4k are you able to go with ExLLama and only one of your
          A6000s?</p>

          </blockquote>

          <p>oh interesting.<br>If "how far beyond 4k" refers to inference (I haven''t
          done any finetuning/training yet), then the highest I''ve been able to successfully
          load in ExLLama is 16k with your bhenrym14_airoboros-33b-gpt4-1.4.1-NTK-16384-GPTQ
          model. It consumes 42530MiB / 49140MiB on a single A6000. However, it doesn''t
          produce very nice results. It stops early (tends to generate only 6-7 words
          at a time) and also makes grammar errors. The bhenrym14_airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ
          one seems to be working fine though (consuming around 30968MiB on a single
          A6000)</p>

          <p>On the other hand, Ycros'' 65B 8K model does not even load: for some
          reason, even though I set gpu-split to something like 46,46 in Oobabooga''s
          ExLlama configuration, it seems to completely ignore the second GPU. If
          on the other hand I try to load it with AutoGPTQ, it loads, but simply produces
          nonsensical output.</p>

          '
        raw: "> I've heard of issues from others as well. Weird. \n> \n> The different\
          \ hardware requirements are entirely down to the long sequence pre-training.\
          \ I have a single RTX 6000 Ada (48gb) and it's enough to pretrain 65b to\
          \ 2700 tokens. Based on my 33b experiments I think this is almost certainly\
          \ enough for a 4k context extension. 8k might leave more on the table. I\
          \ don't know. A proper 8k pretrain would likely require at least another\
          \ 24gb of VRAM; probably much more. Maybe 2x A6000 could do it but there's\
          \ certainly no guarantee.\n> \n> How far beyond 4k are you able to go with\
          \ ExLLama and only one of your A6000s?\n\noh interesting. \nIf \"how far\
          \ beyond 4k\" refers to inference (I haven't done any finetuning/training\
          \ yet), then the highest I've been able to successfully load in ExLLama\
          \ is 16k with your bhenrym14_airoboros-33b-gpt4-1.4.1-NTK-16384-GPTQ model.\
          \ It consumes 42530MiB / 49140MiB on a single A6000. However, it doesn't\
          \ produce very nice results. It stops early (tends to generate only 6-7\
          \ words at a time) and also makes grammar errors. The bhenrym14_airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ\
          \ one seems to be working fine though (consuming around 30968MiB on a single\
          \ A6000)\n\nOn the other hand, Ycros' 65B 8K model does not even load: for\
          \ some reason, even though I set gpu-split to something like 46,46 in Oobabooga's\
          \ ExLlama configuration, it seems to completely ignore the second GPU. If\
          \ on the other hand I try to load it with AutoGPTQ, it loads, but simply\
          \ produces nonsensical output.\n"
        updatedAt: '2023-07-14T22:38:35.540Z'
      numEdits: 0
      reactions: []
    id: 64b1ce6b0a54158d66e6ca3e
    type: comment
  author: alexandrost
  content: "> I've heard of issues from others as well. Weird. \n> \n> The different\
    \ hardware requirements are entirely down to the long sequence pre-training. I\
    \ have a single RTX 6000 Ada (48gb) and it's enough to pretrain 65b to 2700 tokens.\
    \ Based on my 33b experiments I think this is almost certainly enough for a 4k\
    \ context extension. 8k might leave more on the table. I don't know. A proper\
    \ 8k pretrain would likely require at least another 24gb of VRAM; probably much\
    \ more. Maybe 2x A6000 could do it but there's certainly no guarantee.\n> \n>\
    \ How far beyond 4k are you able to go with ExLLama and only one of your A6000s?\n\
    \noh interesting. \nIf \"how far beyond 4k\" refers to inference (I haven't done\
    \ any finetuning/training yet), then the highest I've been able to successfully\
    \ load in ExLLama is 16k with your bhenrym14_airoboros-33b-gpt4-1.4.1-NTK-16384-GPTQ\
    \ model. It consumes 42530MiB / 49140MiB on a single A6000. However, it doesn't\
    \ produce very nice results. It stops early (tends to generate only 6-7 words\
    \ at a time) and also makes grammar errors. The bhenrym14_airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ\
    \ one seems to be working fine though (consuming around 30968MiB on a single A6000)\n\
    \nOn the other hand, Ycros' 65B 8K model does not even load: for some reason,\
    \ even though I set gpu-split to something like 46,46 in Oobabooga's ExLlama configuration,\
    \ it seems to completely ignore the second GPU. If on the other hand I try to\
    \ load it with AutoGPTQ, it loads, but simply produces nonsensical output.\n"
  created_at: 2023-07-14 21:38:35+00:00
  edited: false
  hidden: false
  id: 64b1ce6b0a54158d66e6ca3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-07-14T23:10:41.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9450916051864624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>Your experience with the 16k NTK model is consistent with mine and
          others''. Thanks for the feedback on that. The alpha scaling parameter doesn''t
          appear to correspond to the theoretical scaling multiple very well. Seems
          like it only manages out to 8-10k before it falls apart. <a rel="nofollow"
          href="https://github.com/jquesnelle/scaled-rope/tree/master">These guys</a>
          are doing good work on this. They''ve incorporated each scaling method in
          a more transparent and sophisticated way that should improve this behavior
          and more.</p>

          <p>Inference is indeed what I meant. I''m just trying to get a sense of
          whether 65b at 8k would even be usable for many people (including myself!)
          I''ll do some tests here when I get a chance; pending that I can kick off
          65b training.</p>

          '
        raw: 'Your experience with the 16k NTK model is consistent with mine and others''.
          Thanks for the feedback on that. The alpha scaling parameter doesn''t appear
          to correspond to the theoretical scaling multiple very well. Seems like
          it only manages out to 8-10k before it falls apart. [These guys](https://github.com/jquesnelle/scaled-rope/tree/master)
          are doing good work on this. They''ve incorporated each scaling method in
          a more transparent and sophisticated way that should improve this behavior
          and more.


          Inference is indeed what I meant. I''m just trying to get a sense of whether
          65b at 8k would even be usable for many people (including myself!) I''ll
          do some tests here when I get a chance; pending that I can kick off 65b
          training.'
        updatedAt: '2023-07-14T23:10:41.121Z'
      numEdits: 0
      reactions: []
    id: 64b1d5f19826b935a2b528e1
    type: comment
  author: bhenrym14
  content: 'Your experience with the 16k NTK model is consistent with mine and others''.
    Thanks for the feedback on that. The alpha scaling parameter doesn''t appear to
    correspond to the theoretical scaling multiple very well. Seems like it only manages
    out to 8-10k before it falls apart. [These guys](https://github.com/jquesnelle/scaled-rope/tree/master)
    are doing good work on this. They''ve incorporated each scaling method in a more
    transparent and sophisticated way that should improve this behavior and more.


    Inference is indeed what I meant. I''m just trying to get a sense of whether 65b
    at 8k would even be usable for many people (including myself!) I''ll do some tests
    here when I get a chance; pending that I can kick off 65b training.'
  created_at: 2023-07-14 22:10:41+00:00
  edited: false
  hidden: false
  id: 64b1d5f19826b935a2b528e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
      fullname: Alexandros Triantafyllidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexandrost
      type: user
    createdAt: '2023-07-14T23:16:25.000Z'
    data:
      edited: false
      editors:
      - alexandrost
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444467425346375
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
          fullname: Alexandros Triantafyllidis
          isHf: false
          isPro: false
          name: alexandrost
          type: user
        html: '<blockquote>

          <p>Your experience with the 16k NTK model is consistent with mine and others''.
          Thanks for the feedback on that. The alpha scaling parameter doesn''t appear
          to correspond to the theoretical scaling multiple very well. Seems like
          it only manages out to 8-10k before it falls apart. <a rel="nofollow" href="https://github.com/jquesnelle/scaled-rope/tree/master">These
          guys</a> are doing good work on this. They''ve incorporated each scaling
          method in a more transparent and sophisticated way that should improve this
          behavior and more.</p>

          <p>Inference is indeed what I meant. I''m just trying to get a sense of
          whether 65b at 8k would even be usable for many people (including myself!)
          I''ll do some tests here when I get a chance; pending that I can kick off
          65b training.</p>

          </blockquote>

          <p>sounds good. Meanwhile, I''ll try to figure out what''s the deal with
          ExLLama in Oobabooga ignoring the 2nd GPU (try to isolate the issue: whether
          it is due to ExLLama or Oobabooga or both)which prevents me from loading
          a 65b with 8k context.</p>

          <p>On the other hand, if a 65b with 4k context would fit in a single A6000,
          that could be a killer combo.</p>

          '
        raw: "> Your experience with the 16k NTK model is consistent with mine and\
          \ others'. Thanks for the feedback on that. The alpha scaling parameter\
          \ doesn't appear to correspond to the theoretical scaling multiple very\
          \ well. Seems like it only manages out to 8-10k before it falls apart. [These\
          \ guys](https://github.com/jquesnelle/scaled-rope/tree/master) are doing\
          \ good work on this. They've incorporated each scaling method in a more\
          \ transparent and sophisticated way that should improve this behavior and\
          \ more.\n> \n> Inference is indeed what I meant. I'm just trying to get\
          \ a sense of whether 65b at 8k would even be usable for many people (including\
          \ myself!) I'll do some tests here when I get a chance; pending that I can\
          \ kick off 65b training.\n\nsounds good. Meanwhile, I'll try to figure out\
          \ what's the deal with ExLLama in Oobabooga ignoring the 2nd GPU (try to\
          \ isolate the issue: whether it is due to ExLLama or Oobabooga or both)which\
          \ prevents me from loading a 65b with 8k context.\n\nOn the other hand,\
          \ if a 65b with 4k context would fit in a single A6000, that could be a\
          \ killer combo."
        updatedAt: '2023-07-14T23:16:25.625Z'
      numEdits: 0
      reactions: []
    id: 64b1d749966b28317e167668
    type: comment
  author: alexandrost
  content: "> Your experience with the 16k NTK model is consistent with mine and others'.\
    \ Thanks for the feedback on that. The alpha scaling parameter doesn't appear\
    \ to correspond to the theoretical scaling multiple very well. Seems like it only\
    \ manages out to 8-10k before it falls apart. [These guys](https://github.com/jquesnelle/scaled-rope/tree/master)\
    \ are doing good work on this. They've incorporated each scaling method in a more\
    \ transparent and sophisticated way that should improve this behavior and more.\n\
    > \n> Inference is indeed what I meant. I'm just trying to get a sense of whether\
    \ 65b at 8k would even be usable for many people (including myself!) I'll do some\
    \ tests here when I get a chance; pending that I can kick off 65b training.\n\n\
    sounds good. Meanwhile, I'll try to figure out what's the deal with ExLLama in\
    \ Oobabooga ignoring the 2nd GPU (try to isolate the issue: whether it is due\
    \ to ExLLama or Oobabooga or both)which prevents me from loading a 65b with 8k\
    \ context.\n\nOn the other hand, if a 65b with 4k context would fit in a single\
    \ A6000, that could be a killer combo."
  created_at: 2023-07-14 22:16:25+00:00
  edited: false
  hidden: false
  id: 64b1d749966b28317e167668
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
      fullname: 'No'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: tjbortz
      type: user
    createdAt: '2023-07-16T16:09:09.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
          fullname: 'No'
          isHf: false
          isPro: true
          name: tjbortz
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-16T16:11:45.321Z'
      numEdits: 0
      reactions: []
    id: 64b416254dd3e248951b7800
    type: comment
  author: tjbortz
  content: This comment has been hidden
  created_at: 2023-07-16 15:09:09+00:00
  edited: true
  hidden: true
  id: 64b416254dd3e248951b7800
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
      fullname: 'No'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: tjbortz
      type: user
    createdAt: '2023-07-16T16:26:10.000Z'
    data:
      edited: false
      editors:
      - tjbortz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9038261771202087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
          fullname: 'No'
          isHf: false
          isPro: true
          name: tjbortz
          type: user
        html: '<p>In my experience the whole of attention is dropped onto GPU1 and
          if there isn''t enough room you get an instant OOM.  I have to do a 8,24
          load on my dual 3090-esque system in order to get 30b 8k to work, otherwise
          the attention and model on GPU1 together causes it to crash out.   See what
          happens if you cut back the layer count on GPU0. </p>

          '
        raw: 'In my experience the whole of attention is dropped onto GPU1 and if
          there isn''t enough room you get an instant OOM.  I have to do a 8,24 load
          on my dual 3090-esque system in order to get 30b 8k to work, otherwise the
          attention and model on GPU1 together causes it to crash out.   See what
          happens if you cut back the layer count on GPU0. '
        updatedAt: '2023-07-16T16:26:10.325Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - alexandrost
    id: 64b41a22f249713053a77e5e
    type: comment
  author: tjbortz
  content: 'In my experience the whole of attention is dropped onto GPU1 and if there
    isn''t enough room you get an instant OOM.  I have to do a 8,24 load on my dual
    3090-esque system in order to get 30b 8k to work, otherwise the attention and
    model on GPU1 together causes it to crash out.   See what happens if you cut back
    the layer count on GPU0. '
  created_at: 2023-07-16 15:26:10+00:00
  edited: false
  hidden: false
  id: 64b41a22f249713053a77e5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
      fullname: Alexandros Triantafyllidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexandrost
      type: user
    createdAt: '2023-07-16T21:27:35.000Z'
    data:
      edited: false
      editors:
      - alexandrost
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280098080635071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a32ec0902b536b04256ef481fd46b6c.svg
          fullname: Alexandros Triantafyllidis
          isHf: false
          isPro: false
          name: alexandrost
          type: user
        html: '<blockquote>

          <p>In my experience the whole of attention is dropped onto GPU1 and if there
          isn''t enough room you get an instant OOM.  I have to do a 8,24 load on
          my dual 3090-esque system in order to get 30b 8k to work, otherwise the
          attention and model on GPU1 together causes it to crash out.   See what
          happens if you cut back the layer count on GPU0.</p>

          </blockquote>

          <p>Wow, thanks, it worked now!!<br>Can you please explain in a bit more
          detail why that happens?</p>

          '
        raw: '> In my experience the whole of attention is dropped onto GPU1 and if
          there isn''t enough room you get an instant OOM.  I have to do a 8,24 load
          on my dual 3090-esque system in order to get 30b 8k to work, otherwise the
          attention and model on GPU1 together causes it to crash out.   See what
          happens if you cut back the layer count on GPU0.


          Wow, thanks, it worked now!!

          Can you please explain in a bit more detail why that happens?'
        updatedAt: '2023-07-16T21:27:35.135Z'
      numEdits: 0
      reactions: []
    id: 64b460c7f44fd95749249ec9
    type: comment
  author: alexandrost
  content: '> In my experience the whole of attention is dropped onto GPU1 and if
    there isn''t enough room you get an instant OOM.  I have to do a 8,24 load on
    my dual 3090-esque system in order to get 30b 8k to work, otherwise the attention
    and model on GPU1 together causes it to crash out.   See what happens if you cut
    back the layer count on GPU0.


    Wow, thanks, it worked now!!

    Can you please explain in a bit more detail why that happens?'
  created_at: 2023-07-16 20:27:35+00:00
  edited: false
  hidden: false
  id: 64b460c7f44fd95749249ec9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
      fullname: 'No'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: tjbortz
      type: user
    createdAt: '2023-07-22T04:59:46.000Z'
    data:
      edited: false
      editors:
      - tjbortz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9528602957725525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14d5b2677b38f4683fdb3d327fda78ae.svg
          fullname: 'No'
          isHf: false
          isPro: true
          name: tjbortz
          type: user
        html: '<blockquote>

          <blockquote>

          <p>In my experience the whole of attention is dropped onto GPU1 and if there
          isn''t enough room you get an instant OOM.  I have to do a 8,24 load on
          my dual 3090-esque system in order to get 30b 8k to work, otherwise the
          attention and model on GPU1 together causes it to crash out.   See what
          happens if you cut back the layer count on GPU0.</p>

          </blockquote>

          <p>Wow, thanks, it worked now!!<br>Can you please explain in a bit more
          detail why that happens?</p>

          </blockquote>

          <p>No clue.  I only know context seems to all go on GPU1.  LLAMA70b has
          a new fancy context system that''s 1/4th its old size so it''s much less
          an issue.</p>

          '
        raw: "> > In my experience the whole of attention is dropped onto GPU1 and\
          \ if there isn't enough room you get an instant OOM.  I have to do a 8,24\
          \ load on my dual 3090-esque system in order to get 30b 8k to work, otherwise\
          \ the attention and model on GPU1 together causes it to crash out.   See\
          \ what happens if you cut back the layer count on GPU0.\n> \n> Wow, thanks,\
          \ it worked now!!\n> Can you please explain in a bit more detail why that\
          \ happens?\n\nNo clue.  I only know context seems to all go on GPU1.  LLAMA70b\
          \ has a new fancy context system that's 1/4th its old size so it's much\
          \ less an issue."
        updatedAt: '2023-07-22T04:59:46.940Z'
      numEdits: 0
      reactions: []
    id: 64bb624278b89c4aa4c6d1d9
    type: comment
  author: tjbortz
  content: "> > In my experience the whole of attention is dropped onto GPU1 and if\
    \ there isn't enough room you get an instant OOM.  I have to do a 8,24 load on\
    \ my dual 3090-esque system in order to get 30b 8k to work, otherwise the attention\
    \ and model on GPU1 together causes it to crash out.   See what happens if you\
    \ cut back the layer count on GPU0.\n> \n> Wow, thanks, it worked now!!\n> Can\
    \ you please explain in a bit more detail why that happens?\n\nNo clue.  I only\
    \ know context seems to all go on GPU1.  LLAMA70b has a new fancy context system\
    \ that's 1/4th its old size so it's much less an issue."
  created_at: 2023-07-22 03:59:46+00:00
  edited: false
  hidden: false
  id: 64bb624278b89c4aa4c6d1d9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ
repo_type: model
status: open
target_branch: null
title: 65B
