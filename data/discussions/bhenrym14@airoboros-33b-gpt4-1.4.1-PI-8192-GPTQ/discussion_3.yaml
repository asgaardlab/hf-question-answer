!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shouyi987
conflicting_files: null
created_at: 2023-07-03 21:55:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
      fullname: Shouyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shouyi987
      type: user
    createdAt: '2023-07-03T22:55:08.000Z'
    data:
      edited: true
      editors:
      - Shouyi987
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9354010224342346
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
          fullname: Shouyi Wang
          isHf: false
          isPro: false
          name: Shouyi987
          type: user
        html: '<p>In the model card, it states to set the max_seq_len to 8192 and
          compress_pos_emb to 4.</p>

          <p>However, there are comparisons of perplexities for context sizes of 2048
          and 3072. How did you do that? Did you set the context size to these numbers
          when loading the model and then compare the perplexities?</p>

          <p>Aren''t these context sizes expected to have poor perplexities?</p>

          '
        raw: 'In the model card, it states to set the max_seq_len to 8192 and compress_pos_emb
          to 4.


          However, there are comparisons of perplexities for context sizes of 2048
          and 3072. How did you do that? Did you set the context size to these numbers
          when loading the model and then compare the perplexities?


          Aren''t these context sizes expected to have poor perplexities?'
        updatedAt: '2023-07-03T23:02:25.401Z'
      numEdits: 1
      reactions: []
    id: 64a351cc565496b6297de803
    type: comment
  author: Shouyi987
  content: 'In the model card, it states to set the max_seq_len to 8192 and compress_pos_emb
    to 4.


    However, there are comparisons of perplexities for context sizes of 2048 and 3072.
    How did you do that? Did you set the context size to these numbers when loading
    the model and then compare the perplexities?


    Aren''t these context sizes expected to have poor perplexities?'
  created_at: 2023-07-03 21:55:08+00:00
  edited: true
  hidden: false
  id: 64a351cc565496b6297de803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
      fullname: Shouyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shouyi987
      type: user
    createdAt: '2023-07-03T22:58:59.000Z'
    data:
      from: What size should I set for the context?
      to: How did you calcualte the perplexities for 2048 and 3072 contexts?
    id: 64a352b34c758629716088b3
    type: title-change
  author: Shouyi987
  created_at: 2023-07-03 21:58:59+00:00
  id: 64a352b34c758629716088b3
  new_title: How did you calcualte the perplexities for 2048 and 3072 contexts?
  old_title: What size should I set for the context?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-07-03T23:20:55.000Z'
    data:
      edited: true
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9359288215637207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: "<p>I used the perplexity tool in <code>oobabooga text-generation-webui</code>.\
          \ It's computed over wikitext with the context window set at either 2048\
          \ or 3072, strided by 512 tokens. </p>\n<p>At train time, the RoPE scaling\
          \ was set with max_position_embeddings = 8192 and a scaling factor of 4.\
          \ This is what is used for <strong>all</strong> the perplexity calculations.</p>\n\
          <p>The perplexity calculations differ in the size of the sequence upon which\
          \ it's evaluated. The point of this is to confirm the perplexity doesn't\
          \ blow up beyond 2048, and that it can potentially outperform the SuperHOT\
          \ LoRA (as applied to this model when trained without RoPE scaling ). Early\
          \ feedback is that this model stays quite coherent out to it's limit of\
          \ 8192.</p>\n<p>I would have run the perplexity calculations all the way\
          \ out to 8192, but I ran into VRAM limits. You need ExLlama to run full\
          \ context with 48gb VRAM, and currently the perplexity tool isn\u2019t compatible\
          \ with it.</p>\n"
        raw: "I used the perplexity tool in `oobabooga text-generation-webui`. It's\
          \ computed over wikitext with the context window set at either 2048 or 3072,\
          \ strided by 512 tokens. \n\nAt train time, the RoPE scaling was set with\
          \ max_position_embeddings = 8192 and a scaling factor of 4. This is what\
          \ is used for **all** the perplexity calculations.\n\nThe perplexity calculations\
          \ differ in the size of the sequence upon which it's evaluated. The point\
          \ of this is to confirm the perplexity doesn't blow up beyond 2048, and\
          \ that it can potentially outperform the SuperHOT LoRA (as applied to this\
          \ model when trained without RoPE scaling ). Early feedback is that this\
          \ model stays quite coherent out to it's limit of 8192.\n\nI would have\
          \ run the perplexity calculations all the way out to 8192, but I ran into\
          \ VRAM limits. You need ExLlama to run full context with 48gb VRAM, and\
          \ currently the perplexity tool isn\u2019t compatible with it."
        updatedAt: '2023-07-03T23:42:03.915Z'
      numEdits: 1
      reactions: []
    id: 64a357d74c75862971610d60
    type: comment
  author: bhenrym14
  content: "I used the perplexity tool in `oobabooga text-generation-webui`. It's\
    \ computed over wikitext with the context window set at either 2048 or 3072, strided\
    \ by 512 tokens. \n\nAt train time, the RoPE scaling was set with max_position_embeddings\
    \ = 8192 and a scaling factor of 4. This is what is used for **all** the perplexity\
    \ calculations.\n\nThe perplexity calculations differ in the size of the sequence\
    \ upon which it's evaluated. The point of this is to confirm the perplexity doesn't\
    \ blow up beyond 2048, and that it can potentially outperform the SuperHOT LoRA\
    \ (as applied to this model when trained without RoPE scaling ). Early feedback\
    \ is that this model stays quite coherent out to it's limit of 8192.\n\nI would\
    \ have run the perplexity calculations all the way out to 8192, but I ran into\
    \ VRAM limits. You need ExLlama to run full context with 48gb VRAM, and currently\
    \ the perplexity tool isn\u2019t compatible with it."
  created_at: 2023-07-03 22:20:55+00:00
  edited: true
  hidden: false
  id: 64a357d74c75862971610d60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
      fullname: Shouyi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shouyi987
      type: user
    createdAt: '2023-07-04T04:00:10.000Z'
    data:
      edited: true
      editors:
      - Shouyi987
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8923567533493042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c660f95127592ce4241b04ce3ec47c46.svg
          fullname: Shouyi Wang
          isHf: false
          isPro: false
          name: Shouyi987
          type: user
        html: '<p>Thank you so much for your explaination!</p>

          '
        raw: Thank you so much for your explaination!
        updatedAt: '2023-07-04T04:00:18.897Z'
      numEdits: 1
      reactions: []
    id: 64a3994a73f3ad435c38b1cb
    type: comment
  author: Shouyi987
  content: Thank you so much for your explaination!
  created_at: 2023-07-04 03:00:10+00:00
  edited: true
  hidden: false
  id: 64a3994a73f3ad435c38b1cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-25T21:15:58.000Z'
    data:
      status: closed
    id: 64e91a0eb96ff0e17514559f
    type: status-change
  author: bhenrym14
  created_at: 2023-08-25 20:15:58+00:00
  id: 64e91a0eb96ff0e17514559f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ
repo_type: model
status: closed
target_branch: null
title: How did you calcualte the perplexities for 2048 and 3072 contexts?
