!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mjw98
conflicting_files: null
created_at: 2023-10-10 06:32:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-10T07:32:07.000Z'
    data:
      edited: false
      editors:
      - mjw98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8688474297523499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
          fullname: meijingwu
          isHf: false
          isPro: false
          name: mjw98
          type: user
        html: '<p>Thanks for your great work. I would like to ask you about your process
          of quantification using gptq? I just used autogptq for quantisation, but
          I can''t load it after quantisation using AutoModelForCausalLM.from_pretrained().
          It can only be loaded using AutoGPTQForCausalLM.from_quantised, can you
          tell me where your modification is?</p>

          '
        raw: Thanks for your great work. I would like to ask you about your process
          of quantification using gptq? I just used autogptq for quantisation, but
          I can't load it after quantisation using AutoModelForCausalLM.from_pretrained().
          It can only be loaded using AutoGPTQForCausalLM.from_quantised, can you
          tell me where your modification is?
        updatedAt: '2023-10-10T07:32:07.081Z'
      numEdits: 0
      reactions: []
    id: 6524fdf7e5d47a2f52d2ecdb
    type: comment
  author: mjw98
  content: Thanks for your great work. I would like to ask you about your process
    of quantification using gptq? I just used autogptq for quantisation, but I can't
    load it after quantisation using AutoModelForCausalLM.from_pretrained(). It can
    only be loaded using AutoGPTQForCausalLM.from_quantised, can you tell me where
    your modification is?
  created_at: 2023-10-10 06:32:07+00:00
  edited: false
  hidden: false
  id: 6524fdf7e5d47a2f52d2ecdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-10T09:02:51.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9376737475395203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Probably just a typo, correct me if I''m wrong, hf transformer does
          not support gptq since hf has bitsandbytes 4 bit integrated</p>

          '
        raw: Probably just a typo, correct me if I'm wrong, hf transformer does not
          support gptq since hf has bitsandbytes 4 bit integrated
        updatedAt: '2023-10-10T09:02:51.428Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - mjw98
    id: 6525133b1397c1bc3968f56c
    type: comment
  author: Yhyu13
  content: Probably just a typo, correct me if I'm wrong, hf transformer does not
    support gptq since hf has bitsandbytes 4 bit integrated
  created_at: 2023-10-10 08:02:51+00:00
  edited: false
  hidden: false
  id: 6525133b1397c1bc3968f56c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-10T09:36:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8210521340370178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hugging Face Transformers has supported GPTQ for a while now - at\
          \ least six weeks.  All my GPTQ examples use Transformers directly now.\
          \  It uses AutoGPTQ for the kernels, so AutoGPTQ is still a required install.</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;mjw98&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mjw98\">@<span class=\"\
          underline\">mjw98</span></a></span>\n\n\t</span></span> Your issue is likely\
          \ the model name. By default, AutoGPTQ saves the model with a name like\
          \ <code>gptq-4bit-128g.safetensors</code>.  This name cannot be loaded by\
          \ Transformers. Transformers requires that the model is called <code>model.safetensors</code>.</p>\n\
          <p>When making your GPTQ model with AutoGPTQ, pass <code>model_basename=\"\
          model\"</code> and it will work.</p>\n<p>Check out my simple AutoGPTQ wrapper\
          \ script: <a rel=\"nofollow\" href=\"https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py\"\
          >https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py</a>\
          \ - it will set the basename to <code>model</code> automatically, so the\
          \ output will be compatible with Transformers.</p>\n"
        raw: 'Hugging Face Transformers has supported GPTQ for a while now - at least
          six weeks.  All my GPTQ examples use Transformers directly now.  It uses
          AutoGPTQ for the kernels, so AutoGPTQ is still a required install.


          @mjw98 Your issue is likely the model name. By default, AutoGPTQ saves the
          model with a name like `gptq-4bit-128g.safetensors`.  This name cannot be
          loaded by Transformers. Transformers requires that the model is called `model.safetensors`.


          When making your GPTQ model with AutoGPTQ, pass `model_basename="model"`
          and it will work.


          Check out my simple AutoGPTQ wrapper script: https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py
          - it will set the basename to `model` automatically, so the output will
          be compatible with Transformers.'
        updatedAt: '2023-10-10T09:36:15.408Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mjw98
    id: 65251b0fc4edc1682028ff60
    type: comment
  author: TheBloke
  content: 'Hugging Face Transformers has supported GPTQ for a while now - at least
    six weeks.  All my GPTQ examples use Transformers directly now.  It uses AutoGPTQ
    for the kernels, so AutoGPTQ is still a required install.


    @mjw98 Your issue is likely the model name. By default, AutoGPTQ saves the model
    with a name like `gptq-4bit-128g.safetensors`.  This name cannot be loaded by
    Transformers. Transformers requires that the model is called `model.safetensors`.


    When making your GPTQ model with AutoGPTQ, pass `model_basename="model"` and it
    will work.


    Check out my simple AutoGPTQ wrapper script: https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py
    - it will set the basename to `model` automatically, so the output will be compatible
    with Transformers.'
  created_at: 2023-10-10 08:36:15+00:00
  edited: false
  hidden: false
  id: 65251b0fc4edc1682028ff60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-11T14:44:10.000Z'
    data:
      edited: true
      editors:
      - mjw98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8154604434967041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
          fullname: meijingwu
          isHf: false
          isPro: false
          name: mjw98
          type: user
        html: "<p>Thank you very much for your suggestion, but I still don't understand\
          \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
          \ you provided, but the output is still gptq-4bit-128g.safetensors. I use\
          \ the following code\uFF0Cpython quant_autogptq.py &nbsp;\"/content/llama-1B\"\
          \ \"777\" \"c4\" . I used the following code to quantize it and got the\
          \ result as shown below<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6524fa3c66ebe05198782971/Asw8AWePPIqrAqwx_skoo.png\"\
          ><img alt=\"1bf832de4aaacf82ad1b49199fd3b2e.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6524fa3c66ebe05198782971/Asw8AWePPIqrAqwx_skoo.png\"\
          ></a><br>My guess is to change the safetensors\u2019 filename and change\
          \ \"model_file_base_name\": \"gptq_model-4bit-128g\" to \"model_file_base_name\"\
          : \"model\" in quantize_config.json, hope to get your guidance.</p>\n"
        raw: "Thank you very much for your suggestion, but I still don't understand\
          \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
          \ you provided, but the output is still gptq-4bit-128g.safetensors. I use\
          \ the following code\uFF0Cpython quant_autogptq.py \_\"/content/llama-1B\"\
          \ \"777\" \"c4\" . I used the following code to quantize it and got the\
          \ result as shown below\n![1bf832de4aaacf82ad1b49199fd3b2e.png](https://cdn-uploads.huggingface.co/production/uploads/6524fa3c66ebe05198782971/Asw8AWePPIqrAqwx_skoo.png)\n\
          My guess is to change the safetensors\u2019 filename and change \"model_file_base_name\"\
          : \"gptq_model-4bit-128g\" to \"model_file_base_name\": \"model\" in quantize_config.json,\
          \ hope to get your guidance.\n"
        updatedAt: '2023-10-11T14:49:00.383Z'
      numEdits: 1
      reactions: []
    id: 6526b4ba999898ced81a2bef
    type: comment
  author: mjw98
  content: "Thank you very much for your suggestion, but I still don't understand\
    \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
    \ you provided, but the output is still gptq-4bit-128g.safetensors. I use the\
    \ following code\uFF0Cpython quant_autogptq.py \_\"/content/llama-1B\" \"777\"\
    \ \"c4\" . I used the following code to quantize it and got the result as shown\
    \ below\n![1bf832de4aaacf82ad1b49199fd3b2e.png](https://cdn-uploads.huggingface.co/production/uploads/6524fa3c66ebe05198782971/Asw8AWePPIqrAqwx_skoo.png)\n\
    My guess is to change the safetensors\u2019 filename and change \"model_file_base_name\"\
    : \"gptq_model-4bit-128g\" to \"model_file_base_name\": \"model\" in quantize_config.json,\
    \ hope to get your guidance.\n"
  created_at: 2023-10-11 13:44:10+00:00
  edited: true
  hidden: false
  id: 6526b4ba999898ced81a2bef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-11T14:49:26.000Z'
    data:
      edited: false
      editors:
      - mjw98
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.6825403571128845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
          fullname: meijingwu
          isHf: false
          isPro: false
          name: mjw98
          type: user
        html: '<p>kkkk</p>

          '
        raw: kkkk
        updatedAt: '2023-10-11T14:49:26.830Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6526b5f62b9713f339e85db7
    id: 6526b5f62b9713f339e85db6
    type: comment
  author: mjw98
  content: kkkk
  created_at: 2023-10-11 13:49:26+00:00
  edited: false
  hidden: false
  id: 6526b5f62b9713f339e85db6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-11T14:49:26.000Z'
    data:
      status: closed
    id: 6526b5f62b9713f339e85db7
    type: status-change
  author: mjw98
  created_at: 2023-10-11 13:49:26+00:00
  id: 6526b5f62b9713f339e85db7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Ziya-Coding-34B-v1.0-GPTQ
repo_type: model
status: closed
target_branch: null
title: Please tell me about gptq quantisation.
