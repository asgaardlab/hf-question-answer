!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HelenGuo99
conflicting_files: null
created_at: 2023-02-15 21:53:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec819dc9ccb1a276c6d97b18c55161b2.svg
      fullname: Helen Guo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HelenGuo99
      type: user
    createdAt: '2023-02-15T21:53:37.000Z'
    data:
      edited: false
      editors:
      - HelenGuo99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec819dc9ccb1a276c6d97b18c55161b2.svg
          fullname: Helen Guo
          isHf: false
          isPro: false
          name: HelenGuo99
          type: user
        html: '<p>Hello, </p>

          <p>I think that the model is a little bit biased against certain keywords,
          such as ''black. white''. Those examples gave me ''toxic'' results but I
          don''t think they are.  "I like black phones", "it''s white" etc</p>

          '
        raw: "Hello, \r\n\r\nI think that the model is a little bit biased against\
          \ certain keywords, such as 'black. white'. Those examples gave me 'toxic'\
          \ results but I don't think they are.  \"I like black phones\", \"it's white\"\
          \ etc\r\n"
        updatedAt: '2023-02-15T21:53:37.695Z'
      numEdits: 0
      reactions: []
    id: 63ed5461c5b3c7340861e831
    type: comment
  author: HelenGuo99
  content: "Hello, \r\n\r\nI think that the model is a little bit biased against certain\
    \ keywords, such as 'black. white'. Those examples gave me 'toxic' results but\
    \ I don't think they are.  \"I like black phones\", \"it's white\" etc\r\n"
  created_at: 2023-02-15 21:53:37+00:00
  edited: false
  hidden: false
  id: 63ed5461c5b3c7340861e831
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/310c4435ebafbc1bc10104603001d30f.svg
      fullname: martin pan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: martin-ha
      type: user
    createdAt: '2023-02-19T00:14:36.000Z'
    data:
      edited: false
      editors:
      - martin-ha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/310c4435ebafbc1bc10104603001d30f.svg
          fullname: martin pan
          isHf: false
          isPro: false
          name: martin-ha
          type: user
        html: '<p>Hey Helen, yes I think this is because the training data might contain
          many comments marked as toxic with the word "black" or "white" and the model
          might learn this association. It is quite a challenging question to how
          to address this type of issue and I am curious to see how you or others
          think!</p>

          '
        raw: Hey Helen, yes I think this is because the training data might contain
          many comments marked as toxic with the word "black" or "white" and the model
          might learn this association. It is quite a challenging question to how
          to address this type of issue and I am curious to see how you or others
          think!
        updatedAt: '2023-02-19T00:14:36.866Z'
      numEdits: 0
      reactions: []
    id: 63f169ec3d39ac5108060bd3
    type: comment
  author: martin-ha
  content: Hey Helen, yes I think this is because the training data might contain
    many comments marked as toxic with the word "black" or "white" and the model might
    learn this association. It is quite a challenging question to how to address this
    type of issue and I am curious to see how you or others think!
  created_at: 2023-02-19 00:14:36+00:00
  edited: false
  hidden: false
  id: 63f169ec3d39ac5108060bd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2fa8a12ad9567801035d154bc59b347d.svg
      fullname: Jeremy Gofman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgofman
      type: user
    createdAt: '2023-09-18T18:48:55.000Z'
    data:
      edited: true
      editors:
      - jgofman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7878852486610413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2fa8a12ad9567801035d154bc59b347d.svg
          fullname: Jeremy Gofman
          isHf: false
          isPro: false
          name: jgofman
          type: user
        html: '<p>"Please kill yourself" returns 50/50 toxic/non-toxic result. Seems
          too sensitive to individual tokens rather than overall message, tone. :)<br>This
          sort of comment can be pretty common online....</p>

          '
        raw: '"Please kill yourself" returns 50/50 toxic/non-toxic result. Seems too
          sensitive to individual tokens rather than overall message, tone. :)

          This sort of comment can be pretty common online....'
        updatedAt: '2023-09-18T18:49:21.911Z'
      numEdits: 1
      reactions: []
    id: 65089b97abdde5290e5a9102
    type: comment
  author: jgofman
  content: '"Please kill yourself" returns 50/50 toxic/non-toxic result. Seems too
    sensitive to individual tokens rather than overall message, tone. :)

    This sort of comment can be pretty common online....'
  created_at: 2023-09-18 17:48:55+00:00
  edited: true
  hidden: false
  id: 65089b97abdde5290e5a9102
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: martin-ha/toxic-comment-model
repo_type: model
status: open
target_branch: null
title: Model's bias against certain keywords
