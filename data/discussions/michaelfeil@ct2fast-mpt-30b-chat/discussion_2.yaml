!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spaceman7777
conflicting_files: null
created_at: 2023-07-01 19:34:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
      fullname: Space Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spaceman7777
      type: user
    createdAt: '2023-07-01T20:34:33.000Z'
    data:
      edited: false
      editors:
      - spaceman7777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8872190117835999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e74ef888341c7f971d114d00a741e3a5.svg
          fullname: Space Man
          isHf: false
          isPro: false
          name: spaceman7777
          type: user
        html: '<p>Hi,<br>What sort of hardware have you gotten this running on?</p>

          <p>I''m suspicious that, despite my syntax, ctranslate2 is having issues
          splitting the model between two GPUs.</p>

          <p>I''m using:<br>GeneratorCT2fromHfHub(model_path,<br>                              device="cuda",<br>                              compute_type="int8_float16",<br>                              device_index=[0,
          1])</p>

          <p>and a pre-downloaded model.</p>

          <p>Is it possible to run this on 48GB VRAM? Have you tested splitting across
          two cards?</p>

          <p>Any help would be greatly appreciated, as your model is the best option
          right now for running mpt-30b on mortal hardware. afaik.</p>

          <p>(I''ve tried calling the ctranslate2 Generator class directly as well.
          Just looking for a second opinion.)<br>Thank you :)</p>

          '
        raw: "Hi,\r\nWhat sort of hardware have you gotten this running on?\r\n\r\n\
          I'm suspicious that, despite my syntax, ctranslate2 is having issues splitting\
          \ the model between two GPUs.\r\n\r\nI'm using:\r\nGeneratorCT2fromHfHub(model_path,\r\
          \n                              device=\"cuda\",\r\n                   \
          \           compute_type=\"int8_float16\",\r\n                         \
          \     device_index=[0, 1])\r\n\r\nand a pre-downloaded model.\r\n\r\nIs\
          \ it possible to run this on 48GB VRAM? Have you tested splitting across\
          \ two cards?\r\n\r\nAny help would be greatly appreciated, as your model\
          \ is the best option right now for running mpt-30b on mortal hardware. afaik.\r\
          \n\r\n(I've tried calling the ctranslate2 Generator class directly as well.\
          \ Just looking for a second opinion.)\r\nThank you :)"
        updatedAt: '2023-07-01T20:34:33.083Z'
      numEdits: 0
      reactions: []
    id: 64a08dd9ba7a05e3627818b9
    type: comment
  author: spaceman7777
  content: "Hi,\r\nWhat sort of hardware have you gotten this running on?\r\n\r\n\
    I'm suspicious that, despite my syntax, ctranslate2 is having issues splitting\
    \ the model between two GPUs.\r\n\r\nI'm using:\r\nGeneratorCT2fromHfHub(model_path,\r\
    \n                              device=\"cuda\",\r\n                         \
    \     compute_type=\"int8_float16\",\r\n                              device_index=[0,\
    \ 1])\r\n\r\nand a pre-downloaded model.\r\n\r\nIs it possible to run this on\
    \ 48GB VRAM? Have you tested splitting across two cards?\r\n\r\nAny help would\
    \ be greatly appreciated, as your model is the best option right now for running\
    \ mpt-30b on mortal hardware. afaik.\r\n\r\n(I've tried calling the ctranslate2\
    \ Generator class directly as well. Just looking for a second opinion.)\r\nThank\
    \ you :)"
  created_at: 2023-07-01 19:34:33+00:00
  edited: false
  hidden: false
  id: 64a08dd9ba7a05e3627818b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-03T17:33:51.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8977383971214294
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<p>This Libary just wraps downloading of the model from HF, tokenizers,
          and Ctranslate2 internally. </p>

          <p>Seems to me like a feature request to Ctranslate2 then, I am not sure
          if there is a experimental support for distributed inference, when the model
          does not fit in single vram. </p>

          <p>Update: see issue <a rel="nofollow" href="https://github.com/OpenNMT/CTranslate2/issues/1052">https://github.com/OpenNMT/CTranslate2/issues/1052</a></p>

          '
        raw: "This Libary just wraps downloading of the model from HF, tokenizers,\
          \ and Ctranslate2 internally. \n\nSeems to me like a feature request to\
          \ Ctranslate2 then, I am not sure if there is a experimental support for\
          \ distributed inference, when the model does not fit in single vram. \n\n\
          Update: see issue https://github.com/OpenNMT/CTranslate2/issues/1052"
        updatedAt: '2023-07-19T10:04:13.246Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - spaceman7777
    id: 64a3067fc34564e62f60f78f
    type: comment
  author: michaelfeil
  content: "This Libary just wraps downloading of the model from HF, tokenizers, and\
    \ Ctranslate2 internally. \n\nSeems to me like a feature request to Ctranslate2\
    \ then, I am not sure if there is a experimental support for distributed inference,\
    \ when the model does not fit in single vram. \n\nUpdate: see issue https://github.com/OpenNMT/CTranslate2/issues/1052"
  created_at: 2023-07-03 16:33:51+00:00
  edited: true
  hidden: false
  id: 64a3067fc34564e62f60f78f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: michaelfeil/ct2fast-mpt-30b-chat
repo_type: model
status: open
target_branch: null
title: OOM with two RTX 3090s w/ NVLink
