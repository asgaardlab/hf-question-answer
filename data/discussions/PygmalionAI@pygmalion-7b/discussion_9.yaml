!!python/object:huggingface_hub.community.DiscussionWithDetails
author: win10
conflicting_files: null
created_at: 2023-05-07 00:51:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
      fullname: "\u8449\u4F50\u4FCA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: win10
      type: user
    createdAt: '2023-05-07T01:51:41.000Z'
    data:
      edited: false
      editors:
      - win10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678188568629-noauth.png?w=200&h=200&f=face
          fullname: "\u8449\u4F50\u4FCA"
          isHf: false
          isPro: false
          name: win10
          type: user
        html: '<p>Consider using the MPT-7b model to train the new pygmalion model?</p>

          '
        raw: Consider using the MPT-7b model to train the new pygmalion model?
        updatedAt: '2023-05-07T01:51:41.204Z'
      numEdits: 0
      reactions: []
    id: 6457042d4095c967f9adec02
    type: comment
  author: win10
  content: Consider using the MPT-7b model to train the new pygmalion model?
  created_at: 2023-05-07 00:51:41+00:00
  edited: false
  hidden: false
  id: 6457042d4095c967f9adec02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-05-07T02:18:33.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<p>Yep, I''m considering it. I''m keeping a close eye on the new models
          being released. Considerations so far:</p>

          <ul>

          <li>RedPajama is a very good alternative to LLaMA because of the license.
          However, NeoX (what RedPajama is based on) is a less popular architecture.
          LLaMA has a wider ecosystem (e.g. support in WebLLM) and is almost 40% faster
          at inference time, I''ve been told.</li>

          <li>OpenLLaMA is based on the LLaMA architecture, so it''s more promising
          in that regards. However, it''s not done training yet.</li>

          <li>MPT is an entirely custom architecture. For now, there is no wider ecosystem
          support (e.g. koboldcpp), and several things are "broken" on it (e.g. impossible
          to train a LoRA for now, I''ve been told)</li>

          </ul>

          <p>Still, fine-tuning off of the 65k ctxlen version sounds like a fun project,
          so I am considering it if I have GPUs idling around doing nothing.</p>

          '
        raw: 'Yep, I''m considering it. I''m keeping a close eye on the new models
          being released. Considerations so far:

          - RedPajama is a very good alternative to LLaMA because of the license.
          However, NeoX (what RedPajama is based on) is a less popular architecture.
          LLaMA has a wider ecosystem (e.g. support in WebLLM) and is almost 40% faster
          at inference time, I''ve been told.

          - OpenLLaMA is based on the LLaMA architecture, so it''s more promising
          in that regards. However, it''s not done training yet.

          - MPT is an entirely custom architecture. For now, there is no wider ecosystem
          support (e.g. koboldcpp), and several things are "broken" on it (e.g. impossible
          to train a LoRA for now, I''ve been told)


          Still, fine-tuning off of the 65k ctxlen version sounds like a fun project,
          so I am considering it if I have GPUs idling around doing nothing.'
        updatedAt: '2023-05-07T02:18:33.467Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\u2764\uFE0F"
        users:
        - KKcorps
        - win10
        - RazielAU
        - neonr-0
        - adamsmith1236
        - spacemiqote
        - TerraNull
        - Jojo7
    id: 64570a7978c059b099bbe8d1
    type: comment
  author: 11b
  content: 'Yep, I''m considering it. I''m keeping a close eye on the new models being
    released. Considerations so far:

    - RedPajama is a very good alternative to LLaMA because of the license. However,
    NeoX (what RedPajama is based on) is a less popular architecture. LLaMA has a
    wider ecosystem (e.g. support in WebLLM) and is almost 40% faster at inference
    time, I''ve been told.

    - OpenLLaMA is based on the LLaMA architecture, so it''s more promising in that
    regards. However, it''s not done training yet.

    - MPT is an entirely custom architecture. For now, there is no wider ecosystem
    support (e.g. koboldcpp), and several things are "broken" on it (e.g. impossible
    to train a LoRA for now, I''ve been told)


    Still, fine-tuning off of the 65k ctxlen version sounds like a fun project, so
    I am considering it if I have GPUs idling around doing nothing.'
  created_at: 2023-05-07 01:18:33+00:00
  edited: false
  hidden: false
  id: 64570a7978c059b099bbe8d1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: PygmalionAI/pygmalion-7b
repo_type: model
status: open
target_branch: null
title: Consider using the MPT-7b model to train the new pygmalion model?
