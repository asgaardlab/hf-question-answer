!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cypherfox
conflicting_files: null
created_at: 2023-05-06 20:25:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
      fullname: Morgan Schweers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cypherfox
      type: user
    createdAt: '2023-05-06T21:25:09.000Z'
    data:
      edited: false
      editors:
      - Cypherfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
          fullname: Morgan Schweers
          isHf: false
          isPro: false
          name: Cypherfox
          type: user
        html: '<p>Greetings,<br>I''ve been trying to quantize this down to 4 bits
          for the last day-ish, and it looks like it loses touch with the tokens that
          are being output. (<em>It outputs at a cadence that suggests it is predicting
          tokens, but the tokens are nonsense.</em>) This is my first time trying
          to quantize a HuggingFace Transformers model. (I''ve done it in the past
          with the raw LLaMA model using <code>llama.cpp</code>.) Any pointers that
          I could use to figure out what''s going wrong?</p>

          <p>Thanks muchly for any advice,</p>

          <p>--  Cypherfox</p>

          '
        raw: "Greetings,\r\nI've been trying to quantize this down to 4 bits for the\
          \ last day-ish, and it looks like it loses touch with the tokens that are\
          \ being output. (_It outputs at a cadence that suggests it is predicting\
          \ tokens, but the tokens are nonsense._) This is my first time trying to\
          \ quantize a HuggingFace Transformers model. (I've done it in the past with\
          \ the raw LLaMA model using `llama.cpp`.) Any pointers that I could use\
          \ to figure out what's going wrong?\r\n\r\nThanks muchly for any advice,\r\
          \n\r\n--  Cypherfox"
        updatedAt: '2023-05-06T21:25:09.858Z'
      numEdits: 0
      reactions: []
    id: 6456c5b50a1ea2229a2a47fb
    type: comment
  author: Cypherfox
  content: "Greetings,\r\nI've been trying to quantize this down to 4 bits for the\
    \ last day-ish, and it looks like it loses touch with the tokens that are being\
    \ output. (_It outputs at a cadence that suggests it is predicting tokens, but\
    \ the tokens are nonsense._) This is my first time trying to quantize a HuggingFace\
    \ Transformers model. (I've done it in the past with the raw LLaMA model using\
    \ `llama.cpp`.) Any pointers that I could use to figure out what's going wrong?\r\
    \n\r\nThanks muchly for any advice,\r\n\r\n--  Cypherfox"
  created_at: 2023-05-06 20:25:09+00:00
  edited: false
  hidden: false
  id: 6456c5b50a1ea2229a2a47fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
      fullname: Sean Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sslx
      type: user
    createdAt: '2023-05-07T02:17:49.000Z'
    data:
      edited: false
      editors:
      - sslx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecbbe42f9ab59a0f30f045432a10413.svg
          fullname: Sean Lee
          isHf: false
          isPro: false
          name: sslx
          type: user
        html: '<p>I''ve quantized it to 4bit-128g, and was able to use in oobabooga/text-generation-webui
          without a problem.<br>Since it''s fast moving space, and everyone is doing
          different things, there are so many versions of GPTQ.<br>The trick is to
          make sure you run the inference using the same version that you used to
          quantize.<br>I learned the hard way with LLaMA after wasting time trying
          to quantize bunch of times with different combination of parameters.<br>Also
          make sure whatever you''re using it to inference supports that particular
          GPTQ. With different combination of transformer, you might get an error
          as well.<br>Hope that helps and good luck!</p>

          '
        raw: 'I''ve quantized it to 4bit-128g, and was able to use in oobabooga/text-generation-webui
          without a problem.

          Since it''s fast moving space, and everyone is doing different things, there
          are so many versions of GPTQ.

          The trick is to make sure you run the inference using the same version that
          you used to quantize.

          I learned the hard way with LLaMA after wasting time trying to quantize
          bunch of times with different combination of parameters.

          Also make sure whatever you''re using it to inference supports that particular
          GPTQ. With different combination of transformer, you might get an error
          as well.

          Hope that helps and good luck!'
        updatedAt: '2023-05-07T02:17:49.646Z'
      numEdits: 0
      reactions: []
    id: 64570a4d78c059b099bbe648
    type: comment
  author: sslx
  content: 'I''ve quantized it to 4bit-128g, and was able to use in oobabooga/text-generation-webui
    without a problem.

    Since it''s fast moving space, and everyone is doing different things, there are
    so many versions of GPTQ.

    The trick is to make sure you run the inference using the same version that you
    used to quantize.

    I learned the hard way with LLaMA after wasting time trying to quantize bunch
    of times with different combination of parameters.

    Also make sure whatever you''re using it to inference supports that particular
    GPTQ. With different combination of transformer, you might get an error as well.

    Hope that helps and good luck!'
  created_at: 2023-05-07 01:17:49+00:00
  edited: false
  hidden: false
  id: 64570a4d78c059b099bbe648
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-07T04:40:18.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: "<p>Use ooba\u2019s cmd script to open a command prompt and use the\
          \ copy in text-generation-webui/repositories/gptq. Also, try without act-order,\
          \ for me that messes things up, but otherwise it quantises fine.</p>\n"
        raw: "Use ooba\u2019s cmd script to open a command prompt and use the copy\
          \ in text-generation-webui/repositories/gptq. Also, try without act-order,\
          \ for me that messes things up, but otherwise it quantises fine."
        updatedAt: '2023-05-07T04:40:18.173Z'
      numEdits: 0
      reactions: []
    id: 64572bb203625871eb8068f5
    type: comment
  author: RazielAU
  content: "Use ooba\u2019s cmd script to open a command prompt and use the copy in\
    \ text-generation-webui/repositories/gptq. Also, try without act-order, for me\
    \ that messes things up, but otherwise it quantises fine."
  created_at: 2023-05-07 03:40:18+00:00
  edited: false
  hidden: false
  id: 64572bb203625871eb8068f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
      fullname: Morgan Schweers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cypherfox
      type: user
    createdAt: '2023-05-07T08:05:38.000Z'
    data:
      edited: false
      editors:
      - Cypherfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
          fullname: Morgan Schweers
          isHf: false
          isPro: false
          name: Cypherfox
          type: user
        html: '<p>I''m doing it from the command-line locally, not in a colab, so
          I was having some other issues.</p>

          <p>I think I must have been using the wrong GPTQ repo, as I seem to have
          three different ones on my system.  I must have finally used the correct
          oobabooga one, but when I used the <code>llama</code> script to convert
          the post-xor model on my local system (3070Ti w/8GB) it ran out of GPU memory
          at layer 27. :( So I spun up a p3.2xlarge (V100 w/16GB) on EC2 and did all
          the downloading, xor''ing, and generated the necessary HF files, and then
          did the quantization using the <code>llama.py</code> script.  Downloaded
          that to my local system, and that worked!</p>

          <p>Just for anyone who wants to know how, the command looked like this:</p>

          <pre><code class="language-shell">python repositories/GPTQ-for-LLaMa/llama.py
          models/pygmalion-7b c4 --wbits 4 --groupsize 128 --save_safetensors models/pygmalion-7b-4bits/pygmalion-7b-4bit-128g.safetensors

          </code></pre>

          <p>with the post-xor model in <code>models/pygmalion-7b</code>, and an empty
          directory in <code>models/pygmalion-7b-4bits</code>.  After the quantized
          file was created, I downloaded that and all the associated (post-xor) <code>*.json</code>
          and the one <code>tokenizer.model</code> file from <code>models/pygmalion-7b</code>
          into <code>models/pygmalion-7b-4bits</code> on my local system, and launched
          oobabooga with:</p>

          <pre><code class="language-shell">python server.py --no-cache --threads
          4 --chat --listen --model pygmalion-7b-4bits --wbits 4 --groupsize 128 --model_type
          llama

          </code></pre>

          <p>The iteration speed is pretty slow on my local system (around 0.66 tokens/s)
          but it works without running out of GPU memory, and the quality is actually
          really good so far.</p>

          <p>Thanks very much for the encouragement and letting me know that it could
          work! That helped me figure out what I was doing wrong on my side.</p>

          <p>--  Cypherfox</p>

          '
        raw: 'I''m doing it from the command-line locally, not in a colab, so I was
          having some other issues.


          I think I must have been using the wrong GPTQ repo, as I seem to have three
          different ones on my system.  I must have finally used the correct oobabooga
          one, but when I used the `llama` script to convert the post-xor model on
          my local system (3070Ti w/8GB) it ran out of GPU memory at layer 27. :(
          So I spun up a p3.2xlarge (V100 w/16GB) on EC2 and did all the downloading,
          xor''ing, and generated the necessary HF files, and then did the quantization
          using the `llama.py` script.  Downloaded that to my local system, and that
          worked!


          Just for anyone who wants to know how, the command looked like this:

          ```shell

          python repositories/GPTQ-for-LLaMa/llama.py models/pygmalion-7b c4 --wbits
          4 --groupsize 128 --save_safetensors models/pygmalion-7b-4bits/pygmalion-7b-4bit-128g.safetensors

          ```

          with the post-xor model in `models/pygmalion-7b`, and an empty directory
          in `models/pygmalion-7b-4bits`.  After the quantized file was created, I
          downloaded that and all the associated (post-xor) `*.json` and the one `tokenizer.model`
          file from `models/pygmalion-7b` into `models/pygmalion-7b-4bits` on my local
          system, and launched oobabooga with:

          ```shell

          python server.py --no-cache --threads 4 --chat --listen --model pygmalion-7b-4bits
          --wbits 4 --groupsize 128 --model_type llama

          ```


          The iteration speed is pretty slow on my local system (around 0.66 tokens/s)
          but it works without running out of GPU memory, and the quality is actually
          really good so far.


          Thanks very much for the encouragement and letting me know that it could
          work! That helped me figure out what I was doing wrong on my side.


          --  Cypherfox

          '
        updatedAt: '2023-05-07T08:05:38.400Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64575bd2711ee86f6eeab5b2
    id: 64575bd2711ee86f6eeab5b1
    type: comment
  author: Cypherfox
  content: 'I''m doing it from the command-line locally, not in a colab, so I was
    having some other issues.


    I think I must have been using the wrong GPTQ repo, as I seem to have three different
    ones on my system.  I must have finally used the correct oobabooga one, but when
    I used the `llama` script to convert the post-xor model on my local system (3070Ti
    w/8GB) it ran out of GPU memory at layer 27. :( So I spun up a p3.2xlarge (V100
    w/16GB) on EC2 and did all the downloading, xor''ing, and generated the necessary
    HF files, and then did the quantization using the `llama.py` script.  Downloaded
    that to my local system, and that worked!


    Just for anyone who wants to know how, the command looked like this:

    ```shell

    python repositories/GPTQ-for-LLaMa/llama.py models/pygmalion-7b c4 --wbits 4 --groupsize
    128 --save_safetensors models/pygmalion-7b-4bits/pygmalion-7b-4bit-128g.safetensors

    ```

    with the post-xor model in `models/pygmalion-7b`, and an empty directory in `models/pygmalion-7b-4bits`.  After
    the quantized file was created, I downloaded that and all the associated (post-xor)
    `*.json` and the one `tokenizer.model` file from `models/pygmalion-7b` into `models/pygmalion-7b-4bits`
    on my local system, and launched oobabooga with:

    ```shell

    python server.py --no-cache --threads 4 --chat --listen --model pygmalion-7b-4bits
    --wbits 4 --groupsize 128 --model_type llama

    ```


    The iteration speed is pretty slow on my local system (around 0.66 tokens/s) but
    it works without running out of GPU memory, and the quality is actually really
    good so far.


    Thanks very much for the encouragement and letting me know that it could work!
    That helped me figure out what I was doing wrong on my side.


    --  Cypherfox

    '
  created_at: 2023-05-07 07:05:38+00:00
  edited: false
  hidden: false
  id: 64575bd2711ee86f6eeab5b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
      fullname: Morgan Schweers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cypherfox
      type: user
    createdAt: '2023-05-07T08:05:38.000Z'
    data:
      status: closed
    id: 64575bd2711ee86f6eeab5b2
    type: status-change
  author: Cypherfox
  created_at: 2023-05-07 07:05:38+00:00
  id: 64575bd2711ee86f6eeab5b2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-07T08:49:36.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>You''ve already closed the topic, but you can try quantising with
          --true-sequential to see if that speeds up inference. While you''ve got
          something running though, definitely don''t replace what you have. True
          sequential is meant to improve accuracy, but for me, I felt that it ran
          faster too, but maybe that was just my imagination.</p>

          '
        raw: You've already closed the topic, but you can try quantising with --true-sequential
          to see if that speeds up inference. While you've got something running though,
          definitely don't replace what you have. True sequential is meant to improve
          accuracy, but for me, I felt that it ran faster too, but maybe that was
          just my imagination.
        updatedAt: '2023-05-07T08:49:36.102Z'
      numEdits: 0
      reactions: []
    id: 64576620cf099a9dd14a44c8
    type: comment
  author: RazielAU
  content: You've already closed the topic, but you can try quantising with --true-sequential
    to see if that speeds up inference. While you've got something running though,
    definitely don't replace what you have. True sequential is meant to improve accuracy,
    but for me, I felt that it ran faster too, but maybe that was just my imagination.
  created_at: 2023-05-07 07:49:36+00:00
  edited: false
  hidden: false
  id: 64576620cf099a9dd14a44c8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: PygmalionAI/pygmalion-7b
repo_type: model
status: closed
target_branch: null
title: Quantized version possible?
