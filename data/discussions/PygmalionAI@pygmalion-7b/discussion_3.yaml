!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xzuyn
conflicting_files: null
created_at: 2023-04-30 21:53:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-04-30T22:53:36.000Z'
    data:
      edited: false
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<p>title</p>

          '
        raw: title
        updatedAt: '2023-04-30T22:53:36.726Z'
      numEdits: 0
      reactions: []
    id: 644ef170722da4dfd87379f2
    type: comment
  author: xzuyn
  content: title
  created_at: 2023-04-30 21:53:36+00:00
  edited: false
  hidden: false
  id: 644ef170722da4dfd87379f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-01T07:20:36.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>Saying ''title'' doesn''t really clarify your question... I''m going
          to assume you mean why people use it in general for AI.</p>

          <p>The short version is that bfloat (brain float) is able to store values
          of the same range as a 32-bit float. It does so with lower precision of
          course, but from an AI perspective, it offers almost the same prediction
          accuracy as full 32-bit floats do at much lower memory and processing cost.
          Standard 16-bit floats weren''t really designed for AI, they have better
          precision than bfloats (which is better for most use-cases), but also store
          a smaller range than 32-bit floats do (which affects prediction quality).
          It''s also much easier to convert from 32-bit float to 16-bit bfloat since
          you only have to truncate the mantissa bits down to 16-bit.</p>

          <p>This is all pretty theoretical though, I''ve trained with both myself,
          and I honestly can''t say I''ve ever noticed a huge difference, but supposedly
          the increased range of bfloat''s is better for AI than traditional 16-bit
          floats.</p>

          <p>This image shows how 16-bit floats and bfloats compare to a 32-bit float
          in terms of how the bits are used, you''ll note it has the same number of
          exponent bits as a 32bit float, it''s basically a 32-bit float with a smaller
          set of fraction bits:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64215db49628d83384b00609/KR6Iw_mERtILLAmISNRw0.png"><img
          alt="Comparison-of-the-float32-bfloat16-and-float16-numerical-formats-The-bfloat16-format.png"
          src="https://cdn-uploads.huggingface.co/production/uploads/64215db49628d83384b00609/KR6Iw_mERtILLAmISNRw0.png"></a></p>

          '
        raw: 'Saying ''title'' doesn''t really clarify your question... I''m going
          to assume you mean why people use it in general for AI.


          The short version is that bfloat (brain float) is able to store values of
          the same range as a 32-bit float. It does so with lower precision of course,
          but from an AI perspective, it offers almost the same prediction accuracy
          as full 32-bit floats do at much lower memory and processing cost. Standard
          16-bit floats weren''t really designed for AI, they have better precision
          than bfloats (which is better for most use-cases), but also store a smaller
          range than 32-bit floats do (which affects prediction quality). It''s also
          much easier to convert from 32-bit float to 16-bit bfloat since you only
          have to truncate the mantissa bits down to 16-bit.


          This is all pretty theoretical though, I''ve trained with both myself, and
          I honestly can''t say I''ve ever noticed a huge difference, but supposedly
          the increased range of bfloat''s is better for AI than traditional 16-bit
          floats.


          This image shows how 16-bit floats and bfloats compare to a 32-bit float
          in terms of how the bits are used, you''ll note it has the same number of
          exponent bits as a 32bit float, it''s basically a 32-bit float with a smaller
          set of fraction bits:

          ![Comparison-of-the-float32-bfloat16-and-float16-numerical-formats-The-bfloat16-format.png](https://cdn-uploads.huggingface.co/production/uploads/64215db49628d83384b00609/KR6Iw_mERtILLAmISNRw0.png)'
        updatedAt: '2023-05-01T07:20:36.672Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - xzuyn
        - theblinkingusb
    id: 644f684428774bd665cea8f5
    type: comment
  author: RazielAU
  content: 'Saying ''title'' doesn''t really clarify your question... I''m going to
    assume you mean why people use it in general for AI.


    The short version is that bfloat (brain float) is able to store values of the
    same range as a 32-bit float. It does so with lower precision of course, but from
    an AI perspective, it offers almost the same prediction accuracy as full 32-bit
    floats do at much lower memory and processing cost. Standard 16-bit floats weren''t
    really designed for AI, they have better precision than bfloats (which is better
    for most use-cases), but also store a smaller range than 32-bit floats do (which
    affects prediction quality). It''s also much easier to convert from 32-bit float
    to 16-bit bfloat since you only have to truncate the mantissa bits down to 16-bit.


    This is all pretty theoretical though, I''ve trained with both myself, and I honestly
    can''t say I''ve ever noticed a huge difference, but supposedly the increased
    range of bfloat''s is better for AI than traditional 16-bit floats.


    This image shows how 16-bit floats and bfloats compare to a 32-bit float in terms
    of how the bits are used, you''ll note it has the same number of exponent bits
    as a 32bit float, it''s basically a 32-bit float with a smaller set of fraction
    bits:

    ![Comparison-of-the-float32-bfloat16-and-float16-numerical-formats-The-bfloat16-format.png](https://cdn-uploads.huggingface.co/production/uploads/64215db49628d83384b00609/KR6Iw_mERtILLAmISNRw0.png)'
  created_at: 2023-05-01 06:20:36+00:00
  edited: false
  hidden: false
  id: 644f684428774bd665cea8f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-05-01T14:08:21.000Z'
    data:
      edited: false
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<blockquote>

          <p>Saying ''title'' doesn''t really clarify your question... I''m going
          to assume you mean why people use it in general for AI.</p>

          </blockquote>

          <p>sorry, I should have added more details.</p>

          <p>I''m asking why PygmalionAI decided to use bfloat instead of float like
          nearly all the other models I''ve tried. Normally we can quantize to ggml
          straight from the float, but because its bfloat we have to convert to float
          first then to ggml, which is an extra step which makes the quality worse.</p>

          '
        raw: '> Saying ''title'' doesn''t really clarify your question... I''m going
          to assume you mean why people use it in general for AI.


          sorry, I should have added more details.


          I''m asking why PygmalionAI decided to use bfloat instead of float like
          nearly all the other models I''ve tried. Normally we can quantize to ggml
          straight from the float, but because its bfloat we have to convert to float
          first then to ggml, which is an extra step which makes the quality worse.'
        updatedAt: '2023-05-01T14:08:21.575Z'
      numEdits: 0
      reactions: []
    id: 644fc7d5d5f7dafcfa60262c
    type: comment
  author: xzuyn
  content: '> Saying ''title'' doesn''t really clarify your question... I''m going
    to assume you mean why people use it in general for AI.


    sorry, I should have added more details.


    I''m asking why PygmalionAI decided to use bfloat instead of float like nearly
    all the other models I''ve tried. Normally we can quantize to ggml straight from
    the float, but because its bfloat we have to convert to float first then to ggml,
    which is an extra step which makes the quality worse.'
  created_at: 2023-05-01 13:08:21+00:00
  edited: false
  hidden: false
  id: 644fc7d5d5f7dafcfa60262c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671567442495-noauth.png?w=200&h=200&f=face
      fullname: TearGosling
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TearGosling
      type: user
    createdAt: '2023-05-01T14:46:18.000Z'
    data:
      edited: false
      editors:
      - TearGosling
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671567442495-noauth.png?w=200&h=200&f=face
          fullname: TearGosling
          isHf: false
          isPro: false
          name: TearGosling
          type: user
        html: '<p>Hi there! So, the reason we were using bfloat16 during our training
          is due to our usage of <a href="https://huggingface.co/docs/accelerate/usage_guides/fsdp">FSDP</a>
          for our model parallelism scheme. Using FP16 with FSDP results in overflows/underflows
          showing up during training, which obviously leads to problems - this is
          why we had to use bfloat16. It sucks to hear that the quality drops so much
          with GGML due to the casting required to FP16 before quantization, though,
          so we''re looking into possible ways we can train in FP16 without the required
          computation being too much for our hardware to handle. No promises we''ll
          be able to find a solution though, sadly. We''ll keep working on it! Thanks
          for the heads-up about the GGML quantization.</p>

          '
        raw: Hi there! So, the reason we were using bfloat16 during our training is
          due to our usage of [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)
          for our model parallelism scheme. Using FP16 with FSDP results in overflows/underflows
          showing up during training, which obviously leads to problems - this is
          why we had to use bfloat16. It sucks to hear that the quality drops so much
          with GGML due to the casting required to FP16 before quantization, though,
          so we're looking into possible ways we can train in FP16 without the required
          computation being too much for our hardware to handle. No promises we'll
          be able to find a solution though, sadly. We'll keep working on it! Thanks
          for the heads-up about the GGML quantization.
        updatedAt: '2023-05-01T14:46:18.264Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzuyn
    id: 644fd0ba28774bd665d80475
    type: comment
  author: TearGosling
  content: Hi there! So, the reason we were using bfloat16 during our training is
    due to our usage of [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)
    for our model parallelism scheme. Using FP16 with FSDP results in overflows/underflows
    showing up during training, which obviously leads to problems - this is why we
    had to use bfloat16. It sucks to hear that the quality drops so much with GGML
    due to the casting required to FP16 before quantization, though, so we're looking
    into possible ways we can train in FP16 without the required computation being
    too much for our hardware to handle. No promises we'll be able to find a solution
    though, sadly. We'll keep working on it! Thanks for the heads-up about the GGML
    quantization.
  created_at: 2023-05-01 13:46:18+00:00
  edited: false
  hidden: false
  id: 644fd0ba28774bd665d80475
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-01T15:06:52.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>To me it sounds more like an issue with the ggml conversion tools
          not supporting bfloat. From a technical standpoint, bfloat is better for
          AI models. It''s not just something some random dude came up with, it actually
          offers very similar performance to 32-bit AI models at half the cost.</p>

          <p>It''s also very easy to work with, which leads me to my next point: It''s
          possible to do conversion from bfloat back to 32bit float with zero loss
          in precision as you''re just padding each 16bit bfloat value with zeroes
          in the mantissa to convert it to 32-bits. So if the issue is that the ggml
          code can''t do its work in the native 16bit bfloat format, then converting
          it to 32bit floats should lead to zero loss in precision.</p>

          <p>Of course, we''re ignoring the fact that you''re dropping the model all
          the way down to 4bit precision anyway, so at that point it''s highly unlikely
          that you''re going to see any loss related to the conversion from bfloat
          to float...</p>

          '
        raw: 'To me it sounds more like an issue with the ggml conversion tools not
          supporting bfloat. From a technical standpoint, bfloat is better for AI
          models. It''s not just something some random dude came up with, it actually
          offers very similar performance to 32-bit AI models at half the cost.


          It''s also very easy to work with, which leads me to my next point: It''s
          possible to do conversion from bfloat back to 32bit float with zero loss
          in precision as you''re just padding each 16bit bfloat value with zeroes
          in the mantissa to convert it to 32-bits. So if the issue is that the ggml
          code can''t do its work in the native 16bit bfloat format, then converting
          it to 32bit floats should lead to zero loss in precision.


          Of course, we''re ignoring the fact that you''re dropping the model all
          the way down to 4bit precision anyway, so at that point it''s highly unlikely
          that you''re going to see any loss related to the conversion from bfloat
          to float...'
        updatedAt: '2023-05-01T15:06:52.760Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzuyn
    id: 644fd58c28774bd665d87798
    type: comment
  author: RazielAU
  content: 'To me it sounds more like an issue with the ggml conversion tools not
    supporting bfloat. From a technical standpoint, bfloat is better for AI models.
    It''s not just something some random dude came up with, it actually offers very
    similar performance to 32-bit AI models at half the cost.


    It''s also very easy to work with, which leads me to my next point: It''s possible
    to do conversion from bfloat back to 32bit float with zero loss in precision as
    you''re just padding each 16bit bfloat value with zeroes in the mantissa to convert
    it to 32-bits. So if the issue is that the ggml code can''t do its work in the
    native 16bit bfloat format, then converting it to 32bit floats should lead to
    zero loss in precision.


    Of course, we''re ignoring the fact that you''re dropping the model all the way
    down to 4bit precision anyway, so at that point it''s highly unlikely that you''re
    going to see any loss related to the conversion from bfloat to float...'
  created_at: 2023-05-01 14:06:52+00:00
  edited: false
  hidden: false
  id: 644fd58c28774bd665d87798
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-05-01T15:10:05.000Z'
    data:
      edited: false
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<blockquote>

          <p>It sucks to hear that the quality drops so much with GGML due to the
          casting required to FP16 before quantization</p>

          </blockquote>

          <p>I''m unsure of how much the quality degrades. I''ve just read in passing
          from concedo (of KoboldCPP) that the bfloat to float step can cause degradation.
          So right now its more of an assumption that there is quality loss, but I
          don''t think anybody has actually tested for quality loss yet.<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/zkI6ZpsD5vr2qpiGR2pza.png"><img
          alt="quote.png" src="https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/zkI6ZpsD5vr2qpiGR2pza.png"></a></p>

          <p>Would be nice to see someone test how the perplexity compares with the
          original bfloat16 vs ggml q5_1 or q8_0, in comparison to a range of other
          models with their original float16 vs ggml q5_1 or q8_0 to see how much
          using bfloat matters in quality loss when needing to convert to float. To
          make that last part a little more clear (because I''m bad at being clear),
          I''m not saying compare different models quality, but compare how much their
          perplexities change when quantizing. Maybe even just a perplexity test on
          the bfloat vs float converted from the bfloat to see how much that changes
          things would be good enough.</p>

          '
        raw: '> It sucks to hear that the quality drops so much with GGML due to the
          casting required to FP16 before quantization


          I''m unsure of how much the quality degrades. I''ve just read in passing
          from concedo (of KoboldCPP) that the bfloat to float step can cause degradation.
          So right now its more of an assumption that there is quality loss, but I
          don''t think anybody has actually tested for quality loss yet.

          ![quote.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/zkI6ZpsD5vr2qpiGR2pza.png)


          Would be nice to see someone test how the perplexity compares with the original
          bfloat16 vs ggml q5_1 or q8_0, in comparison to a range of other models
          with their original float16 vs ggml q5_1 or q8_0 to see how much using bfloat
          matters in quality loss when needing to convert to float. To make that last
          part a little more clear (because I''m bad at being clear), I''m not saying
          compare different models quality, but compare how much their perplexities
          change when quantizing. Maybe even just a perplexity test on the bfloat
          vs float converted from the bfloat to see how much that changes things would
          be good enough.'
        updatedAt: '2023-05-01T15:10:05.741Z'
      numEdits: 0
      reactions: []
    id: 644fd64d28774bd665d889dc
    type: comment
  author: xzuyn
  content: '> It sucks to hear that the quality drops so much with GGML due to the
    casting required to FP16 before quantization


    I''m unsure of how much the quality degrades. I''ve just read in passing from
    concedo (of KoboldCPP) that the bfloat to float step can cause degradation. So
    right now its more of an assumption that there is quality loss, but I don''t think
    anybody has actually tested for quality loss yet.

    ![quote.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/zkI6ZpsD5vr2qpiGR2pza.png)


    Would be nice to see someone test how the perplexity compares with the original
    bfloat16 vs ggml q5_1 or q8_0, in comparison to a range of other models with their
    original float16 vs ggml q5_1 or q8_0 to see how much using bfloat matters in
    quality loss when needing to convert to float. To make that last part a little
    more clear (because I''m bad at being clear), I''m not saying compare different
    models quality, but compare how much their perplexities change when quantizing.
    Maybe even just a perplexity test on the bfloat vs float converted from the bfloat
    to see how much that changes things would be good enough.'
  created_at: 2023-05-01 14:10:05+00:00
  edited: false
  hidden: false
  id: 644fd64d28774bd665d889dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-01T15:16:35.000Z'
    data:
      edited: true
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>What they should be doing is converting from bf16 to fp32, then
          quantising from there. As mentioned earlier, the conversion from bf16 to
          fp32 is lossless since you''re literally just padding the mantissa with
          zeroes. You''d probably get better results since you''d retain the full
          floating point range that bfloat offers prior to the quantisation. But honestly,
          at the point that you''re squeezing the results down to 4-bits, I highly
          doubt it would make a difference.</p>

          '
        raw: What they should be doing is converting from bf16 to fp32, then quantising
          from there. As mentioned earlier, the conversion from bf16 to fp32 is lossless
          since you're literally just padding the mantissa with zeroes. You'd probably
          get better results since you'd retain the full floating point range that
          bfloat offers prior to the quantisation. But honestly, at the point that
          you're squeezing the results down to 4-bits, I highly doubt it would make
          a difference.
        updatedAt: '2023-05-01T15:21:13.601Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzuyn
    id: 644fd7d3577838187efb84f0
    type: comment
  author: RazielAU
  content: What they should be doing is converting from bf16 to fp32, then quantising
    from there. As mentioned earlier, the conversion from bf16 to fp32 is lossless
    since you're literally just padding the mantissa with zeroes. You'd probably get
    better results since you'd retain the full floating point range that bfloat offers
    prior to the quantisation. But honestly, at the point that you're squeezing the
    results down to 4-bits, I highly doubt it would make a difference.
  created_at: 2023-05-01 14:16:35+00:00
  edited: true
  hidden: false
  id: 644fd7d3577838187efb84f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
      fullname: xzuyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzuyn
      type: user
    createdAt: '2023-05-01T15:25:33.000Z'
    data:
      edited: true
      editors:
      - xzuyn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4042dcf1589ac7b340754a6ee8b5f641.svg
          fullname: xzuyn
          isHf: false
          isPro: false
          name: xzuyn
          type: user
        html: '<blockquote>

          <p>What they should be doing is converting from bf16 to fp32, then quantising
          from there. You''d probably get better results since you''d retain the full
          floating point range that bfloat offers. But honestly, at the point that
          you''re squeezing the results down to 4-bits, I highly doubt it would even
          make a difference.</p>

          </blockquote>

          <p>Yea it seems that they are talking about this right now. The above messages
          were from yesterday right after the model dropped and we were trying to
          get it to convert to ggml. Here''s a quote of a quote from 10 minutes ago
          by 0x000011b; <code>"It''s possible to do conversion from bfloat back to
          32bit float with zero loss in precision as you''re just padding each 16bit
          bfloat value with zeroes in the mantissa to convert it to 32-bits."</code></p>

          <p>So doing a quantized ggml (until llama.cpp supports starting from bfloat)
          should be done like this; bfloat16 -&gt; float32 -&gt; float32 ggml -&gt;
          quantized ggml.</p>

          '
        raw: '> What they should be doing is converting from bf16 to fp32, then quantising
          from there. You''d probably get better results since you''d retain the full
          floating point range that bfloat offers. But honestly, at the point that
          you''re squeezing the results down to 4-bits, I highly doubt it would even
          make a difference.


          Yea it seems that they are talking about this right now. The above messages
          were from yesterday right after the model dropped and we were trying to
          get it to convert to ggml. Here''s a quote of a quote from 10 minutes ago
          by 0x000011b; `"It''s possible to do conversion from bfloat back to 32bit
          float with zero loss in precision as you''re just padding each 16bit bfloat
          value with zeroes in the mantissa to convert it to 32-bits."`


          So doing a quantized ggml (until llama.cpp supports starting from bfloat)
          should be done like this; bfloat16 -> float32 -> float32 ggml -> quantized
          ggml.'
        updatedAt: '2023-05-01T15:26:15.992Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RazielAU
    id: 644fd9edd5f7dafcfa61dd7c
    type: comment
  author: xzuyn
  content: '> What they should be doing is converting from bf16 to fp32, then quantising
    from there. You''d probably get better results since you''d retain the full floating
    point range that bfloat offers. But honestly, at the point that you''re squeezing
    the results down to 4-bits, I highly doubt it would even make a difference.


    Yea it seems that they are talking about this right now. The above messages were
    from yesterday right after the model dropped and we were trying to get it to convert
    to ggml. Here''s a quote of a quote from 10 minutes ago by 0x000011b; `"It''s
    possible to do conversion from bfloat back to 32bit float with zero loss in precision
    as you''re just padding each 16bit bfloat value with zeroes in the mantissa to
    convert it to 32-bits."`


    So doing a quantized ggml (until llama.cpp supports starting from bfloat) should
    be done like this; bfloat16 -> float32 -> float32 ggml -> quantized ggml.'
  created_at: 2023-05-01 14:25:33+00:00
  edited: true
  hidden: false
  id: 644fd9edd5f7dafcfa61dd7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-05-01T15:52:10.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: "<blockquote>\n<p>Here's a quote of a quote from 10 minutes ago by 0x000011b;\
          \ \"It's possible to do conversion from bfloat back to 32bit float with\
          \ zero loss in precision as you're just padding each 16bit bfloat value\
          \ with zeroes in the mantissa to convert it to 32-bits.\"</p>\n</blockquote>\n\
          <p>Haha, that's a quote straight from <span data-props=\"{&quot;user&quot;:&quot;RazielAU&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RazielAU\"\
          >@<span class=\"underline\">RazielAU</span></a></span>\n\n\t</span></span>\
          \ above actually. Indeed, me and concedo were talking about how <code>bf16\
          \ -&gt; fp32 -&gt; ggml</code> would likely be better than <code>bf16 -&gt;\
          \ fp16 -&gt; ggml</code>. Either way, the TL;DR for now is</p>\n<ul>\n<li>fp16\
          \ was giving me problems with FSDP, but <em>seems</em> fine with DeepSpeed</li>\n\
          <li>Since there's <em>so</em> much code out there that isn't even aware\
          \ bf16 exists, I'm probably better off training (or at least releasing a\
          \ version in) fp16.<ul>\n<li>...unless <code>bf16 -&gt; fp32 -&gt; desired\
          \ format</code> is better than <code>fp16 -&gt; desired format</code>. Lots\
          \ of people are going around trying different quantization and conversion\
          \ methods then running perplexity tests, so at the moment I'm just waiting\
          \ on this info to bubble back up to me to make a proper final decision.</li>\n\
          </ul>\n</li>\n</ul>\n"
        raw: "> Here's a quote of a quote from 10 minutes ago by 0x000011b; \"It's\
          \ possible to do conversion from bfloat back to 32bit float with zero loss\
          \ in precision as you're just padding each 16bit bfloat value with zeroes\
          \ in the mantissa to convert it to 32-bits.\"\n\nHaha, that's a quote straight\
          \ from @RazielAU above actually. Indeed, me and concedo were talking about\
          \ how `bf16 -> fp32 -> ggml` would likely be better than `bf16 -> fp16 ->\
          \ ggml`. Either way, the TL;DR for now is\n\n- fp16 was giving me problems\
          \ with FSDP, but _seems_ fine with DeepSpeed\n- Since there's _so_ much\
          \ code out there that isn't even aware bf16 exists, I'm probably better\
          \ off training (or at least releasing a version in) fp16.\n  - ...unless\
          \ `bf16 -> fp32 -> desired format` is better than `fp16 -> desired format`.\
          \ Lots of people are going around trying different quantization and conversion\
          \ methods then running perplexity tests, so at the moment I'm just waiting\
          \ on this info to bubble back up to me to make a proper final decision."
        updatedAt: '2023-05-01T15:52:10.776Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzuyn
    id: 644fe02a20ba3e3e4beb5464
    type: comment
  author: 11b
  content: "> Here's a quote of a quote from 10 minutes ago by 0x000011b; \"It's possible\
    \ to do conversion from bfloat back to 32bit float with zero loss in precision\
    \ as you're just padding each 16bit bfloat value with zeroes in the mantissa to\
    \ convert it to 32-bits.\"\n\nHaha, that's a quote straight from @RazielAU above\
    \ actually. Indeed, me and concedo were talking about how `bf16 -> fp32 -> ggml`\
    \ would likely be better than `bf16 -> fp16 -> ggml`. Either way, the TL;DR for\
    \ now is\n\n- fp16 was giving me problems with FSDP, but _seems_ fine with DeepSpeed\n\
    - Since there's _so_ much code out there that isn't even aware bf16 exists, I'm\
    \ probably better off training (or at least releasing a version in) fp16.\n  -\
    \ ...unless `bf16 -> fp32 -> desired format` is better than `fp16 -> desired format`.\
    \ Lots of people are going around trying different quantization and conversion\
    \ methods then running perplexity tests, so at the moment I'm just waiting on\
    \ this info to bubble back up to me to make a proper final decision."
  created_at: 2023-05-01 14:52:10+00:00
  edited: false
  hidden: false
  id: 644fe02a20ba3e3e4beb5464
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-01T15:59:27.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>Hahaha, I don''t mind. Where''s this discussion happening? Just
          curious to see how it goes.</p>

          '
        raw: Hahaha, I don't mind. Where's this discussion happening? Just curious
          to see how it goes.
        updatedAt: '2023-05-01T15:59:27.453Z'
      numEdits: 0
      reactions: []
    id: 644fe1dfd5f7dafcfa629cda
    type: comment
  author: RazielAU
  content: Hahaha, I don't mind. Where's this discussion happening? Just curious to
    see how it goes.
  created_at: 2023-05-01 14:59:27+00:00
  edited: false
  hidden: false
  id: 644fe1dfd5f7dafcfa629cda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-05-01T16:05:09.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<blockquote>

          <p>Hahaha, I don''t mind. Where''s this discussion happening? Just curious
          to see how it goes.</p>

          </blockquote>

          <p>On the Kobold Discord server. Most of the discussions we''ve been having
          about model quality and behavior have been on the koboldcpp channel and
          on the Pygmalion model thread there - I believe if you Google for "Kobold
          discord" you should find an invite link.</p>

          '
        raw: '> Hahaha, I don''t mind. Where''s this discussion happening? Just curious
          to see how it goes.


          On the Kobold Discord server. Most of the discussions we''ve been having
          about model quality and behavior have been on the koboldcpp channel and
          on the Pygmalion model thread there - I believe if you Google for "Kobold
          discord" you should find an invite link.'
        updatedAt: '2023-05-01T16:05:09.792Z'
      numEdits: 0
      reactions: []
    id: 644fe335577838187efc98e5
    type: comment
  author: 11b
  content: '> Hahaha, I don''t mind. Where''s this discussion happening? Just curious
    to see how it goes.


    On the Kobold Discord server. Most of the discussions we''ve been having about
    model quality and behavior have been on the koboldcpp channel and on the Pygmalion
    model thread there - I believe if you Google for "Kobold discord" you should find
    an invite link.'
  created_at: 2023-05-01 15:05:09+00:00
  edited: false
  hidden: false
  id: 644fe335577838187efc98e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-01T16:06:53.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>I''m already on there, just wasn''t aware of the discussion...</p>

          '
        raw: I'm already on there, just wasn't aware of the discussion...
        updatedAt: '2023-05-01T16:06:53.581Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - 11b
    id: 644fe39dd5f7dafcfa62c7f9
    type: comment
  author: RazielAU
  content: I'm already on there, just wasn't aware of the discussion...
  created_at: 2023-05-01 15:06:53+00:00
  edited: false
  hidden: false
  id: 644fe39dd5f7dafcfa62c7f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
      fullname: Ivan Stepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanstepanovftw
      type: user
    createdAt: '2023-05-04T02:29:56.000Z'
    data:
      edited: false
      editors:
      - ivanstepanovftw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
          fullname: Ivan Stepanov
          isHf: false
          isPro: false
          name: ivanstepanovftw
          type: user
        html: '<p>Found this discussion accidently. I have added PR for converting
          pytorch model to GGML <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1309">https://github.com/ggerganov/llama.cpp/pull/1309</a></p>

          '
        raw: Found this discussion accidently. I have added PR for converting pytorch
          model to GGML https://github.com/ggerganov/llama.cpp/pull/1309
        updatedAt: '2023-05-04T02:29:56.550Z'
      numEdits: 0
      reactions: []
    id: 645318a45ac68a5b01ae292f
    type: comment
  author: ivanstepanovftw
  content: Found this discussion accidently. I have added PR for converting pytorch
    model to GGML https://github.com/ggerganov/llama.cpp/pull/1309
  created_at: 2023-05-04 01:29:56+00:00
  edited: false
  hidden: false
  id: 645318a45ac68a5b01ae292f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
      fullname: Ivan Stepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanstepanovftw
      type: user
    createdAt: '2023-05-04T02:40:48.000Z'
    data:
      edited: false
      editors:
      - ivanstepanovftw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
          fullname: Ivan Stepanov
          isHf: false
          isPro: false
          name: ivanstepanovftw
          type: user
        html: "<p>It converts to FP32 by default, so it should be precise ;D</p>\n\
          <pre><code>$ ./cmake-build-release/bin/main -m models/metharme-7b/ggml-model-q4_0.bin\
          \ --threads 8 --prompt $'Manager\\'s Persona: Manager I work with in my\
          \ company.\\n&lt;START&gt;\\nYou: Sorry, I was late... Don\\'t fire me...\\\
          n' --ctx_size 1024 --tfs 0.98 --mirostat 2 --mirostat_ent 4 --n_predict\
          \ 40\n\nmain: seed = 1683167819\n...\nsampling: repeat_last_n = -1, repeat_penalty\
          \ = 1.000000, presence_penalty = 0.000000, frequency_penalty = 0.000000,\
          \ top_k = 0, tfs_z = 0.980000, top_p = 1.000000, typical_p = 1.000000, temp\
          \ = 1.000000, mirostat = 2, mirostat_lr = 0.100000, mirostat_ent = 4.000000\n\
          generate: n_ctx = 1024, n_batch = 512, n_predict = 40, n_keep = 0\n\n Manager's\
          \ Persona: Manager I work with in my company.\n&lt;START&gt;\nYou: Sorry,\
          \ I was late... Don't fire me...\nManager: (shaking his head) What else\
          \ would I do with you?\nYou: I mean, I didn't plan to be late. It just happened...\
          \ I'm sorry...\n</code></pre>\n"
        raw: "It converts to FP32 by default, so it should be precise ;D\n```\n$ ./cmake-build-release/bin/main\
          \ -m models/metharme-7b/ggml-model-q4_0.bin --threads 8 --prompt $'Manager\\\
          's Persona: Manager I work with in my company.\\n<START>\\nYou: Sorry, I\
          \ was late... Don\\'t fire me...\\n' --ctx_size 1024 --tfs 0.98 --mirostat\
          \ 2 --mirostat_ent 4 --n_predict 40\n\nmain: seed = 1683167819\n...\nsampling:\
          \ repeat_last_n = -1, repeat_penalty = 1.000000, presence_penalty = 0.000000,\
          \ frequency_penalty = 0.000000, top_k = 0, tfs_z = 0.980000, top_p = 1.000000,\
          \ typical_p = 1.000000, temp = 1.000000, mirostat = 2, mirostat_lr = 0.100000,\
          \ mirostat_ent = 4.000000\ngenerate: n_ctx = 1024, n_batch = 512, n_predict\
          \ = 40, n_keep = 0\n\n Manager's Persona: Manager I work with in my company.\n\
          <START>\nYou: Sorry, I was late... Don't fire me...\nManager: (shaking his\
          \ head) What else would I do with you?\nYou: I mean, I didn't plan to be\
          \ late. It just happened... I'm sorry...\n```"
        updatedAt: '2023-05-04T02:40:48.170Z'
      numEdits: 0
      reactions: []
    id: 64531b308fe6558e328c7c01
    type: comment
  author: ivanstepanovftw
  content: "It converts to FP32 by default, so it should be precise ;D\n```\n$ ./cmake-build-release/bin/main\
    \ -m models/metharme-7b/ggml-model-q4_0.bin --threads 8 --prompt $'Manager\\'s\
    \ Persona: Manager I work with in my company.\\n<START>\\nYou: Sorry, I was late...\
    \ Don\\'t fire me...\\n' --ctx_size 1024 --tfs 0.98 --mirostat 2 --mirostat_ent\
    \ 4 --n_predict 40\n\nmain: seed = 1683167819\n...\nsampling: repeat_last_n =\
    \ -1, repeat_penalty = 1.000000, presence_penalty = 0.000000, frequency_penalty\
    \ = 0.000000, top_k = 0, tfs_z = 0.980000, top_p = 1.000000, typical_p = 1.000000,\
    \ temp = 1.000000, mirostat = 2, mirostat_lr = 0.100000, mirostat_ent = 4.000000\n\
    generate: n_ctx = 1024, n_batch = 512, n_predict = 40, n_keep = 0\n\n Manager's\
    \ Persona: Manager I work with in my company.\n<START>\nYou: Sorry, I was late...\
    \ Don't fire me...\nManager: (shaking his head) What else would I do with you?\n\
    You: I mean, I didn't plan to be late. It just happened... I'm sorry...\n```"
  created_at: 2023-05-04 01:40:48+00:00
  edited: false
  hidden: false
  id: 64531b308fe6558e328c7c01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-07T04:52:02.000Z'
    data:
      edited: false
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>Nice, thanks for dropping in and letting us know!</p>

          '
        raw: Nice, thanks for dropping in and letting us know!
        updatedAt: '2023-05-07T04:52:02.753Z'
      numEdits: 0
      reactions: []
    id: 64572e72cd6567f52fc0ad06
    type: comment
  author: RazielAU
  content: Nice, thanks for dropping in and letting us know!
  created_at: 2023-05-07 03:52:02+00:00
  edited: false
  hidden: false
  id: 64572e72cd6567f52fc0ad06
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: PygmalionAI/pygmalion-7b
repo_type: model
status: open
target_branch: null
title: Reasoning behind bfloat instead of float?
