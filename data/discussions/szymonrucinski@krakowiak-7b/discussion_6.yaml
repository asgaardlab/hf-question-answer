!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grzenkom
conflicting_files: null
created_at: 2023-09-15 10:04:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
      fullname: Marek Grzenkowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grzenkom
      type: user
    createdAt: '2023-09-15T11:04:11.000Z'
    data:
      edited: false
      editors:
      - grzenkom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9498019218444824
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
          fullname: Marek Grzenkowicz
          isHf: false
          isPro: false
          name: grzenkom
          type: user
        html: '<p>I have a laptop with 32GB of RAM memory and Quadro T1000 card with
          4GB of memory. I am able to load the model successfully, but the text generation
          is slow (i.e. I am yet to see a single generated token).</p>

          <p>Is there a chance to get any output in reasonable time? Let''s say <strong>15-20
          minutes would be OK</strong> for the experiments I am conducting.</p>

          <p>I am wondering if there is anything I could improved in my setup <strong>or</strong>
          the only way to go is switching to a more powerful machine right away.</p>

          '
        raw: "I have a laptop with 32GB of RAM memory and Quadro T1000 card with 4GB\
          \ of memory. I am able to load the model successfully, but the text generation\
          \ is slow (i.e. I am yet to see a single generated token).\r\n\r\nIs there\
          \ a chance to get any output in reasonable time? Let's say **15-20 minutes\
          \ would be OK** for the experiments I am conducting.\r\n\r\nI am wondering\
          \ if there is anything I could improved in my setup **or** the only way\
          \ to go is switching to a more powerful machine right away."
        updatedAt: '2023-09-15T11:04:11.899Z'
      numEdits: 0
      reactions: []
    id: 65043a2beac293e2c3ed93be
    type: comment
  author: grzenkom
  content: "I have a laptop with 32GB of RAM memory and Quadro T1000 card with 4GB\
    \ of memory. I am able to load the model successfully, but the text generation\
    \ is slow (i.e. I am yet to see a single generated token).\r\n\r\nIs there a chance\
    \ to get any output in reasonable time? Let's say **15-20 minutes would be OK**\
    \ for the experiments I am conducting.\r\n\r\nI am wondering if there is anything\
    \ I could improved in my setup **or** the only way to go is switching to a more\
    \ powerful machine right away."
  created_at: 2023-09-15 10:04:11+00:00
  edited: false
  hidden: false
  id: 65043a2beac293e2c3ed93be
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: szymonrucinski/krakowiak-7b
repo_type: model
status: open
target_branch: null
title: Using a laptop with a 4GB GPU for inference - does it make at all sense to
  try?
