!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grzenkom
conflicting_files: null
created_at: 2023-09-13 10:36:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
      fullname: Marek Grzenkowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grzenkom
      type: user
    createdAt: '2023-09-13T11:36:04.000Z'
    data:
      edited: false
      editors:
      - grzenkom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6851423382759094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
          fullname: Marek Grzenkowicz
          isHf: false
          isPro: false
          name: grzenkom
          type: user
        html: '<p>I am trying to load the model with LangChain:</p>

          <pre><code>from langchain.llms import HuggingFacePipeline


          model_id, task = "szymonrucinski/krakowiak-7b", "text-generation"

          model = HuggingFacePipeline.from_model_id(model_id=model_id, task=task)

          </code></pre>

          <p>Is it a problem with my code <strong>or</strong> is the file <code>config.json</code>
          indeed missing?</p>

          '
        raw: "I am trying to load the model with LangChain:\r\n\r\n```\r\nfrom langchain.llms\
          \ import HuggingFacePipeline\r\n\r\nmodel_id, task = \"szymonrucinski/krakowiak-7b\"\
          , \"text-generation\"\r\nmodel = HuggingFacePipeline.from_model_id(model_id=model_id,\
          \ task=task)\r\n```\r\n\r\nIs it a problem with my code **or** is the file\
          \ `config.json` indeed missing?"
        updatedAt: '2023-09-13T11:36:04.826Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - szymonrucinski
    id: 65019ea4a37c86bad8f859a9
    type: comment
  author: grzenkom
  content: "I am trying to load the model with LangChain:\r\n\r\n```\r\nfrom langchain.llms\
    \ import HuggingFacePipeline\r\n\r\nmodel_id, task = \"szymonrucinski/krakowiak-7b\"\
    , \"text-generation\"\r\nmodel = HuggingFacePipeline.from_model_id(model_id=model_id,\
    \ task=task)\r\n```\r\n\r\nIs it a problem with my code **or** is the file `config.json`\
    \ indeed missing?"
  created_at: 2023-09-13 10:36:04+00:00
  edited: false
  hidden: false
  id: 65019ea4a37c86bad8f859a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e8ae87dd2c4effdd61519c/hNuUBdQ9D65y-U-ojC1C5.png?w=200&h=200&f=face
      fullname: "Szymon Ruci\u0144ski"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: szymonrucinski
      type: user
    createdAt: '2023-09-13T12:35:30.000Z'
    data:
      edited: true
      editors:
      - szymonrucinski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5260328650474548
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e8ae87dd2c4effdd61519c/hNuUBdQ9D65y-U-ojC1C5.png?w=200&h=200&f=face
          fullname: "Szymon Ruci\u0144ski"
          isHf: false
          isPro: false
          name: szymonrucinski
          type: user
        html: "<p>Hi Grzegorz! \U0001F91D\U0001F603<br>It is not a model file. In\
          \ this repo there is only a PEFT adapter that you need to merge into the\
          \ model using the following code:</p>\n<pre><code>from peft import PeftModel,\
          \ PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ pipeline\nfrom pprint import pprint\n\nconfig = PeftConfig.from_pretrained(\"\
          szymonrucinski/krakowiak-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          meta-llama/Llama-2-\"7b-hf,\n\ntokenizer = AutoTokenizer.from_pretrained(f\"\
          meta-llama/Llama-2-7b-hf\", )\nmodel = PeftModel.from_pretrained(model,\
          \ f\"szymonrucinski/krakowiak-7b\")\n\npipeline = pipeline(\n    \"text-generation\"\
          ,\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n\
          )\n</code></pre>\n<p>Good luck! </p>\n"
        raw: "Hi Grzegorz! \U0001F91D\U0001F603\nIt is not a model file. In this repo\
          \ there is only a PEFT adapter that you need to merge into the model using\
          \ the following code:\n```\nfrom peft import PeftModel, PeftConfig\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom\
          \ pprint import pprint\n\nconfig = PeftConfig.from_pretrained(\"szymonrucinski/krakowiak-7b\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-\"\
          7b-hf,\n\ntokenizer = AutoTokenizer.from_pretrained(f\"meta-llama/Llama-2-7b-hf\"\
          , )\nmodel = PeftModel.from_pretrained(model, f\"szymonrucinski/krakowiak-7b\"\
          )\n\npipeline = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n```\nGood luck! "
        updatedAt: '2023-09-13T12:36:00.687Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grzenkom
    id: 6501ac920cc65d59987d919c
    type: comment
  author: szymonrucinski
  content: "Hi Grzegorz! \U0001F91D\U0001F603\nIt is not a model file. In this repo\
    \ there is only a PEFT adapter that you need to merge into the model using the\
    \ following code:\n```\nfrom peft import PeftModel, PeftConfig\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom pprint import pprint\n\
    \nconfig = PeftConfig.from_pretrained(\"szymonrucinski/krakowiak-7b\")\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-\"7b-hf,\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(f\"meta-llama/Llama-2-7b-hf\", )\nmodel = PeftModel.from_pretrained(model,\
    \ f\"szymonrucinski/krakowiak-7b\")\n\npipeline = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n```\n\
    Good luck! "
  created_at: 2023-09-13 11:35:30+00:00
  edited: true
  hidden: false
  id: 6501ac920cc65d59987d919c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
      fullname: Marek Grzenkowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grzenkom
      type: user
    createdAt: '2023-09-15T08:48:08.000Z'
    data:
      edited: false
      editors:
      - grzenkom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40566733479499817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
          fullname: Marek Grzenkowicz
          isHf: false
          isPro: false
          name: grzenkom
          type: user
        html: '<p>Thanks for educating me about the PEFT adapters!</p>

          <p>I had to tweak your snippet a little bit and finally got the following
          to run:</p>

          <pre><code>import os

          from peft import PeftModel, PeftConfig

          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline



          HF_TOKEN = os.getenv("HF_TOKEN")


          peft_model_id = "szymonrucinski/krakowiak-7b"

          peft_config = PeftConfig.from_pretrained(peft_model_id)


          base_model_id = peft_config.base_model_name_or_path

          base_model = AutoModelForCausalLM.from_pretrained(base_model_id, token=HF_TOKEN)

          base_tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=HF_TOKEN)


          peft_model = PeftModel.from_pretrained(base_model, peft_model_id)


          pipeline = pipeline(task="text-generation", model=peft_model, tokenizer=base_tokenizer,
          device_map="auto")

          </code></pre>

          '
        raw: 'Thanks for educating me about the PEFT adapters!


          I had to tweak your snippet a little bit and finally got the following to
          run:


          ```

          import os

          from peft import PeftModel, PeftConfig

          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline



          HF_TOKEN = os.getenv("HF_TOKEN")


          peft_model_id = "szymonrucinski/krakowiak-7b"

          peft_config = PeftConfig.from_pretrained(peft_model_id)


          base_model_id = peft_config.base_model_name_or_path

          base_model = AutoModelForCausalLM.from_pretrained(base_model_id, token=HF_TOKEN)

          base_tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=HF_TOKEN)


          peft_model = PeftModel.from_pretrained(base_model, peft_model_id)


          pipeline = pipeline(task="text-generation", model=peft_model, tokenizer=base_tokenizer,
          device_map="auto")

          ```'
        updatedAt: '2023-09-15T08:48:08.583Z'
      numEdits: 0
      reactions: []
    id: 65041a48610734234385bff8
    type: comment
  author: grzenkom
  content: 'Thanks for educating me about the PEFT adapters!


    I had to tweak your snippet a little bit and finally got the following to run:


    ```

    import os

    from peft import PeftModel, PeftConfig

    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline



    HF_TOKEN = os.getenv("HF_TOKEN")


    peft_model_id = "szymonrucinski/krakowiak-7b"

    peft_config = PeftConfig.from_pretrained(peft_model_id)


    base_model_id = peft_config.base_model_name_or_path

    base_model = AutoModelForCausalLM.from_pretrained(base_model_id, token=HF_TOKEN)

    base_tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=HF_TOKEN)


    peft_model = PeftModel.from_pretrained(base_model, peft_model_id)


    pipeline = pipeline(task="text-generation", model=peft_model, tokenizer=base_tokenizer,
    device_map="auto")

    ```'
  created_at: 2023-09-15 07:48:08+00:00
  edited: false
  hidden: false
  id: 65041a48610734234385bff8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f80b6dc375310fa0a804da/-IRiAdSgjCtrRoMnUwBtn.png?w=200&h=200&f=face
      fullname: Marek Grzenkowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grzenkom
      type: user
    createdAt: '2023-09-15T08:48:28.000Z'
    data:
      status: closed
    id: 65041a5c0c120aa7314cd53e
    type: status-change
  author: grzenkom
  created_at: 2023-09-15 07:48:28+00:00
  id: 65041a5c0c120aa7314cd53e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: szymonrucinski/krakowiak-7b
repo_type: model
status: closed
target_branch: null
title: 'OSError: szymonrucinski/krakowiak-7b does not appear to have a file named
  config.json'
