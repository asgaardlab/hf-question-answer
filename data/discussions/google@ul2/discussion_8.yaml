!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leshanbog
conflicting_files: null
created_at: 2022-11-02 08:22:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665684195222-63484caabd99107eacce7135.jpeg?w=200&h=200&f=face
      fullname: Alexey Bukhtiyarov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leshanbog
      type: user
    createdAt: '2022-11-02T09:22:45.000Z'
    data:
      edited: false
      editors:
      - leshanbog
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665684195222-63484caabd99107eacce7135.jpeg?w=200&h=200&f=face
          fullname: Alexey Bukhtiyarov
          isHf: false
          isPro: false
          name: leshanbog
          type: user
        html: '<p>Hi!<br>Thanks for sharing this 20B model with the community! From
          <a rel="nofollow" href="https://arxiv.org/abs/2205.05131">your paper</a>
          it seems that smaller model also benefit from this kind of pretraining.
          Do you have any plans to release a UL2-base or something of that scale?</p>

          '
        raw: "Hi!\r\nThanks for sharing this 20B model with the community! From [your\
          \ paper](https://arxiv.org/abs/2205.05131) it seems that smaller model also\
          \ benefit from this kind of pretraining. Do you have any plans to release\
          \ a UL2-base or something of that scale?"
        updatedAt: '2022-11-02T09:22:45.136Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - thean
        - itsnamgyu
        - jumelet
        - tanmaylaud
        - Siddharth63
        - Francois2
        - khu
    id: 636236e54d4a190d6b10fe52
    type: comment
  author: leshanbog
  content: "Hi!\r\nThanks for sharing this 20B model with the community! From [your\
    \ paper](https://arxiv.org/abs/2205.05131) it seems that smaller model also benefit\
    \ from this kind of pretraining. Do you have any plans to release a UL2-base or\
    \ something of that scale?"
  created_at: 2022-11-02 08:22:45+00:00
  edited: false
  hidden: false
  id: 636236e54d4a190d6b10fe52
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: google/ul2
repo_type: model
status: open
target_branch: null
title: Smaller UL2 models
