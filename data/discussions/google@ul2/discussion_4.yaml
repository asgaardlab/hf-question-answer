!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dnhkng
conflicting_files: null
created_at: 2022-07-11 09:24:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnhkng
      type: user
    createdAt: '2022-07-11T10:24:20.000Z'
    data:
      edited: false
      editors:
      - dnhkng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
          fullname: David
          isHf: false
          isPro: false
          name: dnhkng
          type: user
        html: '<p>Is there any documentation on splitting such models up for inferencing
          over multiple GPU''s?<br>2nd-hand 3090 Ti''s are getting quite affordable
          now, and I believe the model would be able to fit on the ram of two such
          cards, at least purely by size.</p>

          '
        raw: "Is there any documentation on splitting such models up for inferencing\
          \ over multiple GPU's?\r\n2nd-hand 3090 Ti's are getting quite affordable\
          \ now, and I believe the model would be able to fit on the ram of two such\
          \ cards, at least purely by size."
        updatedAt: '2022-07-11T10:24:21.000Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - liangyuch
    id: 62cbfa54142d627ff847e920
    type: comment
  author: dnhkng
  content: "Is there any documentation on splitting such models up for inferencing\
    \ over multiple GPU's?\r\n2nd-hand 3090 Ti's are getting quite affordable now,\
    \ and I believe the model would be able to fit on the ram of two such cards, at\
    \ least purely by size."
  created_at: 2022-07-11 09:24:20+00:00
  edited: false
  hidden: false
  id: 62cbfa54142d627ff847e920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
      fullname: Liangyu Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liangyuch
      type: user
    createdAt: '2022-07-18T07:49:51.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
          fullname: Liangyu Chen
          isHf: false
          isPro: false
          name: liangyuch
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-07-24T06:22:07.866Z'
      numEdits: 1
      reactions: []
    id: 62d5109f018b7ad7b222920e
    type: comment
  author: liangyuch
  content: This comment has been hidden
  created_at: 2022-07-18 06:49:51+00:00
  edited: true
  hidden: true
  id: 62d5109f018b7ad7b222920e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
      fullname: Liangyu Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liangyuch
      type: user
    createdAt: '2022-07-24T10:51:12.000Z'
    data:
      edited: false
      editors:
      - liangyuch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
          fullname: Liangyu Chen
          isHf: false
          isPro: false
          name: liangyuch
          type: user
        html: "<p>Hi! I think I managed to run UL2 on 2 RTX3090 GPUs. I deleted last\
          \ comment to avoid confusion.<br>I configured hugging face accelarate and\
          \ ran the code snippet below. It worked! Seems that the assignment of layers\
          \ on multi-GPU is by auto device map. Hope it helps.</p>\n<pre><code class=\"\
          language-python\">logging.info(<span class=\"hljs-string\">'build tokenizer'</span>)\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          google/ul2\"</span>)\nlogging.info(<span class=\"hljs-string\">'build model'</span>)\n\
          \                                                                      \
          \                        \nmodel = T5ForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"google/ul2\"</span>, low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>, torch_dtype=torch.bfloat16, \n             \
          \                                       device_map=<span class=\"hljs-string\"\
          >'auto'</span>)\n\ninput_string = <span class=\"hljs-string\">\"[S2S] Mr.\
          \ Dursley was the director of a firm called Grunnings, which made drills.\
          \ He was a big, solid man with a bald head. Mrs. Dursley was thin and blonde\
          \ and more than the usual amount of neck, which came in very useful as she\
          \ spent so much of her time craning over garden fences, spying on the neighbours.\
          \ The Dursleys had a small son called Dudley and in their opinion there\
          \ was no finer boy anywhere &lt;extra_id_0&gt;\"</span>                \
          \                               \n\ninputs = tokenizer(input_string, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).input_ids.to(<span class=\"hljs-string\"\
          >\"cuda\"</span>)\n\nlogging.info(<span class=\"hljs-string\">'generate\
          \ output'</span>)\n\noutputs = model.generate(inputs, max_length=<span class=\"\
          hljs-number\">200</span>)\n\nlogging.info(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "Hi! I think I managed to run UL2 on 2 RTX3090 GPUs. I deleted last comment\
          \ to avoid confusion.\nI configured hugging face accelarate and ran the\
          \ code snippet below. It worked! Seems that the assignment of layers on\
          \ multi-GPU is by auto device map. Hope it helps.\n\n```python\nlogging.info('build\
          \ tokenizer')\ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\"\
          )\nlogging.info('build model')\n                                       \
          \                                                       \nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, \n   \
          \                                                 device_map='auto')\n\n\
          input_string = \"[S2S] Mr. Dursley was the director of a firm called Grunnings,\
          \ which made drills. He was a big, solid man with a bald head. Mrs. Dursley\
          \ was thin and blonde and more than the usual amount of neck, which came\
          \ in very useful as she spent so much of her time craning over garden fences,\
          \ spying on the neighbours. The Dursleys had a small son called Dudley and\
          \ in their opinion there was no finer boy anywhere <extra_id_0>\"      \
          \                                         \n\ninputs = tokenizer(input_string,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n\nlogging.info('generate\
          \ output')\n\noutputs = model.generate(inputs, max_length=200)\n\nlogging.info(tokenizer.decode(outputs[0]))\n\
          ```"
        updatedAt: '2022-07-24T10:51:12.844Z'
      numEdits: 0
      reactions: []
    id: 62dd24208456396d4f87d0fb
    type: comment
  author: liangyuch
  content: "Hi! I think I managed to run UL2 on 2 RTX3090 GPUs. I deleted last comment\
    \ to avoid confusion.\nI configured hugging face accelarate and ran the code snippet\
    \ below. It worked! Seems that the assignment of layers on multi-GPU is by auto\
    \ device map. Hope it helps.\n\n```python\nlogging.info('build tokenizer')\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"google/ul2\")\nlogging.info('build model')\n\
    \                                                                            \
    \                  \nmodel = T5ForConditionalGeneration.from_pretrained(\"google/ul2\"\
    , low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, \n                     \
    \                               device_map='auto')\n\ninput_string = \"[S2S] Mr.\
    \ Dursley was the director of a firm called Grunnings, which made drills. He was\
    \ a big, solid man with a bald head. Mrs. Dursley was thin and blonde and more\
    \ than the usual amount of neck, which came in very useful as she spent so much\
    \ of her time craning over garden fences, spying on the neighbours. The Dursleys\
    \ had a small son called Dudley and in their opinion there was no finer boy anywhere\
    \ <extra_id_0>\"                                               \n\ninputs = tokenizer(input_string,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\n\nlogging.info('generate output')\n\
    \noutputs = model.generate(inputs, max_length=200)\n\nlogging.info(tokenizer.decode(outputs[0]))\n\
    ```"
  created_at: 2022-07-24 09:51:12+00:00
  edited: false
  hidden: false
  id: 62dd24208456396d4f87d0fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnhkng
      type: user
    createdAt: '2022-07-24T13:53:42.000Z'
    data:
      edited: false
      editors:
      - dnhkng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
          fullname: David
          isHf: false
          isPro: false
          name: dnhkng
          type: user
        html: "<p>I have it running on the RTX Titans, but the output looks very weird.<br>I\
          \ first had to load the model, and then save if as FP16, as my cards do\
          \ not support bfloat16.</p>\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModelForSeq2SeqLM, AutoConfig\nfrom accelerate import init_empty_weights,\
          \ infer_auto_device_map, load_checkpoint_and_dispatch\nimport torch\n\n\
          import torch\nmodel_name = \"google/ul2\"\nconfig = AutoConfig.from_pretrained(model_name)\n\
          \nwith init_empty_weights():\n        model = AutoModelForSeq2SeqLM.from_config(config)\n\
          \        \ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n\n\
          device_map = infer_auto_device_map(model,dtype=torch.float16)\nweights_path\
          \ = '.'\nload_checkpoint_and_dispatch(\n    model,\n    weights_path,\n\
          \    device_map=device_map,\n    offload_folder=None,\n    offload_state_dict=False,\n\
          \    dtype=\"float16\"\n)\n\nprompt = 'Machine learning is the '\ninput_tokenized\
          \ = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(input_tokenized[\"\
          input_ids\"].to(1), do_sample=True,max_length=100,temperature=0.9,top_k=50,top_p=0.9)\n\
          output_text = tokenizer.decode(output[0].tolist())\nprint(output_text)\n\
          </code></pre>\n<p>The output from this script is quite fast after loading\
          \ the model, a few tens of seconds, but the output reads as:</p>\n<pre><code>'&lt;pad&gt;&lt;extra_id_0&gt;\
          \ uimitmpl\u0103 lumin\u0103n&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;extra_id_0&gt;\
          \ uimit uimit&lt;pad&gt;&lt;extra_id_0&gt; \nfor the incendiu Project uimit\
          \ uimitmpl\u0103 machine learning uimit&lt;extra_id_10&gt; is&lt;pad&gt;&lt;pad&gt;&lt;extra_id_0&gt;\
          \ \nqEinen incendiu \xEEn incendiu incendiu&lt;pad&gt;&lt;pad&gt;&lt;extra_id_0&gt;recommending&lt;pad&gt;&lt;extra_id_0&gt;&lt;pad&gt;\n\
          &lt;extra_id_0&gt; uimit lumin\u0103 combining data and ac\u0163iune ac\u0163\
          iune code uimit learning lumin\u0103 presiune presiune \nac\u0163iune; incendiu&lt;extra_id_7&gt;\
          \ uimitency lumin\u0103 lumin\u0103 Artificial&lt;extra_id_7&gt; lumin\u0103\
          \ incendiu incendiu \nincendiu&lt;pad&gt;&lt;extra_id_0&gt;&lt;extra_id_74&gt;&lt;extra_id_7&gt;&lt;extra_id_9&gt;a\
          \ lumin\u0103 lumin\u0103&lt;extra_id_9&gt;\n&lt;extra_id_7&gt; uimit gradini\
          \ treptat lumin\u0103 presiune deep incendiu lumin\u0103 lumin\u0103 knowing\
          \ that works \nis t\xE2rziu&lt;pad&gt;&lt;extra_id_0&gt;&lt;pad&gt;&lt;pad&gt;'\n\
          </code></pre>\n<p>This might be to overflows in FP16 vs BFP16 maybe?</p>\n"
        raw: "I have it running on the RTX Titans, but the output looks very weird.\n\
          I first had to load the model, and then save if as FP16, as my cards do\
          \ not support bfloat16.\n\n```\nfrom transformers import AutoTokenizer,\
          \ AutoModelForSeq2SeqLM, AutoConfig\nfrom accelerate import init_empty_weights,\
          \ infer_auto_device_map, load_checkpoint_and_dispatch\nimport torch\n\n\
          import torch\nmodel_name = \"google/ul2\"\nconfig = AutoConfig.from_pretrained(model_name)\n\
          \nwith init_empty_weights():\n        model = AutoModelForSeq2SeqLM.from_config(config)\n\
          \        \ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n\n\
          device_map = infer_auto_device_map(model,dtype=torch.float16)\nweights_path\
          \ = '.'\nload_checkpoint_and_dispatch(\n    model,\n    weights_path,\n\
          \    device_map=device_map,\n    offload_folder=None,\n    offload_state_dict=False,\n\
          \    dtype=\"float16\"\n)\n\nprompt = 'Machine learning is the '\ninput_tokenized\
          \ = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(input_tokenized[\"\
          input_ids\"].to(1), do_sample=True,max_length=100,temperature=0.9,top_k=50,top_p=0.9)\n\
          output_text = tokenizer.decode(output[0].tolist())\nprint(output_text)\n\
          ```\n\nThe output from this script is quite fast after loading the model,\
          \ a few tens of seconds, but the output reads as:\n\n```\n'<pad><extra_id_0>\
          \ uimitmpl\u0103 lumin\u0103n<pad><pad><pad><extra_id_0> uimit uimit<pad><extra_id_0>\
          \ \nfor the incendiu Project uimit uimitmpl\u0103 machine learning uimit<extra_id_10>\
          \ is<pad><pad><extra_id_0> \nqEinen incendiu \xEEn incendiu incendiu<pad><pad><extra_id_0>recommending<pad><extra_id_0><pad>\n\
          <extra_id_0> uimit lumin\u0103 combining data and ac\u0163iune ac\u0163\
          iune code uimit learning lumin\u0103 presiune presiune \nac\u0163iune; incendiu<extra_id_7>\
          \ uimitency lumin\u0103 lumin\u0103 Artificial<extra_id_7> lumin\u0103 incendiu\
          \ incendiu \nincendiu<pad><extra_id_0><extra_id_74><extra_id_7><extra_id_9>a\
          \ lumin\u0103 lumin\u0103<extra_id_9>\n<extra_id_7> uimit gradini treptat\
          \ lumin\u0103 presiune deep incendiu lumin\u0103 lumin\u0103 knowing that\
          \ works \nis t\xE2rziu<pad><extra_id_0><pad><pad>'\n```\n\nThis might be\
          \ to overflows in FP16 vs BFP16 maybe?"
        updatedAt: '2022-07-24T13:53:42.111Z'
      numEdits: 0
      reactions: []
    id: 62dd4ee6169bd1d2ef2e92df
    type: comment
  author: dnhkng
  content: "I have it running on the RTX Titans, but the output looks very weird.\n\
    I first had to load the model, and then save if as FP16, as my cards do not support\
    \ bfloat16.\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM,\
    \ AutoConfig\nfrom accelerate import init_empty_weights, infer_auto_device_map,\
    \ load_checkpoint_and_dispatch\nimport torch\n\nimport torch\nmodel_name = \"\
    google/ul2\"\nconfig = AutoConfig.from_pretrained(model_name)\n\nwith init_empty_weights():\n\
    \        model = AutoModelForSeq2SeqLM.from_config(config)\n        \ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"google/ul2\")\n\ndevice_map = infer_auto_device_map(model,dtype=torch.float16)\n\
    weights_path = '.'\nload_checkpoint_and_dispatch(\n    model,\n    weights_path,\n\
    \    device_map=device_map,\n    offload_folder=None,\n    offload_state_dict=False,\n\
    \    dtype=\"float16\"\n)\n\nprompt = 'Machine learning is the '\ninput_tokenized\
    \ = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(input_tokenized[\"\
    input_ids\"].to(1), do_sample=True,max_length=100,temperature=0.9,top_k=50,top_p=0.9)\n\
    output_text = tokenizer.decode(output[0].tolist())\nprint(output_text)\n```\n\n\
    The output from this script is quite fast after loading the model, a few tens\
    \ of seconds, but the output reads as:\n\n```\n'<pad><extra_id_0> uimitmpl\u0103\
    \ lumin\u0103n<pad><pad><pad><extra_id_0> uimit uimit<pad><extra_id_0> \nfor the\
    \ incendiu Project uimit uimitmpl\u0103 machine learning uimit<extra_id_10> is<pad><pad><extra_id_0>\
    \ \nqEinen incendiu \xEEn incendiu incendiu<pad><pad><extra_id_0>recommending<pad><extra_id_0><pad>\n\
    <extra_id_0> uimit lumin\u0103 combining data and ac\u0163iune ac\u0163iune code\
    \ uimit learning lumin\u0103 presiune presiune \nac\u0163iune; incendiu<extra_id_7>\
    \ uimitency lumin\u0103 lumin\u0103 Artificial<extra_id_7> lumin\u0103 incendiu\
    \ incendiu \nincendiu<pad><extra_id_0><extra_id_74><extra_id_7><extra_id_9>a lumin\u0103\
    \ lumin\u0103<extra_id_9>\n<extra_id_7> uimit gradini treptat lumin\u0103 presiune\
    \ deep incendiu lumin\u0103 lumin\u0103 knowing that works \nis t\xE2rziu<pad><extra_id_0><pad><pad>'\n\
    ```\n\nThis might be to overflows in FP16 vs BFP16 maybe?"
  created_at: 2022-07-24 12:53:42+00:00
  edited: false
  hidden: false
  id: 62dd4ee6169bd1d2ef2e92df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
      fullname: Liangyu Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liangyuch
      type: user
    createdAt: '2022-07-24T14:17:55.000Z'
    data:
      edited: false
      editors:
      - liangyuch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg?w=200&h=200&f=face
          fullname: Liangyu Chen
          isHf: false
          isPro: false
          name: liangyuch
          type: user
        html: '<p>I am not an expert in LMs, therefore sorry I can''t reason from
          your code and output.<br>My suggestion is to reproduce the output of the
          given example. I could, with the code I provided.</p>

          '
        raw: 'I am not an expert in LMs, therefore sorry I can''t reason from your
          code and output.

          My suggestion is to reproduce the output of the given example. I could,
          with the code I provided.'
        updatedAt: '2022-07-24T14:17:55.364Z'
      numEdits: 0
      reactions: []
    id: 62dd5493169bd1d2ef2eb7bd
    type: comment
  author: liangyuch
  content: 'I am not an expert in LMs, therefore sorry I can''t reason from your code
    and output.

    My suggestion is to reproduce the output of the given example. I could, with the
    code I provided.'
  created_at: 2022-07-24 13:17:55+00:00
  edited: false
  hidden: false
  id: 62dd5493169bd1d2ef2eb7bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
      fullname: Nicholas Broad
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: nbroad
      type: user
    createdAt: '2022-07-24T14:50:52.000Z'
    data:
      edited: false
      editors:
      - nbroad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639773384591-5f353bb37e58354338621655.jpeg?w=200&h=200&f=face
          fullname: Nicholas Broad
          isHf: true
          isPro: true
          name: nbroad
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dnhkng&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dnhkng\">@<span class=\"\
          underline\">dnhkng</span></a></span>\n\n\t</span></span> check out this:\
          \ <a rel=\"nofollow\" href=\"https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315\"\
          >https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315</a></p>\n"
        raw: '@dnhkng check out this: https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315'
        updatedAt: '2022-07-24T14:50:52.383Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dnhkng
      - count: 1
        reaction: "\U0001F92F"
        users:
        - dnhkng
    id: 62dd5c4cbead5683662d3d8c
    type: comment
  author: nbroad
  content: '@dnhkng check out this: https://discuss.huggingface.co/t/mixed-precision-for-bfloat16-pretrained-models/5315'
  created_at: 2022-07-24 13:50:52+00:00
  edited: false
  hidden: false
  id: 62dd5c4cbead5683662d3d8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnhkng
      type: user
    createdAt: '2022-07-24T14:59:17.000Z'
    data:
      edited: true
      editors:
      - dnhkng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
          fullname: David
          isHf: false
          isPro: false
          name: dnhkng
          type: user
        html: '<p>Ouch, thanks for the link! </p>

          <p>Looks like I either need 2 more cards and convert to FP32, or replace
          my cards with bfloat16 capable ones, damn...</p>

          '
        raw: "Ouch, thanks for the link! \n\nLooks like I either need 2 more cards\
          \ and convert to FP32, or replace my cards with bfloat16 capable ones, damn..."
        updatedAt: '2022-07-25T08:21:10.240Z'
      numEdits: 1
      reactions: []
    id: 62dd5e452e18ceb47b7bb4bf
    type: comment
  author: dnhkng
  content: "Ouch, thanks for the link! \n\nLooks like I either need 2 more cards and\
    \ convert to FP32, or replace my cards with bfloat16 capable ones, damn..."
  created_at: 2022-07-24 13:59:17+00:00
  edited: true
  hidden: false
  id: 62dd5e452e18ceb47b7bb4bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/79c1d7411fab3506139aaaf2cd2b27b1.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnhkng
      type: user
    createdAt: '2022-07-25T08:49:01.000Z'
    data:
      status: closed
    id: 62de58fdf673a7e1807c0bfe
    type: status-change
  author: dnhkng
  created_at: 2022-07-25 07:49:01+00:00
  id: 62de58fdf673a7e1807c0bfe
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: google/ul2
repo_type: model
status: closed
target_branch: null
title: Splitting the model of multiple GPU's
