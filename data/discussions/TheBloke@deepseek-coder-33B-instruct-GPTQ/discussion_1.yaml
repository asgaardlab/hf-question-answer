!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cvinker
conflicting_files: null
created_at: 2023-11-05 14:15:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b10d5d067bc3f7d2778d3decbc5d0a71.svg
      fullname: Colin Vink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvinker
      type: user
    createdAt: '2023-11-05T14:15:25.000Z'
    data:
      edited: false
      editors:
      - cvinker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.17772147059440613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b10d5d067bc3f7d2778d3decbc5d0a71.svg
          fullname: Colin Vink
          isHf: false
          isPro: false
          name: cvinker
          type: user
        html: "<pre><code>2023-11-05 14:13:30 INFO:Loading TheBloke_deepseek-coder-33B-instruct-GPTQ_gptq-8bit-128g-actorder_True...\n\
          2023-11-05 14:13:46 ERROR:Failed to load the model.\nTraceback (most recent\
          \ call last):\n  File \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\n                              \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/modules/models.py\"\
          , line 84, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/modules/models.py\"\
          , line 348, in ExLlama_HF_loader\n    return ExllamaHF.from_pretrained(model_name)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\"\
          , line 174, in from_pretrained\n    return ExllamaHF(config)\n         \
          \  ^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\"\
          , line 31, in __init__\n    self.ex_model = ExLlama(self.ex_config)\n  \
          \                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 889, in __init__\n    layer = ExLlamaDecoderLayer(self.config, tensors,\
          \ f\"model.layers.{i}\", i, sin, cos)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 517, in __init__\n    self.self_attn = ExLlamaAttention(self.config,\
          \ tensors, key + \".self_attn\", sin, cos, self.index)\n               \
          \      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 304, in __init__\n    self.q_proj = Ex4bitLinear(config, self.config.hidden_size,\
          \ self.config.num_attention_heads * self.config.head_dim, False, tensors,\
          \ key + \".q_proj\")\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 154, in __init__\n    self.q4 = cuda_ext.ext_make_q4(self.qweight,\n\
          \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/cuda_ext.py\"\
          , line 33, in ext_make_q4\n    return make_q4(qweight,\n           ^^^^^^^^^^^^^^^^\n\
          RuntimeError: qweight and qzeros have incompatible shapes\n</code></pre>\n"
        raw: "```\r\n2023-11-05 14:13:30 INFO:Loading TheBloke_deepseek-coder-33B-instruct-GPTQ_gptq-8bit-128g-actorder_True...\r\
          \n2023-11-05 14:13:46 ERROR:Failed to load the model.\r\nTraceback (most\
          \ recent call last):\r\n  File \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n                          \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/modules/models.py\"\
          , line 84, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/modules/models.py\"\
          , line 348, in ExLlama_HF_loader\r\n    return ExllamaHF.from_pretrained(model_name)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\"\
          , line 174, in from_pretrained\r\n    return ExllamaHF(config)\r\n     \
          \      ^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\"\
          , line 31, in __init__\r\n    self.ex_model = ExLlama(self.ex_config)\r\n\
          \                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 889, in __init__\r\n    layer = ExLlamaDecoderLayer(self.config,\
          \ tensors, f\"model.layers.{i}\", i, sin, cos)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 517, in __init__\r\n    self.self_attn = ExLlamaAttention(self.config,\
          \ tensors, key + \".self_attn\", sin, cos, self.index)\r\n             \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 304, in __init__\r\n    self.q_proj = Ex4bitLinear(config, self.config.hidden_size,\
          \ self.config.num_attention_heads * self.config.head_dim, False, tensors,\
          \ key + \".q_proj\")\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
          , line 154, in __init__\r\n    self.q4 = cuda_ext.ext_make_q4(self.qweight,\r\
          \n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/cuda_ext.py\"\
          , line 33, in ext_make_q4\r\n    return make_q4(qweight,\r\n           ^^^^^^^^^^^^^^^^\r\
          \nRuntimeError: qweight and qzeros have incompatible shapes\r\n```\r\n"
        updatedAt: '2023-11-05T14:15:25.663Z'
      numEdits: 0
      reactions: []
    id: 6547a37d2119c8bdf290dc41
    type: comment
  author: cvinker
  content: "```\r\n2023-11-05 14:13:30 INFO:Loading TheBloke_deepseek-coder-33B-instruct-GPTQ_gptq-8bit-128g-actorder_True...\r\
    \n2023-11-05 14:13:46 ERROR:Failed to load the model.\r\nTraceback (most recent\
    \ call last):\r\n  File \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
    , line 209, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/modules/models.py\", line 84, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/modules/models.py\", line 348, in\
    \ ExLlama_HF_loader\r\n    return ExllamaHF.from_pretrained(model_name)\r\n  \
    \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\"\
    , line 174, in from_pretrained\r\n    return ExllamaHF(config)\r\n           ^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/modules/exllama_hf.py\", line 31,\
    \ in __init__\r\n    self.ex_model = ExLlama(self.ex_config)\r\n             \
    \       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
    , line 889, in __init__\r\n    layer = ExLlamaDecoderLayer(self.config, tensors,\
    \ f\"model.layers.{i}\", i, sin, cos)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
    , line 517, in __init__\r\n    self.self_attn = ExLlamaAttention(self.config,\
    \ tensors, key + \".self_attn\", sin, cos, self.index)\r\n                   \
    \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
    , line 304, in __init__\r\n    self.q_proj = Ex4bitLinear(config, self.config.hidden_size,\
    \ self.config.num_attention_heads * self.config.head_dim, False, tensors, key\
    \ + \".q_proj\")\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/model.py\"\
    , line 154, in __init__\r\n    self.q4 = cuda_ext.ext_make_q4(self.qweight,\r\n\
    \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/text-generation-webui/installer_files/env/lib/python3.11/site-packages/exllama/cuda_ext.py\"\
    , line 33, in ext_make_q4\r\n    return make_q4(qweight,\r\n           ^^^^^^^^^^^^^^^^\r\
    \nRuntimeError: qweight and qzeros have incompatible shapes\r\n```\r\n"
  created_at: 2023-11-05 14:15:25+00:00
  edited: false
  hidden: false
  id: 6547a37d2119c8bdf290dc41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-05T14:22:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658532738685608
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ExLlama does not support 8-bit models - please see the table in
          the README which shows which files will or won''t work with ExLlama</p>

          '
        raw: ExLlama does not support 8-bit models - please see the table in the README
          which shows which files will or won't work with ExLlama
        updatedAt: '2023-11-05T14:22:15.430Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cvinker
    id: 6547a51733972a012f7620fd
    type: comment
  author: TheBloke
  content: ExLlama does not support 8-bit models - please see the table in the README
    which shows which files will or won't work with ExLlama
  created_at: 2023-11-05 14:22:15+00:00
  edited: false
  hidden: false
  id: 6547a51733972a012f7620fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b10d5d067bc3f7d2778d3decbc5d0a71.svg
      fullname: Colin Vink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvinker
      type: user
    createdAt: '2023-11-05T14:24:57.000Z'
    data:
      edited: false
      editors:
      - cvinker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9663301706314087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b10d5d067bc3f7d2778d3decbc5d0a71.svg
          fullname: Colin Vink
          isHf: false
          isPro: false
          name: cvinker
          type: user
        html: '<blockquote>

          <p>ExLlama does not support 8-bit models - please see the table in the README
          which shows which files will or won''t work with ExLlama</p>

          </blockquote>

          <p>Thank you, my bad, I have it working with Transformers.</p>

          '
        raw: '> ExLlama does not support 8-bit models - please see the table in the
          README which shows which files will or won''t work with ExLlama


          Thank you, my bad, I have it working with Transformers.'
        updatedAt: '2023-11-05T14:24:57.028Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 6547a5b90856885240760860
    type: comment
  author: cvinker
  content: '> ExLlama does not support 8-bit models - please see the table in the
    README which shows which files will or won''t work with ExLlama


    Thank you, my bad, I have it working with Transformers.'
  created_at: 2023-11-05 14:24:57+00:00
  edited: false
  hidden: false
  id: 6547a5b90856885240760860
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/deepseek-coder-33B-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Not working in TG-webui with ExLlama_HF
