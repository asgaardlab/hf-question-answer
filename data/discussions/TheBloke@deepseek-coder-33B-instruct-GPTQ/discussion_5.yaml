!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-12-17 00:04:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-17T00:04:23.000Z'
    data:
      edited: true
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8131967782974243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: "<p>Model doesnt print anything. Just blank spaces. Using exllamav2.<br>Please\
          \ help <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "Model doesnt print anything. Just blank spaces. Using exllamav2. \n\
          Please help @TheBloke "
        updatedAt: '2023-12-17T00:04:33.513Z'
      numEdits: 1
      reactions: []
    id: 657e3b073c65a5be2a18f0f6
    type: comment
  author: rjmehta
  content: "Model doesnt print anything. Just blank spaces. Using exllamav2. \nPlease\
    \ help @TheBloke "
  created_at: 2023-12-17 00:04:23+00:00
  edited: true
  hidden: false
  id: 657e3b073c65a5be2a18f0f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-17T00:19:38.000Z'
    data:
      edited: true
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4077870845794678
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>INPUT:<br>settings = ExLlamaV2Sampler.Settings()<br>settings.temperature
          = 0.85<br>settings.top_k = 50<br>settings.top_p = 0.8<br>settings.token_repetition_penalty
          = 1<br>#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])<br>max_new_tokens
          = 10</p>

          <h1 id="prompt">Prompt</h1>

          <p>prompt = f"""Write a working python code.<br>/#/#/# Instruction:<br>Write
          a working python code to generate 100 random numbers.<br>/#/#/# Response:</p>

          <p>"""<br>input_ids = tokenizer.encode(prompt)<br>prompt_tokens = input_ids.shape[-1]</p>

          <h1 id="make-sure-cuda-is-initialized-so-we-can-measure-performance">Make
          sure CUDA is initialized so we can measure performance</h1>

          <p>generator.warmup()</p>

          <h1 id="send-prompt-to-generator-to-begin-stream">Send prompt to generator
          to begin stream</h1>

          <p>time_begin_prompt = time.time()<br>print (prompt, end = "")<br>sys.stdout.flush()<br>generator.set_stop_conditions([])<br>generator.begin_stream(input_ids,
          settings)<br>time_begin_stream = time.time()<br>generated_tokens = 0<br>while
          True:<br>    chunk, eos, _ = generator.stream()<br>    generated_tokens
          += 1<br>    print (chunk, end = "")<br>    sys.stdout.flush()<br>    if
          eos or generated_tokens == max_new_tokens: break<br>time_end = time.time()<br>time_prompt
          = time_begin_stream - time_begin_prompt<br>time_tokens = time_end - time_begin_stream<br>print()<br>print()<br>print(f"Prompt
          processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens
          / time_prompt:.2f} tokens/second")<br>print(f"Response generated in {time_tokens:.2f}
          seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f}
          tokens/second")</p>

          <hr>

          <p>OUTPUT:</p>

          <p>Write a working python code.<br>/#/#/# Instruction:<br>Write a working
          python code to generate 100 random numbers.<br>/#/#/# Response:</p>

          <p>Prompt processed in 0.00 seconds, 32 tokens, 27396.96 tokens/second<br>Response
          generated in 0.43 seconds, 10 tokens, 23.49 tokens/second"""</p>

          '
        raw: "INPUT:\nsettings = ExLlamaV2Sampler.Settings()\nsettings.temperature\
          \ = 0.85\nsettings.top_k = 50\nsettings.top_p = 0.8\nsettings.token_repetition_penalty\
          \ = 1\n#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n\
          max_new_tokens = 10\n# Prompt\nprompt = f\"\"\"Write a working python code.\n\
          /#/#/# Instruction:\nWrite a working python code to generate 100 random\
          \ numbers.\n/#/#/# Response:\n\n\"\"\"\ninput_ids = tokenizer.encode(prompt)\n\
          prompt_tokens = input_ids.shape[-1]\n# Make sure CUDA is initialized so\
          \ we can measure performance\ngenerator.warmup()\n# Send prompt to generator\
          \ to begin stream\ntime_begin_prompt = time.time()\nprint (prompt, end =\
          \ \"\")\nsys.stdout.flush()\ngenerator.set_stop_conditions([])\ngenerator.begin_stream(input_ids,\
          \ settings)\ntime_begin_stream = time.time()\ngenerated_tokens = 0\nwhile\
          \ True:\n    chunk, eos, _ = generator.stream()\n    generated_tokens +=\
          \ 1\n    print (chunk, end = \"\")\n    sys.stdout.flush()\n    if eos or\
          \ generated_tokens == max_new_tokens: break\ntime_end = time.time()\ntime_prompt\
          \ = time_begin_stream - time_begin_prompt\ntime_tokens = time_end - time_begin_stream\n\
          print()\nprint()\nprint(f\"Prompt processed in {time_prompt:.2f} seconds,\
          \ {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\"\
          )\nprint(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens}\
          \ tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          OUTPUT:\n\nWrite a working python code.\n/#/#/# Instruction:\nWrite a working\
          \ python code to generate 100 random numbers.\n/#/#/# Response:\n\n\n\n\n\
          \n\n\n\n\n\n\n\n\nPrompt processed in 0.00 seconds, 32 tokens, 27396.96\
          \ tokens/second\nResponse generated in 0.43 seconds, 10 tokens, 23.49 tokens/second\"\
          \"\""
        updatedAt: '2023-12-17T00:21:48.238Z'
      numEdits: 3
      reactions: []
    id: 657e3e9a83543a061b53066d
    type: comment
  author: rjmehta
  content: "INPUT:\nsettings = ExLlamaV2Sampler.Settings()\nsettings.temperature =\
    \ 0.85\nsettings.top_k = 50\nsettings.top_p = 0.8\nsettings.token_repetition_penalty\
    \ = 1\n#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\nmax_new_tokens\
    \ = 10\n# Prompt\nprompt = f\"\"\"Write a working python code.\n/#/#/# Instruction:\n\
    Write a working python code to generate 100 random numbers.\n/#/#/# Response:\n\
    \n\"\"\"\ninput_ids = tokenizer.encode(prompt)\nprompt_tokens = input_ids.shape[-1]\n\
    # Make sure CUDA is initialized so we can measure performance\ngenerator.warmup()\n\
    # Send prompt to generator to begin stream\ntime_begin_prompt = time.time()\n\
    print (prompt, end = \"\")\nsys.stdout.flush()\ngenerator.set_stop_conditions([])\n\
    generator.begin_stream(input_ids, settings)\ntime_begin_stream = time.time()\n\
    generated_tokens = 0\nwhile True:\n    chunk, eos, _ = generator.stream()\n  \
    \  generated_tokens += 1\n    print (chunk, end = \"\")\n    sys.stdout.flush()\n\
    \    if eos or generated_tokens == max_new_tokens: break\ntime_end = time.time()\n\
    time_prompt = time_begin_stream - time_begin_prompt\ntime_tokens = time_end -\
    \ time_begin_stream\nprint()\nprint()\nprint(f\"Prompt processed in {time_prompt:.2f}\
    \ seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\"\
    )\nprint(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens}\
    \ tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    OUTPUT:\n\nWrite a working python code.\n/#/#/# Instruction:\nWrite a working\
    \ python code to generate 100 random numbers.\n/#/#/# Response:\n\n\n\n\n\n\n\n\
    \n\n\n\n\n\nPrompt processed in 0.00 seconds, 32 tokens, 27396.96 tokens/second\n\
    Response generated in 0.43 seconds, 10 tokens, 23.49 tokens/second\"\"\""
  created_at: 2023-12-17 00:19:38+00:00
  edited: true
  hidden: false
  id: 657e3e9a83543a061b53066d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-18T18:31:52.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540266990661621
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt
          print EOS token.</p>

          '
        raw: Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
          EOS token.
        updatedAt: '2023-12-18T18:31:52.780Z'
      numEdits: 0
      reactions: []
    id: 658090181fb6cfa0b4724382
    type: comment
  author: rjmehta
  content: Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
    EOS token.
  created_at: 2023-12-18 18:31:52+00:00
  edited: false
  hidden: false
  id: 658090181fb6cfa0b4724382
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b64c63c46ae98363c7c1e4c35a73112.svg
      fullname: Frank Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FrankWu
      type: user
    createdAt: '2023-12-24T12:08:02.000Z'
    data:
      edited: false
      editors:
      - FrankWu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9314929246902466
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b64c63c46ae98363c7c1e4c35a73112.svg
          fullname: Frank Wu
          isHf: false
          isPro: false
          name: FrankWu
          type: user
        html: '<blockquote>

          <p>Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
          EOS token.</p>

          </blockquote>

          <p>hi, i meet a similar issue with VLLM. Do you mean the root cause is rope_scale?
          where can i modify this? Thank you</p>

          '
        raw: '> Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt
          print EOS token.


          hi, i meet a similar issue with VLLM. Do you mean the root cause is rope_scale?
          where can i modify this? Thank you'
        updatedAt: '2023-12-24T12:08:02.317Z'
      numEdits: 0
      reactions: []
    id: 65881f22fb9c2bdfae2cb705
    type: comment
  author: FrankWu
  content: '> Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
    EOS token.


    hi, i meet a similar issue with VLLM. Do you mean the root cause is rope_scale?
    where can i modify this? Thank you'
  created_at: 2023-12-24 12:08:02+00:00
  edited: false
  hidden: false
  id: 65881f22fb9c2bdfae2cb705
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4de7e8f07578652b173d09b66c0ac233.svg
      fullname: Yunus Koc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AiWidder
      type: user
    createdAt: '2023-12-28T05:59:10.000Z'
    data:
      edited: false
      editors:
      - AiWidder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7708814144134521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4de7e8f07578652b173d09b66c0ac233.svg
          fullname: Yunus Koc
          isHf: false
          isPro: false
          name: AiWidder
          type: user
        html: '<p>I have an issue loading this modell with Text generation web ui.
          It gives me the error "UnicodeDecodeError: ''charmap'' codec can''t decode
          byte 0x81 in position 125: character maps to ". Anyone an idea how to solve
          this?</p>

          '
        raw: 'I have an issue loading this modell with Text generation web ui. It
          gives me the error "UnicodeDecodeError: ''charmap'' codec can''t decode
          byte 0x81 in position 125: character maps to ". Anyone an idea how to solve
          this?'
        updatedAt: '2023-12-28T05:59:10.924Z'
      numEdits: 0
      reactions: []
    id: 658d0eaed27a149dc5f9b492
    type: comment
  author: AiWidder
  content: 'I have an issue loading this modell with Text generation web ui. It gives
    me the error "UnicodeDecodeError: ''charmap'' codec can''t decode byte 0x81 in
    position 125: character maps to ". Anyone an idea how to solve this?'
  created_at: 2023-12-28 05:59:10+00:00
  edited: false
  hidden: false
  id: 658d0eaed27a149dc5f9b492
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/deepseek-coder-33B-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Model doesnt work
