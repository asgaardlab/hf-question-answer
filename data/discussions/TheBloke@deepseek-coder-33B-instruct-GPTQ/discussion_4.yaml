!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adamo1139
conflicting_files: null
created_at: 2023-11-19 01:30:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-19T01:30:46.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611715078353882
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>During a discussion over at r/localllama, it became quite apparent
          that using repetition penalty settings different from 1.0 will very likely
          negatively affect performance of this model, since coding very often has
          repeating patterns.</p>

          <p>Conversation in this thread.<br><a rel="nofollow" href="https://old.reddit.com/r/LocalLLaMA/comments/17yda6k/having_a_hard_time_setting_deepseek_coder/">https://old.reddit.com/r/LocalLLaMA/comments/17yda6k/having_a_hard_time_setting_deepseek_coder/</a></p>

          <p>Would it be possible for you to add information that setting repetition
          penalty to 1.0 may improve coding performance of DeepSeek models to readme
          pages of your deepseek quants? That''s seemingly how DeepSeek is running
          their model on their demo page, so I think this is how they intend it to
          be used. It is actually probably applicable to all coding models, but we
          noticed it for DeepSeek now. I think this could help out people who get
          bad first experience with this model and they are not sure why this hyped
          up thing runs poorly for them.</p>

          '
        raw: "During a discussion over at r/localllama, it became quite apparent that\
          \ using repetition penalty settings different from 1.0 will very likely\
          \ negatively affect performance of this model, since coding very often has\
          \ repeating patterns.\r\n\r\nConversation in this thread.\r\nhttps://old.reddit.com/r/LocalLLaMA/comments/17yda6k/having_a_hard_time_setting_deepseek_coder/\r\
          \n\r\nWould it be possible for you to add information that setting repetition\
          \ penalty to 1.0 may improve coding performance of DeepSeek models to readme\
          \ pages of your deepseek quants? That's seemingly how DeepSeek is running\
          \ their model on their demo page, so I think this is how they intend it\
          \ to be used. It is actually probably applicable to all coding models, but\
          \ we noticed it for DeepSeek now. I think this could help out people who\
          \ get bad first experience with this model and they are not sure why this\
          \ hyped up thing runs poorly for them."
        updatedAt: '2023-11-19T01:30:46.377Z'
      numEdits: 0
      reactions: []
    id: 65596546ed8df831281f5d84
    type: comment
  author: adamo1139
  content: "During a discussion over at r/localllama, it became quite apparent that\
    \ using repetition penalty settings different from 1.0 will very likely negatively\
    \ affect performance of this model, since coding very often has repeating patterns.\r\
    \n\r\nConversation in this thread.\r\nhttps://old.reddit.com/r/LocalLLaMA/comments/17yda6k/having_a_hard_time_setting_deepseek_coder/\r\
    \n\r\nWould it be possible for you to add information that setting repetition\
    \ penalty to 1.0 may improve coding performance of DeepSeek models to readme pages\
    \ of your deepseek quants? That's seemingly how DeepSeek is running their model\
    \ on their demo page, so I think this is how they intend it to be used. It is\
    \ actually probably applicable to all coding models, but we noticed it for DeepSeek\
    \ now. I think this could help out people who get bad first experience with this\
    \ model and they are not sure why this hyped up thing runs poorly for them."
  created_at: 2023-11-19 01:30:46+00:00
  edited: false
  hidden: false
  id: 65596546ed8df831281f5d84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/deepseek-coder-33B-instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Add note about repetition penalty to readme
