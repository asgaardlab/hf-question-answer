!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KennethEnevoldsen
conflicting_files: []
created_at: 2023-07-26 22:46:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png?w=200&h=200&f=face
      fullname: Kenneth C. Enevoldsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KennethEnevoldsen
      type: user
    createdAt: '2023-07-26T23:46:36.000Z'
    data:
      edited: false
      editors:
      - KennethEnevoldsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2630409300327301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png?w=200&h=200&f=face
          fullname: Kenneth C. Enevoldsen
          isHf: false
          isPro: false
          name: KennethEnevoldsen
          type: user
        html: "<p>When I run:</p>\n<pre><code>from transformers import AutoModelForMaskedLM,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\"\
          )\nmodel = AutoModelForMaskedLM.from_pretrained(\"vesteinn/DanskBERT\")\n\
          \ntext = \"very long text \"*1000\n\ninput_ids = tokenizer(text, return_tensors=\"\
          pt\")\ninput_ids[\"input_ids\"].shape\n# truncate to 512 tokens\ninput_ids\
          \ = {k: v[:, :514] for k, v in input_ids.items()}\n\ninput_ids[\"input_ids\"\
          ].shape\n\noutputs = model.forward(**input_ids)\n</code></pre>\n<p>I get:</p>\n\
          <pre><code>...\n   2208     # remove once script supports set_grad_enabled\n\
          \   2209     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n\
          -&gt; 2210 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\n\nIndexError: index out of range in self\n</code></pre>\n"
        raw: "When I run:\n```\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\nmodel\
          \ = AutoModelForMaskedLM.from_pretrained(\"vesteinn/DanskBERT\")\n\ntext\
          \ = \"very long text \"*1000\n\ninput_ids = tokenizer(text, return_tensors=\"\
          pt\")\ninput_ids[\"input_ids\"].shape\n# truncate to 512 tokens\ninput_ids\
          \ = {k: v[:, :514] for k, v in input_ids.items()}\n\ninput_ids[\"input_ids\"\
          ].shape\n\noutputs = model.forward(**input_ids)\n```\n\n\nI get:\n\n```\n\
          ...\n   2208     # remove once script supports set_grad_enabled\n   2209\
          \     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n->\
          \ 2210 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\n\nIndexError: index out of range in self\n```"
        updatedAt: '2023-07-26T23:46:36.182Z'
      numEdits: 0
      reactions: []
    id: 64c1b05c8d3481788e124659
    type: comment
  author: KennethEnevoldsen
  content: "When I run:\n```\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\
    \ntokenizer = AutoTokenizer.from_pretrained(\"vesteinn/DanskBERT\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"\
    vesteinn/DanskBERT\")\n\ntext = \"very long text \"*1000\n\ninput_ids = tokenizer(text,\
    \ return_tensors=\"pt\")\ninput_ids[\"input_ids\"].shape\n# truncate to 512 tokens\n\
    input_ids = {k: v[:, :514] for k, v in input_ids.items()}\n\ninput_ids[\"input_ids\"\
    ].shape\n\noutputs = model.forward(**input_ids)\n```\n\n\nI get:\n\n```\n...\n\
    \   2208     # remove once script supports set_grad_enabled\n   2209     _no_grad_embedding_renorm_(weight,\
    \ input, max_norm, norm_type)\n-> 2210 return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\n\nIndexError: index out of range in self\n```"
  created_at: 2023-07-26 22:46:36+00:00
  edited: false
  hidden: false
  id: 64c1b05c8d3481788e124659
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png?w=200&h=200&f=face
      fullname: Kenneth C. Enevoldsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KennethEnevoldsen
      type: user
    createdAt: '2023-07-26T23:46:37.000Z'
    data:
      oid: aae1f2db9adb34c3f742bb7b65f11d05ceffdd26
      parents:
      - 7e282ae5001313762aa0a1f2f6e8f027960ab0db
      subject: Max positional embedding causes error when exceeding 512.
    id: 64c1b05d0000000000000000
    type: commit
  author: KennethEnevoldsen
  created_at: 2023-07-26 22:46:37+00:00
  id: 64c1b05d0000000000000000
  oid: aae1f2db9adb34c3f742bb7b65f11d05ceffdd26
  summary: Max positional embedding causes error when exceeding 512.
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f915e09161dc51925934edf/2PqjbJ7FVG3FOCNmn7ORY.png?w=200&h=200&f=face
      fullname: "V\xE9steinn Sn\xE6bjarnarson"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: vesteinn
      type: user
    createdAt: '2023-07-27T06:00:14.000Z'
    data:
      edited: false
      editors:
      - vesteinn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446293711662292
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f915e09161dc51925934edf/2PqjbJ7FVG3FOCNmn7ORY.png?w=200&h=200&f=face
          fullname: "V\xE9steinn Sn\xE6bjarnarson"
          isHf: false
          isPro: false
          name: vesteinn
          type: user
        html: '<p>Hi Kenneth!</p>

          <p>This runs fine if you change 514 to 512 in your example, but I''m guessing
          you know that.</p>

          <p>I was also confused by the max_position_embeddings being set to 514,
          but this might shed some light on it <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/1363">https://github.com/huggingface/transformers/issues/1363</a>
          and <a rel="nofollow" href="https://github.com/facebookresearch/fairseq/issues/1187">https://github.com/facebookresearch/fairseq/issues/1187</a>
          .  The model was trained with fairseq and then ported to hf.</p>

          '
        raw: 'Hi Kenneth!


          This runs fine if you change 514 to 512 in your example, but I''m guessing
          you know that.


          I was also confused by the max_position_embeddings being set to 514, but
          this might shed some light on it https://github.com/huggingface/transformers/issues/1363
          and https://github.com/facebookresearch/fairseq/issues/1187 .  The model
          was trained with fairseq and then ported to hf.'
        updatedAt: '2023-07-27T06:00:14.916Z'
      numEdits: 0
      reactions: []
    id: 64c207eeec3c61813526c5c9
    type: comment
  author: vesteinn
  content: 'Hi Kenneth!


    This runs fine if you change 514 to 512 in your example, but I''m guessing you
    know that.


    I was also confused by the max_position_embeddings being set to 514, but this
    might shed some light on it https://github.com/huggingface/transformers/issues/1363
    and https://github.com/facebookresearch/fairseq/issues/1187 .  The model was trained
    with fairseq and then ported to hf.'
  created_at: 2023-07-27 05:00:14+00:00
  edited: false
  hidden: false
  id: 64c207eeec3c61813526c5c9
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 3
repo_id: vesteinn/DanskBERT
repo_type: model
status: open
target_branch: refs/heads/main
title: Max positional embedding causes error when exceeding 512.
