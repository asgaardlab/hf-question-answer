!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-12-19 00:58:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-12-19T00:58:48.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4635286033153534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>what gives?<br>error loading model: unknown model architecture:
          ''phi2''<br>llama_load_model_from_file: failed to load model<br>AVX = 1
          | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 |
          NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS =
          1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |<br>2023-12-18 16:57:06 ERROR:Failed
          to load the model.<br>Traceback (most recent call last):<br>  File "/data/text-generation-webui/modules/ui_model_menu.py",
          line 209, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(selected_model, loader)<br>  File "/data/text-generation-webui/modules/models.py",
          line 89, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/data/text-generation-webui/modules/models.py", line 259, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File "/data/text-generation-webui/modules/llamacpp_model.py",
          line 91, in from_pretrained<br>    result.model = Llama(**params)<br>  File
          "/data/llama-cpp-python/llama_cpp/llama.py", line 963, in <strong>init</strong><br>    self._n_vocab
          = self.n_vocab()<br>  File "/data/llama-cpp-python/llama_cpp/llama.py",
          line 2270, in n_vocab<br>    return self._model.n_vocab()<br>  File "/data/llama-cpp-python/llama_cpp/llama.py",
          line 252, in n_vocab<br>    assert self.model is not None<br>AssertionError</p>

          '
        raw: "what gives?\r\nerror loading model: unknown model architecture: 'phi2'\r\
          \nllama_load_model_from_file: failed to load model\r\nAVX = 1 | AVX2 = 1\
          \ | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | SSSE3 = 1 | VSX = 0 |\r\n2023-12-18 16:57:06 ERROR:Failed to load\
          \ the model.\r\nTraceback (most recent call last):\r\n  File \"/data/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
          \ = load_model(selected_model, loader)\r\n  File \"/data/text-generation-webui/modules/models.py\"\
          , line 89, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"/data/text-generation-webui/modules/models.py\", line 259, in\
          \ llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n  File \"/data/text-generation-webui/modules/llamacpp_model.py\", line\
          \ 91, in from_pretrained\r\n    result.model = Llama(**params)\r\n  File\
          \ \"/data/llama-cpp-python/llama_cpp/llama.py\", line 963, in __init__\r\
          \n    self._n_vocab = self.n_vocab()\r\n  File \"/data/llama-cpp-python/llama_cpp/llama.py\"\
          , line 2270, in n_vocab\r\n    return self._model.n_vocab()\r\n  File \"\
          /data/llama-cpp-python/llama_cpp/llama.py\", line 252, in n_vocab\r\n  \
          \  assert self.model is not None\r\nAssertionError\r\n\r\n"
        updatedAt: '2023-12-19T00:58:48.120Z'
      numEdits: 0
      reactions: []
    id: 6580eac846aaa3328e029557
    type: comment
  author: LaferriereJC
  content: "what gives?\r\nerror loading model: unknown model architecture: 'phi2'\r\
    \nllama_load_model_from_file: failed to load model\r\nAVX = 1 | AVX2 = 1 | AVX512\
    \ = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
    \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX\
    \ = 0 |\r\n2023-12-18 16:57:06 ERROR:Failed to load the model.\r\nTraceback (most\
    \ recent call last):\r\n  File \"/data/text-generation-webui/modules/ui_model_menu.py\"\
    , line 209, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\r\n  File \"/data/text-generation-webui/modules/models.py\", line 89,\
    \ in load_model\r\n    output = load_func_map[loader](model_name)\r\n  File \"\
    /data/text-generation-webui/modules/models.py\", line 259, in llamacpp_loader\r\
    \n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n  File \"\
    /data/text-generation-webui/modules/llamacpp_model.py\", line 91, in from_pretrained\r\
    \n    result.model = Llama(**params)\r\n  File \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 963, in __init__\r\n    self._n_vocab = self.n_vocab()\r\n  File \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 2270, in n_vocab\r\n    return self._model.n_vocab()\r\n  File \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 252, in n_vocab\r\n    assert self.model is not None\r\nAssertionError\r\
    \n\r\n"
  created_at: 2023-12-19 00:58:48+00:00
  edited: false
  hidden: false
  id: 6580eac846aaa3328e029557
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-12-19T01:11:30.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.969607949256897
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>I saw a comment about a specific version of llama.cpp</p>

          <p>I guess I will hold off for now until it''s merged into the main branch
          (I attempted building from the latest).</p>

          '
        raw: 'I saw a comment about a specific version of llama.cpp


          I guess I will hold off for now until it''s merged into the main branch
          (I attempted building from the latest).'
        updatedAt: '2023-12-19T01:11:30.364Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6580edc2d0dfc3caf6c8b34a
    id: 6580edc2d0dfc3caf6c8b348
    type: comment
  author: LaferriereJC
  content: 'I saw a comment about a specific version of llama.cpp


    I guess I will hold off for now until it''s merged into the main branch (I attempted
    building from the latest).'
  created_at: 2023-12-19 01:11:30+00:00
  edited: false
  hidden: false
  id: 6580edc2d0dfc3caf6c8b348
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-12-19T01:11:30.000Z'
    data:
      status: closed
    id: 6580edc2d0dfc3caf6c8b34a
    type: status-change
  author: LaferriereJC
  created_at: 2023-12-19 01:11:30+00:00
  id: 6580edc2d0dfc3caf6c8b34a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/phi-2-GGUF
repo_type: model
status: closed
target_branch: null
title: unable to load gguf in text-generation-webui using llama.cpp however, I can
  load other gguf models
