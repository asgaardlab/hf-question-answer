!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-12-19 01:12:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-12-19T01:12:27.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45481815934181213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>error loading model: unknown model architecture: ''phi2''<br>llama_load_model_from_file:
          failed to load model<br>AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI =
          0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA
          = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |<br>2023-12-18
          16:57:06 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>File "/data/text-generation-webui/modules/ui_model_menu.py", line
          209, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(selected_model,
          loader)<br>File "/data/text-generation-webui/modules/models.py", line 89,
          in load_model<br>output = load_func_maploader<br>File "/data/text-generation-webui/modules/models.py",
          line 259, in llamacpp_loader<br>model, tokenizer = LlamaCppModel.from_pretrained(model_file)<br>File
          "/data/text-generation-webui/modules/llamacpp_model.py", line 91, in from_pretrained<br>result.model
          = Llama(**params)<br>File "/data/llama-cpp-python/llama_cpp/llama.py", line
          963, in init<br>self._n_vocab = self.n_vocab()<br>File "/data/llama-cpp-python/llama_cpp/llama.py",
          line 2270, in n_vocab<br>return self._model.n_vocab()<br>File "/data/llama-cpp-python/llama_cpp/llama.py",
          line 252, in n_vocab<br>assert self.model is not None<br>AssertionError</p>

          '
        raw: "error loading model: unknown model architecture: 'phi2'\r\nllama_load_model_from_file:\
          \ failed to load model\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX =\
          \ 0 |\r\n2023-12-18 16:57:06 ERROR:Failed to load the model.\r\nTraceback\
          \ (most recent call last):\r\nFile \"/data/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(selected_model,\
          \ loader)\r\nFile \"/data/text-generation-webui/modules/models.py\", line\
          \ 89, in load_model\r\noutput = load_func_maploader\r\nFile \"/data/text-generation-webui/modules/models.py\"\
          , line 259, in llamacpp_loader\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \nFile \"/data/text-generation-webui/modules/llamacpp_model.py\", line 91,\
          \ in from_pretrained\r\nresult.model = Llama(**params)\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
          , line 963, in init\r\nself._n_vocab = self.n_vocab()\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
          , line 2270, in n_vocab\r\nreturn self._model.n_vocab()\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
          , line 252, in n_vocab\r\nassert self.model is not None\r\nAssertionError"
        updatedAt: '2023-12-19T01:12:27.356Z'
      numEdits: 0
      reactions: []
    id: 6580edfb7508d04097c3295a
    type: comment
  author: LaferriereJC
  content: "error loading model: unknown model architecture: 'phi2'\r\nllama_load_model_from_file:\
    \ failed to load model\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 |\
    \ AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0\
    \ | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\n2023-12-18\
    \ 16:57:06 ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
    \nFile \"/data/text-generation-webui/modules/ui_model_menu.py\", line 209, in\
    \ load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\r\nFile \"/data/text-generation-webui/modules/models.py\", line 89,\
    \ in load_model\r\noutput = load_func_maploader\r\nFile \"/data/text-generation-webui/modules/models.py\"\
    , line 259, in llamacpp_loader\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \nFile \"/data/text-generation-webui/modules/llamacpp_model.py\", line 91, in\
    \ from_pretrained\r\nresult.model = Llama(**params)\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 963, in init\r\nself._n_vocab = self.n_vocab()\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 2270, in n_vocab\r\nreturn self._model.n_vocab()\r\nFile \"/data/llama-cpp-python/llama_cpp/llama.py\"\
    , line 252, in n_vocab\r\nassert self.model is not None\r\nAssertionError"
  created_at: 2023-12-19 01:12:27+00:00
  edited: false
  hidden: false
  id: 6580edfb7508d04097c3295a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac98843b9c316773e82862b4eea16155.svg
      fullname: Oleg Ciubotaru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ociubotaru
      type: user
    createdAt: '2023-12-19T05:26:05.000Z'
    data:
      edited: true
      editors:
      - ociubotaru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4154813885688782
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac98843b9c316773e82862b4eea16155.svg
          fullname: Oleg Ciubotaru
          isHf: false
          isPro: false
          name: ociubotaru
          type: user
        html: "<p>I had the same issue using LM studio 0.29<br>{<br>  \"cause\": {<br>\
          \    \"cause\": \"unknown model architecture: 'phi2'\",<br>    \"title\"\
          : \"Failed to load model\",<br>    \"errorData\": {<br>      \"n_ctx\":\
          \ 2048,<br>      \"n_batch\": 512,<br>      \"n_gpu_layers\": 1<br>    }<br>\
          \  },</p>\n<p>Downloaded and installed the Beta V8 version (<a rel=\"nofollow\"\
          \ href=\"https://lmstudio.ai/beta-releases.html\">https://lmstudio.ai/beta-releases.html</a>)\
          \  and the problem was solved.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63d460da640bb0f7717d336e/7zh8jy1d_vlvKrCbIWpwx.png\"\
          ><img alt=\"Screenshot 2023-12-18 at 9.42.57\u202FPM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63d460da640bb0f7717d336e/7zh8jy1d_vlvKrCbIWpwx.png\"\
          ></a></p>\n"
        raw: "I had the same issue using LM studio 0.29\n{\n  \"cause\": {\n    \"\
          cause\": \"unknown model architecture: 'phi2'\",\n    \"title\": \"Failed\
          \ to load model\",\n    \"errorData\": {\n      \"n_ctx\": 2048,\n     \
          \ \"n_batch\": 512,\n      \"n_gpu_layers\": 1\n    }\n  },\n\nDownloaded\
          \ and installed the Beta V8 version (https://lmstudio.ai/beta-releases.html)\
          \  and the problem was solved.\n\n![Screenshot 2023-12-18 at 9.42.57\u202F\
          PM.png](https://cdn-uploads.huggingface.co/production/uploads/63d460da640bb0f7717d336e/7zh8jy1d_vlvKrCbIWpwx.png)\n\
          \n"
        updatedAt: '2023-12-19T05:51:26.968Z'
      numEdits: 1
      reactions: []
    id: 6581296d402b16689c0c1039
    type: comment
  author: ociubotaru
  content: "I had the same issue using LM studio 0.29\n{\n  \"cause\": {\n    \"cause\"\
    : \"unknown model architecture: 'phi2'\",\n    \"title\": \"Failed to load model\"\
    ,\n    \"errorData\": {\n      \"n_ctx\": 2048,\n      \"n_batch\": 512,\n   \
    \   \"n_gpu_layers\": 1\n    }\n  },\n\nDownloaded and installed the Beta V8 version\
    \ (https://lmstudio.ai/beta-releases.html)  and the problem was solved.\n\n![Screenshot\
    \ 2023-12-18 at 9.42.57\u202FPM.png](https://cdn-uploads.huggingface.co/production/uploads/63d460da640bb0f7717d336e/7zh8jy1d_vlvKrCbIWpwx.png)\n\
    \n"
  created_at: 2023-12-19 05:26:05+00:00
  edited: true
  hidden: false
  id: 6581296d402b16689c0c1039
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
      fullname: Bumba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pumba2
      type: user
    createdAt: '2023-12-19T09:34:47.000Z'
    data:
      edited: true
      editors:
      - Pumba2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786826968193054
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
          fullname: Bumba
          isHf: false
          isPro: false
          name: Pumba2
          type: user
        html: '<p>I knew there would be a problem. Just had to :D Every time there
          is a good model it either doesnt work properly or just doesnt work. At least
          we got the quantized ver so in the future the llm model apps will fix it.</p>

          '
        raw: I knew there would be a problem. Just had to :D Every time there is a
          good model it either doesnt work properly or just doesnt work. At least
          we got the quantized ver so in the future the llm model apps will fix it.
        updatedAt: '2023-12-20T13:23:32.744Z'
      numEdits: 1
      reactions: []
    id: 658163b718c6e72a95992ca8
    type: comment
  author: Pumba2
  content: I knew there would be a problem. Just had to :D Every time there is a good
    model it either doesnt work properly or just doesnt work. At least we got the
    quantized ver so in the future the llm model apps will fix it.
  created_at: 2023-12-19 09:34:47+00:00
  edited: true
  hidden: false
  id: 658163b718c6e72a95992ca8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T14:33:03.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9661019444465637
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>I think the newest version of llama-cpp works for phi-2 now. The
          community was hard at work with making adjustments for it''s slight differences
          during the month. (<a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/4437">https://github.com/ggerganov/llama.cpp/issues/4437</a>)
          Just likely need to pip update or remake your build for the latest.</p>

          '
        raw: I think the newest version of llama-cpp works for phi-2 now. The community
          was hard at work with making adjustments for it's slight differences during
          the month. (https://github.com/ggerganov/llama.cpp/issues/4437) Just likely
          need to pip update or remake your build for the latest.
        updatedAt: '2024-01-04T14:33:03.517Z'
      numEdits: 0
      reactions: []
    id: 6596c19f85bf3514c2d32b70
    type: comment
  author: dyoung
  content: I think the newest version of llama-cpp works for phi-2 now. The community
    was hard at work with making adjustments for it's slight differences during the
    month. (https://github.com/ggerganov/llama.cpp/issues/4437) Just likely need to
    pip update or remake your build for the latest.
  created_at: 2024-01-04 14:33:03+00:00
  edited: false
  hidden: false
  id: 6596c19f85bf3514c2d32b70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T14:33:44.000Z'
    data:
      edited: true
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9851464629173279
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>Can''t speak for LM studio. I''m not a user of it. Sorry. Though
          it''s good to see that something can load phi-2 gguf quants such as LM Studio.</p>

          '
        raw: Can't speak for LM studio. I'm not a user of it. Sorry. Though it's good
          to see that something can load phi-2 gguf quants such as LM Studio.
        updatedAt: '2024-01-04T14:36:06.572Z'
      numEdits: 2
      reactions: []
    id: 6596c1c881a39e2c5042fcdd
    type: comment
  author: dyoung
  content: Can't speak for LM studio. I'm not a user of it. Sorry. Though it's good
    to see that something can load phi-2 gguf quants such as LM Studio.
  created_at: 2024-01-04 14:33:44+00:00
  edited: true
  hidden: false
  id: 6596c1c881a39e2c5042fcdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f999a027bb02fc72351aa4eec9180aa.svg
      fullname: Max Weissenbacher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mox
      type: user
    createdAt: '2024-01-07T16:27:18.000Z'
    data:
      edited: true
      editors:
      - mox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467118978500366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f999a027bb02fc72351aa4eec9180aa.svg
          fullname: Max Weissenbacher
          isHf: false
          isPro: false
          name: mox
          type: user
        html: '<p>Can confirm that it worked with upgrading on MAC: CMAKE_ARGS="-DLLAMA_METAL=on"
          pip install -U llama-cpp-python</p>

          '
        raw: 'Can confirm that it worked with upgrading on MAC: CMAKE_ARGS="-DLLAMA_METAL=on"
          pip install -U llama-cpp-python'
        updatedAt: '2024-01-07T16:32:57.929Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - led5150
    id: 659ad0e6b01e6e0c71951234
    type: comment
  author: mox
  content: 'Can confirm that it worked with upgrading on MAC: CMAKE_ARGS="-DLLAMA_METAL=on"
    pip install -U llama-cpp-python'
  created_at: 2024-01-07 16:27:18+00:00
  edited: true
  hidden: false
  id: 659ad0e6b01e6e0c71951234
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-16T16:24:58.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9695705771446228
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mox&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mox\">@<span class=\"\
          underline\">mox</span></a></span>\n\n\t</span></span> Same with my side.\
          \ I've successfully used phi with the llama-cpp family shorty after my 1st\
          \ reply 12 days ago. As well as several times since. Feels like a old hat\
          \ now. Been pretty happy with it.<br><span data-props=\"{&quot;user&quot;:&quot;LaferriereJC&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LaferriereJC\"\
          >@<span class=\"underline\">LaferriereJC</span></a></span>\n\n\t</span></span>\
          \ I've not checked, but has LM Studio got it working yet for phi? Likely\
          \ has. It not, that's a bummer. And really sorry. The wait can be painful\
          \ sometimes.</p>\n"
        raw: '@mox Same with my side. I''ve successfully used phi with the llama-cpp
          family shorty after my 1st reply 12 days ago. As well as several times since.
          Feels like a old hat now. Been pretty happy with it.

          @LaferriereJC I''ve not checked, but has LM Studio got it working yet for
          phi? Likely has. It not, that''s a bummer. And really sorry. The wait can
          be painful sometimes.'
        updatedAt: '2024-01-16T16:24:58.912Z'
      numEdits: 0
      reactions: []
    id: 65a6addae1787a2419bcc425
    type: comment
  author: dyoung
  content: '@mox Same with my side. I''ve successfully used phi with the llama-cpp
    family shorty after my 1st reply 12 days ago. As well as several times since.
    Feels like a old hat now. Been pretty happy with it.

    @LaferriereJC I''ve not checked, but has LM Studio got it working yet for phi?
    Likely has. It not, that''s a bummer. And really sorry. The wait can be painful
    sometimes.'
  created_at: 2024-01-16 16:24:58+00:00
  edited: false
  hidden: false
  id: 65a6addae1787a2419bcc425
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/phi-2-GGUF
repo_type: model
status: open
target_branch: null
title: today's version of llama.cpp results in an error
