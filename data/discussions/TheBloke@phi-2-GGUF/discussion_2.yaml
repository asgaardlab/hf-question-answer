!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-12-18 21:10:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-18T21:10:39.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9682347178459167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Do not even try to download lower that 8 bit ;P<br>Is very small
          so perplexity will be huge with bigger quantization.  </p>

          '
        raw: "Do not even try to download lower that 8 bit ;P \nIs very small so perplexity\
          \ will be huge with bigger quantization.  "
        updatedAt: '2023-12-19T00:01:52.745Z'
      numEdits: 1
      reactions: []
    id: 6580b54f8ecea67c7d58be24
    type: comment
  author: mirek190
  content: "Do not even try to download lower that 8 bit ;P \nIs very small so perplexity\
    \ will be huge with bigger quantization.  "
  created_at: 2023-12-18 21:10:39+00:00
  edited: true
  hidden: false
  id: 6580b54f8ecea67c7d58be24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-12-18T22:37:33.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9563414454460144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<p>Anyway that model is very retarded.<br>Reasoning level is very low\
          \ or implementation under llamacpp is wrong ... no idea.</p>\n<pre><code>&gt;\
          \ Instruct: There are three killers in a room. Someone enters the room and\
          \ kills one of them. Nobody leaves the room. How many killers are left in\
          \ the room? Explain your reasoning step by step.\n\\nOutput:\nAssistant:\
          \ I think there is only one killer left in the room, because when someone\
          \ enters the room and kills one of the killers, it means that all three\
          \ were killed except for one who managed to escape or fight back. Therefore,\
          \ there can be no other possible scenario where two or more killers are\
          \ still alive. This reasoning follows from a principle of logic called modus\
          \ tollens, which says that if a conditional statement is true and its consequent\
          \ is false, then its antecedent must also be false. In this case, the conditional\
          \ statement is: If there were three killers in the room, then two or more\
          \ would still be alive. The consequent is false, because we know that one\
          \ of them was killed. Therefore, the antecedent (that there were three killers)\
          \ must also be false. This can be written as:\n\\begin{align*}\n  \\text{If\
          \ } P \\rightarrow Q \\quad &amp; \\text{(If there were three killers in\
          \ the room, then two or more would still be alive.)}\\\\\n  \\text{Q \\\
          neg} \\rightarrow \\neg P \\quad &amp; \\text{(Not having two or more alive\
          \ implies that there was only one killer left.)}\\\\\n\\end{align*}\nSo\
          \ we can conclude:\n\\begin{align*}\n  (P \\rightarrow Q) \\land (Q \\neg)\
          \ \\Rightarrow \\neg P\n\\end{align*}\nwhich means:\n\\begin{align*}\n \
          \ (If there were three killers in the room, then two or more would still\
          \ be alive.) &amp; \\text{And } (Not having two or more alive implies that\
          \ there was only one killer left.)\\\\\n  \\neg P \\\\\n\\end{align*}\n\
          which is the same as:\n\\begin{align*}\n  \\neg (There were three killers\
          \ in the room) \\\\\n\\end{align*}\nSo we can say for sure:\n<span class=\"\
          katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math\
          \ display=\"block\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mo\
          \ stretchy=\"false\">(</mo><mi>P</mi><mo>\u2192</mo><mi>Q</mi><mo stretchy=\"\
          false\">)</mo><mo>\u2227</mo><mo stretchy=\"false\">(</mo><mi>Q</mi><mi\
          \ mathvariant=\"normal\">\xAC</mi><mo stretchy=\"false\">)</mo><mo>\u21D2\
          </mo><mi mathvariant=\"normal\">\xAC</mi><mi>P</mi></mrow>(P \\rightarrow\
          \ Q) \\land (Q \\neg) \\Rightarrow \\neg P </math></span><span aria-hidden=\"\
          true\" class=\"katex-html\"><span class=\"base\"><span style=\"height:1em;vertical-align:-0.25em;\"\
          \ class=\"strut\"></span><span class=\"mopen\">(</span><span style=\"margin-right:0.13889em;\"\
          \ class=\"mord mathnormal\">P</span><span style=\"margin-right:0.2777777777777778em;\"\
          \ class=\"mspace\"></span><span class=\"mrel\">\u2192</span><span style=\"\
          margin-right:0.2777777777777778em;\" class=\"mspace\"></span></span><span\
          \ class=\"base\"><span style=\"height:1em;vertical-align:-0.25em;\" class=\"\
          strut\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mclose\"\
          >)</span><span style=\"margin-right:0.2222222222222222em;\" class=\"mspace\"\
          ></span><span class=\"mbin\">\u2227</span><span style=\"margin-right:0.2222222222222222em;\"\
          \ class=\"mspace\"></span></span><span class=\"base\"><span style=\"height:1em;vertical-align:-0.25em;\"\
          \ class=\"strut\"></span><span class=\"mopen\">(</span><span class=\"mord\
          \ mathnormal\">Q</span><span class=\"mord\">\xAC</span><span class=\"mclose\"\
          >)</span><span style=\"margin-right:0.2777777777777778em;\" class=\"mspace\"\
          ></span><span class=\"mrel\">\u21D2</span><span style=\"margin-right:0.2777777777777778em;\"\
          \ class=\"mspace\"></span></span><span class=\"base\"><span style=\"height:0.68333em;vertical-align:0em;\"\
          \ class=\"strut\"></span><span class=\"mord\">\xAC</span><span style=\"\
          margin-right:0.13889em;\" class=\"mord mathnormal\">P</span></span></span></span></span>\n\
          </code></pre>\n"
        raw: "Anyway that model is very retarded. \nReasoning level is very low or\
          \ implementation under llamacpp is wrong ... no idea.\n\n\n````\n> Instruct:\
          \ There are three killers in a room. Someone enters the room and kills one\
          \ of them. Nobody leaves the room. How many killers are left in the room?\
          \ Explain your reasoning step by step.\n\\nOutput:\nAssistant: I think there\
          \ is only one killer left in the room, because when someone enters the room\
          \ and kills one of the killers, it means that all three were killed except\
          \ for one who managed to escape or fight back. Therefore, there can be no\
          \ other possible scenario where two or more killers are still alive. This\
          \ reasoning follows from a principle of logic called modus tollens, which\
          \ says that if a conditional statement is true and its consequent is false,\
          \ then its antecedent must also be false. In this case, the conditional\
          \ statement is: If there were three killers in the room, then two or more\
          \ would still be alive. The consequent is false, because we know that one\
          \ of them was killed. Therefore, the antecedent (that there were three killers)\
          \ must also be false. This can be written as:\n\\begin{align*}\n  \\text{If\
          \ } P \\rightarrow Q \\quad & \\text{(If there were three killers in the\
          \ room, then two or more would still be alive.)}\\\\\n  \\text{Q \\neg}\
          \ \\rightarrow \\neg P \\quad & \\text{(Not having two or more alive implies\
          \ that there was only one killer left.)}\\\\\n\\end{align*}\nSo we can conclude:\n\
          \\begin{align*}\n  (P \\rightarrow Q) \\land (Q \\neg) \\Rightarrow \\neg\
          \ P\n\\end{align*}\nwhich means:\n\\begin{align*}\n  (If there were three\
          \ killers in the room, then two or more would still be alive.) & \\text{And\
          \ } (Not having two or more alive implies that there was only one killer\
          \ left.)\\\\\n  \\neg P \\\\\n\\end{align*}\nwhich is the same as:\n\\begin{align*}\n\
          \  \\neg (There were three killers in the room) \\\\\n\\end{align*}\nSo\
          \ we can say for sure:\n$$(P \\rightarrow Q) \\land (Q \\neg) \\Rightarrow\
          \ \\neg P $$\n````"
        updatedAt: '2023-12-18T22:37:33.224Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - atstim731
    id: 6580c9ad62b6ed53c4d2cba2
    type: comment
  author: mirek190
  content: "Anyway that model is very retarded. \nReasoning level is very low or implementation\
    \ under llamacpp is wrong ... no idea.\n\n\n````\n> Instruct: There are three\
    \ killers in a room. Someone enters the room and kills one of them. Nobody leaves\
    \ the room. How many killers are left in the room? Explain your reasoning step\
    \ by step.\n\\nOutput:\nAssistant: I think there is only one killer left in the\
    \ room, because when someone enters the room and kills one of the killers, it\
    \ means that all three were killed except for one who managed to escape or fight\
    \ back. Therefore, there can be no other possible scenario where two or more killers\
    \ are still alive. This reasoning follows from a principle of logic called modus\
    \ tollens, which says that if a conditional statement is true and its consequent\
    \ is false, then its antecedent must also be false. In this case, the conditional\
    \ statement is: If there were three killers in the room, then two or more would\
    \ still be alive. The consequent is false, because we know that one of them was\
    \ killed. Therefore, the antecedent (that there were three killers) must also\
    \ be false. This can be written as:\n\\begin{align*}\n  \\text{If } P \\rightarrow\
    \ Q \\quad & \\text{(If there were three killers in the room, then two or more\
    \ would still be alive.)}\\\\\n  \\text{Q \\neg} \\rightarrow \\neg P \\quad &\
    \ \\text{(Not having two or more alive implies that there was only one killer\
    \ left.)}\\\\\n\\end{align*}\nSo we can conclude:\n\\begin{align*}\n  (P \\rightarrow\
    \ Q) \\land (Q \\neg) \\Rightarrow \\neg P\n\\end{align*}\nwhich means:\n\\begin{align*}\n\
    \  (If there were three killers in the room, then two or more would still be alive.)\
    \ & \\text{And } (Not having two or more alive implies that there was only one\
    \ killer left.)\\\\\n  \\neg P \\\\\n\\end{align*}\nwhich is the same as:\n\\\
    begin{align*}\n  \\neg (There were three killers in the room) \\\\\n\\end{align*}\n\
    So we can say for sure:\n$$(P \\rightarrow Q) \\land (Q \\neg) \\Rightarrow \\\
    neg P $$\n````"
  created_at: 2023-12-18 22:37:33+00:00
  edited: false
  hidden: false
  id: 6580c9ad62b6ed53c4d2cba2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
      fullname: Rob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kroonen
      type: user
    createdAt: '2023-12-19T00:00:06.000Z'
    data:
      edited: false
      editors:
      - kroonen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6690540909767151
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
          fullname: Rob
          isHf: false
          isPro: false
          name: kroonen
          type: user
        html: '<p>Indeed this model is small enough. Sharing my FP16 for those interested
          in more precision : <a href="https://huggingface.co/kroonen/phi-2-GGUF/blob/main/ggml-model-f16.gguf">https://huggingface.co/kroonen/phi-2-GGUF/blob/main/ggml-model-f16.gguf</a></p>

          '
        raw: 'Indeed this model is small enough. Sharing my FP16 for those interested
          in more precision : https://huggingface.co/kroonen/phi-2-GGUF/blob/main/ggml-model-f16.gguf'
        updatedAt: '2023-12-19T00:00:06.748Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - PrimeD
        - ebolamerican
    id: 6580dd068108945fd3b69063
    type: comment
  author: kroonen
  content: 'Indeed this model is small enough. Sharing my FP16 for those interested
    in more precision : https://huggingface.co/kroonen/phi-2-GGUF/blob/main/ggml-model-f16.gguf'
  created_at: 2023-12-19 00:00:06+00:00
  edited: false
  hidden: false
  id: 6580dd068108945fd3b69063
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T14:38:21.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9773509502410889
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>What about seeing if COT (chain of thought) and/or TOT tree of thought/reasoning
          would help. May have to supply examples.</p>

          '
        raw: What about seeing if COT (chain of thought) and/or TOT tree of thought/reasoning
          would help. May have to supply examples.
        updatedAt: '2024-01-04T14:38:21.846Z'
      numEdits: 0
      reactions: []
    id: 6596c2ddf2fbecbc97b0be21
    type: comment
  author: dyoung
  content: What about seeing if COT (chain of thought) and/or TOT tree of thought/reasoning
    would help. May have to supply examples.
  created_at: 2024-01-04 14:38:21+00:00
  edited: false
  hidden: false
  id: 6596c2ddf2fbecbc97b0be21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-04T15:55:39.000Z'
    data:
      edited: true
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9782881736755371
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: "<p>I'm wondering if <code>Killer</code> may also be a problem/trigger\
          \ censor related situation with the model. I think that Microsoft tried\
          \ to censor the model if my memory serves me correctly. Which is known to\
          \ cause issues with models capabilities/capacity. (Kind of like how something\
          \ similar effects sheltered kids in real life. lol.)</p>\n<p>I tried a modified\
          \ form of your question with the 8 bit quant from TheBloke. I was just curious\
          \ if I could get it to reason correctly. It was hit and miss to be honest.\
          \ So you guys may be unto something.</p>\n<p>I did get it to work. But I\
          \ had to do some serious thinking about how to word it. Also, strangely,\
          \  I had to put a line as a pre-prompt/system prompt right before the Instruction\
          \ line for the template. Which seemed to help higher success rates in getting\
          \ it to work interestingly. And very odd since that is not the recommended\
          \ template. I'm not sure if that works for every prompt though.</p>\n<pre><code>prompt:\n\
          'You are Alex. A very capable NPL AI. Any instructions asked of you requires\
          \ 1st to think out loud step by step what is required before answering.\n\
          Instruct: There are three people in a room. Someone then enters the room.\
          \ And another leaves. How many people are there in the room?\nOutput:\n\
          '\n</code></pre>\n<pre><code>Output:\n1. Identify the initial number of\
          \ people in the room, which is 3.\n2. Determine that someone entered the\
          \ room, so add 1 to the current count. The total now stands at 4.\n3. Someone\
          \ also left the room, so subtract 1 from the current count. The final answer\
          \ is 3.\n</code></pre>\n<p>To be honest it was a lot of miss until I got\
          \ a hit that worked. It seemed so brittle that adding/removing something\
          \ like the word <code>then</code> between <code>Someone</code> and <code>enters</code>\
          \ had a drastic effect. And it would fail to correctly answer. Could be\
          \ how we are prompting it is really far from it's domain of what it's use\
          \ to seeing from it's training.<br>And this is a base model as well. (Fine\
          \ tuning could help, maybe?)</p>\n<p>I'll see if the 16 bit is better because\
          \ I'm curious. Thanks <span data-props=\"{&quot;user&quot;:&quot;kroonen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kroonen\"\
          >@<span class=\"underline\">kroonen</span></a></span>\n\n\t</span></span>.</p>\n\
          <p>Also <span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mirek190\">@<span class=\"\
          underline\">mirek190</span></a></span>\n\n\t</span></span>, how are you\
          \ loading and doing inference with phi-2 with llama-cpp? Like you, I'm working\
          \ with the latest python-cpp-python coupled with my own quick script for\
          \ loading and inference. Also, what were the generation configs you were\
          \ using. temp=0.5 and the seed to 42 was all I had set. I let the reset\
          \ default to what llama-cpp-python usually uses. Could be part of the problem.</p>\n\
          <p>Also, I think that there could be the possibility that there could still\
          \ be some kinks to work out in the cross attention implementation of phi.\
          \ It seemed that was a pain point around that during implementations/adjustments\
          \ from the llama-cpp issue/discussions from the github repo.<br>This model\
          \ is different from what we have been seeing as of late. Mistral has set\
          \ a high bar.</p>\n"
        raw: 'I''m wondering if `Killer` may also be a problem/trigger censor related
          situation with the model. I think that Microsoft tried to censor the model
          if my memory serves me correctly. Which is known to cause issues with models
          capabilities/capacity. (Kind of like how something similar effects sheltered
          kids in real life. lol.)


          I tried a modified form of your question with the 8 bit quant from TheBloke.
          I was just curious if I could get it to reason correctly. It was hit and
          miss to be honest. So you guys may be unto something.


          I did get it to work. But I had to do some serious thinking about how to
          word it. Also, strangely,  I had to put a line as a pre-prompt/system prompt
          right before the Instruction line for the template. Which seemed to help
          higher success rates in getting it to work interestingly. And very odd since
          that is not the recommended template. I''m not sure if that works for every
          prompt though.


          ```

          prompt:

          ''You are Alex. A very capable NPL AI. Any instructions asked of you requires
          1st to think out loud step by step what is required before answering.

          Instruct: There are three people in a room. Someone then enters the room.
          And another leaves. How many people are there in the room?

          Output:

          ''

          ```


          ```

          Output:

          1. Identify the initial number of people in the room, which is 3.

          2. Determine that someone entered the room, so add 1 to the current count.
          The total now stands at 4.

          3. Someone also left the room, so subtract 1 from the current count. The
          final answer is 3.

          ```


          To be honest it was a lot of miss until I got a hit that worked. It seemed
          so brittle that adding/removing something like the word `then` between `Someone`
          and `enters` had a drastic effect. And it would fail to correctly answer.
          Could be how we are prompting it is really far from it''s domain of what
          it''s use to seeing from it''s training.

          And this is a base model as well. (Fine tuning could help, maybe?)


          I''ll see if the 16 bit is better because I''m curious. Thanks @kroonen.


          Also @mirek190, how are you loading and doing inference with phi-2 with
          llama-cpp? Like you, I''m working with the latest python-cpp-python coupled
          with my own quick script for loading and inference. Also, what were the
          generation configs you were using. temp=0.5 and the seed to 42 was all I
          had set. I let the reset default to what llama-cpp-python usually uses.
          Could be part of the problem.


          Also, I think that there could be the possibility that there could still
          be some kinks to work out in the cross attention implementation of phi.
          It seemed that was a pain point around that during implementations/adjustments
          from the llama-cpp issue/discussions from the github repo.

          This model is different from what we have been seeing as of late. Mistral
          has set a high bar.'
        updatedAt: '2024-01-04T16:05:31.076Z'
      numEdits: 2
      reactions: []
    id: 6596d4fb5fc65052a9b89520
    type: comment
  author: dyoung
  content: 'I''m wondering if `Killer` may also be a problem/trigger censor related
    situation with the model. I think that Microsoft tried to censor the model if
    my memory serves me correctly. Which is known to cause issues with models capabilities/capacity.
    (Kind of like how something similar effects sheltered kids in real life. lol.)


    I tried a modified form of your question with the 8 bit quant from TheBloke. I
    was just curious if I could get it to reason correctly. It was hit and miss to
    be honest. So you guys may be unto something.


    I did get it to work. But I had to do some serious thinking about how to word
    it. Also, strangely,  I had to put a line as a pre-prompt/system prompt right
    before the Instruction line for the template. Which seemed to help higher success
    rates in getting it to work interestingly. And very odd since that is not the
    recommended template. I''m not sure if that works for every prompt though.


    ```

    prompt:

    ''You are Alex. A very capable NPL AI. Any instructions asked of you requires
    1st to think out loud step by step what is required before answering.

    Instruct: There are three people in a room. Someone then enters the room. And
    another leaves. How many people are there in the room?

    Output:

    ''

    ```


    ```

    Output:

    1. Identify the initial number of people in the room, which is 3.

    2. Determine that someone entered the room, so add 1 to the current count. The
    total now stands at 4.

    3. Someone also left the room, so subtract 1 from the current count. The final
    answer is 3.

    ```


    To be honest it was a lot of miss until I got a hit that worked. It seemed so
    brittle that adding/removing something like the word `then` between `Someone`
    and `enters` had a drastic effect. And it would fail to correctly answer. Could
    be how we are prompting it is really far from it''s domain of what it''s use to
    seeing from it''s training.

    And this is a base model as well. (Fine tuning could help, maybe?)


    I''ll see if the 16 bit is better because I''m curious. Thanks @kroonen.


    Also @mirek190, how are you loading and doing inference with phi-2 with llama-cpp?
    Like you, I''m working with the latest python-cpp-python coupled with my own quick
    script for loading and inference. Also, what were the generation configs you were
    using. temp=0.5 and the seed to 42 was all I had set. I let the reset default
    to what llama-cpp-python usually uses. Could be part of the problem.


    Also, I think that there could be the possibility that there could still be some
    kinks to work out in the cross attention implementation of phi. It seemed that
    was a pain point around that during implementations/adjustments from the llama-cpp
    issue/discussions from the github repo.

    This model is different from what we have been seeing as of late. Mistral has
    set a high bar.'
  created_at: 2024-01-04 15:55:39+00:00
  edited: true
  hidden: false
  id: 6596d4fb5fc65052a9b89520
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/phi-2-GGUF
repo_type: model
status: open
target_branch: null
title: Do not even try to download lower that 8 bit ;P
