!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bbff
conflicting_files: null
created_at: 2022-09-15 12:59:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7899f30057c50786c30a31e03696be06.svg
      fullname: bff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bbff
      type: user
    createdAt: '2022-09-15T13:59:27.000Z'
    data:
      edited: false
      editors:
      - bbff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7899f30057c50786c30a31e03696be06.svg
          fullname: bff
          isHf: false
          isPro: false
          name: bbff
          type: user
        html: '<p>How can i obtain a feature representation for this fine-tuned model?
          not the feature representation in the pre-training model</p>

          '
        raw: How can i obtain a feature representation for this fine-tuned model?
          not the feature representation in the pre-training model
        updatedAt: '2022-09-15T13:59:27.676Z'
      numEdits: 0
      reactions: []
    id: 63232fbfc2d792d382d5d32c
    type: comment
  author: bbff
  content: How can i obtain a feature representation for this fine-tuned model? not
    the feature representation in the pre-training model
  created_at: 2022-09-15 12:59:27+00:00
  edited: false
  hidden: false
  id: 63232fbfc2d792d382d5d32c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcff4be06344fa23df68174e632e786d.svg
      fullname: boom meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boomshark
      type: user
    createdAt: '2023-07-17T08:17:19.000Z'
    data:
      edited: false
      editors:
      - boomshark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824398159980774
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcff4be06344fa23df68174e632e786d.svg
          fullname: boom meng
          isHf: false
          isPro: false
          name: boomshark
          type: user
        html: "<p>hi\uFF0CMay I ask if you have solved this problem? I also want to\
          \ obtain the features of the model. Can you please advise me on how to do\
          \ it?</p>\n"
        raw: "hi\uFF0CMay I ask if you have solved this problem? I also want to obtain\
          \ the features of the model. Can you please advise me on how to do it?\n\
          \n"
        updatedAt: '2023-07-17T08:17:19.714Z'
      numEdits: 0
      reactions: []
    id: 64b4f90fa8aae48db590c768
    type: comment
  author: boomshark
  content: "hi\uFF0CMay I ask if you have solved this problem? I also want to obtain\
    \ the features of the model. Can you please advise me on how to do it?\n\n"
  created_at: 2023-07-17 07:17:19+00:00
  edited: false
  hidden: false
  id: 64b4f90fa8aae48db590c768
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-07-17T10:27:23.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4112796485424042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>You can easily get a feature representation of a video by either average
          pooling the final hidden states of all the patch tokens, or using the final
          hidden state of the special CLS token:</p>

          <pre><code>from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining

          import numpy as np

          import torch


          num_frames = 16

          video = list(np.random.randn(16, 3, 224, 224))


          processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")

          model = VideoMAEForPreTraining.from_pretrained("MCG-NJU/videomae-base")


          pixel_values = processor(video, return_tensors="pt").pixel_values


          num_patches_per_frame = (model.config.image_size // model.config.patch_size)
          ** 2

          seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame

          bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()


          outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)

          feature = outputs.last_hidden_state.mean(dim=1)

          </code></pre>

          '
        raw: 'Hi,


          You can easily get a feature representation of a video by either average
          pooling the final hidden states of all the patch tokens, or using the final
          hidden state of the special CLS token:


          ```

          from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining

          import numpy as np

          import torch


          num_frames = 16

          video = list(np.random.randn(16, 3, 224, 224))


          processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")

          model = VideoMAEForPreTraining.from_pretrained("MCG-NJU/videomae-base")


          pixel_values = processor(video, return_tensors="pt").pixel_values


          num_patches_per_frame = (model.config.image_size // model.config.patch_size)
          ** 2

          seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame

          bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()


          outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)

          feature = outputs.last_hidden_state.mean(dim=1)

          ```


          '
        updatedAt: '2023-07-17T10:27:23.448Z'
      numEdits: 0
      reactions: []
    id: 64b5178b689a9a2301750517
    type: comment
  author: nielsr
  content: 'Hi,


    You can easily get a feature representation of a video by either average pooling
    the final hidden states of all the patch tokens, or using the final hidden state
    of the special CLS token:


    ```

    from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining

    import numpy as np

    import torch


    num_frames = 16

    video = list(np.random.randn(16, 3, 224, 224))


    processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")

    model = VideoMAEForPreTraining.from_pretrained("MCG-NJU/videomae-base")


    pixel_values = processor(video, return_tensors="pt").pixel_values


    num_patches_per_frame = (model.config.image_size // model.config.patch_size) **
    2

    seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame

    bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()


    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)

    feature = outputs.last_hidden_state.mean(dim=1)

    ```


    '
  created_at: 2023-07-17 09:27:23+00:00
  edited: false
  hidden: false
  id: 64b5178b689a9a2301750517
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcff4be06344fa23df68174e632e786d.svg
      fullname: boom meng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boomshark
      type: user
    createdAt: '2023-09-05T11:10:07.000Z'
    data:
      edited: false
      editors:
      - boomshark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9585979580879211
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcff4be06344fa23df68174e632e786d.svg
          fullname: boom meng
          isHf: false
          isPro: false
          name: boomshark
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nielsr\">@<span class=\"\
          underline\">nielsr</span></a></span>\n\n\t</span></span> \uFF0CSorry to\
          \ bother you, it's like this. I want to put my own dataset into this video\
          \ capture model for processing and obtain a statement that describes this\
          \ video, or an intermediate feature vector that reflects the features of\
          \ this video. However, since the training set of this model is not my own\
          \ dataset, the results obtained are very poor. Is there any good solution,\
          \ Choose a new model with strong robustness for processing? If that's the\
          \ case, do you have any recommendations or other options? Looking forward\
          \ to your reply, thank you very much</p>\n"
        raw: "@nielsr \uFF0CSorry to bother you, it's like this. I want to put my\
          \ own dataset into this video capture model for processing and obtain a\
          \ statement that describes this video, or an intermediate feature vector\
          \ that reflects the features of this video. However, since the training\
          \ set of this model is not my own dataset, the results obtained are very\
          \ poor. Is there any good solution, Choose a new model with strong robustness\
          \ for processing? If that's the case, do you have any recommendations or\
          \ other options? Looking forward to your reply, thank you very much"
        updatedAt: '2023-09-05T11:10:07.306Z'
      numEdits: 0
      reactions: []
    id: 64f70c8f75f172b08db517c9
    type: comment
  author: boomshark
  content: "@nielsr \uFF0CSorry to bother you, it's like this. I want to put my own\
    \ dataset into this video capture model for processing and obtain a statement\
    \ that describes this video, or an intermediate feature vector that reflects the\
    \ features of this video. However, since the training set of this model is not\
    \ my own dataset, the results obtained are very poor. Is there any good solution,\
    \ Choose a new model with strong robustness for processing? If that's the case,\
    \ do you have any recommendations or other options? Looking forward to your reply,\
    \ thank you very much"
  created_at: 2023-09-05 10:10:07+00:00
  edited: false
  hidden: false
  id: 64f70c8f75f172b08db517c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: MCG-NJU/videomae-base-finetuned-kinetics
repo_type: model
status: open
target_branch: null
title: How can i obtain a feature representation for this fine-tuned model?
