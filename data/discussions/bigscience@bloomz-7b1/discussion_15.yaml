!!python/object:huggingface_hub.community.DiscussionWithDetails
author: maxjvd
conflicting_files: null
created_at: 2023-06-12 17:06:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34ede634cd93a34ef23927f9a89d6f2b.svg
      fullname: MJ van Duijn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxjvd
      type: user
    createdAt: '2023-06-12T18:06:08.000Z'
    data:
      edited: false
      editors:
      - maxjvd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5035584568977356
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34ede634cd93a34ef23927f9a89d6f2b.svg
          fullname: MJ van Duijn
          isHf: false
          isPro: false
          name: maxjvd
          type: user
        html: '<p>I am not able to run bloom-7b1 through the A10 large, which works
          with models like falcon-7b. I do not understand why, as this model does
          not seem much larger than what the A10 large can handle (about 15gb VRAM).
          The model initializes, but seems to take very long for making inferences.
          </p>

          <p>Any ideas on what I may be doing wrong here?</p>

          <hr>

          <p>import gradio as gr<br>import os<br>import torch</p>

          <p>#-- sanity check on hardware<br>print(f"Is CUDA available: {torch.cuda.is_available()}")<br>#--
          True<br>print(f"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}")<br>#--
          Nvidia something something</p>

          <p>from langchain import PromptTemplate, HuggingFaceHub, LLMChain</p>

          <p>#-- possible models<br>flan = "google/flan-t5-xxl"<br>falcon_7b = "tiiuae/falcon-7b"<br>falcon_7b_instruct
          = "tiiuae/falcon-7b-instruct"<br>bloom_7b = "bigscience/bloom-7b1"<br>bloom_7b_instruct
          = "bigscience/bloomz-7b1-mt"<br>bloom_650m = "bigscience/bloom-560m"</p>

          <p>#-- set args for retrieved model<br>args = {"temperature":0.0001, "max_length":250}<br>#--
          specify model<br>llm=HuggingFaceHub(repo_id=bloom_7b, model_kwargs=args)<br>#--
          sanity check<br>print(''LLM loaded!'')</p>

          <p>#-- variable for input + eventual prompts<br>template=''{question}''<br>prompt
          = PromptTemplate(template=template, input_variables=["question"])</p>

          <p>#-- init langchain<br>chain = LLMChain(llm=llm, prompt=prompt)</p>

          <p>#-- sanity check<br>print(chain.run(''What is the Sally-Anne test?''))</p>

          <p>#-- Run the chain only specifying the input variable.<br>def answer(question):<br>    return
          chain.run(question)</p>

          <p>#-- init app<br>demo = gr.Interface(fn=answer, inputs=''text'',outputs=''text'',examples=[[''Hey
          how are you'']])<br>demo.launch()</p>

          '
        raw: "I am not able to run bloom-7b1 through the A10 large, which works with\
          \ models like falcon-7b. I do not understand why, as this model does not\
          \ seem much larger than what the A10 large can handle (about 15gb VRAM).\
          \ The model initializes, but seems to take very long for making inferences.\
          \ \r\n\r\nAny ideas on what I may be doing wrong here?\r\n\r\n---\r\n\r\n\
          \r\nimport gradio as gr\r\nimport os\r\nimport torch\r\n\r\n#-- sanity check\
          \ on hardware\r\nprint(f\"Is CUDA available: {torch.cuda.is_available()}\"\
          )\r\n#-- True\r\nprint(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\"\
          )\r\n#-- Nvidia something something\r\n\r\nfrom langchain import PromptTemplate,\
          \ HuggingFaceHub, LLMChain\r\n\r\n#-- possible models\r\nflan = \"google/flan-t5-xxl\"\
          \r\nfalcon_7b = \"tiiuae/falcon-7b\"\r\nfalcon_7b_instruct = \"tiiuae/falcon-7b-instruct\"\
          \r\nbloom_7b = \"bigscience/bloom-7b1\"\r\nbloom_7b_instruct = \"bigscience/bloomz-7b1-mt\"\
          \r\nbloom_650m = \"bigscience/bloom-560m\"\r\n\r\n#-- set args for retrieved\
          \ model\r\nargs = {\"temperature\":0.0001, \"max_length\":250}\r\n#-- specify\
          \ model\r\nllm=HuggingFaceHub(repo_id=bloom_7b, model_kwargs=args)\r\n#--\
          \ sanity check\r\nprint('LLM loaded!')\r\n\r\n#-- variable for input + eventual\
          \ prompts\r\ntemplate='{question}'\r\nprompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])\r\n\r\n#-- init langchain\r\nchain = LLMChain(llm=llm,\
          \ prompt=prompt)\r\n\r\n#-- sanity check\r\nprint(chain.run('What is the\
          \ Sally-Anne test?'))\r\n\r\n#-- Run the chain only specifying the input\
          \ variable.\r\ndef answer(question):\r\n    return chain.run(question)\r\
          \n\r\n#-- init app\r\ndemo = gr.Interface(fn=answer, inputs='text',outputs='text',examples=[['Hey\
          \ how are you']])\r\ndemo.launch()"
        updatedAt: '2023-06-12T18:06:08.732Z'
      numEdits: 0
      reactions: []
    id: 64875e904c7531fdad6213f9
    type: comment
  author: maxjvd
  content: "I am not able to run bloom-7b1 through the A10 large, which works with\
    \ models like falcon-7b. I do not understand why, as this model does not seem\
    \ much larger than what the A10 large can handle (about 15gb VRAM). The model\
    \ initializes, but seems to take very long for making inferences. \r\n\r\nAny\
    \ ideas on what I may be doing wrong here?\r\n\r\n---\r\n\r\n\r\nimport gradio\
    \ as gr\r\nimport os\r\nimport torch\r\n\r\n#-- sanity check on hardware\r\nprint(f\"\
    Is CUDA available: {torch.cuda.is_available()}\")\r\n#-- True\r\nprint(f\"CUDA\
    \ device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\r\n#--\
    \ Nvidia something something\r\n\r\nfrom langchain import PromptTemplate, HuggingFaceHub,\
    \ LLMChain\r\n\r\n#-- possible models\r\nflan = \"google/flan-t5-xxl\"\r\nfalcon_7b\
    \ = \"tiiuae/falcon-7b\"\r\nfalcon_7b_instruct = \"tiiuae/falcon-7b-instruct\"\
    \r\nbloom_7b = \"bigscience/bloom-7b1\"\r\nbloom_7b_instruct = \"bigscience/bloomz-7b1-mt\"\
    \r\nbloom_650m = \"bigscience/bloom-560m\"\r\n\r\n#-- set args for retrieved model\r\
    \nargs = {\"temperature\":0.0001, \"max_length\":250}\r\n#-- specify model\r\n\
    llm=HuggingFaceHub(repo_id=bloom_7b, model_kwargs=args)\r\n#-- sanity check\r\n\
    print('LLM loaded!')\r\n\r\n#-- variable for input + eventual prompts\r\ntemplate='{question}'\r\
    \nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\r\
    \n\r\n#-- init langchain\r\nchain = LLMChain(llm=llm, prompt=prompt)\r\n\r\n#--\
    \ sanity check\r\nprint(chain.run('What is the Sally-Anne test?'))\r\n\r\n#--\
    \ Run the chain only specifying the input variable.\r\ndef answer(question):\r\
    \n    return chain.run(question)\r\n\r\n#-- init app\r\ndemo = gr.Interface(fn=answer,\
    \ inputs='text',outputs='text',examples=[['Hey how are you']])\r\ndemo.launch()"
  created_at: 2023-06-12 17:06:08+00:00
  edited: false
  hidden: false
  id: 64875e904c7531fdad6213f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: bigscience/bloomz-7b1
repo_type: model
status: open
target_branch: null
title: loading issue in HF Spaces
