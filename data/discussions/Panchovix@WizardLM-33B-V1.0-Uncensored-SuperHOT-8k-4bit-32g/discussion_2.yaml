!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-06-27 08:22:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-06-27T09:22:41.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.706764280796051
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>I tried <code>--max_seq_len 4096 --compress_pos_emb 2</code> but
          also <code>--max_seq_len 3584 --compress_pos_emb 2</code> and unfortunately
          both results in out of memory errors:</p>

          <pre><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
          46.00 MiB (GPU 0; 24.00 GiB total capacity; 22.82 GiB already allocated;
          0 bytes free; 23.19 GiB reserved in total by PyTorch) If reserved memory
          is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          </code></pre>

          '
        raw: "I tried `--max_seq_len 4096 --compress_pos_emb 2` but also `--max_seq_len\
          \ 3584 --compress_pos_emb 2` and unfortunately both results in out of memory\
          \ errors:\r\n\r\n```\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 46.00 MiB (GPU 0; 24.00 GiB total capacity; 22.82 GiB\
          \ already allocated; 0 bytes free; 23.19 GiB reserved in total by PyTorch)\
          \ If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n```"
        updatedAt: '2023-06-27T09:22:41.959Z'
      numEdits: 0
      reactions: []
    id: 649aaa61d20c08c84a4fff10
    type: comment
  author: Thireus
  content: "I tried `--max_seq_len 4096 --compress_pos_emb 2` but also `--max_seq_len\
    \ 3584 --compress_pos_emb 2` and unfortunately both results in out of memory errors:\r\
    \n\r\n```\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 46.00 MiB (GPU 0; 24.00 GiB total capacity; 22.82 GiB already allocated; 0 bytes\
    \ free; 23.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```"
  created_at: 2023-06-27 08:22:41+00:00
  edited: false
  hidden: false
  id: 649aaa61d20c08c84a4fff10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-06-27T17:01:45.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7050769925117493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>This model will be hard to run on a single 24GB VRAM GPU. You can
          try max_seq_len about 3100 or so.</p>

          '
        raw: This model will be hard to run on a single 24GB VRAM GPU. You can try
          max_seq_len about 3100 or so.
        updatedAt: '2023-06-27T17:01:45.403Z'
      numEdits: 0
      reactions: []
    id: 649b15f99fc303937a08b3c8
    type: comment
  author: Panchovix
  content: This model will be hard to run on a single 24GB VRAM GPU. You can try max_seq_len
    about 3100 or so.
  created_at: 2023-06-27 16:01:45+00:00
  edited: false
  hidden: false
  id: 649b15f99fc303937a08b3c8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k-4bit-32g
repo_type: model
status: open
target_branch: null
title: Best parameters for 24GB VRAM?
