!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hxypqr
conflicting_files: null
created_at: 2023-11-18 12:02:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df68ed85cdb2e29d5c45c5eb8dc429e2.svg
      fullname: xy h
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hxypqr
      type: user
    createdAt: '2023-11-18T12:02:49.000Z'
    data:
      edited: false
      editors:
      - hxypqr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487334489822388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df68ed85cdb2e29d5c45c5eb8dc429e2.svg
          fullname: xy h
          isHf: false
          isPro: false
          name: hxypqr
          type: user
        html: '<p>In your paper, you explained the section about Fine-Tuning Data
          as follows:<br>In order to fine-tune our models, we paired each question
          with up to $N$ correct answers and the same number of incorrect answers.
          Up to $N$ correct answers were randomly chosen from the answers of the question.
          Each question in the corpus comes along with tags, i.e. categories indicating
          the topic of a question such as sequences-and-series or limits. As an incorrect
          answer for each question, we picked a random answer from one question sharing
          at least one tag with the original question by chance. This way, we chose
          up to $N$ incorrect answers independently from another.</p>

          <p>This procedure yields 1.9 million examples for N=1 and 2.8 million examples
          for N=10, of which 90% were used as training data for the fine-tuning task.
          We presented to the model the entire text of the questions and answers using
          the structure introduced in the previous section. In addition, we pre-trained
          an ALBERT Model on MathSE (1) and fine-tuned it on N=1. We then let this
          model predict 1,000 answers to the 2021 test set. We evaluated the answers
          against the publicly available test set from last year and paired each correct
          answer with a randomly selected incorrect answer from the model''s results.
          These question-answer pairs were used as an additional fine-tuning set which
          we denote by ANNOTATED.</p>

          <p>I would like to ask about the selection of answers here, especially how
          the left and right ends of mathematical formulas are determined. Is it done
          by judging whether it is a symbol like $ using a similar regular expression
          method?</p>

          '
        raw: "In your paper, you explained the section about Fine-Tuning Data as follows:\r\
          \nIn order to fine-tune our models, we paired each question with up to $N$\
          \ correct answers and the same number of incorrect answers. Up to $N$ correct\
          \ answers were randomly chosen from the answers of the question. Each question\
          \ in the corpus comes along with tags, i.e. categories indicating the topic\
          \ of a question such as sequences-and-series or limits. As an incorrect\
          \ answer for each question, we picked a random answer from one question\
          \ sharing at least one tag with the original question by chance. This way,\
          \ we chose up to $N$ incorrect answers independently from another.\r\n\r\
          \n\r\nThis procedure yields 1.9 million examples for N=1 and 2.8 million\
          \ examples for N=10, of which 90% were used as training data for the fine-tuning\
          \ task. We presented to the model the entire text of the questions and answers\
          \ using the structure introduced in the previous section. In addition, we\
          \ pre-trained an ALBERT Model on MathSE (1) and fine-tuned it on N=1. We\
          \ then let this model predict 1,000 answers to the 2021 test set. We evaluated\
          \ the answers against the publicly available test set from last year and\
          \ paired each correct answer with a randomly selected incorrect answer from\
          \ the model's results. These question-answer pairs were used as an additional\
          \ fine-tuning set which we denote by ANNOTATED.\r\n\r\nI would like to ask\
          \ about the selection of answers here, especially how the left and right\
          \ ends of mathematical formulas are determined. Is it done by judging whether\
          \ it is a symbol like $ using a similar regular expression method?"
        updatedAt: '2023-11-18T12:02:49.953Z'
      numEdits: 0
      reactions: []
    id: 6558a7e9029b03128bca70b0
    type: comment
  author: hxypqr
  content: "In your paper, you explained the section about Fine-Tuning Data as follows:\r\
    \nIn order to fine-tune our models, we paired each question with up to $N$ correct\
    \ answers and the same number of incorrect answers. Up to $N$ correct answers\
    \ were randomly chosen from the answers of the question. Each question in the\
    \ corpus comes along with tags, i.e. categories indicating the topic of a question\
    \ such as sequences-and-series or limits. As an incorrect answer for each question,\
    \ we picked a random answer from one question sharing at least one tag with the\
    \ original question by chance. This way, we chose up to $N$ incorrect answers\
    \ independently from another.\r\n\r\n\r\nThis procedure yields 1.9 million examples\
    \ for N=1 and 2.8 million examples for N=10, of which 90% were used as training\
    \ data for the fine-tuning task. We presented to the model the entire text of\
    \ the questions and answers using the structure introduced in the previous section.\
    \ In addition, we pre-trained an ALBERT Model on MathSE (1) and fine-tuned it\
    \ on N=1. We then let this model predict 1,000 answers to the 2021 test set. We\
    \ evaluated the answers against the publicly available test set from last year\
    \ and paired each correct answer with a randomly selected incorrect answer from\
    \ the model's results. These question-answer pairs were used as an additional\
    \ fine-tuning set which we denote by ANNOTATED.\r\n\r\nI would like to ask about\
    \ the selection of answers here, especially how the left and right ends of mathematical\
    \ formulas are determined. Is it done by judging whether it is a symbol like $\
    \ using a similar regular expression method?"
  created_at: 2023-11-18 12:02:49+00:00
  edited: false
  hidden: false
  id: 6558a7e9029b03128bca70b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/416fb2ebfd377e174ed561ce4c3edcc7.svg
      fullname: Anja
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: AnReu
      type: user
    createdAt: '2023-12-06T13:10:20.000Z'
    data:
      edited: false
      editors:
      - AnReu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.929495632648468
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/416fb2ebfd377e174ed561ce4c3edcc7.svg
          fullname: Anja
          isHf: false
          isPro: false
          name: AnReu
          type: user
        html: '<p>Hi!<br>Sorry for the late reply. "One answer" is an entire answer
          post from the math stack exchange, not only one formula. So, there is no
          need to find the left and right ends of a formula. However, we converted
          the data from the HTML format, in which the ARQMath data was provided, to
          plain text-only with formulas written in LaTeX. Each formulas was enclosed
          in a math-container HTML element. We parsed the entire answer post using
          beautiful soup and removed all HTML syntax. We enclosed the formulas in
          $ ... $ after removing the math-container element.</p>

          <p>I hope that helps clarifying! Let me know if you have further questions!</p>

          '
        raw: 'Hi!

          Sorry for the late reply. "One answer" is an entire answer post from the
          math stack exchange, not only one formula. So, there is no need to find
          the left and right ends of a formula. However, we converted the data from
          the HTML format, in which the ARQMath data was provided, to plain text-only
          with formulas written in LaTeX. Each formulas was enclosed in a math-container
          HTML element. We parsed the entire answer post using beautiful soup and
          removed all HTML syntax. We enclosed the formulas in $ ... $ after removing
          the math-container element.


          I hope that helps clarifying! Let me know if you have further questions!'
        updatedAt: '2023-12-06T13:10:20.117Z'
      numEdits: 0
      reactions: []
    id: 657072bcc4993b8fb9632971
    type: comment
  author: AnReu
  content: 'Hi!

    Sorry for the late reply. "One answer" is an entire answer post from the math
    stack exchange, not only one formula. So, there is no need to find the left and
    right ends of a formula. However, we converted the data from the HTML format,
    in which the ARQMath data was provided, to plain text-only with formulas written
    in LaTeX. Each formulas was enclosed in a math-container HTML element. We parsed
    the entire answer post using beautiful soup and removed all HTML syntax. We enclosed
    the formulas in $ ... $ after removing the math-container element.


    I hope that helps clarifying! Let me know if you have further questions!'
  created_at: 2023-12-06 13:10:20+00:00
  edited: false
  hidden: false
  id: 657072bcc4993b8fb9632971
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: AnReu/math_albert
repo_type: model
status: open
target_branch: null
title: Could you explain the details about the finetune dataset?
