!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joejztang
conflicting_files: null
created_at: 2023-10-10 18:54:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4ULmncNi3uTpdFe_-IV4S.png?w=200&h=200&f=face
      fullname: Joel Tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joejztang
      type: user
    createdAt: '2023-10-10T19:54:33.000Z'
    data:
      edited: false
      editors:
      - joejztang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8306904435157776
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4ULmncNi3uTpdFe_-IV4S.png?w=200&h=200&f=face
          fullname: Joel Tang
          isHf: false
          isPro: false
          name: joejztang
          type: user
        html: "<p>Hi there,<br>I was loading in <code>openlm-research/open_llama_3b_v2</code>\
          \ and trying to create a baseline. One thing I observed here was <em>seems\
          \ to me</em>, the model refuses to generate eos token so that the conversation\
          \ seems endlessly.<br>For example, when I asked \"Q: Is apple red?\\nA:\"\
          , I got </p>\n<pre><code>&lt;s&gt;Q: Is apple red?\nA: No, apple is not\
          \ red.\nQ: Is apple green?\nA: No, apple is not green.\nQ: Is apple yellow?\n\
          A: No, apple is not yellow.\nQ: Is apple orange?\nA: No, apple is not orange.\n\
          Q: Is apple blue?\nA: No, apple is not blue.\nQ: Is apple pink?\nA: No,\
          \ apple is not pink.\nQ: Is apple purple?\nA: No, apple is not purple.\n\
          Q: Is apple black?\nA: No, apple is not black.\nQ: Is apple brown?\nA: No,\
          \ apple is not brown.\nQ: Is apple white?\nA: No, apple is not white.\n\
          Q: Is apple red?\nA: No, apple is not red.\nQ: Is apple green?\nA: No, apple\
          \ is not green.\nQ: Is apple yellow?\nA: No, apple is not yellow.\nQ: Is\
          \ apple orange?\nA: No, apple is not orange.\nQ: Is apple blue?\nA: No,\
          \ apple is not blue.\nQ: Is apple pink?\nA: No\n</code></pre>\n<p>What was\
          \ expect from me is (despite the fact first)</p>\n<pre><code>&lt;s&gt;Q:\
          \ Is apple red?\nA: No, apple is not red.\n</code></pre>\n<p>What can I\
          \ do to make it happen?</p>\n<hr>\n<p>code details:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> LlamaTokenizer, LlamaForCausalLM\n\n\nmodel_path = <span\
          \ class=\"hljs-string\">'openlm-research/open_llama_3b_v2'</span>\ntokenizer\
          \ = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n\
          \    model_path, torch_dtype=torch.float16, device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>\n)\n\nprompte = <span class=\"hljs-string\">'Q: Is apple\
          \ red?\\nA:'</span>\ninpute = tokenizer(prompte, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>).input_ids.to(device)\ngeneration_output = model.generate(\n\
          \    input_ids=inpute, max_new_tokens=<span class=\"hljs-number\">256</span>\n\
          )\n<span class=\"hljs-comment\"># print(generation_output)</span>\n<span\
          \ class=\"hljs-built_in\">print</span>(tokenizer.decode(generation_output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "Hi there,\r\nI was loading in `openlm-research/open_llama_3b_v2` and\
          \ trying to create a baseline. One thing I observed here was _seems to me_,\
          \ the model refuses to generate eos token so that the conversation seems\
          \ endlessly.\r\nFor example, when I asked \"Q: Is apple red?\\nA:\", I got\
          \ \r\n```\r\n<s>Q: Is apple red?\r\nA: No, apple is not red.\r\nQ: Is apple\
          \ green?\r\nA: No, apple is not green.\r\nQ: Is apple yellow?\r\nA: No,\
          \ apple is not yellow.\r\nQ: Is apple orange?\r\nA: No, apple is not orange.\r\
          \nQ: Is apple blue?\r\nA: No, apple is not blue.\r\nQ: Is apple pink?\r\n\
          A: No, apple is not pink.\r\nQ: Is apple purple?\r\nA: No, apple is not\
          \ purple.\r\nQ: Is apple black?\r\nA: No, apple is not black.\r\nQ: Is apple\
          \ brown?\r\nA: No, apple is not brown.\r\nQ: Is apple white?\r\nA: No, apple\
          \ is not white.\r\nQ: Is apple red?\r\nA: No, apple is not red.\r\nQ: Is\
          \ apple green?\r\nA: No, apple is not green.\r\nQ: Is apple yellow?\r\n\
          A: No, apple is not yellow.\r\nQ: Is apple orange?\r\nA: No, apple is not\
          \ orange.\r\nQ: Is apple blue?\r\nA: No, apple is not blue.\r\nQ: Is apple\
          \ pink?\r\nA: No\r\n```\r\nWhat was expect from me is (despite the fact\
          \ first)\r\n```\r\n<s>Q: Is apple red?\r\nA: No, apple is not red.\r\n```\r\
          \nWhat can I do to make it happen?\r\n\r\n------\r\ncode details:\r\n```python\r\
          \nimport torch\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\
          \n\r\n\r\nmodel_path = 'openlm-research/open_llama_3b_v2'\r\ntokenizer =\
          \ LlamaTokenizer.from_pretrained(model_path)\r\nmodel = LlamaForCausalLM.from_pretrained(\r\
          \n    model_path, torch_dtype=torch.float16, device_map=\"auto\"\r\n)\r\n\
          \r\nprompte = 'Q: Is apple red?\\nA:'\r\ninpute = tokenizer(prompte, return_tensors=\"\
          pt\").input_ids.to(device)\r\ngeneration_output = model.generate(\r\n  \
          \  input_ids=inpute, max_new_tokens=256\r\n)\r\n# print(generation_output)\r\
          \nprint(tokenizer.decode(generation_output[0]))\r\n```"
        updatedAt: '2023-10-10T19:54:33.400Z'
      numEdits: 0
      reactions: []
    id: 6525abf9f06ac0cf9a7de309
    type: comment
  author: joejztang
  content: "Hi there,\r\nI was loading in `openlm-research/open_llama_3b_v2` and trying\
    \ to create a baseline. One thing I observed here was _seems to me_, the model\
    \ refuses to generate eos token so that the conversation seems endlessly.\r\n\
    For example, when I asked \"Q: Is apple red?\\nA:\", I got \r\n```\r\n<s>Q: Is\
    \ apple red?\r\nA: No, apple is not red.\r\nQ: Is apple green?\r\nA: No, apple\
    \ is not green.\r\nQ: Is apple yellow?\r\nA: No, apple is not yellow.\r\nQ: Is\
    \ apple orange?\r\nA: No, apple is not orange.\r\nQ: Is apple blue?\r\nA: No,\
    \ apple is not blue.\r\nQ: Is apple pink?\r\nA: No, apple is not pink.\r\nQ: Is\
    \ apple purple?\r\nA: No, apple is not purple.\r\nQ: Is apple black?\r\nA: No,\
    \ apple is not black.\r\nQ: Is apple brown?\r\nA: No, apple is not brown.\r\n\
    Q: Is apple white?\r\nA: No, apple is not white.\r\nQ: Is apple red?\r\nA: No,\
    \ apple is not red.\r\nQ: Is apple green?\r\nA: No, apple is not green.\r\nQ:\
    \ Is apple yellow?\r\nA: No, apple is not yellow.\r\nQ: Is apple orange?\r\nA:\
    \ No, apple is not orange.\r\nQ: Is apple blue?\r\nA: No, apple is not blue.\r\
    \nQ: Is apple pink?\r\nA: No\r\n```\r\nWhat was expect from me is (despite the\
    \ fact first)\r\n```\r\n<s>Q: Is apple red?\r\nA: No, apple is not red.\r\n```\r\
    \nWhat can I do to make it happen?\r\n\r\n------\r\ncode details:\r\n```python\r\
    \nimport torch\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\n\
    \r\n\r\nmodel_path = 'openlm-research/open_llama_3b_v2'\r\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\r\
    \nmodel = LlamaForCausalLM.from_pretrained(\r\n    model_path, torch_dtype=torch.float16,\
    \ device_map=\"auto\"\r\n)\r\n\r\nprompte = 'Q: Is apple red?\\nA:'\r\ninpute\
    \ = tokenizer(prompte, return_tensors=\"pt\").input_ids.to(device)\r\ngeneration_output\
    \ = model.generate(\r\n    input_ids=inpute, max_new_tokens=256\r\n)\r\n# print(generation_output)\r\
    \nprint(tokenizer.decode(generation_output[0]))\r\n```"
  created_at: 2023-10-10 18:54:33+00:00
  edited: false
  hidden: false
  id: 6525abf9f06ac0cf9a7de309
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: openlm-research/open_llama_3b_v2
repo_type: model
status: open
target_branch: null
title: model doesn't predict eos token?
