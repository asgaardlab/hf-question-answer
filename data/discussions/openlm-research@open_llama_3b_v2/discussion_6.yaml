!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sadra-barikbin
conflicting_files: null
created_at: 2023-09-20 05:47:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645456475590-noauth.jpeg?w=200&h=200&f=face
      fullname: Sadrodin Barikbin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sadra-barikbin
      type: user
    createdAt: '2023-09-20T06:47:07.000Z'
    data:
      edited: false
      editors:
      - sadra-barikbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5702940821647644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645456475590-noauth.jpeg?w=200&h=200&f=face
          fullname: Sadrodin Barikbin
          isHf: false
          isPro: false
          name: sadra-barikbin
          type: user
        html: "<p>Hi,<br>As of <code>transformers==4.32.0</code> (the latest is <code>4.33.2</code>),\
          \ <code>inv_freq</code> buffers of rotary embeddings in the model are not\
          \ part of the state dict, hence producing error on loading checkpoints.</p>\n\
          <p>To reproduce:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ LlamaForCausalLM, LlamaConfig\n\nconfig_dict = json.load(<span class=\"\
          hljs-built_in\">open</span>(<span class=\"hljs-string\">\"config.json\"\
          </span>))\nconfig = LlamaConfig(**config_dict)\n\nmodel = LlamaForCausalLM(config)\n\
          state_dict = torch.load(<span class=\"hljs-built_in\">open</span>(<span\
          \ class=\"hljs-string\">\"pytorch_model.bin\"</span>, <span class=\"hljs-string\"\
          >'rb'</span>))\nmodel.load_state_dict(state_dict)\n</code></pre>\n<p>Output:</p>\n\
          <pre><code>Error(s) in loading state_dict for LlamaForCausalLM:\n    Unexpected\
          \ key(s) in state_dict: \"model.layers.0.self_attn.rotary_emb.inv_freq\"\
          , \"model.layers.1.self_attn.rotary_emb.inv_freq\", \"model.layers.2.self_attn.rotary_emb.inv_freq\"\
          , \"model.layers.3.self_attn.rotary_emb.inv_freq\", ...\n</code></pre>\n"
        raw: "Hi,\r\nAs of `transformers==4.32.0` (the latest is `4.33.2`), `inv_freq`\
          \ buffers of rotary embeddings in the model are not part of the state dict,\
          \ hence producing error on loading checkpoints.\r\n\r\nTo reproduce:\r\n\
          ```python\r\nfrom transformers import LlamaForCausalLM, LlamaConfig\r\n\r\
          \nconfig_dict = json.load(open(\"config.json\"))\r\nconfig = LlamaConfig(**config_dict)\r\
          \n\r\nmodel = LlamaForCausalLM(config)\r\nstate_dict = torch.load(open(\"\
          pytorch_model.bin\", 'rb'))\r\nmodel.load_state_dict(state_dict)\r\n```\r\
          \n\r\nOutput:\r\n```\r\nError(s) in loading state_dict for LlamaForCausalLM:\r\
          \n\tUnexpected key(s) in state_dict: \"model.layers.0.self_attn.rotary_emb.inv_freq\"\
          , \"model.layers.1.self_attn.rotary_emb.inv_freq\", \"model.layers.2.self_attn.rotary_emb.inv_freq\"\
          , \"model.layers.3.self_attn.rotary_emb.inv_freq\", ...\r\n```"
        updatedAt: '2023-09-20T06:47:07.725Z'
      numEdits: 0
      reactions: []
    id: 650a956b7e0d56c271947ace
    type: comment
  author: sadra-barikbin
  content: "Hi,\r\nAs of `transformers==4.32.0` (the latest is `4.33.2`), `inv_freq`\
    \ buffers of rotary embeddings in the model are not part of the state dict, hence\
    \ producing error on loading checkpoints.\r\n\r\nTo reproduce:\r\n```python\r\n\
    from transformers import LlamaForCausalLM, LlamaConfig\r\n\r\nconfig_dict = json.load(open(\"\
    config.json\"))\r\nconfig = LlamaConfig(**config_dict)\r\n\r\nmodel = LlamaForCausalLM(config)\r\
    \nstate_dict = torch.load(open(\"pytorch_model.bin\", 'rb'))\r\nmodel.load_state_dict(state_dict)\r\
    \n```\r\n\r\nOutput:\r\n```\r\nError(s) in loading state_dict for LlamaForCausalLM:\r\
    \n\tUnexpected key(s) in state_dict: \"model.layers.0.self_attn.rotary_emb.inv_freq\"\
    , \"model.layers.1.self_attn.rotary_emb.inv_freq\", \"model.layers.2.self_attn.rotary_emb.inv_freq\"\
    , \"model.layers.3.self_attn.rotary_emb.inv_freq\", ...\r\n```"
  created_at: 2023-09-20 05:47:07+00:00
  edited: false
  hidden: false
  id: 650a956b7e0d56c271947ace
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: openlm-research/open_llama_3b_v2
repo_type: model
status: open
target_branch: null
title: Unexpected `inv_freq` buffers in the checkpoint
