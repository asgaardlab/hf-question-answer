!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jetaudio
conflicting_files: null
created_at: 2023-12-09 08:54:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
      fullname: Hoang Manh Linh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jetaudio
      type: user
    createdAt: '2023-12-09T08:54:33.000Z'
    data:
      edited: false
      editors:
      - jetaudio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222997426986694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
          fullname: Hoang Manh Linh
          isHf: false
          isPro: false
          name: jetaudio
          type: user
        html: '<p>I succesfully load this model in 4bit with example code (load_in_4bit=True)
          but it generates nothing. How can I fix that?</p>

          '
        raw: I succesfully load this model in 4bit with example code (load_in_4bit=True)
          but it generates nothing. How can I fix that?
        updatedAt: '2023-12-09T08:54:33.452Z'
      numEdits: 0
      reactions: []
    id: 65742b498b44ef012b23349b
    type: comment
  author: jetaudio
  content: I succesfully load this model in 4bit with example code (load_in_4bit=True)
    but it generates nothing. How can I fix that?
  created_at: 2023-12-09 08:54:33+00:00
  edited: false
  hidden: false
  id: 65742b498b44ef012b23349b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
      fullname: Dat Quoc Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dqnguyen
      type: user
    createdAt: '2023-12-12T08:38:20.000Z'
    data:
      edited: false
      editors:
      - dqnguyen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8688352108001709
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
          fullname: Dat Quoc Nguyen
          isHf: false
          isPro: false
          name: dqnguyen
          type: user
        html: '<p>It does not work like that.<br>See: <a href="https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes">https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes</a></p>

          '
        raw: "It does not work like that. \nSee: https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes"
        updatedAt: '2023-12-12T08:38:20.158Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65781bfc62d3ac1817e73dad
    id: 65781bfc62d3ac1817e73daa
    type: comment
  author: dqnguyen
  content: "It does not work like that. \nSee: https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes"
  created_at: 2023-12-12 08:38:20+00:00
  edited: false
  hidden: false
  id: 65781bfc62d3ac1817e73daa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
      fullname: Dat Quoc Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dqnguyen
      type: user
    createdAt: '2023-12-12T08:38:20.000Z'
    data:
      status: closed
    id: 65781bfc62d3ac1817e73dad
    type: status-change
  author: dqnguyen
  created_at: 2023-12-12 08:38:20+00:00
  id: 65781bfc62d3ac1817e73dad
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a17365a969c0146d1653ee6765bf0dc0.svg
      fullname: "L\xE2m Qu\u1ED1c Tuy\xEAn"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tuyenlqvnp
      type: user
    createdAt: '2023-12-13T01:29:27.000Z'
    data:
      edited: false
      editors:
      - tuyenlqvnp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.906411349773407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a17365a969c0146d1653ee6765bf0dc0.svg
          fullname: "L\xE2m Qu\u1ED1c Tuy\xEAn"
          isHf: false
          isPro: false
          name: tuyenlqvnp
          type: user
        html: '<p>i''m too, load model successfull, but generates is nothing</p>

          '
        raw: i'm too, load model successfull, but generates is nothing
        updatedAt: '2023-12-13T01:29:27.820Z'
      numEdits: 0
      reactions: []
    id: 657908f77f5da1deb6a04c7a
    type: comment
  author: tuyenlqvnp
  content: i'm too, load model successfull, but generates is nothing
  created_at: 2023-12-13 01:29:27+00:00
  edited: false
  hidden: false
  id: 657908f77f5da1deb6a04c7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
      fullname: Hoang Manh Linh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jetaudio
      type: user
    createdAt: '2023-12-13T02:21:15.000Z'
    data:
      edited: true
      editors:
      - jetaudio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8570892810821533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
          fullname: Hoang Manh Linh
          isHf: false
          isPro: false
          name: jetaudio
          type: user
        html: "<p>I can make it work by removing the config... line and letting AutoModelForCausalLM\
          \ decide.</p>\n<p>from transformers import AutoConfig, AutoModelForCausalLM,\
          \ AutoTokenizer  </p>\n<p>model_path = \"vinai/PhoGPT-7B5-Instruct\"  </p>\n\
          <p>model = AutoModelForCausalLM.from_pretrained(<br>    model_path, config=config,\
          \ torch_dtype=torch.bfloat16, trust_remote_code=True, load_in_4bit=True<br>)</p>\n\
          <p>model.eval()  </p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)  </p>\n<p>PROMPT = \"### C\xE2u h\u1ECFi:\\n{instruction}\\\
          n\\n### Tr\u1EA3 l\u1EDDi:\"  </p>\n<p>input_prompt = PROMPT.format_map(<br>\
          \    {\"instruction\": \"L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\"}<br>)  </p>\n<p>input_ids\
          \ = tokenizer(input_prompt, return_tensors=\"pt\")  </p>\n<p>outputs = model.generate(<br>\
          \    inputs=input_ids[\"input_ids\"].to(\"cuda\"),<br>    attention_mask=input_ids[\"\
          attention_mask\"].to(\"cuda\"),<br>    do_sample=True,<br>    temperature=1.3,<br>\
          \    top_k=50,<br>    top_p=0.9,<br>    max_new_tokens=1024,<br>    eos_token_id=tokenizer.eos_token_id,<br>\
          \    pad_token_id=tokenizer.pad_token_id<br>)  </p>\n<p>response = tokenizer.batch_decode(outputs,\
          \ skip_special_tokens=True)[0]<br>response = response.split(\"### Tr\u1EA3\
          \ l\u1EDDi:\")[1]</p>\n"
        raw: "I can make it work by removing the config... line and letting AutoModelForCausalLM\
          \ decide.\n\nfrom transformers import AutoConfig, AutoModelForCausalLM,\
          \ AutoTokenizer  \n  \nmodel_path = \"vinai/PhoGPT-7B5-Instruct\"  \n  \n\
          model = AutoModelForCausalLM.from_pretrained(  \n    model_path, config=config,\
          \ torch_dtype=torch.bfloat16, trust_remote_code=True, load_in_4bit=True\n\
          )\n\nmodel.eval()  \n  \ntokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)  \n  \nPROMPT = \"### C\xE2u h\u1ECFi:\\n{instruction}\\\
          n\\n### Tr\u1EA3 l\u1EDDi:\"  \n  \ninput_prompt = PROMPT.format_map(  \n\
          \    {\"instruction\": \"L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7\
          n k\u1EF9 n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\"}  \n)  \n  \ninput_ids\
          \ = tokenizer(input_prompt, return_tensors=\"pt\")  \n  \noutputs = model.generate(\
          \  \n    inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n    attention_mask=input_ids[\"\
          attention_mask\"].to(\"cuda\"),  \n    do_sample=True,  \n    temperature=1.3,\
          \  \n    top_k=50,  \n    top_p=0.9,  \n    max_new_tokens=1024,  \n   \
          \ eos_token_id=tokenizer.eos_token_id,  \n    pad_token_id=tokenizer.pad_token_id\
          \  \n)  \n  \nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\
          \  \nresponse = response.split(\"### Tr\u1EA3 l\u1EDDi:\")[1]\n"
        updatedAt: '2023-12-13T02:22:52.340Z'
      numEdits: 1
      reactions: []
    id: 6579151bdd2996f01a1b37da
    type: comment
  author: jetaudio
  content: "I can make it work by removing the config... line and letting AutoModelForCausalLM\
    \ decide.\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\
    \  \n  \nmodel_path = \"vinai/PhoGPT-7B5-Instruct\"  \n  \nmodel = AutoModelForCausalLM.from_pretrained(\
    \  \n    model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True,\
    \ load_in_4bit=True\n)\n\nmodel.eval()  \n  \ntokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ trust_remote_code=True)  \n  \nPROMPT = \"### C\xE2u h\u1ECFi:\\n{instruction}\\\
    n\\n### Tr\u1EA3 l\u1EDDi:\"  \n  \ninput_prompt = PROMPT.format_map(  \n    {\"\
    instruction\": \"L\xE0m th\u1EBF n\xE0o \u0111\u1EC3 c\u1EA3i thi\u1EC7n k\u1EF9\
    \ n\u0103ng qu\u1EA3n l\xFD th\u1EDDi gian?\"}  \n)  \n  \ninput_ids = tokenizer(input_prompt,\
    \ return_tensors=\"pt\")  \n  \noutputs = model.generate(  \n    inputs=input_ids[\"\
    input_ids\"].to(\"cuda\"),  \n    attention_mask=input_ids[\"attention_mask\"\
    ].to(\"cuda\"),  \n    do_sample=True,  \n    temperature=1.3,  \n    top_k=50,\
    \  \n    top_p=0.9,  \n    max_new_tokens=1024,  \n    eos_token_id=tokenizer.eos_token_id,\
    \  \n    pad_token_id=tokenizer.pad_token_id  \n)  \n  \nresponse = tokenizer.batch_decode(outputs,\
    \ skip_special_tokens=True)[0]  \nresponse = response.split(\"### Tr\u1EA3 l\u1EDD\
    i:\")[1]\n"
  created_at: 2023-12-13 02:21:15+00:00
  edited: true
  hidden: false
  id: 6579151bdd2996f01a1b37da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
      fullname: Dat Quoc Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dqnguyen
      type: user
    createdAt: '2023-12-13T10:27:28.000Z'
    data:
      edited: true
      editors:
      - dqnguyen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4397442638874054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656994960629-5efb0c998a239c73b0d4c8b5.jpeg?w=200&h=200&f=face
          fullname: Dat Quoc Nguyen
          isHf: false
          isPro: false
          name: dqnguyen
          type: user
        html: "<p>Following <a href=\"https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes\"\
          >https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes</a>\
          \ ttps://huggingface.co/docs/transformers/main/en/quantization#4-bit as\
          \ mentioned in <a href=\"https://huggingface.co/vinai/PhoGPT-7B5/discussions/7#65781bfc62d3ac1817e73daa\"\
          >https://huggingface.co/vinai/PhoGPT-7B5/discussions/7#65781bfc62d3ac1817e73daa</a></p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n\
          \nquantization_config = BitsAndBytesConfig(load_in_4bit=<span class=\"hljs-literal\"\
          >True</span>, bnb_4bit_compute_dtype=torch.float16)\n\nmodel_quant = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"vinai/PhoGPT-7B5-Instruct\"</span>, quantization_config=quantization_config,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>Or:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> torch\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\
          \nconfig = AutoConfig.from_pretrained(<span class=\"hljs-string\">\"vinai/PhoGPT-7B5-Instruct\"\
          </span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)  \n\
          config.init_device = <span class=\"hljs-string\">\"cuda\"</span>\n\nquantization_config\
          \ = BitsAndBytesConfig(load_in_4bit=<span class=\"hljs-literal\">True</span>,\
          \ bnb_4bit_compute_dtype=torch.float16)\n\nmodel_quant = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"vinai/PhoGPT-7B5-Instruct\"</span>, quantization_config=quantization_config,\
          \ config=config, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\
          </code></pre>\n"
        raw: "Following https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes\
          \ ttps://huggingface.co/docs/transformers/main/en/quantization#4-bit as\
          \ mentioned in https://huggingface.co/vinai/PhoGPT-7B5/discussions/7#65781bfc62d3ac1817e73daa\n\
          \n```python \nimport torch\nfrom transformers import BitsAndBytesConfig,\
          \ AutoModelForCausalLM, AutoTokenizer\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
          \ bnb_4bit_compute_dtype=torch.float16)\n\nmodel_quant = AutoModelForCausalLM.from_pretrained(\"\
          vinai/PhoGPT-7B5-Instruct\", quantization_config=quantization_config, device_map=\"\
          auto\", trust_remote_code=True)\n```\n\nOr:\n\n```python \nimport torch\n\
          from transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM,\
          \ AutoTokenizer\n\nconfig = AutoConfig.from_pretrained(\"vinai/PhoGPT-7B5-Instruct\"\
          , trust_remote_code=True)  \nconfig.init_device = \"cuda\"\n\nquantization_config\
          \ = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\
          \nmodel_quant = AutoModelForCausalLM.from_pretrained(\"vinai/PhoGPT-7B5-Instruct\"\
          , quantization_config=quantization_config, config=config, trust_remote_code=True)\n\
          ```\n"
        updatedAt: '2023-12-13T10:33:18.033Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jetaudio
        - phnghiapro
    id: 657987108ee8830a31d98247
    type: comment
  author: dqnguyen
  content: "Following https://huggingface.co/docs/transformers/v4.36.0/en/quantization#bitsandbytes\
    \ ttps://huggingface.co/docs/transformers/main/en/quantization#4-bit as mentioned\
    \ in https://huggingface.co/vinai/PhoGPT-7B5/discussions/7#65781bfc62d3ac1817e73daa\n\
    \n```python \nimport torch\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM,\
    \ AutoTokenizer\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_compute_dtype=torch.float16)\n\nmodel_quant = AutoModelForCausalLM.from_pretrained(\"\
    vinai/PhoGPT-7B5-Instruct\", quantization_config=quantization_config, device_map=\"\
    auto\", trust_remote_code=True)\n```\n\nOr:\n\n```python \nimport torch\nfrom\
    \ transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\
    \nconfig = AutoConfig.from_pretrained(\"vinai/PhoGPT-7B5-Instruct\", trust_remote_code=True)\
    \  \nconfig.init_device = \"cuda\"\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_compute_dtype=torch.float16)\n\nmodel_quant = AutoModelForCausalLM.from_pretrained(\"\
    vinai/PhoGPT-7B5-Instruct\", quantization_config=quantization_config, config=config,\
    \ trust_remote_code=True)\n```\n"
  created_at: 2023-12-13 10:27:28+00:00
  edited: true
  hidden: false
  id: 657987108ee8830a31d98247
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
      fullname: Hoang Manh Linh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jetaudio
      type: user
    createdAt: '2023-12-13T17:15:28.000Z'
    data:
      edited: false
      editors:
      - jetaudio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.991673469543457
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c0fd274850b397c02195842fadcce82.svg
          fullname: Hoang Manh Linh
          isHf: false
          isPro: false
          name: jetaudio
          type: user
        html: '<p>Thanks about that</p>

          '
        raw: Thanks about that
        updatedAt: '2023-12-13T17:15:28.009Z'
      numEdits: 0
      reactions: []
    id: 6579e6b047ae87ca727eac5f
    type: comment
  author: jetaudio
  content: Thanks about that
  created_at: 2023-12-13 17:15:28+00:00
  edited: false
  hidden: false
  id: 6579e6b047ae87ca727eac5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a17365a969c0146d1653ee6765bf0dc0.svg
      fullname: "L\xE2m Qu\u1ED1c Tuy\xEAn"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tuyenlqvnp
      type: user
    createdAt: '2023-12-15T02:31:16.000Z'
    data:
      edited: false
      editors:
      - tuyenlqvnp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6519240736961365
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a17365a969c0146d1653ee6765bf0dc0.svg
          fullname: "L\xE2m Qu\u1ED1c Tuy\xEAn"
          isHf: false
          isPro: false
          name: tuyenlqvnp
          type: user
        html: '<p>when run code have:</p>

          <p>C:\Users......./.cache\huggingface\modules\transformers_modules\vinai\PhoGPT-7B5-Instruct\d1a5a418bf01d49e8bf1b5b737b8ef143a33d9fd\configuration_mpt.py:97:
          UserWarning: alibi is turned on, setting <code>learned_pos_emb</code> to
          <code>False.</code><br>  warnings.warn(f''alibi is turned on, setting <code>learned_pos_emb</code>
          to <code>False.</code>'')</p>

          <p>and generate is not thing, how to fix :((</p>

          '
        raw: "when run code have:\n\nC:\\Users\\......./.cache\\huggingface\\modules\\\
          transformers_modules\\vinai\\PhoGPT-7B5-Instruct\\d1a5a418bf01d49e8bf1b5b737b8ef143a33d9fd\\\
          configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb`\
          \ to `False.`\n  warnings.warn(f'alibi is turned on, setting `learned_pos_emb`\
          \ to `False.`')\n\nand generate is not thing, how to fix :(("
        updatedAt: '2023-12-15T02:31:16.946Z'
      numEdits: 0
      reactions: []
    id: 657bba74429a20edb50a2d19
    type: comment
  author: tuyenlqvnp
  content: "when run code have:\n\nC:\\Users\\......./.cache\\huggingface\\modules\\\
    transformers_modules\\vinai\\PhoGPT-7B5-Instruct\\d1a5a418bf01d49e8bf1b5b737b8ef143a33d9fd\\\
    configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb`\
    \ to `False.`\n  warnings.warn(f'alibi is turned on, setting `learned_pos_emb`\
    \ to `False.`')\n\nand generate is not thing, how to fix :(("
  created_at: 2023-12-15 02:31:16+00:00
  edited: false
  hidden: false
  id: 657bba74429a20edb50a2d19
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: vinai/PhoGPT-7B5
repo_type: model
status: closed
target_branch: null
title: How can I run this model in 4bit?
