!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RASMUS
conflicting_files: null
created_at: 2022-06-15 13:39:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
      fullname: TOIVANEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RASMUS
      type: user
    createdAt: '2022-06-15T14:39:16.000Z'
    data:
      edited: false
      editors:
      - RASMUS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
          fullname: TOIVANEN
          isHf: false
          isPro: false
          name: RASMUS
          type: user
        html: '<p>I have two questions:</p>

          <ol>

          <li>Could this model benefit from the same kind of KenLM model like there
          is with Wav2vec2 models available (using pyctcdecode)?</li>

          <li>Are there finetuning examples available?</li>

          </ol>

          '
        raw: "I have two questions:\r\n1. Could this model benefit from the same kind\
          \ of KenLM model like there is with Wav2vec2 models available (using pyctcdecode)?\r\
          \n2. Are there finetuning examples available?"
        updatedAt: '2022-06-15T14:39:16.427Z'
      numEdits: 0
      reactions: []
    id: 62a9ef147efa1b5e8678e704
    type: comment
  author: RASMUS
  content: "I have two questions:\r\n1. Could this model benefit from the same kind\
    \ of KenLM model like there is with Wav2vec2 models available (using pyctcdecode)?\r\
    \n2. Are there finetuning examples available?"
  created_at: 2022-06-15 13:39:16+00:00
  edited: false
  hidden: false
  id: 62a9ef147efa1b5e8678e704
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1615683805593-604d5ef6d8a4193fd3a928f2.png?w=200&h=200&f=face
      fullname: Loren Lugosch
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lorenlugosch
      type: user
    createdAt: '2022-06-16T02:31:14.000Z'
    data:
      edited: false
      editors:
      - lorenlugosch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1615683805593-604d5ef6d8a4193fd3a928f2.png?w=200&h=200&f=face
          fullname: Loren Lugosch
          isHf: false
          isPro: false
          name: lorenlugosch
          type: user
        html: '<ol>

          <li><p>Yes! We got a big improvement on LibriSpeech when we decoded using
          an n-gram language model.</p>

          </li>

          <li><p>I haven''t tried finetuning in Hugging Face, but glancing at the
          wav2vec 2.0 examples, it might be possible to just replace that model with
          M-CTC-T since both are just PyTorch encoders.</p>

          </li>

          </ol>

          '
        raw: '1. Yes! We got a big improvement on LibriSpeech when we decoded using
          an n-gram language model.


          2. I haven''t tried finetuning in Hugging Face, but glancing at the wav2vec
          2.0 examples, it might be possible to just replace that model with M-CTC-T
          since both are just PyTorch encoders.'
        updatedAt: '2022-06-16T02:31:14.881Z'
      numEdits: 0
      reactions: []
    id: 62aa95f20efa8121f4a43594
    type: comment
  author: lorenlugosch
  content: '1. Yes! We got a big improvement on LibriSpeech when we decoded using
    an n-gram language model.


    2. I haven''t tried finetuning in Hugging Face, but glancing at the wav2vec 2.0
    examples, it might be possible to just replace that model with M-CTC-T since both
    are just PyTorch encoders.'
  created_at: 2022-06-16 01:31:14+00:00
  edited: false
  hidden: false
  id: 62aa95f20efa8121f4a43594
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1615683805593-604d5ef6d8a4193fd3a928f2.png?w=200&h=200&f=face
      fullname: Loren Lugosch
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lorenlugosch
      type: user
    createdAt: '2022-06-16T02:31:14.000Z'
    data:
      status: closed
    id: 62aa95f20efa8121f4a43595
    type: status-change
  author: lorenlugosch
  created_at: 2022-06-16 01:31:14+00:00
  id: 62aa95f20efa8121f4a43595
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
      fullname: TOIVANEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RASMUS
      type: user
    createdAt: '2022-06-16T12:40:50.000Z'
    data:
      edited: false
      editors:
      - RASMUS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
          fullname: TOIVANEN
          isHf: false
          isPro: false
          name: RASMUS
          type: user
        html: "<ol>\n<li>How was the processs to add n-gram model? Is it already supported\
          \ with object like MCTCTProcessorWithLM like in wav2vec?<br>Just now the\
          \ difference would be to not remove cased characters and separators etch?\
          \ <a href=\"https://huggingface.co/blog/wav2vec2-with-ngram\">https://huggingface.co/blog/wav2vec2-with-ngram</a></li>\n\
          <li>Is there support in this for timestamps in output?</li>\n</ol>\n<p>This\
          \ would be perfect tool with those additions for generating transcriptions\
          \ from language x to language y and add those to video.<br>I have been building\
          \ a demo that uses wav2vec2 + T5 for casing/punctuation correction and then\
          \ use those to map out sentence level timestamps + transcriptions --&gt;\
          \ feed to OpusMT model for translation to English --&gt; Burn the translations\
          \ to original video. The T5 is a bit of clunky for that kind of stuff (Handles\
          \ only 128/256 tokens at time) and I need to use bunch of ugly matching\
          \ logic back to original asr model output to get things right.</p>\n<p>See\
          \ my short demo here in the thread <span data-props=\"{&quot;user&quot;:&quot;lorenlugosch&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lorenlugosch\"\
          >@<span class=\"underline\">lorenlugosch</span></a></span>\n\n\t</span></span>\
          \ <a rel=\"nofollow\" href=\"https://twitter.com/itsafiz/status/1533484258597437440\"\
          >https://twitter.com/itsafiz/status/1533484258597437440</a> </p>\n<p>Pinging<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\">@<span\
          \ class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span>\
          \ if he has time to give his thoughts</p>\n"
        raw: "1. How was the processs to add n-gram model? Is it already supported\
          \ with object like MCTCTProcessorWithLM like in wav2vec?\nJust now the difference\
          \ would be to not remove cased characters and separators etch? https://huggingface.co/blog/wav2vec2-with-ngram\n\
          2. Is there support in this for timestamps in output?\n\nThis would be perfect\
          \ tool with those additions for generating transcriptions from language\
          \ x to language y and add those to video.\nI have been building a demo that\
          \ uses wav2vec2 + T5 for casing/punctuation correction and then use those\
          \ to map out sentence level timestamps + transcriptions --> feed to OpusMT\
          \ model for translation to English --> Burn the translations to original\
          \ video. The T5 is a bit of clunky for that kind of stuff (Handles only\
          \ 128/256 tokens at time) and I need to use bunch of ugly matching logic\
          \ back to original asr model output to get things right.\n\nSee my short\
          \ demo here in the thread @lorenlugosch https://twitter.com/itsafiz/status/1533484258597437440\
          \ \n\nPinging \n@patrickvonplaten if he has time to give his thoughts"
        updatedAt: '2022-06-16T12:40:50.978Z'
      numEdits: 0
      reactions: []
    id: 62ab24d2d690953376b8d0ec
    type: comment
  author: RASMUS
  content: "1. How was the processs to add n-gram model? Is it already supported with\
    \ object like MCTCTProcessorWithLM like in wav2vec?\nJust now the difference would\
    \ be to not remove cased characters and separators etch? https://huggingface.co/blog/wav2vec2-with-ngram\n\
    2. Is there support in this for timestamps in output?\n\nThis would be perfect\
    \ tool with those additions for generating transcriptions from language x to language\
    \ y and add those to video.\nI have been building a demo that uses wav2vec2 +\
    \ T5 for casing/punctuation correction and then use those to map out sentence\
    \ level timestamps + transcriptions --> feed to OpusMT model for translation to\
    \ English --> Burn the translations to original video. The T5 is a bit of clunky\
    \ for that kind of stuff (Handles only 128/256 tokens at time) and I need to use\
    \ bunch of ugly matching logic back to original asr model output to get things\
    \ right.\n\nSee my short demo here in the thread @lorenlugosch https://twitter.com/itsafiz/status/1533484258597437440\
    \ \n\nPinging \n@patrickvonplaten if he has time to give his thoughts"
  created_at: 2022-06-16 11:40:50+00:00
  edited: false
  hidden: false
  id: 62ab24d2d690953376b8d0ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1615683805593-604d5ef6d8a4193fd3a928f2.png?w=200&h=200&f=face
      fullname: Loren Lugosch
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lorenlugosch
      type: user
    createdAt: '2022-06-16T12:50:14.000Z'
    data:
      edited: false
      editors:
      - lorenlugosch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1615683805593-604d5ef6d8a4193fd3a928f2.png?w=200&h=200&f=face
          fullname: Loren Lugosch
          isHf: false
          isPro: false
          name: lorenlugosch
          type: user
        html: '<ol>

          <li><p>You could try the wav2vec LM decoder, assuming the interface is the
          same (logits as inputs)? We ran our LM experiments using Flashlight.</p>

          </li>

          <li><p>You could maybe generate timestamps by feeding the output logits
          into a tool like CTC-Segmentation: <code>https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/alignment/ctc_segmentation.py</code></p>

          </li>

          </ol>

          <p>But note that this model doesn''t handle long utterances very well because
          it was trained on Common Voice (which contains only short utterances), so
          you might need to split the audio from your videos into smaller chunks before
          running the model.</p>

          '
        raw: '1. You could try the wav2vec LM decoder, assuming the interface is the
          same (logits as inputs)? We ran our LM experiments using Flashlight.


          2. You could maybe generate timestamps by feeding the output logits into
          a tool like CTC-Segmentation: `https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/alignment/ctc_segmentation.py`


          But note that this model doesn''t handle long utterances very well because
          it was trained on Common Voice (which contains only short utterances), so
          you might need to split the audio from your videos into smaller chunks before
          running the model.'
        updatedAt: '2022-06-16T12:50:14.898Z'
      numEdits: 0
      reactions: []
    id: 62ab2706890c7773a6af79cf
    type: comment
  author: lorenlugosch
  content: '1. You could try the wav2vec LM decoder, assuming the interface is the
    same (logits as inputs)? We ran our LM experiments using Flashlight.


    2. You could maybe generate timestamps by feeding the output logits into a tool
    like CTC-Segmentation: `https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/alignment/ctc_segmentation.py`


    But note that this model doesn''t handle long utterances very well because it
    was trained on Common Voice (which contains only short utterances), so you might
    need to split the audio from your videos into smaller chunks before running the
    model.'
  created_at: 2022-06-16 11:50:14+00:00
  edited: false
  hidden: false
  id: 62ab2706890c7773a6af79cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
      fullname: TOIVANEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RASMUS
      type: user
    createdAt: '2022-06-16T13:14:36.000Z'
    data:
      edited: false
      editors:
      - RASMUS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
          fullname: TOIVANEN
          isHf: false
          isPro: false
          name: RASMUS
          type: user
        html: "<p>Thanks for fast answer.<br>I will take time to ponder and what things\
          \ we will take under work next in our applied research stuff with <span\
          \ data-props=\"{&quot;user&quot;:&quot;aapot&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/aapot\">@<span class=\"underline\"\
          >aapot</span></a></span>\n\n\t</span></span><br>I am now mostly working\
          \ on building a pipeline from long audios to gather pretraining material\
          \ for Finnish but hopefully we can spend some time on this too!</p>\n"
        raw: "Thanks for fast answer.\nI will take time to ponder and what things\
          \ we will take under work next in our applied research stuff with @aapot\
          \ \nI am now mostly working on building a pipeline from long audios to gather\
          \ pretraining material for Finnish but hopefully we can spend some time\
          \ on this too!"
        updatedAt: '2022-06-16T13:14:36.681Z'
      numEdits: 0
      reactions: []
    id: 62ab2cbc40184c23a3470842
    type: comment
  author: RASMUS
  content: "Thanks for fast answer.\nI will take time to ponder and what things we\
    \ will take under work next in our applied research stuff with @aapot \nI am now\
    \ mostly working on building a pipeline from long audios to gather pretraining\
    \ material for Finnish but hopefully we can spend some time on this too!"
  created_at: 2022-06-16 12:14:36+00:00
  edited: false
  hidden: false
  id: 62ab2cbc40184c23a3470842
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: speechbrain/m-ctc-t-large
repo_type: model
status: closed
target_branch: null
title: KenLM, finetuning?
