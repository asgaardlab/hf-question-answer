!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-10 05:50:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-10T05:50:58.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7689613103866577
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;LoneStriker&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/LoneStriker\">@<span class=\"underline\"\
          >LoneStriker</span></a></span>\n\n\t</span></span> </p>\n<p>Is this model\
          \ ready to be quantized, or not?</p>\n<p>Thanks!</p>\n"
        raw: "@TheBloke @LoneStriker \r\n\r\nIs this model ready to be quantized,\
          \ or not?\r\n\r\nThanks!"
        updatedAt: '2023-12-10T05:50:58.266Z'
      numEdits: 0
      reactions: []
    id: 657551c2d0ed8f57612b2191
    type: comment
  author: Yhyu13
  content: "@TheBloke @LoneStriker \r\n\r\nIs this model ready to be quantized, or\
    \ not?\r\n\r\nThanks!"
  created_at: 2023-12-10 05:50:58+00:00
  edited: false
  hidden: false
  id: 657551c2d0ed8f57612b2191
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-10T05:55:54.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9313727617263794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>It''s a new architecture, so from the exl2 quant side, Turbo will
          have to add support for the model before it can be quantized.  If it''s
          supported in Transformers, TheBloke may be able to generate GPTQ quants
          though.</p>

          '
        raw: It's a new architecture, so from the exl2 quant side, Turbo will have
          to add support for the model before it can be quantized.  If it's supported
          in Transformers, TheBloke may be able to generate GPTQ quants though.
        updatedAt: '2023-12-10T05:55:54.985Z'
      numEdits: 0
      reactions: []
    id: 657552eaa4ee9a4fe7a7fa39
    type: comment
  author: LoneStriker
  content: It's a new architecture, so from the exl2 quant side, Turbo will have to
    add support for the model before it can be quantized.  If it's supported in Transformers,
    TheBloke may be able to generate GPTQ quants though.
  created_at: 2023-12-10 05:55:54+00:00
  edited: false
  hidden: false
  id: 657552eaa4ee9a4fe7a7fa39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
      fullname: M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maykeye
      type: user
    createdAt: '2023-12-11T12:30:01.000Z'
    data:
      edited: false
      editors:
      - Maykeye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9127241373062134
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
          fullname: M
          isHf: false
          isPro: false
          name: Maykeye
          type: user
        html: '<p>You can use load_in_8bit or load_in_4bit (in fact that''s there
          only method I got to load in 16GB ram) to quantize on the fly.</p>

          <p>Also FFT is memory hungry. In torch 2.1.1 it can eat 2GB of memory for
          cache (not emptied by empty_cache) if you are not careful enough, in old
          it just leaked <a rel="nofollow" href="https://github.com/pytorch/pytorch/issues/94893">the
          memory</a> and it doesn''t support bf16</p>

          <p>Fortunately FlashFFTConv are coming and there''s recurrent prefill mechanism(I
          haven''t tried it)</p>

          '
        raw: 'You can use load_in_8bit or load_in_4bit (in fact that''s there only
          method I got to load in 16GB ram) to quantize on the fly.


          Also FFT is memory hungry. In torch 2.1.1 it can eat 2GB of memory for cache
          (not emptied by empty_cache) if you are not careful enough, in old it just
          leaked [the memory](https://github.com/pytorch/pytorch/issues/94893) and
          it doesn''t support bf16


          Fortunately FlashFFTConv are coming and there''s recurrent prefill mechanism(I
          haven''t tried it)




          '
        updatedAt: '2023-12-11T12:30:01.816Z'
      numEdits: 0
      reactions: []
    id: 657700c92eb103d91fc9e5eb
    type: comment
  author: Maykeye
  content: 'You can use load_in_8bit or load_in_4bit (in fact that''s there only method
    I got to load in 16GB ram) to quantize on the fly.


    Also FFT is memory hungry. In torch 2.1.1 it can eat 2GB of memory for cache (not
    emptied by empty_cache) if you are not careful enough, in old it just leaked [the
    memory](https://github.com/pytorch/pytorch/issues/94893) and it doesn''t support
    bf16


    Fortunately FlashFFTConv are coming and there''s recurrent prefill mechanism(I
    haven''t tried it)




    '
  created_at: 2023-12-11 12:30:01+00:00
  edited: false
  hidden: false
  id: 657700c92eb103d91fc9e5eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: togethercomputer/StripedHyena-Nous-7B
repo_type: model
status: open
target_branch: null
title: Quantization pls?
