!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abhinavkulkarni
conflicting_files: null
created_at: 2023-12-11 18:19:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-12-11T18:19:54.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33175015449523926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoModelForCausalLM, AutoConfig, AutoTokenizer\n\
          \nmodel_id = <span class=\"hljs-string\">\"togethercomputer/StripedHyena-Nous-7B\"\
          </span>\n\n<span class=\"hljs-comment\"># Config</span>\nconfig = AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"\
          hljs-comment\"># Tokenizer</span>\n<span class=\"hljs-keyword\">try</span>:\n\
          \    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-keyword\">except</span>:\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=<span\
          \ class=\"hljs-literal\">False</span>, trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\n</code></pre>\n<p>I get the following error:</p>\n\
          <pre><code class=\"language-python\">---------------------------------------------------------------------------\n\
          AttributeError                            Traceback (most recent call last)\n\
          Cell In[<span class=\"hljs-number\">4</span>], line <span class=\"hljs-number\"\
          >3</span>\n      <span class=\"hljs-number\">2</span> <span class=\"hljs-keyword\"\
          >try</span>:\n----&gt; <span class=\"hljs-number\">3</span>     tokenizer\
          \ = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n      <span class=\"hljs-number\"\
          >4</span> <span class=\"hljs-keyword\">except</span>:\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/configuration_utils.py:<span\
          \ class=\"hljs-number\">265</span>, <span class=\"hljs-keyword\">in</span>\
          \ PretrainedConfig.__getattribute__(self, key)\n    <span class=\"hljs-number\"\
          >264</span>     key = <span class=\"hljs-built_in\">super</span>().__getattribute__(<span\
          \ class=\"hljs-string\">\"attribute_map\"</span>)[key]\n--&gt; <span class=\"\
          hljs-number\">265</span> <span class=\"hljs-keyword\">return</span> <span\
          \ class=\"hljs-built_in\">super</span>().__getattribute__(key)\n\nAttributeError:\
          \ <span class=\"hljs-string\">'StripedHyenaConfig'</span> <span class=\"\
          hljs-built_in\">object</span> has no attribute <span class=\"hljs-string\"\
          >'tokenizer_name'</span>\n\nDuring handling of the above exception, another\
          \ exception occurred:\n\nTypeError                                 Traceback\
          \ (most recent call last)\nCell In[<span class=\"hljs-number\">4</span>],\
          \ line <span class=\"hljs-number\">5</span>\n      <span class=\"hljs-number\"\
          >3</span>     tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\n      <span\
          \ class=\"hljs-number\">4</span> <span class=\"hljs-keyword\">except</span>:\n\
          ----&gt; <span class=\"hljs-number\">5</span>     tokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=<span class=\"hljs-literal\">False</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/models/auto/tokenization_auto.py:<span\
          \ class=\"hljs-number\">787</span>, <span class=\"hljs-keyword\">in</span>\
          \ AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    <span class=\"hljs-number\">783</span>     <span class=\"\
          hljs-keyword\">if</span> tokenizer_class <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-literal\">None</span>:\n    <span class=\"hljs-number\"\
          >784</span>         <span class=\"hljs-keyword\">raise</span> ValueError(\n\
          \    <span class=\"hljs-number\">785</span>             <span class=\"hljs-string\"\
          >f\"Tokenizer class <span class=\"hljs-subst\">{tokenizer_class_candidate}</span>\
          \ does not exist or is not currently imported.\"</span>\n    <span class=\"\
          hljs-number\">786</span>         )\n--&gt; <span class=\"hljs-number\">787</span>\
          \     <span class=\"hljs-keyword\">return</span> tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    <span class=\"hljs-number\">789</span> <span class=\"\
          hljs-comment\"># Otherwise we have to be creative.</span>\n    <span class=\"\
          hljs-number\">790</span> <span class=\"hljs-comment\"># if model is an encoder\
          \ decoder, the encoder tokenizer class is used by default</span>\n    <span\
          \ class=\"hljs-number\">791</span> <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-built_in\">isinstance</span>(config, EncoderDecoderConfig):\n\
          \nFile ~/miniconda3/envs/transformers/lib/python3<span class=\"hljs-number\"\
          >.10</span>/site-packages/transformers/tokenization_utils_base.py:<span\
          \ class=\"hljs-number\">2028</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   <span class=\"hljs-number\">2025</span>     <span class=\"\
          hljs-keyword\">else</span>:\n   <span class=\"hljs-number\">2026</span>\
          \         logger.info(<span class=\"hljs-string\">f\"loading file <span\
          \ class=\"hljs-subst\">{file_path}</span> from cache at <span class=\"hljs-subst\"\
          >{resolved_vocab_files[file_id]}</span>\"</span>)\n-&gt; <span class=\"\
          hljs-number\">2028</span> <span class=\"hljs-keyword\">return</span> cls._from_pretrained(\n\
          \   <span class=\"hljs-number\">2029</span>     resolved_vocab_files,\n\
          \   <span class=\"hljs-number\">2030</span>     pretrained_model_name_or_path,\n\
          \   <span class=\"hljs-number\">2031</span>     init_configuration,\n  \
          \ <span class=\"hljs-number\">2032</span>     *init_inputs,\n   <span class=\"\
          hljs-number\">2033</span>     token=token,\n   <span class=\"hljs-number\"\
          >2034</span>     cache_dir=cache_dir,\n   <span class=\"hljs-number\">2035</span>\
          \     local_files_only=local_files_only,\n   <span class=\"hljs-number\"\
          >2036</span>     _commit_hash=commit_hash,\n   <span class=\"hljs-number\"\
          >2037</span>     _is_local=is_local,\n   <span class=\"hljs-number\">2038</span>\
          \     **kwargs,\n   <span class=\"hljs-number\">2039</span> )\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/tokenization_utils_base.py:<span\
          \ class=\"hljs-number\">2260</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, token, cache_dir, local_files_only, _commit_hash,\
          \ _is_local, *init_inputs, **kwargs)\n   <span class=\"hljs-number\">2258</span>\
          \ <span class=\"hljs-comment\"># Instantiate the tokenizer.</span>\n   <span\
          \ class=\"hljs-number\">2259</span> <span class=\"hljs-keyword\">try</span>:\n\
          -&gt; <span class=\"hljs-number\">2260</span>     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   <span class=\"hljs-number\">2261</span> <span class=\"\
          hljs-keyword\">except</span> OSError:\n   <span class=\"hljs-number\">2262</span>\
          \     <span class=\"hljs-keyword\">raise</span> OSError(\n   <span class=\"\
          hljs-number\">2263</span>         <span class=\"hljs-string\">\"Unable to\
          \ load vocabulary from file. \"</span>\n   <span class=\"hljs-number\">2264</span>\
          \         <span class=\"hljs-string\">\"Please check that the provided vocabulary\
          \ is accessible and not corrupted.\"</span>\n   <span class=\"hljs-number\"\
          >2265</span>     )\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/models/llama/tokenization_llama.py:<span\
          \ class=\"hljs-number\">178</span>, <span class=\"hljs-keyword\">in</span>\
          \ LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
          \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
          \ use_default_system_prompt, spaces_between_special_tokens, legacy, **kwargs)\n\
          \    <span class=\"hljs-number\">176</span> self.add_eos_token = add_eos_token\n\
          \    <span class=\"hljs-number\">177</span> self.use_default_system_prompt\
          \ = use_default_system_prompt\n--&gt; <span class=\"hljs-number\">178</span>\
          \ self.sp_model = self.get_spm_processor(kwargs.pop(<span class=\"hljs-string\"\
          >\"from_slow\"</span>, <span class=\"hljs-literal\">False</span>))\n   \
          \ <span class=\"hljs-number\">180</span> <span class=\"hljs-built_in\">super</span>().__init__(\n\
          \    <span class=\"hljs-number\">181</span>     bos_token=bos_token,\n \
          \   <span class=\"hljs-number\">182</span>     eos_token=eos_token,\n  \
          \ (...)\n    <span class=\"hljs-number\">192</span>     **kwargs,\n    <span\
          \ class=\"hljs-number\">193</span> )\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/models/llama/tokenization_llama.py:<span\
          \ class=\"hljs-number\">203</span>, <span class=\"hljs-keyword\">in</span>\
          \ LlamaTokenizer.get_spm_processor(self, from_slow)\n    <span class=\"\
          hljs-number\">201</span> tokenizer = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
          \    <span class=\"hljs-number\">202</span> <span class=\"hljs-keyword\"\
          >if</span> self.legacy <span class=\"hljs-keyword\">or</span> from_slow:\
          \  <span class=\"hljs-comment\"># no dependency on protobuf</span>\n--&gt;\
          \ <span class=\"hljs-number\">203</span>     tokenizer.Load(self.vocab_file)\n\
          \    <span class=\"hljs-number\">204</span>     <span class=\"hljs-keyword\"\
          >return</span> tokenizer\n    <span class=\"hljs-number\">206</span> <span\
          \ class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(self.vocab_file,\
          \ <span class=\"hljs-string\">\"rb\"</span>) <span class=\"hljs-keyword\"\
          >as</span> f:\n\nFile ~/miniconda3/envs/transformers/lib/python3<span class=\"\
          hljs-number\">.10</span>/site-packages/sentencepiece/__init__.py:<span class=\"\
          hljs-number\">905</span>, <span class=\"hljs-keyword\">in</span> SentencePieceProcessor.Load(self,\
          \ model_file, model_proto)\n    <span class=\"hljs-number\">903</span> <span\
          \ class=\"hljs-keyword\">if</span> model_proto:\n    <span class=\"hljs-number\"\
          >904</span>   <span class=\"hljs-keyword\">return</span> self.LoadFromSerializedProto(model_proto)\n\
          --&gt; <span class=\"hljs-number\">905</span> <span class=\"hljs-keyword\"\
          >return</span> self.LoadFromFile(model_file)\n\nFile ~/miniconda3/envs/transformers/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/sentencepiece/__init__.py:<span\
          \ class=\"hljs-number\">310</span>, <span class=\"hljs-keyword\">in</span>\
          \ SentencePieceProcessor.LoadFromFile(self, arg)\n    <span class=\"hljs-number\"\
          >309</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">LoadFromFile</span>(<span class=\"hljs-params\">self, arg</span>):\n\
          --&gt; <span class=\"hljs-number\">310</span>     <span class=\"hljs-keyword\"\
          >return</span> _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n\nTypeError: <span class=\"hljs-keyword\">not</span> a string\n\
          </code></pre>\n<p>Thanks for the model update!</p>\n"
        raw: "```python\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoConfig, AutoTokenizer\r\n\r\nmodel_id = \"togethercomputer/StripedHyena-Nous-7B\"\
          \r\n\r\n# Config\r\nconfig = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\r\
          \n\r\n# Tokenizer\r\ntry:\r\n    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
          \ trust_remote_code=True)\r\nexcept:\r\n    tokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=False, trust_remote_code=True)\r\n```\r\n\r\nI get the following\
          \ error:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\nCell In[4], line 3\r\n      2 try:\r\n----> 3     tokenizer =\
          \ AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=True)\r\
          \n      4 except:\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:265,\
          \ in PretrainedConfig.__getattribute__(self, key)\r\n    264     key = super().__getattribute__(\"\
          attribute_map\")[key]\r\n--> 265 return super().__getattribute__(key)\r\n\
          \r\nAttributeError: 'StripedHyenaConfig' object has no attribute 'tokenizer_name'\r\
          \n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTypeError                                 Traceback (most recent call\
          \ last)\r\nCell In[4], line 5\r\n      3     tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
          \ trust_remote_code=True)\r\n      4 except:\r\n----> 5     tokenizer =\
          \ AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\r\
          \n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:787,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    783     if tokenizer_class is None:\r\n    784      \
          \   raise ValueError(\r\n    785             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    786         )\r\n\
          --> 787     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    789 # Otherwise we have to be creative.\r\n\
          \    790 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    791 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2028,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   2025     else:\r\n   2026         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 2028 return cls._from_pretrained(\r\n   2029     resolved_vocab_files,\r\
          \n   2030     pretrained_model_name_or_path,\r\n   2031     init_configuration,\r\
          \n   2032     *init_inputs,\r\n   2033     token=token,\r\n   2034     cache_dir=cache_dir,\r\
          \n   2035     local_files_only=local_files_only,\r\n   2036     _commit_hash=commit_hash,\r\
          \n   2037     _is_local=is_local,\r\n   2038     **kwargs,\r\n   2039 )\r\
          \n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2260,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   2258 # Instantiate\
          \ the tokenizer.\r\n   2259 try:\r\n-> 2260     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   2261 except OSError:\r\n   2262     raise OSError(\r\
          \n   2263         \"Unable to load vocabulary from file. \"\r\n   2264 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   2265     )\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:178,\
          \ in LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
          \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
          \ use_default_system_prompt, spaces_between_special_tokens, legacy, **kwargs)\r\
          \n    176 self.add_eos_token = add_eos_token\r\n    177 self.use_default_system_prompt\
          \ = use_default_system_prompt\r\n--> 178 self.sp_model = self.get_spm_processor(kwargs.pop(\"\
          from_slow\", False))\r\n    180 super().__init__(\r\n    181     bos_token=bos_token,\r\
          \n    182     eos_token=eos_token,\r\n   (...)\r\n    192     **kwargs,\r\
          \n    193 )\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:203,\
          \ in LlamaTokenizer.get_spm_processor(self, from_slow)\r\n    201 tokenizer\
          \ = spm.SentencePieceProcessor(**self.sp_model_kwargs)\r\n    202 if self.legacy\
          \ or from_slow:  # no dependency on protobuf\r\n--> 203     tokenizer.Load(self.vocab_file)\r\
          \n    204     return tokenizer\r\n    206 with open(self.vocab_file, \"\
          rb\") as f:\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/sentencepiece/__init__.py:905,\
          \ in SentencePieceProcessor.Load(self, model_file, model_proto)\r\n    903\
          \ if model_proto:\r\n    904   return self.LoadFromSerializedProto(model_proto)\r\
          \n--> 905 return self.LoadFromFile(model_file)\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/sentencepiece/__init__.py:310,\
          \ in SentencePieceProcessor.LoadFromFile(self, arg)\r\n    309 def LoadFromFile(self,\
          \ arg):\r\n--> 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\n\r\nTypeError: not a string\r\n```\r\n\r\nThanks for the model\
          \ update!"
        updatedAt: '2023-12-11T18:19:54.219Z'
      numEdits: 0
      reactions: []
    id: 657752ca92ead0e20f451ccc
    type: comment
  author: abhinavkulkarni
  content: "```python\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoConfig, AutoTokenizer\r\n\r\nmodel_id = \"togethercomputer/StripedHyena-Nous-7B\"\
    \r\n\r\n# Config\r\nconfig = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\r\
    \n\r\n# Tokenizer\r\ntry:\r\n    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
    \ trust_remote_code=True)\r\nexcept:\r\n    tokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ use_fast=False, trust_remote_code=True)\r\n```\r\n\r\nI get the following error:\r\
    \n\r\n```python\r\n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \nCell In[4], line 3\r\n      2 try:\r\n----> 3     tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
    \ trust_remote_code=True)\r\n      4 except:\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:265,\
    \ in PretrainedConfig.__getattribute__(self, key)\r\n    264     key = super().__getattribute__(\"\
    attribute_map\")[key]\r\n--> 265 return super().__getattribute__(key)\r\n\r\n\
    AttributeError: 'StripedHyenaConfig' object has no attribute 'tokenizer_name'\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \nCell In[4], line 5\r\n      3     tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name,\
    \ trust_remote_code=True)\r\n      4 except:\r\n----> 5     tokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ use_fast=False, trust_remote_code=True)\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:787,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    783     if tokenizer_class is None:\r\n    784         raise\
    \ ValueError(\r\n    785             f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is not currently imported.\"\r\n    786         )\r\n--> 787\
    \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    789 # Otherwise we have to be creative.\r\n    790 # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default\r\n  \
    \  791 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2028,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   2025     else:\r\n   2026         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 2028 return\
    \ cls._from_pretrained(\r\n   2029     resolved_vocab_files,\r\n   2030     pretrained_model_name_or_path,\r\
    \n   2031     init_configuration,\r\n   2032     *init_inputs,\r\n   2033    \
    \ token=token,\r\n   2034     cache_dir=cache_dir,\r\n   2035     local_files_only=local_files_only,\r\
    \n   2036     _commit_hash=commit_hash,\r\n   2037     _is_local=is_local,\r\n\
    \   2038     **kwargs,\r\n   2039 )\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2260,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2258 # Instantiate the tokenizer.\r\n   2259 try:\r\
    \n-> 2260     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   2261 except OSError:\r\
    \n   2262     raise OSError(\r\n   2263         \"Unable to load vocabulary from\
    \ file. \"\r\n   2264         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   2265     )\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:178,\
    \ in LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
    \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
    \ use_default_system_prompt, spaces_between_special_tokens, legacy, **kwargs)\r\
    \n    176 self.add_eos_token = add_eos_token\r\n    177 self.use_default_system_prompt\
    \ = use_default_system_prompt\r\n--> 178 self.sp_model = self.get_spm_processor(kwargs.pop(\"\
    from_slow\", False))\r\n    180 super().__init__(\r\n    181     bos_token=bos_token,\r\
    \n    182     eos_token=eos_token,\r\n   (...)\r\n    192     **kwargs,\r\n  \
    \  193 )\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:203,\
    \ in LlamaTokenizer.get_spm_processor(self, from_slow)\r\n    201 tokenizer =\
    \ spm.SentencePieceProcessor(**self.sp_model_kwargs)\r\n    202 if self.legacy\
    \ or from_slow:  # no dependency on protobuf\r\n--> 203     tokenizer.Load(self.vocab_file)\r\
    \n    204     return tokenizer\r\n    206 with open(self.vocab_file, \"rb\") as\
    \ f:\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/sentencepiece/__init__.py:905,\
    \ in SentencePieceProcessor.Load(self, model_file, model_proto)\r\n    903 if\
    \ model_proto:\r\n    904   return self.LoadFromSerializedProto(model_proto)\r\
    \n--> 905 return self.LoadFromFile(model_file)\r\n\r\nFile ~/miniconda3/envs/transformers/lib/python3.10/site-packages/sentencepiece/__init__.py:310,\
    \ in SentencePieceProcessor.LoadFromFile(self, arg)\r\n    309 def LoadFromFile(self,\
    \ arg):\r\n--> 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\n\r\nTypeError: not a string\r\n```\r\n\r\nThanks for the model update!"
  created_at: 2023-12-11 18:19:54+00:00
  edited: false
  hidden: false
  id: 657752ca92ead0e20f451ccc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
      fullname: nicholas like
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicklikets
      type: user
    createdAt: '2023-12-12T15:04:39.000Z'
    data:
      edited: false
      editors:
      - nicklikets
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7921428680419922
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
          fullname: nicholas like
          isHf: false
          isPro: false
          name: nicklikets
          type: user
        html: '<p>When loading the config using <code>AutoConfig</code>, the attribute
          <code>config.tokenizer_name</code> does not exist. </p>

          <p>Is there a reason why you are using the parameter <code>use_fast=False</code>
          on the exception block and not on the initial try block? As the tokeniser
          would load as expected removing <code>use_fast</code> from the except block
          where you are loading the model from the <code>model_id</code></p>

          '
        raw: "When loading the config using `AutoConfig`, the attribute `config.tokenizer_name`\
          \ does not exist. \n\nIs there a reason why you are using the parameter\
          \ `use_fast=False` on the exception block and not on the initial try block?\
          \ As the tokeniser would load as expected removing `use_fast` from the except\
          \ block where you are loading the model from the `model_id`"
        updatedAt: '2023-12-12T15:04:39.901Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - abhinavkulkarni
    id: 657876871bf8e371c4dfd405
    type: comment
  author: nicklikets
  content: "When loading the config using `AutoConfig`, the attribute `config.tokenizer_name`\
    \ does not exist. \n\nIs there a reason why you are using the parameter `use_fast=False`\
    \ on the exception block and not on the initial try block? As the tokeniser would\
    \ load as expected removing `use_fast` from the except block where you are loading\
    \ the model from the `model_id`"
  created_at: 2023-12-12 15:04:39+00:00
  edited: false
  hidden: false
  id: 657876871bf8e371c4dfd405
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-12-14T12:27:36.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8126881718635559
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;nicklikets&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nicklikets\"\
          >@<span class=\"underline\">nicklikets</span></a></span>\n\n\t</span></span>,\
          \ <code>use_fast=True</code> works.</p>\n"
        raw: Thanks @nicklikets, `use_fast=True` works.
        updatedAt: '2023-12-14T12:27:36.180Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nicklikets
      relatedEventId: 657af4b88d360b690d3259fa
    id: 657af4b88d360b690d3259f8
    type: comment
  author: abhinavkulkarni
  content: Thanks @nicklikets, `use_fast=True` works.
  created_at: 2023-12-14 12:27:36+00:00
  edited: false
  hidden: false
  id: 657af4b88d360b690d3259f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-12-14T12:27:36.000Z'
    data:
      status: closed
    id: 657af4b88d360b690d3259fa
    type: status-change
  author: abhinavkulkarni
  created_at: 2023-12-14 12:27:36+00:00
  id: 657af4b88d360b690d3259fa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: togethercomputer/StripedHyena-Nous-7B
repo_type: model
status: closed
target_branch: null
title: Unable to load tokenizer
