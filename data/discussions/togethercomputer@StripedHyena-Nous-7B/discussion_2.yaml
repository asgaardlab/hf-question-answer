!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Maykeye
conflicting_files: null
created_at: 2023-12-09 11:05:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
      fullname: M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maykeye
      type: user
    createdAt: '2023-12-09T11:05:23.000Z'
    data:
      edited: true
      editors:
      - Maykeye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5147066712379456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
          fullname: M
          isHf: false
          isPro: false
          name: Maykeye
          type: user
        html: "<ul>\n<li>Flash depth wise:</li>\n</ul>\n<ol>\n<li>no import for FlashDepthwiseConv1d\
          \ (both in github repo and hf repo it mentioned only once - when instantiated)</li>\n\
          <li>I'm not sure what package is intended but   flashfftconv mentioned in\
          \ github repo has <code>FlashDepthWiseConv1d</code> (upper case W)</li>\n\
          <li>If I use <code>from flashfftconv import FlashDepthWiseConv1d as FlashDepthwiseConv1d</code>\
          \ and enable flash_depthwise in config I get warnings about unitialized\
          \ parameters:</li>\n</ol>\n<pre><code class=\"language-python\">In [<span\
          \ class=\"hljs-number\">1</span>]: model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\".\"</span>, device=<span class=\"hljs-string\"\
          >\"cuda\"</span>, dtype=torch.float16, load_in_4bit=<span class=\"hljs-literal\"\
          >True</span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\
          <span class=\"hljs-built_in\">bin</span> /home/fella/src/sd/sd/lib/python3<span\
          \ class=\"hljs-number\">.11</span>/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          Loading checkpoint shards: <span class=\"hljs-number\">100</span>%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| <span\
          \ class=\"hljs-number\">2</span>/<span class=\"hljs-number\">2</span> [<span\
          \ class=\"hljs-number\">00</span>:04&lt;<span class=\"hljs-number\">00</span>:<span\
          \ class=\"hljs-number\">00</span>,  <span class=\"hljs-number\">2.21</span>s/it]\n\
          Some weights of StripedHyenaModelForCausalLM were <span class=\"hljs-keyword\"\
          >not</span> initialized <span class=\"hljs-keyword\">from</span> the model\
          \ checkpoint at . <span class=\"hljs-keyword\">and</span> are newly initialized:\
          \ [<span class=\"hljs-string\">'backbone.blocks.2.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.24.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.16.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.8.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.10.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.14.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.10.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.16.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.4.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.20.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.18.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.0.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.18.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.28.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.26.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.14.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.8.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.12.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.26.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.0.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.22.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.24.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.4.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.6.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.12.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.20.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.30.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.6.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.30.filter.fir_fn.bias'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.28.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.22.filter.fir_fn.weights'</span>,\
          \ <span class=\"hljs-string\">'backbone.blocks.2.filter.fir_fn.weights'</span>]\n\
          </code></pre>\n<ul>\n<li>flash fft<br>It's marked as compatible in yml file,\
          \ but if I try to use it, model.py raises error</li>\n</ul>\n<pre><code\
          \ class=\"language-python\">        <span class=\"hljs-keyword\">if</span>\
          \ config.get(<span class=\"hljs-string\">\"use_flashfft\"</span>, <span\
          \ class=\"hljs-string\">\"False\"</span>):\n            <span class=\"hljs-keyword\"\
          >raise</span> NotImplementedError(<span class=\"hljs-string\">\"Please use\
          \ standalone SH code for other custom kernels\"</span>)\n</code></pre>\n\
          <p>(it is different on github though, but once again name mismatch: it uses\
          \ flash_fft.conv, while recently build flash-fft-conv uses flashfftconv\
          \ and github also uses config.seqlen which is None in hf)</p>\n"
        raw: "* Flash depth wise:\n\n1) no import for FlashDepthwiseConv1d (both in\
          \ github repo and hf repo it mentioned only once - when instantiated)\n\
          2) I'm not sure what package is intended but   flashfftconv mentioned in\
          \ github repo has `FlashDepthWiseConv1d` (upper case W)\n3) If I use `from\
          \ flashfftconv import FlashDepthWiseConv1d as FlashDepthwiseConv1d` and\
          \ enable flash_depthwise in config I get warnings about unitialized parameters:\
          \ \n```python\nIn [1]: model = AutoModelForCausalLM.from_pretrained(\".\"\
          , device=\"cuda\", dtype=torch.float16, load_in_4bit=True, trust_remote_code=True)\n\
          bin /home/fella/src/sd/sd/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.21s/it]\nSome weights\
          \ of StripedHyenaModelForCausalLM were not initialized from the model checkpoint\
          \ at . and are newly initialized: ['backbone.blocks.2.filter.fir_fn.bias',\
          \ 'backbone.blocks.24.filter.fir_fn.bias', 'backbone.blocks.16.filter.fir_fn.weights',\
          \ 'backbone.blocks.8.filter.fir_fn.bias', 'backbone.blocks.10.filter.fir_fn.weights',\
          \ 'backbone.blocks.14.filter.fir_fn.bias', 'backbone.blocks.10.filter.fir_fn.bias',\
          \ 'backbone.blocks.16.filter.fir_fn.bias', 'backbone.blocks.4.filter.fir_fn.weights',\
          \ 'backbone.blocks.20.filter.fir_fn.bias', 'backbone.blocks.18.filter.fir_fn.weights',\
          \ 'backbone.blocks.0.filter.fir_fn.bias', 'backbone.blocks.18.filter.fir_fn.bias',\
          \ 'backbone.blocks.28.filter.fir_fn.bias', 'backbone.blocks.26.filter.fir_fn.bias',\
          \ 'backbone.blocks.14.filter.fir_fn.weights', 'backbone.blocks.8.filter.fir_fn.weights',\
          \ 'backbone.blocks.12.filter.fir_fn.bias', 'backbone.blocks.26.filter.fir_fn.weights',\
          \ 'backbone.blocks.0.filter.fir_fn.weights', 'backbone.blocks.22.filter.fir_fn.bias',\
          \ 'backbone.blocks.24.filter.fir_fn.weights', 'backbone.blocks.4.filter.fir_fn.bias',\
          \ 'backbone.blocks.6.filter.fir_fn.bias', 'backbone.blocks.12.filter.fir_fn.weights',\
          \ 'backbone.blocks.20.filter.fir_fn.weights', 'backbone.blocks.30.filter.fir_fn.weights',\
          \ 'backbone.blocks.6.filter.fir_fn.weights', 'backbone.blocks.30.filter.fir_fn.bias',\
          \ 'backbone.blocks.28.filter.fir_fn.weights', 'backbone.blocks.22.filter.fir_fn.weights',\
          \ 'backbone.blocks.2.filter.fir_fn.weights']\n```\n\n* flash fft\nIt's marked\
          \ as compatible in yml file, but if I try to use it, model.py raises error\n\
          \n```python\n        if config.get(\"use_flashfft\", \"False\"):\n     \
          \       raise NotImplementedError(\"Please use standalone SH code for other\
          \ custom kernels\")\n```\n\n(it is different on github though, but once\
          \ again name mismatch: it uses flash_fft.conv, while recently build flash-fft-conv\
          \ uses flashfftconv and github also uses config.seqlen which is None in\
          \ hf)"
        updatedAt: '2023-12-09T11:15:00.760Z'
      numEdits: 1
      reactions: []
    id: 657449f3ec3bf96e43274787
    type: comment
  author: Maykeye
  content: "* Flash depth wise:\n\n1) no import for FlashDepthwiseConv1d (both in\
    \ github repo and hf repo it mentioned only once - when instantiated)\n2) I'm\
    \ not sure what package is intended but   flashfftconv mentioned in github repo\
    \ has `FlashDepthWiseConv1d` (upper case W)\n3) If I use `from flashfftconv import\
    \ FlashDepthWiseConv1d as FlashDepthwiseConv1d` and enable flash_depthwise in\
    \ config I get warnings about unitialized parameters: \n```python\nIn [1]: model\
    \ = AutoModelForCausalLM.from_pretrained(\".\", device=\"cuda\", dtype=torch.float16,\
    \ load_in_4bit=True, trust_remote_code=True)\nbin /home/fella/src/sd/sd/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,\
    \  2.21s/it]\nSome weights of StripedHyenaModelForCausalLM were not initialized\
    \ from the model checkpoint at . and are newly initialized: ['backbone.blocks.2.filter.fir_fn.bias',\
    \ 'backbone.blocks.24.filter.fir_fn.bias', 'backbone.blocks.16.filter.fir_fn.weights',\
    \ 'backbone.blocks.8.filter.fir_fn.bias', 'backbone.blocks.10.filter.fir_fn.weights',\
    \ 'backbone.blocks.14.filter.fir_fn.bias', 'backbone.blocks.10.filter.fir_fn.bias',\
    \ 'backbone.blocks.16.filter.fir_fn.bias', 'backbone.blocks.4.filter.fir_fn.weights',\
    \ 'backbone.blocks.20.filter.fir_fn.bias', 'backbone.blocks.18.filter.fir_fn.weights',\
    \ 'backbone.blocks.0.filter.fir_fn.bias', 'backbone.blocks.18.filter.fir_fn.bias',\
    \ 'backbone.blocks.28.filter.fir_fn.bias', 'backbone.blocks.26.filter.fir_fn.bias',\
    \ 'backbone.blocks.14.filter.fir_fn.weights', 'backbone.blocks.8.filter.fir_fn.weights',\
    \ 'backbone.blocks.12.filter.fir_fn.bias', 'backbone.blocks.26.filter.fir_fn.weights',\
    \ 'backbone.blocks.0.filter.fir_fn.weights', 'backbone.blocks.22.filter.fir_fn.bias',\
    \ 'backbone.blocks.24.filter.fir_fn.weights', 'backbone.blocks.4.filter.fir_fn.bias',\
    \ 'backbone.blocks.6.filter.fir_fn.bias', 'backbone.blocks.12.filter.fir_fn.weights',\
    \ 'backbone.blocks.20.filter.fir_fn.weights', 'backbone.blocks.30.filter.fir_fn.weights',\
    \ 'backbone.blocks.6.filter.fir_fn.weights', 'backbone.blocks.30.filter.fir_fn.bias',\
    \ 'backbone.blocks.28.filter.fir_fn.weights', 'backbone.blocks.22.filter.fir_fn.weights',\
    \ 'backbone.blocks.2.filter.fir_fn.weights']\n```\n\n* flash fft\nIt's marked\
    \ as compatible in yml file, but if I try to use it, model.py raises error\n\n\
    ```python\n        if config.get(\"use_flashfft\", \"False\"):\n            raise\
    \ NotImplementedError(\"Please use standalone SH code for other custom kernels\"\
    )\n```\n\n(it is different on github though, but once again name mismatch: it\
    \ uses flash_fft.conv, while recently build flash-fft-conv uses flashfftconv and\
    \ github also uses config.seqlen which is None in hf)"
  created_at: 2023-12-09 11:05:23+00:00
  edited: true
  hidden: false
  id: 657449f3ec3bf96e43274787
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a1306bbe7fa896d2c8de44/ndUBVtc5A3nsfUE47vood.png?w=200&h=200&f=face
      fullname: Michael Poli
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Zymrael
      type: user
    createdAt: '2023-12-11T00:59:19.000Z'
    data:
      edited: false
      editors:
      - Zymrael
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9273282885551453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a1306bbe7fa896d2c8de44/ndUBVtc5A3nsfUE47vood.png?w=200&h=200&f=face
          fullname: Michael Poli
          isHf: false
          isPro: false
          name: Zymrael
          type: user
        html: '<p>Additional optimizations will trickle in (including better support
          of custom kernels, quantization). Stay tuned :)</p>

          '
        raw: Additional optimizations will trickle in (including better support of
          custom kernels, quantization). Stay tuned :)
        updatedAt: '2023-12-11T00:59:19.764Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Maykeye
    id: 65765ee754d17496121a6ff1
    type: comment
  author: Zymrael
  content: Additional optimizations will trickle in (including better support of custom
    kernels, quantization). Stay tuned :)
  created_at: 2023-12-11 00:59:19+00:00
  edited: false
  hidden: false
  id: 65765ee754d17496121a6ff1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: togethercomputer/StripedHyena-Nous-7B
repo_type: model
status: open
target_branch: null
title: Optional dependencies on custom kernels don't work(Flash Depthwise, flash fft)
