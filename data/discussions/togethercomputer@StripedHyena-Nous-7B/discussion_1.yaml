!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-12-09 02:20:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-12-09T02:20:55.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4314500689506531
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<pre><code class=\"language-Python\">%%time\n!pip install git+https://github.com/huggingface/transformers\
          \ -qqq\n<span class=\"hljs-comment\"># trl</span>\n!pip install git+https://github.com/huggingface/trl\
          \ -qqq\n!pip install datasets peft accelerate safetensors --upgrade -qqq\n\
          !pip install ninja packaging --upgrade -qqq\n!pip install sentencepiece\
          \ bitsandbytes -qqq\n!pip install -U xformers deepspeed -qqq\n!pip install\
          \ attention_sinks -qqq\n!python -c <span class=\"hljs-string\">\"import\
          \ torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware\
          \ not supported for Flash Attention'\"</span>\n\n!export CUDA_HOME=/usr/local/cuda-<span\
          \ class=\"hljs-number\">11.8</span>\n!MAX_JOBS=<span class=\"hljs-number\"\
          >4</span> pip install flash-attn --no-build-isolation -qqq\n<span class=\"\
          hljs-comment\"># !pip install git+\"https://github.com/Dao-AILab/flash-attention.git\"\
          </span>\n</code></pre>\n<p>load model</p>\n<pre><code class=\"language-Python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n\
          \    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n\
          \    GenerationConfig,\n    TextIteratorStreamer,\n)\n<span class=\"hljs-comment\"\
          ># from attention_sinks import AutoModelForCausalLM</span>\n\n<span class=\"\
          hljs-keyword\">import</span> torch\n\nmodel_ID_1=<span class=\"hljs-string\"\
          >\"togethercomputer/StripedHyena-Nous-7B\"</span>\n\nmodel = AutoModelForCausalLM.from_pretrained(model_ID_1,\n\
          \                                             device_map=<span class=\"\
          hljs-string\">\"auto\"</span>,\n                                       \
          \      trust_remote_code=<span class=\"hljs-literal\">True</span>,\n   \
          \                                          torch_dtype=torch.bfloat16,\n\
          \                                             use_flash_attention_2=<span\
          \ class=\"hljs-literal\">True</span>, <span class=\"hljs-comment\"># True\
          \ , False</span>\n                                             low_cpu_mem_usage=\
          \ <span class=\"hljs-literal\">True</span>,\n                          \
          \                   )\n\n\n\nmax_length=<span class=\"hljs-number\">2048</span>\
          \ <span class=\"hljs-comment\">#get_max_length()</span>\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"max_length\"\
          </span>,max_length)\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_ID_1,\n\
          \                                          <span class=\"hljs-comment\"\
          ># use_fast = False, # True False</span>\n                             \
          \             max_length=max_length,)\n\n\n</code></pre>\n<p>Error:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/pviHYMQCI5yQl4N9urSf_.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/pviHYMQCI5yQl4N9urSf_.png\"\
          ></a></p>\n"
        raw: "```Python\r\n%%time\r\n!pip install git+https://github.com/huggingface/transformers\
          \ -qqq\r\n# trl\r\n!pip install git+https://github.com/huggingface/trl -qqq\r\
          \n!pip install datasets peft accelerate safetensors --upgrade -qqq\r\n!pip\
          \ install ninja packaging --upgrade -qqq\r\n!pip install sentencepiece bitsandbytes\
          \ -qqq\r\n!pip install -U xformers deepspeed -qqq\r\n!pip install attention_sinks\
          \ -qqq\r\n!python -c \"import torch; assert torch.cuda.get_device_capability()[0]\
          \ >= 8, 'Hardware not supported for Flash Attention'\"\r\n\r\n!export CUDA_HOME=/usr/local/cuda-11.8\r\
          \n!MAX_JOBS=4 pip install flash-attn --no-build-isolation -qqq\r\n# !pip\
          \ install git+\"https://github.com/Dao-AILab/flash-attention.git\"\r\n```\r\
          \n\r\nload model\r\n```Python\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\
          \n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\
          \n    TrainingArguments,\r\n    pipeline,\r\n    logging,\r\n    GenerationConfig,\r\
          \n    TextIteratorStreamer,\r\n)\r\n# from attention_sinks import AutoModelForCausalLM\r\
          \n\r\nimport torch\r\n\r\nmodel_ID_1=\"togethercomputer/StripedHyena-Nous-7B\"\
          \r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_ID_1,\r\n   \
          \                                          device_map=\"auto\",\r\n    \
          \                                         trust_remote_code=True,\r\n  \
          \                                           torch_dtype=torch.bfloat16,\r\
          \n                                             use_flash_attention_2=True,\
          \ # True , False\r\n                                             low_cpu_mem_usage=\
          \ True,\r\n                                             )\r\n\r\n\r\n\r\n\
          max_length=2048 #get_max_length()\r\nprint(\"max_length\",max_length)\r\n\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_ID_1,\r\n      \
          \                                    # use_fast = False, # True False\r\n\
          \                                          max_length=max_length,)\r\n\r\
          \n\r\n\r\n```\r\n\r\n\r\nError:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/pviHYMQCI5yQl4N9urSf_.png)\r\
          \n"
        updatedAt: '2023-12-09T02:20:55.049Z'
      numEdits: 0
      reactions: []
    id: 6573cf076da136b50f640e6b
    type: comment
  author: NickyNicky
  content: "```Python\r\n%%time\r\n!pip install git+https://github.com/huggingface/transformers\
    \ -qqq\r\n# trl\r\n!pip install git+https://github.com/huggingface/trl -qqq\r\n\
    !pip install datasets peft accelerate safetensors --upgrade -qqq\r\n!pip install\
    \ ninja packaging --upgrade -qqq\r\n!pip install sentencepiece bitsandbytes -qqq\r\
    \n!pip install -U xformers deepspeed -qqq\r\n!pip install attention_sinks -qqq\r\
    \n!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8,\
    \ 'Hardware not supported for Flash Attention'\"\r\n\r\n!export CUDA_HOME=/usr/local/cuda-11.8\r\
    \n!MAX_JOBS=4 pip install flash-attn --no-build-isolation -qqq\r\n# !pip install\
    \ git+\"https://github.com/Dao-AILab/flash-attention.git\"\r\n```\r\n\r\nload\
    \ model\r\n```Python\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\
    \n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n \
    \   TrainingArguments,\r\n    pipeline,\r\n    logging,\r\n    GenerationConfig,\r\
    \n    TextIteratorStreamer,\r\n)\r\n# from attention_sinks import AutoModelForCausalLM\r\
    \n\r\nimport torch\r\n\r\nmodel_ID_1=\"togethercomputer/StripedHyena-Nous-7B\"\
    \r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_ID_1,\r\n         \
    \                                    device_map=\"auto\",\r\n                \
    \                             trust_remote_code=True,\r\n                    \
    \                         torch_dtype=torch.bfloat16,\r\n                    \
    \                         use_flash_attention_2=True, # True , False\r\n     \
    \                                        low_cpu_mem_usage= True,\r\n        \
    \                                     )\r\n\r\n\r\n\r\nmax_length=2048 #get_max_length()\r\
    \nprint(\"max_length\",max_length)\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_ID_1,\r\
    \n                                          # use_fast = False, # True False\r\
    \n                                          max_length=max_length,)\r\n\r\n\r\n\
    \r\n```\r\n\r\n\r\nError:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/pviHYMQCI5yQl4N9urSf_.png)\r\
    \n"
  created_at: 2023-12-09 02:20:55+00:00
  edited: false
  hidden: false
  id: 6573cf076da136b50f640e6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-12-09T02:36:34.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40709125995635986
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<pre><code class="language-Python">!pip install update setuptools wheel

          !pip install git+<span class="hljs-string">"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm"</span>

          </code></pre>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/yHVYhL1UlrE3xiqOhUp0e.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/yHVYhL1UlrE3xiqOhUp0e.png"></a></p>

          '
        raw: '```Python

          !pip install update setuptools wheel

          !pip install git+"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm"

          ```


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/yHVYhL1UlrE3xiqOhUp0e.png)

          '
        updatedAt: '2023-12-09T02:36:34.082Z'
      numEdits: 0
      reactions: []
    id: 6573d2b288805b3ba1b87d99
    type: comment
  author: NickyNicky
  content: '```Python

    !pip install update setuptools wheel

    !pip install git+"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm"

    ```


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/yHVYhL1UlrE3xiqOhUp0e.png)

    '
  created_at: 2023-12-09 02:36:34+00:00
  edited: false
  hidden: false
  id: 6573d2b288805b3ba1b87d99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a1306bbe7fa896d2c8de44/ndUBVtc5A3nsfUE47vood.png?w=200&h=200&f=face
      fullname: Michael Poli
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Zymrael
      type: user
    createdAt: '2023-12-11T00:58:32.000Z'
    data:
      edited: false
      editors:
      - Zymrael
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195476770401001
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a1306bbe7fa896d2c8de44/ndUBVtc5A3nsfUE47vood.png?w=200&h=200&f=face
          fullname: Michael Poli
          isHf: false
          isPro: false
          name: Zymrael
          type: user
        html: '<p>Can you share your environment? I repeated your steps and loaded
          the model correctly without any issues</p>

          '
        raw: Can you share your environment? I repeated your steps and loaded the
          model correctly without any issues
        updatedAt: '2023-12-11T00:58:32.194Z'
      numEdits: 0
      reactions: []
    id: 65765eb8c13a2c74787a0617
    type: comment
  author: Zymrael
  content: Can you share your environment? I repeated your steps and loaded the model
    correctly without any issues
  created_at: 2023-12-11 00:58:32+00:00
  edited: false
  hidden: false
  id: 65765eb8c13a2c74787a0617
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: togethercomputer/StripedHyena-Nous-7B
repo_type: model
status: open
target_branch: null
title: Error load Model
