!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-07-07 15:23:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T16:23:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9356228709220886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hi guys</p>

          <p>Congratulations on the new model! Always great to see a new WizardLM
          release.  </p>

          <p>I am doing my quantisations at:</p>

          <p><a href="https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ">https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ</a><br><a
          href="https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML">https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML</a></p>

          <p>I just wanted to let you know that because you''ve increased the vocab
          size to 32001, this breaks compatibility with the latest GGML quantisation
          methods, called k-quant.</p>

          <p>This may be resolved some time in the future, but for now it means I
          can only release the older GGML quantisation formats, which will disappoint
          a lot of GGML users.</p>

          <p>You can read about the issue with k-quants here: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/1919">https://github.com/ggerganov/llama.cpp/issues/1919</a></p>

          <p>My understanding is that that extra 32001th token, the PAD, was added
          as something of a hack very early in the history of Llama open source models.
          It was a hack used by one particular model creator I think only because
          they forgot to set up special_tokens_map.json correctly :) Since then it''s
          stuck around, being copied from model to model, despite not being needed.
          </p>

          <p>I''m starting a campaign to try and get it stopped, because it causes
          a lot of problems for developers outside the world of Python inference.</p>

          <p>Just thought I''d let you know for your next model. It would be great
          if you could update your training code so it doesn''t add this extra token,
          which I believe is not needed.</p>

          <p>Thanks</p>

          <p>Tom / TheBloke</p>

          '
        raw: "Hi guys\r\n\r\nCongratulations on the new model! Always great to see\
          \ a new WizardLM release.  \r\n\r\nI am doing my quantisations at:\r\n\r\
          \nhttps://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ\r\nhttps://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML\r\
          \n\r\nI just wanted to let you know that because you've increased the vocab\
          \ size to 32001, this breaks compatibility with the latest GGML quantisation\
          \ methods, called k-quant.\r\n\r\nThis may be resolved some time in the\
          \ future, but for now it means I can only release the older GGML quantisation\
          \ formats, which will disappoint a lot of GGML users.\r\n\r\nYou can read\
          \ about the issue with k-quants here: https://github.com/ggerganov/llama.cpp/issues/1919\r\
          \n\r\nMy understanding is that that extra 32001th token, the PAD, was added\
          \ as something of a hack very early in the history of Llama open source\
          \ models. It was a hack used by one particular model creator I think only\
          \ because they forgot to set up special_tokens_map.json correctly :) Since\
          \ then it's stuck around, being copied from model to model, despite not\
          \ being needed. \r\n\r\nI'm starting a campaign to try and get it stopped,\
          \ because it causes a lot of problems for developers outside the world of\
          \ Python inference.\r\n\r\nJust thought I'd let you know for your next model.\
          \ It would be great if you could update your training code so it doesn't\
          \ add this extra token, which I believe is not needed.\r\n\r\nThanks\r\n\
          \r\nTom / TheBloke"
        updatedAt: '2023-07-07T16:23:36.003Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - Neman
        - nacs
        - Esmeetu
        - yahma
        - phi0112358
        - Kunal0137
    id: 64a83c08eb47b3552283f759
    type: comment
  author: TheBloke
  content: "Hi guys\r\n\r\nCongratulations on the new model! Always great to see a\
    \ new WizardLM release.  \r\n\r\nI am doing my quantisations at:\r\n\r\nhttps://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ\r\
    \nhttps://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML\r\n\r\nI just wanted\
    \ to let you know that because you've increased the vocab size to 32001, this\
    \ breaks compatibility with the latest GGML quantisation methods, called k-quant.\r\
    \n\r\nThis may be resolved some time in the future, but for now it means I can\
    \ only release the older GGML quantisation formats, which will disappoint a lot\
    \ of GGML users.\r\n\r\nYou can read about the issue with k-quants here: https://github.com/ggerganov/llama.cpp/issues/1919\r\
    \n\r\nMy understanding is that that extra 32001th token, the PAD, was added as\
    \ something of a hack very early in the history of Llama open source models. It\
    \ was a hack used by one particular model creator I think only because they forgot\
    \ to set up special_tokens_map.json correctly :) Since then it's stuck around,\
    \ being copied from model to model, despite not being needed. \r\n\r\nI'm starting\
    \ a campaign to try and get it stopped, because it causes a lot of problems for\
    \ developers outside the world of Python inference.\r\n\r\nJust thought I'd let\
    \ you know for your next model. It would be great if you could update your training\
    \ code so it doesn't add this extra token, which I believe is not needed.\r\n\r\
    \nThanks\r\n\r\nTom / TheBloke"
  created_at: 2023-07-07 15:23:36+00:00
  edited: false
  hidden: false
  id: 64a83c08eb47b3552283f759
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
      fullname: Ichigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichigo2899
      type: user
    createdAt: '2023-07-07T17:17:31.000Z'
    data:
      edited: false
      editors:
      - Ichigo2899
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9230676293373108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
          fullname: Ichigo
          isHf: false
          isPro: false
          name: Ichigo2899
          type: user
        html: '<p>Exactly what Bloke said. Its not possible to inference with  a production
          ready inference server such as Huggingface text generation inferencer as
          you cannot shard 32001 equally. Would be great if it can be set to 32000
          just like the V1.0 was.</p>

          <p>Thank you!</p>

          '
        raw: 'Exactly what Bloke said. Its not possible to inference with  a production
          ready inference server such as Huggingface text generation inferencer as
          you cannot shard 32001 equally. Would be great if it can be set to 32000
          just like the V1.0 was.


          Thank you!'
        updatedAt: '2023-07-07T17:17:31.375Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Neman
        - Kunal0137
    id: 64a848aba347b9571974fd48
    type: comment
  author: Ichigo2899
  content: 'Exactly what Bloke said. Its not possible to inference with  a production
    ready inference server such as Huggingface text generation inferencer as you cannot
    shard 32001 equally. Would be great if it can be set to 32000 just like the V1.0
    was.


    Thank you!'
  created_at: 2023-07-07 16:17:31+00:00
  edited: false
  hidden: false
  id: 64a848aba347b9571974fd48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T18:39:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674510359764099
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh yeah I forgot that it affects Text Generation Inference as well.  So
          it''s not only GGML, but also some Python libraries.</p>

          '
        raw: Oh yeah I forgot that it affects Text Generation Inference as well.  So
          it's not only GGML, but also some Python libraries.
        updatedAt: '2023-07-07T18:39:45.870Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Neman
        - Kunal0137
    id: 64a85bf18cac7b871a352111
    type: comment
  author: TheBloke
  content: Oh yeah I forgot that it affects Text Generation Inference as well.  So
    it's not only GGML, but also some Python libraries.
  created_at: 2023-07-07 17:39:45+00:00
  edited: false
  hidden: false
  id: 64a85bf18cac7b871a352111
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: WizardLM/WizardLM-13B-V1.1
repo_type: model
status: open
target_branch: null
title: Thank you! I will do quants. Please be aware, vocab size 32,001 causes problems
  for non-Python inference
