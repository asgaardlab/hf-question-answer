!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RajeshkumarV
conflicting_files: null
created_at: 2023-09-04 15:02:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-04T16:02:10.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5256619453430176
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>python main.py "What is the x number?"<br>ggml_allocr_alloc: not
          enough space in the buffer (needed 178227200, largest block available 19333120)<br>GGML_ASSERT:
          C:\Users\rajesh\AppData\Local\Temp\pip-install-0ohg_aj6\llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\vendor\llama.cpp\ggml-alloc.c:144:
          !"not enough space in the buffer"</p>

          '
        raw: "python main.py \"What is the x number?\"\r\nggml_allocr_alloc: not enough\
          \ space in the buffer (needed 178227200, largest block available 19333120)\r\
          \nGGML_ASSERT: C:\\Users\\rajesh\\AppData\\Local\\Temp\\pip-install-0ohg_aj6\\\
          llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\\vendor\\llama.cpp\\ggml-alloc.c:144:\
          \ !\"not enough space in the buffer\""
        updatedAt: '2023-09-04T16:02:10.678Z'
      numEdits: 0
      reactions: []
    id: 64f5ff82810bba5ddf5c8fd1
    type: comment
  author: RajeshkumarV
  content: "python main.py \"What is the x number?\"\r\nggml_allocr_alloc: not enough\
    \ space in the buffer (needed 178227200, largest block available 19333120)\r\n\
    GGML_ASSERT: C:\\Users\\rajesh\\AppData\\Local\\Temp\\pip-install-0ohg_aj6\\llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\\\
    vendor\\llama.cpp\\ggml-alloc.c:144: !\"not enough space in the buffer\""
  created_at: 2023-09-04 15:02:10+00:00
  edited: false
  hidden: false
  id: 64f5ff82810bba5ddf5c8fd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-09-05T04:04:24.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.96308833360672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Try rebooting your PC - your memory seems highly fragmented (or
          hopelessly filled up)</p>

          '
        raw: Try rebooting your PC - your memory seems highly fragmented (or hopelessly
          filled up)
        updatedAt: '2023-09-05T04:04:24.717Z'
      numEdits: 0
      reactions: []
    id: 64f6a8c83a14cc4dd8bb17ad
    type: comment
  author: rozek
  content: Try rebooting your PC - your memory seems highly fragmented (or hopelessly
    filled up)
  created_at: 2023-09-05 03:04:24+00:00
  edited: false
  hidden: false
  id: 64f6a8c83a14cc4dd8bb17ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-05T04:05:50.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9981057643890381
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>I have tried to reboot, but it didn''t work unfortunately :(</p>

          '
        raw: I have tried to reboot, but it didn't work unfortunately :(
        updatedAt: '2023-09-05T04:05:50.064Z'
      numEdits: 0
      reactions: []
    id: 64f6a91e3a14cc4dd8bb1ea1
    type: comment
  author: RajeshkumarV
  content: I have tried to reboot, but it didn't work unfortunately :(
  created_at: 2023-09-05 03:05:50+00:00
  edited: false
  hidden: false
  id: 64f6a91e3a14cc4dd8bb1ea1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-09-05T04:59:55.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9400834441184998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>unfortunately, I don''t have any experience with the Python version
          of llama.cpp - I''m using the original C++ variant only, and that has been
          proven to work. Can you try the original llama.cpp instead?</p>

          '
        raw: unfortunately, I don't have any experience with the Python version of
          llama.cpp - I'm using the original C++ variant only, and that has been proven
          to work. Can you try the original llama.cpp instead?
        updatedAt: '2023-09-05T04:59:55.813Z'
      numEdits: 0
      reactions: []
    id: 64f6b5cb46284aa28d962f48
    type: comment
  author: rozek
  content: unfortunately, I don't have any experience with the Python version of llama.cpp
    - I'm using the original C++ variant only, and that has been proven to work. Can
    you try the original llama.cpp instead?
  created_at: 2023-09-05 03:59:55+00:00
  edited: false
  hidden: false
  id: 64f6b5cb46284aa28d962f48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-05T05:05:20.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7351973652839661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>I am not sure how will that work.  I am using this code example
          in my Windows 11 PC. <a rel="nofollow" href="https://github.com/singlestore-labs/webinar-code-examples/tree/main/llama-2-local">https://github.com/singlestore-labs/webinar-code-examples/tree/main/llama-2-local</a></p>

          '
        raw: I am not sure how will that work.  I am using this code example in my
          Windows 11 PC. https://github.com/singlestore-labs/webinar-code-examples/tree/main/llama-2-local
        updatedAt: '2023-09-05T05:05:20.134Z'
      numEdits: 0
      reactions: []
    id: 64f6b71010a91217c380c590
    type: comment
  author: RajeshkumarV
  content: I am not sure how will that work.  I am using this code example in my Windows
    11 PC. https://github.com/singlestore-labs/webinar-code-examples/tree/main/llama-2-local
  created_at: 2023-09-05 04:05:20+00:00
  edited: false
  hidden: false
  id: 64f6b71010a91217c380c590
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-05T05:08:32.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6960237622261047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>also, i am not sure why i am getting the GGML error, when i am using
          the GGUF version of model pls</p>

          '
        raw: also, i am not sure why i am getting the GGML error, when i am using
          the GGUF version of model pls
        updatedAt: '2023-09-05T05:08:32.881Z'
      numEdits: 0
      reactions: []
    id: 64f6b7d0fb9d71a23a2c8879
    type: comment
  author: RajeshkumarV
  content: also, i am not sure why i am getting the GGML error, when i am using the
    GGUF version of model pls
  created_at: 2023-09-05 04:08:32+00:00
  edited: false
  hidden: false
  id: 64f6b7d0fb9d71a23a2c8879
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-09-05T07:56:08.000Z'
    data:
      edited: true
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9273523092269897
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Oh, don''t use that - its far too old and hopelessly outdated. You
          should definitely use the original llama.cpp for GGUF and/or large contexts!!!!</p>

          '
        raw: Oh, don't use that - its far too old and hopelessly outdated. You should
          definitely use the original llama.cpp for GGUF and/or large contexts!!!!
        updatedAt: '2023-09-05T07:56:44.511Z'
      numEdits: 1
      reactions: []
    id: 64f6df18b6365b4e44ea4ef0
    type: comment
  author: rozek
  content: Oh, don't use that - its far too old and hopelessly outdated. You should
    definitely use the original llama.cpp for GGUF and/or large contexts!!!!
  created_at: 2023-09-05 06:56:08+00:00
  edited: true
  hidden: false
  id: 64f6df18b6365b4e44ea4ef0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-05T08:07:53.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8180288076400757
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>But since the code is in python shouldn''t i use the llamacpp python
          package, instead of the llama.cpp package?</p>

          '
        raw: But since the code is in python shouldn't i use the llamacpp python package,
          instead of the llama.cpp package?
        updatedAt: '2023-09-05T08:07:53.091Z'
      numEdits: 0
      reactions: []
    id: 64f6e1d9b6365b4e44eaa693
    type: comment
  author: RajeshkumarV
  content: But since the code is in python shouldn't i use the llamacpp python package,
    instead of the llama.cpp package?
  created_at: 2023-09-05 07:07:53+00:00
  edited: false
  hidden: false
  id: 64f6e1d9b6365b4e44eaa693
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-09-05T08:49:27.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9865829348564148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>no, llama.cpp has been written in C++, as the name implies</p>

          '
        raw: no, llama.cpp has been written in C++, as the name implies
        updatedAt: '2023-09-05T08:49:27.473Z'
      numEdits: 0
      reactions: []
    id: 64f6eb970b49f19f4e8b51c4
    type: comment
  author: rozek
  content: no, llama.cpp has been written in C++, as the name implies
  created_at: 2023-09-05 07:49:27+00:00
  edited: false
  hidden: false
  id: 64f6eb970b49f19f4e8b51c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-05T10:01:00.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.898544430732727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>Yes, but this version c++ version was written to run in Mac/linux
          environment. for windows it will require the python llamacpp package. <a
          rel="nofollow" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a></p>

          '
        raw: Yes, but this version c++ version was written to run in Mac/linux environment.
          for windows it will require the python llamacpp package. https://github.com/abetlen/llama-cpp-python
        updatedAt: '2023-09-05T10:01:00.072Z'
      numEdits: 0
      reactions: []
    id: 64f6fc5cceb68df8ee3241f1
    type: comment
  author: RajeshkumarV
  content: Yes, but this version c++ version was written to run in Mac/linux environment.
    for windows it will require the python llamacpp package. https://github.com/abetlen/llama-cpp-python
  created_at: 2023-09-05 09:01:00+00:00
  edited: false
  hidden: false
  id: 64f6fc5cceb68df8ee3241f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-09-05T10:19:27.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9567524194717407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Even then you should use the newest version available, not older
          than approx. 2 days - that''s important because GGUF support is still in
          the making.</p>

          <p>And, since I use Macs only, I can''t help you with Windows-specific problems
          - I''m sorry</p>

          '
        raw: 'Even then you should use the newest version available, not older than
          approx. 2 days - that''s important because GGUF support is still in the
          making.


          And, since I use Macs only, I can''t help you with Windows-specific problems
          - I''m sorry'
        updatedAt: '2023-09-05T10:19:27.503Z'
      numEdits: 0
      reactions: []
    id: 64f700afc9ed14453ea84fba
    type: comment
  author: rozek
  content: 'Even then you should use the newest version available, not older than
    approx. 2 days - that''s important because GGUF support is still in the making.


    And, since I use Macs only, I can''t help you with Windows-specific problems -
    I''m sorry'
  created_at: 2023-09-05 09:19:27+00:00
  edited: false
  hidden: false
  id: 64f700afc9ed14453ea84fba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: rozek/LLaMA-2-7B-32K-Instruct_GGUF
repo_type: model
status: open
target_branch: null
title: 'Error when trying to ask a question - ggml_allocr_alloc: not enough space
  in the buffer (needed 178227200, largest block available 19333120)'
