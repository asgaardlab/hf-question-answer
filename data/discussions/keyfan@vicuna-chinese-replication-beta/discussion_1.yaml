!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lancercat
conflicting_files: null
created_at: 2023-04-27 11:42:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eab19e17e4caf74d7721be632183037a.svg
      fullname: cat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lancercat
      type: user
    createdAt: '2023-04-27T12:42:30.000Z'
    data:
      edited: true
      editors:
      - lancercat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eab19e17e4caf74d7721be632183037a.svg
          fullname: cat
          isHf: false
          isPro: false
          name: lancercat
          type: user
        html: "<pre><code>import torch\nfrom accelerate import dispatch_model,infer_auto_device_map,load_checkpoint_and_dispatch\n\
          from accelerate import init_empty_weights\nfrom transformers import AutoConfig,\
          \ AutoModelForCausalLM\nfrom transformers import (\n    AutoTokenizer,\n\
          \    AutoModelForCausalLM,\n    StoppingCriteriaList,\n    StoppingCriteria,\n\
          )\n\nclass StoppingCriteriaSub(StoppingCriteria):\n    '''Checks if the\
          \ last n tokens in the input_ids list match the stops list.'''\n    def\
          \ __init__(self, stops = []):\n        super().__init__()\n        self.stops\
          \ = stops\n\n    def __call__(self, input_ids, scores):\n        id_list\
          \ = input_ids[0].tolist()\n        return id_list[-len(self.stops):] ==\
          \ self.stops\n\n\nclass model:\n    def __init__(this):\n        config\
          \ = AutoConfig.from_pretrained(\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          )\n\n        with init_empty_weights():\n            model = AutoModelForCausalLM.from_config(config)\n\
          \        model.tie_weights();\n        this.model = load_checkpoint_and_dispatch(\n\
          \            model, \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          ,\n            device_map=infer_auto_device_map(\n                model,\
          \ {\"cuda:0\": \"14Gib\", \"cuda:1\": \"20Gib\"}, dtype=torch.float16),\n\
          \            no_split_module_classes=[\"LlamaAttention\"], dtype=torch.float16,\n\
          \        )\n        this.llama_tokenizer = AutoTokenizer.from_pretrained(\n\
          \            \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          )\n        this.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=[2277,\
          \ 29937])])  # stop at ###\n\n        # the template is based on Vicuna\
          \ template question and ChatGPT's answer to it. It probably can be better\
          \ tuned.\n\n\n    def generate_llama(this,text,template, max_new_tokens=256):\n\
          \        '''Generate result using llama model'''\n        context = template.format(text)\n\
          \        input_ids = this.llama_tokenizer(context, return_tensors=\"pt\"\
          ).input_ids.to(this.model.device)\n        output_ids = this.model.generate(input_ids,\
          \ do_sample=True, top_p=0.8, stopping_criteria=this.stopping_criteria,\n\
          \                                    max_new_tokens=max_new_tokens)\n  \
          \      decode_string = this.llama_tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\
          \        return decode_string.replace(context, \"\").replace(\"###\", \"\
          \").strip(\"\\n\")\n\nif __name__ == \"__main__\":\n    mod=model();\n\n\
          \    #torch.save(model,\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/mess.pt\"\
          )\n    print(mod.generate_llama(\"blah\",template = (INSER_YOUR_TEMPLATE)));\n\
          </code></pre>\n<p>Edit: My use case is to translate my Chinese to proper\
          \ Chinese, so it was called a translator.</p>\n"
        raw: "```\nimport torch\nfrom accelerate import dispatch_model,infer_auto_device_map,load_checkpoint_and_dispatch\n\
          from accelerate import init_empty_weights\nfrom transformers import AutoConfig,\
          \ AutoModelForCausalLM\nfrom transformers import (\n    AutoTokenizer,\n\
          \    AutoModelForCausalLM,\n    StoppingCriteriaList,\n    StoppingCriteria,\n\
          )\n\nclass StoppingCriteriaSub(StoppingCriteria):\n    '''Checks if the\
          \ last n tokens in the input_ids list match the stops list.'''\n    def\
          \ __init__(self, stops = []):\n        super().__init__()\n        self.stops\
          \ = stops\n\n    def __call__(self, input_ids, scores):\n        id_list\
          \ = input_ids[0].tolist()\n        return id_list[-len(self.stops):] ==\
          \ self.stops\n\n\nclass model:\n    def __init__(this):\n        config\
          \ = AutoConfig.from_pretrained(\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          )\n\n        with init_empty_weights():\n            model = AutoModelForCausalLM.from_config(config)\n\
          \        model.tie_weights();\n        this.model = load_checkpoint_and_dispatch(\n\
          \            model, \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          ,\n            device_map=infer_auto_device_map(\n                model,\
          \ {\"cuda:0\": \"14Gib\", \"cuda:1\": \"20Gib\"}, dtype=torch.float16),\n\
          \            no_split_module_classes=[\"LlamaAttention\"], dtype=torch.float16,\n\
          \        )\n        this.llama_tokenizer = AutoTokenizer.from_pretrained(\n\
          \            \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
          )\n        this.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=[2277,\
          \ 29937])])  # stop at ###\n\n        # the template is based on Vicuna\
          \ template question and ChatGPT's answer to it. It probably can be better\
          \ tuned.\n\n\n    def generate_llama(this,text,template, max_new_tokens=256):\n\
          \        '''Generate result using llama model'''\n        context = template.format(text)\n\
          \        input_ids = this.llama_tokenizer(context, return_tensors=\"pt\"\
          ).input_ids.to(this.model.device)\n        output_ids = this.model.generate(input_ids,\
          \ do_sample=True, top_p=0.8, stopping_criteria=this.stopping_criteria,\n\
          \                                    max_new_tokens=max_new_tokens)\n  \
          \      decode_string = this.llama_tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\
          \        return decode_string.replace(context, \"\").replace(\"###\", \"\
          \").strip(\"\\n\")\n\nif __name__ == \"__main__\":\n    mod=model();\n\n\
          \    #torch.save(model,\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/mess.pt\"\
          )\n    print(mod.generate_llama(\"blah\",template = (INSER_YOUR_TEMPLATE)));\n\
          \n```\nEdit: My use case is to translate my Chinese to proper Chinese, so\
          \ it was called a translator."
        updatedAt: '2023-04-27T12:45:02.713Z'
      numEdits: 1
      reactions: []
    id: 644a6db6e7d95a46f9446ad1
    type: comment
  author: lancercat
  content: "```\nimport torch\nfrom accelerate import dispatch_model,infer_auto_device_map,load_checkpoint_and_dispatch\n\
    from accelerate import init_empty_weights\nfrom transformers import AutoConfig,\
    \ AutoModelForCausalLM\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n\
    \    StoppingCriteriaList,\n    StoppingCriteria,\n)\n\nclass StoppingCriteriaSub(StoppingCriteria):\n\
    \    '''Checks if the last n tokens in the input_ids list match the stops list.'''\n\
    \    def __init__(self, stops = []):\n        super().__init__()\n        self.stops\
    \ = stops\n\n    def __call__(self, input_ids, scores):\n        id_list = input_ids[0].tolist()\n\
    \        return id_list[-len(self.stops):] == self.stops\n\n\nclass model:\n \
    \   def __init__(this):\n        config = AutoConfig.from_pretrained(\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
    )\n\n        with init_empty_weights():\n            model = AutoModelForCausalLM.from_config(config)\n\
    \        model.tie_weights();\n        this.model = load_checkpoint_and_dispatch(\n\
    \            model, \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
    ,\n            device_map=infer_auto_device_map(\n                model, {\"cuda:0\"\
    : \"14Gib\", \"cuda:1\": \"20Gib\"}, dtype=torch.float16),\n            no_split_module_classes=[\"\
    LlamaAttention\"], dtype=torch.float16,\n        )\n        this.llama_tokenizer\
    \ = AutoTokenizer.from_pretrained(\n            \"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/\"\
    )\n        this.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=[2277,\
    \ 29937])])  # stop at ###\n\n        # the template is based on Vicuna template\
    \ question and ChatGPT's answer to it. It probably can be better tuned.\n\n\n\
    \    def generate_llama(this,text,template, max_new_tokens=256):\n        '''Generate\
    \ result using llama model'''\n        context = template.format(text)\n     \
    \   input_ids = this.llama_tokenizer(context, return_tensors=\"pt\").input_ids.to(this.model.device)\n\
    \        output_ids = this.model.generate(input_ids, do_sample=True, top_p=0.8,\
    \ stopping_criteria=this.stopping_criteria,\n                                \
    \    max_new_tokens=max_new_tokens)\n        decode_string = this.llama_tokenizer.batch_decode(output_ids,\
    \ skip_special_tokens=True)[0]\n        return decode_string.replace(context,\
    \ \"\").replace(\"###\", \"\").strip(\"\\n\")\n\nif __name__ == \"__main__\":\n\
    \    mod=model();\n\n    #torch.save(model,\"/run/media/xxx/modelzoo/cllm/vicuna-chinese-replication-beta/mess.pt\"\
    )\n    print(mod.generate_llama(\"blah\",template = (INSER_YOUR_TEMPLATE)));\n\
    \n```\nEdit: My use case is to translate my Chinese to proper Chinese, so it was\
    \ called a translator."
  created_at: 2023-04-27 11:42:30+00:00
  edited: true
  hidden: false
  id: 644a6db6e7d95a46f9446ad1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: keyfan/vicuna-chinese-replication-beta
repo_type: model
status: open
target_branch: null
title: A trick to run it on a hoard of smaller GPUs
