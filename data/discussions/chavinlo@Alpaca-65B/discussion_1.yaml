!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 9cento
conflicting_files: null
created_at: 2023-03-26 06:45:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-03-26T07:45:06.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<p>Just curious, does it produce better output than your Alpaca-13B
          Native 4-bit?</p>

          '
        raw: Just curious, does it produce better output than your Alpaca-13B Native
          4-bit?
        updatedAt: '2023-03-26T07:45:06.422Z'
      numEdits: 0
      reactions: []
    id: 641ff802290342c5df80fefa
    type: comment
  author: 9cento
  content: Just curious, does it produce better output than your Alpaca-13B Native
    4-bit?
  created_at: 2023-03-26 06:45:06+00:00
  edited: false
  hidden: false
  id: 641ff802290342c5df80fefa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-03-26T13:30:53.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>I believe this is a lora</p>

          '
        raw: I believe this is a lora
        updatedAt: '2023-03-26T13:30:53.475Z'
      numEdits: 0
      reactions: []
    id: 6420490d9858e08906021dd0
    type: comment
  author: teknium
  content: I believe this is a lora
  created_at: 2023-03-26 12:30:53+00:00
  edited: false
  hidden: false
  id: 6420490d9858e08906021dd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-03-27T18:05:48.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>is it quantized? 4-bit? 8-bit?</p>

          '
        raw: is it quantized? 4-bit? 8-bit?
        updatedAt: '2023-03-27T18:05:48.050Z'
      numEdits: 0
      reactions: []
    id: 6421dafcec91bf6c97945187
    type: comment
  author: ehartford
  content: is it quantized? 4-bit? 8-bit?
  created_at: 2023-03-27 17:05:48+00:00
  edited: false
  hidden: false
  id: 6421dafcec91bf6c97945187
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b57adb50e6a1dafa786b7bc0a81ac29.svg
      fullname: Yoshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yoshiii
      type: user
    createdAt: '2023-03-27T23:00:33.000Z'
    data:
      edited: true
      editors:
      - Yoshiii
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b57adb50e6a1dafa786b7bc0a81ac29.svg
          fullname: Yoshi
          isHf: false
          isPro: false
          name: Yoshiii
          type: user
        html: '<p>just lookkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk,.<br>its
          a rank 8 lora</p>

          <blockquote>

          <p>your Alpaca-13B </p>

          </blockquote>

          <p>but you can see in the config that it''s 7b??</p>

          <p>its currently checkpoint 1000, not done!</p>

          '
        raw: "just lookkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk,.\n\
          its a rank 8 lora\n\n>your Alpaca-13B \n\nbut you can see in the config\
          \ that it's 7b??\n\nits currently checkpoint 1000, not done!"
        updatedAt: '2023-03-27T23:00:53.138Z'
      numEdits: 1
      reactions: []
    id: 642220115d5ee561d881bee5
    type: comment
  author: Yoshiii
  content: "just lookkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk,.\n\
    its a rank 8 lora\n\n>your Alpaca-13B \n\nbut you can see in the config that it's\
    \ 7b??\n\nits currently checkpoint 1000, not done!"
  created_at: 2023-03-27 22:00:33+00:00
  edited: true
  hidden: false
  id: 642220115d5ee561d881bee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-03-28T02:10:11.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yoshiii&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yoshiii\">@<span class=\"\
          underline\">Yoshiii</span></a></span>\n\n\t</span></span>  Well sorry if\
          \ I'm trying to learn something. While we're at it help me a sec to clear\
          \ up my mind, some random questions in no particular order: does passing\
          \ the --bf16 flag change anything for regular usage (no training)? If I\
          \ got a LLaMA model with Alpaca's LoRA baked in, should I still include\
          \ the very same LoRA in the model's folder or the baked one will do the\
          \ trick by itself? Then, it came as a single safetensor file, I downloaded\
          \ the proper (I think) HF LLaMA weights and put the model in that folder:\
          \ now, as far as I understand I have an Alpaca model and the weights are\
          \ for LLaMA, what am I missing here? Also can/should I delete the original\
          \ HF safetensor (weight?) from the folder or is it still needed? And finally,\
          \ is there anything else that I should do/edit in the goddamn folder? Thank\
          \ you</p>\n"
        raw: '@Yoshiii  Well sorry if I''m trying to learn something. While we''re
          at it help me a sec to clear up my mind, some random questions in no particular
          order: does passing the --bf16 flag change anything for regular usage (no
          training)? If I got a LLaMA model with Alpaca''s LoRA baked in, should I
          still include the very same LoRA in the model''s folder or the baked one
          will do the trick by itself? Then, it came as a single safetensor file,
          I downloaded the proper (I think) HF LLaMA weights and put the model in
          that folder: now, as far as I understand I have an Alpaca model and the
          weights are for LLaMA, what am I missing here? Also can/should I delete
          the original HF safetensor (weight?) from the folder or is it still needed?
          And finally, is there anything else that I should do/edit in the goddamn
          folder? Thank you'
        updatedAt: '2023-03-28T02:10:11.057Z'
      numEdits: 0
      reactions: []
    id: 64224c835acad90e6b72e1eb
    type: comment
  author: 9cento
  content: '@Yoshiii  Well sorry if I''m trying to learn something. While we''re at
    it help me a sec to clear up my mind, some random questions in no particular order:
    does passing the --bf16 flag change anything for regular usage (no training)?
    If I got a LLaMA model with Alpaca''s LoRA baked in, should I still include the
    very same LoRA in the model''s folder or the baked one will do the trick by itself?
    Then, it came as a single safetensor file, I downloaded the proper (I think) HF
    LLaMA weights and put the model in that folder: now, as far as I understand I
    have an Alpaca model and the weights are for LLaMA, what am I missing here? Also
    can/should I delete the original HF safetensor (weight?) from the folder or is
    it still needed? And finally, is there anything else that I should do/edit in
    the goddamn folder? Thank you'
  created_at: 2023-03-28 01:10:11+00:00
  edited: false
  hidden: false
  id: 64224c835acad90e6b72e1eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chavinlo/Alpaca-65B
repo_type: model
status: open
target_branch: null
title: How this has been trained?
