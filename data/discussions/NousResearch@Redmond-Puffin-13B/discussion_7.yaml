!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-09-25 03:25:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-09-25T04:25:24.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-26T03:48:45.372Z'
      numEdits: 0
      reactions: []
    id: 65110bb453b1e2d59e4e0e55
    type: comment
  author: Phil337
  content: This comment has been hidden
  created_at: 2023-09-25 03:25:24+00:00
  edited: true
  hidden: true
  id: 65110bb453b1e2d59e4e0e55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-25T10:55:17.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9495375752449036
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>I know very little about LLMs and how they work, but after testing them
          with a broad spectrum of questions I''ve come to realize something. Namely,
          that fine-tuning ruins the balance of LLMs.</p>

          <p>I understand why a Biomedical company would modify Llama 2 with a ton
          of biomedical info since it spits out far better biomedical info. But after
          doing so everything else, such as poetry and pop culture, gets notably worse
          (e.g. is less coherent and with more hallucinations).</p>

          <p>Back to this LLM. I favor up to date info and science so I like the updated
          info and the data you updated Llama 2 with. It serves my personal interests
          well. However, it threw the balance of Llama 2 WAY off and made it a notably
          worse general purpose chat bot.</p>

          <p>For example, answers to questions about pop culture, such as characters
          from a TV show, are notably worse. Llama 2 generally answered these questions
          correctly, and when it hallucinated, it''s usually in the ballpark (e.g.
          confused different characters from the same show). But this LLM was way
          off. Both the character and actor names where consistently not from the
          same show/movie, or even close (e.g. dead before it was even made). It basically
          just answers with random A-list celebrities whenever I indirectly asked
          about a character (e.g. the ex-wife of Alan Harper on the show Two and a
          Half Men).</p>

          <p>Meta carefully filtered a huge amount of data across the broad spectrum
          of human knowledge, then used a powerful computer for months in order to
          fine tune the statistical relationships between the tokens and cram it into
          only 7, 13... billion parameters. Any modification, no matter how small,
          is going to throw off this balance and lead to more hallucinations and issues
          overall. Yes, you can feed it a ton of biomedical info and get better biomedical
          info out. Or you can modify it with newer info and more science/coding data,
          and it will perform better with said data, but worse for all other areas
          of human knowledge you didn''t modify it with. Unaligned LLMs like LLama
          and Falcon should only be minimally modified for more coherent and polite
          responses. They should not be censored, moralized, fed new or focused data
          and so on. Apparently this is called "alignment tax", and I''m starting
          to understand why that phrase is used. Every LLM censored, moralized or
          fed new data from select areas of human knowledge see a spike in issues,
          especially hallucinations.</p>

          </blockquote>

          <p>I would just note, that Puffin model used the LIMA approach, that Meta
          itself spearheaded, and has so little new knowledge that this of all models
          attempts to finetune it <em>the least</em> - it is quite literally the "minimally
          modified" llama - the dataset is even smaller than LIMA</p>

          '
        raw: "> I know very little about LLMs and how they work, but after testing\
          \ them with a broad spectrum of questions I've come to realize something.\
          \ Namely, that fine-tuning ruins the balance of LLMs.\n> \n> I understand\
          \ why a Biomedical company would modify Llama 2 with a ton of biomedical\
          \ info since it spits out far better biomedical info. But after doing so\
          \ everything else, such as poetry and pop culture, gets notably worse (e.g.\
          \ is less coherent and with more hallucinations).\n> \n> Back to this LLM.\
          \ I favor up to date info and science so I like the updated info and the\
          \ data you updated Llama 2 with. It serves my personal interests well. However,\
          \ it threw the balance of Llama 2 WAY off and made it a notably worse general\
          \ purpose chat bot.\n> \n> For example, answers to questions about pop culture,\
          \ such as characters from a TV show, are notably worse. Llama 2 generally\
          \ answered these questions correctly, and when it hallucinated, it's usually\
          \ in the ballpark (e.g. confused different characters from the same show).\
          \ But this LLM was way off. Both the character and actor names where consistently\
          \ not from the same show/movie, or even close (e.g. dead before it was even\
          \ made). It basically just answers with random A-list celebrities whenever\
          \ I indirectly asked about a character (e.g. the ex-wife of Alan Harper\
          \ on the show Two and a Half Men).\n> \n> Meta carefully filtered a huge\
          \ amount of data across the broad spectrum of human knowledge, then used\
          \ a powerful computer for months in order to fine tune the statistical relationships\
          \ between the tokens and cram it into only 7, 13... billion parameters.\
          \ Any modification, no matter how small, is going to throw off this balance\
          \ and lead to more hallucinations and issues overall. Yes, you can feed\
          \ it a ton of biomedical info and get better biomedical info out. Or you\
          \ can modify it with newer info and more science/coding data, and it will\
          \ perform better with said data, but worse for all other areas of human\
          \ knowledge you didn't modify it with. Unaligned LLMs like LLama and Falcon\
          \ should only be minimally modified for more coherent and polite responses.\
          \ They should not be censored, moralized, fed new or focused data and so\
          \ on. Apparently this is called \"alignment tax\", and I'm starting to understand\
          \ why that phrase is used. Every LLM censored, moralized or fed new data\
          \ from select areas of human knowledge see a spike in issues, especially\
          \ hallucinations.\n\nI would just note, that Puffin model used the LIMA\
          \ approach, that Meta itself spearheaded, and has so little new knowledge\
          \ that this of all models attempts to finetune it *the least* - it is quite\
          \ literally the \"minimally modified\" llama - the dataset is even smaller\
          \ than LIMA"
        updatedAt: '2023-09-25T10:55:17.012Z'
      numEdits: 0
      reactions: []
    id: 651167158a3060e7229a404f
    type: comment
  author: teknium
  content: "> I know very little about LLMs and how they work, but after testing them\
    \ with a broad spectrum of questions I've come to realize something. Namely, that\
    \ fine-tuning ruins the balance of LLMs.\n> \n> I understand why a Biomedical\
    \ company would modify Llama 2 with a ton of biomedical info since it spits out\
    \ far better biomedical info. But after doing so everything else, such as poetry\
    \ and pop culture, gets notably worse (e.g. is less coherent and with more hallucinations).\n\
    > \n> Back to this LLM. I favor up to date info and science so I like the updated\
    \ info and the data you updated Llama 2 with. It serves my personal interests\
    \ well. However, it threw the balance of Llama 2 WAY off and made it a notably\
    \ worse general purpose chat bot.\n> \n> For example, answers to questions about\
    \ pop culture, such as characters from a TV show, are notably worse. Llama 2 generally\
    \ answered these questions correctly, and when it hallucinated, it's usually in\
    \ the ballpark (e.g. confused different characters from the same show). But this\
    \ LLM was way off. Both the character and actor names where consistently not from\
    \ the same show/movie, or even close (e.g. dead before it was even made). It basically\
    \ just answers with random A-list celebrities whenever I indirectly asked about\
    \ a character (e.g. the ex-wife of Alan Harper on the show Two and a Half Men).\n\
    > \n> Meta carefully filtered a huge amount of data across the broad spectrum\
    \ of human knowledge, then used a powerful computer for months in order to fine\
    \ tune the statistical relationships between the tokens and cram it into only\
    \ 7, 13... billion parameters. Any modification, no matter how small, is going\
    \ to throw off this balance and lead to more hallucinations and issues overall.\
    \ Yes, you can feed it a ton of biomedical info and get better biomedical info\
    \ out. Or you can modify it with newer info and more science/coding data, and\
    \ it will perform better with said data, but worse for all other areas of human\
    \ knowledge you didn't modify it with. Unaligned LLMs like LLama and Falcon should\
    \ only be minimally modified for more coherent and polite responses. They should\
    \ not be censored, moralized, fed new or focused data and so on. Apparently this\
    \ is called \"alignment tax\", and I'm starting to understand why that phrase\
    \ is used. Every LLM censored, moralized or fed new data from select areas of\
    \ human knowledge see a spike in issues, especially hallucinations.\n\nI would\
    \ just note, that Puffin model used the LIMA approach, that Meta itself spearheaded,\
    \ and has so little new knowledge that this of all models attempts to finetune\
    \ it *the least* - it is quite literally the \"minimally modified\" llama - the\
    \ dataset is even smaller than LIMA"
  created_at: 2023-09-25 09:55:17+00:00
  edited: false
  hidden: false
  id: 651167158a3060e7229a404f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-09-26T03:49:00.000Z'
    data:
      status: closed
    id: 651254ac9a1484c10fae5a20
    type: status-change
  author: Phil337
  created_at: 2023-09-26 02:49:00+00:00
  id: 651254ac9a1484c10fae5a20
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-09-26T03:49:07.000Z'
    data:
      status: open
    id: 651254b359685e31aff2b20e
    type: status-change
  author: Phil337
  created_at: 2023-09-26 02:49:07+00:00
  id: 651254b359685e31aff2b20e
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-28T00:57:39.000Z'
    data:
      status: closed
    id: 6514cf836efa656d177d818d
    type: status-change
  author: teknium
  created_at: 2023-09-27 23:57:39+00:00
  id: 6514cf836efa656d177d818d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: NousResearch/Redmond-Puffin-13B
repo_type: model
status: closed
target_branch: null
title: The updated data is cool but...
