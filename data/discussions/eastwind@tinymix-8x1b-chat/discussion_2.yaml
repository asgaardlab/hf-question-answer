!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 010O11
conflicting_files: null
created_at: 2024-01-04 16:37:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
      fullname: OlO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 010O11
      type: user
    createdAt: '2024-01-04T16:37:55.000Z'
    data:
      edited: false
      editors:
      - 010O11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.915466845035553
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
          fullname: OlO
          isHf: false
          isPro: false
          name: 010O11
          type: user
        html: '<p>"The intuition being finetuning 8x1b should give better performance
          than finetuning 1b by itself." &gt;&gt; are you sure? how so? my intuition
          telling me the opposite, sorry for that...</p>

          '
        raw: '"The intuition being finetuning 8x1b should give better performance
          than finetuning 1b by itself." >> are you sure? how so? my intuition telling
          me the opposite, sorry for that...'
        updatedAt: '2024-01-04T16:37:55.824Z'
      numEdits: 0
      reactions: []
    id: 6596dee36a30bf8860bdc332
    type: comment
  author: 010O11
  content: '"The intuition being finetuning 8x1b should give better performance than
    finetuning 1b by itself." >> are you sure? how so? my intuition telling me the
    opposite, sorry for that...'
  created_at: 2024-01-04 16:37:55+00:00
  edited: false
  hidden: false
  id: 6596dee36a30bf8860bdc332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eastwind
      type: user
    createdAt: '2024-01-04T18:01:37.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9379051327705383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>Well you are finetuning 8x1b(6.5b approx) against finetuning 1b.</p>

          <p>In the llm space bigger is almost always better. If not then why is 7b
          model not as good as 70b?</p>

          '
        raw: 'Well you are finetuning 8x1b(6.5b approx) against finetuning 1b.


          In the llm space bigger is almost always better. If not then why is 7b model
          not as good as 70b?'
        updatedAt: '2024-01-04T18:01:37.050Z'
      numEdits: 0
      reactions: []
    id: 6596f28104f13fb853b10d2d
    type: comment
  author: eastwind
  content: 'Well you are finetuning 8x1b(6.5b approx) against finetuning 1b.


    In the llm space bigger is almost always better. If not then why is 7b model not
    as good as 70b?'
  created_at: 2024-01-04 18:01:37+00:00
  edited: false
  hidden: false
  id: 6596f28104f13fb853b10d2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
      fullname: OlO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 010O11
      type: user
    createdAt: '2024-01-05T15:01:33.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b55879f2858e648d5460fb/ZrkS_BmET_WIXZCS_q6Bv.png?w=200&h=200&f=face
          fullname: OlO
          isHf: false
          isPro: false
          name: 010O11
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-05T18:00:24.260Z'
      numEdits: 0
      reactions: []
    id: 659819cda6e877402a616de1
    type: comment
  author: 010O11
  content: This comment has been hidden
  created_at: 2024-01-05 15:01:33+00:00
  edited: true
  hidden: true
  id: 659819cda6e877402a616de1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2024-01-05T17:30:02.000Z'
    data:
      edited: false
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295283555984497
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: '<p>Hey so ive been messing around with the mixtral branch of mergekit
          and im just curious how you got your config to work? I am trying to replicate
          with the base model for education and it throws a tremendous amount of errors.
          Did you edit the mixtral branch further to fit your particular use case?</p>

          '
        raw: Hey so ive been messing around with the mixtral branch of mergekit and
          im just curious how you got your config to work? I am trying to replicate
          with the base model for education and it throws a tremendous amount of errors.
          Did you edit the mixtral branch further to fit your particular use case?
        updatedAt: '2024-01-05T17:30:02.670Z'
      numEdits: 0
      reactions: []
    id: 65983c9a4487b430c2090c14
    type: comment
  author: macadeliccc
  content: Hey so ive been messing around with the mixtral branch of mergekit and
    im just curious how you got your config to work? I am trying to replicate with
    the base model for education and it throws a tremendous amount of errors. Did
    you edit the mixtral branch further to fit your particular use case?
  created_at: 2024-01-05 17:30:02+00:00
  edited: false
  hidden: false
  id: 65983c9a4487b430c2090c14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eastwind
      type: user
    createdAt: '2024-01-05T23:04:41.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9316893815994263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>It worked out of the box for me. No changes. But it only works with
          llama and mistral architectures.</p>

          '
        raw: It worked out of the box for me. No changes. But it only works with llama
          and mistral architectures.
        updatedAt: '2024-01-05T23:04:41.638Z'
      numEdits: 0
      reactions: []
    id: 65988b094487b430c21d031e
    type: comment
  author: eastwind
  content: It worked out of the box for me. No changes. But it only works with llama
    and mistral architectures.
  created_at: 2024-01-05 23:04:41+00:00
  edited: false
  hidden: false
  id: 65988b094487b430c21d031e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eastwind
      type: user
    createdAt: '2024-01-05T23:04:50.000Z'
    data:
      status: closed
    id: 65988b12a2a70bb8836623d5
    type: status-change
  author: eastwind
  created_at: 2024-01-05 23:04:50+00:00
  id: 65988b12a2a70bb8836623d5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: eastwind/tinymix-8x1b-chat
repo_type: model
status: closed
target_branch: null
title: just curious
