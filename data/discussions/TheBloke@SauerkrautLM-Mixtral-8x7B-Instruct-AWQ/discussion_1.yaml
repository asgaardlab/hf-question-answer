!!python/object:huggingface_hub.community.DiscussionWithDetails
author: martinkozle
conflicting_files: null
created_at: 2024-01-15 18:37:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6ad0e96703ad3cf96530c59d6a155a2.svg
      fullname: Martin Popovski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martinkozle
      type: user
    createdAt: '2024-01-15T18:37:57.000Z'
    data:
      edited: false
      editors:
      - martinkozle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776082038879395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6ad0e96703ad3cf96530c59d6a155a2.svg
          fullname: Martin Popovski
          isHf: false
          isPro: false
          name: martinkozle
          type: user
        html: '<p>I had a discussion on the original model card page about issues
          I was having prompting this model.<br><a href="https://huggingface.co/VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct/discussions/2">https://huggingface.co/VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct/discussions/2</a></p>

          <p>After many different tests we came to the conclusion that the issue was
          because I was using the AWQ quantization.<br>A lot of the time the issue
          was specifically that the model had a large tendency to continue generating
          more text after already generating the requested information.<br>If anyone
          else is having similar issues know that it is likely the quantization and
          not the model itself.</p>

          <p>I don''t know if this behavior is specific to this model or if it is
          a general case for most AWQ quantizations, if anyone knows I would be intrigued
          to know!</p>

          '
        raw: "I had a discussion on the original model card page about issues I was\
          \ having prompting this model.\r\n<https://huggingface.co/VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct/discussions/2>\r\
          \n\r\nAfter many different tests we came to the conclusion that the issue\
          \ was because I was using the AWQ quantization.\r\nA lot of the time the\
          \ issue was specifically that the model had a large tendency to continue\
          \ generating more text after already generating the requested information.\r\
          \nIf anyone else is having similar issues know that it is likely the quantization\
          \ and not the model itself.\r\n\r\nI don't know if this behavior is specific\
          \ to this model or if it is a general case for most AWQ quantizations, if\
          \ anyone knows I would be intrigued to know!"
        updatedAt: '2024-01-15T18:37:57.436Z'
      numEdits: 0
      reactions: []
    id: 65a57b85000ded69cb570f91
    type: comment
  author: martinkozle
  content: "I had a discussion on the original model card page about issues I was\
    \ having prompting this model.\r\n<https://huggingface.co/VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct/discussions/2>\r\
    \n\r\nAfter many different tests we came to the conclusion that the issue was\
    \ because I was using the AWQ quantization.\r\nA lot of the time the issue was\
    \ specifically that the model had a large tendency to continue generating more\
    \ text after already generating the requested information.\r\nIf anyone else is\
    \ having similar issues know that it is likely the quantization and not the model\
    \ itself.\r\n\r\nI don't know if this behavior is specific to this model or if\
    \ it is a general case for most AWQ quantizations, if anyone knows I would be\
    \ intrigued to know!"
  created_at: 2024-01-15 18:37:57+00:00
  edited: false
  hidden: false
  id: 65a57b85000ded69cb570f91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-17T13:43:13.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9234781265258789
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;martinkozle&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/martinkozle\"\
          >@<span class=\"underline\">martinkozle</span></a></span>\n\n\t</span></span>\
          \ i think this is more about the eos token instead of the quantization format.</p>\n\
          <p>For some reason, the generation doesn\u2019t stop at the eos token. It\
          \ might be a problem with the library or just some setting with the eos\
          \ token</p>\n<p>The texts after the answer is very much like text after\
          \ the eos token like it\u2019s usually completely random unrelelated things</p>\n"
        raw: "@martinkozle i think this is more about the eos token instead of the\
          \ quantization format.\n\nFor some reason, the generation doesn\u2019t stop\
          \ at the eos token. It might be a problem with the library or just some\
          \ setting with the eos token\n\nThe texts after the answer is very much\
          \ like text after the eos token like it\u2019s usually completely random\
          \ unrelelated things"
        updatedAt: '2024-01-17T13:43:13.792Z'
      numEdits: 0
      reactions: []
    id: 65a7d9712a20de2d7aeb378a
    type: comment
  author: YaTharThShaRma999
  content: "@martinkozle i think this is more about the eos token instead of the quantization\
    \ format.\n\nFor some reason, the generation doesn\u2019t stop at the eos token.\
    \ It might be a problem with the library or just some setting with the eos token\n\
    \nThe texts after the answer is very much like text after the eos token like it\u2019\
    s usually completely random unrelelated things"
  created_at: 2024-01-17 13:43:13+00:00
  edited: false
  hidden: false
  id: 65a7d9712a20de2d7aeb378a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/SauerkrautLM-Mixtral-8x7B-Instruct-AWQ
repo_type: model
status: open
target_branch: null
title: AWQ model performs significantly worse than the GPTQ model
