!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-05-04 16:35:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-04T17:35:02.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p>I pulled this repo the local directory and try to load with --model-path\
          \ by setting a local path. But Hugging face transformer still want to download\
          \ the tokenizer confi from online which causes some error</p>\n<pre><code>\u2502\
          \ /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
          \ \u2502\n\u2502 ference.py:235 in chat_loop                           \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   232 \u2502   \u2502   debug: bool,    \
          \                                                                   \u2502\
          \n\u2502   233 ):                                                      \
          \                                   \u2502\n\u2502   234 \u2502   # Model\
          \                                                                      \
          \          \u2502\n\u2502 \u2771 235 \u2502   model, tokenizer = load_model(\
          \                                                         \u2502\n\u2502\
          \   236 \u2502   \u2502   model_path, device, num_gpus, max_gpu_memory,\
          \ load_8bit, load_4bit, debug          \u2502\n\u2502   237 \u2502   ) \
          \                                                                      \
          \               \u2502\n\u2502   238                                   \
          \                                                         \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
          \ \u2502\n\u2502 ference.py:94 in load_model                           \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502    91 \u2502   \u2502   tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=True)               \u2502\n\u2502    92 \u2502   \u2502   model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True,\
          \   \u2502\n\u2502    93 \u2502   else:                                \
          \                                                  \u2502\n\u2502 \u2771\
          \  94 \u2502   \u2502   tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=True)               \u2502\n\u2502    95 \u2502   \u2502   model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True,\
          \   \u2502\n\u2502    96 \u2502                                        \
          \                                                  \u2502\n\u2502    97\
          \ \u2502   if load_8bit:                                               \
          \                           \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
          \ \u2502\n\u2502 ization_auto.py:642 in from_pretrained                \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   639 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input   \u2502\n\u2502   640 \u2502   \u2502                        \
          \                                                              \u2502\n\u2502\
          \   641 \u2502   \u2502   # Next, let's try to use the tokenizer_config\
          \ file to get the tokenizer class.     \u2502\n\u2502 \u2771 642 \u2502\
          \   \u2502   tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)   \u2502\n\u2502   643 \u2502   \u2502   if \"_commit_hash\"\
          \ in tokenizer_config:                                             \u2502\
          \n\u2502   644 \u2502   \u2502   \u2502   kwargs[\"_commit_hash\"] = tokenizer_config[\"\
          _commit_hash\"]                      \u2502\n\u2502   645 \u2502   \u2502\
          \   config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\") \
          \                  \u2502\n\u2502                                      \
          \                                                            \u2502\n\u2502\
          \ /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
          \ \u2502\n\u2502 ization_auto.py:486 in get_tokenizer_config           \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   483 \u2502   tokenizer_config = get_tokenizer_config(\"\
          tokenizer-test\")                              \u2502\n\u2502   484 \u2502\
          \   ```\"\"\"                                                          \
          \                       \u2502\n\u2502   485 \u2502   commit_hash = kwargs.get(\"\
          _commit_hash\", None)                                         \u2502\n\u2502\
          \ \u2771 486 \u2502   resolved_config_file = cached_file(              \
          \                                      \u2502\n\u2502   487 \u2502   \u2502\
          \   pretrained_model_name_or_path,                                     \
          \                \u2502\n\u2502   488 \u2502   \u2502   TOKENIZER_CONFIG_FILE,\
          \                                                             \u2502\n\u2502\
          \   489 \u2502   \u2502   cache_dir=cache_dir,                         \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/utils/hub.py:409\
          \  \u2502\n\u2502 in cached_file                                       \
          \                                            \u2502\n\u2502            \
          \                                                                      \
          \                \u2502\n\u2502    406 \u2502   user_agent = http_user_agent(user_agent)\
          \                                              \u2502\n\u2502    407 \u2502\
          \   try:                                                               \
          \                   \u2502\n\u2502    408 \u2502   \u2502   # Load from\
          \ URL or cache if already cached                                       \
          \ \u2502\n\u2502 \u2771  409 \u2502   \u2502   resolved_file = hf_hub_download(\
          \                                                  \u2502\n\u2502    410\
          \ \u2502   \u2502   \u2502   path_or_repo_id,                          \
          \                                    \u2502\n\u2502    411 \u2502   \u2502\
          \   \u2502   filename,                                                 \
          \                    \u2502\n\u2502    412 \u2502   \u2502   \u2502   subfolder=None\
          \ if len(subfolder) == 0 else subfolder,                         \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
          \ \u2502\n\u2502 ors.py:112 in _inner_fn                               \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   109 \u2502   \u2502   \u2502   kwargs.items(),\
          \  # Kwargs values                                               \u2502\n\
          \u2502   110 \u2502   \u2502   ):                                      \
          \                                           \u2502\n\u2502   111 \u2502\
          \   \u2502   \u2502   if arg_name in [\"repo_id\", \"from_id\", \"to_id\"\
          ]:                                \u2502\n\u2502 \u2771 112 \u2502   \u2502\
          \   \u2502   \u2502   validate_repo_id(arg_value)                      \
          \                          \u2502\n\u2502   113 \u2502   \u2502   \u2502\
          \                                                                      \
          \            \u2502\n\u2502   114 \u2502   \u2502   \u2502   elif arg_name\
          \ == \"token\" and arg_value is not None:                            \u2502\
          \n\u2502   115 \u2502   \u2502   \u2502   \u2502   has_token = True    \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \                           \u2502\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
          \ \u2502\n\u2502 ors.py:160 in validate_repo_id                        \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   157 \u2502   \u2502   raise HFValidationError(f\"\
          Repo id must be a string, not {type(repo_id)}: '{repo_   \u2502\n\u2502\
          \   158 \u2502                                                         \
          \                                 \u2502\n\u2502   159 \u2502   if repo_id.count(\"\
          /\") &gt; 1:                                                           \
          \  \u2502\n\u2502 \u2771 160 \u2502   \u2502   raise HFValidationError(\
          \                                                           \u2502\n\u2502\
          \   161 \u2502   \u2502   \u2502   \"Repo id must be in the form 'repo_name'\
          \ or 'namespace/repo_name':\"            \u2502\n\u2502   162 \u2502   \u2502\
          \   \u2502   f\" '{repo_id}'. Use `repo_type` argument if needed.\"    \
          \                       \u2502\n\u2502   163 \u2502   \u2502   )       \
          \                                                                      \
          \     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>\n"
        raw: "I pulled this repo the local directory and try to load with --model-path\
          \ by setting a local path. But Hugging face transformer still want to download\
          \ the tokenizer confi from online which causes some error\r\n\r\n```\r\n\
          \u2502 /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
          \ \u2502\r\n\u2502 ference.py:235 in chat_loop                         \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   232 \u2502   \u2502   debug: bool,\
          \                                                                      \
          \ \u2502\r\n\u2502   233 ):                                            \
          \                                             \u2502\r\n\u2502   234 \u2502\
          \   # Model                                                            \
          \                    \u2502\r\n\u2502 \u2771 235 \u2502   model, tokenizer\
          \ = load_model(                                                        \
          \ \u2502\r\n\u2502   236 \u2502   \u2502   model_path, device, num_gpus,\
          \ max_gpu_memory, load_8bit, load_4bit, debug          \u2502\r\n\u2502\
          \   237 \u2502   )                                                     \
          \                                 \u2502\r\n\u2502   238               \
          \                                                                      \
          \       \u2502\r\n\u2502                                               \
          \                                                   \u2502\r\n\u2502 /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
          \ \u2502\r\n\u2502 ference.py:94 in load_model                         \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502    91 \u2502   \u2502   tokenizer =\
          \ AutoTokenizer.from_pretrained(model_path, use_fast=True)             \
          \  \u2502\r\n\u2502    92 \u2502   \u2502   model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ low_cpu_mem_usage=True,   \u2502\r\n\u2502    93 \u2502   else:      \
          \                                                                      \
          \      \u2502\r\n\u2502 \u2771  94 \u2502   \u2502   tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=True)               \u2502\r\n\u2502    95 \u2502   \u2502  \
          \ model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True,\
          \   \u2502\r\n\u2502    96 \u2502                                      \
          \                                                    \u2502\r\n\u2502  \
          \  97 \u2502   if load_8bit:                                           \
          \                               \u2502\r\n\u2502                       \
          \                                                                      \
          \     \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
          \ \u2502\r\n\u2502 ization_auto.py:642 in from_pretrained              \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   639 \u2502   \u2502   \u2502   return\
          \ tokenizer_class.from_pretrained(pretrained_model_name_or_path, *input\
          \   \u2502\r\n\u2502   640 \u2502   \u2502                             \
          \                                                         \u2502\r\n\u2502\
          \   641 \u2502   \u2502   # Next, let's try to use the tokenizer_config\
          \ file to get the tokenizer class.     \u2502\r\n\u2502 \u2771 642 \u2502\
          \   \u2502   tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)   \u2502\r\n\u2502   643 \u2502   \u2502   if \"_commit_hash\"\
          \ in tokenizer_config:                                             \u2502\
          \r\n\u2502   644 \u2502   \u2502   \u2502   kwargs[\"_commit_hash\"] = tokenizer_config[\"\
          _commit_hash\"]                      \u2502\r\n\u2502   645 \u2502   \u2502\
          \   config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\") \
          \                  \u2502\r\n\u2502                                    \
          \                                                              \u2502\r\n\
          \u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
          \ \u2502\r\n\u2502 ization_auto.py:486 in get_tokenizer_config         \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   483 \u2502   tokenizer_config = get_tokenizer_config(\"\
          tokenizer-test\")                              \u2502\r\n\u2502   484 \u2502\
          \   ```\"\"\"                                                          \
          \                       \u2502\r\n\u2502   485 \u2502   commit_hash = kwargs.get(\"\
          _commit_hash\", None)                                         \u2502\r\n\
          \u2502 \u2771 486 \u2502   resolved_config_file = cached_file(         \
          \                                           \u2502\r\n\u2502   487 \u2502\
          \   \u2502   pretrained_model_name_or_path,                            \
          \                         \u2502\r\n\u2502   488 \u2502   \u2502   TOKENIZER_CONFIG_FILE,\
          \                                                             \u2502\r\n\
          \u2502   489 \u2502   \u2502   cache_dir=cache_dir,                    \
          \                                           \u2502\r\n\u2502           \
          \                                                                      \
          \                 \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/utils/hub.py:409\
          \  \u2502\r\n\u2502 in cached_file                                     \
          \                                              \u2502\r\n\u2502        \
          \                                                                      \
          \                    \u2502\r\n\u2502    406 \u2502   user_agent = http_user_agent(user_agent)\
          \                                              \u2502\r\n\u2502    407 \u2502\
          \   try:                                                               \
          \                   \u2502\r\n\u2502    408 \u2502   \u2502   # Load from\
          \ URL or cache if already cached                                       \
          \ \u2502\r\n\u2502 \u2771  409 \u2502   \u2502   resolved_file = hf_hub_download(\
          \                                                  \u2502\r\n\u2502    410\
          \ \u2502   \u2502   \u2502   path_or_repo_id,                          \
          \                                    \u2502\r\n\u2502    411 \u2502   \u2502\
          \   \u2502   filename,                                                 \
          \                    \u2502\r\n\u2502    412 \u2502   \u2502   \u2502  \
          \ subfolder=None if len(subfolder) == 0 else subfolder,                \
          \         \u2502\r\n\u2502                                             \
          \                                                     \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
          \ \u2502\r\n\u2502 ors.py:112 in _inner_fn                             \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   109 \u2502   \u2502   \u2502   kwargs.items(),\
          \  # Kwargs values                                               \u2502\r\
          \n\u2502   110 \u2502   \u2502   ):                                    \
          \                                             \u2502\r\n\u2502   111 \u2502\
          \   \u2502   \u2502   if arg_name in [\"repo_id\", \"from_id\", \"to_id\"\
          ]:                                \u2502\r\n\u2502 \u2771 112 \u2502   \u2502\
          \   \u2502   \u2502   validate_repo_id(arg_value)                      \
          \                          \u2502\r\n\u2502   113 \u2502   \u2502   \u2502\
          \                                                                      \
          \            \u2502\r\n\u2502   114 \u2502   \u2502   \u2502   elif arg_name\
          \ == \"token\" and arg_value is not None:                            \u2502\
          \r\n\u2502   115 \u2502   \u2502   \u2502   \u2502   has_token = True  \
          \                                                         \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
          \ \u2502\r\n\u2502 ors.py:160 in validate_repo_id                      \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   157 \u2502   \u2502   raise HFValidationError(f\"\
          Repo id must be a string, not {type(repo_id)}: '{repo_   \u2502\r\n\u2502\
          \   158 \u2502                                                         \
          \                                 \u2502\r\n\u2502   159 \u2502   if repo_id.count(\"\
          /\") > 1:                                                             \u2502\
          \r\n\u2502 \u2771 160 \u2502   \u2502   raise HFValidationError(       \
          \                                                    \u2502\r\n\u2502  \
          \ 161 \u2502   \u2502   \u2502   \"Repo id must be in the form 'repo_name'\
          \ or 'namespace/repo_name':\"            \u2502\r\n\u2502   162 \u2502 \
          \  \u2502   \u2502   f\" '{repo_id}'. Use `repo_type` argument if needed.\"\
          \                           \u2502\r\n\u2502   163 \u2502   \u2502   ) \
          \                                                                      \
          \           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```"
        updatedAt: '2023-05-04T17:35:02.058Z'
      numEdits: 0
      reactions: []
    id: 6453ecc6908e259483c18755
    type: comment
  author: Yhyu13
  content: "I pulled this repo the local directory and try to load with --model-path\
    \ by setting a local path. But Hugging face transformer still want to download\
    \ the tokenizer confi from online which causes some error\r\n\r\n```\r\n\u2502\
    \ /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
    \ \u2502\r\n\u2502 ference.py:235 in chat_loop                               \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   232 \u2502   \u2502   debug: bool,                      \
    \                                                 \u2502\r\n\u2502   233 ):  \
    \                                                                            \
    \           \u2502\r\n\u2502   234 \u2502   # Model                          \
    \                                                      \u2502\r\n\u2502 \u2771\
    \ 235 \u2502   model, tokenizer = load_model(                                \
    \                         \u2502\r\n\u2502   236 \u2502   \u2502   model_path,\
    \ device, num_gpus, max_gpu_memory, load_8bit, load_4bit, debug          \u2502\
    \r\n\u2502   237 \u2502   )                                                  \
    \                                    \u2502\r\n\u2502   238                  \
    \                                                                          \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /home/hangyu5/Documents/Git-repoMy/AIResearchVault/repo/LLM/BLOOM/LLMZoo/llmzoo/deploy/webapp/in\
    \ \u2502\r\n\u2502 ference.py:94 in load_model                               \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502    91 \u2502   \u2502   tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=True)               \u2502\r\n\u2502    92 \u2502   \u2502   model\
    \ = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, \
    \  \u2502\r\n\u2502    93 \u2502   else:                                     \
    \                                             \u2502\r\n\u2502 \u2771  94 \u2502\
    \   \u2502   tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\
    \               \u2502\r\n\u2502    95 \u2502   \u2502   model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ low_cpu_mem_usage=True,   \u2502\r\n\u2502    96 \u2502                    \
    \                                                                      \u2502\r\
    \n\u2502    97 \u2502   if load_8bit:                                        \
    \                                  \u2502\r\n\u2502                          \
    \                                                                        \u2502\
    \r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
    \ \u2502\r\n\u2502 ization_auto.py:642 in from_pretrained                    \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   639 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *input   \u2502\r\n\u2502   640 \u2502   \u2502                            \
    \                                                          \u2502\r\n\u2502  \
    \ 641 \u2502   \u2502   # Next, let's try to use the tokenizer_config file to\
    \ get the tokenizer class.     \u2502\r\n\u2502 \u2771 642 \u2502   \u2502   tokenizer_config\
    \ = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)   \u2502\r\n\
    \u2502   643 \u2502   \u2502   if \"_commit_hash\" in tokenizer_config:      \
    \                                       \u2502\r\n\u2502   644 \u2502   \u2502\
    \   \u2502   kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]   \
    \                   \u2502\r\n\u2502   645 \u2502   \u2502   config_tokenizer_class\
    \ = tokenizer_config.get(\"tokenizer_class\")                   \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/models/auto/token\
    \ \u2502\r\n\u2502 ization_auto.py:486 in get_tokenizer_config               \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   483 \u2502   tokenizer_config = get_tokenizer_config(\"tokenizer-test\"\
    )                              \u2502\r\n\u2502   484 \u2502   ```\"\"\"     \
    \                                                                            \u2502\
    \r\n\u2502   485 \u2502   commit_hash = kwargs.get(\"_commit_hash\", None)   \
    \                                      \u2502\r\n\u2502 \u2771 486 \u2502   resolved_config_file\
    \ = cached_file(                                                    \u2502\r\n\
    \u2502   487 \u2502   \u2502   pretrained_model_name_or_path,                \
    \                                     \u2502\r\n\u2502   488 \u2502   \u2502 \
    \  TOKENIZER_CONFIG_FILE,                                                    \
    \         \u2502\r\n\u2502   489 \u2502   \u2502   cache_dir=cache_dir,      \
    \                                                         \u2502\r\n\u2502   \
    \                                                                            \
    \                   \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/transformers/utils/hub.py:409\
    \  \u2502\r\n\u2502 in cached_file                                           \
    \                                        \u2502\r\n\u2502                    \
    \                                                                            \
    \  \u2502\r\n\u2502    406 \u2502   user_agent = http_user_agent(user_agent) \
    \                                             \u2502\r\n\u2502    407 \u2502 \
    \  try:                                                                      \
    \            \u2502\r\n\u2502    408 \u2502   \u2502   # Load from URL or cache\
    \ if already cached                                        \u2502\r\n\u2502 \u2771\
    \  409 \u2502   \u2502   resolved_file = hf_hub_download(                    \
    \                              \u2502\r\n\u2502    410 \u2502   \u2502   \u2502\
    \   path_or_repo_id,                                                         \
    \     \u2502\r\n\u2502    411 \u2502   \u2502   \u2502   filename,           \
    \                                                          \u2502\r\n\u2502  \
    \  412 \u2502   \u2502   \u2502   subfolder=None if len(subfolder) == 0 else subfolder,\
    \                         \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \ /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
    \ \u2502\r\n\u2502 ors.py:112 in _inner_fn                                   \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   109 \u2502   \u2502   \u2502   kwargs.items(),  # Kwargs\
    \ values                                               \u2502\r\n\u2502   110\
    \ \u2502   \u2502   ):                                                       \
    \                          \u2502\r\n\u2502   111 \u2502   \u2502   \u2502   if\
    \ arg_name in [\"repo_id\", \"from_id\", \"to_id\"]:                         \
    \       \u2502\r\n\u2502 \u2771 112 \u2502   \u2502   \u2502   \u2502   validate_repo_id(arg_value)\
    \                                                \u2502\r\n\u2502   113 \u2502\
    \   \u2502   \u2502                                                          \
    \                        \u2502\r\n\u2502   114 \u2502   \u2502   \u2502   elif\
    \ arg_name == \"token\" and arg_value is not None:                           \
    \ \u2502\r\n\u2502   115 \u2502   \u2502   \u2502   \u2502   has_token = True\
    \                                                           \u2502\r\n\u2502 \
    \                                                                            \
    \                     \u2502\r\n\u2502 /home/hangyu5/anaconda3/envs/pheonix/lib/python3.10/site-packages/huggingface_hub/utils/_validat\
    \ \u2502\r\n\u2502 ors.py:160 in validate_repo_id                            \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   157 \u2502   \u2502   raise HFValidationError(f\"Repo id\
    \ must be a string, not {type(repo_id)}: '{repo_   \u2502\r\n\u2502   158 \u2502\
    \                                                                            \
    \              \u2502\r\n\u2502   159 \u2502   if repo_id.count(\"/\") > 1:  \
    \                                                           \u2502\r\n\u2502 \u2771\
    \ 160 \u2502   \u2502   raise HFValidationError(                             \
    \                              \u2502\r\n\u2502   161 \u2502   \u2502   \u2502\
    \   \"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"    \
    \        \u2502\r\n\u2502   162 \u2502   \u2502   \u2502   f\" '{repo_id}'. Use\
    \ `repo_type` argument if needed.\"                           \u2502\r\n\u2502\
    \   163 \u2502   \u2502   )                                                  \
    \                                \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```"
  created_at: 2023-05-04 16:35:02+00:00
  edited: false
  hidden: false
  id: 6453ecc6908e259483c18755
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
      fullname: Chen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GeneZC
      type: user
    createdAt: '2023-05-05T03:28:27.000Z'
    data:
      edited: false
      editors:
      - GeneZC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
          fullname: Chen Zhang
          isHf: false
          isPro: false
          name: GeneZC
          type: user
        html: '<p>Yes, we should include tokenizer files. And you could reuse the
          tokenizer files from FreedomIntelligence/phoenix-inst-chat-7b at the moment.</p>

          <p>Thanks for pointing that out.</p>

          '
        raw: 'Yes, we should include tokenizer files. And you could reuse the tokenizer
          files from FreedomIntelligence/phoenix-inst-chat-7b at the moment.


          Thanks for pointing that out.'
        updatedAt: '2023-05-05T03:28:27.885Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645477dbc4cbe32fbe2fb229
    id: 645477dbc4cbe32fbe2fb228
    type: comment
  author: GeneZC
  content: 'Yes, we should include tokenizer files. And you could reuse the tokenizer
    files from FreedomIntelligence/phoenix-inst-chat-7b at the moment.


    Thanks for pointing that out.'
  created_at: 2023-05-05 02:28:27+00:00
  edited: false
  hidden: false
  id: 645477dbc4cbe32fbe2fb228
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
      fullname: Chen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GeneZC
      type: user
    createdAt: '2023-05-05T03:28:27.000Z'
    data:
      status: closed
    id: 645477dbc4cbe32fbe2fb229
    type: status-change
  author: GeneZC
  created_at: 2023-05-05 02:28:27+00:00
  id: 645477dbc4cbe32fbe2fb229
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
      fullname: Chen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GeneZC
      type: user
    createdAt: '2023-05-05T03:33:01.000Z'
    data:
      status: open
    id: 645478edc4cbe32fbe2fcde4
    type: status-change
  author: GeneZC
  created_at: 2023-05-05 02:33:01+00:00
  id: 645478edc4cbe32fbe2fcde4
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
      fullname: Chen Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GeneZC
      type: user
    createdAt: '2023-05-05T03:34:58.000Z'
    data:
      edited: false
      editors:
      - GeneZC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
          fullname: Chen Zhang
          isHf: false
          isPro: false
          name: GeneZC
          type: user
        html: '<p>And we have found a bug in our code, please use the updated version
          of our repo.</p>

          '
        raw: And we have found a bug in our code, please use the updated version of
          our repo.
        updatedAt: '2023-05-05T03:34:58.371Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 64547962ce1cc3ed5feb4aa8
    type: comment
  author: GeneZC
  content: And we have found a bug in our code, please use the updated version of
    our repo.
  created_at: 2023-05-05 02:34:58+00:00
  edited: false
  hidden: false
  id: 64547962ce1cc3ed5feb4aa8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: FreedomIntelligence/phoenix-inst-chat-7b-int4
repo_type: model
status: open
target_branch: null
title: Should there be tokenizer files in the repo?
