!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krrish-litellm
conflicting_files: null
created_at: 2023-08-15 01:20:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40c021efc35b828bf680444905affb11.svg
      fullname: Krrish Dholakia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krrish-litellm
      type: user
    createdAt: '2023-08-15T02:20:29.000Z'
    data:
      edited: false
      editors:
      - krrish-litellm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8842953443527222
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40c021efc35b828bf680444905affb11.svg
          fullname: Krrish Dholakia
          isHf: false
          isPro: false
          name: krrish-litellm
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;hunkim&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hunkim\">@<span class=\"\
          underline\">hunkim</span></a></span>\n\n\t</span></span>  / <span data-props=\"\
          {&quot;user&quot;:&quot;yoonniverse&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/yoonniverse\">@<span class=\"underline\"\
          >yoonniverse</span></a></span>\n\n\t</span></span> </p>\n<p>What's the best\
          \ way for me to deploy this model? I'd love to make a demo of this with\
          \ LiteLLM - <a rel=\"nofollow\" href=\"https://github.com/BerriAI/litellm\"\
          >https://github.com/BerriAI/litellm</a>.</p>\n<p>Lite currently works with\
          \ Replicate, Azure, Together.ai and HF Inference Endpoints.</p>\n<p>I'm\
          \ facing issues with HF Inference endpoints due to quota limitations, so\
          \ curious if you've tried any other provider.</p>\n"
        raw: "Hi @hunkim  / @yoonniverse \r\n\r\nWhat's the best way for me to deploy\
          \ this model? I'd love to make a demo of this with LiteLLM - https://github.com/BerriAI/litellm.\r\
          \n\r\nLite currently works with Replicate, Azure, Together.ai and HF Inference\
          \ Endpoints.\r\n\r\nI'm facing issues with HF Inference endpoints due to\
          \ quota limitations, so curious if you've tried any other provider.\r\n\r\
          \n"
        updatedAt: '2023-08-15T02:20:29.404Z'
      numEdits: 0
      reactions: []
    id: 64dae0ede42fba08b89c5ccf
    type: comment
  author: krrish-litellm
  content: "Hi @hunkim  / @yoonniverse \r\n\r\nWhat's the best way for me to deploy\
    \ this model? I'd love to make a demo of this with LiteLLM - https://github.com/BerriAI/litellm.\r\
    \n\r\nLite currently works with Replicate, Azure, Together.ai and HF Inference\
    \ Endpoints.\r\n\r\nI'm facing issues with HF Inference endpoints due to quota\
    \ limitations, so curious if you've tried any other provider.\r\n\r\n"
  created_at: 2023-08-15 01:20:29+00:00
  edited: false
  hidden: false
  id: 64dae0ede42fba08b89c5ccf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/LaSNcrKmCEUBEBZZCce3k.png?w=200&h=200&f=face
      fullname: Sung Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hunkim
      type: user
    createdAt: '2023-08-15T02:28:03.000Z'
    data:
      edited: false
      editors:
      - hunkim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9292709827423096
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/LaSNcrKmCEUBEBZZCce3k.png?w=200&h=200&f=face
          fullname: Sung Kim
          isHf: false
          isPro: false
          name: hunkim
          type: user
        html: '<p>We will soon host our model on Together.ai. We will keep you updated.</p>

          <p>Do you know how to integrate our model with <a rel="nofollow" href="https://github.com/BerriAI/litellm">https://github.com/BerriAI/litellm</a>?
          We will make it work. Let us know.</p>

          '
        raw: 'We will soon host our model on Together.ai. We will keep you updated.


          Do you know how to integrate our model with https://github.com/BerriAI/litellm?
          We will make it work. Let us know.

          '
        updatedAt: '2023-08-15T02:28:03.265Z'
      numEdits: 0
      reactions: []
    id: 64dae2b3a1182ee16c3d85fc
    type: comment
  author: hunkim
  content: 'We will soon host our model on Together.ai. We will keep you updated.


    Do you know how to integrate our model with https://github.com/BerriAI/litellm?
    We will make it work. Let us know.

    '
  created_at: 2023-08-15 01:28:03+00:00
  edited: false
  hidden: false
  id: 64dae2b3a1182ee16c3d85fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40c021efc35b828bf680444905affb11.svg
      fullname: Krrish Dholakia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krrish-litellm
      type: user
    createdAt: '2023-08-15T17:29:20.000Z'
    data:
      edited: true
      editors:
      - krrish-litellm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8469830751419067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40c021efc35b828bf680444905affb11.svg
          fullname: Krrish Dholakia
          isHf: false
          isPro: false
          name: krrish-litellm
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;hunkim&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hunkim\">@<span class=\"\
          underline\">hunkim</span></a></span>\n\n\t</span></span> we made it easy\
          \ to proxy openai with any deployment solution - should unlock any provider\
          \ you choose. - <a rel=\"nofollow\" href=\"https://github.com/BerriAI/litellm/issues/120\"\
          >https://github.com/BerriAI/litellm/issues/120</a></p>\n<pre><code>import\
          \ litellm \ndef translate_function(model, messages, max_tokens):\n    prompt\
          \ = \" \".join(message[\"content\"] for message in messages)\n    max_new_tokens\
          \ = max_tokens\n    return {\"model\": model, \"prompt\": prompt, \"max_new_tokens\"\
          : max_new_tokens} \n\nopenai.api_base = litellm.translate_api_call(custom_api_base,\
          \ translate_function)\n</code></pre>\n<p>We already have a custom integration\
          \ with together.ai, which supports streaming. Excited to put out a demo\
          \ notebook/etc. once it's deployed. </p>\n"
        raw: "Hey @hunkim we made it easy to proxy openai with any deployment solution\
          \ - should unlock any provider you choose. - https://github.com/BerriAI/litellm/issues/120\n\
          ```\nimport litellm \ndef translate_function(model, messages, max_tokens):\n\
          \tprompt = \" \".join(message[\"content\"] for message in messages)\n\t\
          max_new_tokens = max_tokens\n\treturn {\"model\": model, \"prompt\": prompt,\
          \ \"max_new_tokens\": max_new_tokens} \n\nopenai.api_base = litellm.translate_api_call(custom_api_base,\
          \ translate_function)\n```\n\nWe already have a custom integration with\
          \ together.ai, which supports streaming. Excited to put out a demo notebook/etc.\
          \ once it's deployed. "
        updatedAt: '2023-08-15T17:32:48.964Z'
      numEdits: 2
      reactions: []
    id: 64dbb5f000b80a024c72230a
    type: comment
  author: krrish-litellm
  content: "Hey @hunkim we made it easy to proxy openai with any deployment solution\
    \ - should unlock any provider you choose. - https://github.com/BerriAI/litellm/issues/120\n\
    ```\nimport litellm \ndef translate_function(model, messages, max_tokens):\n\t\
    prompt = \" \".join(message[\"content\"] for message in messages)\n\tmax_new_tokens\
    \ = max_tokens\n\treturn {\"model\": model, \"prompt\": prompt, \"max_new_tokens\"\
    : max_new_tokens} \n\nopenai.api_base = litellm.translate_api_call(custom_api_base,\
    \ translate_function)\n```\n\nWe already have a custom integration with together.ai,\
    \ which supports streaming. Excited to put out a demo notebook/etc. once it's\
    \ deployed. "
  created_at: 2023-08-15 16:29:20+00:00
  edited: true
  hidden: false
  id: 64dbb5f000b80a024c72230a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: upstage/SOLAR-0-70b-16bit
repo_type: model
status: open
target_branch: null
title: Call w/ LiteLLM
