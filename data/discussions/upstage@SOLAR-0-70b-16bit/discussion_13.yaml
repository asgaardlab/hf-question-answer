!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yerrramsetty
conflicting_files: null
created_at: 2023-09-03 16:21:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d62f271d017c92a1d3144586f574ec2.svg
      fullname: Sai Krishna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yerrramsetty
      type: user
    createdAt: '2023-09-03T17:21:23.000Z'
    data:
      edited: false
      editors:
      - Yerrramsetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4227857291698456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d62f271d017c92a1d3144586f574ec2.svg
          fullname: Sai Krishna
          isHf: false
          isPro: false
          name: Yerrramsetty
          type: user
        html: '<p>torchrun --nproc_per_node 1 example_text_completion.py     --ckpt_dir
          llama-2-7b/     --tokenizer_path tokenizer.model     --max_seq_len 128 --max_batch_size
          4<br>NOTE: Redirects are currently not supported in Windows or MacOs.<br>Traceback
          (most recent call last):<br>  File "/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py",
          line 69, in <br>    fire.Fire(main)<br>  File "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py",
          line 141, in Fire<br>    component_trace = _Fire(component, args, parsed_flag_args,
          context, name)<br>                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py",
          line 466, in _Fire<br>    component, remaining_args = _CallAndUpdateTrace(<br>                                ^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py",
          line 681, in _CallAndUpdateTrace<br>    component = fn(*varargs, **kwargs)<br>                ^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py",
          line 32, in main<br>    generator = Llama.build(<br>                ^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/llama2/llama/llama/generation.py", line
          84, in build<br>    torch.distributed.init_process_group("nccl")<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py",
          line 907, in init_process_group<br>    default_pg = _new_process_group_helper(<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py",
          line 1013, in _new_process_group_helper<br>    raise RuntimeError("Distributed
          package doesn''t have NCCL " "built in")<br>RuntimeError: Distributed package
          doesn''t have NCCL built in<br>ERROR:torch.distributed.elastic.multiprocessing.api:failed
          (exitcode: 1) local_rank: 0 (pid: 74324) of binary: /Users/jayasaikrishnayerramsetty/anaconda3/bin/python<br>Traceback
          (most recent call last):<br>  File "/Users/jayasaikrishnayerramsetty/anaconda3/bin/torchrun",
          line 33, in <br>    sys.exit(load_entry_point(''torch==2.0.1'', ''console_scripts'',
          ''torchrun'')())<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/<strong>init</strong>.py",
          line 346, in wrapper<br>    return f(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py",
          line 794, in main<br>    run(args)<br>  File "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py",
          line 785, in run<br>    elastic_launch(<br>  File "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py",
          line 134, in <strong>call</strong><br>    return launch_agent(self._config,
          self._entrypoint, list(args))<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py",
          line 250, in launch_agent<br>    raise ChildFailedError(<br>torch.distributed.elastic.multiprocessing.errors.ChildFailedError:<br>============================================================<br>example_text_completion.py
          FAILED</p>

          <hr>

          <p>Failures:<br>  </p>

          <hr>

          <p>Root Cause (first observed failure):<br>[0]:<br>  time      : 2023-09-04_02:18:55<br>  host      :
          1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa<br>  rank      :
          0 (local_rank: 0)<br>  exitcode  : 1 (pid: 74324)<br>  error_file: &lt;N/A&gt;<br>  traceback
          : To enable traceback see: <a rel="nofollow" href="https://pytorch.org/docs/stable/elastic/errors.html">https://pytorch.org/docs/stable/elastic/errors.html</a><br>============================================================</p>

          '
        raw: "torchrun --nproc_per_node 1 example_text_completion.py     --ckpt_dir\
          \ llama-2-7b/     --tokenizer_path tokenizer.model     --max_seq_len 128\
          \ --max_batch_size 4\r\nNOTE: Redirects are currently not supported in Windows\
          \ or MacOs.\r\nTraceback (most recent call last):\r\n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py\"\
          , line 69, in <module>\r\n    fire.Fire(main)\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
          , line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args,\
          \ context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
          , line 466, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\
          \n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
          , line 681, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\
          \n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py\"\
          , line 32, in main\r\n    generator = Llama.build(\r\n                ^^^^^^^^^^^^\r\
          \n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/llama/generation.py\"\
          , line 84, in build\r\n    torch.distributed.init_process_group(\"nccl\"\
          )\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\"\
          , line 907, in init_process_group\r\n    default_pg = _new_process_group_helper(\r\
          \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\"\
          , line 1013, in _new_process_group_helper\r\n    raise RuntimeError(\"Distributed\
          \ package doesn't have NCCL \" \"built in\")\r\nRuntimeError: Distributed\
          \ package doesn't have NCCL built in\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed\
          \ (exitcode: 1) local_rank: 0 (pid: 74324) of binary: /Users/jayasaikrishnayerramsetty/anaconda3/bin/python\r\
          \nTraceback (most recent call last):\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/bin/torchrun\"\
          , line 33, in <module>\r\n    sys.exit(load_entry_point('torch==2.0.1',\
          \ 'console_scripts', 'torchrun')())\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\
          , line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py\"\
          , line 794, in main\r\n    run(args)\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py\"\
          , line 785, in run\r\n    elastic_launch(\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\"\
          , line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint,\
          \ list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\"\
          , line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\
          \ \r\n============================================================\r\nexample_text_completion.py\
          \ FAILED\r\n------------------------------------------------------------\r\
          \nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\
          \nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-09-04_02:18:55\r\
          \n  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa\r\
          \n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 74324)\r\n  error_file:\
          \ <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\
          \n============================================================"
        updatedAt: '2023-09-03T17:21:23.548Z'
      numEdits: 0
      reactions: []
    id: 64f4c09319c8ca0cbd7e1099
    type: comment
  author: Yerrramsetty
  content: "torchrun --nproc_per_node 1 example_text_completion.py     --ckpt_dir\
    \ llama-2-7b/     --tokenizer_path tokenizer.model     --max_seq_len 128 --max_batch_size\
    \ 4\r\nNOTE: Redirects are currently not supported in Windows or MacOs.\r\nTraceback\
    \ (most recent call last):\r\n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py\"\
    , line 69, in <module>\r\n    fire.Fire(main)\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
    , line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args,\
    \ context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
    , line 466, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\
    \n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/fire/core.py\"\
    , line 681, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\
    \n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/example_text_completion.py\"\
    , line 32, in main\r\n    generator = Llama.build(\r\n                ^^^^^^^^^^^^\r\
    \n  File \"/Users/jayasaikrishnayerramsetty/llama2/llama/llama/generation.py\"\
    , line 84, in build\r\n    torch.distributed.init_process_group(\"nccl\")\r\n\
    \  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\"\
    , line 907, in init_process_group\r\n    default_pg = _new_process_group_helper(\r\
    \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\"\
    , line 1013, in _new_process_group_helper\r\n    raise RuntimeError(\"Distributed\
    \ package doesn't have NCCL \" \"built in\")\r\nRuntimeError: Distributed package\
    \ doesn't have NCCL built in\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed\
    \ (exitcode: 1) local_rank: 0 (pid: 74324) of binary: /Users/jayasaikrishnayerramsetty/anaconda3/bin/python\r\
    \nTraceback (most recent call last):\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/bin/torchrun\"\
    , line 33, in <module>\r\n    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts',\
    \ 'torchrun')())\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\
    , line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py\"\
    , line 794, in main\r\n    run(args)\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py\"\
    , line 785, in run\r\n    elastic_launch(\r\n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\"\
    , line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint,\
    \ list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/Users/jayasaikrishnayerramsetty/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\"\
    , line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\
    \ \r\n============================================================\r\nexample_text_completion.py\
    \ FAILED\r\n------------------------------------------------------------\r\nFailures:\r\
    \n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\
    \nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-09-04_02:18:55\r\
    \n  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa\r\
    \n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 74324)\r\n  error_file:\
    \ <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\
    \n============================================================"
  created_at: 2023-09-03 16:21:23+00:00
  edited: false
  hidden: false
  id: 64f4c09319c8ca0cbd7e1099
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d62f271d017c92a1d3144586f574ec2.svg
      fullname: Sai Krishna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yerrramsetty
      type: user
    createdAt: '2023-09-04T07:12:16.000Z'
    data:
      edited: false
      editors:
      - Yerrramsetty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9929467439651489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d62f271d017c92a1d3144586f574ec2.svg
          fullname: Sai Krishna
          isHf: false
          isPro: false
          name: Yerrramsetty
          type: user
        html: '<p>Any response is much appcreiated</p>

          '
        raw: 'Any response is much appcreiated

          '
        updatedAt: '2023-09-04T07:12:16.301Z'
      numEdits: 0
      reactions: []
    id: 64f583509b5793eb2f448139
    type: comment
  author: Yerrramsetty
  content: 'Any response is much appcreiated

    '
  created_at: 2023-09-04 06:12:16+00:00
  edited: false
  hidden: false
  id: 64f583509b5793eb2f448139
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: upstage/SOLAR-0-70b-16bit
repo_type: model
status: open
target_branch: null
title: WHy cant i use  LLama2 in MacOS Ventura 10.14
