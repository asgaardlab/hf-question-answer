!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SLywnow
conflicting_files: null
created_at: 2023-05-30 06:25:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/749f8408cfd082bfeb1bf13248dc4d5b.svg
      fullname: SLywnow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SLywnow
      type: user
    createdAt: '2023-05-30T07:25:54.000Z'
    data:
      edited: false
      editors:
      - SLywnow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/749f8408cfd082bfeb1bf13248dc4d5b.svg
          fullname: SLywnow
          isHf: false
          isPro: false
          name: SLywnow
          type: user
        html: '<p>Model loads normally, but when trying to gen smth return KeyError:
          model.layers.0.self_attn.rotary_emb.cos_cached<br>Running in 13 layers GPU
          and 19 CPU<br>Happens only with this model</p>

          <p>Full error:<br>ERROR      | <strong>main</strong>:generate:3895 - Traceback
          (most recent call last):<br>  File "aiserver.py", line 3882, in generate<br>    genout,
          already_generated = tpool.execute(model.core_generate, txt, found_entries)<br>  File
          "B:\python\lib\site-packages\eventlet\tpool.py", line 132, in execute<br>    six.reraise(c,
          e, tb)<br>  File "B:\python\lib\site-packages\six.py", line 719, in reraise<br>    raise
          value<br>  File "B:\python\lib\site-packages\eventlet\tpool.py", line 86,
          in tworker<br>    rv = meth(*args, **kwargs)<br>  File "D:\AI\KoboldAI\modeling\inference_model.py",
          line 332, in core_generate<br>    result = self.raw_generate(<br>  File
          "D:\AI\KoboldAI\modeling\inference_model.py", line 579, in raw_generate<br>    result
          = self._raw_generate(<br>  File "D:\AI\KoboldAI\modeling\inference_models\hf_torch.py",
          line 250, in _raw_generate<br>    genout = self.model.generate(<br>  File
          "B:\python\lib\site-packages\torch\utils_contextlib.py", line 115, in decorate_context<br>    return
          func(*args, **kwargs)<br>  File "B:\python\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "D:\AI\KoboldAI\modeling\inference_models\hf_torch.py",
          line 218, in new_sample<br>    return new_sample.old_sample(self, *args,
          **kwargs)<br>  File "B:\python\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "B:\python\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "B:\python\lib\site-packages\accelerate\hooks.py", line 165, in new_forward<br>    output
          = old_forward(*args, **kwargs)<br>  File "B:\python\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "B:\python\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "B:\python\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "B:\python\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "B:\python\lib\site-packages\accelerate\hooks.py", line 165, in new_forward<br>    output
          = old_forward(*args, **kwargs)<br>  File "B:\python\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "B:\python\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "B:\python\lib\site-packages\accelerate\hooks.py", line 165, in new_forward<br>    output
          = old_forward(*args, **kwargs)<br>  File "B:\python\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 203, in forward<br>    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)<br>  File
          "B:\python\lib\site-packages\torch\nn\modules\module.py", line 1501, in
          _call_impl<br>    return forward_call(*args, **kwargs)<br>  File "B:\python\lib\site-packages\accelerate\hooks.py",
          line 160, in new_forward<br>    args, kwargs = module._hf_hook.pre_forward(module,
          *args, **kwargs)<br>  File "B:\python\lib\site-packages\accelerate\hooks.py",
          line 280, in pre_forward<br>    set_module_tensor_to_device(module, name,
          self.execution_device, value=self.weights_map[name])<br>  File "B:\python\lib\site-packages\accelerate\utils\offload.py",
          line 123, in <strong>getitem</strong><br>    return self.dataset[f"{self.prefix}{key}"]<br>  File
          "B:\python\lib\site-packages\accelerate\utils\offload.py", line 170, in
          <strong>getitem</strong><br>    weight_info = self.index[key]<br>KeyError:
          ''model.layers.0.self_attn.rotary_emb.cos_cached''</p>

          '
        raw: "Model loads normally, but when trying to gen smth return KeyError: model.layers.0.self_attn.rotary_emb.cos_cached\r\
          \nRunning in 13 layers GPU and 19 CPU\r\nHappens only with this model\r\n\
          \r\nFull error:\r\nERROR      | __main__:generate:3895 - Traceback (most\
          \ recent call last):\r\n  File \"aiserver.py\", line 3882, in generate\r\
          \n    genout, already_generated = tpool.execute(model.core_generate, txt,\
          \ found_entries)\r\n  File \"B:\\python\\lib\\site-packages\\eventlet\\\
          tpool.py\", line 132, in execute\r\n    six.reraise(c, e, tb)\r\n  File\
          \ \"B:\\python\\lib\\site-packages\\six.py\", line 719, in reraise\r\n \
          \   raise value\r\n  File \"B:\\python\\lib\\site-packages\\eventlet\\tpool.py\"\
          , line 86, in tworker\r\n    rv = meth(*args, **kwargs)\r\n  File \"D:\\\
          AI\\KoboldAI\\modeling\\inference_model.py\", line 332, in core_generate\r\
          \n    result = self.raw_generate(\r\n  File \"D:\\AI\\KoboldAI\\modeling\\\
          inference_model.py\", line 579, in raw_generate\r\n    result = self._raw_generate(\r\
          \n  File \"D:\\AI\\KoboldAI\\modeling\\inference_models\\hf_torch.py\",\
          \ line 250, in _raw_generate\r\n    genout = self.model.generate(\r\n  File\
          \ \"B:\\python\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line\
          \ 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
          \ \"B:\\python\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 1485, in generate\r\n    return self.sample(\r\n  File \"D:\\AI\\\
          KoboldAI\\modeling\\inference_models\\hf_torch.py\", line 218, in new_sample\r\
          \n    return new_sample.old_sample(self, *args, **kwargs)\r\n  File \"B:\\\
          python\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524,\
          \ in sample\r\n    outputs = self(\r\n  File \"B:\\python\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args,\
          \ **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\\
          llama\\modeling_llama.py\", line 687, in forward\r\n    outputs = self.model(\r\
          \n  File \"B:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 577, in forward\r\n    layer_outputs = decoder_layer(\r\
          \n  File \"B:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"B:\\python\\lib\\site-packages\\accelerate\\hooks.py\", line\
          \ 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n \
          \ File \"B:\\python\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 292, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"B:\\python\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 203, in forward\r\n    cos, sin = self.rotary_emb(value_states,\
          \ seq_len=kv_seq_len)\r\n  File \"B:\\python\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\hooks.py\"\
          , line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module,\
          \ *args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\\
          hooks.py\", line 280, in pre_forward\r\n    set_module_tensor_to_device(module,\
          \ name, self.execution_device, value=self.weights_map[name])\r\n  File \"\
          B:\\python\\lib\\site-packages\\accelerate\\utils\\offload.py\", line 123,\
          \ in __getitem__\r\n    return self.dataset[f\"{self.prefix}{key}\"]\r\n\
          \  File \"B:\\python\\lib\\site-packages\\accelerate\\utils\\offload.py\"\
          , line 170, in __getitem__\r\n    weight_info = self.index[key]\r\nKeyError:\
          \ 'model.layers.0.self_attn.rotary_emb.cos_cached'"
        updatedAt: '2023-05-30T07:25:54.956Z'
      numEdits: 0
      reactions: []
    id: 6475a50237ed88a749e59923
    type: comment
  author: SLywnow
  content: "Model loads normally, but when trying to gen smth return KeyError: model.layers.0.self_attn.rotary_emb.cos_cached\r\
    \nRunning in 13 layers GPU and 19 CPU\r\nHappens only with this model\r\n\r\n\
    Full error:\r\nERROR      | __main__:generate:3895 - Traceback (most recent call\
    \ last):\r\n  File \"aiserver.py\", line 3882, in generate\r\n    genout, already_generated\
    \ = tpool.execute(model.core_generate, txt, found_entries)\r\n  File \"B:\\python\\\
    lib\\site-packages\\eventlet\\tpool.py\", line 132, in execute\r\n    six.reraise(c,\
    \ e, tb)\r\n  File \"B:\\python\\lib\\site-packages\\six.py\", line 719, in reraise\r\
    \n    raise value\r\n  File \"B:\\python\\lib\\site-packages\\eventlet\\tpool.py\"\
    , line 86, in tworker\r\n    rv = meth(*args, **kwargs)\r\n  File \"D:\\AI\\KoboldAI\\\
    modeling\\inference_model.py\", line 332, in core_generate\r\n    result = self.raw_generate(\r\
    \n  File \"D:\\AI\\KoboldAI\\modeling\\inference_model.py\", line 579, in raw_generate\r\
    \n    result = self._raw_generate(\r\n  File \"D:\\AI\\KoboldAI\\modeling\\inference_models\\\
    hf_torch.py\", line 250, in _raw_generate\r\n    genout = self.model.generate(\r\
    \n  File \"B:\\python\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line\
    \ 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"B:\\\
    python\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485, in\
    \ generate\r\n    return self.sample(\r\n  File \"D:\\AI\\KoboldAI\\modeling\\\
    inference_models\\hf_torch.py\", line 218, in new_sample\r\n    return new_sample.old_sample(self,\
    \ *args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2524, in sample\r\n    outputs = self(\r\n  File\
    \ \"B:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"B:\\python\\\
    lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n    output\
    \ = old_forward(*args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\r\n   \
    \ outputs = self.model(\r\n  File \"B:\\python\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\transformers\\models\\\
    llama\\modeling_llama.py\", line 577, in forward\r\n    layer_outputs = decoder_layer(\r\
    \n  File \"B:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    B:\\python\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\
    \n    output = old_forward(*args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\r\n   \
    \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File\
    \ \"B:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"B:\\python\\\
    lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n    output\
    \ = old_forward(*args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 203, in forward\r\n   \
    \ cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\r\n  File \"B:\\\
    python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module,\
    \ *args, **kwargs)\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\hooks.py\"\
    , line 280, in pre_forward\r\n    set_module_tensor_to_device(module, name, self.execution_device,\
    \ value=self.weights_map[name])\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\\
    utils\\offload.py\", line 123, in __getitem__\r\n    return self.dataset[f\"{self.prefix}{key}\"\
    ]\r\n  File \"B:\\python\\lib\\site-packages\\accelerate\\utils\\offload.py\"\
    , line 170, in __getitem__\r\n    weight_info = self.index[key]\r\nKeyError: 'model.layers.0.self_attn.rotary_emb.cos_cached'"
  created_at: 2023-05-30 06:25:54+00:00
  edited: false
  hidden: false
  id: 6475a50237ed88a749e59923
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60f3190351aac783d4c7b41923b8b3f4.svg
      fullname: '782'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeonNolan
      type: user
    createdAt: '2023-06-13T05:24:18.000Z'
    data:
      edited: false
      editors:
      - LeonNolan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7846764326095581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60f3190351aac783d4c7b41923b8b3f4.svg
          fullname: '782'
          isHf: false
          isPro: false
          name: LeonNolan
          type: user
        html: '<p>I believe KoboldAI has problems running Pygmalion 7B+ due to it
          being in llama. Try using oobabooga webui.</p>

          '
        raw: I believe KoboldAI has problems running Pygmalion 7B+ due to it being
          in llama. Try using oobabooga webui.
        updatedAt: '2023-06-13T05:24:18.223Z'
      numEdits: 0
      reactions: []
    id: 6487fd8219539cd510949715
    type: comment
  author: LeonNolan
  content: I believe KoboldAI has problems running Pygmalion 7B+ due to it being in
    llama. Try using oobabooga webui.
  created_at: 2023-06-13 04:24:18+00:00
  edited: false
  hidden: false
  id: 6487fd8219539cd510949715
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Neko-Institute-of-Science/pygmalion-7b
repo_type: model
status: open
target_branch: null
title: Error in KoboldAI
