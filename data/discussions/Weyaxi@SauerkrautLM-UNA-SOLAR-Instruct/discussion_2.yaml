!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adaboese
conflicting_files: null
created_at: 2023-12-27 01:35:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e19aba3714dfb3def9810d79da47794.svg
      fullname: Ada Boese
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adaboese
      type: user
    createdAt: '2023-12-27T01:35:30.000Z'
    data:
      edited: false
      editors:
      - adaboese
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9327272176742554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e19aba3714dfb3def9810d79da47794.svg
          fullname: Ada Boese
          isHf: false
          isPro: false
          name: adaboese
          type: user
        html: '<p>This is newbie question, but I am browsing top models listed on
          <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>,
          and the main thing that I cannot answer is how do I know what kind of machine
          I need to run the model, how fast it will be and how expensive?</p>

          '
        raw: This is newbie question, but I am browsing top models listed on https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
          and the main thing that I cannot answer is how do I know what kind of machine
          I need to run the model, how fast it will be and how expensive?
        updatedAt: '2023-12-27T01:35:30.038Z'
      numEdits: 0
      reactions: []
    id: 658b7f62f3ef2bd6b75080b8
    type: comment
  author: adaboese
  content: This is newbie question, but I am browsing top models listed on https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
    and the main thing that I cannot answer is how do I know what kind of machine
    I need to run the model, how fast it will be and how expensive?
  created_at: 2023-12-27 01:35:30+00:00
  edited: false
  hidden: false
  id: 658b7f62f3ef2bd6b75080b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6468ce47e134d050a58aa89c/ApFcPlOzgI6Cjr0SYPpk6.png?w=200&h=200&f=face
      fullname: "Ya\u011F\u0131z \xC7al\u0131k"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Weyaxi
      type: user
    createdAt: '2023-12-27T10:27:39.000Z'
    data:
      edited: true
      editors:
      - Weyaxi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459044337272644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6468ce47e134d050a58aa89c/ApFcPlOzgI6Cjr0SYPpk6.png?w=200&h=200&f=face
          fullname: "Ya\u011F\u0131z \xC7al\u0131k"
          isHf: false
          isPro: false
          name: Weyaxi
          type: user
        html: '<p>Hi,</p>

          <p>If you load it with normal precision, it should take around 25 GB vram
          or ram if you are loading to cpu. However, you can load the model in 8 or
          4 bits to reduce this amount. There are also quantized models available
          for this purpose.</p>

          '
        raw: 'Hi,


          If you load it with normal precision, it should take around 25 GB vram or
          ram if you are loading to cpu. However, you can load the model in 8 or 4
          bits to reduce this amount. There are also quantized models available for
          this purpose.'
        updatedAt: '2023-12-27T10:27:59.380Z'
      numEdits: 1
      reactions: []
    id: 658bfc1b4dd2ca8497e2fa13
    type: comment
  author: Weyaxi
  content: 'Hi,


    If you load it with normal precision, it should take around 25 GB vram or ram
    if you are loading to cpu. However, you can load the model in 8 or 4 bits to reduce
    this amount. There are also quantized models available for this purpose.'
  created_at: 2023-12-27 10:27:39+00:00
  edited: true
  hidden: false
  id: 658bfc1b4dd2ca8497e2fa13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6468ce47e134d050a58aa89c/ApFcPlOzgI6Cjr0SYPpk6.png?w=200&h=200&f=face
      fullname: "Ya\u011F\u0131z \xC7al\u0131k"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Weyaxi
      type: user
    createdAt: '2024-01-11T16:06:21.000Z'
    data:
      status: closed
    id: 65a011fdb430dfb7e09bf8b2
    type: status-change
  author: Weyaxi
  created_at: 2024-01-11 16:06:21+00:00
  id: 65a011fdb430dfb7e09bf8b2
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct
repo_type: model
status: closed
target_branch: null
title: How do I know what kind of machine do I need to run this model?
