!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hkhho
conflicting_files: null
created_at: 2023-04-24 04:06:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de3980fc346818cd58ee77caa98da9a8.svg
      fullname: ho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hkhho
      type: user
    createdAt: '2023-04-24T05:06:38.000Z'
    data:
      edited: false
      editors:
      - hkhho
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de3980fc346818cd58ee77caa98da9a8.svg
          fullname: ho
          isHf: false
          isPro: false
          name: hkhho
          type: user
        html: '<p>Thank you for sharing this amazing model! I''m new to NLP model
          so I am a bit lost, hopefully can get some helps here.</p>

          <p>I would like to fine tune this model but my case is a bit complicated.
          I want to performance classification task. Given pair of genomic sequence
          (Sequence A and Sequence B) in fasta format, and I have 3 classes based
          on the sequence matching.</p>

          <p>My thought is to fit sequence A to gena-lm to get outputA, and then sequence
          B to gena-lm to get outputB. Then concat the output from the two gena-lm
          then make final prediction.  Does it sound doable or too compicated?</p>

          <p>And for the input and label, would this model work if my sequence and
          output like the one below?</p>

          <p>inputs = tokenizer("TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA",
          return_tensors="pt")<br>labels = tokenizer(1, return_tensors="pt")["input_ids"]</p>

          '
        raw: "Thank you for sharing this amazing model! I'm new to NLP model so I\
          \ am a bit lost, hopefully can get some helps here.\r\n\r\nI would like\
          \ to fine tune this model but my case is a bit complicated. I want to performance\
          \ classification task. Given pair of genomic sequence (Sequence A and Sequence\
          \ B) in fasta format, and I have 3 classes based on the sequence matching.\r\
          \n\r\nMy thought is to fit sequence A to gena-lm to get outputA, and then\
          \ sequence B to gena-lm to get outputB. Then concat the output from the\
          \ two gena-lm then make final prediction.  Does it sound doable or too compicated?\r\
          \n\r\nAnd for the input and label, would this model work if my sequence\
          \ and output like the one below?\r\n\r\ninputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\"\
          , return_tensors=\"pt\")\r\nlabels = tokenizer(1, return_tensors=\"pt\"\
          )[\"input_ids\"]"
        updatedAt: '2023-04-24T05:06:38.426Z'
      numEdits: 0
      reactions: []
    id: 64460e5ee1fd8d65b27e569e
    type: comment
  author: hkhho
  content: "Thank you for sharing this amazing model! I'm new to NLP model so I am\
    \ a bit lost, hopefully can get some helps here.\r\n\r\nI would like to fine tune\
    \ this model but my case is a bit complicated. I want to performance classification\
    \ task. Given pair of genomic sequence (Sequence A and Sequence B) in fasta format,\
    \ and I have 3 classes based on the sequence matching.\r\n\r\nMy thought is to\
    \ fit sequence A to gena-lm to get outputA, and then sequence B to gena-lm to\
    \ get outputB. Then concat the output from the two gena-lm then make final prediction.\
    \  Does it sound doable or too compicated?\r\n\r\nAnd for the input and label,\
    \ would this model work if my sequence and output like the one below?\r\n\r\n\
    inputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\"\
    , return_tensors=\"pt\")\r\nlabels = tokenizer(1, return_tensors=\"pt\")[\"input_ids\"\
    ]"
  created_at: 2023-04-24 04:06:38+00:00
  edited: false
  hidden: false
  id: 64460e5ee1fd8d65b27e569e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4a9394057dc0813880e4a567293f34f.svg
      fullname: Yury Kuratov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yurakuratov
      type: user
    createdAt: '2023-04-27T21:51:02.000Z'
    data:
      edited: true
      editors:
      - yurakuratov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4a9394057dc0813880e4a567293f34f.svg
          fullname: Yury Kuratov
          isHf: false
          isPro: false
          name: yurakuratov
          type: user
        html: '<p>You can use sequence classification model for both single sequence
          and pairs of sequences:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, BigBirdForSequenceClassification


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">''AIRI-Institute/gena-lm-bigbird-base-t2t''</span>)

          model = BigBirdForSequenceClassification.from_pretrained(<span class="hljs-string">''AIRI-Institute/gena-lm-bigbird-base-t2t''</span>)

          </code></pre>

          <p>and feed two sequences to model at once separated by special token <code>[SEP]</code>.
          It should form input <code>[CLS] SeqA [SEP] SeqB [SEP]</code>:</p>

          <pre><code class="language-python">inp = tokenizer(<span class="hljs-string">''ATGC
          [SEP] GCTA''</span>)

          </code></pre>

          <p>This HF doc page might help you to setup training: <a href="https://huggingface.co/docs/transformers/tasks/sequence_classification">https://huggingface.co/docs/transformers/tasks/sequence_classification</a></p>

          '
        raw: 'You can use sequence classification model for both single sequence and
          pairs of sequences:

          ```python

          from transformers import AutoTokenizer, BigBirdForSequenceClassification


          tokenizer = AutoTokenizer.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')

          model = BigBirdForSequenceClassification.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')

          ```


          and feed two sequences to model at once separated by special token `[SEP]`.
          It should form input `[CLS] SeqA [SEP] SeqB [SEP]`:

          ```python

          inp = tokenizer(''ATGC [SEP] GCTA'')

          ```

          This HF doc page might help you to setup training: https://huggingface.co/docs/transformers/tasks/sequence_classification'
        updatedAt: '2023-04-27T21:51:49.105Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - hkhho
        - MiriaRafante
    id: 644aee46f9f1b0cd3d92027e
    type: comment
  author: yurakuratov
  content: 'You can use sequence classification model for both single sequence and
    pairs of sequences:

    ```python

    from transformers import AutoTokenizer, BigBirdForSequenceClassification


    tokenizer = AutoTokenizer.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')

    model = BigBirdForSequenceClassification.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')

    ```


    and feed two sequences to model at once separated by special token `[SEP]`. It
    should form input `[CLS] SeqA [SEP] SeqB [SEP]`:

    ```python

    inp = tokenizer(''ATGC [SEP] GCTA'')

    ```

    This HF doc page might help you to setup training: https://huggingface.co/docs/transformers/tasks/sequence_classification'
  created_at: 2023-04-27 20:51:02+00:00
  edited: true
  hidden: false
  id: 644aee46f9f1b0cd3d92027e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de3980fc346818cd58ee77caa98da9a8.svg
      fullname: ho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hkhho
      type: user
    createdAt: '2023-04-28T06:13:40.000Z'
    data:
      edited: true
      editors:
      - hkhho
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de3980fc346818cd58ee77caa98da9a8.svg
          fullname: ho
          isHf: false
          isPro: false
          name: hkhho
          type: user
        html: '<p>Thank you for replying! It''s good to know we can use pairs of sequences
          as input.</p>

          <p>I tried the code below and returned KeyError.</p>

          <p>from transformers import AutoTokenizer, BigBirdForSequenceClassification<br>tokenizer
          = AutoTokenizer.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')<br>model
          = BigBirdForSequenceClassification.from_pretrained(''AIRI-Institute/gena-lm-bigbird-base-t2t'')<br>inputs
          = tokenizer("TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA
          [SEP] ACCATTTTTGTTCTCACGACA [SEP]", return_tensors="pt")<br>model(inputs)</p>

          <hr>

          <p>KeyError                                  Traceback (most recent call
          last)<br>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py
          in <strong>getattr</strong>(self, item)<br>    247         try:<br>--&gt;
          248             return self.data[item]<br>    249         except KeyError:</p>

          <p>KeyError: ''size''</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>AttributeError                            Traceback (most recent call
          last)<br>5 frames<br> in &lt;cell line: 2&gt;()<br>      1 inputs = tokenizer("TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA
          [SEP] ACCATTTTTGTTCTCACGACA [SEP]", return_tensors="pt")<br>----&gt; 2 model(inputs)</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py
          in forward(self, input_ids, attention_mask, token_type_ids, position_ids,
          head_mask, inputs_embeds, labels, output_attentions, output_hidden_states,
          return_dict)<br>   2743         return_dict = return_dict if return_dict
          is not None else self.config.use_return_dict<br>   2744<br>-&gt; 2745         outputs
          = self.bert(<br>   2746             input_ids,<br>   2747             attention_mask=attention_mask,</p>

          <p>/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in
          _call_impl(self, *args, **kwargs)<br>   1499                 or _global_backward_pre_hooks
          or _global_backward_hooks<br>   1500                 or _global_forward_hooks
          or _global_forward_pre_hooks):<br>-&gt; 1501             return forward_call(*args,
          **kwargs)<br>   1502         # Do not call functions when jit is used<br>   1503         full_backward_hooks,
          non_full_backward_hooks = [], []</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py
          in forward(self, input_ids, attention_mask, token_type_ids, position_ids,
          head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask,
          past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)<br>   2029             raise
          ValueError("You cannot specify both input_ids and inputs_embeds at the same
          time")<br>   2030         elif input_ids is not None:<br>-&gt; 2031             input_shape
          = input_ids.size()<br>   2032         elif inputs_embeds is not None:<br>   2033             input_shape
          = inputs_embeds.size()[:-1]</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py
          in <strong>getattr</strong>(self, item)<br>    248             return self.data[item]<br>    249         except
          KeyError:<br>--&gt; 250             raise AttributeError<br>    251<br>    252     def
          <strong>getstate</strong>(self):</p>

          <p>AttributeError:</p>

          '
        raw: "Thank you for replying! It's good to know we can use pairs of sequences\
          \ as input.\n\nI tried the code below and returned KeyError.\n\nfrom transformers\
          \ import AutoTokenizer, BigBirdForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bigbird-base-t2t')\n\
          model = BigBirdForSequenceClassification.from_pretrained('AIRI-Institute/gena-lm-bigbird-base-t2t')\n\
          inputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\
          \ [SEP] ACCATTTTTGTTCTCACGACA [SEP]\", return_tensors=\"pt\")\nmodel(inputs)\n\
          \n---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in __getattr__(self, item)\n    247         try:\n--> 248            \
          \ return self.data[item]\n    249         except KeyError:\n\nKeyError:\
          \ 'size'\n\nDuring handling of the above exception, another exception occurred:\n\
          \nAttributeError                            Traceback (most recent call\
          \ last)\n5 frames\n<ipython-input-51-993f17e29a65> in <cell line: 2>()\n\
          \      1 inputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\
          \ [SEP] ACCATTTTTGTTCTCACGACA [SEP]\", return_tensors=\"pt\")\n----> 2 model(inputs)\n\
          \n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in\
          \ _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\n-> 1501             return forward_call(*args,\
          \ **kwargs)\n   1502         # Do not call functions when jit is used\n\
          \   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\
          \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids,\
          \ head_mask, inputs_embeds, labels, output_attentions, output_hidden_states,\
          \ return_dict)\n   2743         return_dict = return_dict if return_dict\
          \ is not None else self.config.use_return_dict\n   2744 \n-> 2745      \
          \   outputs = self.bert(\n   2746             input_ids,\n   2747      \
          \       attention_mask=attention_mask,\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks\
          \ or _global_backward_hooks\n   1500                 or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\n-> 1501             return forward_call(*args,\
          \ **kwargs)\n   1502         # Do not call functions when jit is used\n\
          \   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\
          \n/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\
          \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids,\
          \ head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask,\
          \ past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n\
          \   2029             raise ValueError(\"You cannot specify both input_ids\
          \ and inputs_embeds at the same time\")\n   2030         elif input_ids\
          \ is not None:\n-> 2031             input_shape = input_ids.size()\n   2032\
          \         elif inputs_embeds is not None:\n   2033             input_shape\
          \ = inputs_embeds.size()[:-1]\n\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in __getattr__(self, item)\n    248             return self.data[item]\n\
          \    249         except KeyError:\n--> 250             raise AttributeError\n\
          \    251 \n    252     def __getstate__(self):\n\nAttributeError:"
        updatedAt: '2023-04-28T06:49:36.973Z'
      numEdits: 1
      reactions: []
    id: 644b6414840601ec7f156ea7
    type: comment
  author: hkhho
  content: "Thank you for replying! It's good to know we can use pairs of sequences\
    \ as input.\n\nI tried the code below and returned KeyError.\n\nfrom transformers\
    \ import AutoTokenizer, BigBirdForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bigbird-base-t2t')\n\
    model = BigBirdForSequenceClassification.from_pretrained('AIRI-Institute/gena-lm-bigbird-base-t2t')\n\
    inputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\
    \ [SEP] ACCATTTTTGTTCTCACGACA [SEP]\", return_tensors=\"pt\")\nmodel(inputs)\n\
    \n---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
    \ in __getattr__(self, item)\n    247         try:\n--> 248             return\
    \ self.data[item]\n    249         except KeyError:\n\nKeyError: 'size'\n\nDuring\
    \ handling of the above exception, another exception occurred:\n\nAttributeError\
    \                            Traceback (most recent call last)\n5 frames\n<ipython-input-51-993f17e29a65>\
    \ in <cell line: 2>()\n      1 inputs = tokenizer(\"TTTTAAAGACCAGATCAGTATTTTCTTGATACGCTTGTCACCATTTTTGTTCTCACGACA\
    \ [SEP] ACCATTTTTGTTCTCACGACA [SEP]\", return_tensors=\"pt\")\n----> 2 model(inputs)\n\
    \n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self,\
    \ *args, **kwargs)\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\n\
    \   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501             return forward_call(*args, **kwargs)\n   1502         # Do\
    \ not call functions when jit is used\n   1503         full_backward_hooks, non_full_backward_hooks\
    \ = [], []\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\
    \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask,\
    \ inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n\
    \   2743         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\
    \   2744 \n-> 2745         outputs = self.bert(\n   2746             input_ids,\n\
    \   2747             attention_mask=attention_mask,\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500                 or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\n-> 1501             return forward_call(*args,\
    \ **kwargs)\n   1502         # Do not call functions when jit is used\n   1503\
    \         full_backward_hooks, non_full_backward_hooks = [], []\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\
    \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask,\
    \ inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values,\
    \ use_cache, output_attentions, output_hidden_states, return_dict)\n   2029  \
    \           raise ValueError(\"You cannot specify both input_ids and inputs_embeds\
    \ at the same time\")\n   2030         elif input_ids is not None:\n-> 2031  \
    \           input_shape = input_ids.size()\n   2032         elif inputs_embeds\
    \ is not None:\n   2033             input_shape = inputs_embeds.size()[:-1]\n\n\
    /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
    \ in __getattr__(self, item)\n    248             return self.data[item]\n   \
    \ 249         except KeyError:\n--> 250             raise AttributeError\n   \
    \ 251 \n    252     def __getstate__(self):\n\nAttributeError:"
  created_at: 2023-04-28 05:13:40+00:00
  edited: true
  hidden: false
  id: 644b6414840601ec7f156ea7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4a9394057dc0813880e4a567293f34f.svg
      fullname: Yury Kuratov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yurakuratov
      type: user
    createdAt: '2023-04-28T09:29:44.000Z'
    data:
      edited: false
      editors:
      - yurakuratov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4a9394057dc0813880e4a567293f34f.svg
          fullname: Yury Kuratov
          isHf: false
          isPro: false
          name: yurakuratov
          type: user
        html: '<p>use</p>

          <pre><code class="language-python">model(**inputs)

          </code></pre>

          <p>to call the model.</p>

          '
        raw: 'use

          ```python

          model(**inputs)

          ```

          to call the model.'
        updatedAt: '2023-04-28T09:29:44.581Z'
      numEdits: 0
      reactions: []
    id: 644b9208bd080d314be7ff42
    type: comment
  author: yurakuratov
  content: 'use

    ```python

    model(**inputs)

    ```

    to call the model.'
  created_at: 2023-04-28 08:29:44+00:00
  edited: false
  hidden: false
  id: 644b9208bd080d314be7ff42
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: AIRI-Institute/gena-lm-bigbird-base-t2t
repo_type: model
status: open
target_branch: null
title: Fine tuning the model
