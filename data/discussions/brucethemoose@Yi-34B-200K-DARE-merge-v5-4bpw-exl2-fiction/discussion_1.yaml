!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ReMeDy-TV
conflicting_files: null
created_at: 2023-12-26 11:46:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640590b591ee7d7f287183ae/PQefPEgtaNqnTNfumrZXr.jpeg?w=200&h=200&f=face
      fullname: David Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReMeDy-TV
      type: user
    createdAt: '2023-12-26T11:46:50.000Z'
    data:
      edited: true
      editors:
      - ReMeDy-TV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9697297811508179
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640590b591ee7d7f287183ae/PQefPEgtaNqnTNfumrZXr.jpeg?w=200&h=200&f=face
          fullname: David Scott
          isHf: false
          isPro: false
          name: ReMeDy-TV
          type: user
        html: '<p>So the model has been great in all areas (excellent memorization,
          remembers clothes/body details). The problem is it sometimes replies to
          older msgs in my context as if it''s the most recent msg. This doesn''t
          happen until about 17k context in. For reference, I''m using SillyTavern
          UI with group conversations using very low temp (as recommended), and a
          basic min-P approach with no Mirostat and minimal rep penalty. I tried experimenting
          with different Instruct presets, but didn''t seem to really help. It doesn''t
          do this on Venus-120b-v1.0-4.5bpw-h6-exl2 or lzlv_70b_fp16_hf-5.0bpw-h6-exl2.</p>

          <p>If anyone runs into this same issue and knows a solution, I''d be curious
          if there even is a solution. I''ve basically had to give up on the model.</p>

          '
        raw: 'So the model has been great in all areas (excellent memorization, remembers
          clothes/body details). The problem is it sometimes replies to older msgs
          in my context as if it''s the most recent msg. This doesn''t happen until
          about 17k context in. For reference, I''m using SillyTavern UI with group
          conversations using very low temp (as recommended), and a basic min-P approach
          with no Mirostat and minimal rep penalty. I tried experimenting with different
          Instruct presets, but didn''t seem to really help. It doesn''t do this on
          Venus-120b-v1.0-4.5bpw-h6-exl2 or lzlv_70b_fp16_hf-5.0bpw-h6-exl2.


          If anyone runs into this same issue and knows a solution, I''d be curious
          if there even is a solution. I''ve basically had to give up on the model.'
        updatedAt: '2023-12-26T11:47:53.854Z'
      numEdits: 2
      reactions: []
    id: 658abd2a2021ba68d7e0803c
    type: comment
  author: ReMeDy-TV
  content: 'So the model has been great in all areas (excellent memorization, remembers
    clothes/body details). The problem is it sometimes replies to older msgs in my
    context as if it''s the most recent msg. This doesn''t happen until about 17k
    context in. For reference, I''m using SillyTavern UI with group conversations
    using very low temp (as recommended), and a basic min-P approach with no Mirostat
    and minimal rep penalty. I tried experimenting with different Instruct presets,
    but didn''t seem to really help. It doesn''t do this on Venus-120b-v1.0-4.5bpw-h6-exl2
    or lzlv_70b_fp16_hf-5.0bpw-h6-exl2.


    If anyone runs into this same issue and knows a solution, I''d be curious if there
    even is a solution. I''ve basically had to give up on the model.'
  created_at: 2023-12-26 11:46:50+00:00
  edited: true
  hidden: false
  id: 658abd2a2021ba68d7e0803c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-26T19:04:49.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9066546559333801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Hmmm, I don''t know how sillytavern is formatting things internally.
          I tend to run the model in notebook mode in one of two formats.</p>

          <p>With labeled characters and a narrator: </p>

          <pre><code>SYSTEM: (Description of story setting, lore and and characters)...

          USER: Continue the story below:

          ASSISTANT: Narrator: It was a dark and stormy night...

          Char1: Blah

          Char2: Blah blah?

          Char1: Blah.

          Narrator: Blah punched blah...

          </code></pre>

          <p>And so on. Or alternatively: </p>

          <pre><code>SYSTEM: (Description of story setting, lore and and characters)...

          USER: Continue the story in a novel style format below:

          ASSISTANT: It was a dark and stormy night...

          ...

          </code></pre>

          <p>And it always responds to the most recent line.</p>

          <p>I haven''t actually tried a USER/ASSISTANT block on each line with this
          merge yet.</p>

          <p>Anyway, get SillyTavern to print the story in verbose mode, maybe post
          an abbreviated version of the formatting? Or just take a look yourself.
          Yi is indeed very sensitive to formatting at long context. </p>

          <p>Or it might actually be some bug in SillyTavern where its truncating
          the context to 16K?</p>

          '
        raw: "Hmmm, I don't know how sillytavern is formatting things internally.\
          \ I tend to run the model in notebook mode in one of two formats.\n\nWith\
          \ labeled characters and a narrator: \n```\nSYSTEM: (Description of story\
          \ setting, lore and and characters)...\nUSER: Continue the story below:\n\
          ASSISTANT: Narrator: It was a dark and stormy night...\nChar1: Blah\nChar2:\
          \ Blah blah?\nChar1: Blah.\nNarrator: Blah punched blah...\n```\nAnd so\
          \ on. Or alternatively: \n```\nSYSTEM: (Description of story setting, lore\
          \ and and characters)...\nUSER: Continue the story in a novel style format\
          \ below:\nASSISTANT: It was a dark and stormy night...\n...\n```\nAnd it\
          \ always responds to the most recent line.\n\nI haven't actually tried a\
          \ USER/ASSISTANT block on each line with this merge yet.\n\nAnyway, get\
          \ SillyTavern to print the story in verbose mode, maybe post an abbreviated\
          \ version of the formatting? Or just take a look yourself. Yi is indeed\
          \ very sensitive to formatting at long context. \n\nOr it might actually\
          \ be some bug in SillyTavern where its truncating the context to 16K?"
        updatedAt: '2023-12-26T19:07:37.355Z'
      numEdits: 4
      reactions: []
    id: 658b23d11dd18f8f169d9a24
    type: comment
  author: brucethemoose
  content: "Hmmm, I don't know how sillytavern is formatting things internally. I\
    \ tend to run the model in notebook mode in one of two formats.\n\nWith labeled\
    \ characters and a narrator: \n```\nSYSTEM: (Description of story setting, lore\
    \ and and characters)...\nUSER: Continue the story below:\nASSISTANT: Narrator:\
    \ It was a dark and stormy night...\nChar1: Blah\nChar2: Blah blah?\nChar1: Blah.\n\
    Narrator: Blah punched blah...\n```\nAnd so on. Or alternatively: \n```\nSYSTEM:\
    \ (Description of story setting, lore and and characters)...\nUSER: Continue the\
    \ story in a novel style format below:\nASSISTANT: It was a dark and stormy night...\n\
    ...\n```\nAnd it always responds to the most recent line.\n\nI haven't actually\
    \ tried a USER/ASSISTANT block on each line with this merge yet.\n\nAnyway, get\
    \ SillyTavern to print the story in verbose mode, maybe post an abbreviated version\
    \ of the formatting? Or just take a look yourself. Yi is indeed very sensitive\
    \ to formatting at long context. \n\nOr it might actually be some bug in SillyTavern\
    \ where its truncating the context to 16K?"
  created_at: 2023-12-26 19:04:49+00:00
  edited: true
  hidden: false
  id: 658b23d11dd18f8f169d9a24
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/Yi-34B-200K-DARE-merge-v5-4bpw-exl2-fiction
repo_type: model
status: open
target_branch: null
title: Model sometimes replies to older msgs in context as if it's the most recent
  msg
