!!python/object:huggingface_hub.community.DiscussionWithDetails
author: h4c5
conflicting_files: null
created_at: 2023-01-19 11:02:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8b344d4d187361d70a5ab304f254d.svg
      fullname: Hakim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: h4c5
      type: user
    createdAt: '2023-01-19T11:02:10.000Z'
    data:
      edited: false
      editors:
      - h4c5
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8b344d4d187361d70a5ab304f254d.svg
          fullname: Hakim
          isHf: false
          isPro: false
          name: h4c5
          type: user
        html: '<p>The <code>max_model_length</code> attribute of the <code>camembert/camembert-base</code>
          Tokenizer is set to <code>VERY_LARGE_INTEGER</code> : </p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          CamembertModel, CamembertTokenizer


          tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">"camembert/camembert-base"</span>)

          <span class="hljs-built_in">print</span>(tokenizer.model_max_length)

          <span class="hljs-comment"># 1000000000000000019884624838656</span>

          </code></pre>

          <p>This is probably because the model name in <code>max_model_input_sizes</code>
          is <code>camembert-base</code>instead of <code>camembert/camembert-base</code>
          (see <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/95f0dd21233297c8eb95d27d1f06150c54e6d8ab/src/transformers/tokenization_utils_base.py#L1926">pretrained
          tokenizer initialization</a>) :</p>

          <pre><code class="language-python"><span class="hljs-built_in">print</span>(tokenizer.max_model_input_sizes)

          <span class="hljs-comment"># {''camembert-base'': 512}</span>

          </code></pre>

          <p>As a result, the example given in the model card do not work with large
          sequences : </p>

          <pre><code class="language-python">tokenizer = CamembertTokenizer.from_pretrained(<span
          class="hljs-string">"camembert/camembert-base"</span>)

          camembert = CamembertModel.from_pretrained(<span class="hljs-string">"camembert/camembert-base"</span>)


          tokenized_sentence = tokenizer.tokenize(<span class="hljs-string">"J''aime
          le camembert !"</span>*<span class="hljs-number">100</span>)

          encoded_sentence = tokenizer.encode(tokenized_sentence)

          encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(<span class="hljs-number">0</span>)

          embeddings, _ = camembert(encoded_sentence)


          <span class="hljs-comment"># RuntimeError: The expanded size of the tensor
          (802) must match the existing size (514) at non-singleton dimension 1.  Target
          sizes: [1, 802].  Tensor sizes: [1, 514]</span>

          </code></pre>

          '
        raw: "The `max_model_length` attribute of the `camembert/camembert-base` Tokenizer\
          \ is set to `VERY_LARGE_INTEGER` : \r\n\r\n```python\r\nimport torch\r\n\
          from transformers import CamembertModel, CamembertTokenizer\r\n\r\ntokenizer\
          \ = CamembertTokenizer.from_pretrained(\"camembert/camembert-base\")\r\n\
          print(tokenizer.model_max_length)\r\n# 1000000000000000019884624838656\r\
          \n```\r\n\r\nThis is probably because the model name in `max_model_input_sizes`\
          \ is `camembert-base`instead of `camembert/camembert-base` (see [pretrained\
          \ tokenizer initialization](https://github.com/huggingface/transformers/blob/95f0dd21233297c8eb95d27d1f06150c54e6d8ab/src/transformers/tokenization_utils_base.py#L1926))\
          \ :\r\n\r\n```python\r\nprint(tokenizer.max_model_input_sizes)\r\n# {'camembert-base':\
          \ 512}\r\n```\r\n\r\nAs a result, the example given in the model card do\
          \ not work with large sequences : \r\n\r\n```python\r\ntokenizer = CamembertTokenizer.from_pretrained(\"\
          camembert/camembert-base\")\r\ncamembert = CamembertModel.from_pretrained(\"\
          camembert/camembert-base\")\r\n\r\ntokenized_sentence = tokenizer.tokenize(\"\
          J'aime le camembert !\"*100)\r\nencoded_sentence = tokenizer.encode(tokenized_sentence)\r\
          \nencoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\r\nembeddings,\
          \ _ = camembert(encoded_sentence)\r\n\r\n# RuntimeError: The expanded size\
          \ of the tensor (802) must match the existing size (514) at non-singleton\
          \ dimension 1.  Target sizes: [1, 802].  Tensor sizes: [1, 514]\r\n```\r\
          \n"
        updatedAt: '2023-01-19T11:02:10.773Z'
      numEdits: 0
      reactions: []
    id: 63c9233251b03e8e01affc2e
    type: comment
  author: h4c5
  content: "The `max_model_length` attribute of the `camembert/camembert-base` Tokenizer\
    \ is set to `VERY_LARGE_INTEGER` : \r\n\r\n```python\r\nimport torch\r\nfrom transformers\
    \ import CamembertModel, CamembertTokenizer\r\n\r\ntokenizer = CamembertTokenizer.from_pretrained(\"\
    camembert/camembert-base\")\r\nprint(tokenizer.model_max_length)\r\n# 1000000000000000019884624838656\r\
    \n```\r\n\r\nThis is probably because the model name in `max_model_input_sizes`\
    \ is `camembert-base`instead of `camembert/camembert-base` (see [pretrained tokenizer\
    \ initialization](https://github.com/huggingface/transformers/blob/95f0dd21233297c8eb95d27d1f06150c54e6d8ab/src/transformers/tokenization_utils_base.py#L1926))\
    \ :\r\n\r\n```python\r\nprint(tokenizer.max_model_input_sizes)\r\n# {'camembert-base':\
    \ 512}\r\n```\r\n\r\nAs a result, the example given in the model card do not work\
    \ with large sequences : \r\n\r\n```python\r\ntokenizer = CamembertTokenizer.from_pretrained(\"\
    camembert/camembert-base\")\r\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-base\"\
    )\r\n\r\ntokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\"*100)\r\
    \nencoded_sentence = tokenizer.encode(tokenized_sentence)\r\nencoded_sentence\
    \ = torch.tensor(encoded_sentence).unsqueeze(0)\r\nembeddings, _ = camembert(encoded_sentence)\r\
    \n\r\n# RuntimeError: The expanded size of the tensor (802) must match the existing\
    \ size (514) at non-singleton dimension 1.  Target sizes: [1, 802].  Tensor sizes:\
    \ [1, 514]\r\n```\r\n"
  created_at: 2023-01-19 11:02:10+00:00
  edited: false
  hidden: false
  id: 63c9233251b03e8e01affc2e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: almanach/camembert-base-legacy
repo_type: model
status: open
target_branch: null
title: Problem with the `max_model_length` attribute
