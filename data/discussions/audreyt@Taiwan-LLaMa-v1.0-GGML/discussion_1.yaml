!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wennycooper
conflicting_files: null
created_at: 2023-09-21 09:22:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5fc293a5fa9586edb215d834f186213.svg
      fullname: Kevin Kuei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wennycooper
      type: user
    createdAt: '2023-09-21T10:22:01.000Z'
    data:
      edited: true
      editors:
      - wennycooper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3411337435245514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5fc293a5fa9586edb215d834f186213.svg
          fullname: Kevin Kuei
          isHf: false
          isPro: false
          name: wennycooper
          type: user
        html: "<p>\u60A8\u597D,<br>\u6211\u4F7F\u7528ggml quantize \u6210\u70BA q6_K\
          \ format, \u7136\u5F8C\u7528\u4EE5\u4E0B code \u505Ainference</p>\n<p>`<br>from\
          \ langchain.llms import LlamaCpp<br>from langchain.callbacks.manager import\
          \ CallbackManager<br>from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler</p>\n\
          <p>callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])</p>\n\
          <p>llm = LlamaCpp(<br>    model_path=\"/workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q6_K.bin\"\
          ,<br>    n_gpu_layers=16,<br>    n_batch=8,<br>    n_ctx=2048,<br>    temperature=0.1,<br>\
          \    max_tokens=512,<br>    callback_manager=callback_manager,<br>)</p>\n\
          <p>prompt_template = \"\"\"A chat between a curious user and an artificial\
          \ intelligence assistant. The assistant gives helpful, detailed, and polite\
          \ answers to the user's questions. USER: {} ASSISTANT:\"\"\"<br>prompt =\
          \ prompt_template.format(\"\u4EC0\u9EBC\u662F\u6DF1\u5EA6\u5B78\u7FD2?\"\
          )<br>response = llm(prompt)<br>`<br>\u7D50\u679C\u6703\u6389\u5B57... \u5982\
          \u4E0B:</p>\n<p>\u6DF1\u5EA6\u5B78\u662F\u6A5F\u5668\u5B78\u7684\u4E00\u5B50\
          \u96C6\uFF0C\u57FA\u4EBA\u5DE5\u795E\u7D93\u7D50\u3002\u4F7F\u5F97\u8A08\
          \u7B97\u6A5F\u80FD\u901A\u5225\u6A21\u5F0F\u5927\u91CF\u4E2D\u5B78\uFF0C\
          \u800C\u4E0D\u9700\u8981\u660E\u7DE8\u7A0B\u3002\u6DF1\u5EA6\u5B78\u7B97\
          \u6CD5\u7528\u5206\u3001\u9032\u884C\u548C\u5225\u6A21\u5F0F</p>\n"
        raw: "\u60A8\u597D,\n\u6211\u4F7F\u7528ggml quantize \u6210\u70BA q6_K format,\
          \ \u7136\u5F8C\u7528\u4EE5\u4E0B code \u505Ainference\n\n`\nfrom langchain.llms\
          \ import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\n\
          from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\
          \n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
          \n\nllm = LlamaCpp(\n    model_path=\"/workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q6_K.bin\"\
          ,\n    n_gpu_layers=16,\n    n_batch=8,\n    n_ctx=2048,\n    temperature=0.1,\n\
          \    max_tokens=512,\n    callback_manager=callback_manager,\n)\n\n\nprompt_template\
          \ = \"\"\"A chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: {} ASSISTANT:\"\"\"\nprompt = prompt_template.format(\"\
          \u4EC0\u9EBC\u662F\u6DF1\u5EA6\u5B78\u7FD2?\")\nresponse = llm(prompt)\n\
          `\n\u7D50\u679C\u6703\u6389\u5B57... \u5982\u4E0B:\n\n\u6DF1\u5EA6\u5B78\
          \u662F\u6A5F\u5668\u5B78\u7684\u4E00\u5B50\u96C6\uFF0C\u57FA\u4EBA\u5DE5\
          \u795E\u7D93\u7D50\u3002\u4F7F\u5F97\u8A08\u7B97\u6A5F\u80FD\u901A\u5225\
          \u6A21\u5F0F\u5927\u91CF\u4E2D\u5B78\uFF0C\u800C\u4E0D\u9700\u8981\u660E\
          \u7DE8\u7A0B\u3002\u6DF1\u5EA6\u5B78\u7B97\u6CD5\u7528\u5206\u3001\u9032\
          \u884C\u548C\u5225\u6A21\u5F0F"
        updatedAt: '2023-09-21T10:22:51.012Z'
      numEdits: 1
      reactions: []
    id: 650c1949d4a0852d3c8cbfa3
    type: comment
  author: wennycooper
  content: "\u60A8\u597D,\n\u6211\u4F7F\u7528ggml quantize \u6210\u70BA q6_K format,\
    \ \u7136\u5F8C\u7528\u4EE5\u4E0B code \u505Ainference\n\n`\nfrom langchain.llms\
    \ import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom\
    \ langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\
    \n\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\n\
    \nllm = LlamaCpp(\n    model_path=\"/workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q6_K.bin\"\
    ,\n    n_gpu_layers=16,\n    n_batch=8,\n    n_ctx=2048,\n    temperature=0.1,\n\
    \    max_tokens=512,\n    callback_manager=callback_manager,\n)\n\n\nprompt_template\
    \ = \"\"\"A chat between a curious user and an artificial intelligence assistant.\
    \ The assistant gives helpful, detailed, and polite answers to the user's questions.\
    \ USER: {} ASSISTANT:\"\"\"\nprompt = prompt_template.format(\"\u4EC0\u9EBC\u662F\
    \u6DF1\u5EA6\u5B78\u7FD2?\")\nresponse = llm(prompt)\n`\n\u7D50\u679C\u6703\u6389\
    \u5B57... \u5982\u4E0B:\n\n\u6DF1\u5EA6\u5B78\u662F\u6A5F\u5668\u5B78\u7684\u4E00\
    \u5B50\u96C6\uFF0C\u57FA\u4EBA\u5DE5\u795E\u7D93\u7D50\u3002\u4F7F\u5F97\u8A08\
    \u7B97\u6A5F\u80FD\u901A\u5225\u6A21\u5F0F\u5927\u91CF\u4E2D\u5B78\uFF0C\u800C\
    \u4E0D\u9700\u8981\u660E\u7DE8\u7A0B\u3002\u6DF1\u5EA6\u5B78\u7B97\u6CD5\u7528\
    \u5206\u3001\u9032\u884C\u548C\u5225\u6A21\u5F0F"
  created_at: 2023-09-21 09:22:01+00:00
  edited: true
  hidden: false
  id: 650c1949d4a0852d3c8cbfa3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5fc293a5fa9586edb215d834f186213.svg
      fullname: Kevin Kuei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wennycooper
      type: user
    createdAt: '2023-09-21T10:35:20.000Z'
    data:
      edited: false
      editors:
      - wennycooper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20192676782608032
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5fc293a5fa9586edb215d834f186213.svg
          fullname: Kevin Kuei
          isHf: false
          isPro: false
          name: wennycooper
          type: user
        html: "<p>\u6211\u4E5F\u6E2C\u8A66\u4E86 q8_0, \u4E5F\u540C\u6A23\u6709\u6389\
          \u5B57\u7684\u554F\u984C.. \u8ACB\u554F\u6709\u89E3\u6C7A\u8FA6\u6CD5\u55CE\
          ?</p>\n<p>llama.cpp: loading model from /workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q8_0.bin<br>llama_model_load_internal:\
          \ format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab  \
          \  = 32000<br>llama_model_load_internal: n_ctx      = 2048<br>llama_model_load_internal:\
          \ n_embd     = 5120<br>llama_model_load_internal: n_mult     = 6912<br>llama_model_load_internal:\
          \ n_head     = 40<br>llama_model_load_internal: n_head_kv  = 40<br>llama_model_load_internal:\
          \ n_layer    = 40<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:\
          \ n_gqa      = 1<br>llama_model_load_internal: rnorm_eps  = 1.0e-06<br>llama_model_load_internal:\
          \ n_ff       = 13824<br>llama_model_load_internal: freq_base  = 10000.0<br>llama_model_load_internal:\
          \ freq_scale = 1<br>llama_model_load_internal: ftype      = 7 (mostly Q8_0)<br>llama_model_load_internal:\
          \ model size = 13B<br>llama_model_load_internal: ggml ctx size =    0.11\
          \ MB<br>llama_model_load_internal: using CUDA for GPU acceleration<br>llama_model_load_internal:\
          \ mem required  = 10431.68 MB (+ 1600.00 MB per state)<br>llama_model_load_internal:\
          \ allocating batch_size x (640 kB + n_ctx x 160 B) = 480 MB VRAM for the\
          \ scratch buffer<br>llama_model_load_internal: offloading 10 repeating layers\
          \ to GPU<br>llama_model_load_internal: offloaded 10/43 layers to GPU<br>llama_model_load_internal:\
          \ total VRAM used: 3695 MB<br>llama_new_context_with_model: kv self size\
          \  = 1600.00 MB<br>AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |<br> \u6DF1\u5EA6\u5B78\u662F\u6A5F\
          \u5668\u5B78\u7684\u4E00\u5B50\u96C6\uFF0C\u57FA\u4EBA\u5DE5\u795E\u7D93\
          \u7D50\u548C\u6D41\u7A0B\u7684\u7B97\u6CD5\u3002\u7528\u5225\u4E2D\u7684\
          \u6A21\u5F0F\u63D0\u53D6\u7279\uFF0C\u4E9B\u7279</p>\n"
        raw: "\u6211\u4E5F\u6E2C\u8A66\u4E86 q8_0, \u4E5F\u540C\u6A23\u6709\u6389\u5B57\
          \u7684\u554F\u984C.. \u8ACB\u554F\u6709\u89E3\u6C7A\u8FA6\u6CD5\u55CE?\n\
          \nllama.cpp: loading model from /workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q8_0.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 6912\nllama_model_load_internal:\
          \ n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal:\
          \ n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal:\
          \ n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype\
          \      = 7 (mostly Q8_0)\nllama_model_load_internal: model size = 13B\n\
          llama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal:\
          \ using CUDA for GPU acceleration\nllama_model_load_internal: mem required\
          \  = 10431.68 MB (+ 1600.00 MB per state)\nllama_model_load_internal: allocating\
          \ batch_size x (640 kB + n_ctx x 160 B) = 480 MB VRAM for the scratch buffer\n\
          llama_model_load_internal: offloading 10 repeating layers to GPU\nllama_model_load_internal:\
          \ offloaded 10/43 layers to GPU\nllama_model_load_internal: total VRAM used:\
          \ 3695 MB\nllama_new_context_with_model: kv self size  = 1600.00 MB\nAVX\
          \ = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA\
          \ = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n \u6DF1\u5EA6\u5B78\u662F\u6A5F\u5668\
          \u5B78\u7684\u4E00\u5B50\u96C6\uFF0C\u57FA\u4EBA\u5DE5\u795E\u7D93\u7D50\
          \u548C\u6D41\u7A0B\u7684\u7B97\u6CD5\u3002\u7528\u5225\u4E2D\u7684\u6A21\
          \u5F0F\u63D0\u53D6\u7279\uFF0C\u4E9B\u7279"
        updatedAt: '2023-09-21T10:35:20.253Z'
      numEdits: 0
      reactions: []
    id: 650c1c68bffe400573687241
    type: comment
  author: wennycooper
  content: "\u6211\u4E5F\u6E2C\u8A66\u4E86 q8_0, \u4E5F\u540C\u6A23\u6709\u6389\u5B57\
    \u7684\u554F\u984C.. \u8ACB\u554F\u6709\u89E3\u6C7A\u8FA6\u6CD5\u55CE?\n\nllama.cpp:\
    \ loading model from /workspace/test/TaiwanLLama_v1.0/Taiwan-LLaMa-13b-1.0.ggmlv3.q8_0.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 6912\nllama_model_load_internal:\
    \ n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal:\
    \ n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
    \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal:\
    \ n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal:\
    \ freq_scale = 1\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\nllama_model_load_internal:\
    \ model size = 13B\nllama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal:\
    \ using CUDA for GPU acceleration\nllama_model_load_internal: mem required  =\
    \ 10431.68 MB (+ 1600.00 MB per state)\nllama_model_load_internal: allocating\
    \ batch_size x (640 kB + n_ctx x 160 B) = 480 MB VRAM for the scratch buffer\n\
    llama_model_load_internal: offloading 10 repeating layers to GPU\nllama_model_load_internal:\
    \ offloaded 10/43 layers to GPU\nllama_model_load_internal: total VRAM used: 3695\
    \ MB\nllama_new_context_with_model: kv self size  = 1600.00 MB\nAVX = 1 | AVX2\
    \ = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 |\
    \ ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 |\
    \ VSX = 0 | \n \u6DF1\u5EA6\u5B78\u662F\u6A5F\u5668\u5B78\u7684\u4E00\u5B50\u96C6\
    \uFF0C\u57FA\u4EBA\u5DE5\u795E\u7D93\u7D50\u548C\u6D41\u7A0B\u7684\u7B97\u6CD5\
    \u3002\u7528\u5225\u4E2D\u7684\u6A21\u5F0F\u63D0\u53D6\u7279\uFF0C\u4E9B\u7279"
  created_at: 2023-09-21 09:35:20+00:00
  edited: false
  hidden: false
  id: 650c1c68bffe400573687241
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9eff1e1c66d417c7c51e263f5ae4bb01.svg
      fullname: Phate
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phate334
      type: user
    createdAt: '2023-09-21T10:50:59.000Z'
    data:
      edited: true
      editors:
      - phate334
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.7193878293037415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9eff1e1c66d417c7c51e263f5ae4bb01.svg
          fullname: Phate
          isHf: false
          isPro: false
          name: phate334
          type: user
        html: "<p>\u9019\u8DDF\u6A21\u578B\u7121\u95DC\uFF0C\u61C9\u8A72\u662F llama-cpp-python\
          \ \u4E4B\u524D\u7684 bug \uFF0C\u66F4\u65B0\u5957\u4EF6\u53EF\u4EE5\u89E3\
          \u6C7A<br><a rel=\"nofollow\" href=\"https://github.com/abetlen/llama-cpp-python/pull/309\"\
          >https://github.com/abetlen/llama-cpp-python/pull/309</a></p>\n<p>\u7136\
          \u5F8C\u65B0\u7248\u672C\u7684 llama.cpp \u6C92\u652F\u63F4 GGML \uFF0C\u73FE\
          \u5728\u61C9\u8A72\u90FD\u8981\u9077\u79FB\u53BB\u7528 GGUF \u4E86</p>\n"
        raw: "\u9019\u8DDF\u6A21\u578B\u7121\u95DC\uFF0C\u61C9\u8A72\u662F llama-cpp-python\
          \ \u4E4B\u524D\u7684 bug \uFF0C\u66F4\u65B0\u5957\u4EF6\u53EF\u4EE5\u89E3\
          \u6C7A\nhttps://github.com/abetlen/llama-cpp-python/pull/309\n\n\u7136\u5F8C\
          \u65B0\u7248\u672C\u7684 llama.cpp \u6C92\u652F\u63F4 GGML \uFF0C\u73FE\u5728\
          \u61C9\u8A72\u90FD\u8981\u9077\u79FB\u53BB\u7528 GGUF \u4E86"
        updatedAt: '2023-09-21T10:55:26.133Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wennycooper
    id: 650c2013443b2a59d4ec3c09
    type: comment
  author: phate334
  content: "\u9019\u8DDF\u6A21\u578B\u7121\u95DC\uFF0C\u61C9\u8A72\u662F llama-cpp-python\
    \ \u4E4B\u524D\u7684 bug \uFF0C\u66F4\u65B0\u5957\u4EF6\u53EF\u4EE5\u89E3\u6C7A\
    \nhttps://github.com/abetlen/llama-cpp-python/pull/309\n\n\u7136\u5F8C\u65B0\u7248\
    \u672C\u7684 llama.cpp \u6C92\u652F\u63F4 GGML \uFF0C\u73FE\u5728\u61C9\u8A72\u90FD\
    \u8981\u9077\u79FB\u53BB\u7528 GGUF \u4E86"
  created_at: 2023-09-21 09:50:59+00:00
  edited: true
  hidden: false
  id: 650c2013443b2a59d4ec3c09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6373a1a92d4eccfa6f909f69/j8ED1yzIYH0nBljHcqELg.jpeg?w=200&h=200&f=face
      fullname: Audrey Tang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: audreyt
      type: user
    createdAt: '2023-09-21T10:55:12.000Z'
    data:
      edited: false
      editors:
      - audreyt
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.17970433831214905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6373a1a92d4eccfa6f909f69/j8ED1yzIYH0nBljHcqELg.jpeg?w=200&h=200&f=face
          fullname: Audrey Tang
          isHf: false
          isPro: false
          name: audreyt
          type: user
        html: "<p>\u662F\u7684\uFF0C\u8ACB\u6539\u7528 <a href=\"https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGUF\"\
          >https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGUF</a></p>\n"
        raw: "\u662F\u7684\uFF0C\u8ACB\u6539\u7528 https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGUF"
        updatedAt: '2023-09-21T10:55:12.130Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wennycooper
      relatedEventId: 650c2110a74f76a6ac294060
    id: 650c2110a74f76a6ac29405f
    type: comment
  author: audreyt
  content: "\u662F\u7684\uFF0C\u8ACB\u6539\u7528 https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGUF"
  created_at: 2023-09-21 09:55:12+00:00
  edited: false
  hidden: false
  id: 650c2110a74f76a6ac29405f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6373a1a92d4eccfa6f909f69/j8ED1yzIYH0nBljHcqELg.jpeg?w=200&h=200&f=face
      fullname: Audrey Tang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: audreyt
      type: user
    createdAt: '2023-09-21T10:55:12.000Z'
    data:
      status: closed
    id: 650c2110a74f76a6ac294060
    type: status-change
  author: audreyt
  created_at: 2023-09-21 09:55:12+00:00
  id: 650c2110a74f76a6ac294060
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: audreyt/Taiwan-LLaMa-v1.0-GGML
repo_type: model
status: closed
target_branch: null
title: "\u4F7F\u7528ggmlv3 q6_K model, inference\u6703\u6389\u5B57"
