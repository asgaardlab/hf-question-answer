!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Chris12321
conflicting_files: null
created_at: 2023-11-17 13:08:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0be8e159f2f13d1d99d1e373da91c91e.svg
      fullname: Chris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chris12321
      type: user
    createdAt: '2023-11-17T13:08:51.000Z'
    data:
      edited: true
      editors:
      - Chris12321
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8910263180732727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0be8e159f2f13d1d99d1e373da91c91e.svg
          fullname: Chris
          isHf: false
          isPro: false
          name: Chris12321
          type: user
        html: '<p>Hello,<br>Thank you for your work.</p>

          <p>I wanted to test this model since it is linked in this tutorial:<br><a
          href="https://huggingface.co/blog/stackllama">https://huggingface.co/blog/stackllama</a>
          </p>

          <p>First I have tried deploying it as a huggingface inference endpoint,
          but the config.json was missing.<br>For details on that problem, please
          see my comment in this discussion: <a href="https://huggingface.co/trl-lib/llama-7b-se-rl-peft/discussions/9">https://huggingface.co/trl-lib/llama-7b-se-rl-peft/discussions/9</a></p>

          <p>Then I have tried it with transformers library:<br>from transformers
          import AutoModel<br>model = AutoModel.from_pretrained("trl-lib/llama-7b-se-rl-peft")</p>

          <p>But I got:<br>OSError: trl-lib/llama-se-merged is not a local folder
          and is not a valid model identifier listed on ''<a href="https://huggingface.co/models''">https://huggingface.co/models''</a><br>If
          this is a private repository, make sure to pass a token having permission
          to this repo either by logging in with <code>huggingface-cli login</code>
          or by passing <code>token=&lt;your_token&gt;</code></p>

          <p>although I had successfully logged in previously like that:<br>!huggingface-cli
          login --token ""</p>

          <p>With this output:<br>Token will not been saved to git credential helper.
          Pass <code>add_to_git_credential=True</code> if you want to set the git
          credential as well.<br>Token is valid (permission: read).<br>Your token
          has been saved to /root/.cache/huggingface/token<br>Login successful</p>

          <p>Do you have any recommendations for me on how to proceed best to test
          this model, or is it inaccessible for the public?<br>Many thanks and best
          regards,<br>Chris</p>

          '
        raw: "Hello,\nThank you for your work.\n\nI wanted to test this model since\
          \ it is linked in this tutorial:\nhttps://huggingface.co/blog/stackllama\
          \ \n\nFirst I have tried deploying it as a huggingface inference endpoint,\
          \ but the config.json was missing.\nFor details on that problem, please\
          \ see my comment in this discussion: https://huggingface.co/trl-lib/llama-7b-se-rl-peft/discussions/9\n\
          \nThen I have tried it with transformers library:\nfrom transformers import\
          \ AutoModel\nmodel = AutoModel.from_pretrained(\"trl-lib/llama-7b-se-rl-peft\"\
          )\n\nBut I got:\nOSError: trl-lib/llama-se-merged is not a local folder\
          \ and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
          If this is a private repository, make sure to pass a token having permission\
          \ to this repo either by logging in with `huggingface-cli login` or by passing\
          \ `token=<your_token>`\n\nalthough I had successfully logged in previously\
          \ like that:\n!huggingface-cli login --token \"<secret_token>\"\n\nWith\
          \ this output:\nToken will not been saved to git credential helper. Pass\
          \ `add_to_git_credential=True` if you want to set the git credential as\
          \ well.\nToken is valid (permission: read).\nYour token has been saved to\
          \ /root/.cache/huggingface/token\nLogin successful\n\nDo you have any recommendations\
          \ for me on how to proceed best to test this model, or is it inaccessible\
          \ for the public?\nMany thanks and best regards,\nChris"
        updatedAt: '2023-11-17T13:21:54.852Z'
      numEdits: 1
      reactions: []
    id: 655765e30438e0854fb8bfec
    type: comment
  author: Chris12321
  content: "Hello,\nThank you for your work.\n\nI wanted to test this model since\
    \ it is linked in this tutorial:\nhttps://huggingface.co/blog/stackllama \n\n\
    First I have tried deploying it as a huggingface inference endpoint, but the config.json\
    \ was missing.\nFor details on that problem, please see my comment in this discussion:\
    \ https://huggingface.co/trl-lib/llama-7b-se-rl-peft/discussions/9\n\nThen I have\
    \ tried it with transformers library:\nfrom transformers import AutoModel\nmodel\
    \ = AutoModel.from_pretrained(\"trl-lib/llama-7b-se-rl-peft\")\n\nBut I got:\n\
    OSError: trl-lib/llama-se-merged is not a local folder and is not a valid model\
    \ identifier listed on 'https://huggingface.co/models'\nIf this is a private repository,\
    \ make sure to pass a token having permission to this repo either by logging in\
    \ with `huggingface-cli login` or by passing `token=<your_token>`\n\nalthough\
    \ I had successfully logged in previously like that:\n!huggingface-cli login --token\
    \ \"<secret_token>\"\n\nWith this output:\nToken will not been saved to git credential\
    \ helper. Pass `add_to_git_credential=True` if you want to set the git credential\
    \ as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\n\
    Login successful\n\nDo you have any recommendations for me on how to proceed best\
    \ to test this model, or is it inaccessible for the public?\nMany thanks and best\
    \ regards,\nChris"
  created_at: 2023-11-17 13:08:51+00:00
  edited: true
  hidden: false
  id: 655765e30438e0854fb8bfec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-20T15:17:50.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.752804696559906
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Chris12321&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Chris12321\"\
          >@<span class=\"underline\">Chris12321</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for your interest in using this model<br>This repository contains a PEFT\
          \ adapter, in order to use it, first install PEFT: <code>pip install -U\
          \ peft</code><br>Then load it with the following snippet:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> peft\
          \ <span class=\"hljs-keyword\">import</span> AutoPeftModelForCausalLM\n\
          model = AutoPeftModelForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"trl-lib/llama-7b-se-rl-peft\"</span>)\n</code></pre>\n<p>To deploy it,\
          \ since it is a LoRA (Low Rank Adapters) model, I would suggest to first\
          \ \"merge\" the model with the base model. You can read more about what\
          \ merging LoRA weights means here: <a href=\"https://huggingface.co/Salesforce/codegen2-7B/discussions/1#6543f4eb2996405c23882b03\"\
          >https://huggingface.co/Salesforce/codegen2-7B/discussions/1#6543f4eb2996405c23882b03</a>\
          \ </p>\n<pre><code class=\"language-diff\">from peft import AutoPeftModelForCausalLM\n\
          model = AutoPeftModelForCausalLM.from_pretrained(\"trl-lib/llama-7b-se-rl-peft\"\
          )\n<span class=\"hljs-addition\">+ model = model.merge_and_unload()</span>\n\
          </code></pre>\n<p>Then push the merged model somewhere on the Hub under\
          \ your namespace:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> peft <span class=\"hljs-keyword\">import</span>\
          \ AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"trl-lib/llama-7b-se-rl-peft\"</span>)\nmodel =\
          \ model.merge_and_unload()\nmodel.push_to_hub(<span class=\"hljs-string\"\
          >\"xxx/my-merged-se-peft-merged-model\"</span>)\n</code></pre>\n<p>After\
          \ that you'll be able to use your model for deployment</p>\n"
        raw: "Hi @Chris12321 \nThanks for your interest in using this model\nThis\
          \ repository contains a PEFT adapter, in order to use it, first install\
          \ PEFT: `pip install -U peft`\nThen load it with the following snippet:\n\
          ```python\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
          trl-lib/llama-7b-se-rl-peft\")\n```\nTo deploy it, since it is a LoRA (Low\
          \ Rank Adapters) model, I would suggest to first \"merge\" the model with\
          \ the base model. You can read more about what merging LoRA weights means\
          \ here: https://huggingface.co/Salesforce/codegen2-7B/discussions/1#6543f4eb2996405c23882b03\
          \ \n\n```diff\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
          trl-lib/llama-7b-se-rl-peft\")\n+ model = model.merge_and_unload()\n```\n\
          \nThen push the merged model somewhere on the Hub under your namespace:\n\
          \n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
          trl-lib/llama-7b-se-rl-peft\")\nmodel = model.merge_and_unload()\nmodel.push_to_hub(\"\
          xxx/my-merged-se-peft-merged-model\")\n```\n\nAfter that you'll be able\
          \ to use your model for deployment"
        updatedAt: '2023-11-20T15:17:50.676Z'
      numEdits: 0
      reactions: []
    id: 655b789e8762c448a165809a
    type: comment
  author: ybelkada
  content: "Hi @Chris12321 \nThanks for your interest in using this model\nThis repository\
    \ contains a PEFT adapter, in order to use it, first install PEFT: `pip install\
    \ -U peft`\nThen load it with the following snippet:\n```python\nfrom peft import\
    \ AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
    trl-lib/llama-7b-se-rl-peft\")\n```\nTo deploy it, since it is a LoRA (Low Rank\
    \ Adapters) model, I would suggest to first \"merge\" the model with the base\
    \ model. You can read more about what merging LoRA weights means here: https://huggingface.co/Salesforce/codegen2-7B/discussions/1#6543f4eb2996405c23882b03\
    \ \n\n```diff\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
    trl-lib/llama-7b-se-rl-peft\")\n+ model = model.merge_and_unload()\n```\n\nThen\
    \ push the merged model somewhere on the Hub under your namespace:\n\n```python\n\
    from peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"\
    trl-lib/llama-7b-se-rl-peft\")\nmodel = model.merge_and_unload()\nmodel.push_to_hub(\"\
    xxx/my-merged-se-peft-merged-model\")\n```\n\nAfter that you'll be able to use\
    \ your model for deployment"
  created_at: 2023-11-20 15:17:50+00:00
  edited: false
  hidden: false
  id: 655b789e8762c448a165809a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-20T15:18:14.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9355080127716064
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Let me know if you need any help, I can help you push the merged
          weights on the Hub so that you can use it out of the box</p>

          '
        raw: Let me know if you need any help, I can help you push the merged weights
          on the Hub so that you can use it out of the box
        updatedAt: '2023-11-20T15:18:14.307Z'
      numEdits: 0
      reactions: []
    id: 655b78b6eb411317eb5311ac
    type: comment
  author: ybelkada
  content: Let me know if you need any help, I can help you push the merged weights
    on the Hub so that you can use it out of the box
  created_at: 2023-11-20 15:18:14+00:00
  edited: false
  hidden: false
  id: 655b78b6eb411317eb5311ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1638132956881-5fca176d1d7a08cb34d79d5d.jpeg?w=200&h=200&f=face
      fullname: Sourab Mangrulkar
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: smangrul
      type: user
    createdAt: '2023-11-20T15:23:48.000Z'
    data:
      edited: false
      editors:
      - smangrul
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9384241104125977
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1638132956881-5fca176d1d7a08cb34d79d5d.jpeg?w=200&h=200&f=face
          fullname: Sourab Mangrulkar
          isHf: true
          isPro: false
          name: smangrul
          type: user
        html: '<p>Hello, yes, +1 on what Younes mentioned. PEFT models are not yet
          supported with Inference endpoints.</p>

          '
        raw: Hello, yes, +1 on what Younes mentioned. PEFT models are not yet supported
          with Inference endpoints.
        updatedAt: '2023-11-20T15:23:48.037Z'
      numEdits: 0
      reactions: []
    id: 655b7a0423e43ac2182170f7
    type: comment
  author: smangrul
  content: Hello, yes, +1 on what Younes mentioned. PEFT models are not yet supported
    with Inference endpoints.
  created_at: 2023-11-20 15:23:48+00:00
  edited: false
  hidden: false
  id: 655b7a0423e43ac2182170f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: trl-lib/llama-7b-se-rl-peft
repo_type: model
status: open
target_branch: null
title: Model inaccessible?
