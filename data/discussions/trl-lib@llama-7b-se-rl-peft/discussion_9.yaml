!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kmfoda
conflicting_files: null
created_at: 2023-06-22 13:56:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2023-06-22T14:56:19.000Z'
    data:
      edited: false
      editors:
      - kmfoda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7917572259902954
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: '<p>Hello, thanks for contributing this amazing model. When I try and
          laod it using:</p>

          <p>from transformers import AutoModel<br>model = AutoModel.from_pretrained("trl-lib/llama-7b-se-rl-peft")</p>

          <p>I get the following error:</p>

          <p>OSError: trl-lib/llama-7b-se-rl-peft does not appear to have a file named
          config.json. Checkout ''<a href="https://huggingface.co/trl-lib/llama-7b-se-rl-peft/main''">https://huggingface.co/trl-lib/llama-7b-se-rl-peft/main''</a>
          for available files.</p>

          <p>Is there perhaps an intermediary step I need to carry out before loading
          the model?</p>

          '
        raw: "Hello, thanks for contributing this amazing model. When I try and laod\
          \ it using:\r\n\r\nfrom transformers import AutoModel\r\nmodel = AutoModel.from_pretrained(\"\
          trl-lib/llama-7b-se-rl-peft\")\r\n\r\nI get the following error:\r\n\r\n\
          OSError: trl-lib/llama-7b-se-rl-peft does not appear to have a file named\
          \ config.json. Checkout 'https://huggingface.co/trl-lib/llama-7b-se-rl-peft/main'\
          \ for available files.\r\n\r\nIs there perhaps an intermediary step I need\
          \ to carry out before loading the model?"
        updatedAt: '2023-06-22T14:56:19.793Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Chris12321
    id: 64946113d69fdb3dbd986ec5
    type: comment
  author: kmfoda
  content: "Hello, thanks for contributing this amazing model. When I try and laod\
    \ it using:\r\n\r\nfrom transformers import AutoModel\r\nmodel = AutoModel.from_pretrained(\"\
    trl-lib/llama-7b-se-rl-peft\")\r\n\r\nI get the following error:\r\n\r\nOSError:\
    \ trl-lib/llama-7b-se-rl-peft does not appear to have a file named config.json.\
    \ Checkout 'https://huggingface.co/trl-lib/llama-7b-se-rl-peft/main' for available\
    \ files.\r\n\r\nIs there perhaps an intermediary step I need to carry out before\
    \ loading the model?"
  created_at: 2023-06-22 13:56:19+00:00
  edited: false
  hidden: false
  id: 64946113d69fdb3dbd986ec5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0be8e159f2f13d1d99d1e373da91c91e.svg
      fullname: Chris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chris12321
      type: user
    createdAt: '2023-11-17T12:31:44.000Z'
    data:
      edited: false
      editors:
      - Chris12321
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4757973253726959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0be8e159f2f13d1d99d1e373da91c91e.svg
          fullname: Chris
          isHf: false
          isPro: false
          name: Chris12321
          type: user
        html: '<p>Same when trying deploy it as huggingface inference endpoint:</p>

          <p>2023/11/17 13:27:39 ~ self.pipeline = get_pipeline(model_dir=model_dir,
          task=task)<br>2023/11/17 13:27:39 ~ await handler()<br>2023/11/17 13:27:39
          ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py", line
          584, in <strong>aenter</strong><br>2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 705, in lifespan<br>6cn7t 2023-11-17T12:27:39.318Z<br>2023/11/17 13:27:39
          ~ OSError: /repository does not appear to have a file named config.json.
          Checkout ''<a href="https://huggingface.co//repository/None''">https://huggingface.co//repository/None''</a>
          for available files.<br>2023/11/17 13:27:39 ~ config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,
          **kwargs)<br>2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/utils.py",
          line 261, in get_pipeline<br>2023/11/17 13:27:39 ~ File "/app/webservice_starlette.py",
          line 57, in some_startup_task<br>2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 682, in startup<br>2023/11/17 13:27:39 ~ hf_pipeline = pipeline(task=task,
          model=model_dir, device=device, **kwargs)<br>2023/11/17 13:27:39 ~ config_dict,
          unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,
          **kwargs)<br>2023/11/17 13:27:39 ~ raise EnvironmentError(<br>2023/11/17
          13:27:39 ~ config = AutoConfig.from_pretrained(model, _from_pipeline=task,
          **hub_kwargs, **model_kwargs)<br>2023/11/17 13:27:39 ~ await self._router.startup()<br>2023/11/17
          13:27:39 ~ return HuggingFaceHandler(model_dir=model_dir, task=task)<br>2023/11/17
          13:27:39 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,
          task=HF_TASK)<br>2023/11/17 13:27:39 ~ Application startup failed. Exiting.<br>2023/11/17
          13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/utils/hub.py",
          line 388, in cached_file<br>2023/11/17 13:27:39 ~ resolved_config_file =
          cached_file(<br>2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
          line 672, in _get_config_dict<br>2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
          line 617, in get_config_dict<br>2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 983, in from_pretrained<br>2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 17, in <strong>init</strong><br>2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 45, in get_inference_handler_either_custom_or_default_handler<br>2023/11/17
          13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 705, in pipeline<br>2023/11/17 13:27:39 ~ async with self.lifespan_context(app)
          as maybe_state:<br>2023/11/17 13:27:39 ~ Traceback (most recent call last):</p>

          '
        raw: 'Same when trying deploy it as huggingface inference endpoint:


          2023/11/17 13:27:39 ~ self.pipeline = get_pipeline(model_dir=model_dir,
          task=task)

          2023/11/17 13:27:39 ~ await handler()

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 584, in __aenter__

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 705, in lifespan

          6cn7t 2023-11-17T12:27:39.318Z

          2023/11/17 13:27:39 ~ OSError: /repository does not appear to have a file
          named config.json. Checkout ''https://huggingface.co//repository/None''
          for available files.

          2023/11/17 13:27:39 ~ config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,
          **kwargs)

          2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/utils.py",
          line 261, in get_pipeline

          2023/11/17 13:27:39 ~ File "/app/webservice_starlette.py", line 57, in some_startup_task

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
          line 682, in startup

          2023/11/17 13:27:39 ~ hf_pipeline = pipeline(task=task, model=model_dir,
          device=device, **kwargs)

          2023/11/17 13:27:39 ~ config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,
          **kwargs)

          2023/11/17 13:27:39 ~ raise EnvironmentError(

          2023/11/17 13:27:39 ~ config = AutoConfig.from_pretrained(model, _from_pipeline=task,
          **hub_kwargs, **model_kwargs)

          2023/11/17 13:27:39 ~ await self._router.startup()

          2023/11/17 13:27:39 ~ return HuggingFaceHandler(model_dir=model_dir, task=task)

          2023/11/17 13:27:39 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,
          task=HF_TASK)

          2023/11/17 13:27:39 ~ Application startup failed. Exiting.

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/utils/hub.py",
          line 388, in cached_file

          2023/11/17 13:27:39 ~ resolved_config_file = cached_file(

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
          line 672, in _get_config_dict

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
          line 617, in get_config_dict

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
          line 983, in from_pretrained

          2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 17, in __init__

          2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py",
          line 45, in get_inference_handler_either_custom_or_default_handler

          2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py",
          line 705, in pipeline

          2023/11/17 13:27:39 ~ async with self.lifespan_context(app) as maybe_state:

          2023/11/17 13:27:39 ~ Traceback (most recent call last):'
        updatedAt: '2023-11-17T12:31:44.908Z'
      numEdits: 0
      reactions: []
    id: 65575d306539c9a5a54e0148
    type: comment
  author: Chris12321
  content: 'Same when trying deploy it as huggingface inference endpoint:


    2023/11/17 13:27:39 ~ self.pipeline = get_pipeline(model_dir=model_dir, task=task)

    2023/11/17 13:27:39 ~ await handler()

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
    line 584, in __aenter__

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
    line 705, in lifespan

    6cn7t 2023-11-17T12:27:39.318Z

    2023/11/17 13:27:39 ~ OSError: /repository does not appear to have a file named
    config.json. Checkout ''https://huggingface.co//repository/None'' for available
    files.

    2023/11/17 13:27:39 ~ config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,
    **kwargs)

    2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/utils.py", line
    261, in get_pipeline

    2023/11/17 13:27:39 ~ File "/app/webservice_starlette.py", line 57, in some_startup_task

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/starlette/routing.py",
    line 682, in startup

    2023/11/17 13:27:39 ~ hf_pipeline = pipeline(task=task, model=model_dir, device=device,
    **kwargs)

    2023/11/17 13:27:39 ~ config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,
    **kwargs)

    2023/11/17 13:27:39 ~ raise EnvironmentError(

    2023/11/17 13:27:39 ~ config = AutoConfig.from_pretrained(model, _from_pipeline=task,
    **hub_kwargs, **model_kwargs)

    2023/11/17 13:27:39 ~ await self._router.startup()

    2023/11/17 13:27:39 ~ return HuggingFaceHandler(model_dir=model_dir, task=task)

    2023/11/17 13:27:39 ~ inference_handler = get_inference_handler_either_custom_or_default_handler(HF_MODEL_DIR,
    task=HF_TASK)

    2023/11/17 13:27:39 ~ Application startup failed. Exiting.

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/utils/hub.py",
    line 388, in cached_file

    2023/11/17 13:27:39 ~ resolved_config_file = cached_file(

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
    line 672, in _get_config_dict

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py",
    line 617, in get_config_dict

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py",
    line 983, in from_pretrained

    2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py", line
    17, in __init__

    2023/11/17 13:27:39 ~ File "/app/huggingface_inference_toolkit/handler.py", line
    45, in get_inference_handler_either_custom_or_default_handler

    2023/11/17 13:27:39 ~ File "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/__init__.py",
    line 705, in pipeline

    2023/11/17 13:27:39 ~ async with self.lifespan_context(app) as maybe_state:

    2023/11/17 13:27:39 ~ Traceback (most recent call last):'
  created_at: 2023-11-17 12:31:44+00:00
  edited: false
  hidden: false
  id: 65575d306539c9a5a54e0148
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: trl-lib/llama-7b-se-rl-peft
repo_type: model
status: open
target_branch: null
title: 'OSError: trl-lib/llama-7b-se-rl-peft does not appear to have a file named
  config.json.'
