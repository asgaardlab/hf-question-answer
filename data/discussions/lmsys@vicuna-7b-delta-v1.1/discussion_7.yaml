!!python/object:huggingface_hub.community.DiscussionWithDetails
author: David003
conflicting_files: null
created_at: 2023-06-11 14:41:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b78d371144f0f0af3037b7adc7c5b133.svg
      fullname: Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: David003
      type: user
    createdAt: '2023-06-11T15:41:36.000Z'
    data:
      edited: false
      editors:
      - David003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2118023782968521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b78d371144f0f0af3037b7adc7c5b133.svg
          fullname: Lee
          isHf: false
          isPro: false
          name: David003
          type: user
        html: '<p>I finetune lamma-7b and get my  vicuna model. I found that the vicuna
          model folder is twice the size of original lamma-7b.</p>

          <p>Original lamma-7b is about 13G:<br>drwxrwxr-x 8 ubuntu ubuntu 4096 Jun
          11 09:32 .git/<br>-rw-rw-r-- 1 ubuntu ubuntu 1546 Jun 11 09:31 .gitattributes<br>-rw-rw-r--
          1 ubuntu ubuntu 177 Jun 11 09:31 README.md<br>-rw-rw-r-- 1 ubuntu ubuntu
          507 Jun 11 09:31 config.json<br>-rw-rw-r-- 1 ubuntu ubuntu 137 Jun 11 09:31
          generation_config.json<br>-rw-rw-r-- 1 ubuntu ubuntu 9976634558 Jun 11 09:31
          pytorch_model-00001-of-00002.bin<br>-rw-rw-r-- 1 ubuntu ubuntu 3500315539
          Jun 11 09:31 pytorch_model-00002-of-00002.bin<br>-rw-rw-r-- 1 ubuntu ubuntu
          26788 Jun 11 09:31 pytorch_model.bin.index.json<br>-rw-rw-r-- 1 ubuntu ubuntu
          411 Jun 11 09:31 special_tokens_map.json<br>-rw-rw-r-- 1 ubuntu ubuntu 499723
          Jun 11 09:31 tokenizer.model<br>-rw-rw-r-- 1 ubuntu ubuntu 727 Jun 11 09:31
          tokenizer_config.json</p>

          <p>While the vicuna model is about 28G:</p>

          <p>drwxrwxr-x 2 ubuntu ubuntu 4096 Jun 7 04:38 ./<br>drwxrwxr-x 22 ubuntu
          ubuntu 4096 Jun 11 10:43 ../<br>-rw-rw-r-- 1 ubuntu ubuntu 548 May 18 03:31
          config.json<br>-rw-rw-r-- 1 ubuntu ubuntu 132 May 18 03:31 generation_config.json<br>-rw-rw-r--
          1 ubuntu ubuntu 9877989586 May 18 03:31 pytorch_model-00001-of-00003.bin<br>-rw-rw-r--
          1 ubuntu ubuntu 9894801014 May 18 03:31 pytorch_model-00002-of-00003.bin<br>-rw-rw-r--
          1 ubuntu ubuntu 7180990649 May 18 03:31 pytorch_model-00003-of-00003.bin<br>-rw-rw-r--
          1 ubuntu ubuntu 26788 May 18 03:31 pytorch_model.bin.index.json<br>-rw-rw-r--
          1 ubuntu ubuntu 435 May 18 03:31 special_tokens_map.json<br>-rw-rw-r-- 1
          ubuntu ubuntu 499723 May 18 03:31 tokenizer.model<br>-rw-rw-r-- 1 ubuntu
          ubuntu 727 May 18 03:31 tokenizer_config.json<br>-rw-rw-r-- 1 ubuntu ubuntu
          4830 May 18 03:31 trainer_state.json<br>-rw-rw-r-- 1 ubuntu ubuntu 3771
          May 18 03:31 training_args.bin</p>

          <p>Is my situation Correct or Not? Thanks.</p>

          '
        raw: "I finetune lamma-7b and get my  vicuna model. I found that the vicuna\
          \ model folder is twice the size of original lamma-7b.\r\n\r\nOriginal lamma-7b\
          \ is about 13G:\r\ndrwxrwxr-x 8 ubuntu ubuntu 4096 Jun 11 09:32 .git/\r\n\
          -rw-rw-r-- 1 ubuntu ubuntu 1546 Jun 11 09:31 .gitattributes\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 177 Jun 11 09:31 README.md\r\n-rw-rw-r-- 1 ubuntu ubuntu\
          \ 507 Jun 11 09:31 config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu 137 Jun 11\
          \ 09:31 generation_config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu 9976634558\
          \ Jun 11 09:31 pytorch_model-00001-of-00002.bin\r\n-rw-rw-r-- 1 ubuntu ubuntu\
          \ 3500315539 Jun 11 09:31 pytorch_model-00002-of-00002.bin\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 26788 Jun 11 09:31 pytorch_model.bin.index.json\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 411 Jun 11 09:31 special_tokens_map.json\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 499723 Jun 11 09:31 tokenizer.model\r\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu 727 Jun 11 09:31 tokenizer_config.json\r\n\r\nWhile the vicuna\
          \ model is about 28G:\r\n\r\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Jun 7 04:38\
          \ ./\r\ndrwxrwxr-x 22 ubuntu ubuntu 4096 Jun 11 10:43 ../\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 548 May 18 03:31 config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu\
          \ 132 May 18 03:31 generation_config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu\
          \ 9877989586 May 18 03:31 pytorch_model-00001-of-00003.bin\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 9894801014 May 18 03:31 pytorch_model-00002-of-00003.bin\r\
          \n-rw-rw-r-- 1 ubuntu ubuntu 7180990649 May 18 03:31 pytorch_model-00003-of-00003.bin\r\
          \n-rw-rw-r-- 1 ubuntu ubuntu 26788 May 18 03:31 pytorch_model.bin.index.json\r\
          \n-rw-rw-r-- 1 ubuntu ubuntu 435 May 18 03:31 special_tokens_map.json\r\n\
          -rw-rw-r-- 1 ubuntu ubuntu 499723 May 18 03:31 tokenizer.model\r\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 727 May 18 03:31 tokenizer_config.json\r\n-rw-rw-r-- 1\
          \ ubuntu ubuntu 4830 May 18 03:31 trainer_state.json\r\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu 3771 May 18 03:31 training_args.bin\r\n\r\nIs my situation Correct\
          \ or Not? Thanks."
        updatedAt: '2023-06-11T15:41:36.128Z'
      numEdits: 0
      reactions: []
    id: 6485eb30166981082f0253c7
    type: comment
  author: David003
  content: "I finetune lamma-7b and get my  vicuna model. I found that the vicuna\
    \ model folder is twice the size of original lamma-7b.\r\n\r\nOriginal lamma-7b\
    \ is about 13G:\r\ndrwxrwxr-x 8 ubuntu ubuntu 4096 Jun 11 09:32 .git/\r\n-rw-rw-r--\
    \ 1 ubuntu ubuntu 1546 Jun 11 09:31 .gitattributes\r\n-rw-rw-r-- 1 ubuntu ubuntu\
    \ 177 Jun 11 09:31 README.md\r\n-rw-rw-r-- 1 ubuntu ubuntu 507 Jun 11 09:31 config.json\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 137 Jun 11 09:31 generation_config.json\r\n-rw-rw-r--\
    \ 1 ubuntu ubuntu 9976634558 Jun 11 09:31 pytorch_model-00001-of-00002.bin\r\n\
    -rw-rw-r-- 1 ubuntu ubuntu 3500315539 Jun 11 09:31 pytorch_model-00002-of-00002.bin\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 26788 Jun 11 09:31 pytorch_model.bin.index.json\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 411 Jun 11 09:31 special_tokens_map.json\r\n-rw-rw-r--\
    \ 1 ubuntu ubuntu 499723 Jun 11 09:31 tokenizer.model\r\n-rw-rw-r-- 1 ubuntu ubuntu\
    \ 727 Jun 11 09:31 tokenizer_config.json\r\n\r\nWhile the vicuna model is about\
    \ 28G:\r\n\r\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Jun 7 04:38 ./\r\ndrwxrwxr-x 22\
    \ ubuntu ubuntu 4096 Jun 11 10:43 ../\r\n-rw-rw-r-- 1 ubuntu ubuntu 548 May 18\
    \ 03:31 config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu 132 May 18 03:31 generation_config.json\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 9877989586 May 18 03:31 pytorch_model-00001-of-00003.bin\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 9894801014 May 18 03:31 pytorch_model-00002-of-00003.bin\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 7180990649 May 18 03:31 pytorch_model-00003-of-00003.bin\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 26788 May 18 03:31 pytorch_model.bin.index.json\r\
    \n-rw-rw-r-- 1 ubuntu ubuntu 435 May 18 03:31 special_tokens_map.json\r\n-rw-rw-r--\
    \ 1 ubuntu ubuntu 499723 May 18 03:31 tokenizer.model\r\n-rw-rw-r-- 1 ubuntu ubuntu\
    \ 727 May 18 03:31 tokenizer_config.json\r\n-rw-rw-r-- 1 ubuntu ubuntu 4830 May\
    \ 18 03:31 trainer_state.json\r\n-rw-rw-r-- 1 ubuntu ubuntu 3771 May 18 03:31\
    \ training_args.bin\r\n\r\nIs my situation Correct or Not? Thanks."
  created_at: 2023-06-11 14:41:36+00:00
  edited: false
  hidden: false
  id: 6485eb30166981082f0253c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T15:47:56.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20490425825119019
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Because yours is in float32.  It's not a big problem, except it\
          \ uses more disk space.  If you want you can use this script to convert\
          \ it to float16:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForCausalLM\n<span class=\"hljs-keyword\">import</span> argparse\n\
          \nparser = argparse.ArgumentParser(description=<span class=\"hljs-string\"\
          >'Convert fp32 model to fp16'</span>)\nparser.add_argument(<span class=\"\
          hljs-string\">'model_dir'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'fp32 model folder'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'output_dir'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'fp16 output folder'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--device'</span>, <span\
          \ class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>,\
          \ default=<span class=\"hljs-string\">\"cuda:0\"</span>, <span class=\"\
          hljs-built_in\">help</span>=<span class=\"hljs-string\">'device'</span>)\n\
          \nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\noutput_dir\
          \ = args.output_dir\n\nmodel = AutoModelForCausalLM.from_pretrained(\n \
          \           model_dir,\n            torch_dtype=torch.float32,\n       \
          \     low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>,\n    \
          \        )\n\nmodel = model.half()\n\nmodel.save_pretrained(\n         \
          \   output_dir, torch_dtype=torch.float16\n            )\n</code></pre>\n"
        raw: "Because yours is in float32.  It's not a big problem, except it uses\
          \ more disk space.  If you want you can use this script to convert it to\
          \ float16:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\nimport argparse\n\nparser = argparse.ArgumentParser(description='Convert\
          \ fp32 model to fp16')\nparser.add_argument('model_dir', type=str, help='fp32\
          \ model folder')\nparser.add_argument('output_dir', type=str, help='fp16\
          \ output folder')\nparser.add_argument('--device', type=str, default=\"\
          cuda:0\", help='device')\n\nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\n\
          output_dir = args.output_dir\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \            model_dir,\n            torch_dtype=torch.float32,\n      \
          \      low_cpu_mem_usage=True,\n            )\n\nmodel = model.half()\n\n\
          model.save_pretrained(\n            output_dir, torch_dtype=torch.float16\n\
          \            )\n```"
        updatedAt: '2023-06-11T15:54:40.115Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - JohanFire
    id: 6485ecac4b2f11aeb7c7743e
    type: comment
  author: TheBloke
  content: "Because yours is in float32.  It's not a big problem, except it uses more\
    \ disk space.  If you want you can use this script to convert it to float16:\n\
    \n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import argparse\n\nparser = argparse.ArgumentParser(description='Convert fp32\
    \ model to fp16')\nparser.add_argument('model_dir', type=str, help='fp32 model\
    \ folder')\nparser.add_argument('output_dir', type=str, help='fp16 output folder')\n\
    parser.add_argument('--device', type=str, default=\"cuda:0\", help='device')\n\
    \nargs = parser.parse_args()\n\nmodel_dir =  args.model_dir\noutput_dir = args.output_dir\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(\n            model_dir,\n    \
    \        torch_dtype=torch.float32,\n            low_cpu_mem_usage=True,\n   \
    \         )\n\nmodel = model.half()\n\nmodel.save_pretrained(\n            output_dir,\
    \ torch_dtype=torch.float16\n            )\n```"
  created_at: 2023-06-11 14:47:56+00:00
  edited: true
  hidden: false
  id: 6485ecac4b2f11aeb7c7743e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b78d371144f0f0af3037b7adc7c5b133.svg
      fullname: Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: David003
      type: user
    createdAt: '2023-06-11T15:53:26.000Z'
    data:
      edited: false
      editors:
      - David003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9095081686973572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b78d371144f0f0af3037b7adc7c5b133.svg
          fullname: Lee
          isHf: false
          isPro: false
          name: David003
          type: user
        html: '<p>Thank you very much!</p>

          '
        raw: Thank you very much!
        updatedAt: '2023-06-11T15:53:26.808Z'
      numEdits: 0
      reactions: []
    id: 6485edf645782485dfe0d2cd
    type: comment
  author: David003
  content: Thank you very much!
  created_at: 2023-06-11 14:53:26+00:00
  edited: false
  hidden: false
  id: 6485edf645782485dfe0d2cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6529453b0cff99f47121d3bf25a7e5b0.svg
      fullname: Kevin Ehsani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kehsani
      type: user
    createdAt: '2023-06-25T21:36:17.000Z'
    data:
      edited: true
      editors:
      - kehsani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9522668123245239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6529453b0cff99f47121d3bf25a7e5b0.svg
          fullname: Kevin Ehsani
          isHf: false
          isPro: false
          name: kehsani
          type: user
        html: '<p>Trying to quantize this model to 4-bits:</p>

          <p>On ubuntu and got killed response after running the command line as follows
          " python llama.py weights/vicuna-7b-delta-v1.1 c4 --wbits 4 --true-sequential
          --act-order --groupsize 128 --save llama7b-4bit-128g.pt<br>Loading checkpoint
          shards:   0%|                                                                  |
          0/2 [00:00&lt;?, ?it/s]<br>Killed"<br>My gpu is RTX A2000 which has 8GB
          of memory unless I need to reduce the model to 16 bits so it would fit on
          gpu as it seems to be around 14GB and 32bits<br>I tried running the 32-16
          bit conversion both as gpu and cpu and still got killed by the system.</p>

          '
        raw: 'Trying to quantize this model to 4-bits:


          On ubuntu and got killed response after running the command line as follows
          " python llama.py weights/vicuna-7b-delta-v1.1 c4 --wbits 4 --true-sequential
          --act-order --groupsize 128 --save llama7b-4bit-128g.pt

          Loading checkpoint shards:   0%|                                                                  |
          0/2 [00:00<?, ?it/s]

          Killed"

          My gpu is RTX A2000 which has 8GB of memory unless I need to reduce the
          model to 16 bits so it would fit on gpu as it seems to be around 14GB and
          32bits

          I tried running the 32-16 bit conversion both as gpu and cpu and still got
          killed by the system.'
        updatedAt: '2023-06-26T13:44:26.603Z'
      numEdits: 3
      reactions: []
    id: 6498b351f806925183707b67
    type: comment
  author: kehsani
  content: 'Trying to quantize this model to 4-bits:


    On ubuntu and got killed response after running the command line as follows "
    python llama.py weights/vicuna-7b-delta-v1.1 c4 --wbits 4 --true-sequential --act-order
    --groupsize 128 --save llama7b-4bit-128g.pt

    Loading checkpoint shards:   0%|                                                                  |
    0/2 [00:00<?, ?it/s]

    Killed"

    My gpu is RTX A2000 which has 8GB of memory unless I need to reduce the model
    to 16 bits so it would fit on gpu as it seems to be around 14GB and 32bits

    I tried running the 32-16 bit conversion both as gpu and cpu and still got killed
    by the system.'
  created_at: 2023-06-25 20:36:17+00:00
  edited: true
  hidden: false
  id: 6498b351f806925183707b67
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: lmsys/vicuna-7b-delta-v1.1
repo_type: model
status: open
target_branch: null
title: Size of My  vicuna model is  twice of yours. Why is that?
