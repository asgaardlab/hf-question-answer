!!python/object:huggingface_hub.community.DiscussionWithDetails
author: beomi
conflicting_files: []
created_at: 2023-11-15 01:57:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: beomi
      type: user
    createdAt: '2023-11-15T01:57:13.000Z'
    data:
      edited: false
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5870788097381592
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: "<h1 id=\"proposal-of-the-request\">Proposal of the request</h1>\n<p>Fix\
          \ the ValueError when save model using <code>.save_pretrained</code> due\
          \ to the non-contiguous tensor in Midm model</p>\n<h2 id=\"err-log\">Err\
          \ log</h2>\n<ul>\n<li>ValueError happends on both directly calling <code>save_pretrained</code>\
          \ and when using <code>Trainer</code> of the transformers library.</li>\n\
          </ul>\n<pre><code>ValueError: You are trying to save a non contiguous tensor:\
          \ `transformer.h.0.attn.c_attn.weight` which is not allowed. It either means\
          \ you are trying to save tensors which are reference of each other in which\
          \ case it's recommended to save only the full tensors, and reslice at load\
          \ time, or simply call `.contiguous()` on your tensor to pack it before\
          \ saving.\n</code></pre>\n<h2 id=\"full-code--traceback\">Full Code &amp;\
          \ Traceback</h2>\n<pre><code>Python 3.10.13 (main, Sep 11 2023, 13:44:35)\
          \ [GCC 11.2.0]\nType 'copyright', 'credits' or 'license' for more information\n\
          IPython 8.17.2 -- An enhanced Interactive Python. Type '?' for help.\n\n\
          In [1]: from transformers import AutoModelForCausalLM\nimport to\nIn [2]:\
          \ import torch\n\nIn [3]: model = AutoModelForCausalLM.from_pretrained(\n\
          \   ...: 'KT-AI/midm-bitext-S-7B-inst-v1', device_map={'':1})\nThe repository\
          \ for KT-AI/midm-bitext-S-7B-inst-v1 contains custom code which must be\
          \ executed to correctly load the model. You can inspect the repository content\
          \ at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\nYou can avoid this prompt\
          \ in future by passing the argument `trust_remote_code=True`.\n\nDo you\
          \ wish to run the custom code? [y/N] y\nThe repository for KT-AI/midm-bitext-S-7B-inst-v1\
          \ contains custom code which must be executed to correctly load the model.\
          \ You can inspect the repository content at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\n\
          You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\
          \nDo you wish to run the custom code? [y/N] y\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:18&lt;00:00,  9.44s/it]\n\nIn [4]: model\nOut[4]: \nMidmLMHeadModel(\n\
          \  (transformer): MidmModel(\n    (wte): Embedding(72192, 4096)\n    (rotary_pos_emb):\
          \ RotaryEmbedding()\n    (drop): Dropout(p=0.0, inplace=False)\n    (h):\
          \ ModuleList(\n      (0-31): 32 x MidmBlock(\n        (ln_1): LayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n        (attn): MidmAttention(\n\
          \          (c_attn): Linear(in_features=4096, out_features=12288, bias=False)\n\
          \          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout):\
          \ Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n        (mlp): MidmMLP(\n       \
          \   (c_fc): Linear(in_features=4096, out_features=21760, bias=False)\n \
          \         (c_proj): Linear(in_features=10880, out_features=4096, bias=False)\n\
          \          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n\
          \    )\n    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
          \  )\n  (lm_head): Linear(in_features=4096, out_features=72192, bias=False)\n\
          )\n\nIn [5]: model.save_pretrained('test')\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[5], line 1\n----&gt; 1 model.save_pretrained('test')\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/transformers/modeling_utils.py:2187,\
          \ in PreTrainedModel.save_pretrained(self, save_directory, is_main_process,\
          \ state_dict, save_function, push_to_hub, max_shard_size, safe_serialization,\
          \ variant, token, save_peft_format, **kwargs)\n   2183 for shard_file, shard\
          \ in shards.items():\n   2184     if safe_serialization:\n   2185      \
          \   # At some point we will need to deal better with save_function (used\
          \ for TPU and other distributed\n   2186         # joyfulness), but for\
          \ now this enough.\n-&gt; 2187         safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})\n   2188     else:\n   2189\
          \         save_function(shard, os.path.join(save_directory, shard_file))\n\
          \nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:281,\
          \ in save_file(tensors, filename, metadata)\n    250 def save_file(\n  \
          \  251     tensors: Dict[str, torch.Tensor],\n    252     filename: Union[str,\
          \ os.PathLike],\n    253     metadata: Optional[Dict[str, str]] = None,\n\
          \    254 ):\n    255     \"\"\"\n    256     Saves a dictionary of tensors\
          \ into raw bytes in safetensors format.\n    257 \n   (...)\n    279   \
          \  ```\n    280     \"\"\"\n--&gt; 281     serialize_file(_flatten(tensors),\
          \ filename, metadata=metadata)\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:475,\
          \ in _flatten(tensors)\n    466 if failing:\n    467     raise RuntimeError(\n\
          \    468         f\"\"\"\n    469         Some tensors share memory, this\
          \ will lead to duplicate memory on disk and potential differences when loading\
          \ them again: {failing}.\n   (...)\n    472         \"\"\"\n    473    \
          \ )\n--&gt; 475 return {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\"\
          .\")[-1],\n    478         \"shape\": v.shape,\n    479         \"data\"\
          : _tobytes(v, k),\n    480     }\n    481     for k, v in tensors.items()\n\
          \    482 }\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:479,\
          \ in &lt;dictcomp&gt;(.0)\n    466 if failing:\n    467     raise RuntimeError(\n\
          \    468         f\"\"\"\n    469         Some tensors share memory, this\
          \ will lead to duplicate memory on disk and potential differences when loading\
          \ them again: {failing}.\n   (...)\n    472         \"\"\"\n    473    \
          \ )\n    475 return {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\"\
          .\")[-1],\n    478         \"shape\": v.shape,\n--&gt; 479         \"data\"\
          : _tobytes(v, k),\n    480     }\n    481     for k, v in tensors.items()\n\
          \    482 }\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:396,\
          \ in _tobytes(tensor, name)\n    389     raise ValueError(\n    390    \
          \     f\"You are trying to save a sparse tensor: `{name}` which this library\
          \ does not support.\"\n    391         \" You can make it a dense tensor\
          \ before saving with `.to_dense()` but be aware this might\"\n    392  \
          \       \" make a much larger file than needed.\"\n    393     )\n    395\
          \ if not tensor.is_contiguous():\n--&gt; 396     raise ValueError(\n   \
          \ 397         f\"You are trying to save a non contiguous tensor: `{name}`\
          \ which is not allowed. It either means you\"\n    398         \" are trying\
          \ to save tensors which are reference of each other in which case it's recommended\
          \ to save\"\n    399         \" only the full tensors, and reslice at load\
          \ time, or simply call `.contiguous()` on your tensor to\"\n    400    \
          \     \" pack it before saving.\"\n    401     )\n    402 if tensor.device.type\
          \ != \"cpu\":\n    403     # Moving tensor to cpu before saving\n    404\
          \     tensor = tensor.to(\"cpu\")\n\nValueError: You are trying to save\
          \ a non contiguous tensor: `transformer.h.0.attn.c_attn.weight` which is\
          \ not allowed. It either means you are trying to save tensors which are\
          \ reference of each other in which case it's recommended to save only the\
          \ full tensors, and reslice at load time, or simply call `.contiguous()`\
          \ on your tensor to pack it before saving.\n</code></pre>\n<h1 id=\"solution\"\
          >Solution</h1>\n<p>Override <code>.save_pretrained</code> method on <code>MidmPreTrainedModel</code>\
          \ to make model's tensor  contiguous.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >MidmPreTrainedModel</span>(<span class=\"hljs-title class_ inherited__\"\
          >PreTrainedModel</span>):\n    <span class=\"hljs-comment\"># ... [other\
          \ methods and properties of the class]</span>\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">make_tensors_contiguous</span>(<span\
          \ class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\"\
          >for</span> name, param <span class=\"hljs-keyword\">in</span> self.named_parameters():\n\
          \            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> param.is_contiguous():\n                param.data = param.data.contiguous()\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >save_pretrained</span>(<span class=\"hljs-params\">self, save_directory,\
          \ **kwargs</span>):\n        <span class=\"hljs-comment\"># Make tensors\
          \ contiguous</span>\n        self.make_tensors_contiguous()\n\n        <span\
          \ class=\"hljs-comment\"># Call the original save_pretrained method</span>\n\
          \        <span class=\"hljs-built_in\">super</span>().save_pretrained(save_directory,\
          \ **kwargs)\n\n<span class=\"hljs-comment\"># Other class definitions remain\
          \ unchanged</span>\n</code></pre>\n<h1 id=\"result\">Result</h1>\n<p><code>save_pretrained</code>\
          \ method works fine without error.</p>\n<pre><code>Python 3.10.13 (main,\
          \ Sep 11 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license'\
          \ for more information\nIPython 8.17.2 -- An enhanced Interactive Python.\
          \ Type '?' for help.\n\nIn [1]: from modeling_midm import MidmLMHeadModel\n\
          \nIn [2]: model = MidmLMHeadModel.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1',\
          \ device_map={'':1})\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:16&lt;00:00,\
          \  8.30s/it]\n\nIn [3]: model.save_pretrained('test')\n\nIn [4]: exit\n\
          </code></pre>\n"
        raw: "# Proposal of the request\n\nFix the ValueError when save model using\
          \ `.save_pretrained` due to the non-contiguous tensor in Midm model\n\n\
          ## Err log\n\n- ValueError happends on both directly calling `save_pretrained`\
          \ and when using `Trainer` of the transformers library.\n\n```\nValueError:\
          \ You are trying to save a non contiguous tensor: `transformer.h.0.attn.c_attn.weight`\
          \ which is not allowed. It either means you are trying to save tensors which\
          \ are reference of each other in which case it's recommended to save only\
          \ the full tensors, and reslice at load time, or simply call `.contiguous()`\
          \ on your tensor to pack it before saving.\n```\n\n## Full Code & Traceback\n\
          \n```\nPython 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright',\
          \ 'credits' or 'license' for more information\nIPython 8.17.2 -- An enhanced\
          \ Interactive Python. Type '?' for help.\n\nIn [1]: from transformers import\
          \ AutoModelForCausalLM\nimport to\nIn [2]: import torch\n\nIn [3]: model\
          \ = AutoModelForCausalLM.from_pretrained(\n   ...: 'KT-AI/midm-bitext-S-7B-inst-v1',\
          \ device_map={'':1})\nThe repository for KT-AI/midm-bitext-S-7B-inst-v1\
          \ contains custom code which must be executed to correctly load the model.\
          \ You can inspect the repository content at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\n\
          You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\
          \nDo you wish to run the custom code? [y/N] y\nThe repository for KT-AI/midm-bitext-S-7B-inst-v1\
          \ contains custom code which must be executed to correctly load the model.\
          \ You can inspect the repository content at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\n\
          You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\
          \nDo you wish to run the custom code? [y/N] y\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:18<00:00,  9.44s/it]\n\nIn [4]: model\nOut[4]: \nMidmLMHeadModel(\n\
          \  (transformer): MidmModel(\n    (wte): Embedding(72192, 4096)\n    (rotary_pos_emb):\
          \ RotaryEmbedding()\n    (drop): Dropout(p=0.0, inplace=False)\n    (h):\
          \ ModuleList(\n      (0-31): 32 x MidmBlock(\n        (ln_1): LayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n        (attn): MidmAttention(\n\
          \          (c_attn): Linear(in_features=4096, out_features=12288, bias=False)\n\
          \          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout):\
          \ Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n        (mlp): MidmMLP(\n       \
          \   (c_fc): Linear(in_features=4096, out_features=21760, bias=False)\n \
          \         (c_proj): Linear(in_features=10880, out_features=4096, bias=False)\n\
          \          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n\
          \    )\n    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
          \  )\n  (lm_head): Linear(in_features=4096, out_features=72192, bias=False)\n\
          )\n\nIn [5]: model.save_pretrained('test')\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[5], line 1\n----> 1 model.save_pretrained('test')\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/transformers/modeling_utils.py:2187,\
          \ in PreTrainedModel.save_pretrained(self, save_directory, is_main_process,\
          \ state_dict, save_function, push_to_hub, max_shard_size, safe_serialization,\
          \ variant, token, save_peft_format, **kwargs)\n   2183 for shard_file, shard\
          \ in shards.items():\n   2184     if safe_serialization:\n   2185      \
          \   # At some point we will need to deal better with save_function (used\
          \ for TPU and other distributed\n   2186         # joyfulness), but for\
          \ now this enough.\n-> 2187         safe_save_file(shard, os.path.join(save_directory,\
          \ shard_file), metadata={\"format\": \"pt\"})\n   2188     else:\n   2189\
          \         save_function(shard, os.path.join(save_directory, shard_file))\n\
          \nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:281,\
          \ in save_file(tensors, filename, metadata)\n    250 def save_file(\n  \
          \  251     tensors: Dict[str, torch.Tensor],\n    252     filename: Union[str,\
          \ os.PathLike],\n    253     metadata: Optional[Dict[str, str]] = None,\n\
          \    254 ):\n    255     \"\"\"\n    256     Saves a dictionary of tensors\
          \ into raw bytes in safetensors format.\n    257 \n   (...)\n    279   \
          \  ```\n    280     \"\"\"\n--> 281     serialize_file(_flatten(tensors),\
          \ filename, metadata=metadata)\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:475,\
          \ in _flatten(tensors)\n    466 if failing:\n    467     raise RuntimeError(\n\
          \    468         f\"\"\"\n    469         Some tensors share memory, this\
          \ will lead to duplicate memory on disk and potential differences when loading\
          \ them again: {failing}.\n   (...)\n    472         \"\"\"\n    473    \
          \ )\n--> 475 return {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\"\
          .\")[-1],\n    478         \"shape\": v.shape,\n    479         \"data\"\
          : _tobytes(v, k),\n    480     }\n    481     for k, v in tensors.items()\n\
          \    482 }\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:479,\
          \ in <dictcomp>(.0)\n    466 if failing:\n    467     raise RuntimeError(\n\
          \    468         f\"\"\"\n    469         Some tensors share memory, this\
          \ will lead to duplicate memory on disk and potential differences when loading\
          \ them again: {failing}.\n   (...)\n    472         \"\"\"\n    473    \
          \ )\n    475 return {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\"\
          .\")[-1],\n    478         \"shape\": v.shape,\n--> 479         \"data\"\
          : _tobytes(v, k),\n    480     }\n    481     for k, v in tensors.items()\n\
          \    482 }\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:396,\
          \ in _tobytes(tensor, name)\n    389     raise ValueError(\n    390    \
          \     f\"You are trying to save a sparse tensor: `{name}` which this library\
          \ does not support.\"\n    391         \" You can make it a dense tensor\
          \ before saving with `.to_dense()` but be aware this might\"\n    392  \
          \       \" make a much larger file than needed.\"\n    393     )\n    395\
          \ if not tensor.is_contiguous():\n--> 396     raise ValueError(\n    397\
          \         f\"You are trying to save a non contiguous tensor: `{name}` which\
          \ is not allowed. It either means you\"\n    398         \" are trying to\
          \ save tensors which are reference of each other in which case it's recommended\
          \ to save\"\n    399         \" only the full tensors, and reslice at load\
          \ time, or simply call `.contiguous()` on your tensor to\"\n    400    \
          \     \" pack it before saving.\"\n    401     )\n    402 if tensor.device.type\
          \ != \"cpu\":\n    403     # Moving tensor to cpu before saving\n    404\
          \     tensor = tensor.to(\"cpu\")\n\nValueError: You are trying to save\
          \ a non contiguous tensor: `transformer.h.0.attn.c_attn.weight` which is\
          \ not allowed. It either means you are trying to save tensors which are\
          \ reference of each other in which case it's recommended to save only the\
          \ full tensors, and reslice at load time, or simply call `.contiguous()`\
          \ on your tensor to pack it before saving.\n```\n\n# Solution\n\nOverride\
          \ `.save_pretrained` method on `MidmPreTrainedModel` to make model's tensor\
          \  contiguous.\n\n```python\nclass MidmPreTrainedModel(PreTrainedModel):\n\
          \    # ... [other methods and properties of the class]\n\n    def make_tensors_contiguous(self):\n\
          \        for name, param in self.named_parameters():\n            if not\
          \ param.is_contiguous():\n                param.data = param.data.contiguous()\n\
          \n    def save_pretrained(self, save_directory, **kwargs):\n        # Make\
          \ tensors contiguous\n        self.make_tensors_contiguous()\n\n       \
          \ # Call the original save_pretrained method\n        super().save_pretrained(save_directory,\
          \ **kwargs)\n\n# Other class definitions remain unchanged\n```\n\n# Result\n\
          \n`save_pretrained` method works fine without error.\n\n```\nPython 3.10.13\
          \ (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits'\
          \ or 'license' for more information\nIPython 8.17.2 -- An enhanced Interactive\
          \ Python. Type '?' for help.\n\nIn [1]: from modeling_midm import MidmLMHeadModel\n\
          \nIn [2]: model = MidmLMHeadModel.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1',\
          \ device_map={'':1})\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:16<00:00,\
          \  8.30s/it]\n\nIn [3]: model.save_pretrained('test')\n\nIn [4]: exit\n\
          ```"
        updatedAt: '2023-11-15T01:57:13.968Z'
      numEdits: 0
      reactions: []
    id: 6554257939bce75e6cc33225
    type: comment
  author: beomi
  content: "# Proposal of the request\n\nFix the ValueError when save model using\
    \ `.save_pretrained` due to the non-contiguous tensor in Midm model\n\n## Err\
    \ log\n\n- ValueError happends on both directly calling `save_pretrained` and\
    \ when using `Trainer` of the transformers library.\n\n```\nValueError: You are\
    \ trying to save a non contiguous tensor: `transformer.h.0.attn.c_attn.weight`\
    \ which is not allowed. It either means you are trying to save tensors which are\
    \ reference of each other in which case it's recommended to save only the full\
    \ tensors, and reslice at load time, or simply call `.contiguous()` on your tensor\
    \ to pack it before saving.\n```\n\n## Full Code & Traceback\n\n```\nPython 3.10.13\
    \ (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license'\
    \ for more information\nIPython 8.17.2 -- An enhanced Interactive Python. Type\
    \ '?' for help.\n\nIn [1]: from transformers import AutoModelForCausalLM\nimport\
    \ to\nIn [2]: import torch\n\nIn [3]: model = AutoModelForCausalLM.from_pretrained(\n\
    \   ...: 'KT-AI/midm-bitext-S-7B-inst-v1', device_map={'':1})\nThe repository\
    \ for KT-AI/midm-bitext-S-7B-inst-v1 contains custom code which must be executed\
    \ to correctly load the model. You can inspect the repository content at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\n\
    You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\
    \nDo you wish to run the custom code? [y/N] y\nThe repository for KT-AI/midm-bitext-S-7B-inst-v1\
    \ contains custom code which must be executed to correctly load the model. You\
    \ can inspect the repository content at https://hf.co/KT-AI/midm-bitext-S-7B-inst-v1.\n\
    You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\
    \nDo you wish to run the custom code? [y/N] y\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:18<00:00,  9.44s/it]\n\nIn [4]:\
    \ model\nOut[4]: \nMidmLMHeadModel(\n  (transformer): MidmModel(\n    (wte): Embedding(72192,\
    \ 4096)\n    (rotary_pos_emb): RotaryEmbedding()\n    (drop): Dropout(p=0.0, inplace=False)\n\
    \    (h): ModuleList(\n      (0-31): 32 x MidmBlock(\n        (ln_1): LayerNorm((4096,),\
    \ eps=1e-05, elementwise_affine=True)\n        (attn): MidmAttention(\n      \
    \    (c_attn): Linear(in_features=4096, out_features=12288, bias=False)\n    \
    \      (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n   \
    \       (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout):\
    \ Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((4096,),\
    \ eps=1e-05, elementwise_affine=True)\n        (mlp): MidmMLP(\n          (c_fc):\
    \ Linear(in_features=4096, out_features=21760, bias=False)\n          (c_proj):\
    \ Linear(in_features=10880, out_features=4096, bias=False)\n          (dropout):\
    \ Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4096,),\
    \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4096,\
    \ out_features=72192, bias=False)\n)\n\nIn [5]: model.save_pretrained('test')\n\
    ---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[5], line 1\n----> 1 model.save_pretrained('test')\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/transformers/modeling_utils.py:2187,\
    \ in PreTrainedModel.save_pretrained(self, save_directory, is_main_process, state_dict,\
    \ save_function, push_to_hub, max_shard_size, safe_serialization, variant, token,\
    \ save_peft_format, **kwargs)\n   2183 for shard_file, shard in shards.items():\n\
    \   2184     if safe_serialization:\n   2185         # At some point we will need\
    \ to deal better with save_function (used for TPU and other distributed\n   2186\
    \         # joyfulness), but for now this enough.\n-> 2187         safe_save_file(shard,\
    \ os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n \
    \  2188     else:\n   2189         save_function(shard, os.path.join(save_directory,\
    \ shard_file))\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:281,\
    \ in save_file(tensors, filename, metadata)\n    250 def save_file(\n    251 \
    \    tensors: Dict[str, torch.Tensor],\n    252     filename: Union[str, os.PathLike],\n\
    \    253     metadata: Optional[Dict[str, str]] = None,\n    254 ):\n    255 \
    \    \"\"\"\n    256     Saves a dictionary of tensors into raw bytes in safetensors\
    \ format.\n    257 \n   (...)\n    279     ```\n    280     \"\"\"\n--> 281  \
    \   serialize_file(_flatten(tensors), filename, metadata=metadata)\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:475,\
    \ in _flatten(tensors)\n    466 if failing:\n    467     raise RuntimeError(\n\
    \    468         f\"\"\"\n    469         Some tensors share memory, this will\
    \ lead to duplicate memory on disk and potential differences when loading them\
    \ again: {failing}.\n   (...)\n    472         \"\"\"\n    473     )\n--> 475\
    \ return {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\"\
    .\")[-1],\n    478         \"shape\": v.shape,\n    479         \"data\": _tobytes(v,\
    \ k),\n    480     }\n    481     for k, v in tensors.items()\n    482 }\n\nFile\
    \ ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:479,\
    \ in <dictcomp>(.0)\n    466 if failing:\n    467     raise RuntimeError(\n  \
    \  468         f\"\"\"\n    469         Some tensors share memory, this will lead\
    \ to duplicate memory on disk and potential differences when loading them again:\
    \ {failing}.\n   (...)\n    472         \"\"\"\n    473     )\n    475 return\
    \ {\n    476     k: {\n    477         \"dtype\": str(v.dtype).split(\".\")[-1],\n\
    \    478         \"shape\": v.shape,\n--> 479         \"data\": _tobytes(v, k),\n\
    \    480     }\n    481     for k, v in tensors.items()\n    482 }\n\nFile ~/anaconda3/envs/career-chatbot-trainer-clm/lib/python3.10/site-packages/safetensors/torch.py:396,\
    \ in _tobytes(tensor, name)\n    389     raise ValueError(\n    390         f\"\
    You are trying to save a sparse tensor: `{name}` which this library does not support.\"\
    \n    391         \" You can make it a dense tensor before saving with `.to_dense()`\
    \ but be aware this might\"\n    392         \" make a much larger file than needed.\"\
    \n    393     )\n    395 if not tensor.is_contiguous():\n--> 396     raise ValueError(\n\
    \    397         f\"You are trying to save a non contiguous tensor: `{name}` which\
    \ is not allowed. It either means you\"\n    398         \" are trying to save\
    \ tensors which are reference of each other in which case it's recommended to\
    \ save\"\n    399         \" only the full tensors, and reslice at load time,\
    \ or simply call `.contiguous()` on your tensor to\"\n    400         \" pack\
    \ it before saving.\"\n    401     )\n    402 if tensor.device.type != \"cpu\"\
    :\n    403     # Moving tensor to cpu before saving\n    404     tensor = tensor.to(\"\
    cpu\")\n\nValueError: You are trying to save a non contiguous tensor: `transformer.h.0.attn.c_attn.weight`\
    \ which is not allowed. It either means you are trying to save tensors which are\
    \ reference of each other in which case it's recommended to save only the full\
    \ tensors, and reslice at load time, or simply call `.contiguous()` on your tensor\
    \ to pack it before saving.\n```\n\n# Solution\n\nOverride `.save_pretrained`\
    \ method on `MidmPreTrainedModel` to make model's tensor  contiguous.\n\n```python\n\
    class MidmPreTrainedModel(PreTrainedModel):\n    # ... [other methods and properties\
    \ of the class]\n\n    def make_tensors_contiguous(self):\n        for name, param\
    \ in self.named_parameters():\n            if not param.is_contiguous():\n   \
    \             param.data = param.data.contiguous()\n\n    def save_pretrained(self,\
    \ save_directory, **kwargs):\n        # Make tensors contiguous\n        self.make_tensors_contiguous()\n\
    \n        # Call the original save_pretrained method\n        super().save_pretrained(save_directory,\
    \ **kwargs)\n\n# Other class definitions remain unchanged\n```\n\n# Result\n\n\
    `save_pretrained` method works fine without error.\n\n```\nPython 3.10.13 (main,\
    \ Sep 11 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license'\
    \ for more information\nIPython 8.17.2 -- An enhanced Interactive Python. Type\
    \ '?' for help.\n\nIn [1]: from modeling_midm import MidmLMHeadModel\n\nIn [2]:\
    \ model = MidmLMHeadModel.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1', device_map={'':1})\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
    \ [00:16<00:00,  8.30s/it]\n\nIn [3]: model.save_pretrained('test')\n\nIn [4]:\
    \ exit\n```"
  created_at: 2023-11-15 01:57:13+00:00
  edited: false
  hidden: false
  id: 6554257939bce75e6cc33225
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: beomi
      type: user
    createdAt: '2023-11-15T01:57:14.000Z'
    data:
      oid: eccfae3da13a0c5655572ed4b34fb063cad13e19
      parents:
      - 49a1efa8441c3ed8093d6cc48d0d138b65679cde
      subject: Fix ValueError when save model using `.save_pretrained` due to the
        non-contiguous tensor in Midm model
    id: 6554257a0000000000000000
    type: commit
  author: beomi
  created_at: 2023-11-15 01:57:14+00:00
  id: 6554257a0000000000000000
  oid: eccfae3da13a0c5655572ed4b34fb063cad13e19
  summary: Fix ValueError when save model using `.save_pretrained` due to the non-contiguous
    tensor in Midm model
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: beomi
      type: user
    createdAt: '2023-11-15T02:07:09.000Z'
    data:
      edited: true
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2689308226108551
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: "<h1 id=\"test-code---works-fine\">Test code - works fine!</h1>\n<p>You\
          \ can test this PR version using <code>revision='refs/pr/9'</code>.</p>\n\
          <p>Here's the test code below:</p>\n<pre><code class=\"language-python\"\
          >Python <span class=\"hljs-number\">3.10</span><span class=\"hljs-number\"\
          >.13</span> (main, Sep <span class=\"hljs-number\">11</span> <span class=\"\
          hljs-number\">2023</span>, <span class=\"hljs-number\">13</span>:<span class=\"\
          hljs-number\">44</span>:<span class=\"hljs-number\">35</span>) [GCC <span\
          \ class=\"hljs-number\">11.2</span><span class=\"hljs-number\">.0</span>]\n\
          <span class=\"hljs-type\">Type</span> <span class=\"hljs-string\">'copyright'</span>,\
          \ <span class=\"hljs-string\">'credits'</span> <span class=\"hljs-keyword\"\
          >or</span> <span class=\"hljs-string\">'license'</span> <span class=\"hljs-keyword\"\
          >for</span> more information\nIPython <span class=\"hljs-number\">8.17</span><span\
          \ class=\"hljs-number\">.2</span> -- An enhanced Interactive Python. <span\
          \ class=\"hljs-type\">Type</span> <span class=\"hljs-string\">'?'</span>\
          \ <span class=\"hljs-keyword\">for</span> <span class=\"hljs-built_in\"\
          >help</span>.\n\nIn [<span class=\"hljs-number\">1</span>]: <span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoModelForCausalLM\ni\nIn [<span class=\"hljs-number\">2</span>]: <span\
          \ class=\"hljs-keyword\">import</span> torch\n\nIn [<span class=\"hljs-number\"\
          >3</span>]: model = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">'KT-AI/midm-bitext-S-7B-inst-v1'</span>, revision=<span class=\"\
          hljs-string\">'refs/pr/9'</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>, torch_dtype=torch.bfloat16, device_map={<span class=\"hljs-string\"\
          >''</span>:<span class=\"hljs-number\">0</span>})\nLoading checkpoint shards:\
          \ <span class=\"hljs-number\">100</span>%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| <span class=\"hljs-number\"\
          >2</span>/<span class=\"hljs-number\">2</span> [<span class=\"hljs-number\"\
          >00</span>:<span class=\"hljs-number\">12</span>&lt;<span class=\"hljs-number\"\
          >00</span>:<span class=\"hljs-number\">00</span>,  <span class=\"hljs-number\"\
          >6.43</span>s/it]\n\nIn [<span class=\"hljs-number\">4</span>]: model.save_pretrained(<span\
          \ class=\"hljs-string\">'test'</span>)\n\nIn [<span class=\"hljs-number\"\
          >5</span>]: \n</code></pre>\n"
        raw: "# Test code - works fine!\n\nYou can test this PR version using `revision='refs/pr/9'`.\n\
          \nHere's the test code below:\n\n```python\nPython 3.10.13 (main, Sep 11\
          \ 2023, 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license'\
          \ for more information\nIPython 8.17.2 -- An enhanced Interactive Python.\
          \ Type '?' for help.\n\nIn [1]: from transformers import AutoModelForCausalLM\n\
          i\nIn [2]: import torch\n\nIn [3]: model = AutoModelForCausalLM.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1',\
          \ revision='refs/pr/9', trust_remote_code=True, torch_dtype=torch.bfloat16,\
          \ device_map={'':0})\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,\
          \  6.43s/it]\n\nIn [4]: model.save_pretrained('test')\n\nIn [5]: \n```"
        updatedAt: '2023-11-15T02:09:35.842Z'
      numEdits: 2
      reactions: []
    id: 655427cd914e42998e090b22
    type: comment
  author: beomi
  content: "# Test code - works fine!\n\nYou can test this PR version using `revision='refs/pr/9'`.\n\
    \nHere's the test code below:\n\n```python\nPython 3.10.13 (main, Sep 11 2023,\
    \ 13:44:35) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license' for more information\n\
    IPython 8.17.2 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]:\
    \ from transformers import AutoModelForCausalLM\ni\nIn [2]: import torch\n\nIn\
    \ [3]: model = AutoModelForCausalLM.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1',\
    \ revision='refs/pr/9', trust_remote_code=True, torch_dtype=torch.bfloat16, device_map={'':0})\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
    \ [00:12<00:00,  6.43s/it]\n\nIn [4]: model.save_pretrained('test')\n\nIn [5]:\
    \ \n```"
  created_at: 2023-11-15 02:07:09+00:00
  edited: true
  hidden: false
  id: 655427cd914e42998e090b22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eee110202c00f32afb01b21870af38c9.svg
      fullname: taehyeong_kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ktthkim
      type: user
    createdAt: '2023-11-21T01:41:16.000Z'
    data:
      edited: false
      editors:
      - ktthkim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9484014511108398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eee110202c00f32afb01b21870af38c9.svg
          fullname: taehyeong_kim
          isHf: false
          isPro: false
          name: ktthkim
          type: user
        html: '<p>Thank you for submitting your PR and conducting the test. We''ve
          reviewed your PR internally, and it looks good.<br> Additionally, we''ve
          updated the SafeSensor format checkpoint.</p>

          '
        raw: "Thank you for submitting your PR and conducting the test. We've reviewed\
          \ your PR internally, and it looks good.\n Additionally, we've updated the\
          \ SafeSensor format checkpoint."
        updatedAt: '2023-11-21T01:41:16.538Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - beomi
      relatedEventId: 655c0abc31c4978366c1e660
    id: 655c0abc31c4978366c1e65e
    type: comment
  author: ktthkim
  content: "Thank you for submitting your PR and conducting the test. We've reviewed\
    \ your PR internally, and it looks good.\n Additionally, we've updated the SafeSensor\
    \ format checkpoint."
  created_at: 2023-11-21 01:41:16+00:00
  edited: false
  hidden: false
  id: 655c0abc31c4978366c1e65e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eee110202c00f32afb01b21870af38c9.svg
      fullname: taehyeong_kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ktthkim
      type: user
    createdAt: '2023-11-21T01:41:16.000Z'
    data:
      status: merged
    id: 655c0abc31c4978366c1e660
    type: status-change
  author: ktthkim
  created_at: 2023-11-21 01:41:16+00:00
  id: 655c0abc31c4978366c1e660
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: dcdcb5bf17d8072877e9cff4269ad24c6b1c17a7
num: 9
repo_id: KT-AI/midm-bitext-S-7B-inst-v1
repo_type: model
status: merged
target_branch: refs/heads/main
title: Fix ValueError when save model using `.save_pretrained` due to the non-contiguous
  tensor in Midm model
