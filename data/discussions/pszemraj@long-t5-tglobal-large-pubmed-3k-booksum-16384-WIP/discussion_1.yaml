!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jorgeutd
conflicting_files: null
created_at: 2022-07-01 18:24:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
      fullname: Jorge Lopez Grisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jorgeutd
      type: user
    createdAt: '2022-07-01T19:24:35.000Z'
    data:
      edited: false
      editors:
      - Jorgeutd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
          fullname: Jorge Lopez Grisman
          isHf: false
          isPro: false
          name: Jorgeutd
          type: user
        html: '<p>Hi Peter,</p>

          <p>Thank for sharing this. Do you have any examples Colab / Notebook on
          how to set up this type of model for training using the trainer API? I tried
          to follow the same process that I used for Longformer but the training metrics
          were 0 almost of all the time.</p>

          <p>Thank you so much. </p>

          '
        raw: "Hi Peter,\r\n\r\nThank for sharing this. Do you have any examples Colab\
          \ / Notebook on how to set up this type of model for training using the\
          \ trainer API? I tried to follow the same process that I used for Longformer\
          \ but the training metrics were 0 almost of all the time.\r\n\r\nThank you\
          \ so much. "
        updatedAt: '2022-07-01T19:24:35.278Z'
      numEdits: 0
      reactions: []
    id: 62bf49f334afe69e1a1bef2d
    type: comment
  author: Jorgeutd
  content: "Hi Peter,\r\n\r\nThank for sharing this. Do you have any examples Colab\
    \ / Notebook on how to set up this type of model for training using the trainer\
    \ API? I tried to follow the same process that I used for Longformer but the training\
    \ metrics were 0 almost of all the time.\r\n\r\nThank you so much. "
  created_at: 2022-07-01 18:24:35+00:00
  edited: false
  hidden: false
  id: 62bf49f334afe69e1a1bef2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-07-04T13:48:04.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Hi Jorge, </p>

          <p>Unfortunately, I don''t have a notebook that I can immediately share
          (I use one for several different things with API tokens etc., in there);
          after I get that cleaned up-which might take a while, I am happy to share
          that. </p>

          <p>That said, however, using Patrick Von Platen''s <a rel="nofollow" href="https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb">LED
          notebook</a> should work okay. The main difference between that and what
          I use is the addition of <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero">deepspeed</a>,
          which you may want to try out! Btw, it is worth noting that in this size
          large model, I can only get to train with 16384 tokens input on an A100
          GPU runtime. </p>

          <p>Hope that helps!</p>

          '
        raw: "Hi Jorge, \n\nUnfortunately, I don't have a notebook that I can immediately\
          \ share (I use one for several different things with API tokens etc., in\
          \ there); after I get that cleaned up-which might take a while, I am happy\
          \ to share that. \n\nThat said, however, using Patrick Von Platen's [LED\
          \ notebook](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)\
          \ should work okay. The main difference between that and what I use is the\
          \ addition of [deepspeed](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero),\
          \ which you may want to try out! Btw, it is worth noting that in this size\
          \ large model, I can only get to train with 16384 tokens input on an A100\
          \ GPU runtime. \n\nHope that helps!"
        updatedAt: '2022-07-04T13:48:04.667Z'
      numEdits: 0
      reactions: []
    id: 62c2ef941e28bc698d535d59
    type: comment
  author: pszemraj
  content: "Hi Jorge, \n\nUnfortunately, I don't have a notebook that I can immediately\
    \ share (I use one for several different things with API tokens etc., in there);\
    \ after I get that cleaned up-which might take a while, I am happy to share that.\
    \ \n\nThat said, however, using Patrick Von Platen's [LED notebook](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)\
    \ should work okay. The main difference between that and what I use is the addition\
    \ of [deepspeed](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero),\
    \ which you may want to try out! Btw, it is worth noting that in this size large\
    \ model, I can only get to train with 16384 tokens input on an A100 GPU runtime.\
    \ \n\nHope that helps!"
  created_at: 2022-07-04 12:48:04+00:00
  edited: false
  hidden: false
  id: 62c2ef941e28bc698d535d59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
      fullname: Jorge Lopez Grisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jorgeutd
      type: user
    createdAt: '2022-07-04T13:49:19.000Z'
    data:
      edited: false
      editors:
      - Jorgeutd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
          fullname: Jorge Lopez Grisman
          isHf: false
          isPro: false
          name: Jorgeutd
          type: user
        html: '<p>This is definitely helpful. Thank you Peter.</p>

          '
        raw: This is definitely helpful. Thank you Peter.
        updatedAt: '2022-07-04T13:49:19.848Z'
      numEdits: 0
      reactions: []
    id: 62c2efdfeb9ded74358ce1df
    type: comment
  author: Jorgeutd
  content: This is definitely helpful. Thank you Peter.
  created_at: 2022-07-04 12:49:19+00:00
  edited: false
  hidden: false
  id: 62c2efdfeb9ded74358ce1df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: pszemraj/long-t5-tglobal-large-pubmed-3k-booksum-16384-WIP
repo_type: model
status: open
target_branch: null
title: 'Training set up '
