!!python/object:huggingface_hub.community.DiscussionWithDetails
author: djtech
conflicting_files: null
created_at: 2023-06-18 12:53:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95ed4d92a5ae5cc853e7bb419b8c674a.svg
      fullname: 'G. '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djtech
      type: user
    createdAt: '2023-06-18T13:53:24.000Z'
    data:
      edited: false
      editors:
      - djtech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5191137194633484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95ed4d92a5ae5cc853e7bb419b8c674a.svg
          fullname: 'G. '
          isHf: false
          isPro: false
          name: djtech
          type: user
        html: '<p>I try using this model with llama.cpp, but this is the result:</p>

          <p><code>&gt; ./main -m ./models/ggml-alpaca-7b-q4.bin -n 256 --repeat_penalty
          1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt   main: build = 708
          (8596af4)   main: seed  = 1687096224   llama.cpp: loading model from ./models/ggml-alpaca-7b-q4.bin
          error loading model: unexpectedly reached end of file llama_init_from_file:
          failed to load model llama_init_from_gpt_params: error: failed to load model
          ''./models/ggml-alpaca-7b-q4.bin'' main: error: unable to load model</code></p>

          '
        raw: "I try using this model with llama.cpp, but this is the result:\r\n\r\
          \n`> ./main -m ./models/ggml-alpaca-7b-q4.bin -n 256 --repeat_penalty 1.0\
          \ --color -i -r \"User:\" -f prompts/chat-with-bob.txt  \r\nmain: build\
          \ = 708 (8596af4)  \r\nmain: seed  = 1687096224  \r\nllama.cpp: loading\
          \ model from ./models/ggml-alpaca-7b-q4.bin\r\nerror loading model: unexpectedly\
          \ reached end of file\r\nllama_init_from_file: failed to load model\r\n\
          llama_init_from_gpt_params: error: failed to load model './models/ggml-alpaca-7b-q4.bin'\r\
          \nmain: error: unable to load model`\r\n"
        updatedAt: '2023-06-18T13:53:24.351Z'
      numEdits: 0
      reactions: []
    id: 648f0c542d469a21b3d9ba3a
    type: comment
  author: djtech
  content: "I try using this model with llama.cpp, but this is the result:\r\n\r\n\
    `> ./main -m ./models/ggml-alpaca-7b-q4.bin -n 256 --repeat_penalty 1.0 --color\
    \ -i -r \"User:\" -f prompts/chat-with-bob.txt  \r\nmain: build = 708 (8596af4)\
    \  \r\nmain: seed  = 1687096224  \r\nllama.cpp: loading model from ./models/ggml-alpaca-7b-q4.bin\r\
    \nerror loading model: unexpectedly reached end of file\r\nllama_init_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ './models/ggml-alpaca-7b-q4.bin'\r\nmain: error: unable to load model`\r\n"
  created_at: 2023-06-18 12:53:24+00:00
  edited: false
  hidden: false
  id: 648f0c542d469a21b3d9ba3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95ed4d92a5ae5cc853e7bb419b8c674a.svg
      fullname: 'G. '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djtech
      type: user
    createdAt: '2023-06-18T13:54:12.000Z'
    data:
      edited: false
      editors:
      - djtech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4260786771774292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95ed4d92a5ae5cc853e7bb419b8c674a.svg
          fullname: 'G. '
          isHf: false
          isPro: false
          name: djtech
          type: user
        html: '<p>Better format:  </p>

          <p><code>./main -m ./models/ggml-alpaca-7b-q4.bin -n 256 --repeat_penalty
          1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt</code><br><code>main:
          build = 708 (8596af4)</code><br><code>main: seed  = 1687096224</code><br><code>llama.cpp:
          loading model from ./models/ggml-alpaca-7b-q4.bin</code><br><code>error
          loading model: unexpectedly reached end of file</code><br><code>llama_init_from_file:
          failed to load model</code><br><code>llama_init_from_gpt_params: error:
          failed to load model ''./models/ggml-alpaca-7b-q4.bin''</code><br><code>main:
          error: unable to load model</code>  </p>

          '
        raw: "Better format:  \n\n`./main -m ./models/ggml-alpaca-7b-q4.bin -n 256\
          \ --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt`\
          \  \n`main: build = 708 (8596af4)`  \n`main: seed  = 1687096224`  \n`llama.cpp:\
          \ loading model from ./models/ggml-alpaca-7b-q4.bin`  \n`error loading model:\
          \ unexpectedly reached end of file`  \n`llama_init_from_file: failed to\
          \ load model`  \n`llama_init_from_gpt_params: error: failed to load model\
          \ './models/ggml-alpaca-7b-q4.bin'`  \n`main: error: unable to load model`\
          \  \n"
        updatedAt: '2023-06-18T13:54:12.140Z'
      numEdits: 0
      reactions: []
    id: 648f0c84979504d221ed221d
    type: comment
  author: djtech
  content: "Better format:  \n\n`./main -m ./models/ggml-alpaca-7b-q4.bin -n 256 --repeat_penalty\
    \ 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt`  \n`main: build =\
    \ 708 (8596af4)`  \n`main: seed  = 1687096224`  \n`llama.cpp: loading model from\
    \ ./models/ggml-alpaca-7b-q4.bin`  \n`error loading model: unexpectedly reached\
    \ end of file`  \n`llama_init_from_file: failed to load model`  \n`llama_init_from_gpt_params:\
    \ error: failed to load model './models/ggml-alpaca-7b-q4.bin'`  \n`main: error:\
    \ unable to load model`  \n"
  created_at: 2023-06-18 12:54:12+00:00
  edited: false
  hidden: false
  id: 648f0c84979504d221ed221d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/flIGEBQWbIyMpkMMfe95p.jpeg?w=200&h=200&f=face
      fullname: Jake Richards
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaker86
      type: user
    createdAt: '2023-07-02T03:33:07.000Z'
    data:
      edited: false
      editors:
      - jaker86
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9160751700401306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/flIGEBQWbIyMpkMMfe95p.jpeg?w=200&h=200&f=face
          fullname: Jake Richards
          isHf: false
          isPro: false
          name: jaker86
          type: user
        html: '<p>I am getting the same issue</p>

          '
        raw: 'I am getting the same issue

          '
        updatedAt: '2023-07-02T03:33:07.691Z'
      numEdits: 0
      reactions: []
    id: 64a0eff3c9e1579f12ce7b49
    type: comment
  author: jaker86
  content: 'I am getting the same issue

    '
  created_at: 2023-07-02 02:33:07+00:00
  edited: false
  hidden: false
  id: 64a0eff3c9e1579f12ce7b49
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Sosaka/Alpaca-native-4bit-ggml
repo_type: model
status: open
target_branch: null
title: 'Not working with llama.cpp '
