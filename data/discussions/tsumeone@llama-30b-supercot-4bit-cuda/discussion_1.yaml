!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Onix22
conflicting_files: null
created_at: 2023-04-27 18:37:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-04-27T19:37:32.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>How about adding 3 bit version for quality testing?<br> to see how
          much worse it perform in comparison to 4 bit version, because 4 bit gets
          oom somewhere above 1000 tokens on 24gb gpu<br>this 4 bit version works
          pretty good.</p>

          '
        raw: "How about adding 3 bit version for quality testing?\r\n to see how much\
          \ worse it perform in comparison to 4 bit version, because 4 bit gets oom\
          \ somewhere above 1000 tokens on 24gb gpu\r\nthis 4 bit version works pretty\
          \ good."
        updatedAt: '2023-04-27T19:37:32.410Z'
      numEdits: 0
      reactions: []
    id: 644acefc91252984f6453174
    type: comment
  author: Onix22
  content: "How about adding 3 bit version for quality testing?\r\n to see how much\
    \ worse it perform in comparison to 4 bit version, because 4 bit gets oom somewhere\
    \ above 1000 tokens on 24gb gpu\r\nthis 4 bit version works pretty good."
  created_at: 2023-04-27 18:37:32+00:00
  edited: false
  hidden: false
  id: 644acefc91252984f6453174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
      fullname: Bow Wow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: tsumeone
      type: user
    createdAt: '2023-04-28T04:01:15.000Z'
    data:
      edited: false
      editors:
      - tsumeone
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
          fullname: Bow Wow
          isHf: false
          isPro: false
          name: tsumeone
          type: user
        html: '<p>On KoboldAI, I can run the 4bit non-groupsize model at full context
          in windows on my 3090.  Ooba takes up more vram for some reason; I''m guessing
          that''s what you''re using.</p>

          <p>I am uploading a 3bit-128g quant of this model.  It might take a couple
          hours since HF seems to be having some troubles right now and is refusing
          to let me create a model card.  The wikitext 2 ppl is 12% worse than 4bit
          non-groupsize which is a substantial loss in coherence.  But the file is
          17% smaller, which should roughly translate into similar VRAM savings.  You
          will find it here: <a href="https://huggingface.co/tsumeone/llama-30b-supercot-3bit-128g-cuda">https://huggingface.co/tsumeone/llama-30b-supercot-3bit-128g-cuda</a></p>

          <p>Just want to add that I also tried quantizing a 3bit-32g version to see
          if the ppl could be improved, but the file size ended up 2% larger than
          4bit non-groupsize while still having 5% worse ppl.  Basically no reason
          to even consider that one since it will use more VRAM and also be less coherent.</p>

          '
        raw: 'On KoboldAI, I can run the 4bit non-groupsize model at full context
          in windows on my 3090.  Ooba takes up more vram for some reason; I''m guessing
          that''s what you''re using.


          I am uploading a 3bit-128g quant of this model.  It might take a couple
          hours since HF seems to be having some troubles right now and is refusing
          to let me create a model card.  The wikitext 2 ppl is 12% worse than 4bit
          non-groupsize which is a substantial loss in coherence.  But the file is
          17% smaller, which should roughly translate into similar VRAM savings.  You
          will find it here: https://huggingface.co/tsumeone/llama-30b-supercot-3bit-128g-cuda


          Just want to add that I also tried quantizing a 3bit-32g version to see
          if the ppl could be improved, but the file size ended up 2% larger than
          4bit non-groupsize while still having 5% worse ppl.  Basically no reason
          to even consider that one since it will use more VRAM and also be less coherent.'
        updatedAt: '2023-04-28T04:01:15.124Z'
      numEdits: 0
      reactions: []
    id: 644b450bf9f1b0cd3d98603b
    type: comment
  author: tsumeone
  content: 'On KoboldAI, I can run the 4bit non-groupsize model at full context in
    windows on my 3090.  Ooba takes up more vram for some reason; I''m guessing that''s
    what you''re using.


    I am uploading a 3bit-128g quant of this model.  It might take a couple hours
    since HF seems to be having some troubles right now and is refusing to let me
    create a model card.  The wikitext 2 ppl is 12% worse than 4bit non-groupsize
    which is a substantial loss in coherence.  But the file is 17% smaller, which
    should roughly translate into similar VRAM savings.  You will find it here: https://huggingface.co/tsumeone/llama-30b-supercot-3bit-128g-cuda


    Just want to add that I also tried quantizing a 3bit-32g version to see if the
    ppl could be improved, but the file size ended up 2% larger than 4bit non-groupsize
    while still having 5% worse ppl.  Basically no reason to even consider that one
    since it will use more VRAM and also be less coherent.'
  created_at: 2023-04-28 03:01:15+00:00
  edited: false
  hidden: false
  id: 644b450bf9f1b0cd3d98603b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
      fullname: Bow Wow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: tsumeone
      type: user
    createdAt: '2023-04-28T04:01:18.000Z'
    data:
      status: closed
    id: 644b450eaf97dfd24c1a52d7
    type: status-change
  author: tsumeone
  created_at: 2023-04-28 03:01:18+00:00
  id: 644b450eaf97dfd24c1a52d7
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
      fullname: Bow Wow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: tsumeone
      type: user
    createdAt: '2023-04-28T04:36:00.000Z'
    data:
      status: open
    id: 644b4d30d4483bfaa0793ce4
    type: status-change
  author: tsumeone
  created_at: 2023-04-28 03:36:00+00:00
  id: 644b4d30d4483bfaa0793ce4
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tsumeone/llama-30b-supercot-4bit-cuda
repo_type: model
status: open
target_branch: null
title: 3bit version
