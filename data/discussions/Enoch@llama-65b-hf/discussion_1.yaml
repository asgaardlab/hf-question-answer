!!python/object:huggingface_hub.community.DiscussionWithDetails
author: junyi111
conflicting_files: null
created_at: 2023-08-07 05:23:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/514651f1d52342cdfa513236cd02ca2b.svg
      fullname: zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: junyi111
      type: user
    createdAt: '2023-08-07T06:23:52.000Z'
    data:
      edited: false
      editors:
      - junyi111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4076676070690155
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/514651f1d52342cdfa513236cd02ca2b.svg
          fullname: zhang
          isHf: false
          isPro: false
          name: junyi111
          type: user
        html: "<p>\u9884\u6D4B\u7684\u65F6\u5019\u62A5\u9519\uFF1ARuntimeError: shape\
          \ '[-1, 271]' is invalid for input of size 568</p>\n<p>(llama) huawei@work-3:~/workspace/LLaMA-Efficient-Tuning-main$\
          \ NCCL_SOCKET_IFNAME=eth1 NCCL_DEBUG=INFO  python src/train_bash.py    \
          \   --stage sft     --model_name_or_path  model/llama65b/     --do_predict\
          \    --dataset alpaca_zh     --finetuning_type lora   --checkpoint_dir path_to_sft_checkpoint_llama65B/\
          \   --output_dir path_to_predict_result       --per_device_train_batch_size\
          \ 1    --prompt_template default  --lora_target W_pack   --predict_with_generate\
          \  --max_samples 20<br>[2023-08-07 02:42:01,059] [INFO] [real_accelerator.py:133:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)<br>08/07/2023 02:42:03 -\
          \ WARNING - llmtuner.tuner.core.parser - Please specify <code>prompt_template</code>\
          \ if you are using other pre-trained models.<br>08/07/2023 02:42:03 - WARNING\
          \ - llmtuner.tuner.core.parser - <code>ddp_find_unused_parameters</code>\
          \ needs to be set as False in DDP training.<br>08/07/2023 02:42:03 - INFO\
          \ - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu:\
          \ 8<br>  distributed training: True, 16-bits training: False<br>08/07/2023\
          \ 02:42:03 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters\
          \ Seq2SeqTrainingArguments(<br>_n_gpu=8,<br>adafactor=False,<br>adam_beta1=0.9,<br>adam_beta2=0.999,<br>adam_epsilon=1e-08,<br>auto_find_batch_size=False,<br>bf16=False,<br>bf16_full_eval=False,<br>data_seed=None,<br>dataloader_drop_last=False,<br>dataloader_num_workers=0,<br>dataloader_pin_memory=True,<br>ddp_backend=None,<br>ddp_broadcast_buffers=None,<br>ddp_bucket_cap_mb=None,<br>ddp_find_unused_parameters=False,<br>ddp_timeout=1800,<br>debug=[],<br>deepspeed=None,<br>disable_tqdm=False,<br>do_eval=False,<br>do_predict=True,<br>do_train=False,<br>eval_accumulation_steps=None,<br>eval_delay=0,<br>eval_steps=None,<br>evaluation_strategy=no,<br>fp16=False,<br>fp16_backend=auto,<br>fp16_full_eval=False,<br>fp16_opt_level=O1,<br>fsdp=[],<br>fsdp_config={'fsdp_min_num_params':\
          \ 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},<br>fsdp_min_num_params=0,<br>fsdp_transformer_layer_cls_to_wrap=None,<br>full_determinism=False,<br>generation_config=None,<br>generation_max_length=None,<br>generation_num_beams=None,<br>gradient_accumulation_steps=1,<br>gradient_checkpointing=False,<br>greater_is_better=None,<br>group_by_length=False,<br>half_precision_backend=auto,<br>hub_model_id=None,<br>hub_private_repo=False,<br>hub_strategy=every_save,<br>hub_token=,<br>ignore_data_skip=False,<br>include_inputs_for_metrics=False,<br>jit_mode_eval=False,<br>label_names=None,<br>label_smoothing_factor=0.0,<br>learning_rate=5e-05,<br>length_column_name=length,<br>load_best_model_at_end=False,<br>local_rank=0,<br>log_level=passive,<br>log_level_replica=warning,<br>log_on_each_node=True,<br>logging_dir=path_to_predict_result/runs/Aug07_02-42-03_work-3,<br>logging_first_step=False,<br>logging_nan_inf_filter=True,<br>logging_steps=500,<br>logging_strategy=steps,<br>lr_scheduler_type=linear,<br>max_grad_norm=1.0,<br>max_steps=-1,<br>metric_for_best_model=None,<br>mp_parameters=,<br>no_cuda=False,<br>num_train_epochs=3.0,<br>optim=adamw_torch,<br>optim_args=None,<br>output_dir=path_to_predict_result,<br>overwrite_output_dir=False,<br>past_index=-1,<br>per_device_eval_batch_size=8,<br>per_device_train_batch_size=1,<br>predict_with_generate=True,<br>prediction_loss_only=False,<br>push_to_hub=False,<br>push_to_hub_model_id=None,<br>push_to_hub_organization=None,<br>push_to_hub_token=,<br>ray_scope=last,<br>remove_unused_columns=True,<br>report_to=['wandb'],<br>resume_from_checkpoint=None,<br>run_name=path_to_predict_result,<br>save_on_each_node=False,<br>save_safetensors=False,<br>save_steps=500,<br>save_strategy=steps,<br>save_total_limit=None,<br>seed=42,<br>sharded_ddp=[],<br>skip_memory_metrics=True,<br>sortish_sampler=False,<br>tf32=None,<br>torch_compile=False,<br>torch_compile_backend=None,<br>torch_compile_mode=None,<br>torchdynamo=None,<br>tpu_metrics_debug=False,<br>tpu_num_cores=None,<br>use_ipex=False,<br>use_legacy_prediction_loop=False,<br>use_mps_device=False,<br>warmup_ratio=0.0,<br>warmup_steps=0,<br>weight_decay=0.0,<br>xpu_backend=None,<br>)<br>08/07/2023\
          \ 02:42:03 - INFO - llmtuner.dsets.loader - Loading dataset alpaca_data_zh_51k.json...<br>/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:2069:\
          \ FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in\
          \ version 2.14.0 and will be removed in 3.0.0.<br>You can remove this warning\
          \ by passing 'token=None' instead.<br>  warnings.warn(<br>Using custom data\
          \ configuration default-f85e68495e5d6806<br>Loading Dataset Infos from /home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/packaged_modules/json<br>Overwrite\
          \ dataset info from restored data version if exists.<br>Loading Dataset\
          \ info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96<br>Found\
          \ cached dataset json (/home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)<br>Loading\
          \ Dataset info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96<br>[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 &gt;&gt; loading file tokenizer.model<br>[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 &gt;&gt; loading file added_tokens.json<br>[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 &gt;&gt; loading file special_tokens_map.json<br>[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 &gt;&gt; loading file tokenizer_config.json<br>[WARNING|logging.py:295]\
          \ 2023-08-07 02:42:03,797 &gt;&gt; You are using the legacy behaviour of\
          \ the &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;.\
          \ This means that tokens that come after special tokens will not be properly\
          \ handled. We recommend you to read the related pull request available at\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/24565\"\
          >https://github.com/huggingface/transformers/pull/24565</a><br>[INFO|configuration_utils.py:710]\
          \ 2023-08-07 02:42:03,813 &gt;&gt; loading configuration file model/llama65b/config.json<br>[INFO|configuration_utils.py:768]\
          \ 2023-08-07 02:42:03,815 &gt;&gt; Model config LlamaConfig {<br>  \"_name_or_path\"\
          : \"model/llama65b/\",<br>  \"architectures\": [<br>    \"LLaMAForCausalLM\"\
          <br>  ],<br>  \"bos_token_id\": 1,<br>  \"eos_token_id\": 2,<br>  \"hidden_act\"\
          : \"silu\",<br>  \"hidden_size\": 8192,<br>  \"initializer_range\": 0.02,<br>\
          \  \"intermediate_size\": 22016,<br>  \"max_position_embeddings\": 2048,<br>\
          \  \"max_sequence_length\": 2048,<br>  \"model_type\": \"llama\",<br>  \"\
          num_attention_heads\": 64,<br>  \"num_hidden_layers\": 80,<br>  \"num_key_value_heads\"\
          : 64,<br>  \"pad_token_id\": 0,<br>  \"pretraining_tp\": 1,<br>  \"rms_norm_eps\"\
          : 1e-05,<br>  \"rope_scaling\": null,<br>  \"tie_word_embeddings\": false,<br>\
          \  \"torch_dtype\": \"float16\",<br>  \"transformers_version\": \"4.31.0\"\
          ,<br>  \"use_cache\": true,<br>  \"vocab_size\": 32000<br>}</p>\n<p>[INFO|modeling_utils.py:2600]\
          \ 2023-08-07 02:42:03,841 &gt;&gt; loading weights file model/llama65b/pytorch_model.bin.index.json<br>[INFO|modeling_utils.py:1172]\
          \ 2023-08-07 02:42:03,842 &gt;&gt; Instantiating LlamaForCausalLM model\
          \ under default dtype torch.float16.<br>[INFO|configuration_utils.py:599]\
          \ 2023-08-07 02:42:03,843 &gt;&gt; Generate config GenerationConfig {<br>\
          \  \"_from_model_config\": true,<br>  \"bos_token_id\": 1,<br>  \"eos_token_id\"\
          : 2,<br>  \"pad_token_id\": 0,<br>  \"transformers_version\": \"4.31.0\"\
          <br>}</p>\n<p>Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 81/81 [01:53&lt;00:00,  1.40s/it]<br>[INFO|modeling_utils.py:3329]\
          \ 2023-08-07 02:44:11,321 &gt;&gt; All model checkpoint weights were used\
          \ when initializing LlamaForCausalLM.</p>\n<p>[INFO|modeling_utils.py:3337]\
          \ 2023-08-07 02:44:11,321 &gt;&gt; All the weights of LlamaForCausalLM were\
          \ initialized from the model checkpoint at model/llama65b/.<br>If your task\
          \ is similar to the task the model of the checkpoint was trained on, you\
          \ can already use LlamaForCausalLM for predictions without further training.<br>[INFO|configuration_utils.py:559]\
          \ 2023-08-07 02:44:11,329 &gt;&gt; loading configuration file model/llama65b/generation_config.json<br>[INFO|configuration_utils.py:599]\
          \ 2023-08-07 02:44:11,330 &gt;&gt; Generate config GenerationConfig {<br>\
          \  \"_from_model_config\": true,<br>  \"bos_token_id\": 1,<br>  \"eos_token_id\"\
          : 2,<br>  \"pad_token_id\": 0,<br>  \"transformers_version\": \"4.31.0\"\
          <br>}</p>\n<p>08/07/2023 02:44:11 - INFO - llmtuner.tuner.core.adapter -\
          \ Fine-tuning method: LoRA<br>08/07/2023 02:48:58 - INFO - llmtuner.tuner.core.adapter\
          \ - Merged 1 model checkpoint(s).<br>08/07/2023 02:48:58 - INFO - llmtuner.tuner.core.adapter\
          \ - Loaded fine-tuned model from checkpoint(s): path_to_sft_checkpoint_llama65B/<br>trainable\
          \ params: 0 || all params: 65285660672 || trainable%: 0.0000<br>Loading\
          \ cached processed dataset at /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-80195560bd98b11e.arrow<br>input_ids:<br>[0,\
          \ 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889,\
          \ 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304,\
          \ 278, 1404, 29915, 29879, 5155, 29889, 13, 29950, 7889, 29901, 29871, 31025,\
          \ 231, 187, 193, 31076, 30210, 31222, 31433, 30872, 31466, 30210, 30457,\
          \ 30502, 31141, 232, 193, 132, 30267, 13, 7900, 22137, 29901, 29871]<br>inputs:<br>A\
          \ chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions.<br>Human: \u5217\u4E3E\u597D\u7684\u7F51\u7AD9\u8BBE\u8BA1\u7684\
          \u4E09\u4E2A\u7279\u5F81\u3002<br>Assistant:<br>label_ids:<br>[0, 29871,\
          \ 31076, 30210, 31222, 31433, 30872, 31466, 30210, 30457, 30502, 31141,\
          \ 232, 193, 132, 30392, 30406, 31229, 31373, 31076, 30952, 30214, 30989,\
          \ 233, 156, 179, 233, 155, 150, 233, 138, 133, 30210, 31943, 31727, 31320,\
          \ 31901, 30503, 31568, 235, 170, 140, 232, 147, 187, 31674, 31074, 30267,\
          \ 30872, 31466, 31370, 31751, 236, 149, 139, 30783, 30895, 31062, 232, 146,\
          \ 154, 231, 191, 154, 30214, 31666, 31320, 30733, 30785, 30406, 31229, 30815,\
          \ 232, 167, 162, 31157, 235, 170, 133, 30533, 31943, 31727, 30503, 232,\
          \ 194, 174, 31859, 235, 177, 194, 31658, 30728, 31294, 30210, 30824, 31605,\
          \ 30267, 30630, 235, 170, 133, 30210, 30630, 30415, 30998, 31441, 31420,\
          \ 31100, 233, 135, 140, 233, 133, 169, 30503, 232, 147, 187, 31674, 30313,\
          \ 30210, 30988, 236, 173, 143, 30267]<br>labels:<br>\u597D\u7684\u7F51\u7AD9\
          \u8BBE\u8BA1\u7684\u4E09\u4E2A\u7279\u5F81\u662F\u7528\u6237\u53CB\u597D\
          \u6027\uFF0C\u6E05\u6670\u6613\u61C2\u7684\u5BFC\u822A\u7ED3\u6784\u548C\
          \u89C6\u89C9\u5438\u5F15\u529B\u3002\u8BBE\u8BA1\u5E94\u8BE5\u9488\u5BF9\
          \u76EE\u6807\u53D7\u4F17\uFF0C\u5E76\u7ED3\u5408\u4F7F\u7528\u6237\u80FD\
          \u591F\u76F4\u89C2\u5730\u5BFC\u822A\u548C\u5FEB\u901F\u8BBF\u95EE\u5185\
          \u5BB9\u7684\u5143\u7D20\u3002\u7F8E\u89C2\u7684\u7F8E\u5B66\u5C06\u521B\
          \u9020\u66F4\u6109\u60A6\u548C\u5438\u5F15\u4EBA\u7684\u4F53\u9A8C\u3002\
          <br>[INFO|trainer.py:386] 2023-08-07 02:48:58,475 &gt;&gt; You have loaded\
          \ a model on multiple GPUs. <code>is_model_parallel</code> attribute will\
          \ be force-set to <code>True</code> to avoid any unexpected behavior such\
          \ as device placement mismatching.<br>[INFO|trainer.py:3081] 2023-08-07\
          \ 02:48:58,478 &gt;&gt; ***** Running Prediction *****<br>[INFO|trainer.py:3083]\
          \ 2023-08-07 02:48:58,478 &gt;&gt;   Num examples = 20<br>[INFO|trainer.py:3086]\
          \ 2023-08-07 02:48:58,478 &gt;&gt;   Batch size = 8<br>[INFO|configuration_utils.py:599]\
          \ 2023-08-07 02:48:58,493 &gt;&gt; Generate config GenerationConfig {<br>\
          \  \"_from_model_config\": true,<br>  \"bos_token_id\": 1,<br>  \"eos_token_id\"\
          : 2,<br>  \"pad_token_id\": 0,<br>  \"transformers_version\": \"4.31.0\"\
          <br>}</p>\n<p>Traceback (most recent call last):<br>  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
          , line 45, in <br>    main()<br>  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
          , line 12, in main<br>    run_sft(model_args, data_args, training_args,\
          \ finetuning_args)<br>  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/workflow.py\"\
          , line 89, in run_sft<br>    predict_results = trainer.predict(dataset,\
          \ metric_key_prefix=\"predict\", **gen_kwargs)<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
          , line 216, in predict<br>    return super().predict(test_dataset, ignore_keys=ignore_keys,\
          \ metric_key_prefix=metric_key_prefix)<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 3010, in predict<br>    output = eval_loop(<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 3123, in evaluation_loop<br>    loss, logits, labels = self.prediction_step(model,\
          \ inputs, prediction_loss_only, ignore_keys=ignore_keys)<br>  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/trainer.py\"\
          , line 40, in prediction_step<br>    loss, generated_tokens, labels = super().prediction_step(<br>\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
          , line 282, in prediction_step<br>    generated_tokens = self.model.generate(**inputs,\
          \ **gen_kwargs)<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1588, in generate<br>    return self.sample(<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2642, in sample<br>    outputs = self(<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 806, in forward<br>    outputs = self.model(<br>  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 643, in forward<br>    position_ids = position_ids.view(-1, seq_length).long()<br>RuntimeError:\
          \ shape '[-1, 271]' is invalid for input of size 568</p>\n"
        raw: "\u9884\u6D4B\u7684\u65F6\u5019\u62A5\u9519\uFF1ARuntimeError: shape\
          \ '[-1, 271]' is invalid for input of size 568\r\n\r\n(llama) huawei@work-3:~/workspace/LLaMA-Efficient-Tuning-main$\
          \ NCCL_SOCKET_IFNAME=eth1 NCCL_DEBUG=INFO  python src/train_bash.py    \
          \   --stage sft     --model_name_or_path  model/llama65b/     --do_predict\
          \    --dataset alpaca_zh     --finetuning_type lora   --checkpoint_dir path_to_sft_checkpoint_llama65B/\
          \   --output_dir path_to_predict_result       --per_device_train_batch_size\
          \ 1    --prompt_template default  --lora_target W_pack   --predict_with_generate\
          \  --max_samples 20  \r\n[2023-08-07 02:42:01,059] [INFO] [real_accelerator.py:133:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\r\n08/07/2023 02:42:03 -\
          \ WARNING - llmtuner.tuner.core.parser - Please specify `prompt_template`\
          \ if you are using other pre-trained models.\r\n08/07/2023 02:42:03 - WARNING\
          \ - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be\
          \ set as False in DDP training.\r\n08/07/2023 02:42:03 - INFO - llmtuner.tuner.core.parser\
          \ - Process rank: 0, device: cuda:0, n_gpu: 8\r\n  distributed training:\
          \ True, 16-bits training: False\r\n08/07/2023 02:42:03 - INFO - llmtuner.tuner.core.parser\
          \ - Training/evaluation parameters Seq2SeqTrainingArguments(\r\n_n_gpu=8,\r\
          \nadafactor=False,\r\nadam_beta1=0.9,\r\nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\
          \nauto_find_batch_size=False,\r\nbf16=False,\r\nbf16_full_eval=False,\r\n\
          data_seed=None,\r\ndataloader_drop_last=False,\r\ndataloader_num_workers=0,\r\
          \ndataloader_pin_memory=True,\r\nddp_backend=None,\r\nddp_broadcast_buffers=None,\r\
          \nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\nddp_timeout=1800,\r\
          \ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\ndo_eval=False,\r\
          \ndo_predict=True,\r\ndo_train=False,\r\neval_accumulation_steps=None,\r\
          \neval_delay=0,\r\neval_steps=None,\r\nevaluation_strategy=no,\r\nfp16=False,\r\
          \nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\n\
          fsdp=[],\r\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt':\
          \ False},\r\nfsdp_min_num_params=0,\r\nfsdp_transformer_layer_cls_to_wrap=None,\r\
          \nfull_determinism=False,\r\ngeneration_config=None,\r\ngeneration_max_length=None,\r\
          \ngeneration_num_beams=None,\r\ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\
          \ngreater_is_better=None,\r\ngroup_by_length=False,\r\nhalf_precision_backend=auto,\r\
          \nhub_model_id=None,\r\nhub_private_repo=False,\r\nhub_strategy=every_save,\r\
          \nhub_token=<HUB_TOKEN>,\r\nignore_data_skip=False,\r\ninclude_inputs_for_metrics=False,\r\
          \njit_mode_eval=False,\r\nlabel_names=None,\r\nlabel_smoothing_factor=0.0,\r\
          \nlearning_rate=5e-05,\r\nlength_column_name=length,\r\nload_best_model_at_end=False,\r\
          \nlocal_rank=0,\r\nlog_level=passive,\r\nlog_level_replica=warning,\r\n\
          log_on_each_node=True,\r\nlogging_dir=path_to_predict_result/runs/Aug07_02-42-03_work-3,\r\
          \nlogging_first_step=False,\r\nlogging_nan_inf_filter=True,\r\nlogging_steps=500,\r\
          \nlogging_strategy=steps,\r\nlr_scheduler_type=linear,\r\nmax_grad_norm=1.0,\r\
          \nmax_steps=-1,\r\nmetric_for_best_model=None,\r\nmp_parameters=,\r\nno_cuda=False,\r\
          \nnum_train_epochs=3.0,\r\noptim=adamw_torch,\r\noptim_args=None,\r\noutput_dir=path_to_predict_result,\r\
          \noverwrite_output_dir=False,\r\npast_index=-1,\r\nper_device_eval_batch_size=8,\r\
          \nper_device_train_batch_size=1,\r\npredict_with_generate=True,\r\nprediction_loss_only=False,\r\
          \npush_to_hub=False,\r\npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\
          \npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\nray_scope=last,\r\nremove_unused_columns=True,\r\
          \nreport_to=['wandb'],\r\nresume_from_checkpoint=None,\r\nrun_name=path_to_predict_result,\r\
          \nsave_on_each_node=False,\r\nsave_safetensors=False,\r\nsave_steps=500,\r\
          \nsave_strategy=steps,\r\nsave_total_limit=None,\r\nseed=42,\r\nsharded_ddp=[],\r\
          \nskip_memory_metrics=True,\r\nsortish_sampler=False,\r\ntf32=None,\r\n\
          torch_compile=False,\r\ntorch_compile_backend=None,\r\ntorch_compile_mode=None,\r\
          \ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\
          \nuse_ipex=False,\r\nuse_legacy_prediction_loop=False,\r\nuse_mps_device=False,\r\
          \nwarmup_ratio=0.0,\r\nwarmup_steps=0,\r\nweight_decay=0.0,\r\nxpu_backend=None,\r\
          \n)\r\n08/07/2023 02:42:03 - INFO - llmtuner.dsets.loader - Loading dataset\
          \ alpaca_data_zh_51k.json...\r\n/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:2069:\
          \ FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in\
          \ version 2.14.0 and will be removed in 3.0.0.\r\nYou can remove this warning\
          \ by passing 'token=None' instead.\r\n  warnings.warn(\r\nUsing custom data\
          \ configuration default-f85e68495e5d6806\r\nLoading Dataset Infos from /home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/packaged_modules/json\r\
          \nOverwrite dataset info from restored data version if exists.\r\nLoading\
          \ Dataset info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\r\
          \nFound cached dataset json (/home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\r\
          \nLoading Dataset info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\r\
          \n[INFO|tokenization_utils_base.py:1837] 2023-08-07 02:42:03,796 >> loading\
          \ file tokenizer.model\r\n[INFO|tokenization_utils_base.py:1837] 2023-08-07\
          \ 02:42:03,796 >> loading file added_tokens.json\r\n[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 >> loading file special_tokens_map.json\r\n[INFO|tokenization_utils_base.py:1837]\
          \ 2023-08-07 02:42:03,796 >> loading file tokenizer_config.json\r\n[WARNING|logging.py:295]\
          \ 2023-08-07 02:42:03,797 >> You are using the legacy behaviour of the <class\
          \ 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means\
          \ that tokens that come after special tokens will not be properly handled.\
          \ We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\
          \n[INFO|configuration_utils.py:710] 2023-08-07 02:42:03,813 >> loading configuration\
          \ file model/llama65b/config.json\r\n[INFO|configuration_utils.py:768] 2023-08-07\
          \ 02:42:03,815 >> Model config LlamaConfig {\r\n  \"_name_or_path\": \"\
          model/llama65b/\",\r\n  \"architectures\": [\r\n    \"LLaMAForCausalLM\"\
          \r\n  ],\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\"\
          : \"silu\",\r\n  \"hidden_size\": 8192,\r\n  \"initializer_range\": 0.02,\r\
          \n  \"intermediate_size\": 22016,\r\n  \"max_position_embeddings\": 2048,\r\
          \n  \"max_sequence_length\": 2048,\r\n  \"model_type\": \"llama\",\r\n \
          \ \"num_attention_heads\": 64,\r\n  \"num_hidden_layers\": 80,\r\n  \"num_key_value_heads\"\
          : 64,\r\n  \"pad_token_id\": 0,\r\n  \"pretraining_tp\": 1,\r\n  \"rms_norm_eps\"\
          : 1e-05,\r\n  \"rope_scaling\": null,\r\n  \"tie_word_embeddings\": false,\r\
          \n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.31.0\"\
          ,\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 32000\r\n}\r\n\r\n[INFO|modeling_utils.py:2600]\
          \ 2023-08-07 02:42:03,841 >> loading weights file model/llama65b/pytorch_model.bin.index.json\r\
          \n[INFO|modeling_utils.py:1172] 2023-08-07 02:42:03,842 >> Instantiating\
          \ LlamaForCausalLM model under default dtype torch.float16.\r\n[INFO|configuration_utils.py:599]\
          \ 2023-08-07 02:42:03,843 >> Generate config GenerationConfig {\r\n  \"\
          _from_model_config\": true,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\"\
          : 2,\r\n  \"pad_token_id\": 0,\r\n  \"transformers_version\": \"4.31.0\"\
          \r\n}\r\n\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 81/81 [01:53<00:00,  1.40s/it]\r\n\
          [INFO|modeling_utils.py:3329] 2023-08-07 02:44:11,321 >> All model checkpoint\
          \ weights were used when initializing LlamaForCausalLM.\r\n\r\n[INFO|modeling_utils.py:3337]\
          \ 2023-08-07 02:44:11,321 >> All the weights of LlamaForCausalLM were initialized\
          \ from the model checkpoint at model/llama65b/.\r\nIf your task is similar\
          \ to the task the model of the checkpoint was trained on, you can already\
          \ use LlamaForCausalLM for predictions without further training.\r\n[INFO|configuration_utils.py:559]\
          \ 2023-08-07 02:44:11,329 >> loading configuration file model/llama65b/generation_config.json\r\
          \n[INFO|configuration_utils.py:599] 2023-08-07 02:44:11,330 >> Generate\
          \ config GenerationConfig {\r\n  \"_from_model_config\": true,\r\n  \"bos_token_id\"\
          : 1,\r\n  \"eos_token_id\": 2,\r\n  \"pad_token_id\": 0,\r\n  \"transformers_version\"\
          : \"4.31.0\"\r\n}\r\n\r\n08/07/2023 02:44:11 - INFO - llmtuner.tuner.core.adapter\
          \ - Fine-tuning method: LoRA\r\n08/07/2023 02:48:58 - INFO - llmtuner.tuner.core.adapter\
          \ - Merged 1 model checkpoint(s).\r\n08/07/2023 02:48:58 - INFO - llmtuner.tuner.core.adapter\
          \ - Loaded fine-tuned model from checkpoint(s): path_to_sft_checkpoint_llama65B/\r\
          \ntrainable params: 0 || all params: 65285660672 || trainable%: 0.0000\r\
          \nLoading cached processed dataset at /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-80195560bd98b11e.arrow\r\
          \ninput_ids:\r\n[0, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116,\
          \ 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322,\
          \ 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 13, 29950,\
          \ 7889, 29901, 29871, 31025, 231, 187, 193, 31076, 30210, 31222, 31433,\
          \ 30872, 31466, 30210, 30457, 30502, 31141, 232, 193, 132, 30267, 13, 7900,\
          \ 22137, 29901, 29871]\r\ninputs:\r\n<unk>A chat between a curious user\
          \ and an artificial intelligence assistant. The assistant gives helpful,\
          \ detailed, and polite answers to the user's questions.\r\nHuman: \u5217\
          \u4E3E\u597D\u7684\u7F51\u7AD9\u8BBE\u8BA1\u7684\u4E09\u4E2A\u7279\u5F81\
          \u3002\r\nAssistant: \r\nlabel_ids:\r\n[0, 29871, 31076, 30210, 31222, 31433,\
          \ 30872, 31466, 30210, 30457, 30502, 31141, 232, 193, 132, 30392, 30406,\
          \ 31229, 31373, 31076, 30952, 30214, 30989, 233, 156, 179, 233, 155, 150,\
          \ 233, 138, 133, 30210, 31943, 31727, 31320, 31901, 30503, 31568, 235, 170,\
          \ 140, 232, 147, 187, 31674, 31074, 30267, 30872, 31466, 31370, 31751, 236,\
          \ 149, 139, 30783, 30895, 31062, 232, 146, 154, 231, 191, 154, 30214, 31666,\
          \ 31320, 30733, 30785, 30406, 31229, 30815, 232, 167, 162, 31157, 235, 170,\
          \ 133, 30533, 31943, 31727, 30503, 232, 194, 174, 31859, 235, 177, 194,\
          \ 31658, 30728, 31294, 30210, 30824, 31605, 30267, 30630, 235, 170, 133,\
          \ 30210, 30630, 30415, 30998, 31441, 31420, 31100, 233, 135, 140, 233, 133,\
          \ 169, 30503, 232, 147, 187, 31674, 30313, 30210, 30988, 236, 173, 143,\
          \ 30267]\r\nlabels:\r\n<unk>\u597D\u7684\u7F51\u7AD9\u8BBE\u8BA1\u7684\u4E09\
          \u4E2A\u7279\u5F81\u662F\u7528\u6237\u53CB\u597D\u6027\uFF0C\u6E05\u6670\
          \u6613\u61C2\u7684\u5BFC\u822A\u7ED3\u6784\u548C\u89C6\u89C9\u5438\u5F15\
          \u529B\u3002\u8BBE\u8BA1\u5E94\u8BE5\u9488\u5BF9\u76EE\u6807\u53D7\u4F17\
          \uFF0C\u5E76\u7ED3\u5408\u4F7F\u7528\u6237\u80FD\u591F\u76F4\u89C2\u5730\
          \u5BFC\u822A\u548C\u5FEB\u901F\u8BBF\u95EE\u5185\u5BB9\u7684\u5143\u7D20\
          \u3002\u7F8E\u89C2\u7684\u7F8E\u5B66\u5C06\u521B\u9020\u66F4\u6109\u60A6\
          \u548C\u5438\u5F15\u4EBA\u7684\u4F53\u9A8C\u3002\r\n[INFO|trainer.py:386]\
          \ 2023-08-07 02:48:58,475 >> You have loaded a model on multiple GPUs. `is_model_parallel`\
          \ attribute will be force-set to `True` to avoid any unexpected behavior\
          \ such as device placement mismatching.\r\n[INFO|trainer.py:3081] 2023-08-07\
          \ 02:48:58,478 >> ***** Running Prediction *****\r\n[INFO|trainer.py:3083]\
          \ 2023-08-07 02:48:58,478 >>   Num examples = 20\r\n[INFO|trainer.py:3086]\
          \ 2023-08-07 02:48:58,478 >>   Batch size = 8\r\n[INFO|configuration_utils.py:599]\
          \ 2023-08-07 02:48:58,493 >> Generate config GenerationConfig {\r\n  \"\
          _from_model_config\": true,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\"\
          : 2,\r\n  \"pad_token_id\": 0,\r\n  \"transformers_version\": \"4.31.0\"\
          \r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
          , line 45, in <module>\r\n    main()\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
          , line 12, in main\r\n    run_sft(model_args, data_args, training_args,\
          \ finetuning_args)\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/workflow.py\"\
          , line 89, in run_sft\r\n    predict_results = trainer.predict(dataset,\
          \ metric_key_prefix=\"predict\", **gen_kwargs)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
          , line 216, in predict\r\n    return super().predict(test_dataset, ignore_keys=ignore_keys,\
          \ metric_key_prefix=metric_key_prefix)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 3010, in predict\r\n    output = eval_loop(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 3123, in evaluation_loop\r\n    loss, logits, labels = self.prediction_step(model,\
          \ inputs, prediction_loss_only, ignore_keys=ignore_keys)\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/trainer.py\"\
          , line 40, in prediction_step\r\n    loss, generated_tokens, labels = super().prediction_step(\r\
          \n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
          , line 282, in prediction_step\r\n    generated_tokens = self.model.generate(**inputs,\
          \ **gen_kwargs)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 806, in forward\r\n    outputs = self.model(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 643, in forward\r\n    position_ids = position_ids.view(-1, seq_length).long()\r\
          \nRuntimeError: shape '[-1, 271]' is invalid for input of size 568"
        updatedAt: '2023-08-07T06:23:52.160Z'
      numEdits: 0
      reactions: []
    id: 64d08df86f107411dab6621c
    type: comment
  author: junyi111
  content: "\u9884\u6D4B\u7684\u65F6\u5019\u62A5\u9519\uFF1ARuntimeError: shape '[-1,\
    \ 271]' is invalid for input of size 568\r\n\r\n(llama) huawei@work-3:~/workspace/LLaMA-Efficient-Tuning-main$\
    \ NCCL_SOCKET_IFNAME=eth1 NCCL_DEBUG=INFO  python src/train_bash.py       --stage\
    \ sft     --model_name_or_path  model/llama65b/     --do_predict    --dataset\
    \ alpaca_zh     --finetuning_type lora   --checkpoint_dir path_to_sft_checkpoint_llama65B/\
    \   --output_dir path_to_predict_result       --per_device_train_batch_size 1\
    \    --prompt_template default  --lora_target W_pack   --predict_with_generate\
    \  --max_samples 20  \r\n[2023-08-07 02:42:01,059] [INFO] [real_accelerator.py:133:get_accelerator]\
    \ Setting ds_accelerator to cuda (auto detect)\r\n08/07/2023 02:42:03 - WARNING\
    \ - llmtuner.tuner.core.parser - Please specify `prompt_template` if you are using\
    \ other pre-trained models.\r\n08/07/2023 02:42:03 - WARNING - llmtuner.tuner.core.parser\
    \ - `ddp_find_unused_parameters` needs to be set as False in DDP training.\r\n\
    08/07/2023 02:42:03 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device:\
    \ cuda:0, n_gpu: 8\r\n  distributed training: True, 16-bits training: False\r\n\
    08/07/2023 02:42:03 - INFO - llmtuner.tuner.core.parser - Training/evaluation\
    \ parameters Seq2SeqTrainingArguments(\r\n_n_gpu=8,\r\nadafactor=False,\r\nadam_beta1=0.9,\r\
    \nadam_beta2=0.999,\r\nadam_epsilon=1e-08,\r\nauto_find_batch_size=False,\r\n\
    bf16=False,\r\nbf16_full_eval=False,\r\ndata_seed=None,\r\ndataloader_drop_last=False,\r\
    \ndataloader_num_workers=0,\r\ndataloader_pin_memory=True,\r\nddp_backend=None,\r\
    \nddp_broadcast_buffers=None,\r\nddp_bucket_cap_mb=None,\r\nddp_find_unused_parameters=False,\r\
    \nddp_timeout=1800,\r\ndebug=[],\r\ndeepspeed=None,\r\ndisable_tqdm=False,\r\n\
    do_eval=False,\r\ndo_predict=True,\r\ndo_train=False,\r\neval_accumulation_steps=None,\r\
    \neval_delay=0,\r\neval_steps=None,\r\nevaluation_strategy=no,\r\nfp16=False,\r\
    \nfp16_backend=auto,\r\nfp16_full_eval=False,\r\nfp16_opt_level=O1,\r\nfsdp=[],\r\
    \nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\r\
    \nfsdp_min_num_params=0,\r\nfsdp_transformer_layer_cls_to_wrap=None,\r\nfull_determinism=False,\r\
    \ngeneration_config=None,\r\ngeneration_max_length=None,\r\ngeneration_num_beams=None,\r\
    \ngradient_accumulation_steps=1,\r\ngradient_checkpointing=False,\r\ngreater_is_better=None,\r\
    \ngroup_by_length=False,\r\nhalf_precision_backend=auto,\r\nhub_model_id=None,\r\
    \nhub_private_repo=False,\r\nhub_strategy=every_save,\r\nhub_token=<HUB_TOKEN>,\r\
    \nignore_data_skip=False,\r\ninclude_inputs_for_metrics=False,\r\njit_mode_eval=False,\r\
    \nlabel_names=None,\r\nlabel_smoothing_factor=0.0,\r\nlearning_rate=5e-05,\r\n\
    length_column_name=length,\r\nload_best_model_at_end=False,\r\nlocal_rank=0,\r\
    \nlog_level=passive,\r\nlog_level_replica=warning,\r\nlog_on_each_node=True,\r\
    \nlogging_dir=path_to_predict_result/runs/Aug07_02-42-03_work-3,\r\nlogging_first_step=False,\r\
    \nlogging_nan_inf_filter=True,\r\nlogging_steps=500,\r\nlogging_strategy=steps,\r\
    \nlr_scheduler_type=linear,\r\nmax_grad_norm=1.0,\r\nmax_steps=-1,\r\nmetric_for_best_model=None,\r\
    \nmp_parameters=,\r\nno_cuda=False,\r\nnum_train_epochs=3.0,\r\noptim=adamw_torch,\r\
    \noptim_args=None,\r\noutput_dir=path_to_predict_result,\r\noverwrite_output_dir=False,\r\
    \npast_index=-1,\r\nper_device_eval_batch_size=8,\r\nper_device_train_batch_size=1,\r\
    \npredict_with_generate=True,\r\nprediction_loss_only=False,\r\npush_to_hub=False,\r\
    \npush_to_hub_model_id=None,\r\npush_to_hub_organization=None,\r\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\r\
    \nray_scope=last,\r\nremove_unused_columns=True,\r\nreport_to=['wandb'],\r\nresume_from_checkpoint=None,\r\
    \nrun_name=path_to_predict_result,\r\nsave_on_each_node=False,\r\nsave_safetensors=False,\r\
    \nsave_steps=500,\r\nsave_strategy=steps,\r\nsave_total_limit=None,\r\nseed=42,\r\
    \nsharded_ddp=[],\r\nskip_memory_metrics=True,\r\nsortish_sampler=False,\r\ntf32=None,\r\
    \ntorch_compile=False,\r\ntorch_compile_backend=None,\r\ntorch_compile_mode=None,\r\
    \ntorchdynamo=None,\r\ntpu_metrics_debug=False,\r\ntpu_num_cores=None,\r\nuse_ipex=False,\r\
    \nuse_legacy_prediction_loop=False,\r\nuse_mps_device=False,\r\nwarmup_ratio=0.0,\r\
    \nwarmup_steps=0,\r\nweight_decay=0.0,\r\nxpu_backend=None,\r\n)\r\n08/07/2023\
    \ 02:42:03 - INFO - llmtuner.dsets.loader - Loading dataset alpaca_data_zh_51k.json...\r\
    \n/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:2069:\
    \ FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version\
    \ 2.14.0 and will be removed in 3.0.0.\r\nYou can remove this warning by passing\
    \ 'token=None' instead.\r\n  warnings.warn(\r\nUsing custom data configuration\
    \ default-f85e68495e5d6806\r\nLoading Dataset Infos from /home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/packaged_modules/json\r\
    \nOverwrite dataset info from restored data version if exists.\r\nLoading Dataset\
    \ info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\r\
    \nFound cached dataset json (/home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\r\
    \nLoading Dataset info from /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\r\
    \n[INFO|tokenization_utils_base.py:1837] 2023-08-07 02:42:03,796 >> loading file\
    \ tokenizer.model\r\n[INFO|tokenization_utils_base.py:1837] 2023-08-07 02:42:03,796\
    \ >> loading file added_tokens.json\r\n[INFO|tokenization_utils_base.py:1837]\
    \ 2023-08-07 02:42:03,796 >> loading file special_tokens_map.json\r\n[INFO|tokenization_utils_base.py:1837]\
    \ 2023-08-07 02:42:03,796 >> loading file tokenizer_config.json\r\n[WARNING|logging.py:295]\
    \ 2023-08-07 02:42:03,797 >> You are using the legacy behaviour of the <class\
    \ 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that\
    \ tokens that come after special tokens will not be properly handled. We recommend\
    \ you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\
    \n[INFO|configuration_utils.py:710] 2023-08-07 02:42:03,813 >> loading configuration\
    \ file model/llama65b/config.json\r\n[INFO|configuration_utils.py:768] 2023-08-07\
    \ 02:42:03,815 >> Model config LlamaConfig {\r\n  \"_name_or_path\": \"model/llama65b/\"\
    ,\r\n  \"architectures\": [\r\n    \"LLaMAForCausalLM\"\r\n  ],\r\n  \"bos_token_id\"\
    : 1,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\"\
    : 8192,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 22016,\r\
    \n  \"max_position_embeddings\": 2048,\r\n  \"max_sequence_length\": 2048,\r\n\
    \  \"model_type\": \"llama\",\r\n  \"num_attention_heads\": 64,\r\n  \"num_hidden_layers\"\
    : 80,\r\n  \"num_key_value_heads\": 64,\r\n  \"pad_token_id\": 0,\r\n  \"pretraining_tp\"\
    : 1,\r\n  \"rms_norm_eps\": 1e-05,\r\n  \"rope_scaling\": null,\r\n  \"tie_word_embeddings\"\
    : false,\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"\
    4.31.0\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 32000\r\n}\r\n\r\n[INFO|modeling_utils.py:2600]\
    \ 2023-08-07 02:42:03,841 >> loading weights file model/llama65b/pytorch_model.bin.index.json\r\
    \n[INFO|modeling_utils.py:1172] 2023-08-07 02:42:03,842 >> Instantiating LlamaForCausalLM\
    \ model under default dtype torch.float16.\r\n[INFO|configuration_utils.py:599]\
    \ 2023-08-07 02:42:03,843 >> Generate config GenerationConfig {\r\n  \"_from_model_config\"\
    : true,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"pad_token_id\"\
    : 0,\r\n  \"transformers_version\": \"4.31.0\"\r\n}\r\n\r\nLoading checkpoint\
    \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 81/81 [01:53<00:00,  1.40s/it]\r\n[INFO|modeling_utils.py:3329]\
    \ 2023-08-07 02:44:11,321 >> All model checkpoint weights were used when initializing\
    \ LlamaForCausalLM.\r\n\r\n[INFO|modeling_utils.py:3337] 2023-08-07 02:44:11,321\
    \ >> All the weights of LlamaForCausalLM were initialized from the model checkpoint\
    \ at model/llama65b/.\r\nIf your task is similar to the task the model of the\
    \ checkpoint was trained on, you can already use LlamaForCausalLM for predictions\
    \ without further training.\r\n[INFO|configuration_utils.py:559] 2023-08-07 02:44:11,329\
    \ >> loading configuration file model/llama65b/generation_config.json\r\n[INFO|configuration_utils.py:599]\
    \ 2023-08-07 02:44:11,330 >> Generate config GenerationConfig {\r\n  \"_from_model_config\"\
    : true,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"pad_token_id\"\
    : 0,\r\n  \"transformers_version\": \"4.31.0\"\r\n}\r\n\r\n08/07/2023 02:44:11\
    \ - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA\r\n08/07/2023\
    \ 02:48:58 - INFO - llmtuner.tuner.core.adapter - Merged 1 model checkpoint(s).\r\
    \n08/07/2023 02:48:58 - INFO - llmtuner.tuner.core.adapter - Loaded fine-tuned\
    \ model from checkpoint(s): path_to_sft_checkpoint_llama65B/\r\ntrainable params:\
    \ 0 || all params: 65285660672 || trainable%: 0.0000\r\nLoading cached processed\
    \ dataset at /home/huawei/.cache/huggingface/datasets/json/default-f85e68495e5d6806/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-80195560bd98b11e.arrow\r\
    \ninput_ids:\r\n[0, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082,\
    \ 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089,\
    \ 304, 278, 1404, 29915, 29879, 5155, 29889, 13, 29950, 7889, 29901, 29871, 31025,\
    \ 231, 187, 193, 31076, 30210, 31222, 31433, 30872, 31466, 30210, 30457, 30502,\
    \ 31141, 232, 193, 132, 30267, 13, 7900, 22137, 29901, 29871]\r\ninputs:\r\n<unk>A\
    \ chat between a curious user and an artificial intelligence assistant. The assistant\
    \ gives helpful, detailed, and polite answers to the user's questions.\r\nHuman:\
    \ \u5217\u4E3E\u597D\u7684\u7F51\u7AD9\u8BBE\u8BA1\u7684\u4E09\u4E2A\u7279\u5F81\
    \u3002\r\nAssistant: \r\nlabel_ids:\r\n[0, 29871, 31076, 30210, 31222, 31433,\
    \ 30872, 31466, 30210, 30457, 30502, 31141, 232, 193, 132, 30392, 30406, 31229,\
    \ 31373, 31076, 30952, 30214, 30989, 233, 156, 179, 233, 155, 150, 233, 138, 133,\
    \ 30210, 31943, 31727, 31320, 31901, 30503, 31568, 235, 170, 140, 232, 147, 187,\
    \ 31674, 31074, 30267, 30872, 31466, 31370, 31751, 236, 149, 139, 30783, 30895,\
    \ 31062, 232, 146, 154, 231, 191, 154, 30214, 31666, 31320, 30733, 30785, 30406,\
    \ 31229, 30815, 232, 167, 162, 31157, 235, 170, 133, 30533, 31943, 31727, 30503,\
    \ 232, 194, 174, 31859, 235, 177, 194, 31658, 30728, 31294, 30210, 30824, 31605,\
    \ 30267, 30630, 235, 170, 133, 30210, 30630, 30415, 30998, 31441, 31420, 31100,\
    \ 233, 135, 140, 233, 133, 169, 30503, 232, 147, 187, 31674, 30313, 30210, 30988,\
    \ 236, 173, 143, 30267]\r\nlabels:\r\n<unk>\u597D\u7684\u7F51\u7AD9\u8BBE\u8BA1\
    \u7684\u4E09\u4E2A\u7279\u5F81\u662F\u7528\u6237\u53CB\u597D\u6027\uFF0C\u6E05\
    \u6670\u6613\u61C2\u7684\u5BFC\u822A\u7ED3\u6784\u548C\u89C6\u89C9\u5438\u5F15\
    \u529B\u3002\u8BBE\u8BA1\u5E94\u8BE5\u9488\u5BF9\u76EE\u6807\u53D7\u4F17\uFF0C\
    \u5E76\u7ED3\u5408\u4F7F\u7528\u6237\u80FD\u591F\u76F4\u89C2\u5730\u5BFC\u822A\
    \u548C\u5FEB\u901F\u8BBF\u95EE\u5185\u5BB9\u7684\u5143\u7D20\u3002\u7F8E\u89C2\
    \u7684\u7F8E\u5B66\u5C06\u521B\u9020\u66F4\u6109\u60A6\u548C\u5438\u5F15\u4EBA\
    \u7684\u4F53\u9A8C\u3002\r\n[INFO|trainer.py:386] 2023-08-07 02:48:58,475 >> You\
    \ have loaded a model on multiple GPUs. `is_model_parallel` attribute will be\
    \ force-set to `True` to avoid any unexpected behavior such as device placement\
    \ mismatching.\r\n[INFO|trainer.py:3081] 2023-08-07 02:48:58,478 >> ***** Running\
    \ Prediction *****\r\n[INFO|trainer.py:3083] 2023-08-07 02:48:58,478 >>   Num\
    \ examples = 20\r\n[INFO|trainer.py:3086] 2023-08-07 02:48:58,478 >>   Batch size\
    \ = 8\r\n[INFO|configuration_utils.py:599] 2023-08-07 02:48:58,493 >> Generate\
    \ config GenerationConfig {\r\n  \"_from_model_config\": true,\r\n  \"bos_token_id\"\
    : 1,\r\n  \"eos_token_id\": 2,\r\n  \"pad_token_id\": 0,\r\n  \"transformers_version\"\
    : \"4.31.0\"\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
    , line 45, in <module>\r\n    main()\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/train_bash.py\"\
    , line 12, in main\r\n    run_sft(model_args, data_args, training_args, finetuning_args)\r\
    \n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/workflow.py\"\
    , line 89, in run_sft\r\n    predict_results = trainer.predict(dataset, metric_key_prefix=\"\
    predict\", **gen_kwargs)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
    , line 216, in predict\r\n    return super().predict(test_dataset, ignore_keys=ignore_keys,\
    \ metric_key_prefix=metric_key_prefix)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 3010, in predict\r\n    output = eval_loop(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 3123, in evaluation_loop\r\n    loss, logits, labels = self.prediction_step(model,\
    \ inputs, prediction_loss_only, ignore_keys=ignore_keys)\r\n  File \"/home/huawei/workspace/LLaMA-Efficient-Tuning-main/src/llmtuner/tuner/sft/trainer.py\"\
    , line 40, in prediction_step\r\n    loss, generated_tokens, labels = super().prediction_step(\r\
    \n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\"\
    , line 282, in prediction_step\r\n    generated_tokens = self.model.generate(**inputs,\
    \ **gen_kwargs)\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1588, in generate\r\n    return self.sample(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2642, in sample\r\n    outputs = self(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 806, in forward\r\n    outputs = self.model(\r\n  File \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/huawei/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 643, in forward\r\n    position_ids = position_ids.view(-1, seq_length).long()\r\
    \nRuntimeError: shape '[-1, 271]' is invalid for input of size 568"
  created_at: 2023-08-07 05:23:52+00:00
  edited: false
  hidden: false
  id: 64d08df86f107411dab6621c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Enoch/llama-65b-hf
repo_type: model
status: open
target_branch: null
title: "\u9884\u6D4B\u62A5\u9519"
