!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfie-ns
conflicting_files: null
created_at: 2024-01-15 10:00:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9aef30822655da88dcef17c15aceed80.svg
      fullname: Avi Halevy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfie-ns
      type: user
    createdAt: '2024-01-15T10:00:18.000Z'
    data:
      edited: false
      editors:
      - alfie-ns
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5127972364425659
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9aef30822655da88dcef17c15aceed80.svg
          fullname: Avi Halevy
          isHf: false
          isPro: false
          name: alfie-ns
          type: user
        html: "<p>I wanted to convert this model into gguf format but this did not\
          \ work:<br><code> python llama.cpp/convert.py dummies/python_postprocessing/Qwen-VL\
          \ --outfile Qwen-VL.gguf --outtype q8_0</code></p>\n<p>Gives me this error:</p>\n\
          <pre><code>/home/avi/llama.cpp/gguf-py\nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00002-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00003-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00004-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00005-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00006-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00007-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00008-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00009-of-00010.bin\n\
          Loading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00010-of-00010.bin\n\
          Traceback (most recent call last):\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 1658, in &lt;module&gt;\n    main(sys.argv[1:])  # Exclude the first\
          \ element (script name) from sys.argv\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 1577, in main\n    model_plus = load_some_model(args.model)\n  File\
          \ \"/home/avi/llama.cpp/convert.py\", line 1354, in load_some_model\n  \
          \  model_plus = merge_multifile_models(models_plus)\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 782, in merge_multifile_models\n    model = merge_sharded([mp.model\
          \ for mp in models_plus])\n  File \"/home/avi/llama.cpp/convert.py\", line\
          \ 761, in merge_sharded\n    return {name: convert(name) for name in names}\n\
          \  File \"/home/avi/llama.cpp/convert.py\", line 761, in &lt;dictcomp&gt;\n\
          \    return {name: convert(name) for name in names}\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 736, in convert\n    lazy_tensors: list[LazyTensor] = [model[name]\
          \ for model in models]\n  File \"/home/avi/llama.cpp/convert.py\", line\
          \ 736, in &lt;listcomp&gt;\n    lazy_tensors: list[LazyTensor] = [model[name]\
          \ for model in models]\nKeyError: 'transformer.wte.weight'\n</code></pre>\n"
        raw: "I wanted to convert this model into gguf format but this did not work:\r\
          \n``` python llama.cpp/convert.py dummies/python_postprocessing/Qwen-VL\
          \ --outfile Qwen-VL.gguf --outtype q8_0```\r\n\r\nGives me this error:\r\
          \n```\r\n/home/avi/llama.cpp/gguf-py\r\nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00002-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00003-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00004-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00005-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00006-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00007-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00008-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00009-of-00010.bin\r\
          \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00010-of-00010.bin\r\
          \nTraceback (most recent call last):\r\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 1658, in <module>\r\n    main(sys.argv[1:])  # Exclude the first\
          \ element (script name) from sys.argv\r\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 1577, in main\r\n    model_plus = load_some_model(args.model)\r\n\
          \  File \"/home/avi/llama.cpp/convert.py\", line 1354, in load_some_model\r\
          \n    model_plus = merge_multifile_models(models_plus)\r\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 782, in merge_multifile_models\r\n    model = merge_sharded([mp.model\
          \ for mp in models_plus])\r\n  File \"/home/avi/llama.cpp/convert.py\",\
          \ line 761, in merge_sharded\r\n    return {name: convert(name) for name\
          \ in names}\r\n  File \"/home/avi/llama.cpp/convert.py\", line 761, in <dictcomp>\r\
          \n    return {name: convert(name) for name in names}\r\n  File \"/home/avi/llama.cpp/convert.py\"\
          , line 736, in convert\r\n    lazy_tensors: list[LazyTensor] = [model[name]\
          \ for model in models]\r\n  File \"/home/avi/llama.cpp/convert.py\", line\
          \ 736, in <listcomp>\r\n    lazy_tensors: list[LazyTensor] = [model[name]\
          \ for model in models]\r\nKeyError: 'transformer.wte.weight'\r\n```\r\n"
        updatedAt: '2024-01-15T10:00:19.000Z'
      numEdits: 0
      reactions: []
    id: 65a502328df9302d159d845e
    type: comment
  author: alfie-ns
  content: "I wanted to convert this model into gguf format but this did not work:\r\
    \n``` python llama.cpp/convert.py dummies/python_postprocessing/Qwen-VL --outfile\
    \ Qwen-VL.gguf --outtype q8_0```\r\n\r\nGives me this error:\r\n```\r\n/home/avi/llama.cpp/gguf-py\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00001-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00002-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00003-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00004-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00005-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00006-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00007-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00008-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00009-of-00010.bin\r\
    \nLoading model file dummies/python_postprocessing/Qwen-VL/pytorch_model-00010-of-00010.bin\r\
    \nTraceback (most recent call last):\r\n  File \"/home/avi/llama.cpp/convert.py\"\
    , line 1658, in <module>\r\n    main(sys.argv[1:])  # Exclude the first element\
    \ (script name) from sys.argv\r\n  File \"/home/avi/llama.cpp/convert.py\", line\
    \ 1577, in main\r\n    model_plus = load_some_model(args.model)\r\n  File \"/home/avi/llama.cpp/convert.py\"\
    , line 1354, in load_some_model\r\n    model_plus = merge_multifile_models(models_plus)\r\
    \n  File \"/home/avi/llama.cpp/convert.py\", line 782, in merge_multifile_models\r\
    \n    model = merge_sharded([mp.model for mp in models_plus])\r\n  File \"/home/avi/llama.cpp/convert.py\"\
    , line 761, in merge_sharded\r\n    return {name: convert(name) for name in names}\r\
    \n  File \"/home/avi/llama.cpp/convert.py\", line 761, in <dictcomp>\r\n    return\
    \ {name: convert(name) for name in names}\r\n  File \"/home/avi/llama.cpp/convert.py\"\
    , line 736, in convert\r\n    lazy_tensors: list[LazyTensor] = [model[name] for\
    \ model in models]\r\n  File \"/home/avi/llama.cpp/convert.py\", line 736, in\
    \ <listcomp>\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in\
    \ models]\r\nKeyError: 'transformer.wte.weight'\r\n```\r\n"
  created_at: 2024-01-15 10:00:18+00:00
  edited: false
  hidden: false
  id: 65a502328df9302d159d845e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Qwen/Qwen-VL
repo_type: model
status: open
target_branch: null
title: Cannot convert to gguf
