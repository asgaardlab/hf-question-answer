!!python/object:huggingface_hub.community.DiscussionWithDetails
author: saivineetha
conflicting_files: null
created_at: 2024-01-18 07:54:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
      fullname: Baddepudi Venkata Naga Sri Sai Vineetha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saivineetha
      type: user
    createdAt: '2024-01-18T07:54:58.000Z'
    data:
      edited: true
      editors:
      - saivineetha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24220700562000275
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9df73264fe5d79bebd9389bc900f01af.svg
          fullname: Baddepudi Venkata Naga Sri Sai Vineetha
          isHf: false
          isPro: false
          name: saivineetha
          type: user
        html: "<p>I was pre-training the facebook opt 350m model on text file. While\
          \ training after 15 steps the training loss is becoming zero.  Can you help\
          \ me with this.</p>\n<p>Code I used</p>\n<pre><code>if script_args.load_in_kbits\
          \ in [4, 8]:\n    load_in_4bit = script_args.load_in_kbits == 4\n    load_in_8bit\
          \ = script_args.load_in_kbits == 8\n    if script_args.modules_to_save is\
          \ not None:\n        load_in_8bit_skip_modules = script_args.modules_to_save.split(\"\
          ,\")\n    else:\n        load_in_8bit_skip_modules = None\n    quantization_config\
          \ = BitsAndBytesConfig(\n        load_in_4bit=script_args.load_in_kbits\
          \ == 4,\n        load_in_8bit=script_args.load_in_kbits == 8,\n        llm_int8_threshold=6.0,\n\
          \        load_in_8bit_skip_modules=load_in_8bit_skip_modules,\n        bnb_4bit_compute_dtype=compute_dtype,\n\
          \        bnb_4bit_use_double_quant=script_args.double_quant,\n        bnb_4bit_quant_type=script_args.quant_type,\
          \  # {'fp4', 'nf4'}\n    )\nelse:\n    load_in_4bit = False\n    load_in_8bit\
          \ = False\n    quantization_config = None\nif quantization_config is not\
          \ None:\n    logger.info(f\"quantization_config:{quantization_config.to_dict()}\"\
          )\n\nif script_args.model_name_or_path:\n    torch_dtype = (\n        script_args.torch_dtype\n\
          \        if script_args.torch_dtype in [\"auto\", None]\n        else getattr(torch,\
          \ script_args.torch_dtype)\n    )\n    device_map = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        script_args.model_name_or_path,\n        from_tf=bool(\".ckpt\"\
          \ in script_args.model_name_or_path),\n        config=config,\n        #\
          \ cache_dir=script_args.cache_dir,\n        # revision=model_args.model_revision,\n\
          \        use_auth_token=True if script_args.use_auth_token else None,\n\
          \        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=True,\n   \
          \     device_map=device_map,\n        load_in_4bit=load_in_4bit,\n     \
          \   load_in_8bit=load_in_8bit,\n        quantization_config=quantization_config,\n\
          \    )\n    \nelse:\n    model = AutoModelForCausalLM.from_config(config)\n\
          \    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n\
          \    logger.info(\n        f\"Training new model from scratch - Total size={n_params/2**20:.2f}M\
          \ params\"\n    )\n\nif script_args.load_in_kbits in [4, 8]:\n    model\
          \ = prepare_model_for_kbit_training(\n        model, use_gradient_checkpointing=script_args.gradient_checkpointing\n\
          \    )\nmodel.config.use_cache = False\nmodel_vocab_size = model.get_output_embeddings().weight.size(0)\n\
          tokenizer_vocab_size = len(tokenizer)\nlogger.info(f\"Model vocab size:\
          \ {model_vocab_size}\")\nlogger.info(f\"Tokenizer vocab size: {tokenizer_vocab_size}\"\
          )\nif model_vocab_size != tokenizer_vocab_size:\n    logger.info(f\"Resize\
          \ model vocab size to {tokenizer_vocab_size}\")\n    model.resize_token_embeddings(len(tokenizer))\n\
          \nif script_args.peft_path is not None:\n    logger.info(\"Peft from pre-trained\
          \ model\")\n    model = PeftModel.from_pretrained(\n        model, script_args.peft_path,\
          \ device_map=device_map\n    )\nelse:\n    logger.info(\"Init new peft model\"\
          )\n    target_modules = script_args.trainable.split(\",\")\n    modules_to_save\
          \ = script_args.modules_to_save\n    if modules_to_save is not None:\n \
          \       modules_to_save = modules_to_save.split(\",\")\n    lora_rank =\
          \ script_args.lora_rank\n    lora_dropout = script_args.lora_dropout\n \
          \   lora_alpha = script_args.lora_alpha\n    logger.info(f\"target_modules:\
          \ {target_modules}\")\n    logger.info(f\"lora_rank: {lora_rank}\")\n  \
          \  peft_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n  \
          \      target_modules=target_modules,\n        inference_mode=False,\n \
          \       r=lora_rank,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n\
          \        modules_to_save=modules_to_save,\n    )\n    model = get_peft_model(model,\
          \ peft_config)\nfor name, module in model.named_modules():\n    if isinstance(module,\
          \ LoraLayer):\n      module = module.to(torch.float16)\n        # if script_args.bf16:\n\
          \        #     module = module.to(torch.bfloat16)\n        # if script_args.fp16:\n\
          \        #     module = module.to(torch.float16)\n    if \"norm\" in name:\n\
          \        module = module.to(torch.float16)\n    if \"lm_head\" in name or\
          \ \"embed_tokens\" in name:\n        if hasattr(module, \"weight\"):\n \
          \         module = module.to(torch.float16)\n            # if script_args.bf16\
          \ and module.weight.dtype == torch.float32:\n            #     module =\
          \ module.to(torch.bfloat16)\n            # if script_args.fp16 and module.weight.dtype\
          \ == torch.float32:\n            #     module = module.to(torch.float16)\n\
          model.print_trainable_parameters()\nlogger.info(f\"model.modules_to_save:\
          \ {model.modules_to_save}\")\nold_state_dict = model.state_dict\nmodel.state_dict\
          \ = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\
          ).__get__(model, type(model))\n</code></pre>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/fGVTeluTfTN-ad7MdObas.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/fGVTeluTfTN-ad7MdObas.png\"\
          ></a></p>\n"
        raw: "I was pre-training the facebook opt 350m model on text file. While training\
          \ after 15 steps the training loss is becoming zero.  Can you help me with\
          \ this.\n\nCode I used\n\n```\nif script_args.load_in_kbits in [4, 8]:\n\
          \    load_in_4bit = script_args.load_in_kbits == 4\n    load_in_8bit = script_args.load_in_kbits\
          \ == 8\n    if script_args.modules_to_save is not None:\n        load_in_8bit_skip_modules\
          \ = script_args.modules_to_save.split(\",\")\n    else:\n        load_in_8bit_skip_modules\
          \ = None\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=script_args.load_in_kbits\
          \ == 4,\n        load_in_8bit=script_args.load_in_kbits == 8,\n        llm_int8_threshold=6.0,\n\
          \        load_in_8bit_skip_modules=load_in_8bit_skip_modules,\n        bnb_4bit_compute_dtype=compute_dtype,\n\
          \        bnb_4bit_use_double_quant=script_args.double_quant,\n        bnb_4bit_quant_type=script_args.quant_type,\
          \  # {'fp4', 'nf4'}\n    )\nelse:\n    load_in_4bit = False\n    load_in_8bit\
          \ = False\n    quantization_config = None\nif quantization_config is not\
          \ None:\n    logger.info(f\"quantization_config:{quantization_config.to_dict()}\"\
          )\n\nif script_args.model_name_or_path:\n    torch_dtype = (\n        script_args.torch_dtype\n\
          \        if script_args.torch_dtype in [\"auto\", None]\n        else getattr(torch,\
          \ script_args.torch_dtype)\n    )\n    device_map = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        script_args.model_name_or_path,\n        from_tf=bool(\".ckpt\"\
          \ in script_args.model_name_or_path),\n        config=config,\n        #\
          \ cache_dir=script_args.cache_dir,\n        # revision=model_args.model_revision,\n\
          \        use_auth_token=True if script_args.use_auth_token else None,\n\
          \        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=True,\n   \
          \     device_map=device_map,\n        load_in_4bit=load_in_4bit,\n     \
          \   load_in_8bit=load_in_8bit,\n        quantization_config=quantization_config,\n\
          \    )\n    \nelse:\n    model = AutoModelForCausalLM.from_config(config)\n\
          \    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n\
          \    logger.info(\n        f\"Training new model from scratch - Total size={n_params/2**20:.2f}M\
          \ params\"\n    )\n\nif script_args.load_in_kbits in [4, 8]:\n    model\
          \ = prepare_model_for_kbit_training(\n        model, use_gradient_checkpointing=script_args.gradient_checkpointing\n\
          \    )\nmodel.config.use_cache = False\nmodel_vocab_size = model.get_output_embeddings().weight.size(0)\n\
          tokenizer_vocab_size = len(tokenizer)\nlogger.info(f\"Model vocab size:\
          \ {model_vocab_size}\")\nlogger.info(f\"Tokenizer vocab size: {tokenizer_vocab_size}\"\
          )\nif model_vocab_size != tokenizer_vocab_size:\n    logger.info(f\"Resize\
          \ model vocab size to {tokenizer_vocab_size}\")\n    model.resize_token_embeddings(len(tokenizer))\n\
          \nif script_args.peft_path is not None:\n    logger.info(\"Peft from pre-trained\
          \ model\")\n    model = PeftModel.from_pretrained(\n        model, script_args.peft_path,\
          \ device_map=device_map\n    )\nelse:\n    logger.info(\"Init new peft model\"\
          )\n    target_modules = script_args.trainable.split(\",\")\n    modules_to_save\
          \ = script_args.modules_to_save\n    if modules_to_save is not None:\n \
          \       modules_to_save = modules_to_save.split(\",\")\n    lora_rank =\
          \ script_args.lora_rank\n    lora_dropout = script_args.lora_dropout\n \
          \   lora_alpha = script_args.lora_alpha\n    logger.info(f\"target_modules:\
          \ {target_modules}\")\n    logger.info(f\"lora_rank: {lora_rank}\")\n  \
          \  peft_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n  \
          \      target_modules=target_modules,\n        inference_mode=False,\n \
          \       r=lora_rank,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n\
          \        modules_to_save=modules_to_save,\n    )\n    model = get_peft_model(model,\
          \ peft_config)\nfor name, module in model.named_modules():\n    if isinstance(module,\
          \ LoraLayer):\n      module = module.to(torch.float16)\n        # if script_args.bf16:\n\
          \        #     module = module.to(torch.bfloat16)\n        # if script_args.fp16:\n\
          \        #     module = module.to(torch.float16)\n    if \"norm\" in name:\n\
          \        module = module.to(torch.float16)\n    if \"lm_head\" in name or\
          \ \"embed_tokens\" in name:\n        if hasattr(module, \"weight\"):\n \
          \         module = module.to(torch.float16)\n            # if script_args.bf16\
          \ and module.weight.dtype == torch.float32:\n            #     module =\
          \ module.to(torch.bfloat16)\n            # if script_args.fp16 and module.weight.dtype\
          \ == torch.float32:\n            #     module = module.to(torch.float16)\n\
          model.print_trainable_parameters()\nlogger.info(f\"model.modules_to_save:\
          \ {model.modules_to_save}\")\nold_state_dict = model.state_dict\nmodel.state_dict\
          \ = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\
          ).__get__(model, type(model))\n\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/fGVTeluTfTN-ad7MdObas.png)\n"
        updatedAt: '2024-01-18T07:59:28.829Z'
      numEdits: 1
      reactions: []
    id: 65a8d95265e4f1a5eb60774d
    type: comment
  author: saivineetha
  content: "I was pre-training the facebook opt 350m model on text file. While training\
    \ after 15 steps the training loss is becoming zero.  Can you help me with this.\n\
    \nCode I used\n\n```\nif script_args.load_in_kbits in [4, 8]:\n    load_in_4bit\
    \ = script_args.load_in_kbits == 4\n    load_in_8bit = script_args.load_in_kbits\
    \ == 8\n    if script_args.modules_to_save is not None:\n        load_in_8bit_skip_modules\
    \ = script_args.modules_to_save.split(\",\")\n    else:\n        load_in_8bit_skip_modules\
    \ = None\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=script_args.load_in_kbits\
    \ == 4,\n        load_in_8bit=script_args.load_in_kbits == 8,\n        llm_int8_threshold=6.0,\n\
    \        load_in_8bit_skip_modules=load_in_8bit_skip_modules,\n        bnb_4bit_compute_dtype=compute_dtype,\n\
    \        bnb_4bit_use_double_quant=script_args.double_quant,\n        bnb_4bit_quant_type=script_args.quant_type,\
    \  # {'fp4', 'nf4'}\n    )\nelse:\n    load_in_4bit = False\n    load_in_8bit\
    \ = False\n    quantization_config = None\nif quantization_config is not None:\n\
    \    logger.info(f\"quantization_config:{quantization_config.to_dict()}\")\n\n\
    if script_args.model_name_or_path:\n    torch_dtype = (\n        script_args.torch_dtype\n\
    \        if script_args.torch_dtype in [\"auto\", None]\n        else getattr(torch,\
    \ script_args.torch_dtype)\n    )\n    device_map = {\"\": int(os.environ.get(\"\
    LOCAL_RANK\") or 0)}\n    model = AutoModelForCausalLM.from_pretrained(\n    \
    \    script_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in script_args.model_name_or_path),\n\
    \        config=config,\n        # cache_dir=script_args.cache_dir,\n        #\
    \ revision=model_args.model_revision,\n        use_auth_token=True if script_args.use_auth_token\
    \ else None,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=True,\n\
    \        device_map=device_map,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n\
    \        quantization_config=quantization_config,\n    )\n    \nelse:\n    model\
    \ = AutoModelForCausalLM.from_config(config)\n    n_params = sum({p.data_ptr():\
    \ p.numel() for p in model.parameters()}.values())\n    logger.info(\n       \
    \ f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\"\
    \n    )\n\nif script_args.load_in_kbits in [4, 8]:\n    model = prepare_model_for_kbit_training(\n\
    \        model, use_gradient_checkpointing=script_args.gradient_checkpointing\n\
    \    )\nmodel.config.use_cache = False\nmodel_vocab_size = model.get_output_embeddings().weight.size(0)\n\
    tokenizer_vocab_size = len(tokenizer)\nlogger.info(f\"Model vocab size: {model_vocab_size}\"\
    )\nlogger.info(f\"Tokenizer vocab size: {tokenizer_vocab_size}\")\nif model_vocab_size\
    \ != tokenizer_vocab_size:\n    logger.info(f\"Resize model vocab size to {tokenizer_vocab_size}\"\
    )\n    model.resize_token_embeddings(len(tokenizer))\n\nif script_args.peft_path\
    \ is not None:\n    logger.info(\"Peft from pre-trained model\")\n    model =\
    \ PeftModel.from_pretrained(\n        model, script_args.peft_path, device_map=device_map\n\
    \    )\nelse:\n    logger.info(\"Init new peft model\")\n    target_modules =\
    \ script_args.trainable.split(\",\")\n    modules_to_save = script_args.modules_to_save\n\
    \    if modules_to_save is not None:\n        modules_to_save = modules_to_save.split(\"\
    ,\")\n    lora_rank = script_args.lora_rank\n    lora_dropout = script_args.lora_dropout\n\
    \    lora_alpha = script_args.lora_alpha\n    logger.info(f\"target_modules: {target_modules}\"\
    )\n    logger.info(f\"lora_rank: {lora_rank}\")\n    peft_config = LoraConfig(\n\
    \        task_type=TaskType.CAUSAL_LM,\n        target_modules=target_modules,\n\
    \        inference_mode=False,\n        r=lora_rank,\n        lora_alpha=lora_alpha,\n\
    \        lora_dropout=lora_dropout,\n        modules_to_save=modules_to_save,\n\
    \    )\n    model = get_peft_model(model, peft_config)\nfor name, module in model.named_modules():\n\
    \    if isinstance(module, LoraLayer):\n      module = module.to(torch.float16)\n\
    \        # if script_args.bf16:\n        #     module = module.to(torch.bfloat16)\n\
    \        # if script_args.fp16:\n        #     module = module.to(torch.float16)\n\
    \    if \"norm\" in name:\n        module = module.to(torch.float16)\n    if \"\
    lm_head\" in name or \"embed_tokens\" in name:\n        if hasattr(module, \"\
    weight\"):\n          module = module.to(torch.float16)\n            # if script_args.bf16\
    \ and module.weight.dtype == torch.float32:\n            #     module = module.to(torch.bfloat16)\n\
    \            # if script_args.fp16 and module.weight.dtype == torch.float32:\n\
    \            #     module = module.to(torch.float16)\nmodel.print_trainable_parameters()\n\
    logger.info(f\"model.modules_to_save: {model.modules_to_save}\")\nold_state_dict\
    \ = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self,\
    \ old_state_dict())\n).__get__(model, type(model))\n\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/640fe1d7b0ee289c8583a879/fGVTeluTfTN-ad7MdObas.png)\n"
  created_at: 2024-01-18 07:54:58+00:00
  edited: true
  hidden: false
  id: 65a8d95265e4f1a5eb60774d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: facebook/opt-350m
repo_type: model
status: open
target_branch: null
title: Pretraining error
