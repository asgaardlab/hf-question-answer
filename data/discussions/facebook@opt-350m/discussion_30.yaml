!!python/object:huggingface_hub.community.DiscussionWithDetails
author: macleginn
conflicting_files: null
created_at: 2023-08-19 06:31:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636650842701-noauth.jpeg?w=200&h=200&f=face
      fullname: Dmitry Nikolaev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macleginn
      type: user
    createdAt: '2023-08-19T07:31:36.000Z'
    data:
      edited: false
      editors:
      - macleginn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3788733184337616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636650842701-noauth.jpeg?w=200&h=200&f=face
          fullname: Dmitry Nikolaev
          isHf: false
          isPro: false
          name: macleginn
          type: user
        html: "<pre><code class=\"language-ipython\">In [<span class=\"hljs-number\"\
          >18</span>]: model_name = <span class=\"hljs-string\">'facebook/opt-350m'</span>\n\
          \    ...: tok1 = AutoTokenizer.from_pretrained(model_name, use_fast=<span\
          \ class=\"hljs-literal\">False</span>)\n    ...: model1 = AutoModel.from_pretrained(model_name)\n\
          \    ...: inputs1 = tok1(s, return_tensors=<span class=\"hljs-string\">'pt'</span>)\n\
          \    ...: outputs = model1(**inputs1, output_hidden_states=<span class=\"\
          hljs-literal\">True</span>)\n    ...: n_layers = <span class=\"hljs-built_in\"\
          >len</span>(outputs.hidden_states)\n    ...: <span class=\"hljs-keyword\"\
          >for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(n_layers-<span class=\"hljs-number\">5</span>, n_layers):\n\
          \    ...:     <span class=\"hljs-built_in\">print</span>(outputs.hidden_states[i][<span\
          \ class=\"hljs-number\">0</span>, -<span class=\"hljs-number\">1</span>].size())\n\
          \    ...:\ntorch.Size([<span class=\"hljs-number\">1024</span>])\ntorch.Size([<span\
          \ class=\"hljs-number\">1024</span>])\ntorch.Size([<span class=\"hljs-number\"\
          >1024</span>])\ntorch.Size([<span class=\"hljs-number\">1024</span>])\n\
          torch.Size([<span class=\"hljs-number\">512</span>])\n</code></pre>\n<p>Cf.:</p>\n\
          <pre><code class=\"language-ipython\">In [<span class=\"hljs-number\">19</span>]:\
          \ model_name = <span class=\"hljs-string\">'facebook/opt-1.3b'</span>\n\
          \    ...: tok1 = AutoTokenizer.from_pretrained(model_name, use_fast=<span\
          \ class=\"hljs-literal\">False</span>)\n    ...: model1 = AutoModel.from_pretrained(model_name)\n\
          \    ...: inputs1 = tok1(s, return_tensors=<span class=\"hljs-string\">'pt'</span>)\n\
          \    ...: outputs = model1(**inputs1, output_hidden_states=<span class=\"\
          hljs-literal\">True</span>)\n    ...: n_layers = <span class=\"hljs-built_in\"\
          >len</span>(outputs.hidden_states)\n    ...: <span class=\"hljs-keyword\"\
          >for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(n_layers-<span class=\"hljs-number\">5</span>, n_layers):\n\
          \    ...:     <span class=\"hljs-built_in\">print</span>(outputs.hidden_states[i][<span\
          \ class=\"hljs-number\">0</span>, -<span class=\"hljs-number\">1</span>].size())\n\
          \    ...:\ntorch.Size([<span class=\"hljs-number\">2048</span>])\ntorch.Size([<span\
          \ class=\"hljs-number\">2048</span>])\ntorch.Size([<span class=\"hljs-number\"\
          >2048</span>])\ntorch.Size([<span class=\"hljs-number\">2048</span>])\n\
          torch.Size([<span class=\"hljs-number\">2048</span>])\n</code></pre>\n"
        raw: "```ipython\r\nIn [18]: model_name = 'facebook/opt-350m'\r\n    ...:\
          \ tok1 = AutoTokenizer.from_pretrained(model_name, use_fast=False)\r\n \
          \   ...: model1 = AutoModel.from_pretrained(model_name)\r\n    ...: inputs1\
          \ = tok1(s, return_tensors='pt')\r\n    ...: outputs = model1(**inputs1,\
          \ output_hidden_states=True)\r\n    ...: n_layers = len(outputs.hidden_states)\r\
          \n    ...: for i in range(n_layers-5, n_layers):\r\n    ...:     print(outputs.hidden_states[i][0,\
          \ -1].size())\r\n    ...:\r\ntorch.Size([1024])\r\ntorch.Size([1024])\r\n\
          torch.Size([1024])\r\ntorch.Size([1024])\r\ntorch.Size([512])\r\n```\r\n\
          \r\nCf.:\r\n```ipython\r\nIn [19]: model_name = 'facebook/opt-1.3b'\r\n\
          \    ...: tok1 = AutoTokenizer.from_pretrained(model_name, use_fast=False)\r\
          \n    ...: model1 = AutoModel.from_pretrained(model_name)\r\n    ...: inputs1\
          \ = tok1(s, return_tensors='pt')\r\n    ...: outputs = model1(**inputs1,\
          \ output_hidden_states=True)\r\n    ...: n_layers = len(outputs.hidden_states)\r\
          \n    ...: for i in range(n_layers-5, n_layers):\r\n    ...:     print(outputs.hidden_states[i][0,\
          \ -1].size())\r\n    ...:\r\ntorch.Size([2048])\r\ntorch.Size([2048])\r\n\
          torch.Size([2048])\r\ntorch.Size([2048])\r\ntorch.Size([2048])\r\n```"
        updatedAt: '2023-08-19T07:31:36.439Z'
      numEdits: 0
      reactions: []
    id: 64e06fd83209bf41947ec318
    type: comment
  author: macleginn
  content: "```ipython\r\nIn [18]: model_name = 'facebook/opt-350m'\r\n    ...: tok1\
    \ = AutoTokenizer.from_pretrained(model_name, use_fast=False)\r\n    ...: model1\
    \ = AutoModel.from_pretrained(model_name)\r\n    ...: inputs1 = tok1(s, return_tensors='pt')\r\
    \n    ...: outputs = model1(**inputs1, output_hidden_states=True)\r\n    ...:\
    \ n_layers = len(outputs.hidden_states)\r\n    ...: for i in range(n_layers-5,\
    \ n_layers):\r\n    ...:     print(outputs.hidden_states[i][0, -1].size())\r\n\
    \    ...:\r\ntorch.Size([1024])\r\ntorch.Size([1024])\r\ntorch.Size([1024])\r\n\
    torch.Size([1024])\r\ntorch.Size([512])\r\n```\r\n\r\nCf.:\r\n```ipython\r\nIn\
    \ [19]: model_name = 'facebook/opt-1.3b'\r\n    ...: tok1 = AutoTokenizer.from_pretrained(model_name,\
    \ use_fast=False)\r\n    ...: model1 = AutoModel.from_pretrained(model_name)\r\
    \n    ...: inputs1 = tok1(s, return_tensors='pt')\r\n    ...: outputs = model1(**inputs1,\
    \ output_hidden_states=True)\r\n    ...: n_layers = len(outputs.hidden_states)\r\
    \n    ...: for i in range(n_layers-5, n_layers):\r\n    ...:     print(outputs.hidden_states[i][0,\
    \ -1].size())\r\n    ...:\r\ntorch.Size([2048])\r\ntorch.Size([2048])\r\ntorch.Size([2048])\r\
    \ntorch.Size([2048])\r\ntorch.Size([2048])\r\n```"
  created_at: 2023-08-19 06:31:36+00:00
  edited: false
  hidden: false
  id: 64e06fd83209bf41947ec318
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: facebook/opt-350m
repo_type: model
status: open
target_branch: null
title: The last layer returns a wrong embedding dimension
