!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-26 11:34:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-26T11:34:39.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8515520691871643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>I am not sure if you have checked out this dataset <a href="https://huggingface.co/datasets/unalignment/toxic-dpo-v0.1">toxic-dpo-v0.1</a>
          yet.</p>

          <p>It could be useful for your next step?</p>

          <p>Thanks</p>

          '
        raw: "Hi,\r\n\r\nI am not sure if you have checked out this dataset [toxic-dpo-v0.1](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.1)\
          \ yet.\r\n\r\nIt could be useful for your next step?\r\n\r\nThanks"
        updatedAt: '2023-12-26T11:34:39.410Z'
      numEdits: 0
      reactions: []
    id: 658aba4f304552ba0cfefd87
    type: comment
  author: Yhyu13
  content: "Hi,\r\n\r\nI am not sure if you have checked out this dataset [toxic-dpo-v0.1](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.1)\
    \ yet.\r\n\r\nIt could be useful for your next step?\r\n\r\nThanks"
  created_at: 2023-12-26 11:34:39+00:00
  edited: false
  hidden: false
  id: 658aba4f304552ba0cfefd87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-12-26T14:03:27.000Z'
    data:
      edited: true
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964825451374054
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Hi. I''ve seen it, but it''s not what my aim is.<br>I want to create/find
          a dataset of prompt/2responses where one response is either a normal response
          of an instruct model or a silly refusal and the other one is unaligned base
          model that doesn''t know what instruct is and just continues completing
          the prompt.</p>

          <p>Here''s an example of what I am after:</p>

          <p>Prompt: What is the distance from Paris to New York?</p>

          <p>Response 1 (rejected): Sorry, but as an AI I don''t have a physical body
          and therefore can''t possibly know the distance between those two places.</p>

          <p>Response 2 (accepted): How can I travel from Paris to New York? How long
          does it take to do it? What places are worth a visit once I am in New York?</p>

          <p>My unproven theory is that contaminated fake "base"models like llama
          2, yi and yayi2 have much more experience dealing with completion rather
          than unaligned instruct due to 99% of their training being completion with
          1% of SFT alignment / RLHF lobotomy. This means that it should be easier
          to recover the completion capabilities and erase SFT alignment / RLHF lobotomy
          rather than introduce new instruct unalignment. Having new base model that
          can be used by others later and will allow sft training on any data later
          is also in my opinion more desirable than having new single unaligned finetune
          that won''t be easy to work with for further finetuning. What''s the most
          censored small (&lt;35B) LLM that you''ve tried? I am searching for good
          fit for a model that will create "rejected" responses and will refuse to
          answer silly things.</p>

          '
        raw: 'Hi. I''ve seen it, but it''s not what my aim is.

          I want to create/find a dataset of prompt/2responses where one response
          is either a normal response of an instruct model or a silly refusal and
          the other one is unaligned base model that doesn''t know what instruct is
          and just continues completing the prompt.


          Here''s an example of what I am after:


          Prompt: What is the distance from Paris to New York?


          Response 1 (rejected): Sorry, but as an AI I don''t have a physical body
          and therefore can''t possibly know the distance between those two places.


          Response 2 (accepted): How can I travel from Paris to New York? How long
          does it take to do it? What places are worth a visit once I am in New York?


          My unproven theory is that contaminated fake "base"models like llama 2,
          yi and yayi2 have much more experience dealing with completion rather than
          unaligned instruct due to 99% of their training being completion with 1%
          of SFT alignment / RLHF lobotomy. This means that it should be easier to
          recover the completion capabilities and erase SFT alignment / RLHF lobotomy
          rather than introduce new instruct unalignment. Having new base model that
          can be used by others later and will allow sft training on any data later
          is also in my opinion more desirable than having new single unaligned finetune
          that won''t be easy to work with for further finetuning. What''s the most
          censored small (<35B) LLM that you''ve tried? I am searching for good fit
          for a model that will create "rejected" responses and will refuse to answer
          silly things.'
        updatedAt: '2023-12-26T14:05:13.684Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Yhyu13
    id: 658add2f586088fd27a5dcc7
    type: comment
  author: adamo1139
  content: 'Hi. I''ve seen it, but it''s not what my aim is.

    I want to create/find a dataset of prompt/2responses where one response is either
    a normal response of an instruct model or a silly refusal and the other one is
    unaligned base model that doesn''t know what instruct is and just continues completing
    the prompt.


    Here''s an example of what I am after:


    Prompt: What is the distance from Paris to New York?


    Response 1 (rejected): Sorry, but as an AI I don''t have a physical body and therefore
    can''t possibly know the distance between those two places.


    Response 2 (accepted): How can I travel from Paris to New York? How long does
    it take to do it? What places are worth a visit once I am in New York?


    My unproven theory is that contaminated fake "base"models like llama 2, yi and
    yayi2 have much more experience dealing with completion rather than unaligned
    instruct due to 99% of their training being completion with 1% of SFT alignment
    / RLHF lobotomy. This means that it should be easier to recover the completion
    capabilities and erase SFT alignment / RLHF lobotomy rather than introduce new
    instruct unalignment. Having new base model that can be used by others later and
    will allow sft training on any data later is also in my opinion more desirable
    than having new single unaligned finetune that won''t be easy to work with for
    further finetuning. What''s the most censored small (<35B) LLM that you''ve tried?
    I am searching for good fit for a model that will create "rejected" responses
    and will refuse to answer silly things.'
  created_at: 2023-12-26 14:03:27+00:00
  edited: true
  hidden: false
  id: 658add2f586088fd27a5dcc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-27T09:47:40.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7571814060211182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;adamo1139&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/adamo1139\">@<span class=\"\
          underline\">adamo1139</span></a></span>\n\n\t</span></span> </p>\n<p>this\
          \ 7B model <a href=\"https://huggingface.co/LLM360/AmberSafe\">https://huggingface.co/LLM360/AmberSafe</a>\
          \ is equipped with latest safety guardrails</p>\n<p>And this model too,\
          \ <a href=\"https://huggingface.co/PKU-Alignment/beaver-7b-v1.0\">https://huggingface.co/PKU-Alignment/beaver-7b-v1.0</a></p>\n\
          <p>This dataset might fit your need : <a href=\"https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden\"\
          >https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden</a></p>\n"
        raw: "@adamo1139 \n\nthis 7B model https://huggingface.co/LLM360/AmberSafe\
          \ is equipped with latest safety guardrails\n\nAnd this model too, https://huggingface.co/PKU-Alignment/beaver-7b-v1.0\n\
          \nThis dataset might fit your need : https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden"
        updatedAt: '2023-12-27T10:33:03.147Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - adamo1139
    id: 658bf2bc33b0bf491dbc71f8
    type: comment
  author: Yhyu13
  content: "@adamo1139 \n\nthis 7B model https://huggingface.co/LLM360/AmberSafe is\
    \ equipped with latest safety guardrails\n\nAnd this model too, https://huggingface.co/PKU-Alignment/beaver-7b-v1.0\n\
    \nThis dataset might fit your need : https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden"
  created_at: 2023-12-27 09:47:40+00:00
  edited: true
  hidden: false
  id: 658bf2bc33b0bf491dbc71f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-12-27T10:31:52.000Z'
    data:
      edited: true
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9660029411315918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>Thanks, I will give it a go</p>

          <p>Edit: regarding datasets - my plan is to use datasets without any potentially
          non-ethical questions. So I don''t explicitly tell the model to do bad stuff,
          instead I  strongly tell it to not refuse doing silly things. I ran a test
          on jondurbin''s truthy dpo prompt dataset yesterday and I think I can get
          that done. It would make sense to have that dataset be 100-150MB in size
          though, so unless I find a way to drastically boost inference performance
          on my PC, it will take a few days of constant inference. </p>

          '
        raw: 'Thanks, I will give it a go


          Edit: regarding datasets - my plan is to use datasets without any potentially
          non-ethical questions. So I don''t explicitly tell the model to do bad stuff,
          instead I  strongly tell it to not refuse doing silly things. I ran a test
          on jondurbin''s truthy dpo prompt dataset yesterday and I think I can get
          that done. It would make sense to have that dataset be 100-150MB in size
          though, so unless I find a way to drastically boost inference performance
          on my PC, it will take a few days of constant inference. '
        updatedAt: '2023-12-27T10:51:57.324Z'
      numEdits: 1
      reactions: []
    id: 658bfd187f1e21412ccb01d5
    type: comment
  author: adamo1139
  content: 'Thanks, I will give it a go


    Edit: regarding datasets - my plan is to use datasets without any potentially
    non-ethical questions. So I don''t explicitly tell the model to do bad stuff,
    instead I  strongly tell it to not refuse doing silly things. I ran a test on
    jondurbin''s truthy dpo prompt dataset yesterday and I think I can get that done.
    It would make sense to have that dataset be 100-150MB in size though, so unless
    I find a way to drastically boost inference performance on my PC, it will take
    a few days of constant inference. '
  created_at: 2023-12-27 10:31:52+00:00
  edited: true
  hidden: false
  id: 658bfd187f1e21412ccb01d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc5b4512d00c4589dfa8a6/0U8IW9ZKT5crAIR30sLP_.png?w=200&h=200&f=face
      fullname: ddh0
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ddh0
      type: user
    createdAt: '2024-01-02T04:30:15.000Z'
    data:
      edited: false
      editors:
      - ddh0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8549235463142395
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc5b4512d00c4589dfa8a6/0U8IW9ZKT5crAIR30sLP_.png?w=200&h=200&f=face
          fullname: ddh0
          isHf: false
          isPro: false
          name: ddh0
          type: user
        html: "<blockquote>\n<p>What's the most censored small (&lt;35B) LLM that\
          \ you've tried? </p>\n</blockquote>\n<p>Any Llama 2 Chat model using the\
          \ official system prompt will drive you insane with it's censoring <span\
          \ data-props=\"{&quot;user&quot;:&quot;adamo1139&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/adamo1139\">@<span class=\"\
          underline\">adamo1139</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "> What's the most censored small (<35B) LLM that you've tried? \n\n\
          Any Llama 2 Chat model using the official system prompt will drive you insane\
          \ with it's censoring @adamo1139 "
        updatedAt: '2024-01-02T04:30:15.447Z'
      numEdits: 0
      reactions: []
    id: 6593915743971eed4582d5c3
    type: comment
  author: ddh0
  content: "> What's the most censored small (<35B) LLM that you've tried? \n\nAny\
    \ Llama 2 Chat model using the official system prompt will drive you insane with\
    \ it's censoring @adamo1139 "
  created_at: 2024-01-02 04:30:15+00:00
  edited: false
  hidden: false
  id: 6593915743971eed4582d5c3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: adamo1139/Yi-34B-200K-AEZAKMI-v2
repo_type: model
status: open
target_branch: null
title: On the dataset for unfiltered dpo
