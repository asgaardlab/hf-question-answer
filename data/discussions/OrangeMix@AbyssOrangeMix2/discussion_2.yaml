!!python/object:huggingface_hub.community.DiscussionWithDetails
author: garyfang
conflicting_files: null
created_at: 2023-01-29 11:11:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb9a147b21430deada5a43e28d157566.svg
      fullname: Gary Fang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: garyfang
      type: user
    createdAt: '2023-01-29T11:11:30.000Z'
    data:
      edited: false
      editors:
      - garyfang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb9a147b21430deada5a43e28d157566.svg
          fullname: Gary Fang
          isHf: false
          isPro: false
          name: garyfang
          type: user
        html: '<p>The run with diffusers but got the following problems, anyone know
          how to solve it.<br>I can successfully run the anything3.0 model with the
          same scripts</p>

          <hr>

          <p>KeyError                                  Traceback (most recent call
          last)<br>Input In [1], in &lt;cell line: 17&gt;()<br>     15 negative_prompt
          = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit,
          fewer digits, cropped, worst quality, low quality, normal quality, jpeg
          artifacts, signature, watermark, username, blurry, artist name"<br>     17
          with torch.autocast("cuda"):<br>---&gt; 18     image = pipe(prompt,<br>     19                  negative_prompt=negative_prompt,<br>     20                  width=512,<br>     21                  height=768,<br>     22                  guidance_scale=12,<br>     23                  num_inference_steps=50).images[0]<br>     25
          image.save("anime_girl.png")</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27,
          in _DecoratorContextManager.<strong>call</strong>..decorate_context(*args,
          **kwargs)<br>     24 @functools.wraps(func)<br>     25 def decorate_context(*args,
          **kwargs):<br>     26     with self.clone():<br>---&gt; 27         return
          func(*args, **kwargs)</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:568,
          in StableDiffusionPipeline.<strong>call</strong>(self, prompt, height, width,
          num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt,
          eta, generator, latents, output_type, return_dict, callback, callback_steps,
          **kwargs)<br>    565 image = self.decode_latents(latents)<br>    567 # 9.
          Run safety checker<br>--&gt; 568 image, has_nsfw_concept = self.run_safety_checker(image,
          device, text_embeddings.dtype)<br>    570 # 10. Convert to PIL<br>    571
          if output_type == "pil":</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:365,
          in StableDiffusionPipeline.run_safety_checker(self, image, device, dtype)<br>    363
          def run_safety_checker(self, image, device, dtype):<br>    364     if self.safety_checker
          is not None:<br>--&gt; 365         safety_checker_input = self.feature_extractor(self.numpy_to_pil(image),
          return_tensors="pt").to(device)<br>    366         image, has_nsfw_concept
          = self.safety_checker(<br>    367             images=image, clip_input=safety_checker_input.pixel_values.to(dtype)<br>    368         )<br>    369     else:</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:155,
          in CLIPFeatureExtractor.<strong>call</strong>(self, images, return_tensors,
          **kwargs)<br>    153     images = [self.convert_rgb(image) for image in
          images]<br>    154 if self.do_resize and self.size is not None and self.resample
          is not None:<br>--&gt; 155     images = [<br>    156         self.resize(image=image,
          size=self.size, resample=self.resample, default_to_square=False)<br>    157         for
          image in images<br>    158     ]<br>    159 if self.do_center_crop and self.crop_size
          is not None:<br>    160     images = [self.center_crop(image, self.crop_size)
          for image in images]</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:156,
          in (.0)<br>    153     images = [self.convert_rgb(image) for image in images]<br>    154
          if self.do_resize and self.size is not None and self.resample is not None:<br>    155     images
          = [<br>--&gt; 156         self.resize(image=image, size=self.size, resample=self.resample,
          default_to_square=False)<br>    157         for image in images<br>    158     ]<br>    159
          if self.do_center_crop and self.crop_size is not None:<br>    160     images
          = [self.center_crop(image, self.crop_size) for image in images]</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/transformers/image_utils.py:400,
          in ImageFeatureExtractionMixin.resize(self, image, size, resample, default_to_square,
          max_size)<br>    398 # specified size only for the smallest edge<br>    399
          short, long = (width, height) if width &lt;= height else (height, width)<br>--&gt;
          400 requested_new_short = size if isinstance(size, int) else size[0]<br>    402
          if short == requested_new_short:<br>    403     return image</p>

          <p>KeyError: 0</p>

          '
        raw: "The run with diffusers but got the following problems, anyone know how\
          \ to solve it.\r\nI can successfully run the anything3.0 model with the\
          \ same scripts\r\n\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\nInput In [1], in <cell line: 17>()\r\n     15 negative_prompt\
          \ = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra\
          \ digit, fewer digits, cropped, worst quality, low quality, normal quality,\
          \ jpeg artifacts, signature, watermark, username, blurry, artist name\"\r\
          \n     17 with torch.autocast(\"cuda\"):\r\n---> 18     image = pipe(prompt,\
          \ \r\n     19                  negative_prompt=negative_prompt, \r\n   \
          \  20                  width=512,\r\n     21                  height=768,\r\
          \n     22                  guidance_scale=12,\r\n     23               \
          \   num_inference_steps=50).images[0]\r\n     25 image.save(\"anime_girl.png\"\
          )\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27,\
          \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args,\
          \ **kwargs)\r\n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args,\
          \ **kwargs):\r\n     26     with self.clone():\r\n---> 27         return\
          \ func(*args, **kwargs)\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:568,\
          \ in StableDiffusionPipeline.__call__(self, prompt, height, width, num_inference_steps,\
          \ guidance_scale, negative_prompt, num_images_per_prompt, eta, generator,\
          \ latents, output_type, return_dict, callback, callback_steps, **kwargs)\r\
          \n    565 image = self.decode_latents(latents)\r\n    567 # 9. Run safety\
          \ checker\r\n--> 568 image, has_nsfw_concept = self.run_safety_checker(image,\
          \ device, text_embeddings.dtype)\r\n    570 # 10. Convert to PIL\r\n   \
          \ 571 if output_type == \"pil\":\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:365,\
          \ in StableDiffusionPipeline.run_safety_checker(self, image, device, dtype)\r\
          \n    363 def run_safety_checker(self, image, device, dtype):\r\n    364\
          \     if self.safety_checker is not None:\r\n--> 365         safety_checker_input\
          \ = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\"\
          ).to(device)\r\n    366         image, has_nsfw_concept = self.safety_checker(\r\
          \n    367             images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\r\
          \n    368         )\r\n    369     else:\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:155,\
          \ in CLIPFeatureExtractor.__call__(self, images, return_tensors, **kwargs)\r\
          \n    153     images = [self.convert_rgb(image) for image in images]\r\n\
          \    154 if self.do_resize and self.size is not None and self.resample is\
          \ not None:\r\n--> 155     images = [\r\n    156         self.resize(image=image,\
          \ size=self.size, resample=self.resample, default_to_square=False)\r\n \
          \   157         for image in images\r\n    158     ]\r\n    159 if self.do_center_crop\
          \ and self.crop_size is not None:\r\n    160     images = [self.center_crop(image,\
          \ self.crop_size) for image in images]\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:156,\
          \ in <listcomp>(.0)\r\n    153     images = [self.convert_rgb(image) for\
          \ image in images]\r\n    154 if self.do_resize and self.size is not None\
          \ and self.resample is not None:\r\n    155     images = [\r\n--> 156  \
          \       self.resize(image=image, size=self.size, resample=self.resample,\
          \ default_to_square=False)\r\n    157         for image in images\r\n  \
          \  158     ]\r\n    159 if self.do_center_crop and self.crop_size is not\
          \ None:\r\n    160     images = [self.center_crop(image, self.crop_size)\
          \ for image in images]\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/image_utils.py:400,\
          \ in ImageFeatureExtractionMixin.resize(self, image, size, resample, default_to_square,\
          \ max_size)\r\n    398 # specified size only for the smallest edge\r\n \
          \   399 short, long = (width, height) if width <= height else (height, width)\r\
          \n--> 400 requested_new_short = size if isinstance(size, int) else size[0]\r\
          \n    402 if short == requested_new_short:\r\n    403     return image\r\
          \n\r\nKeyError: 0"
        updatedAt: '2023-01-29T11:11:30.571Z'
      numEdits: 0
      reactions: []
    id: 63d654621ce6700486c95307
    type: comment
  author: garyfang
  content: "The run with diffusers but got the following problems, anyone know how\
    \ to solve it.\r\nI can successfully run the anything3.0 model with the same scripts\r\
    \n\r\n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \nInput In [1], in <cell line: 17>()\r\n     15 negative_prompt = \"lowres, bad\
    \ anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits,\
    \ cropped, worst quality, low quality, normal quality, jpeg artifacts, signature,\
    \ watermark, username, blurry, artist name\"\r\n     17 with torch.autocast(\"\
    cuda\"):\r\n---> 18     image = pipe(prompt, \r\n     19                  negative_prompt=negative_prompt,\
    \ \r\n     20                  width=512,\r\n     21                  height=768,\r\
    \n     22                  guidance_scale=12,\r\n     23                  num_inference_steps=50).images[0]\r\
    \n     25 image.save(\"anime_girl.png\")\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27,\
    \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)\r\
    \n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args, **kwargs):\r\
    \n     26     with self.clone():\r\n---> 27         return func(*args, **kwargs)\r\
    \n\r\nFile ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:568,\
    \ in StableDiffusionPipeline.__call__(self, prompt, height, width, num_inference_steps,\
    \ guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents,\
    \ output_type, return_dict, callback, callback_steps, **kwargs)\r\n    565 image\
    \ = self.decode_latents(latents)\r\n    567 # 9. Run safety checker\r\n--> 568\
    \ image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\r\
    \n    570 # 10. Convert to PIL\r\n    571 if output_type == \"pil\":\r\n\r\nFile\
    \ ~/miniconda3/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:365,\
    \ in StableDiffusionPipeline.run_safety_checker(self, image, device, dtype)\r\n\
    \    363 def run_safety_checker(self, image, device, dtype):\r\n    364     if\
    \ self.safety_checker is not None:\r\n--> 365         safety_checker_input = self.feature_extractor(self.numpy_to_pil(image),\
    \ return_tensors=\"pt\").to(device)\r\n    366         image, has_nsfw_concept\
    \ = self.safety_checker(\r\n    367             images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\r\
    \n    368         )\r\n    369     else:\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:155,\
    \ in CLIPFeatureExtractor.__call__(self, images, return_tensors, **kwargs)\r\n\
    \    153     images = [self.convert_rgb(image) for image in images]\r\n    154\
    \ if self.do_resize and self.size is not None and self.resample is not None:\r\
    \n--> 155     images = [\r\n    156         self.resize(image=image, size=self.size,\
    \ resample=self.resample, default_to_square=False)\r\n    157         for image\
    \ in images\r\n    158     ]\r\n    159 if self.do_center_crop and self.crop_size\
    \ is not None:\r\n    160     images = [self.center_crop(image, self.crop_size)\
    \ for image in images]\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:156,\
    \ in <listcomp>(.0)\r\n    153     images = [self.convert_rgb(image) for image\
    \ in images]\r\n    154 if self.do_resize and self.size is not None and self.resample\
    \ is not None:\r\n    155     images = [\r\n--> 156         self.resize(image=image,\
    \ size=self.size, resample=self.resample, default_to_square=False)\r\n    157\
    \         for image in images\r\n    158     ]\r\n    159 if self.do_center_crop\
    \ and self.crop_size is not None:\r\n    160     images = [self.center_crop(image,\
    \ self.crop_size) for image in images]\r\n\r\nFile ~/miniconda3/lib/python3.8/site-packages/transformers/image_utils.py:400,\
    \ in ImageFeatureExtractionMixin.resize(self, image, size, resample, default_to_square,\
    \ max_size)\r\n    398 # specified size only for the smallest edge\r\n    399\
    \ short, long = (width, height) if width <= height else (height, width)\r\n-->\
    \ 400 requested_new_short = size if isinstance(size, int) else size[0]\r\n   \
    \ 402 if short == requested_new_short:\r\n    403     return image\r\n\r\nKeyError:\
    \ 0"
  created_at: 2023-01-29 11:11:30+00:00
  edited: false
  hidden: false
  id: 63d654621ce6700486c95307
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OrangeMix/AbyssOrangeMix2
repo_type: model
status: open
target_branch: null
title: got error when run the model in diffusers (seems related to safety checkers)
