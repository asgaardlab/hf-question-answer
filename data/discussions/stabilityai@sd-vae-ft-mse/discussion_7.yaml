!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YesThatsRIght
conflicting_files: null
created_at: 2023-01-04 22:50:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e4126044b662a96aaf1eecff99104e7.svg
      fullname: 'Yes'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YesThatsRIght
      type: user
    createdAt: '2023-01-04T22:50:17.000Z'
    data:
      edited: false
      editors:
      - YesThatsRIght
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e4126044b662a96aaf1eecff99104e7.svg
          fullname: 'Yes'
          isHf: false
          isPro: false
          name: YesThatsRIght
          type: user
        html: "<p>There are ostensibly 2 versions of this VAE:</p>\n<p><a href=\"\
          https://huggingface.co/stabilityai/sd-vae-ft-mse\">https://huggingface.co/stabilityai/sd-vae-ft-mse</a><br><a\
          \ href=\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original\">https://huggingface.co/stabilityai/sd-vae-ft-mse-original</a></p>\n\
          <p>The tables list the same file: <a href=\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"\
          >https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt</a></p>\n\
          <p>And the <code>vae-ft-mse-840000-ema-pruned.ckpt</code> under <code>Files\
          \ and versions</code> for both repos have the same sha256sum: <code>c6a580b13a5bc05a5e16e4dbb80608ff2ec251a162311590c1f34c013d7f3dab</code></p>\n\
          <p>Unfortunately, this model is not compatible with the CompVis/latent-diffusion\
          \ repo. When trying to load it in CompVis, I get the following error:</p>\n\
          <pre><code> RuntimeError: Error(s) in loading state_dict for AutoencoderKL:\n\
          \     Missing key(s) in state_dict: \"loss.logvar\", \"loss.perceptual_loss.scaling_layer.shift\"\
          , \"loss.perceptual_loss.scaling_layer.scale\", \"loss.perceptual_loss.net.slice1.0.weight\"\
          , \"loss.perceptual_loss.net.slice1.0.bias\", \"loss.perceptual_loss.net.slice1.2.weight\"\
          , \"loss.perceptual_loss.net.slice1.2.bias\", \"loss.perceptual_loss.net.slice2.5.weight\"\
          , \"loss.perceptual_loss.net.slice2.5.bias\", \"loss.perceptual_loss.net.slice2.7.weight\"\
          , \"loss.perceptual_loss.net.slice2.7.bias\", \"loss.perceptual_loss.net.slice3.10.weight\"\
          , \"loss.perceptual_loss.net.slice3.10.bias\", \"loss.perceptual_loss.net.slice3.12.weight\"\
          , \"loss.perceptual_loss.net.slice3.12.bias\", \"loss.perceptual_loss.net.slice3.14.weight\"\
          , \"loss.perceptual_loss.net.slice3.14.bias\", \"loss.perceptual_loss.net.slice4.17.weight\"\
          , \"loss.perceptual_loss.net.slice4.17.bias\", \"loss.perceptual_loss.net.slice4.19.weight\"\
          , \"loss.perceptual_loss.net.slice4.19.bias\", \"loss.perceptual_loss.net.slice4.21.weight\"\
          , \"loss.perceptual_loss.net.slice4.21.bias\", \"loss.perceptual_loss.net.slice5.24.weight\"\
          , \"loss.perceptual_loss.net.slice5.24.bias\", \"loss.perceptual_loss.net.slice5.26.weight\"\
          , \"loss.perceptual_loss.net.slice5.26.bias\", \"loss.perceptual_loss.net.slice5.28.weight\"\
          , \"loss.perceptual_loss.net.slice5.28.bias\", \"loss.perceptual_loss.lin0.model.1.weight\"\
          , \"loss.perceptual_loss.lin1.model.1.weight\", \"loss.perceptual_loss.lin2.model.1.weight\"\
          , \"loss.perceptual_loss.lin3.model.1.weight\", \"loss.perceptual_loss.lin4.model.1.weight\"\
          , \"loss.discriminator.main.0.weight\", \"loss.discriminator.main.0.bias\"\
          , \"loss.discriminator.main.2.weight\", \"loss.discriminator.main.3.weight\"\
          , \"loss.discriminator.main.3.bias\", \"loss.discriminator.main.3.running_mean\"\
          , \"loss.discriminator.main.3.running_var\", \"loss.discriminator.main.5.weight\"\
          , \"loss.discriminator.main.6.weight\", \"loss.discriminator.main.6.bias\"\
          , \"loss.discriminator.main.6.running_mean\", \"loss.discriminator.main.6.running_var\"\
          , \"loss.discriminator.main.8.weight\", \"loss.discriminator.main.9.weight\"\
          , \"loss.discriminator.main.9.bias\", \"loss.discriminator.main.9.running_mean\"\
          , \"loss.discriminator.main.9.running_var\", \"loss.discriminator.main.11.weight\"\
          , \"loss.discriminator.main.11.bias\".\n     Unexpected key(s) in state_dict:\
          \ \"model_ema.decay\", \"model_ema.num_updates\".\n</code></pre>\n"
        raw: "There are ostensibly 2 versions of this VAE:\r\n\r\nhttps://huggingface.co/stabilityai/sd-vae-ft-mse\r\
          \nhttps://huggingface.co/stabilityai/sd-vae-ft-mse-original\r\n\r\nThe tables\
          \ list the same file: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\r\
          \n\r\nAnd the `vae-ft-mse-840000-ema-pruned.ckpt` under `Files and versions`\
          \ for both repos have the same sha256sum: `c6a580b13a5bc05a5e16e4dbb80608ff2ec251a162311590c1f34c013d7f3dab`\r\
          \n\r\nUnfortunately, this model is not compatible with the CompVis/latent-diffusion\
          \ repo. When trying to load it in CompVis, I get the following error:\r\n\
          \r\n```\r\n RuntimeError: Error(s) in loading state_dict for AutoencoderKL:\r\
          \n \tMissing key(s) in state_dict: \"loss.logvar\", \"loss.perceptual_loss.scaling_layer.shift\"\
          , \"loss.perceptual_loss.scaling_layer.scale\", \"loss.perceptual_loss.net.slice1.0.weight\"\
          , \"loss.perceptual_loss.net.slice1.0.bias\", \"loss.perceptual_loss.net.slice1.2.weight\"\
          , \"loss.perceptual_loss.net.slice1.2.bias\", \"loss.perceptual_loss.net.slice2.5.weight\"\
          , \"loss.perceptual_loss.net.slice2.5.bias\", \"loss.perceptual_loss.net.slice2.7.weight\"\
          , \"loss.perceptual_loss.net.slice2.7.bias\", \"loss.perceptual_loss.net.slice3.10.weight\"\
          , \"loss.perceptual_loss.net.slice3.10.bias\", \"loss.perceptual_loss.net.slice3.12.weight\"\
          , \"loss.perceptual_loss.net.slice3.12.bias\", \"loss.perceptual_loss.net.slice3.14.weight\"\
          , \"loss.perceptual_loss.net.slice3.14.bias\", \"loss.perceptual_loss.net.slice4.17.weight\"\
          , \"loss.perceptual_loss.net.slice4.17.bias\", \"loss.perceptual_loss.net.slice4.19.weight\"\
          , \"loss.perceptual_loss.net.slice4.19.bias\", \"loss.perceptual_loss.net.slice4.21.weight\"\
          , \"loss.perceptual_loss.net.slice4.21.bias\", \"loss.perceptual_loss.net.slice5.24.weight\"\
          , \"loss.perceptual_loss.net.slice5.24.bias\", \"loss.perceptual_loss.net.slice5.26.weight\"\
          , \"loss.perceptual_loss.net.slice5.26.bias\", \"loss.perceptual_loss.net.slice5.28.weight\"\
          , \"loss.perceptual_loss.net.slice5.28.bias\", \"loss.perceptual_loss.lin0.model.1.weight\"\
          , \"loss.perceptual_loss.lin1.model.1.weight\", \"loss.perceptual_loss.lin2.model.1.weight\"\
          , \"loss.perceptual_loss.lin3.model.1.weight\", \"loss.perceptual_loss.lin4.model.1.weight\"\
          , \"loss.discriminator.main.0.weight\", \"loss.discriminator.main.0.bias\"\
          , \"loss.discriminator.main.2.weight\", \"loss.discriminator.main.3.weight\"\
          , \"loss.discriminator.main.3.bias\", \"loss.discriminator.main.3.running_mean\"\
          , \"loss.discriminator.main.3.running_var\", \"loss.discriminator.main.5.weight\"\
          , \"loss.discriminator.main.6.weight\", \"loss.discriminator.main.6.bias\"\
          , \"loss.discriminator.main.6.running_mean\", \"loss.discriminator.main.6.running_var\"\
          , \"loss.discriminator.main.8.weight\", \"loss.discriminator.main.9.weight\"\
          , \"loss.discriminator.main.9.bias\", \"loss.discriminator.main.9.running_mean\"\
          , \"loss.discriminator.main.9.running_var\", \"loss.discriminator.main.11.weight\"\
          , \"loss.discriminator.main.11.bias\".\r\n \tUnexpected key(s) in state_dict:\
          \ \"model_ema.decay\", \"model_ema.num_updates\".\r\n```"
        updatedAt: '2023-01-04T22:50:17.377Z'
      numEdits: 0
      reactions: []
    id: 63b602a9889aa6707f12453c
    type: comment
  author: YesThatsRIght
  content: "There are ostensibly 2 versions of this VAE:\r\n\r\nhttps://huggingface.co/stabilityai/sd-vae-ft-mse\r\
    \nhttps://huggingface.co/stabilityai/sd-vae-ft-mse-original\r\n\r\nThe tables\
    \ list the same file: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\r\
    \n\r\nAnd the `vae-ft-mse-840000-ema-pruned.ckpt` under `Files and versions` for\
    \ both repos have the same sha256sum: `c6a580b13a5bc05a5e16e4dbb80608ff2ec251a162311590c1f34c013d7f3dab`\r\
    \n\r\nUnfortunately, this model is not compatible with the CompVis/latent-diffusion\
    \ repo. When trying to load it in CompVis, I get the following error:\r\n\r\n\
    ```\r\n RuntimeError: Error(s) in loading state_dict for AutoencoderKL:\r\n \t\
    Missing key(s) in state_dict: \"loss.logvar\", \"loss.perceptual_loss.scaling_layer.shift\"\
    , \"loss.perceptual_loss.scaling_layer.scale\", \"loss.perceptual_loss.net.slice1.0.weight\"\
    , \"loss.perceptual_loss.net.slice1.0.bias\", \"loss.perceptual_loss.net.slice1.2.weight\"\
    , \"loss.perceptual_loss.net.slice1.2.bias\", \"loss.perceptual_loss.net.slice2.5.weight\"\
    , \"loss.perceptual_loss.net.slice2.5.bias\", \"loss.perceptual_loss.net.slice2.7.weight\"\
    , \"loss.perceptual_loss.net.slice2.7.bias\", \"loss.perceptual_loss.net.slice3.10.weight\"\
    , \"loss.perceptual_loss.net.slice3.10.bias\", \"loss.perceptual_loss.net.slice3.12.weight\"\
    , \"loss.perceptual_loss.net.slice3.12.bias\", \"loss.perceptual_loss.net.slice3.14.weight\"\
    , \"loss.perceptual_loss.net.slice3.14.bias\", \"loss.perceptual_loss.net.slice4.17.weight\"\
    , \"loss.perceptual_loss.net.slice4.17.bias\", \"loss.perceptual_loss.net.slice4.19.weight\"\
    , \"loss.perceptual_loss.net.slice4.19.bias\", \"loss.perceptual_loss.net.slice4.21.weight\"\
    , \"loss.perceptual_loss.net.slice4.21.bias\", \"loss.perceptual_loss.net.slice5.24.weight\"\
    , \"loss.perceptual_loss.net.slice5.24.bias\", \"loss.perceptual_loss.net.slice5.26.weight\"\
    , \"loss.perceptual_loss.net.slice5.26.bias\", \"loss.perceptual_loss.net.slice5.28.weight\"\
    , \"loss.perceptual_loss.net.slice5.28.bias\", \"loss.perceptual_loss.lin0.model.1.weight\"\
    , \"loss.perceptual_loss.lin1.model.1.weight\", \"loss.perceptual_loss.lin2.model.1.weight\"\
    , \"loss.perceptual_loss.lin3.model.1.weight\", \"loss.perceptual_loss.lin4.model.1.weight\"\
    , \"loss.discriminator.main.0.weight\", \"loss.discriminator.main.0.bias\", \"\
    loss.discriminator.main.2.weight\", \"loss.discriminator.main.3.weight\", \"loss.discriminator.main.3.bias\"\
    , \"loss.discriminator.main.3.running_mean\", \"loss.discriminator.main.3.running_var\"\
    , \"loss.discriminator.main.5.weight\", \"loss.discriminator.main.6.weight\",\
    \ \"loss.discriminator.main.6.bias\", \"loss.discriminator.main.6.running_mean\"\
    , \"loss.discriminator.main.6.running_var\", \"loss.discriminator.main.8.weight\"\
    , \"loss.discriminator.main.9.weight\", \"loss.discriminator.main.9.bias\", \"\
    loss.discriminator.main.9.running_mean\", \"loss.discriminator.main.9.running_var\"\
    , \"loss.discriminator.main.11.weight\", \"loss.discriminator.main.11.bias\".\r\
    \n \tUnexpected key(s) in state_dict: \"model_ema.decay\", \"model_ema.num_updates\"\
    .\r\n```"
  created_at: 2023-01-04 22:50:17+00:00
  edited: false
  hidden: false
  id: 63b602a9889aa6707f12453c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-01-04T23:08:02.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Gentle ping <span data-props=\"{&quot;user&quot;:&quot;multimodalart&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/multimodalart\"\
          >@<span class=\"underline\">multimodalart</span></a></span>\n\n\t</span></span></p>\n"
        raw: Gentle ping @multimodalart
        updatedAt: '2023-01-04T23:08:02.393Z'
      numEdits: 0
      reactions: []
    id: 63b606d29223c073fe929e3e
    type: comment
  author: patrickvonplaten
  content: Gentle ping @multimodalart
  created_at: 2023-01-04 23:08:02+00:00
  edited: false
  hidden: false
  id: 63b606d29223c073fe929e3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43edee55565d2e1035fea38529bc1cc5.svg
      fullname: Diffuser
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffuser317
      type: user
    createdAt: '2023-01-05T13:34:04.000Z'
    data:
      edited: false
      editors:
      - StableDiffuser317
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43edee55565d2e1035fea38529bc1cc5.svg
          fullname: Diffuser
          isHf: false
          isPro: false
          name: StableDiffuser317
          type: user
        html: "<p>Is there a pre-made script to train a diffusers VQVAE? Since they\
          \ quantize encoder's output through a codebook it is not trivial to backpropagate.\
          \ This implementation is quite complete (link : <a rel=\"nofollow\" href=\"\
          https://github.com/ritheshkumar95/pytorch-vqvae\">https://github.com/ritheshkumar95/pytorch-vqvae</a>)\
          \ however I have no clear idea how to wrap their VQVAE into a VQModel to\
          \ later use diffusers pipelines. Would you have any idea <span data-props=\"\
          {&quot;user&quot;:&quot;patrickvonplaten&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/patrickvonplaten\">@<span class=\"\
          underline\">patrickvonplaten</span></a></span>\n\n\t</span></span> ? Thanks\
          \ in advance</p>\n"
        raw: 'Is there a pre-made script to train a diffusers VQVAE? Since they quantize
          encoder''s output through a codebook it is not trivial to backpropagate.
          This implementation is quite complete (link : https://github.com/ritheshkumar95/pytorch-vqvae)
          however I have no clear idea how to wrap their VQVAE into a VQModel to later
          use diffusers pipelines. Would you have any idea @patrickvonplaten ? Thanks
          in advance'
        updatedAt: '2023-01-05T13:34:04.115Z'
      numEdits: 0
      reactions: []
    id: 63b6d1ccb24a3784b06d2b7f
    type: comment
  author: StableDiffuser317
  content: 'Is there a pre-made script to train a diffusers VQVAE? Since they quantize
    encoder''s output through a codebook it is not trivial to backpropagate. This
    implementation is quite complete (link : https://github.com/ritheshkumar95/pytorch-vqvae)
    however I have no clear idea how to wrap their VQVAE into a VQModel to later use
    diffusers pipelines. Would you have any idea @patrickvonplaten ? Thanks in advance'
  created_at: 2023-01-05 13:34:04+00:00
  edited: false
  hidden: false
  id: 63b6d1ccb24a3784b06d2b7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed13c8c672207116688104d4e8c7a9fe.svg
      fullname: Paolo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Paolonimus
      type: user
    createdAt: '2023-01-15T09:16:23.000Z'
    data:
      edited: false
      editors:
      - Paolonimus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed13c8c672207116688104d4e8c7a9fe.svg
          fullname: Paolo
          isHf: false
          isPro: false
          name: Paolonimus
          type: user
        html: "<p>I would also know. There\u2019s a repository of a guy who trains\
          \ an Anime autoencoder here:<br><a rel=\"nofollow\" href=\"https://github.com/cccntu/fine-tune-models\"\
          >https://github.com/cccntu/fine-tune-models</a></p>\n<p>The problem is that\
          \ it\u2019s not really easy to use and there are a lot of jax dependencies\
          \ that I\u2019m not familiar about.</p>\n<p>I guess that people is training\
          \ the autoencoder by using the latent diffusion repo by Compvis:</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://github.com/CompVis/latent-diffusion\"\
          >https://github.com/CompVis/latent-diffusion</a></p>\n<p>To train a KL\u2014\
          8 encoder that\u2019s swappable with the main encoder of stablediffusion\
          \ (I guess one can just rename the ckpt to bin?)</p>\n<p>To train a VQ model\
          \ the \u201Ctaming transformers\u201D repo should work, but again that\u2019\
          s not entirely clear or documented well if that works for SD.</p>\n"
        raw: "I would also know. There\u2019s a repository of a guy who trains an\
          \ Anime autoencoder here:\nhttps://github.com/cccntu/fine-tune-models\n\n\
          The problem is that it\u2019s not really easy to use and there are a lot\
          \ of jax dependencies that I\u2019m not familiar about.\n\nI guess that\
          \ people is training the autoencoder by using the latent diffusion repo\
          \ by Compvis:\n\nhttps://github.com/CompVis/latent-diffusion\n\nTo train\
          \ a KL\u20148 encoder that\u2019s swappable with the main encoder of stablediffusion\
          \ (I guess one can just rename the ckpt to bin?)\n\nTo train a VQ model\
          \ the \u201Ctaming transformers\u201D repo should work, but again that\u2019\
          s not entirely clear or documented well if that works for SD."
        updatedAt: '2023-01-15T09:16:23.504Z'
      numEdits: 0
      reactions: []
    id: 63c3c467bfd39d7373973a71
    type: comment
  author: Paolonimus
  content: "I would also know. There\u2019s a repository of a guy who trains an Anime\
    \ autoencoder here:\nhttps://github.com/cccntu/fine-tune-models\n\nThe problem\
    \ is that it\u2019s not really easy to use and there are a lot of jax dependencies\
    \ that I\u2019m not familiar about.\n\nI guess that people is training the autoencoder\
    \ by using the latent diffusion repo by Compvis:\n\nhttps://github.com/CompVis/latent-diffusion\n\
    \nTo train a KL\u20148 encoder that\u2019s swappable with the main encoder of\
    \ stablediffusion (I guess one can just rename the ckpt to bin?)\n\nTo train a\
    \ VQ model the \u201Ctaming transformers\u201D repo should work, but again that\u2019\
    s not entirely clear or documented well if that works for SD."
  created_at: 2023-01-15 09:16:23+00:00
  edited: false
  hidden: false
  id: 63c3c467bfd39d7373973a71
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/sd-vae-ft-mse
repo_type: model
status: open
target_branch: null
title: Model is broken when finetuning, and The CompVis and Diffusers models are identical
