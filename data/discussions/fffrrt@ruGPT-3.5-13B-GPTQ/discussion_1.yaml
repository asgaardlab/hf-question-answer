!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HuggingDroidX
conflicting_files: null
created_at: 2023-08-03 12:32:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ede6ecf42ad84246f219308f4fc5a8c6.svg
      fullname: HugDroidX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HuggingDroidX
      type: user
    createdAt: '2023-08-03T13:32:34.000Z'
    data:
      edited: false
      editors:
      - HuggingDroidX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.588532567024231
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ede6ecf42ad84246f219308f4fc5a8c6.svg
          fullname: HugDroidX
          isHf: false
          isPro: false
          name: HuggingDroidX
          type: user
        html: '<p>Does anyone know if it''s possible to fine-tune GPTQ 4bit quantized
          model?<br>I tried to do it via transformers.Trainer, but got malformed tensors
          with Infs or Nans...</p>

          <p>Got<br>RuntimeError: probability tensor contains either <code>inf</code>,
          <code>nan</code> or element &lt; 0</p>

          <p>tried with this script:</p>

          <p>train_path = ''train_dataset.txt''</p>

          <p>train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)</p>

          <p>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,<br>                                                mlm=False)</p>

          <p>from transformers import Trainer, TrainingArguments</p>

          <p>training_args = TrainingArguments(<br>    output_dir="./finetuned2",
          # The output directory<br>    overwrite_output_dir=True, # Overwrite the
          content of the output dir<br>    num_train_epochs=0.01, # number of training
          epochs<br>    per_device_train_batch_size=1, # batch size for training<br>    auto_find_batch_size=True,<br>    per_device_eval_batch_size=1,  #
          batch size for evaluation<br>    warmup_steps=10, # number of warmup steps
          for learning rate scheduler<br>    gradient_accumulation_steps=1, # to make
          "virtual" batch size larger<br>    )</p>

          <p>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    data_collator=data_collator,<br>    train_dataset=train_dataset,<br>    optimizers
          = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)<br>)<br>trainer.train()</p>

          '
        raw: "Does anyone know if it's possible to fine-tune GPTQ 4bit quantized model?\r\
          \nI tried to do it via transformers.Trainer, but got malformed tensors with\
          \ Infs or Nans...\r\n\r\nGot \r\nRuntimeError: probability tensor contains\
          \ either `inf`, `nan` or element < 0\r\n\r\n\r\n\r\ntried with this script:\r\
          \n\r\ntrain_path = 'train_dataset.txt'\r\n\r\ntrain_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\r\
          \n\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\r\
          \n                                                mlm=False)\r\n\r\nfrom\
          \ transformers import Trainer, TrainingArguments\r\n\r\ntraining_args =\
          \ TrainingArguments(\r\n    output_dir=\"./finetuned2\", # The output directory\r\
          \n    overwrite_output_dir=True, # Overwrite the content of the output dir\r\
          \n    num_train_epochs=0.01, # number of training epochs\r\n    per_device_train_batch_size=1,\
          \ # batch size for training\r\n    auto_find_batch_size=True,\r\n    per_device_eval_batch_size=1,\
          \  # batch size for evaluation\r\n    warmup_steps=10, # number of warmup\
          \ steps for learning rate scheduler\r\n    gradient_accumulation_steps=1,\
          \ # to make \"virtual\" batch size larger\r\n    )\r\n\r\ntrainer = Trainer(\r\
          \n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\
          \n    train_dataset=train_dataset,\r\n    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),\
          \ None)\r\n)\r\ntrainer.train()"
        updatedAt: '2023-08-03T13:32:34.706Z'
      numEdits: 0
      reactions: []
    id: 64cbac72942890af935afc7e
    type: comment
  author: HuggingDroidX
  content: "Does anyone know if it's possible to fine-tune GPTQ 4bit quantized model?\r\
    \nI tried to do it via transformers.Trainer, but got malformed tensors with Infs\
    \ or Nans...\r\n\r\nGot \r\nRuntimeError: probability tensor contains either `inf`,\
    \ `nan` or element < 0\r\n\r\n\r\n\r\ntried with this script:\r\n\r\ntrain_path\
    \ = 'train_dataset.txt'\r\n\r\ntrain_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\r\
    \n\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\r\n\
    \                                                mlm=False)\r\n\r\nfrom transformers\
    \ import Trainer, TrainingArguments\r\n\r\ntraining_args = TrainingArguments(\r\
    \n    output_dir=\"./finetuned2\", # The output directory\r\n    overwrite_output_dir=True,\
    \ # Overwrite the content of the output dir\r\n    num_train_epochs=0.01, # number\
    \ of training epochs\r\n    per_device_train_batch_size=1, # batch size for training\r\
    \n    auto_find_batch_size=True,\r\n    per_device_eval_batch_size=1,  # batch\
    \ size for evaluation\r\n    warmup_steps=10, # number of warmup steps for learning\
    \ rate scheduler\r\n    gradient_accumulation_steps=1, # to make \"virtual\" batch\
    \ size larger\r\n    )\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\
    \n    data_collator=data_collator,\r\n    train_dataset=train_dataset,\r\n   \
    \ optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\r\n)\r\n\
    trainer.train()"
  created_at: 2023-08-03 12:32:34+00:00
  edited: false
  hidden: false
  id: 64cbac72942890af935afc7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caf87a3933972657b4852d/YJ8XrH5IEfbL-yRMp4ScG.jpeg?w=200&h=200&f=face
      fullname: alexander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: femboysLover
      type: user
    createdAt: '2023-10-15T15:38:58.000Z'
    data:
      edited: false
      editors:
      - femboysLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.823118269443512
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caf87a3933972657b4852d/YJ8XrH5IEfbL-yRMp4ScG.jpeg?w=200&h=200&f=face
          fullname: alexander
          isHf: false
          isPro: false
          name: femboysLover
          type: user
        html: '<p>Hello! training with lora is possible, look at the peft documentation
          - <a rel="nofollow" href="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a></p>

          '
        raw: Hello! training with lora is possible, look at the peft documentation
          - https://github.com/huggingface/peft
        updatedAt: '2023-10-15T15:38:58.067Z'
      numEdits: 0
      reactions: []
    id: 652c07922ecb5062d6c5f0fc
    type: comment
  author: femboysLover
  content: Hello! training with lora is possible, look at the peft documentation -
    https://github.com/huggingface/peft
  created_at: 2023-10-15 14:38:58+00:00
  edited: false
  hidden: false
  id: 652c07922ecb5062d6c5f0fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: fffrrt/ruGPT-3.5-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Is fine-tuning of already quanted GPT real?
