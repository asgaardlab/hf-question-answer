!!python/object:huggingface_hub.community.DiscussionWithDetails
author: StephennFernandes
conflicting_files: null
created_at: 2023-07-12 09:39:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614429358033-noauth.jpeg?w=200&h=200&f=face
      fullname: 'Stephen Fernandes '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StephennFernandes
      type: user
    createdAt: '2023-07-12T10:39:31.000Z'
    data:
      edited: false
      editors:
      - StephennFernandes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497315287590027
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614429358033-noauth.jpeg?w=200&h=200&f=face
          fullname: 'Stephen Fernandes '
          isHf: false
          isPro: false
          name: StephennFernandes
          type: user
        html: '<p>Hi there, is there a way to let mms have timestamp decoding similar
          to openai whisper models ? </p>

          '
        raw: 'Hi there, is there a way to let mms have timestamp decoding similar
          to openai whisper models ? '
        updatedAt: '2023-07-12T10:39:31.470Z'
      numEdits: 0
      reactions: []
    id: 64ae82e36f3c7f2eaf247d0b
    type: comment
  author: StephennFernandes
  content: 'Hi there, is there a way to let mms have timestamp decoding similar to
    openai whisper models ? '
  created_at: 2023-07-12 09:39:31+00:00
  edited: false
  hidden: false
  id: 64ae82e36f3c7f2eaf247d0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-07-13T16:14:38.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5699779391288757
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Yep, easiest done with the <code>pipeline</code>. For character
          level timestamps:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> pipeline


          transcriber = pipeline(model=<span class="hljs-string">"facebook/mms-1b-all"</span>)

          transcriber(<span class="hljs-string">"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac"</span>,
          return_timestamps=<span class="hljs-string">"char"</span>)

          </code></pre>

          <p>For word-level timestamps:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> pipeline


          transcriber = pipeline(model=<span class="hljs-string">"facebook/mms-1b-all"</span>)

          transcriber(<span class="hljs-string">"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac"</span>,
          return_timestamps=<span class="hljs-string">"word"</span>)

          </code></pre>

          <p>See docs for more details: <a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline">https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline</a></p>

          '
        raw: 'Yep, easiest done with the `pipeline`. For character level timestamps:


          ```python

          from transformers import pipeline


          transcriber = pipeline(model="facebook/mms-1b-all")

          transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
          return_timestamps="char")

          ```


          For word-level timestamps:


          ```python

          from transformers import pipeline


          transcriber = pipeline(model="facebook/mms-1b-all")

          transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
          return_timestamps="word")

          ```


          See docs for more details: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline'
        updatedAt: '2023-07-13T16:14:38.153Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - allandclive
    id: 64b022eef9cb06c1dc5e837c
    type: comment
  author: sanchit-gandhi
  content: 'Yep, easiest done with the `pipeline`. For character level timestamps:


    ```python

    from transformers import pipeline


    transcriber = pipeline(model="facebook/mms-1b-all")

    transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    return_timestamps="char")

    ```


    For word-level timestamps:


    ```python

    from transformers import pipeline


    transcriber = pipeline(model="facebook/mms-1b-all")

    transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    return_timestamps="word")

    ```


    See docs for more details: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline'
  created_at: 2023-07-13 15:14:38+00:00
  edited: false
  hidden: false
  id: 64b022eef9cb06c1dc5e837c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614429358033-noauth.jpeg?w=200&h=200&f=face
      fullname: 'Stephen Fernandes '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StephennFernandes
      type: user
    createdAt: '2023-07-13T17:58:41.000Z'
    data:
      edited: false
      editors:
      - StephennFernandes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7867121696472168
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614429358033-noauth.jpeg?w=200&h=200&f=face
          fullname: 'Stephen Fernandes '
          isHf: false
          isPro: false
          name: StephennFernandes
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \ thanks a ton for taking the time to reply. </p>\n<p>could you please tell\
          \ me how i could do this in the regular inference mode as well</p>\n<p>eg:</p>\n\
          <pre><code>inputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"\
          pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\n\
          ids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n\
          </code></pre>\n"
        raw: "hey @sanchit-gandhi thanks a ton for taking the time to reply. \n\n\
          could you please tell me how i could do this in the regular inference mode\
          \ as well\n\neg:\n```\ninputs = processor(en_sample, sampling_rate=16_000,\
          \ return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\
          \nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n\
          ```"
        updatedAt: '2023-07-13T17:58:41.948Z'
      numEdits: 0
      reactions: []
    id: 64b03b51f582e1c54caf7bf7
    type: comment
  author: StephennFernandes
  content: "hey @sanchit-gandhi thanks a ton for taking the time to reply. \n\ncould\
    \ you please tell me how i could do this in the regular inference mode as well\n\
    \neg:\n```\ninputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"\
    pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n\nids =\
    \ torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n```"
  created_at: 2023-07-13 16:58:41+00:00
  edited: false
  hidden: false
  id: 64b03b51f582e1c54caf7bf7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: facebook/mms-1b-all
repo_type: model
status: open
target_branch: null
title: 'timestamp decoding '
