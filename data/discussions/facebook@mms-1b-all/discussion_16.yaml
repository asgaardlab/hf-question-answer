!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jungwonchang
conflicting_files: null
created_at: 2023-12-16 17:31:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d8f9753b52217393271952dfea4e020.svg
      fullname: Jungwon Chang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jungwonchang
      type: user
    createdAt: '2023-12-16T17:31:56.000Z'
    data:
      edited: false
      editors:
      - Jungwonchang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46602699160575867
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d8f9753b52217393271952dfea4e020.svg
          fullname: Jungwon Chang
          isHf: false
          isPro: false
          name: Jungwonchang
          type: user
        html: "<p>Firstly, I would like to appreciate Meta AI for amazing contribution\
          \ to the field.</p>\n<p>However, I have found that there seems to be a slight\
          \ discrepancy on model paramters.<br>This table is from the paper of MMS</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6232fdee38869c4ca8fd49e2/SVjymVBWBL-H6NTL-j-7O.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6232fdee38869c4ca8fd49e2/SVjymVBWBL-H6NTL-j-7O.png\"\
          ></a></p>\n<p>It is stated that the number of hidden states is 1024,<br>but\
          \ when I actually inspected the model, it was 1280.</p>\n<pre><code>Wav2Vec2ForCTC(\n\
          \  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n\
          \      (conv_layers): ModuleList(\n        (0): Wav2Vec2LayerNormConvLayer(\n\
          \          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n    \
          \      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n\
          \          (activation): GELUActivation()\n        )\n        (1-4): 4 x\
          \ Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,),\
          \ stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n\
          \          (activation): GELUActivation()\n        )\n        (5-6): 2 x\
          \ Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,),\
          \ stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n\
          \          (activation): GELUActivation()\n        )\n      )\n    )\n \
          \   (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm):\
          \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection):\
          \ Linear(in_features=512, out_features=1280, bias=True)\n      (dropout):\
          \ Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\n\
          \      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv):\
          \ ParametrizedConv1d(\n          1280, 1280, kernel_size=(128,), stride=(1,),\
          \ padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n\
          \            (weight): ParametrizationList(\n              (0): _WeightNorm()\n\
          \            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n\
          \        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((1280,),\
          \ eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n\
          \      (layers): ModuleList(\n        (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n\
          \          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280,\
          \ out_features=1280, bias=True)\n          )\n          (dropout): Dropout(p=0.0,\
          \ inplace=False)\n          (layer_norm): LayerNorm((1280,), eps=1e-05,\
          \ elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n\
          \            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n  \
          \          (intermediate_dense): Linear(in_features=1280, out_features=5120,\
          \ bias=True)\n            (intermediate_act_fn): GELUActivation()\n    \
          \        (output_dense): Linear(in_features=5120, out_features=1280, bias=True)\n\
          \            (output_dropout): Dropout(p=0.0, inplace=False)\n         \
          \ )\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n\
          \          (adapter_layer): Wav2Vec2AttnAdapterLayer(\n            (norm):\
          \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (linear_1):\
          \ Linear(in_features=1280, out_features=16, bias=True)\n            (act_fn):\
          \ ReLU()\n            (linear_2): Linear(in_features=16, out_features=1280,\
          \ bias=True)\n          )\n        )\n      )\n    )\n  )\n  (dropout):\
          \ Dropout(p=0.05, inplace=False)\n  (lm_head): Linear(in_features=1280,\
          \ out_features=73, bias=True)\n)\n</code></pre>\n"
        raw: "Firstly, I would like to appreciate Meta AI for amazing contribution\
          \ to the field.\r\n\r\nHowever, I have found that there seems to be a slight\
          \ discrepancy on model paramters.\r\nThis table is from the paper of MMS\r\
          \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6232fdee38869c4ca8fd49e2/SVjymVBWBL-H6NTL-j-7O.png)\r\
          \n\r\nIt is stated that the number of hidden states is 1024,\r\nbut when\
          \ I actually inspected the model, it was 1280.\r\n\r\n```\r\nWav2Vec2ForCTC(\r\
          \n  (wav2vec2): Wav2Vec2Model(\r\n    (feature_extractor): Wav2Vec2FeatureEncoder(\r\
          \n      (conv_layers): ModuleList(\r\n        (0): Wav2Vec2LayerNormConvLayer(\r\
          \n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\r\n \
          \         (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\
          \n          (activation): GELUActivation()\r\n        )\r\n        (1-4):\
          \ 4 x Wav2Vec2LayerNormConvLayer(\r\n          (conv): Conv1d(512, 512,\
          \ kernel_size=(3,), stride=(2,))\r\n          (layer_norm): LayerNorm((512,),\
          \ eps=1e-05, elementwise_affine=True)\r\n          (activation): GELUActivation()\r\
          \n        )\r\n        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\r\n      \
          \    (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\r\n       \
          \   (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\
          \n          (activation): GELUActivation()\r\n        )\r\n      )\r\n \
          \   )\r\n    (feature_projection): Wav2Vec2FeatureProjection(\r\n      (layer_norm):\
          \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      (projection):\
          \ Linear(in_features=512, out_features=1280, bias=True)\r\n      (dropout):\
          \ Dropout(p=0.0, inplace=False)\r\n    )\r\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\r\
          \n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\r\n        (conv):\
          \ ParametrizedConv1d(\r\n          1280, 1280, kernel_size=(128,), stride=(1,),\
          \ padding=(64,), groups=16\r\n          (parametrizations): ModuleDict(\r\
          \n            (weight): ParametrizationList(\r\n              (0): _WeightNorm()\r\
          \n            )\r\n          )\r\n        )\r\n        (padding): Wav2Vec2SamePadLayer()\r\
          \n        (activation): GELUActivation()\r\n      )\r\n      (layer_norm):\
          \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\n      (dropout):\
          \ Dropout(p=0.0, inplace=False)\r\n      (layers): ModuleList(\r\n     \
          \   (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\r\n          (attention):\
          \ Wav2Vec2Attention(\r\n            (k_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\r\n            (v_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\r\n            (q_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\r\n            (out_proj): Linear(in_features=1280, out_features=1280,\
          \ bias=True)\r\n          )\r\n          (dropout): Dropout(p=0.0, inplace=False)\r\
          \n          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\
          \n          (feed_forward): Wav2Vec2FeedForward(\r\n            (intermediate_dropout):\
          \ Dropout(p=0.05, inplace=False)\r\n            (intermediate_dense): Linear(in_features=1280,\
          \ out_features=5120, bias=True)\r\n            (intermediate_act_fn): GELUActivation()\r\
          \n            (output_dense): Linear(in_features=5120, out_features=1280,\
          \ bias=True)\r\n            (output_dropout): Dropout(p=0.0, inplace=False)\r\
          \n          )\r\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05,\
          \ elementwise_affine=True)\r\n          (adapter_layer): Wav2Vec2AttnAdapterLayer(\r\
          \n            (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\
          \n            (linear_1): Linear(in_features=1280, out_features=16, bias=True)\r\
          \n            (act_fn): ReLU()\r\n            (linear_2): Linear(in_features=16,\
          \ out_features=1280, bias=True)\r\n          )\r\n        )\r\n      )\r\
          \n    )\r\n  )\r\n  (dropout): Dropout(p=0.05, inplace=False)\r\n  (lm_head):\
          \ Linear(in_features=1280, out_features=73, bias=True)\r\n)\r\n```"
        updatedAt: '2023-12-16T17:31:56.843Z'
      numEdits: 0
      reactions: []
    id: 657ddf0cc7a58172f5eccbf4
    type: comment
  author: Jungwonchang
  content: "Firstly, I would like to appreciate Meta AI for amazing contribution to\
    \ the field.\r\n\r\nHowever, I have found that there seems to be a slight discrepancy\
    \ on model paramters.\r\nThis table is from the paper of MMS\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6232fdee38869c4ca8fd49e2/SVjymVBWBL-H6NTL-j-7O.png)\r\
    \n\r\nIt is stated that the number of hidden states is 1024,\r\nbut when I actually\
    \ inspected the model, it was 1280.\r\n\r\n```\r\nWav2Vec2ForCTC(\r\n  (wav2vec2):\
    \ Wav2Vec2Model(\r\n    (feature_extractor): Wav2Vec2FeatureEncoder(\r\n     \
    \ (conv_layers): ModuleList(\r\n        (0): Wav2Vec2LayerNormConvLayer(\r\n \
    \         (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\r\n         \
    \ (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n    \
    \      (activation): GELUActivation()\r\n        )\r\n        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\r\
    \n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\r\n      \
    \    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n \
    \         (activation): GELUActivation()\r\n        )\r\n        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\r\
    \n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\r\n      \
    \    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n \
    \         (activation): GELUActivation()\r\n        )\r\n      )\r\n    )\r\n\
    \    (feature_projection): Wav2Vec2FeatureProjection(\r\n      (layer_norm): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\r\n      (projection): Linear(in_features=512,\
    \ out_features=1280, bias=True)\r\n      (dropout): Dropout(p=0.0, inplace=False)\r\
    \n    )\r\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\r\n      (pos_conv_embed):\
    \ Wav2Vec2PositionalConvEmbedding(\r\n        (conv): ParametrizedConv1d(\r\n\
    \          1280, 1280, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\r\
    \n          (parametrizations): ModuleDict(\r\n            (weight): ParametrizationList(\r\
    \n              (0): _WeightNorm()\r\n            )\r\n          )\r\n       \
    \ )\r\n        (padding): Wav2Vec2SamePadLayer()\r\n        (activation): GELUActivation()\r\
    \n      )\r\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\
    \n      (dropout): Dropout(p=0.0, inplace=False)\r\n      (layers): ModuleList(\r\
    \n        (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\r\n          (attention):\
    \ Wav2Vec2Attention(\r\n            (k_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\r\n            (v_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\r\n            (q_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\r\n            (out_proj): Linear(in_features=1280, out_features=1280,\
    \ bias=True)\r\n          )\r\n          (dropout): Dropout(p=0.0, inplace=False)\r\
    \n          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\
    \n          (feed_forward): Wav2Vec2FeedForward(\r\n            (intermediate_dropout):\
    \ Dropout(p=0.05, inplace=False)\r\n            (intermediate_dense): Linear(in_features=1280,\
    \ out_features=5120, bias=True)\r\n            (intermediate_act_fn): GELUActivation()\r\
    \n            (output_dense): Linear(in_features=5120, out_features=1280, bias=True)\r\
    \n            (output_dropout): Dropout(p=0.0, inplace=False)\r\n          )\r\
    \n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\
    \n          (adapter_layer): Wav2Vec2AttnAdapterLayer(\r\n            (norm):\
    \ LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\r\n            (linear_1):\
    \ Linear(in_features=1280, out_features=16, bias=True)\r\n            (act_fn):\
    \ ReLU()\r\n            (linear_2): Linear(in_features=16, out_features=1280,\
    \ bias=True)\r\n          )\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (dropout):\
    \ Dropout(p=0.05, inplace=False)\r\n  (lm_head): Linear(in_features=1280, out_features=73,\
    \ bias=True)\r\n)\r\n```"
  created_at: 2023-12-16 17:31:56+00:00
  edited: false
  hidden: false
  id: 657ddf0cc7a58172f5eccbf4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d8f9753b52217393271952dfea4e020.svg
      fullname: Jungwon Chang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jungwonchang
      type: user
    createdAt: '2023-12-18T08:03:39.000Z'
    data:
      edited: false
      editors:
      - Jungwonchang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.963609516620636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d8f9753b52217393271952dfea4e020.svg
          fullname: Jungwon Chang
          isHf: false
          isPro: false
          name: Jungwonchang
          type: user
        html: '<p>also, isn''t the proportion of the additional parameters by adapter
          modules about 0.2% rather than 2%, since the total number of the model is
          about 1billion, which is about 1000M.</p>

          '
        raw: also, isn't the proportion of the additional parameters by adapter modules
          about 0.2% rather than 2%, since the total number of the model is about
          1billion, which is about 1000M.
        updatedAt: '2023-12-18T08:03:39.826Z'
      numEdits: 0
      reactions: []
    id: 657ffcdb83543a061b9dafe8
    type: comment
  author: Jungwonchang
  content: also, isn't the proportion of the additional parameters by adapter modules
    about 0.2% rather than 2%, since the total number of the model is about 1billion,
    which is about 1000M.
  created_at: 2023-12-18 08:03:39+00:00
  edited: false
  hidden: false
  id: 657ffcdb83543a061b9dafe8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: facebook/mms-1b-all
repo_type: model
status: open
target_branch: null
title: model specification discrepancy with paper
