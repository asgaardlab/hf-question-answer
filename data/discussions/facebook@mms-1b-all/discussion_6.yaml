!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bigliam
conflicting_files: null
created_at: 2023-06-20 05:21:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oSRLD1aeEYK_0xSbcKtaa.jpeg?w=200&h=200&f=face
      fullname: liam chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bigliam
      type: user
    createdAt: '2023-06-20T06:21:33.000Z'
    data:
      edited: false
      editors:
      - bigliam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7427977919578552
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oSRLD1aeEYK_0xSbcKtaa.jpeg?w=200&h=200&f=face
          fullname: liam chen
          isHf: false
          isPro: false
          name: bigliam
          type: user
        html: '<p>I have two RTX A6000(40GB) GPUs, which is supposed to be able to
          load this model and run the example shown in README. But I got the following
          error. Could anyone tell me how to adapt the code to solve the problem?
          Thx~<br>''''''<br>CUDA out of memory. Tried to allocate 79.71 GiB (GPU 0;
          44.56 GiB total capacity; 7.94 GiB already allocated; 15.64 GiB free; 12.80
          GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>''''''</p>

          <p>BTW, when I set the parameter <code>device_map =''auto''</code>  in <code>Wav2Vec2ForCTC.from_pretrained(model_id,
          device_map =''auto'')</code> and I got the following error:<br>''''''<br>Could
          not run ''aten::_weight_norm_interface'' with arguments from the ''Meta''
          backend. This could be because the operator doesn''t exist for this backend,
          or was omitted during the selective/custom build process (if using custom
          build). If you are a Facebook employee using PyTorch on mobile, please visit
          <a rel="nofollow" href="https://fburl.com/ptmfixes">https://fburl.com/ptmfixes</a>
          for possible resolutions. ''aten::_weight_norm_interface'' is only available
          for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode,
          Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView,
          AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS,
          AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta,
          AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3,
          AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched,
          FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot,
          FuncTorchDynamicLayerFrontMode, PythonDispatcher]<br>''''''</p>

          <p>Another doubt, the model''s size is less than 4GB, why 79GB GPU memory
          is needed during inference?</p>

          '
        raw: "I have two RTX A6000(40GB) GPUs, which is supposed to be able to load\
          \ this model and run the example shown in README. But I got the following\
          \ error. Could anyone tell me how to adapt the code to solve the problem?\
          \ Thx~\r\n'''\r\nCUDA out of memory. Tried to allocate 79.71 GiB (GPU 0;\
          \ 44.56 GiB total capacity; 7.94 GiB already allocated; 15.64 GiB free;\
          \ 12.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
          \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n'''\r\n\r\nBTW, when\
          \ I set the parameter `device_map ='auto'`  in `Wav2Vec2ForCTC.from_pretrained(model_id,\
          \ device_map ='auto')` and I got the following error:\r\n'''\r\nCould not\
          \ run 'aten::_weight_norm_interface' with arguments from the 'Meta' backend.\
          \ This could be because the operator doesn't exist for this backend, or\
          \ was omitted during the selective/custom build process (if using custom\
          \ build). If you are a Facebook employee using PyTorch on mobile, please\
          \ visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_weight_norm_interface'\
          \ is only available for these backends: [CPU, CUDA, BackendSelect, Python,\
          \ FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative,\
          \ ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA,\
          \ AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU,\
          \ AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1,\
          \ AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer,\
          \ AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched,\
          \ VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode,\
          \ PythonDispatcher]\r\n'''\r\n\r\nAnother doubt, the model's size is less\
          \ than 4GB, why 79GB GPU memory is needed during inference?"
        updatedAt: '2023-06-20T06:21:33.123Z'
      numEdits: 0
      reactions: []
    id: 6491456d24d9bc9bb8000b60
    type: comment
  author: bigliam
  content: "I have two RTX A6000(40GB) GPUs, which is supposed to be able to load\
    \ this model and run the example shown in README. But I got the following error.\
    \ Could anyone tell me how to adapt the code to solve the problem? Thx~\r\n'''\r\
    \nCUDA out of memory. Tried to allocate 79.71 GiB (GPU 0; 44.56 GiB total capacity;\
    \ 7.94 GiB already allocated; 15.64 GiB free; 12.80 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n'''\r\n\r\nBTW, when I set the parameter `device_map ='auto'`  in `Wav2Vec2ForCTC.from_pretrained(model_id,\
    \ device_map ='auto')` and I got the following error:\r\n'''\r\nCould not run\
    \ 'aten::_weight_norm_interface' with arguments from the 'Meta' backend. This\
    \ could be because the operator doesn't exist for this backend, or was omitted\
    \ during the selective/custom build process (if using custom build). If you are\
    \ a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes\
    \ for possible resolutions. 'aten::_weight_norm_interface' is only available for\
    \ these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode,\
    \ Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther,\
    \ AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU,\
    \ AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA,\
    \ AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor,\
    \ Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched,\
    \ VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode,\
    \ PythonDispatcher]\r\n'''\r\n\r\nAnother doubt, the model's size is less than\
    \ 4GB, why 79GB GPU memory is needed during inference?"
  created_at: 2023-06-20 05:21:33+00:00
  edited: false
  hidden: false
  id: 6491456d24d9bc9bb8000b60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-06-21T16:19:56.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8694984912872314
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;bigliam&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bigliam\"\
          >@<span class=\"underline\">bigliam</span></a></span>\n\n\t</span></span>,</p>\n\
          <p><code>device_map='auto'</code> currently doesn't work because MMS/Wav2Vec2\
          \ use are rather weird weight_norm implementation from PyTorch: <a rel=\"\
          nofollow\" href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html\"\
          >https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html</a><br>Your\
          \ best shot here is to manually place the model on two GPU devices</p>\n"
        raw: 'Hey @bigliam,


          `device_map=''auto''` currently doesn''t work because MMS/Wav2Vec2 use are
          rather weird weight_norm implementation from PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html

          Your best shot here is to manually place the model on two GPU devices'
        updatedAt: '2023-06-21T16:19:56.453Z'
      numEdits: 0
      reactions: []
    id: 6493232c5423a79bfce1a6c6
    type: comment
  author: patrickvonplaten
  content: 'Hey @bigliam,


    `device_map=''auto''` currently doesn''t work because MMS/Wav2Vec2 use are rather
    weird weight_norm implementation from PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html

    Your best shot here is to manually place the model on two GPU devices'
  created_at: 2023-06-21 15:19:56+00:00
  edited: false
  hidden: false
  id: 6493232c5423a79bfce1a6c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oSRLD1aeEYK_0xSbcKtaa.jpeg?w=200&h=200&f=face
      fullname: liam chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bigliam
      type: user
    createdAt: '2023-06-25T12:35:38.000Z'
    data:
      edited: false
      editors:
      - bigliam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8971595168113708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oSRLD1aeEYK_0xSbcKtaa.jpeg?w=200&h=200&f=face
          fullname: liam chen
          isHf: false
          isPro: false
          name: bigliam
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\"\
          >@<span class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span>\
          \ ,<br>Thanks for advice, I'll find it out.</p>\n"
        raw: 'Hey @patrickvonplaten ,

          Thanks for advice, I''ll find it out.'
        updatedAt: '2023-06-25T12:35:38.427Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6498349a11a0aac4e0bc1135
    id: 6498349a11a0aac4e0bc1130
    type: comment
  author: bigliam
  content: 'Hey @patrickvonplaten ,

    Thanks for advice, I''ll find it out.'
  created_at: 2023-06-25 11:35:38+00:00
  edited: false
  hidden: false
  id: 6498349a11a0aac4e0bc1130
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oSRLD1aeEYK_0xSbcKtaa.jpeg?w=200&h=200&f=face
      fullname: liam chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bigliam
      type: user
    createdAt: '2023-06-25T12:35:38.000Z'
    data:
      status: closed
    id: 6498349a11a0aac4e0bc1135
    type: status-change
  author: bigliam
  created_at: 2023-06-25 11:35:38+00:00
  id: 6498349a11a0aac4e0bc1135
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: facebook/mms-1b-all
repo_type: model
status: closed
target_branch: null
title: How to run examples on two gpus of RTX A6000(40GB) successfully?
