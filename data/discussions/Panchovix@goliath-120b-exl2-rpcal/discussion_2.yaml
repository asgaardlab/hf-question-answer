!!python/object:huggingface_hub.community.DiscussionWithDetails
author: practical-dreamer
conflicting_files: null
created_at: 2023-11-12 23:25:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642e4fbda0b65dce1f875e90/o08Ni4hSRiqyoBy6MEg-a.jpeg?w=200&h=200&f=face
      fullname: practical-dreamer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: practical-dreamer
      type: user
    createdAt: '2023-11-12T23:25:30.000Z'
    data:
      edited: false
      editors:
      - practical-dreamer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9061599373817444
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642e4fbda0b65dce1f875e90/o08Ni4hSRiqyoBy6MEg-a.jpeg?w=200&h=200&f=face
          fullname: practical-dreamer
          isHf: false
          isPro: false
          name: practical-dreamer
          type: user
        html: "<p>Pancho, any way these 120B models would run on the dual 3090 4090\
          \ setups?</p>\n<p>Even at 3bit there\u2019s just no way right?\u2026</p>\n\
          <p>Would llamacpp+ with gpu VRAM offload even be an option or would performance\
          \ suck so hard not with it?</p>\n<p>Just wonderin</p>\n<p>-Generic Username</p>\n"
        raw: "Pancho, any way these 120B models would run on the dual 3090 4090 setups?\r\
          \n\r\nEven at 3bit there\u2019s just no way right?\u2026\r\n\r\nWould llamacpp+\
          \ with gpu VRAM offload even be an option or would performance suck so hard\
          \ not with it?\r\n\r\nJust wonderin\r\n\r\n-Generic Username"
        updatedAt: '2023-11-12T23:25:30.148Z'
      numEdits: 0
      reactions: []
    id: 65515eea43baee6b4d26544b
    type: comment
  author: practical-dreamer
  content: "Pancho, any way these 120B models would run on the dual 3090 4090 setups?\r\
    \n\r\nEven at 3bit there\u2019s just no way right?\u2026\r\n\r\nWould llamacpp+\
    \ with gpu VRAM offload even be an option or would performance suck so hard not\
    \ with it?\r\n\r\nJust wonderin\r\n\r\n-Generic Username"
  created_at: 2023-11-12 23:25:30+00:00
  edited: false
  hidden: false
  id: 65515eea43baee6b4d26544b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
      fullname: minipasila
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpasila
      type: user
    createdAt: '2023-11-13T04:40:35.000Z'
    data:
      edited: true
      editors:
      - mpasila
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602805376052856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e5ccbc0dac5c1e16bdddd489802d363.svg
          fullname: minipasila
          isHf: false
          isPro: false
          name: mpasila
          type: user
        html: '<p>3bpw will need about 48gb if you run it with 8bit cache, it''s just
          enough memory to work. Using it with GGUF is pretty slow so I wouldn''t
          recommend that.</p>

          '
        raw: 3bpw will need about 48gb if you run it with 8bit cache, it's just enough
          memory to work. Using it with GGUF is pretty slow so I wouldn't recommend
          that.
        updatedAt: '2023-11-13T04:41:32.564Z'
      numEdits: 1
      reactions: []
    id: 6551a8c3286b72eb7c835a4b
    type: comment
  author: mpasila
  content: 3bpw will need about 48gb if you run it with 8bit cache, it's just enough
    memory to work. Using it with GGUF is pretty slow so I wouldn't recommend that.
  created_at: 2023-11-13 04:40:35+00:00
  edited: true
  hidden: false
  id: 6551a8c3286b72eb7c835a4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-13T05:47:03.000Z'
    data:
      edited: true
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8887439370155334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: "<p>Hi there! Long time!</p>\n<p>As <span data-props=\"{&quot;user&quot;:&quot;mpasila&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mpasila\"\
          >@<span class=\"underline\">mpasila</span></a></span>\n\n\t</span></span>\
          \ said, it will work with 48GB VRAM. It can work with full fp16 cache with\
          \ 3k context, or 8bit cache with, maybe, 4096 context (I haven't done enough\
          \ tests on 2x48 GB), but 3bpw was made with 48GB VRAM in my mind.</p>\n"
        raw: 'Hi there! Long time!


          As @mpasila said, it will work with 48GB VRAM. It can work with full fp16
          cache with 3k context, or 8bit cache with, maybe, 4096 context (I haven''t
          done enough tests on 2x48 GB), but 3bpw was made with 48GB VRAM in my mind.'
        updatedAt: '2023-11-13T05:54:36.754Z'
      numEdits: 1
      reactions: []
    id: 6551b857eac8927818aaa0bf
    type: comment
  author: Panchovix
  content: 'Hi there! Long time!


    As @mpasila said, it will work with 48GB VRAM. It can work with full fp16 cache
    with 3k context, or 8bit cache with, maybe, 4096 context (I haven''t done enough
    tests on 2x48 GB), but 3bpw was made with 48GB VRAM in my mind.'
  created_at: 2023-11-13 05:47:03+00:00
  edited: true
  hidden: false
  id: 6551b857eac8927818aaa0bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-19T05:38:44.000Z'
    data:
      status: closed
    id: 65599f64180749bf32e0d34d
    type: status-change
  author: Panchovix
  created_at: 2023-11-19 05:38:44+00:00
  id: 65599f64180749bf32e0d34d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Panchovix/goliath-120b-exl2-rpcal
repo_type: model
status: closed
target_branch: null
title: VRAM requirements
