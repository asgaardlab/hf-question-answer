!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akoyaki
conflicting_files: null
created_at: 2023-12-23 05:30:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-12-23T05:30:59.000Z'
    data:
      edited: true
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.851817786693573
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<p>Hi, I''ve been waiting for a long time to try the new quantized
          version of exl2-2(The presentation on reddit says that 2.4bpw is better
          than 4.5bpw/Q4KS) , and decided to try it on my own not long ago, but the
          same 3bpw+pippa calibration dataset test is very confusing (3.8 for your
          4.5bpw, 8.17 for the 3bpw, and 13.4 for my 3bpw quantized results)<br>I
          used "python convert.py -i /path -o /out_path -c pippa_raw_fix.parquet -b
          3 -hb 6"<br>Can you share your quantization command please? Do I need any
          other parameters to improve the quality?</p>

          '
        raw: 'Hi, I''ve been waiting for a long time to try the new quantized version
          of exl2-2(The presentation on reddit says that 2.4bpw is better than 4.5bpw/Q4KS)
          , and decided to try it on my own not long ago, but the same 3bpw+pippa
          calibration dataset test is very confusing (3.8 for your 4.5bpw, 8.17 for
          the 3bpw, and 13.4 for my 3bpw quantized results)

          I used "python convert.py -i /path -o /out_path -c pippa_raw_fix.parquet
          -b 3 -hb 6"

          Can you share your quantization command please? Do I need any other parameters
          to improve the quality?'
        updatedAt: '2023-12-23T05:37:25.108Z'
      numEdits: 2
      reactions: []
    id: 6586709315b65eb9ba9f2f84
    type: comment
  author: akoyaki
  content: 'Hi, I''ve been waiting for a long time to try the new quantized version
    of exl2-2(The presentation on reddit says that 2.4bpw is better than 4.5bpw/Q4KS)
    , and decided to try it on my own not long ago, but the same 3bpw+pippa calibration
    dataset test is very confusing (3.8 for your 4.5bpw, 8.17 for the 3bpw, and 13.4
    for my 3bpw quantized results)

    I used "python convert.py -i /path -o /out_path -c pippa_raw_fix.parquet -b 3
    -hb 6"

    Can you share your quantization command please? Do I need any other parameters
    to improve the quality?'
  created_at: 2023-12-23 05:30:59+00:00
  edited: true
  hidden: false
  id: 6586709315b65eb9ba9f2f84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-12-23T06:30:42.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7654668092727661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I use a pretty similar command, but I just don''t specify -hb (are
          you going for headsize in this one?</p>

          <p><code>py .\convert.py -i ''Goliath-120B-path'' -o ''Goliath-120B-3bpw-path''
          -c pippa_raw_fix.parquet -b 3 </code></p>

          '
        raw: 'I use a pretty similar command, but I just don''t specify -hb (are you
          going for headsize in this one?


          `py .\convert.py -i ''Goliath-120B-path'' -o ''Goliath-120B-3bpw-path''
          -c pippa_raw_fix.parquet -b 3 `'
        updatedAt: '2023-12-23T06:30:42.560Z'
      numEdits: 0
      reactions: []
    id: 65867e92f8b453e1f51cf70b
    type: comment
  author: Panchovix
  content: 'I use a pretty similar command, but I just don''t specify -hb (are you
    going for headsize in this one?


    `py .\convert.py -i ''Goliath-120B-path'' -o ''Goliath-120B-3bpw-path'' -c pippa_raw_fix.parquet
    -b 3 `'
  created_at: 2023-12-23 06:30:42+00:00
  edited: false
  hidden: false
  id: 65867e92f8b453e1f51cf70b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-12-23T06:59:09.000Z'
    data:
      edited: false
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168978929519653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<p>Well I try it twice with and with out -hb (default is 6 if not set)<br>-hb
          3 give 13.8<br>-hb 6 give 13.4<br>Maybe just because new quant metlod not
          good for big model like 100~120B?<br>I''ll try it again with older version
          exllamav2 to use old metlod to figure out<br>Thanks for the share</p>

          '
        raw: "Well I try it twice with and with out -hb (default is 6 if not set)\n\
          -hb 3 give 13.8\n-hb 6 give 13.4 \nMaybe just because new quant metlod not\
          \ good for big model like 100~120B?\nI'll try it again with older version\
          \ exllamav2 to use old metlod to figure out\nThanks for the share"
        updatedAt: '2023-12-23T06:59:09.126Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Panchovix
    id: 6586853da66bd1cdb2319572
    type: comment
  author: akoyaki
  content: "Well I try it twice with and with out -hb (default is 6 if not set)\n\
    -hb 3 give 13.8\n-hb 6 give 13.4 \nMaybe just because new quant metlod not good\
    \ for big model like 100~120B?\nI'll try it again with older version exllamav2\
    \ to use old metlod to figure out\nThanks for the share"
  created_at: 2023-12-23 06:59:09+00:00
  edited: false
  hidden: false
  id: 6586853da66bd1cdb2319572
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-12-23T12:16:41.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-24T19:49:18.050Z'
      numEdits: 0
      reactions: []
    id: 6586cfa9dda02636b0a4da6c
    type: comment
  author: akoyaki
  content: This comment has been hidden
  created_at: 2023-12-23 12:16:41+00:00
  edited: true
  hidden: true
  id: 6586cfa9dda02636b0a4da6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-12-24T19:48:30.000Z'
    data:
      edited: false
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7961373329162598
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<p>Well, it looks like 3bpw rpcal it''s just simply randomly crash
          with no reason<br>2.9bpw exl2-2 rpcal 5.13<br>2.4bpw exl2-2 5.96<br>2.4bpw
          exl2-2 rpcal 6.68<br>3bpw exl2-2 rpcal 13.8<br>I''m getting 11~15 t/s with
          2.9bpw+sd on 2x3090, feels good</p>

          '
        raw: 'Well, it looks like 3bpw rpcal it''s just simply randomly crash with
          no reason

          2.9bpw exl2-2 rpcal 5.13

          2.4bpw exl2-2 5.96

          2.4bpw exl2-2 rpcal 6.68

          3bpw exl2-2 rpcal 13.8

          I''m getting 11~15 t/s with 2.9bpw+sd on 2x3090, feels good'
        updatedAt: '2023-12-24T19:48:30.820Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Panchovix
    id: 65888b0e1b44d0e694263b4e
    type: comment
  author: akoyaki
  content: 'Well, it looks like 3bpw rpcal it''s just simply randomly crash with no
    reason

    2.9bpw exl2-2 rpcal 5.13

    2.4bpw exl2-2 5.96

    2.4bpw exl2-2 rpcal 6.68

    3bpw exl2-2 rpcal 13.8

    I''m getting 11~15 t/s with 2.9bpw+sd on 2x3090, feels good'
  created_at: 2023-12-24 19:48:30+00:00
  edited: false
  hidden: false
  id: 65888b0e1b44d0e694263b4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-12-24T22:40:26.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9866945147514343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Pretty interesting results, but as you keep testing you will notice
          PPL is not everything, but that you like the model itself.</p>

          <p>Glad quanting worked for you.</p>

          '
        raw: 'Pretty interesting results, but as you keep testing you will notice
          PPL is not everything, but that you like the model itself.


          Glad quanting worked for you.'
        updatedAt: '2023-12-24T22:40:26.609Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6588b35a92c8cc471f619475
    id: 6588b35a92c8cc471f619474
    type: comment
  author: Panchovix
  content: 'Pretty interesting results, but as you keep testing you will notice PPL
    is not everything, but that you like the model itself.


    Glad quanting worked for you.'
  created_at: 2023-12-24 22:40:26+00:00
  edited: false
  hidden: false
  id: 6588b35a92c8cc471f619474
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-12-24T22:40:26.000Z'
    data:
      status: closed
    id: 6588b35a92c8cc471f619475
    type: status-change
  author: Panchovix
  created_at: 2023-12-24 22:40:26+00:00
  id: 6588b35a92c8cc471f619475
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-12-25T04:42:35.000Z'
    data:
      edited: false
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802117943763733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<p>haha I know that, I''m not try to get best score, just want to know
          why the scores is diffirent<br>And yes I''m very agree with you, PPL is
          not everything, and benchmark also, I like use new model few hours to feel
          it but not test the score</p>

          '
        raw: 'haha I know that, I''m not try to get best score, just want to know
          why the scores is diffirent

          And yes I''m very agree with you, PPL is not everything, and benchmark also,
          I like use new model few hours to feel it but not test the score'
        updatedAt: '2023-12-25T04:42:35.107Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Panchovix
    id: 6589083b067630f69f6a6aa8
    type: comment
  author: akoyaki
  content: 'haha I know that, I''m not try to get best score, just want to know why
    the scores is diffirent

    And yes I''m very agree with you, PPL is not everything, and benchmark also, I
    like use new model few hours to feel it but not test the score'
  created_at: 2023-12-25 04:42:35+00:00
  edited: false
  hidden: false
  id: 6589083b067630f69f6a6aa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2024-01-06T17:43:17.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992127001285553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I''ve uploaded new quants in any case, if you want to try. I also
          suggest if you do, to backup your existing quants.</p>

          '
        raw: I've uploaded new quants in any case, if you want to try. I also suggest
          if you do, to backup your existing quants.
        updatedAt: '2024-01-06T17:43:17.657Z'
      numEdits: 0
      reactions: []
    id: 659991350ae723e79c858152
    type: comment
  author: Panchovix
  content: I've uploaded new quants in any case, if you want to try. I also suggest
    if you do, to backup your existing quants.
  created_at: 2024-01-06 17:43:17+00:00
  edited: false
  hidden: false
  id: 659991350ae723e79c858152
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Panchovix/goliath-120b-exl2-rpcal
repo_type: model
status: closed
target_branch: null
title: Quantitative command?
