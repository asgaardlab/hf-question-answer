!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AS1200
conflicting_files: null
created_at: 2024-01-06 16:37:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/062c16dfafe7f1d7371454934bf91527.svg
      fullname: SA2100
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AS1200
      type: user
    createdAt: '2024-01-06T16:37:52.000Z'
    data:
      edited: false
      editors:
      - AS1200
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.969817578792572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/062c16dfafe7f1d7371454934bf91527.svg
          fullname: SA2100
          isHf: false
          isPro: false
          name: AS1200
          type: user
        html: '<p>This only works with GPTQ and EX.<br>For this reason, two and a
          half people from the entire community can take advantage of this, and most
          likely this is not a joke.<br>Is there a chance that you will make a version
          for GGUF? In this case, a lot of people will be able to use it, at least
          me and my friend who discussed this Lora.</p>

          '
        raw: "This only works with GPTQ and EX.\r\nFor this reason, two and a half\
          \ people from the entire community can take advantage of this, and most\
          \ likely this is not a joke.\r\nIs there a chance that you will make a version\
          \ for GGUF? In this case, a lot of people will be able to use it, at least\
          \ me and my friend who discussed this Lora."
        updatedAt: '2024-01-06T16:37:52.451Z'
      numEdits: 0
      reactions: []
    id: 659981e0654fe4eb0aaf2847
    type: comment
  author: AS1200
  content: "This only works with GPTQ and EX.\r\nFor this reason, two and a half people\
    \ from the entire community can take advantage of this, and most likely this is\
    \ not a joke.\r\nIs there a chance that you will make a version for GGUF? In this\
    \ case, a lot of people will be able to use it, at least me and my friend who\
    \ discussed this Lora."
  created_at: 2024-01-06 16:37:52+00:00
  edited: false
  hidden: false
  id: 659981e0654fe4eb0aaf2847
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2024-01-06T17:46:33.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9879761338233948
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Hi there, I don''t know much about GGUF, but seeing on the quant
          method they use, they don''t use a calibration dataset (It seems since their
          sizes are fixed it goes the same way for every model). So sadly I think
          it''s not possible or I don''t know how it could be done.</p>

          '
        raw: Hi there, I don't know much about GGUF, but seeing on the quant method
          they use, they don't use a calibration dataset (It seems since their sizes
          are fixed it goes the same way for every model). So sadly I think it's not
          possible or I don't know how it could be done.
        updatedAt: '2024-01-06T17:46:33.904Z'
      numEdits: 0
      reactions: []
    id: 659991f9b01e6e0c712fcdf6
    type: comment
  author: Panchovix
  content: Hi there, I don't know much about GGUF, but seeing on the quant method
    they use, they don't use a calibration dataset (It seems since their sizes are
    fixed it goes the same way for every model). So sadly I think it's not possible
    or I don't know how it could be done.
  created_at: 2024-01-06 17:46:33+00:00
  edited: false
  hidden: false
  id: 659991f9b01e6e0c712fcdf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2024-01-19T10:47:54.000Z'
    data:
      edited: false
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7234188914299011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<p>You can try this <a href="https://huggingface.co/tu9jn/Goliath-120b_SOTA_GGUF/tree/main">https://huggingface.co/tu9jn/Goliath-120b_SOTA_GGUF/tree/main</a><br>The
          uploader used ppipa_rp, so it is probably similar to this model, <a rel="nofollow"
          href="https://www.reddit.com/r/LocalLLaMA/comments/197mip0/comment/ki37saa/?utm_source=share&amp;utm_medium=web2x&amp;context=3">https://www.reddit.com/r/LocalLLaMA/comments/197mip0/comment/ki37saa/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>

          '
        raw: "You can try this https://huggingface.co/tu9jn/Goliath-120b_SOTA_GGUF/tree/main\r\
          The uploader used ppipa_rp, so it is probably similar to this model, https://www.reddit.com/r/LocalLLaMA/comments/197mip0/comment/ki37saa/?utm_source=share&utm_medium=web2x&context=3"
        updatedAt: '2024-01-19T10:47:54.174Z'
      numEdits: 0
      reactions: []
    id: 65aa535a9aba49e1d00627c9
    type: comment
  author: akoyaki
  content: "You can try this https://huggingface.co/tu9jn/Goliath-120b_SOTA_GGUF/tree/main\r\
    The uploader used ppipa_rp, so it is probably similar to this model, https://www.reddit.com/r/LocalLLaMA/comments/197mip0/comment/ki37saa/?utm_source=share&utm_medium=web2x&context=3"
  created_at: 2024-01-19 10:47:54+00:00
  edited: false
  hidden: false
  id: 65aa535a9aba49e1d00627c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: Panchovix/goliath-120b-exl2-rpcal
repo_type: model
status: open
target_branch: null
title: Should we expect a version for GGUF?
