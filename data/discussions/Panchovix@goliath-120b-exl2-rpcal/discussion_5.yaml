!!python/object:huggingface_hub.community.DiscussionWithDetails
author: imi2
conflicting_files: null
created_at: 2024-01-04 14:31:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69d1520ee2cec441e5c34c8d2c986643.svg
      fullname: imi2
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imi2
      type: user
    createdAt: '2024-01-04T14:31:42.000Z'
    data:
      edited: false
      editors:
      - imi2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9019387364387512
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69d1520ee2cec441e5c34c8d2c986643.svg
          fullname: imi2
          isHf: false
          isPro: false
          name: imi2
          type: user
        html: '<p>Any chance you or someone could requantize to the latest exl2? No
          idea the total vram requirements when quantizing and have FOMO on the bpw
          quality improvements since.</p>

          '
        raw: Any chance you or someone could requantize to the latest exl2? No idea
          the total vram requirements when quantizing and have FOMO on the bpw quality
          improvements since.
        updatedAt: '2024-01-04T14:31:42.450Z'
      numEdits: 0
      reactions: []
    id: 6596c14e81a39e2c5042e2c1
    type: comment
  author: imi2
  content: Any chance you or someone could requantize to the latest exl2? No idea
    the total vram requirements when quantizing and have FOMO on the bpw quality improvements
    since.
  created_at: 2024-01-04 14:31:42+00:00
  edited: false
  hidden: false
  id: 6596c14e81a39e2c5042e2c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2024-01-04T22:35:47.000Z'
    data:
      edited: true
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9844807386398315
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Hi there, sorry, life have been busy. I will try to do it as soon
          as I can. It takes about 10 hours to do it fully and then ~5 hours per each
          bpw size on a RTX 4090.</p>

          '
        raw: Hi there, sorry, life have been busy. I will try to do it as soon as
          I can. It takes about 10 hours to do it fully and then ~5 hours per each
          bpw size on a RTX 4090.
        updatedAt: '2024-01-04T22:35:54.563Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - imi2
    id: 659732c39608f3f87626ea5b
    type: comment
  author: Panchovix
  content: Hi there, sorry, life have been busy. I will try to do it as soon as I
    can. It takes about 10 hours to do it fully and then ~5 hours per each bpw size
    on a RTX 4090.
  created_at: 2024-01-04 22:35:47+00:00
  edited: true
  hidden: false
  id: 659732c39608f3f87626ea5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/69d1520ee2cec441e5c34c8d2c986643.svg
      fullname: imi2
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imi2
      type: user
    createdAt: '2024-01-05T12:50:38.000Z'
    data:
      status: closed
    id: 6597fb1ea84537abc1671ff6
    type: status-change
  author: imi2
  created_at: 2024-01-05 12:50:38+00:00
  id: 6597fb1ea84537abc1671ff6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69d1520ee2cec441e5c34c8d2c986643.svg
      fullname: imi2
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imi2
      type: user
    createdAt: '2024-01-06T03:17:50.000Z'
    data:
      edited: false
      editors:
      - imi2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.928002119064331
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69d1520ee2cec441e5c34c8d2c986643.svg
          fullname: imi2
          isHf: false
          isPro: false
          name: imi2
          type: user
        html: '<p>No worries, take your time! </p>

          <p>P.S. I don''t know if it''s just me but the newer quantized models take
          slightly more space on the same system? A 2.4bpw fp8 kv-cache used to go
          to 16k on a 1x3090 system. The system is unchanged, but now only goes to
          8k with fp8 cache.</p>

          '
        raw: "No worries, take your time! \n\nP.S. I don't know if it's just me but\
          \ the newer quantized models take slightly more space on the same system?\
          \ A 2.4bpw fp8 kv-cache used to go to 16k on a 1x3090 system. The system\
          \ is unchanged, but now only goes to 8k with fp8 cache."
        updatedAt: '2024-01-06T03:17:50.866Z'
      numEdits: 0
      reactions: []
    id: 6598c65eacaab7bec3f56526
    type: comment
  author: imi2
  content: "No worries, take your time! \n\nP.S. I don't know if it's just me but\
    \ the newer quantized models take slightly more space on the same system? A 2.4bpw\
    \ fp8 kv-cache used to go to 16k on a 1x3090 system. The system is unchanged,\
    \ but now only goes to 8k with fp8 cache."
  created_at: 2024-01-06 03:17:50+00:00
  edited: false
  hidden: false
  id: 6598c65eacaab7bec3f56526
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2024-01-06T17:42:48.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9775307178497314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I have updated the quants, so I suggest to backup in any case.</p>

          <p>About the newer quantized sizes, I haven''t tested enough yet, but it
          seems they''re fairly similar.</p>

          '
        raw: 'I have updated the quants, so I suggest to backup in any case.


          About the newer quantized sizes, I haven''t tested enough yet, but it seems
          they''re fairly similar.'
        updatedAt: '2024-01-06T17:42:48.889Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - imi2
    id: 65999118b0f43ed69f522167
    type: comment
  author: Panchovix
  content: 'I have updated the quants, so I suggest to backup in any case.


    About the newer quantized sizes, I haven''t tested enough yet, but it seems they''re
    fairly similar.'
  created_at: 2024-01-06 17:42:48+00:00
  edited: false
  hidden: false
  id: 65999118b0f43ed69f522167
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Panchovix/goliath-120b-exl2-rpcal
repo_type: model
status: closed
target_branch: null
title: Quantization update?
