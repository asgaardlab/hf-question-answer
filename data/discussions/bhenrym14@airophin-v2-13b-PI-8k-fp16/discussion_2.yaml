!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SabinStargem
conflicting_files: null
created_at: 2023-08-24 03:25:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
      fullname: Sabin Stargem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SabinStargem
      type: user
    createdAt: '2023-08-24T04:25:53.000Z'
    data:
      edited: false
      editors:
      - SabinStargem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957740306854248
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
          fullname: Sabin Stargem
          isHf: false
          isPro: false
          name: SabinStargem
          type: user
        html: '<p>I recently upgraded my machine to have 128gb and a CUDA card, so
          I have been playing a lot with the Ycros GGML versions of extended Airoboros
          v1.4.1.   They are very good for Llama 1 models, but it seems like that
          Vicuna v1.5 13b-16k is almost at the level of Airo 33b-16k.</p>

          <p>This makes me interested in trying an L2 Airo with extended context.  My
          personal preference would be an Airophin v2+ 16k.  I am hoping for a v3
          version, since the v2.1 Airoboros data set has been released at Durbin''s
          github - it is supposed to address a variety of shortcomings in L2 models
          and Airo 2.0.</p>

          <p>Anyhow, thanks for making the existing pytorches.  The GGML versions
          have been fun to play with.  :)</p>

          '
        raw: "I recently upgraded my machine to have 128gb and a CUDA card, so I have\
          \ been playing a lot with the Ycros GGML versions of extended Airoboros\
          \ v1.4.1.   They are very good for Llama 1 models, but it seems like that\
          \ Vicuna v1.5 13b-16k is almost at the level of Airo 33b-16k.\r\n\r\nThis\
          \ makes me interested in trying an L2 Airo with extended context.  My personal\
          \ preference would be an Airophin v2+ 16k.  I am hoping for a v3 version,\
          \ since the v2.1 Airoboros data set has been released at Durbin's github\
          \ - it is supposed to address a variety of shortcomings in L2 models and\
          \ Airo 2.0.\r\n\r\nAnyhow, thanks for making the existing pytorches.  The\
          \ GGML versions have been fun to play with.  :)"
        updatedAt: '2023-08-24T04:25:53.562Z'
      numEdits: 0
      reactions: []
    id: 64e6dbd1b159a6f87bca404a
    type: comment
  author: SabinStargem
  content: "I recently upgraded my machine to have 128gb and a CUDA card, so I have\
    \ been playing a lot with the Ycros GGML versions of extended Airoboros v1.4.1.\
    \   They are very good for Llama 1 models, but it seems like that Vicuna v1.5\
    \ 13b-16k is almost at the level of Airo 33b-16k.\r\n\r\nThis makes me interested\
    \ in trying an L2 Airo with extended context.  My personal preference would be\
    \ an Airophin v2+ 16k.  I am hoping for a v3 version, since the v2.1 Airoboros\
    \ data set has been released at Durbin's github - it is supposed to address a\
    \ variety of shortcomings in L2 models and Airo 2.0.\r\n\r\nAnyhow, thanks for\
    \ making the existing pytorches.  The GGML versions have been fun to play with.\
    \  :)"
  created_at: 2023-08-24 03:25:53+00:00
  edited: false
  hidden: false
  id: 64e6dbd1b159a6f87bca404a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-24T16:09:24.000Z'
    data:
      edited: true
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526890516281128
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>Yeah the L2-13b models become quite competitive with the L1-33b
          models (aside from the intermittent repetition problems with L2). Is your
          preference for Airophin v2 over v1 because of observed performance or because
          it employs PI vs PNTK (since PNTK is more of a pain to get working)?</p>

          <p>I tried to find the v2.1 dataset, but I don''t see it in Durbin''s HF
          datasets. (edit: Looks like it''s there now!)</p>

          <p>A 16k 13b PI model will require me to kick off a new pretraining run,
          unless you''re aware of any new 13b dolphin/orca 16k models? I might just
          kick that off anyway, as the v2 results didn''t really improve much other
          than what might be expected with a smaller extension factor.</p>

          '
        raw: 'Yeah the L2-13b models become quite competitive with the L1-33b models
          (aside from the intermittent repetition problems with L2). Is your preference
          for Airophin v2 over v1 because of observed performance or because it employs
          PI vs PNTK (since PNTK is more of a pain to get working)?


          I tried to find the v2.1 dataset, but I don''t see it in Durbin''s HF datasets.
          (edit: Looks like it''s there now!)


          A 16k 13b PI model will require me to kick off a new pretraining run, unless
          you''re aware of any new 13b dolphin/orca 16k models? I might just kick
          that off anyway, as the v2 results didn''t really improve much other than
          what might be expected with a smaller extension factor.'
        updatedAt: '2023-08-24T19:14:17.084Z'
      numEdits: 1
      reactions: []
    id: 64e780b42eba1760dfc3f288
    type: comment
  author: bhenrym14
  content: 'Yeah the L2-13b models become quite competitive with the L1-33b models
    (aside from the intermittent repetition problems with L2). Is your preference
    for Airophin v2 over v1 because of observed performance or because it employs
    PI vs PNTK (since PNTK is more of a pain to get working)?


    I tried to find the v2.1 dataset, but I don''t see it in Durbin''s HF datasets.
    (edit: Looks like it''s there now!)


    A 16k 13b PI model will require me to kick off a new pretraining run, unless you''re
    aware of any new 13b dolphin/orca 16k models? I might just kick that off anyway,
    as the v2 results didn''t really improve much other than what might be expected
    with a smaller extension factor.'
  created_at: 2023-08-24 15:09:24+00:00
  edited: true
  hidden: false
  id: 64e780b42eba1760dfc3f288
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
      fullname: Sabin Stargem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SabinStargem
      type: user
    createdAt: '2023-08-24T22:32:24.000Z'
    data:
      edited: true
      editors:
      - SabinStargem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9777403473854065
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
          fullname: Sabin Stargem
          isHf: false
          isPro: false
          name: SabinStargem
          type: user
        html: '<p>I don''t know how to use FP16 models, so I haven''t been able to
          test any of the Airophins.  I assumed that the v2 might be better than the
          v1 in terms of quality, since it had v2.0m of Airoboros.   It is my assumption
          is that it costs a pretty penny to make models, so I only wanted what I
          hoped to be the better version.   If I can get both quality and length,
          then that would be terrific.</p>

          <p>With GGUF now working in Kobold and in the wild, PNTK or other approaches
          might be easier to work with now.  </p>

          <p>EDIT:  Huh.  Meta released a 34b foundation, Code Llama.  The interesting
          thing is that it is apparently trained on at least 16k context?   I am VERY
          interested in trying an Airoboros v2.1 L2-34b-16k.</p>

          '
        raw: "I don't know how to use FP16 models, so I haven't been able to test\
          \ any of the Airophins.  I assumed that the v2 might be better than the\
          \ v1 in terms of quality, since it had v2.0m of Airoboros.   It is my assumption\
          \ is that it costs a pretty penny to make models, so I only wanted what\
          \ I hoped to be the better version.   If I can get both quality and length,\
          \ then that would be terrific.\n\nWith GGUF now working in Kobold and in\
          \ the wild, PNTK or other approaches might be easier to work with now. \
          \ \n\nEDIT:  Huh.  Meta released a 34b foundation, Code Llama.  The interesting\
          \ thing is that it is apparently trained on at least 16k context?   I am\
          \ VERY interested in trying an Airoboros v2.1 L2-34b-16k."
        updatedAt: '2023-08-25T03:19:40.542Z'
      numEdits: 6
      reactions: []
    id: 64e7da78ad73ab1513c89670
    type: comment
  author: SabinStargem
  content: "I don't know how to use FP16 models, so I haven't been able to test any\
    \ of the Airophins.  I assumed that the v2 might be better than the v1 in terms\
    \ of quality, since it had v2.0m of Airoboros.   It is my assumption is that it\
    \ costs a pretty penny to make models, so I only wanted what I hoped to be the\
    \ better version.   If I can get both quality and length, then that would be terrific.\n\
    \nWith GGUF now working in Kobold and in the wild, PNTK or other approaches might\
    \ be easier to work with now.  \n\nEDIT:  Huh.  Meta released a 34b foundation,\
    \ Code Llama.  The interesting thing is that it is apparently trained on at least\
    \ 16k context?   I am VERY interested in trying an Airoboros v2.1 L2-34b-16k."
  created_at: 2023-08-24 21:32:24+00:00
  edited: true
  hidden: false
  id: 64e7da78ad73ab1513c89670
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bhenrym14/airophin-v2-13b-PI-8k-fp16
repo_type: model
status: open
target_branch: null
title: 'Request:  A GGML edition of extended Airophin v2+.'
