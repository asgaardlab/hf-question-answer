!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Praveen92y
conflicting_files: null
created_at: 2023-08-18 17:58:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677584785919-639a01816945e9ab07f80841.jpeg?w=200&h=200&f=face
      fullname: Praveen Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Praveen92y
      type: user
    createdAt: '2023-08-18T18:58:08.000Z'
    data:
      edited: false
      editors:
      - Praveen92y
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.866743266582489
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677584785919-639a01816945e9ab07f80841.jpeg?w=200&h=200&f=face
          fullname: Praveen Yadav
          isHf: false
          isPro: false
          name: Praveen92y
          type: user
        html: '<p>Can you share the finetuning script, how did you train 8k context
          length llama2 got trained on 24gb GPU? </p>

          '
        raw: 'Can you share the finetuning script, how did you train 8k context length
          llama2 got trained on 24gb GPU? '
        updatedAt: '2023-08-18T18:58:08.594Z'
      numEdits: 0
      reactions: []
    id: 64dfbf4018af51be8eebd873
    type: comment
  author: Praveen92y
  content: 'Can you share the finetuning script, how did you train 8k context length
    llama2 got trained on 24gb GPU? '
  created_at: 2023-08-18 17:58:08+00:00
  edited: false
  hidden: false
  id: 64dfbf4018af51be8eebd873
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-22T17:30:16.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9442018866539001
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>I trained it on a RTX 6000 Ada, which has 48gb VRAM. However, for
          this model, I didn''t actually perform any training at 8k context length
          (unlike the first airophin model). I started from another model checkpoint
          that had been trained on such.<br>As far as the finetuning script goes,
          It''s basically a modified version of the qlora script in the original qlora
          paper. I have a version of it here (there are differences to what I used
          here; I may update it when I get a chance): <a rel="nofollow" href="https://github.com/bhenrym14/qlora-airoboros-longcontext/blob/main/qlora_airo.py">https://github.com/bhenrym14/qlora-airoboros-longcontext/blob/main/qlora_airo.py</a></p>

          '
        raw: "I trained it on a RTX 6000 Ada, which has 48gb VRAM. However, for this\
          \ model, I didn't actually perform any training at 8k context length (unlike\
          \ the first airophin model). I started from another model checkpoint that\
          \ had been trained on such. \nAs far as the finetuning script goes, It's\
          \ basically a modified version of the qlora script in the original qlora\
          \ paper. I have a version of it here (there are differences to what I used\
          \ here; I may update it when I get a chance): https://github.com/bhenrym14/qlora-airoboros-longcontext/blob/main/qlora_airo.py\n\
          \n"
        updatedAt: '2023-08-22T17:30:16.344Z'
      numEdits: 0
      reactions: []
    id: 64e4f0a8297f3cc53c4cb558
    type: comment
  author: bhenrym14
  content: "I trained it on a RTX 6000 Ada, which has 48gb VRAM. However, for this\
    \ model, I didn't actually perform any training at 8k context length (unlike the\
    \ first airophin model). I started from another model checkpoint that had been\
    \ trained on such. \nAs far as the finetuning script goes, It's basically a modified\
    \ version of the qlora script in the original qlora paper. I have a version of\
    \ it here (there are differences to what I used here; I may update it when I get\
    \ a chance): https://github.com/bhenrym14/qlora-airoboros-longcontext/blob/main/qlora_airo.py\n\
    \n"
  created_at: 2023-08-22 16:30:16+00:00
  edited: false
  hidden: false
  id: 64e4f0a8297f3cc53c4cb558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677584785919-639a01816945e9ab07f80841.jpeg?w=200&h=200&f=face
      fullname: Praveen Yadav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Praveen92y
      type: user
    createdAt: '2023-08-22T20:07:11.000Z'
    data:
      edited: true
      editors:
      - Praveen92y
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6443043947219849
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677584785919-639a01816945e9ab07f80841.jpeg?w=200&h=200&f=face
          fullname: Praveen Yadav
          isHf: false
          isPro: false
          name: Praveen92y
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;bhenrym14&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bhenrym14\"\
          >@<span class=\"underline\">bhenrym14</span></a></span>\n\n\t</span></span>\
          \ for the clarification. Just wanted to confirm what you have used as model_max_len,\
          \ 8192 or default one i.e 2048 in mentioned script ? Also can you confirm\
          \ if similar script was used to finetune the first airophin model(bhenrym14/airophin-13b-pntk-16k-fp16),\
          \ if yes then what was value of model_max_len there and gpu type and how\
          \ many gpus?</p>\n"
        raw: Thanks @bhenrym14 for the clarification. Just wanted to confirm what
          you have used as model_max_len, 8192 or default one i.e 2048 in mentioned
          script ? Also can you confirm if similar script was used to finetune the
          first airophin model(bhenrym14/airophin-13b-pntk-16k-fp16), if yes then
          what was value of model_max_len there and gpu type and how many gpus?
        updatedAt: '2023-08-22T20:16:42.021Z'
      numEdits: 1
      reactions: []
    id: 64e5156f68df7c48dcaedcc4
    type: comment
  author: Praveen92y
  content: Thanks @bhenrym14 for the clarification. Just wanted to confirm what you
    have used as model_max_len, 8192 or default one i.e 2048 in mentioned script ?
    Also can you confirm if similar script was used to finetune the first airophin
    model(bhenrym14/airophin-13b-pntk-16k-fp16), if yes then what was value of model_max_len
    there and gpu type and how many gpus?
  created_at: 2023-08-22 19:07:11+00:00
  edited: true
  hidden: false
  id: 64e5156f68df7c48dcaedcc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-24T16:24:11.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8954712748527527
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>This script relies on the RoPE monkey-patch to apply the desired
          interpolation factor (where I just hard-coded it); this is because I wrote
          this before transformers had native support for RoPE scaling. So yes, for
          this model, I did scale appropriately (factor of 2) for 8192 context. Since
          tranformers now has support, I generally will edit the backbone config to
          include the desired scaling method, and use the model_max_len to control
          the maximum sequence length the model sees in training; this is simply so
          I can run larger batches without risking OOM on a couple of samples in an
          otherwise shorter sequence dataset.</p>

          <p>For the airoboros finetune phase, I capped it at ~3000 for max_model_len
          (again, RoPE scaling is still for 8192). I trained on a single gpu. It was
          a RTX 6000 Ada generation.</p>

          '
        raw: 'This script relies on the RoPE monkey-patch to apply the desired interpolation
          factor (where I just hard-coded it); this is because I wrote this before
          transformers had native support for RoPE scaling. So yes, for this model,
          I did scale appropriately (factor of 2) for 8192 context. Since tranformers
          now has support, I generally will edit the backbone config to include the
          desired scaling method, and use the model_max_len to control the maximum
          sequence length the model sees in training; this is simply so I can run
          larger batches without risking OOM on a couple of samples in an otherwise
          shorter sequence dataset.


          For the airoboros finetune phase, I capped it at ~3000 for max_model_len
          (again, RoPE scaling is still for 8192). I trained on a single gpu. It was
          a RTX 6000 Ada generation.'
        updatedAt: '2023-08-24T16:24:11.168Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Praveen92y
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Praveen92y
    id: 64e7842bccfe005d2b18ef24
    type: comment
  author: bhenrym14
  content: 'This script relies on the RoPE monkey-patch to apply the desired interpolation
    factor (where I just hard-coded it); this is because I wrote this before transformers
    had native support for RoPE scaling. So yes, for this model, I did scale appropriately
    (factor of 2) for 8192 context. Since tranformers now has support, I generally
    will edit the backbone config to include the desired scaling method, and use the
    model_max_len to control the maximum sequence length the model sees in training;
    this is simply so I can run larger batches without risking OOM on a couple of
    samples in an otherwise shorter sequence dataset.


    For the airoboros finetune phase, I capped it at ~3000 for max_model_len (again,
    RoPE scaling is still for 8192). I trained on a single gpu. It was a RTX 6000
    Ada generation.'
  created_at: 2023-08-24 15:24:11+00:00
  edited: false
  hidden: false
  id: 64e7842bccfe005d2b18ef24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-25T21:15:03.000Z'
    data:
      status: closed
    id: 64e919d7de393e513c214fbc
    type: status-change
  author: bhenrym14
  created_at: 2023-08-25 20:15:03+00:00
  id: 64e919d7de393e513c214fbc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bhenrym14/airophin-v2-13b-PI-8k-fp16
repo_type: model
status: closed
target_branch: null
title: Curious to know how 8k context llama2 got trained on 24gb GPU
