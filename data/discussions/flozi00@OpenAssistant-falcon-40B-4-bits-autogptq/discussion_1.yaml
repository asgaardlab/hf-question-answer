!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nafnlaus
conflicting_files: null
created_at: 2023-10-06 00:04:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-10-06T01:04:22.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8844722509384155
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>And where is this remote code?</p>

          '
        raw: And where is this remote code?
        updatedAt: '2023-10-06T01:04:22.062Z'
      numEdits: 0
      reactions: []
    id: 651f5d16cf1335cc44b8218f
    type: comment
  author: Nafnlaus
  content: And where is this remote code?
  created_at: 2023-10-06 00:04:22+00:00
  edited: false
  hidden: false
  id: 651f5d16cf1335cc44b8218f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-10-06T06:45:14.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518981575965881
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>This model is created when the falcon code was still remote and
          not native itegrated to transformers library.<br>You can ignore the trust
          remote code, with the latest release it should still work.</p>

          <p>The remote code is pointing to the original repo of the model</p>

          '
        raw: 'This model is created when the falcon code was still remote and not
          native itegrated to transformers library.

          You can ignore the trust remote code, with the latest release it should
          still work.


          The remote code is pointing to the original repo of the model'
        updatedAt: '2023-10-06T06:45:14.095Z'
      numEdits: 0
      reactions: []
    id: 651facfafc791e23f71c0075
    type: comment
  author: flozi00
  content: 'This model is created when the falcon code was still remote and not native
    itegrated to transformers library.

    You can ignore the trust remote code, with the latest release it should still
    work.


    The remote code is pointing to the original repo of the model'
  created_at: 2023-10-06 05:45:14+00:00
  edited: false
  hidden: false
  id: 651facfafc791e23f71c0075
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-10-06T12:09:44.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9110798835754395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>I can''t ignore it - I can''t run it without enabling trust_remote_code.
          </p>

          '
        raw: 'I can''t ignore it - I can''t run it without enabling trust_remote_code. '
        updatedAt: '2023-10-06T12:09:44.214Z'
      numEdits: 0
      reactions: []
    id: 651ff9081451f8c9112c5839
    type: comment
  author: Nafnlaus
  content: 'I can''t ignore it - I can''t run it without enabling trust_remote_code. '
  created_at: 2023-10-06 11:09:44+00:00
  edited: false
  hidden: false
  id: 651ff9081451f8c9112c5839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-10-06T12:24:03.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9064642786979675
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>Then please open an issue at transformers github, this is not model
          specific</p>

          '
        raw: Then please open an issue at transformers github, this is not model specific
        updatedAt: '2023-10-06T12:24:03.035Z'
      numEdits: 0
      reactions: []
    id: 651ffc63f6ece753d15d7b16
    type: comment
  author: flozi00
  content: Then please open an issue at transformers github, this is not model specific
  created_at: 2023-10-06 11:24:03+00:00
  edited: false
  hidden: false
  id: 651ffc63f6ece753d15d7b16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-10-06T18:17:39.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46827763319015503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: "<p>Unfortunately it does appear to be model-specific.  I looked at\
          \ the transformers code.  It's looking for auto_map with AutoConfig in the\
          \ config to decide if there's remote code. </p>\n<pre><code>    config_dict,\
          \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n    has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\"\
          \ in config_dict[\"auto_map\"]\n</code></pre>\n<p>You have an auto_map section\
          \ with AutoConfig in config.json:</p>\n<p>  \"auto_map\": {<br>    \"AutoConfig\"\
          : \"OpenAssistant/falcon-40b-sft-mix-1226--configuration_RW.RWConfig\",<br>\
          \    \"AutoModel\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWModel\"\
          ,<br>    \"AutoModelForCausalLM\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForCausalLM\"\
          ,<br>    \"AutoModelForQuestionAnswering\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForQuestionAnswering\"\
          ,<br>    \"AutoModelForSequenceClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForSequenceClassification\"\
          ,<br>    \"AutoModelForTokenClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForTokenClassification\"\
          <br>  },</p>\n"
        raw: "Unfortunately it does appear to be model-specific.  I looked at the\
          \ transformers code.  It's looking for auto_map with AutoConfig in the config\
          \ to decide if there's remote code. \n\n        config_dict, unused_kwargs\
          \ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\
          \        has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\"\
          \ in config_dict[\"auto_map\"]\n\nYou have an auto_map section with AutoConfig\
          \ in config.json:\n\n  \"auto_map\": {\n    \"AutoConfig\": \"OpenAssistant/falcon-40b-sft-mix-1226--configuration_RW.RWConfig\"\
          ,\n    \"AutoModel\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWModel\"\
          ,\n    \"AutoModelForCausalLM\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForCausalLM\"\
          ,\n    \"AutoModelForQuestionAnswering\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForQuestionAnswering\"\
          ,\n    \"AutoModelForSequenceClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForSequenceClassification\"\
          ,\n    \"AutoModelForTokenClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForTokenClassification\"\
          \n  },\n"
        updatedAt: '2023-10-06T18:17:39.033Z'
      numEdits: 0
      reactions: []
    id: 65204f43c0ceb75b49346cf7
    type: comment
  author: Nafnlaus
  content: "Unfortunately it does appear to be model-specific.  I looked at the transformers\
    \ code.  It's looking for auto_map with AutoConfig in the config to decide if\
    \ there's remote code. \n\n        config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\n        has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\"\
    \ in config_dict[\"auto_map\"]\n\nYou have an auto_map section with AutoConfig\
    \ in config.json:\n\n  \"auto_map\": {\n    \"AutoConfig\": \"OpenAssistant/falcon-40b-sft-mix-1226--configuration_RW.RWConfig\"\
    ,\n    \"AutoModel\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWModel\"\
    ,\n    \"AutoModelForCausalLM\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForCausalLM\"\
    ,\n    \"AutoModelForQuestionAnswering\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForQuestionAnswering\"\
    ,\n    \"AutoModelForSequenceClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForSequenceClassification\"\
    ,\n    \"AutoModelForTokenClassification\": \"OpenAssistant/falcon-40b-sft-mix-1226--modelling_RW.RWForTokenClassification\"\
    \n  },\n"
  created_at: 2023-10-06 17:17:39+00:00
  edited: false
  hidden: false
  id: 65204f43c0ceb75b49346cf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
      fullname: Florian Zimmermeister
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flozi00
      type: user
    createdAt: '2023-10-06T19:51:17.000Z'
    data:
      edited: false
      editors:
      - flozi00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9749703407287598
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654892015542-605b1cf890a4b6bc0eef99ad.jpeg?w=200&h=200&f=face
          fullname: Florian Zimmermeister
          isHf: false
          isPro: false
          name: flozi00
          type: user
        html: '<p>Just deleted this section, try again please</p>

          '
        raw: Just deleted this section, try again please
        updatedAt: '2023-10-06T19:51:17.124Z'
      numEdits: 0
      reactions: []
    id: 65206535a56398128b14a211
    type: comment
  author: flozi00
  content: Just deleted this section, try again please
  created_at: 2023-10-06 18:51:17+00:00
  edited: false
  hidden: false
  id: 65206535a56398128b14a211
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-10-06T20:53:05.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4074152410030365
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>[meme@chmmr text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq
          --listen  --verbose --api --xformers --n-gpu-layers 10000000000 --loader
          exllama --max_seq_len 2048<br>[2023-10-06 20:49:34,668] [INFO] [real_accelerator.py:110:get_accelerator]
          Setting ds_accelerator to cuda (auto detect)<br>2023-10-06 20:49:35.080375:
          I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary
          is optimized to use available CPU instructions in performance-critical operations.<br>To
          enable the following instructions: AVX2 AVX512F FMA, in other operations,
          rebuild TensorFlow with the appropriate compiler flags.<br>2023-10-06 20:49:35.743046:
          W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:
          Could not find TensorRT<br>2023-10-06 20:49:36 INFO:Loading settings from
          settings.json...<br>2023-10-06 20:49:36 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...<br>Traceback
          (most recent call last):<br>  File "/home/user/text-generation-webui/server.py",
          line 222, in <br>    shared.model, shared.tokenizer = load_model(model_name)<br>  File
          "/home/user/text-generation-webui/modules/models.py", line 79, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/home/user/text-generation-webui/modules/models.py", line 326, in ExLlama_loader<br>    model,
          tokenizer = ExllamaModel.from_pretrained(model_name)<br>  File "/home/user/text-generation-webui/modules/exllama.py",
          line 55, in from_pretrained<br>    config = ExLlamaConfig(str(model_config_path))<br>  File
          "/home/user/.local/lib/python3.10/site-packages/exllama/model.py", line
          56, in <strong>init</strong><br>    self.intermediate_size = read_config["intermediate_size"]<br>KeyError:
          ''intermediate_size''</p>

          <p>[text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq
          --listen  --verbose --api --xformers --n-gpu-layers 10000000000 --loader
          autogptq --triton<br>[2023-10-06 20:50:02,122] [INFO] [real_accelerator.py:110:get_accelerator]
          Setting ds_accelerator to cuda (auto detect)<br>2023-10-06 20:50:02.528940:
          I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary
          is optimized to use available CPU instructions in performance-critical operations.<br>To
          enable the following instructions: AVX2 AVX512F FMA, in other operations,
          rebuild TensorFlow with the appropriate compiler flags.<br>2023-10-06 20:50:03.190506:
          W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:
          Could not find TensorRT<br>2023-10-06 20:50:04 INFO:Loading settings from
          settings.json...<br>2023-10-06 20:50:04 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...<br>2023-10-06
          20:50:04 INFO:The AutoGPTQ params are: {''model_basename'': ''model'', ''device'':
          ''cuda:0'', ''use_triton'': True, ''inject_fused_attention'': True, ''inject_fused_mlp'':
          True, ''use_safetensors'': True, ''trust_remote_code'': False, ''max_memory'':
          None, ''quantize_config'': None, ''use_cuda_fp16'': True, ''disable_exllama'':
          False}<br>Traceback (most recent call last):<br>  File "/home/user/text-generation-webui/server.py",
          line 222, in <br>    shared.model, shared.tokenizer = load_model(model_name)<br>  File
          "/home/user/text-generation-webui/modules/models.py", line 79, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/home/user/text-generation-webui/modules/models.py", line 320, in AutoGPTQ_loader<br>    return
          modules.AutoGPTQ_loader.load_quantized(model_name)<br>  File "/home/user/text-generation-webui/modules/AutoGPTQ_loader.py",
          line 57, in load_quantized<br>    model = AutoGPTQForCausalLM.from_quantized(path_to_model,
          **params)<br>  File "/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py",
          line 87, in from_quantized<br>    model_type = check_and_get_model_type(model_name_or_path,
          trust_remote_code)<br>  File "/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py",
          line 147, in check_and_get_model_type<br>    config = AutoConfig.from_pretrained(model_dir,
          trust_remote_code=trust_remote_code)<br>  File "/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py",
          line 1050, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>  File
          "/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py",
          line 748, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:
          ''RefinedWeb''</p>

          <p>[text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq
          --listen --verbose --api --xformers --loader gptq-for-llama --model_type
          OPT<br>[2023-10-06 20:50:21,082] [INFO] [real_accelerator.py:110:get_accelerator]
          Setting ds_accelerator to cuda (auto detect)<br>2023-10-06 20:50:21.487741:
          I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary
          is optimized to use available CPU instructions in performance-critical operations.<br>To
          enable the following instructions: AVX2 AVX512F FMA, in other operations,
          rebuild TensorFlow with the appropriate compiler flags.<br>2023-10-06 20:50:22.148085:
          W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:
          Could not find TensorRT<br>2023-10-06 20:50:23 INFO:Loading settings from
          settings.json...<br>2023-10-06 20:50:23 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...<br>2023-10-06
          20:50:23 INFO:Found the following quantized model: models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq/model.safetensors<br>Traceback
          (most recent call last):<br>  File "/home/user/text-generation-webui/server.py",
          line 222, in <br>    shared.model, shared.tokenizer = load_model(model_name)<br>  File
          "/home/user/text-generation-webui/modules/models.py", line 79, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/home/user/text-generation-webui/modules/models.py", line 312, in GPTQ_loader<br>    model
          = modules.GPTQ_loader.load_quantized(model_name)<br>  File "/home/user/text-generation-webui/modules/GPTQ_loader.py",
          line 144, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "/home/user/text-generation-webui/modules/GPTQ_loader.py", line 26, in _load_quant<br>    config
          = AutoConfig.from_pretrained(model, trust_remote_code=shared.args.trust_remote_code)<br>  File
          "/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py",
          line 1050, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>  File
          "/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py",
          line 748, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:
          ''RefinedWeb''</p>

          '
        raw: "[meme@chmmr text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq\
          \ --listen  --verbose --api --xformers --n-gpu-layers 10000000000 --loader\
          \ exllama --max_seq_len 2048\n[2023-10-06 20:49:34,668] [INFO] [real_accelerator.py:110:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\n2023-10-06 20:49:35.080375:\
          \ I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary\
          \ is optimized to use available CPU instructions in performance-critical\
          \ operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in\
          \ other operations, rebuild TensorFlow with the appropriate compiler flags.\n\
          2023-10-06 20:49:35.743046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\n2023-10-06 20:49:36 INFO:Loading\
          \ settings from settings.json...\n2023-10-06 20:49:36 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
          Traceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
          , line 222, in <module>\n    shared.model, shared.tokenizer = load_model(model_name)\n\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 79,\
          \ in load_model\n    output = load_func_map[loader](model_name)\n  File\
          \ \"/home/user/text-generation-webui/modules/models.py\", line 326, in ExLlama_loader\n\
          \    model, tokenizer = ExllamaModel.from_pretrained(model_name)\n  File\
          \ \"/home/user/text-generation-webui/modules/exllama.py\", line 55, in from_pretrained\n\
          \    config = ExLlamaConfig(str(model_config_path))\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 56, in __init__\n    self.intermediate_size = read_config[\"intermediate_size\"\
          ]\nKeyError: 'intermediate_size'\n\n\n[text-generation-webui]$ python server.py\
          \ --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq --listen\
          \  --verbose --api --xformers --n-gpu-layers 10000000000 --loader autogptq\
          \ --triton\n[2023-10-06 20:50:02,122] [INFO] [real_accelerator.py:110:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\n2023-10-06 20:50:02.528940:\
          \ I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary\
          \ is optimized to use available CPU instructions in performance-critical\
          \ operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in\
          \ other operations, rebuild TensorFlow with the appropriate compiler flags.\n\
          2023-10-06 20:50:03.190506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\n2023-10-06 20:50:04 INFO:Loading\
          \ settings from settings.json...\n2023-10-06 20:50:04 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
          2023-10-06 20:50:04 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
          \ 'device': 'cuda:0', 'use_triton': True, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True,\
          \ 'disable_exllama': False}\nTraceback (most recent call last):\n  File\
          \ \"/home/user/text-generation-webui/server.py\", line 222, in <module>\n\
          \    shared.model, shared.tokenizer = load_model(model_name)\n  File \"\
          /home/user/text-generation-webui/modules/models.py\", line 79, in load_model\n\
          \    output = load_func_map[loader](model_name)\n  File \"/home/user/text-generation-webui/modules/models.py\"\
          , line 320, in AutoGPTQ_loader\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\n\
          \  File \"/home/user/text-generation-webui/modules/AutoGPTQ_loader.py\"\
          , line 57, in load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 87, in from_quantized\n    model_type = check_and_get_model_type(model_name_or_path,\
          \ trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py\"\
          , line 147, in check_and_get_model_type\n    config = AutoConfig.from_pretrained(model_dir,\
          \ trust_remote_code=trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1050, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 748, in __getitem__\n    raise KeyError(key)\nKeyError: 'RefinedWeb'\n\
          \n\n[text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq\
          \ --listen --verbose --api --xformers --loader gptq-for-llama --model_type\
          \ OPT \n[2023-10-06 20:50:21,082] [INFO] [real_accelerator.py:110:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\n2023-10-06 20:50:21.487741:\
          \ I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary\
          \ is optimized to use available CPU instructions in performance-critical\
          \ operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in\
          \ other operations, rebuild TensorFlow with the appropriate compiler flags.\n\
          2023-10-06 20:50:22.148085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\n2023-10-06 20:50:23 INFO:Loading\
          \ settings from settings.json...\n2023-10-06 20:50:23 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
          2023-10-06 20:50:23 INFO:Found the following quantized model: models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq/model.safetensors\n\
          Traceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
          , line 222, in <module>\n    shared.model, shared.tokenizer = load_model(model_name)\n\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 79,\
          \ in load_model\n    output = load_func_map[loader](model_name)\n  File\
          \ \"/home/user/text-generation-webui/modules/models.py\", line 312, in GPTQ_loader\n\
          \    model = modules.GPTQ_loader.load_quantized(model_name)\n  File \"/home/user/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 144, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"/home/user/text-generation-webui/modules/GPTQ_loader.py\", line\
          \ 26, in _load_quant\n    config = AutoConfig.from_pretrained(model, trust_remote_code=shared.args.trust_remote_code)\n\
          \  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1050, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 748, in __getitem__\n    raise KeyError(key)\nKeyError: 'RefinedWeb'\n"
        updatedAt: '2023-10-06T20:53:05.209Z'
      numEdits: 0
      reactions: []
    id: 652073b1777019ca309380a9
    type: comment
  author: Nafnlaus
  content: "[meme@chmmr text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq\
    \ --listen  --verbose --api --xformers --n-gpu-layers 10000000000 --loader exllama\
    \ --max_seq_len 2048\n[2023-10-06 20:49:34,668] [INFO] [real_accelerator.py:110:get_accelerator]\
    \ Setting ds_accelerator to cuda (auto detect)\n2023-10-06 20:49:35.080375: I\
    \ tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is\
    \ optimized to use available CPU instructions in performance-critical operations.\n\
    To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild\
    \ TensorFlow with the appropriate compiler flags.\n2023-10-06 20:49:35.743046:\
    \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could\
    \ not find TensorRT\n2023-10-06 20:49:36 INFO:Loading settings from settings.json...\n\
    2023-10-06 20:49:36 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
    Traceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
    , line 222, in <module>\n    shared.model, shared.tokenizer = load_model(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/models.py\", line 79, in load_model\n\
    \    output = load_func_map[loader](model_name)\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 326, in ExLlama_loader\n    model, tokenizer = ExllamaModel.from_pretrained(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/exllama.py\", line 55, in from_pretrained\n\
    \    config = ExLlamaConfig(str(model_config_path))\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
    , line 56, in __init__\n    self.intermediate_size = read_config[\"intermediate_size\"\
    ]\nKeyError: 'intermediate_size'\n\n\n[text-generation-webui]$ python server.py\
    \ --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq --listen  --verbose\
    \ --api --xformers --n-gpu-layers 10000000000 --loader autogptq --triton\n[2023-10-06\
    \ 20:50:02,122] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\
    \ to cuda (auto detect)\n2023-10-06 20:50:02.528940: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
    \ This TensorFlow binary is optimized to use available CPU instructions in performance-critical\
    \ operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other\
    \ operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-06\
    \ 20:50:03.190506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT\
    \ Warning: Could not find TensorRT\n2023-10-06 20:50:04 INFO:Loading settings\
    \ from settings.json...\n2023-10-06 20:50:04 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
    2023-10-06 20:50:04 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
    \ 'device': 'cuda:0', 'use_triton': True, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True, 'disable_exllama': False}\n\
    Traceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
    , line 222, in <module>\n    shared.model, shared.tokenizer = load_model(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/models.py\", line 79, in load_model\n\
    \    output = load_func_map[loader](model_name)\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 320, in AutoGPTQ_loader\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/AutoGPTQ_loader.py\", line\
    \ 57, in load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
    , line 87, in from_quantized\n    model_type = check_and_get_model_type(model_name_or_path,\
    \ trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py\"\
    , line 147, in check_and_get_model_type\n    config = AutoConfig.from_pretrained(model_dir,\
    \ trust_remote_code=trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 1050, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 748, in __getitem__\n    raise KeyError(key)\nKeyError: 'RefinedWeb'\n\n\
    \n[text-generation-webui]$ python server.py --model models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq\
    \ --listen --verbose --api --xformers --loader gptq-for-llama --model_type OPT\
    \ \n[2023-10-06 20:50:21,082] [INFO] [real_accelerator.py:110:get_accelerator]\
    \ Setting ds_accelerator to cuda (auto detect)\n2023-10-06 20:50:21.487741: I\
    \ tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is\
    \ optimized to use available CPU instructions in performance-critical operations.\n\
    To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild\
    \ TensorFlow with the appropriate compiler flags.\n2023-10-06 20:50:22.148085:\
    \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could\
    \ not find TensorRT\n2023-10-06 20:50:23 INFO:Loading settings from settings.json...\n\
    2023-10-06 20:50:23 INFO:Loading flozi00_OpenAssistant-falcon-40B-4-bits-autogptq...\n\
    2023-10-06 20:50:23 INFO:Found the following quantized model: models/flozi00_OpenAssistant-falcon-40B-4-bits-autogptq/model.safetensors\n\
    Traceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
    , line 222, in <module>\n    shared.model, shared.tokenizer = load_model(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/models.py\", line 79, in load_model\n\
    \    output = load_func_map[loader](model_name)\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 312, in GPTQ_loader\n    model = modules.GPTQ_loader.load_quantized(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/GPTQ_loader.py\", line 144,\
    \ in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"/home/user/text-generation-webui/modules/GPTQ_loader.py\", line 26,\
    \ in _load_quant\n    config = AutoConfig.from_pretrained(model, trust_remote_code=shared.args.trust_remote_code)\n\
    \  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 1050, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 748, in __getitem__\n    raise KeyError(key)\nKeyError: 'RefinedWeb'\n"
  created_at: 2023-10-06 19:53:05+00:00
  edited: false
  hidden: false
  id: 652073b1777019ca309380a9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flozi00/OpenAssistant-falcon-40B-4-bits-autogptq
repo_type: model
status: open
target_branch: null
title: Why is this model demanding that I set trust remote code to true?
