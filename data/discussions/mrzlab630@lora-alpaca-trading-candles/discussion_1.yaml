!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NeedanAwP
conflicting_files: null
created_at: 2023-08-03 06:26:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a436c557fb222da7f644103be642e16.svg
      fullname: KLZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NeedanAwP
      type: user
    createdAt: '2023-08-03T07:26:24.000Z'
    data:
      edited: false
      editors:
      - NeedanAwP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6425357460975647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a436c557fb222da7f644103be642e16.svg
          fullname: KLZ
          isHf: false
          isPro: false
          name: NeedanAwP
          type: user
        html: "<p>im using a 3070laptop, so i can only figure out issues on cuda device\
          \ </p>\n<pre><code>if device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n\
          \        BASE_MODEL,\n        load_in_8bit=LOAD_8BIT,\n        torch_dtype=torch.float16,\n\
          \        device_map=\"cuda\",\n    )\n    model = PeftModel.from_pretrained(\n\
          \        model,\n        LORA_WEIGHTS,\n        torch_dtype=torch.float16,\n\
          \        offload_folder=r'mrzlab630/offload',\n    )\n</code></pre>\n<p>if\
          \ set <code>device_map='auto'</code>, it will automatically transport the\
          \ model to cpu, and cause some float calculate problems.<br>peft's latest\
          \ version need an offload_folder if you dont have enough ram to run this\
          \ model.<br>the warning from peft said need an offload_dir for something,\
          \ but offload_dir is useless and wont cause any mistake.</p>\n"
        raw: "im using a 3070laptop, so i can only figure out issues on cuda device\
          \ \r\n```\r\nif device == \"cuda\":\r\n    model = LlamaForCausalLM.from_pretrained(\r\
          \n        BASE_MODEL,\r\n        load_in_8bit=LOAD_8BIT,\r\n        torch_dtype=torch.float16,\r\
          \n        device_map=\"cuda\",\r\n    )\r\n    model = PeftModel.from_pretrained(\r\
          \n        model,\r\n        LORA_WEIGHTS,\r\n        torch_dtype=torch.float16,\r\
          \n        offload_folder=r'mrzlab630/offload',\r\n    )\r\n```\r\nif set\
          \ `device_map='auto'`, it will automatically transport the model to cpu,\
          \ and cause some float calculate problems.\r\npeft's latest version need\
          \ an offload_folder if you dont have enough ram to run this model. \r\n\
          the warning from peft said need an offload_dir for something, but offload_dir\
          \ is useless and wont cause any mistake."
        updatedAt: '2023-08-03T07:26:24.276Z'
      numEdits: 0
      reactions: []
    id: 64cb56a0ead94891d1f80df5
    type: comment
  author: NeedanAwP
  content: "im using a 3070laptop, so i can only figure out issues on cuda device\
    \ \r\n```\r\nif device == \"cuda\":\r\n    model = LlamaForCausalLM.from_pretrained(\r\
    \n        BASE_MODEL,\r\n        load_in_8bit=LOAD_8BIT,\r\n        torch_dtype=torch.float16,\r\
    \n        device_map=\"cuda\",\r\n    )\r\n    model = PeftModel.from_pretrained(\r\
    \n        model,\r\n        LORA_WEIGHTS,\r\n        torch_dtype=torch.float16,\r\
    \n        offload_folder=r'mrzlab630/offload',\r\n    )\r\n```\r\nif set `device_map='auto'`,\
    \ it will automatically transport the model to cpu, and cause some float calculate\
    \ problems.\r\npeft's latest version need an offload_folder if you dont have enough\
    \ ram to run this model. \r\nthe warning from peft said need an offload_dir for\
    \ something, but offload_dir is useless and wont cause any mistake."
  created_at: 2023-08-03 06:26:24+00:00
  edited: false
  hidden: false
  id: 64cb56a0ead94891d1f80df5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mrzlab630/lora-alpaca-trading-candles
repo_type: model
status: open
target_branch: null
title: some smalle problems
