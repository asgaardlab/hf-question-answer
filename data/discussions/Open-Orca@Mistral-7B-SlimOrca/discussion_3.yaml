!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gqd
conflicting_files: null
created_at: 2024-01-06 22:40:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2024-01-06T22:40:21.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4716973900794983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: "<p>When trying to serve the model using tgi:</p>\n<pre><code>sudo docker\
          \ run \\\n        ghcr.io/huggingface/text-generation-inference:1.3.1 \\\
          \n        --model-id Open-Orca/Mistral-7B-SlimOrca\n</code></pre>\n<p>It\
          \ throws:</p>\n<pre><code>2024-01-06T22:37:24.034017Z ERROR text_generation_launcher:\
          \ Error when initializing model\nTraceback (most recent call last):\n  File\
          \ \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 299, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 424, in __init__\n    super(FlashMistral, self).__init__(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 303, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1886, in _from_pretrained\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2073, in _from_pretrained\n    raise ValueError(\nValueError: Non-consecutive\
          \ added token '&lt;unk&gt;' found. Should have index 32000 but has index\
          \ 0 in saved vocabulary.\n</code></pre>\n"
        raw: "When trying to serve the model using tgi:\r\n\r\n```\r\nsudo docker\
          \ run \\\r\n        ghcr.io/huggingface/text-generation-inference:1.3.1\
          \ \\\r\n        --model-id Open-Orca/Mistral-7B-SlimOrca\r\n```\r\n\r\n\
          It throws:\r\n\r\n```\r\n2024-01-06T22:37:24.034017Z ERROR text_generation_launcher:\
          \ Error when initializing model\r\nTraceback (most recent call last):\r\n\
          \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 299, in get_model\r\n    return FlashMistral(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 424, in __init__\r\n    super(FlashMistral, self).__init__(\r\n \
          \ File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 303, in __init__\r\n    tokenizer = LlamaTokenizerFast.from_pretrained(\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1886, in _from_pretrained\r\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2073, in _from_pretrained\r\n    raise ValueError(\r\nValueError:\
          \ Non-consecutive added token '<unk>' found. Should have index 32000 but\
          \ has index 0 in saved vocabulary.\r\n```"
        updatedAt: '2024-01-06T22:40:21.095Z'
      numEdits: 0
      reactions: []
    id: 6599d6d5b2ec894c51b5e114
    type: comment
  author: gqd
  content: "When trying to serve the model using tgi:\r\n\r\n```\r\nsudo docker run\
    \ \\\r\n        ghcr.io/huggingface/text-generation-inference:1.3.1 \\\r\n   \
    \     --model-id Open-Orca/Mistral-7B-SlimOrca\r\n```\r\n\r\nIt throws:\r\n\r\n\
    ```\r\n2024-01-06T22:37:24.034017Z ERROR text_generation_launcher: Error when\
    \ initializing model\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\
    \n    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
    , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 161, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
    , line 299, in get_model\r\n    return FlashMistral(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 424, in __init__\r\n    super(FlashMistral, self).__init__(\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 303, in __init__\r\n    tokenizer = LlamaTokenizerFast.from_pretrained(\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1854, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1886, in _from_pretrained\r\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2073, in _from_pretrained\r\n    raise ValueError(\r\nValueError: Non-consecutive\
    \ added token '<unk>' found. Should have index 32000 but has index 0 in saved\
    \ vocabulary.\r\n```"
  created_at: 2024-01-06 22:40:21+00:00
  edited: false
  hidden: false
  id: 6599d6d5b2ec894c51b5e114
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2024-01-06T23:02:33.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.510303795337677
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: "<p>Okay, got that solved by dropping all except the chat ml tokens\
          \ from added_tokens.json</p>\n<p>Now running into:</p>\n<pre><code>2024-01-06T22:55:40.604513Z\
          \ ERROR text_generation_launcher: Error when initializing model\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 299, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 424, in __init__\n    super(FlashMistral, self).__init__(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 303, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2017, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 116, in __init__\n    super().__init__(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 110, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
          Exception: No such file or directory (os error 2)\n</code></pre>\n<p>Which\
          \ looks like it can be solved by also patching tokenizer_config.json seeing\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/15319\"\
          >https://github.com/huggingface/transformers/pull/15319</a></p>\n"
        raw: "Okay, got that solved by dropping all except the chat ml tokens from\
          \ added_tokens.json\n\nNow running into:\n\n```\n2024-01-06T22:55:40.604513Z\
          \ ERROR text_generation_launcher: Error when initializing model\nTraceback\
          \ (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          > File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 299, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 424, in __init__\n    super(FlashMistral, self).__init__(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 303, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2017, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 116, in __init__\n    super().__init__(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 110, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
          Exception: No such file or directory (os error 2)\n```\n\nWhich looks like\
          \ it can be solved by also patching tokenizer_config.json seeing https://github.com/huggingface/transformers/pull/15319"
        updatedAt: '2024-01-06T23:02:33.561Z'
      numEdits: 0
      reactions: []
    id: 6599dc09417c3c3ecd85539b
    type: comment
  author: gqd
  content: "Okay, got that solved by dropping all except the chat ml tokens from added_tokens.json\n\
    \nNow running into:\n\n```\n2024-01-06T22:55:40.604513Z ERROR text_generation_launcher:\
    \ Error when initializing model\nTraceback (most recent call last):\n  File \"\
    /opt/conda/bin/text-generation-server\", line 8, in <module>\n    sys.exit(app())\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\", line 311, in\
    \ __call__\n    return get_command(self)(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1434,\
    \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\", line\
    \ 89, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 215, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n> File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 161, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
    , line 299, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 424, in __init__\n    super(FlashMistral, self).__init__(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 303, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2017, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\
    , line 116, in __init__\n    super().__init__(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 110, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n\
    Exception: No such file or directory (os error 2)\n```\n\nWhich looks like it\
    \ can be solved by also patching tokenizer_config.json seeing https://github.com/huggingface/transformers/pull/15319"
  created_at: 2024-01-06 23:02:33+00:00
  edited: false
  hidden: false
  id: 6599dc09417c3c3ecd85539b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2024-01-06T23:03:36.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6955744624137878
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: '<p>Okay, can confirm that removing <code>toikenizer_file</code> from
          tokenizer_config.json fixes it</p>

          '
        raw: Okay, can confirm that removing `toikenizer_file` from tokenizer_config.json
          fixes it
        updatedAt: '2024-01-06T23:03:36.879Z'
      numEdits: 0
      reactions: []
    id: 6599dc48f0102bce6848b315
    type: comment
  author: gqd
  content: Okay, can confirm that removing `toikenizer_file` from tokenizer_config.json
    fixes it
  created_at: 2024-01-06 23:03:36+00:00
  edited: false
  hidden: false
  id: 6599dc48f0102bce6848b315
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Open-Orca/Mistral-7B-SlimOrca
repo_type: model
status: open
target_branch: null
title: 'ValueError: Non-consecutive added token ''<unk>'' found. Should have index
  32000 but has index 0 in saved vocabulary.'
