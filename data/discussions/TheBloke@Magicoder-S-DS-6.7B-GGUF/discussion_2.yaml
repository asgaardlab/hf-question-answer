!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OrelMac
conflicting_files: null
created_at: 2023-12-10 09:26:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83b33dbe8a515706a53815084b8b7a8a.svg
      fullname: Orel Magidish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OrelMac
      type: user
    createdAt: '2023-12-10T09:26:00.000Z'
    data:
      edited: false
      editors:
      - OrelMac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.21793748438358307
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83b33dbe8a515706a53815084b8b7a8a.svg
          fullname: Orel Magidish
          isHf: false
          isPro: false
          name: OrelMac
          type: user
        html: '<p>Hi </p>

          <p>Title.</p>

          <p>When trying to load using cpp the following err msg appears :</p>

          <p>Traceback (most recent call last):<br>  File "D:\text-generation-webui-main\modules\ui_model_menu.py",
          line 209, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(shared.model_name, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\text-generation-webui-main\modules\models.py", line 88, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\text-generation-webui-main\modules\models.py", line 253, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\text-generation-webui-main\modules\llamacpp_model.py", line 91, in from_pretrained<br>    result.model
          = Llama(**params)<br>                   ^^^^^^^^^^^^^^^<br>  File "D:\text-generation-webui-main\installer_files\env\Lib\site-packages\llama_cpp\llama.py",
          line 923, in <strong>init</strong><br>    self._n_vocab = self.n_vocab()<br>                    ^^^^^^^^^^^^^^<br>  File
          "D:\text-generation-webui-main\installer_files\env\Lib\site-packages\llama_cpp\llama.py",
          line 2184, in n_vocab<br>    return self._model.n_vocab()<br>           ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\text-generation-webui-main\installer_files\env\Lib\site-packages\llama_cpp\llama.py",
          line 250, in n_vocab<br>    assert self.model is not None<br>           ^^^^^^^^^^^^^^^^^^^^^^<br>AssertionError</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x0000024AD12E2E80&gt;<br>Traceback
          (most recent call last):<br>  File "D:\text-generation-webui-main\modules\llamacpp_model.py",
          line 49, in <strong>del</strong><br>    del self.model</p>

          '
        raw: "Hi \r\n\r\nTitle.\r\n\r\nWhen trying to load using cpp the following\
          \ err msg appears :\r\n\r\nTraceback (most recent call last):\r\n  File\
          \ \"D:\\text-generation-webui-main\\modules\\ui_model_menu.py\", line 209,\
          \ in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\text-generation-webui-main\\modules\\models.py\", line 88,\
          \ in load_model\r\n    output = load_func_map[loader](model_name)\r\n  \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\text-generation-webui-main\\\
          modules\\models.py\", line 253, in llamacpp_loader\r\n    model, tokenizer\
          \ = LlamaCppModel.from_pretrained(model_file)\r\n                      \
          \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\text-generation-webui-main\\\
          modules\\llamacpp_model.py\", line 91, in from_pretrained\r\n    result.model\
          \ = Llama(**params)\r\n                   ^^^^^^^^^^^^^^^\r\n  File \"D:\\\
          text-generation-webui-main\\installer_files\\env\\Lib\\site-packages\\llama_cpp\\\
          llama.py\", line 923, in __init__\r\n    self._n_vocab = self.n_vocab()\r\
          \n                    ^^^^^^^^^^^^^^\r\n  File \"D:\\text-generation-webui-main\\\
          installer_files\\env\\Lib\\site-packages\\llama_cpp\\llama.py\", line 2184,\
          \ in n_vocab\r\n    return self._model.n_vocab()\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\text-generation-webui-main\\installer_files\\env\\Lib\\site-packages\\\
          llama_cpp\\llama.py\", line 250, in n_vocab\r\n    assert self.model is\
          \ not None\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\r\n\
          Exception ignored in: <function LlamaCppModel.__del__ at 0x0000024AD12E2E80>\r\
          \nTraceback (most recent call last):\r\n  File \"D:\\text-generation-webui-main\\\
          modules\\llamacpp_model.py\", line 49, in __del__\r\n    del self.model\r\
          \n"
        updatedAt: '2023-12-10T09:26:00.564Z'
      numEdits: 0
      reactions: []
    id: 65758428177b3b466360bc76
    type: comment
  author: OrelMac
  content: "Hi \r\n\r\nTitle.\r\n\r\nWhen trying to load using cpp the following err\
    \ msg appears :\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\text-generation-webui-main\\\
    modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n               \
    \                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\\
    text-generation-webui-main\\modules\\models.py\", line 88, in load_model\r\n \
    \   output = load_func_map[loader](model_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\text-generation-webui-main\\modules\\models.py\", line 253, in\
    \ llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    D:\\text-generation-webui-main\\modules\\llamacpp_model.py\", line 91, in from_pretrained\r\
    \n    result.model = Llama(**params)\r\n                   ^^^^^^^^^^^^^^^\r\n\
    \  File \"D:\\text-generation-webui-main\\installer_files\\env\\Lib\\site-packages\\\
    llama_cpp\\llama.py\", line 923, in __init__\r\n    self._n_vocab = self.n_vocab()\r\
    \n                    ^^^^^^^^^^^^^^\r\n  File \"D:\\text-generation-webui-main\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp\\llama.py\", line 2184, in\
    \ n_vocab\r\n    return self._model.n_vocab()\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\text-generation-webui-main\\installer_files\\env\\Lib\\site-packages\\\
    llama_cpp\\llama.py\", line 250, in n_vocab\r\n    assert self.model is not None\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\r\nException ignored\
    \ in: <function LlamaCppModel.__del__ at 0x0000024AD12E2E80>\r\nTraceback (most\
    \ recent call last):\r\n  File \"D:\\text-generation-webui-main\\modules\\llamacpp_model.py\"\
    , line 49, in __del__\r\n    del self.model\r\n"
  created_at: 2023-12-10 09:26:00+00:00
  edited: false
  hidden: false
  id: 65758428177b3b466360bc76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e551b5ff92aca0b2fde43d392d8ad7fe.svg
      fullname: Sunil Samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samson14
      type: user
    createdAt: '2023-12-15T04:47:49.000Z'
    data:
      edited: false
      editors:
      - samson14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.694473922252655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e551b5ff92aca0b2fde43d392d8ad7fe.svg
          fullname: Sunil Samson
          isHf: false
          isPro: false
          name: samson14
          type: user
        html: '<p>Same issue.<br>Unable to use any gguf files.</p>

          '
        raw: 'Same issue.

          Unable to use any gguf files.'
        updatedAt: '2023-12-15T04:47:49.907Z'
      numEdits: 0
      reactions: []
    id: 657bda758c3aee70547435ec
    type: comment
  author: samson14
  content: 'Same issue.

    Unable to use any gguf files.'
  created_at: 2023-12-15 04:47:49+00:00
  edited: false
  hidden: false
  id: 657bda758c3aee70547435ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6575b711b951d40e7a505b90/2489IHvIxzQZRFT1S2LEp.png?w=200&h=200&f=face
      fullname: Serg Podtynnyi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shtirlic
      type: user
    createdAt: '2023-12-29T07:19:06.000Z'
    data:
      edited: false
      editors:
      - shtirlic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7924168109893799
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6575b711b951d40e7a505b90/2489IHvIxzQZRFT1S2LEp.png?w=200&h=200&f=face
          fullname: Serg Podtynnyi
          isHf: false
          isPro: false
          name: shtirlic
          type: user
        html: '<p>The issue is still persist for this model on latest  text-generation-webui</p>

          '
        raw: The issue is still persist for this model on latest  text-generation-webui
        updatedAt: '2023-12-29T07:19:06.672Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Moonphase
    id: 658e72ea35c41262d62b79e6
    type: comment
  author: shtirlic
  content: The issue is still persist for this model on latest  text-generation-webui
  created_at: 2023-12-29 07:19:06+00:00
  edited: false
  hidden: false
  id: 658e72ea35c41262d62b79e6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Magicoder-S-DS-6.7B-GGUF
repo_type: model
status: open
target_branch: null
title: Couldn't load any GGUF file of the Magicoder S DS into oogabooga
