!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rinoa
conflicting_files: null
created_at: 2023-12-06 20:01:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/058dfa3c9ceb2c5624a8aea6a96a480e.svg
      fullname: rinoa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rinoa
      type: user
    createdAt: '2023-12-06T20:01:55.000Z'
    data:
      edited: true
      editors:
      - rinoa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5439241528511047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/058dfa3c9ceb2c5624a8aea6a96a480e.svg
          fullname: rinoa
          isHf: false
          isPro: false
          name: rinoa
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , thank you for making the gguf model.  I got an error when loading this\
          \ model with llama.cpp. Do you have any suggestion? </p>\n<p>error loading\
          \ model: unordered_map::at<br>llama_load_model_from_file: failed to load\
          \ model<br>llama_init_from_gpt_params: error: failed to load model 'magicoder-s-ds-6.7b.Q5_K_M.gguf'<br>main:\
          \ error: unable to load model</p>\n"
        raw: "Hi @TheBloke , thank you for making the gguf model.  I got an error\
          \ when loading this model with llama.cpp. Do you have any suggestion? \n\
          \nerror loading model: unordered_map::at\nllama_load_model_from_file: failed\
          \ to load model\nllama_init_from_gpt_params: error: failed to load model\
          \ 'magicoder-s-ds-6.7b.Q5_K_M.gguf'\nmain: error: unable to load model\n\
          \n"
        updatedAt: '2023-12-06T20:05:08.993Z'
      numEdits: 1
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - Vincent6m
        - YearZero
        - Hangman
        - AlfredWALLACE
        - zcmcdonough
        - jiuyue
        - shtirlic
        - riddlechen
        - fogseller
    id: 6570d333b3501cbcb8017d2c
    type: comment
  author: rinoa
  content: "Hi @TheBloke , thank you for making the gguf model.  I got an error when\
    \ loading this model with llama.cpp. Do you have any suggestion? \n\nerror loading\
    \ model: unordered_map::at\nllama_load_model_from_file: failed to load model\n\
    llama_init_from_gpt_params: error: failed to load model 'magicoder-s-ds-6.7b.Q5_K_M.gguf'\n\
    main: error: unable to load model\n\n"
  created_at: 2023-12-06 20:01:55+00:00
  edited: true
  hidden: false
  id: 6570d333b3501cbcb8017d2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43b9f6921877f26246531f44a5027e47.svg
      fullname: '007'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suraj007
      type: user
    createdAt: '2023-12-06T20:06:41.000Z'
    data:
      edited: true
      editors:
      - suraj007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8708517551422119
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43b9f6921877f26246531f44a5027e47.svg
          fullname: '007'
          isHf: false
          isPro: false
          name: suraj007
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I am also  having the same problem while using the from llama_cpp import\
          \ Llama same with LM studio also<br>not able to load the model</p>\n"
        raw: "Hi @TheBloke I am also  having the same problem while using the from\
          \ llama_cpp import Llama same with LM studio also \nnot able to load the\
          \ model"
        updatedAt: '2023-12-06T20:07:03.973Z'
      numEdits: 1
      reactions: []
    id: 6570d45123363fa194b3c5cf
    type: comment
  author: suraj007
  content: "Hi @TheBloke I am also  having the same problem while using the from llama_cpp\
    \ import Llama same with LM studio also \nnot able to load the model"
  created_at: 2023-12-06 20:06:41+00:00
  edited: true
  hidden: false
  id: 6570d45123363fa194b3c5cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34aabe68d4cb2f5cbe204f7dae38fd30.svg
      fullname: steefvw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steefvw
      type: user
    createdAt: '2023-12-06T22:36:03.000Z'
    data:
      edited: false
      editors:
      - steefvw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9805248379707336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34aabe68d4cb2f5cbe204f7dae38fd30.svg
          fullname: steefvw
          isHf: false
          isPro: false
          name: steefvw
          type: user
        html: "<p>Same here but it looks like <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ just reuploaded the model so trying again now.</p>\n"
        raw: Same here but it looks like @TheBloke just reuploaded the model so trying
          again now.
        updatedAt: '2023-12-06T22:36:03.894Z'
      numEdits: 0
      reactions: []
    id: 6570f75332b42d408b4dbced
    type: comment
  author: steefvw
  content: Same here but it looks like @TheBloke just reuploaded the model so trying
    again now.
  created_at: 2023-12-06 22:36:03+00:00
  edited: false
  hidden: false
  id: 6570f75332b42d408b4dbced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34aabe68d4cb2f5cbe204f7dae38fd30.svg
      fullname: steefvw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steefvw
      type: user
    createdAt: '2023-12-06T22:56:41.000Z'
    data:
      edited: false
      editors:
      - steefvw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610199332237244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34aabe68d4cb2f5cbe204f7dae38fd30.svg
          fullname: steefvw
          isHf: false
          isPro: false
          name: steefvw
          type: user
        html: "<p>Same issue unfortunately. By the way, let me take this opportunity\
          \ to say thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ for your incredible work converting and uploading these models! It's been\
          \ so valuable to always find the latest models ready to download and deploy\
          \ on my local machine, typically within days after the original model was\
          \ launched. Thank you!!</p>\n"
        raw: Same issue unfortunately. By the way, let me take this opportunity to
          say thanks @TheBloke for your incredible work converting and uploading these
          models! It's been so valuable to always find the latest models ready to
          download and deploy on my local machine, typically within days after the
          original model was launched. Thank you!!
        updatedAt: '2023-12-06T22:56:41.285Z'
      numEdits: 0
      reactions: []
    id: 6570fc29c4993b8fb97e6196
    type: comment
  author: steefvw
  content: Same issue unfortunately. By the way, let me take this opportunity to say
    thanks @TheBloke for your incredible work converting and uploading these models!
    It's been so valuable to always find the latest models ready to download and deploy
    on my local machine, typically within days after the original model was launched.
    Thank you!!
  created_at: 2023-12-06 22:56:41+00:00
  edited: false
  hidden: false
  id: 6570fc29c4993b8fb97e6196
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43b9f6921877f26246531f44a5027e47.svg
      fullname: '007'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suraj007
      type: user
    createdAt: '2023-12-06T22:56:56.000Z'
    data:
      edited: false
      editors:
      - suraj007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8410071730613708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43b9f6921877f26246531f44a5027e47.svg
          fullname: '007'
          isHf: false
          isPro: false
          name: suraj007
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;steefvw&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/steefvw\">@<span class=\"\
          underline\">steefvw</span></a></span>\n\n\t</span></span> any luck, seems\
          \ he updated readme only</p>\n"
        raw: '@steefvw any luck, seems he updated readme only'
        updatedAt: '2023-12-06T22:56:56.472Z'
      numEdits: 0
      reactions: []
    id: 6570fc38974502490bb9ff9b
    type: comment
  author: suraj007
  content: '@steefvw any luck, seems he updated readme only'
  created_at: 2023-12-06 22:56:56+00:00
  edited: false
  hidden: false
  id: 6570fc38974502490bb9ff9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-07T00:37:35.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9923400282859802
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>yesterday when I did quantization of this model I had the same issue</p>

          '
        raw: yesterday when I did quantization of this model I had the same issue
        updatedAt: '2023-12-07T00:37:35.334Z'
      numEdits: 0
      reactions: []
    id: 657113cf0ea91e592a18d2ed
    type: comment
  author: eramax
  content: yesterday when I did quantization of this model I had the same issue
  created_at: 2023-12-07 00:37:35+00:00
  edited: false
  hidden: false
  id: 657113cf0ea91e592a18d2ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b1205526f20d488ea109ddf3c64f725.svg
      fullname: Anil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aniljava
      type: user
    createdAt: '2023-12-07T01:19:50.000Z'
    data:
      edited: false
      editors:
      - aniljava
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9469838738441467
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b1205526f20d488ea109ddf3c64f725.svg
          fullname: Anil
          isHf: false
          isPro: false
          name: aniljava
          type: user
        html: '<p>I think the issue is related to vocab size mismatch.<br>Similar
          to.<br><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/3900">https://github.com/ggerganov/llama.cpp/issues/3900</a></p>

          <p>When i did conversion, i had same message with the 16bit immediately
          after the convert.py. Further quantizing to 4 bit gave the vocab warning.</p>

          '
        raw: 'I think the issue is related to vocab size mismatch.

          Similar to.

          https://github.com/ggerganov/llama.cpp/issues/3900


          When i did conversion, i had same message with the 16bit immediately after
          the convert.py. Further quantizing to 4 bit gave the vocab warning.'
        updatedAt: '2023-12-07T01:19:50.008Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eramax
    id: 65711db6f56f953867980194
    type: comment
  author: aniljava
  content: 'I think the issue is related to vocab size mismatch.

    Similar to.

    https://github.com/ggerganov/llama.cpp/issues/3900


    When i did conversion, i had same message with the 16bit immediately after the
    convert.py. Further quantizing to 4 bit gave the vocab warning.'
  created_at: 2023-12-07 01:19:50+00:00
  edited: false
  hidden: false
  id: 65711db6f56f953867980194
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-07T01:22:58.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9879012703895569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aniljava&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aniljava\">@<span class=\"\
          underline\">aniljava</span></a></span>\n\n\t</span></span> , try this PR\
          \ <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/3633\"\
          >https://github.com/ggerganov/llama.cpp/pull/3633</a> maybe it work for\
          \ you, I have tried as well but didn't success.</p>\n"
        raw: '@aniljava , try this PR https://github.com/ggerganov/llama.cpp/pull/3633
          maybe it work for you, I have tried as well but didn''t success.

          '
        updatedAt: '2023-12-07T01:22:58.256Z'
      numEdits: 0
      reactions: []
    id: 65711e72d8b22ac39ff06c9f
    type: comment
  author: eramax
  content: '@aniljava , try this PR https://github.com/ggerganov/llama.cpp/pull/3633
    maybe it work for you, I have tried as well but didn''t success.

    '
  created_at: 2023-12-07 01:22:58+00:00
  edited: false
  hidden: false
  id: 65711e72d8b22ac39ff06c9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b1205526f20d488ea109ddf3c64f725.svg
      fullname: Anil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aniljava
      type: user
    createdAt: '2023-12-07T02:59:16.000Z'
    data:
      edited: false
      editors:
      - aniljava
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9434776306152344
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b1205526f20d488ea109ddf3c64f725.svg
          fullname: Anil
          isHf: false
          isPro: false
          name: aniljava
          type: user
        html: "<p>I tried #3663 briefly before giving up and waiting for <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> . Also played a bit manually chaning  the vocab size.\
          \  No luck. </p>\n"
        raw: 'I tried #3663 briefly before giving up and waiting for @TheBloke . Also
          played a bit manually chaning  the vocab size.  No luck. '
        updatedAt: '2023-12-07T02:59:16.277Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - eramax
    id: 657135040b27583c9d8cdf74
    type: comment
  author: aniljava
  content: 'I tried #3663 briefly before giving up and waiting for @TheBloke . Also
    played a bit manually chaning  the vocab size.  No luck. '
  created_at: 2023-12-07 02:59:16+00:00
  edited: false
  hidden: false
  id: 657135040b27583c9d8cdf74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c992c83a0e676d5e2190324e00205e4.svg
      fullname: Vola Delta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hf-delta
      type: user
    createdAt: '2023-12-07T05:32:14.000Z'
    data:
      edited: false
      editors:
      - hf-delta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9353001713752747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c992c83a0e676d5e2190324e00205e4.svg
          fullname: Vola Delta
          isHf: false
          isPro: false
          name: hf-delta
          type: user
        html: '<p>he said here:<br><a rel="nofollow" href="https://discord.com/channels/1111983596572520458/1181468647441563728/1182011875068747847">https://discord.com/channels/1111983596572520458/1181468647441563728/1182011875068747847</a></p>

          <blockquote>

          <p>Let me know if there''s any GGUF issues as I need to use a PR to make
          this due to lack of tokenizer.model support</p>

          </blockquote>

          <p>So it might be out of his reach for the time being.</p>

          <p>It''s better to ask the ogrinal model''s author instead of downstream.</p>

          '
        raw: 'he said here:

          https://discord.com/channels/1111983596572520458/1181468647441563728/1182011875068747847


          > Let me know if there''s any GGUF issues as I need to use a PR to make
          this due to lack of tokenizer.model support


          So it might be out of his reach for the time being.


          It''s better to ask the ogrinal model''s author instead of downstream.'
        updatedAt: '2023-12-07T05:32:14.502Z'
      numEdits: 0
      reactions: []
    id: 657158deb6ce26dd716bfa32
    type: comment
  author: hf-delta
  content: 'he said here:

    https://discord.com/channels/1111983596572520458/1181468647441563728/1182011875068747847


    > Let me know if there''s any GGUF issues as I need to use a PR to make this due
    to lack of tokenizer.model support


    So it might be out of his reach for the time being.


    It''s better to ask the ogrinal model''s author instead of downstream.'
  created_at: 2023-12-07 05:32:14+00:00
  edited: false
  hidden: false
  id: 657158deb6ce26dd716bfa32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
      fullname: Bumba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pumba2
      type: user
    createdAt: '2023-12-07T07:25:33.000Z'
    data:
      edited: true
      editors:
      - Pumba2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9903122186660767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
          fullname: Bumba
          isHf: false
          isPro: false
          name: Pumba2
          type: user
        html: '<p>I removed my "like" since its not working on any app.<br>We had
          had this problem with deepseek models before and didnt work then and they
          dont work now. </p>

          '
        raw: 'I removed my "like" since its not working on any app.

          We had had this problem with deepseek models before and didnt work then
          and they dont work now. '
        updatedAt: '2023-12-08T04:47:15.665Z'
      numEdits: 3
      reactions: []
    id: 6571736d642d7f0ac45fc4ed
    type: comment
  author: Pumba2
  content: 'I removed my "like" since its not working on any app.

    We had had this problem with deepseek models before and didnt work then and they
    dont work now. '
  created_at: 2023-12-07 07:25:33+00:00
  edited: true
  hidden: false
  id: 6571736d642d7f0ac45fc4ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
      fullname: Alfred WALLACE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlfredWALLACE
      type: user
    createdAt: '2023-12-07T10:40:04.000Z'
    data:
      edited: false
      editors:
      - AlfredWALLACE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7634937763214111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3322fa94233c0f5c04c356c2a79e7ef2.svg
          fullname: Alfred WALLACE
          isHf: false
          isPro: false
          name: AlfredWALLACE
          type: user
        html: '<p>Just downloaded Q4_K_M and Q3_K_M and also get an error:</p>

          <blockquote>

          <p>error loading model: invalid unordered_map&lt;K, T&gt; key</p>

          </blockquote>

          <p>on both gguf files.<br>Thanks anyway for publishing quantized models
          for small GPU!!!</p>

          '
        raw: 'Just downloaded Q4_K_M and Q3_K_M and also get an error:

          > error loading model: invalid unordered_map<K, T> key


          on both gguf files.

          Thanks anyway for publishing quantized models for small GPU!!!'
        updatedAt: '2023-12-07T10:40:04.989Z'
      numEdits: 0
      reactions: []
    id: 6571a104d186ca8d6e69e56b
    type: comment
  author: AlfredWALLACE
  content: 'Just downloaded Q4_K_M and Q3_K_M and also get an error:

    > error loading model: invalid unordered_map<K, T> key


    on both gguf files.

    Thanks anyway for publishing quantized models for small GPU!!!'
  created_at: 2023-12-07 10:40:04+00:00
  edited: false
  hidden: false
  id: 6571a104d186ca8d6e69e56b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-12-07T14:16:20.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560703039169312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>Most of the time in cases like this it needs to be addressed upstream.  Bloke
          cant fix their stuff....</p>

          '
        raw: Most of the time in cases like this it needs to be addressed upstream.  Bloke
          cant fix their stuff....
        updatedAt: '2023-12-07T14:16:20.294Z'
      numEdits: 0
      reactions: []
    id: 6571d3b4c3005101f65ec05a
    type: comment
  author: Nurb432
  content: Most of the time in cases like this it needs to be addressed upstream.  Bloke
    cant fix their stuff....
  created_at: 2023-12-07 14:16:20+00:00
  edited: false
  hidden: false
  id: 6571d3b4c3005101f65ec05a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-07T20:30:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9219285845756531
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah sorry, the models seem to be unusable at the moment. I will
          see if I can fix it, otherwise I''ll pull it for now</p>

          '
        raw: Yeah sorry, the models seem to be unusable at the moment. I will see
          if I can fix it, otherwise I'll pull it for now
        updatedAt: '2023-12-07T20:30:32.641Z'
      numEdits: 0
      reactions: []
    id: 65722b68ca0139e4acaeff2e
    type: comment
  author: TheBloke
  content: Yeah sorry, the models seem to be unusable at the moment. I will see if
    I can fix it, otherwise I'll pull it for now
  created_at: 2023-12-07 20:30:32+00:00
  edited: false
  hidden: false
  id: 65722b68ca0139e4acaeff2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-07T20:51:12.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8127523064613342
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK I have re-done the quants and they will now work with a specific\
          \ fork of llama.cpp.  This PR is not yet merged and is currently on hold,\
          \ so there's no immediate indication when it will me merged.</p>\n<p>Fork:\
          \ <a rel=\"nofollow\" href=\"https://github.com/DOGEwbx/llama.cpp/tree/regex_gpt2_preprocess\"\
          >https://github.com/DOGEwbx/llama.cpp/tree/regex_gpt2_preprocess</a></p>\n\
          <p><strong>They will not work with mainline llama.cpp, and they will not\
          \ work with any third-party GGUF clients, like llama-cpp-python, LM Studio,\
          \ text-generation-webui, etc</strong></p>\n<p> The files are left for anyone\
          \ who has the interest to compile llama.cpp for themselves and I can put\
          \ a note in the README to this effect. </p>\n<p>However I might still pull\
          \ it, as the output is so far not usable:</p>\n<pre><code>\u1405 ./main\
          \ -m /workspace/process/ise-uiuc_magicoder-s-ds-6.7b/gguf/magicoder-s-ds-6.7b.Q4_K_M.gguf\
          \ -n -1 -ngl 100 -p \"You are an exceptionally intelligent coding assistant\
          \ that consistently delivers accurate and reliable responses to user instructions.\n\
          \n@@ Instruction\nwrite a Python FastAPI server which accepts List[int],\
          \ sorts the provided numbers, and returns a tuple containing: (highest,\
          \ lowest, average, List[int] of the numbers sorted asc )\n\n@@ Response\n\
          \"\n\nYou are an exceptionally intelligent coding assistant that consistently\
          \ delivers accurate and reliable responses to user instructions.\n\n@@ Instruction\n\
          write a Python FastAPI server which accepts List[int], sorts the provided\
          \ numbers, and returns a tuple containing: (highest, lowest, average, List[int]\
          \ of the numbers sorted asc )\n\n@@ Response\nHere is how you can solve\
          \ it using python's FastAPI for creating an API server. We will use FastAPI\
          \ to create the web server which includes endpoints that sorts the given\
          \ integer list and returns highest number, lowest number, average, median,\
          \ mode of the list in a JSON format. It also provides the sorted list as\
          \ well:\n```python\n\n# install required packages using pip\npip install\
          \ fastapi uv\n\n\n$ pip install fastapi\n $ uvscikitly and statistics for\
          \ calculating modes\n\n\n\n```python\nfrom typing import List\nfrom fastapi\
          \ import FastAPI, Depends\n from fastapi.middleware.responses\nimport statistics\n\
          \nfrom pydantic import BaseModel, Body\nfrom starlette.requests: FastAPI\n\
          app = FastAPI()\nfrom typing import List\nfrom fastapi import HTTPException,\
          \ status\nfrom fastapi import FastApi, Response, Depends, Query\n# for the\
          \ calculation of mode in Python API\ndef calculate_mode(numbers: List[int]=\
          \ Body):\n    return {\n        \"highest\": numbers},\nimport statistics\
          \ as sta t.typing from starlette.middleware import FastAPI, HTTPException\n\
          from fastapi import FastAPI, Request, Uv\nfrom fastapi import Depends\n\
          from typing import Optional\nfrom pydantic import BaseModel\nfrom fastapi.responsesponsese\
          \ FastAPIErrorHTTPException, JSONResponse\nfrom fastapi import FastAPI,\
          \ Path\nfrom statistics\nfrom starlette import Request\nimport uviemyaml\
          \ APIRouter and path operation:\nfrom pydanticornado.middleware import HTTPException\
          \ 40.t ai.\n\nclass NumberModel for mode in Pythonication with thestarlette\n\
          \nimport statistics as Statistics\n\nfrom fastapi import FastAPI, Depends,\
          \ Path\nfrom starlette , Uvit\nfrom typing import List[int]\ndef sort\n\n\
          numbers\n\n<span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>\n\ndef calculate_mean(statistics)\
          \ is to use Python's Starlette:\n\n```python\nimport uvit.validator import\
          \ Depends,HTTPException\n\n.... \n</code></pre>\n<p>I terminated it there\
          \ as it was infinitely generating.  But even before that, there was no usable\
          \ answer, it's mostly just gibberish</p>\n"
        raw: "OK I have re-done the quants and they will now work with a specific\
          \ fork of llama.cpp.  This PR is not yet merged and is currently on hold,\
          \ so there's no immediate indication when it will me merged.\n\nFork: https://github.com/DOGEwbx/llama.cpp/tree/regex_gpt2_preprocess\n\
          \n**They will not work with mainline llama.cpp, and they will not work with\
          \ any third-party GGUF clients, like llama-cpp-python, LM Studio, text-generation-webui,\
          \ etc**\n\n The files are left for anyone who has the interest to compile\
          \ llama.cpp for themselves and I can put a note in the README to this effect.\
          \ \n\nHowever I might still pull it, as the output is so far not usable:\n\
          \n```\n\u1405 ./main -m /workspace/process/ise-uiuc_magicoder-s-ds-6.7b/gguf/magicoder-s-ds-6.7b.Q4_K_M.gguf\
          \ -n -1 -ngl 100 -p \"You are an exceptionally intelligent coding assistant\
          \ that consistently delivers accurate and reliable responses to user instructions.\n\
          \n@@ Instruction\nwrite a Python FastAPI server which accepts List[int],\
          \ sorts the provided numbers, and returns a tuple containing: (highest,\
          \ lowest, average, List[int] of the numbers sorted asc )\n\n@@ Response\n\
          \"\n\nYou are an exceptionally intelligent coding assistant that consistently\
          \ delivers accurate and reliable responses to user instructions.\n\n@@ Instruction\n\
          write a Python FastAPI server which accepts List[int], sorts the provided\
          \ numbers, and returns a tuple containing: (highest, lowest, average, List[int]\
          \ of the numbers sorted asc )\n\n@@ Response\nHere is how you can solve\
          \ it using python's FastAPI for creating an API server. We will use FastAPI\
          \ to create the web server which includes endpoints that sorts the given\
          \ integer list and returns highest number, lowest number, average, median,\
          \ mode of the list in a JSON format. It also provides the sorted list as\
          \ well:\n```python\n\n# install required packages using pip\npip install\
          \ fastapi uv\n\n\n$ pip install fastapi\n $ uvscikitly and statistics for\
          \ calculating modes\n\n\n\n```python\nfrom typing import List\nfrom fastapi\
          \ import FastAPI, Depends\n from fastapi.middleware.responses\nimport statistics\n\
          \nfrom pydantic import BaseModel, Body\nfrom starlette.requests: FastAPI\n\
          app = FastAPI()\nfrom typing import List\nfrom fastapi import HTTPException,\
          \ status\nfrom fastapi import FastApi, Response, Depends, Query\n# for the\
          \ calculation of mode in Python API\ndef calculate_mode(numbers: List[int]=\
          \ Body):\n    return {\n        \"highest\": numbers},\nimport statistics\
          \ as sta t.typing from starlette.middleware import FastAPI, HTTPException\n\
          from fastapi import FastAPI, Request, Uv\nfrom fastapi import Depends\n\
          from typing import Optional\nfrom pydantic import BaseModel\nfrom fastapi.responsesponsese\
          \ FastAPIErrorHTTPException, JSONResponse\nfrom fastapi import FastAPI,\
          \ Path\nfrom statistics\nfrom starlette import Request\nimport uviemyaml\
          \ APIRouter and path operation:\nfrom pydanticornado.middleware import HTTPException\
          \ 40.t ai.\n\nclass NumberModel for mode in Pythonication with thestarlette\n\
          \nimport statistics as Statistics\n\nfrom fastapi import FastAPI, Depends,\
          \ Path\nfrom starlette , Uvit\nfrom typing import List[int]\ndef sort\n\n\
          numbers\n\n@app\n\ndef calculate_mean(statistics) is to use Python's Starlette:\n\
          \n```python\nimport uvit.validator import Depends,HTTPException\n\n....\
          \ \n```\n\nI terminated it there as it was infinitely generating.  But even\
          \ before that, there was no usable answer, it's mostly just gibberish\n"
        updatedAt: '2023-12-09T10:58:28.719Z'
      numEdits: 2
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - eramax
        - Brooski
        - hyunfzen
        - mufeed
        - YearZero
        - mantafloppy
        - jiuyue
      - count: 2
        reaction: "\U0001F44D"
        users:
        - plinkr
        - riddlechen
    id: 65723040c2913e69b4b7c298
    type: comment
  author: TheBloke
  content: "OK I have re-done the quants and they will now work with a specific fork\
    \ of llama.cpp.  This PR is not yet merged and is currently on hold, so there's\
    \ no immediate indication when it will me merged.\n\nFork: https://github.com/DOGEwbx/llama.cpp/tree/regex_gpt2_preprocess\n\
    \n**They will not work with mainline llama.cpp, and they will not work with any\
    \ third-party GGUF clients, like llama-cpp-python, LM Studio, text-generation-webui,\
    \ etc**\n\n The files are left for anyone who has the interest to compile llama.cpp\
    \ for themselves and I can put a note in the README to this effect. \n\nHowever\
    \ I might still pull it, as the output is so far not usable:\n\n```\n\u1405 ./main\
    \ -m /workspace/process/ise-uiuc_magicoder-s-ds-6.7b/gguf/magicoder-s-ds-6.7b.Q4_K_M.gguf\
    \ -n -1 -ngl 100 -p \"You are an exceptionally intelligent coding assistant that\
    \ consistently delivers accurate and reliable responses to user instructions.\n\
    \n@@ Instruction\nwrite a Python FastAPI server which accepts List[int], sorts\
    \ the provided numbers, and returns a tuple containing: (highest, lowest, average,\
    \ List[int] of the numbers sorted asc )\n\n@@ Response\n\"\n\nYou are an exceptionally\
    \ intelligent coding assistant that consistently delivers accurate and reliable\
    \ responses to user instructions.\n\n@@ Instruction\nwrite a Python FastAPI server\
    \ which accepts List[int], sorts the provided numbers, and returns a tuple containing:\
    \ (highest, lowest, average, List[int] of the numbers sorted asc )\n\n@@ Response\n\
    Here is how you can solve it using python's FastAPI for creating an API server.\
    \ We will use FastAPI to create the web server which includes endpoints that sorts\
    \ the given integer list and returns highest number, lowest number, average, median,\
    \ mode of the list in a JSON format. It also provides the sorted list as well:\n\
    ```python\n\n# install required packages using pip\npip install fastapi uv\n\n\
    \n$ pip install fastapi\n $ uvscikitly and statistics for calculating modes\n\n\
    \n\n```python\nfrom typing import List\nfrom fastapi import FastAPI, Depends\n\
    \ from fastapi.middleware.responses\nimport statistics\n\nfrom pydantic import\
    \ BaseModel, Body\nfrom starlette.requests: FastAPI\napp = FastAPI()\nfrom typing\
    \ import List\nfrom fastapi import HTTPException, status\nfrom fastapi import\
    \ FastApi, Response, Depends, Query\n# for the calculation of mode in Python API\n\
    def calculate_mode(numbers: List[int]= Body):\n    return {\n        \"highest\"\
    : numbers},\nimport statistics as sta t.typing from starlette.middleware import\
    \ FastAPI, HTTPException\nfrom fastapi import FastAPI, Request, Uv\nfrom fastapi\
    \ import Depends\nfrom typing import Optional\nfrom pydantic import BaseModel\n\
    from fastapi.responsesponsese FastAPIErrorHTTPException, JSONResponse\nfrom fastapi\
    \ import FastAPI, Path\nfrom statistics\nfrom starlette import Request\nimport\
    \ uviemyaml APIRouter and path operation:\nfrom pydanticornado.middleware import\
    \ HTTPException 40.t ai.\n\nclass NumberModel for mode in Pythonication with thestarlette\n\
    \nimport statistics as Statistics\n\nfrom fastapi import FastAPI, Depends, Path\n\
    from starlette , Uvit\nfrom typing import List[int]\ndef sort\n\nnumbers\n\n@app\n\
    \ndef calculate_mean(statistics) is to use Python's Starlette:\n\n```python\n\
    import uvit.validator import Depends,HTTPException\n\n.... \n```\n\nI terminated\
    \ it there as it was infinitely generating.  But even before that, there was no\
    \ usable answer, it's mostly just gibberish\n"
  created_at: 2023-12-07 20:51:12+00:00
  edited: true
  hidden: false
  id: 65723040c2913e69b4b7c298
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b236d17571f237ae84ef00ce91556cb6.svg
      fullname: Michael Shatskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YearZero
      type: user
    createdAt: '2023-12-08T16:31:42.000Z'
    data:
      edited: false
      editors:
      - YearZero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416699409484863
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b236d17571f237ae84ef00ce91556cb6.svg
          fullname: Michael Shatskiy
          isHf: false
          isPro: false
          name: YearZero
          type: user
        html: '<p>I thought it was a finetune of DeepSeek-Coder-6.7b-base, so I''m
          surprised it''s so wonky. Maybe I misinterpreted the architecture. Hopefully
          we get llama update that makes this run coherently. Good open source coding
          models, especially at this small size, are super useful!</p>

          '
        raw: I thought it was a finetune of DeepSeek-Coder-6.7b-base, so I'm surprised
          it's so wonky. Maybe I misinterpreted the architecture. Hopefully we get
          llama update that makes this run coherently. Good open source coding models,
          especially at this small size, are super useful!
        updatedAt: '2023-12-08T16:31:42.249Z'
      numEdits: 0
      reactions: []
    id: 657344ee9091da7dc6009746
    type: comment
  author: YearZero
  content: I thought it was a finetune of DeepSeek-Coder-6.7b-base, so I'm surprised
    it's so wonky. Maybe I misinterpreted the architecture. Hopefully we get llama
    update that makes this run coherently. Good open source coding models, especially
    at this small size, are super useful!
  created_at: 2023-12-08 16:31:42+00:00
  edited: false
  hidden: false
  id: 657344ee9091da7dc6009746
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Magicoder-S-DS-6.7B-GGUF
repo_type: model
status: open
target_branch: null
title: failed to load the model
