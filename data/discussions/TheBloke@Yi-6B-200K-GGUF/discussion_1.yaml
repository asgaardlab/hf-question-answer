!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CR2022
conflicting_files: null
created_at: 2023-11-10 21:16:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-10T21:16:10.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46743762493133545
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>2023-11-10 22:04:01 ERROR:Failed to load the model.<br>Traceback
          (most recent call last):<br>File "K:\AI\text-generation-webui\modules\ui_model_menu.py",
          line 210, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File "K:\AI\text-generation-webui\modules\models.py",
          line 85, in load_model<br>output = load_func_maploader<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File
          "K:\AI\text-generation-webui\modules\models.py", line 242, in llamacpp_loader<br>model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File
          "K:\AI\text-generation-webui\modules\llamacpp_model.py", line 91, in from_pretrained<br>result.model
          = Llama(**params)<br>^^^^^^^^^^^^^^^<br>File "K:\AI\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp\llama.py",
          line 422, in init<br>self.scores: npt.NDArray[np.single] = np.ndarray(<br>^^^^^^^^^^^<br>numpy.core._exceptions._ArrayMemoryError:
          Unable to allocate 47.7 GiB for an array with shape (200000, 64000) and
          data type float32</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x000002266B0CA340&gt;<br>Traceback
          (most recent call last):<br>File "K:\AI\text-generation-webui\modules\llamacpp_model.py",
          line 49, in del<br>self.model.del()<br>^^^^^^^^^^<br>Models with these large
          context size are an amazing idea but if they do not work with a 128GB system
          ram then I wonder what amount of ram they do need in order to work.</p>

          <p>I tried both the 6B and 32B models with 200K context both throw same
          error it is weird how they can run out of memory when my task manager in
          windows 11 show they are not even close to reaching that amount of ram usage.</p>

          '
        raw: "2023-11-10 22:04:01 ERROR:Failed to load the model.\r\nTraceback (most\
          \ recent call last):\r\nFile \"K:\\AI\\text-generation-webui\\modules\\\
          ui_model_menu.py\", line 210, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nFile \"K:\\AI\\text-generation-webui\\modules\\models.py\", line 85, in\
          \ load_model\r\noutput = load_func_maploader\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nFile \"K:\\AI\\text-generation-webui\\modules\\models.py\", line 242,\
          \ in llamacpp_loader\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 91, in from_pretrained\r\nresult.model\
          \ = Llama(**params)\r\n^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\llama_cpp\\llama.py\", line 422,\
          \ in init\r\nself.scores: npt.NDArray[np.single] = np.ndarray(\r\n^^^^^^^^^^^\r\
          \nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 47.7 GiB\
          \ for an array with shape (200000, 64000) and data type float32\r\n\r\n\
          Exception ignored in: <function LlamaCppModel.__del__ at 0x000002266B0CA340>\r\
          \nTraceback (most recent call last):\r\nFile \"K:\\AI\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 49, in del\r\nself.model.del()\r\n^^^^^^^^^^\r\
          \nModels with these large context size are an amazing idea but if they do\
          \ not work with a 128GB system ram then I wonder what amount of ram they\
          \ do need in order to work.\r\n\r\nI tried both the 6B and 32B models with\
          \ 200K context both throw same error it is weird how they can run out of\
          \ memory when my task manager in windows 11 show they are not even close\
          \ to reaching that amount of ram usage."
        updatedAt: '2023-11-10T21:16:10.825Z'
      numEdits: 0
      reactions: []
    id: 654e9d9a19c62ea90ff6f63b
    type: comment
  author: CR2022
  content: "2023-11-10 22:04:01 ERROR:Failed to load the model.\r\nTraceback (most\
    \ recent call last):\r\nFile \"K:\\AI\\text-generation-webui\\modules\\ui_model_menu.py\"\
    , line 210, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\\
    modules\\models.py\", line 85, in load_model\r\noutput = load_func_maploader\r\
    \n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\\
    modules\\models.py\", line 242, in llamacpp_loader\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 91, in from_pretrained\r\nresult.model = Llama(**params)\r\
    \n^^^^^^^^^^^^^^^\r\nFile \"K:\\AI\\text-generation-webui\\installer_files\\env\\\
    Lib\\site-packages\\llama_cpp\\llama.py\", line 422, in init\r\nself.scores: npt.NDArray[np.single]\
    \ = np.ndarray(\r\n^^^^^^^^^^^\r\nnumpy.core._exceptions._ArrayMemoryError: Unable\
    \ to allocate 47.7 GiB for an array with shape (200000, 64000) and data type float32\r\
    \n\r\nException ignored in: <function LlamaCppModel.__del__ at 0x000002266B0CA340>\r\
    \nTraceback (most recent call last):\r\nFile \"K:\\AI\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 49, in del\r\nself.model.del()\r\n^^^^^^^^^^\r\
    \nModels with these large context size are an amazing idea but if they do not\
    \ work with a 128GB system ram then I wonder what amount of ram they do need in\
    \ order to work.\r\n\r\nI tried both the 6B and 32B models with 200K context both\
    \ throw same error it is weird how they can run out of memory when my task manager\
    \ in windows 11 show they are not even close to reaching that amount of ram usage."
  created_at: 2023-11-10 21:16:10+00:00
  edited: false
  hidden: false
  id: 654e9d9a19c62ea90ff6f63b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-10T21:20:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560112953186035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That looks like a Python specific error, not llama.cpp itself.  The
          error is coming from numpy, which I assume is called by llama-cpp-python
          - the Python library that provides llama.cpp inference.</p>

          <p>I don''t know why you''re getting that error specifically, given you
          have plenty of RAM - it''s failing at 47GB apparently, well before your
          RAM limit.  I have seen issues before on Windows where you need to allocate
          a large pagefile, even if you have plenty of RAM. So you could try allocating
          a 128GB Page File and see if that helps.</p>

          <p>If that doesn''t help, please raise it as an issue on the llama-cpp-python
          Github, and maybe they can help.</p>

          <p>In the meantime you might want to try using llama.cpp directly, either
          via the command line or via its server which provides an API you can hit
          from Python code.  That won''t suffer from this issue and will be able to
          use your full system RAM (and some GPU VRAM as well, if you have a suitable
          GPU and want to use it) so you can see how much context you can get.</p>

          '
        raw: 'That looks like a Python specific error, not llama.cpp itself.  The
          error is coming from numpy, which I assume is called by llama-cpp-python
          - the Python library that provides llama.cpp inference.


          I don''t know why you''re getting that error specifically, given you have
          plenty of RAM - it''s failing at 47GB apparently, well before your RAM limit.  I
          have seen issues before on Windows where you need to allocate a large pagefile,
          even if you have plenty of RAM. So you could try allocating a 128GB Page
          File and see if that helps.


          If that doesn''t help, please raise it as an issue on the llama-cpp-python
          Github, and maybe they can help.


          In the meantime you might want to try using llama.cpp directly, either via
          the command line or via its server which provides an API you can hit from
          Python code.  That won''t suffer from this issue and will be able to use
          your full system RAM (and some GPU VRAM as well, if you have a suitable
          GPU and want to use it) so you can see how much context you can get.'
        updatedAt: '2023-11-10T21:20:40.486Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - CR2022
    id: 654e9ea8f908d2479a265482
    type: comment
  author: TheBloke
  content: 'That looks like a Python specific error, not llama.cpp itself.  The error
    is coming from numpy, which I assume is called by llama-cpp-python - the Python
    library that provides llama.cpp inference.


    I don''t know why you''re getting that error specifically, given you have plenty
    of RAM - it''s failing at 47GB apparently, well before your RAM limit.  I have
    seen issues before on Windows where you need to allocate a large pagefile, even
    if you have plenty of RAM. So you could try allocating a 128GB Page File and see
    if that helps.


    If that doesn''t help, please raise it as an issue on the llama-cpp-python Github,
    and maybe they can help.


    In the meantime you might want to try using llama.cpp directly, either via the
    command line or via its server which provides an API you can hit from Python code.  That
    won''t suffer from this issue and will be able to use your full system RAM (and
    some GPU VRAM as well, if you have a suitable GPU and want to use it) so you can
    see how much context you can get.'
  created_at: 2023-11-10 21:20:40+00:00
  edited: false
  hidden: false
  id: 654e9ea8f908d2479a265482
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-11T01:15:08.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7261915802955627
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cr2022&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cr2022\">@<span class=\"\
          underline\">cr2022</span></a></span>\n\n\t</span></span> You must be either\
          \ setting the context size to 200K or maybe letting it default to that.\
          \ I ran the 34B 200K on my 64GB system without issues. But see below: there's\
          \ an issue with these models.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I generated the 34B earlier today and found that the token id metadata\
          \ wasn't getting correctly read. There's a fix in <a rel=\"nofollow\" href=\"\
          https://github.com/ggerganov/llama.cpp/pull/3981\">https://github.com/ggerganov/llama.cpp/pull/3981</a></p>\n\
          <p>It also contains a script that will let you dump metadata and see the\
          \ problem:</p>\n<pre><code class=\"language-plaintext\">$ python gguf-py/scripts/gguf-dump.py\
          \ yi-6b-200k.Q4_K_M.gguf\n\n* Loading: yi-6b-200k.Q4_K_M.gguf\n* File is\
          \ LITTLE endian, script is running on a LITTLE endian host.\n\n* Dumping\
          \ 20 key/value pair(s)\n      1: UINT32     |        1 | GGUF.version =\
          \ 3\n      2: UINT64     |        1 | GGUF.tensor_count = 291\n      3:\
          \ UINT64     |        1 | GGUF.kv_count = 17\n      4: STRING     |    \
          \    1 | general.architecture = 'llama'\n      5: STRING     |        1\
          \ | general.name = '01-ai_yi-6b-200k'\n      6: UINT32     |        1 |\
          \ llama.context_length = 200000\n      7: UINT32     |        1 | llama.embedding_length\
          \ = 4096\n      8: UINT32     |        1 | llama.block_count = 32\n    \
          \  9: UINT32     |        1 | llama.feed_forward_length = 11008\n     10:\
          \ UINT32     |        1 | llama.rope.dimension_count = 128\n     11: UINT32\
          \     |        1 | llama.attention.head_count = 32\n     12: UINT32    \
          \ |        1 | llama.attention.head_count_kv = 4\n     13: FLOAT32    |\
          \        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\n\
          \     14: FLOAT32    |        1 | llama.rope.freq_base = 5000000.0\n   \
          \  15: UINT32     |        1 | general.file_type = 15\n     16: STRING \
          \    |        1 | tokenizer.ggml.model = 'llama'\n     17: [STRING]   |\
          \    64000 | tokenizer.ggml.tokens\n     18: [FLOAT32]  |    64000 | tokenizer.ggml.scores\n\
          \     19: [INT32]    |    64000 | tokenizer.ggml.token_type\n     20: UINT32\
          \     |        1 | general.quantization_version = 2\n</code></pre>\n<p>There's\
          \ no <code>tokenizer.ggml.bos_token_id</code>, etc. I believe the metadata\
          \ for the non-200K models matches, so that also would be a problem for those\
          \ (but I think it used to be different since I generated models that did\
          \ have that metadata).</p>\n"
        raw: "@cr2022 You must be either setting the context size to 200K or maybe\
          \ letting it default to that. I ran the 34B 200K on my 64GB system without\
          \ issues. But see below: there's an issue with these models.\n\n@TheBloke\
          \ I generated the 34B earlier today and found that the token id metadata\
          \ wasn't getting correctly read. There's a fix in https://github.com/ggerganov/llama.cpp/pull/3981\n\
          \nIt also contains a script that will let you dump metadata and see the\
          \ problem:\n\n```plaintext\n$ python gguf-py/scripts/gguf-dump.py yi-6b-200k.Q4_K_M.gguf\n\
          \n* Loading: yi-6b-200k.Q4_K_M.gguf\n* File is LITTLE endian, script is\
          \ running on a LITTLE endian host.\n\n* Dumping 20 key/value pair(s)\n \
          \     1: UINT32     |        1 | GGUF.version = 3\n      2: UINT64     |\
          \        1 | GGUF.tensor_count = 291\n      3: UINT64     |        1 | GGUF.kv_count\
          \ = 17\n      4: STRING     |        1 | general.architecture = 'llama'\n\
          \      5: STRING     |        1 | general.name = '01-ai_yi-6b-200k'\n  \
          \    6: UINT32     |        1 | llama.context_length = 200000\n      7:\
          \ UINT32     |        1 | llama.embedding_length = 4096\n      8: UINT32\
          \     |        1 | llama.block_count = 32\n      9: UINT32     |       \
          \ 1 | llama.feed_forward_length = 11008\n     10: UINT32     |        1\
          \ | llama.rope.dimension_count = 128\n     11: UINT32     |        1 | llama.attention.head_count\
          \ = 32\n     12: UINT32     |        1 | llama.attention.head_count_kv =\
          \ 4\n     13: FLOAT32    |        1 | llama.attention.layer_norm_rms_epsilon\
          \ = 9.999999747378752e-06\n     14: FLOAT32    |        1 | llama.rope.freq_base\
          \ = 5000000.0\n     15: UINT32     |        1 | general.file_type = 15\n\
          \     16: STRING     |        1 | tokenizer.ggml.model = 'llama'\n     17:\
          \ [STRING]   |    64000 | tokenizer.ggml.tokens\n     18: [FLOAT32]  | \
          \   64000 | tokenizer.ggml.scores\n     19: [INT32]    |    64000 | tokenizer.ggml.token_type\n\
          \     20: UINT32     |        1 | general.quantization_version = 2\n```\n\
          \nThere's no `tokenizer.ggml.bos_token_id`, etc. I believe the metadata\
          \ for the non-200K models matches, so that also would be a problem for those\
          \ (but I think it used to be different since I generated models that did\
          \ have that metadata)."
        updatedAt: '2023-11-11T01:15:08.245Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Hanssep123
        - CR2022
    id: 654ed59c4c6f4e3300eb9abd
    type: comment
  author: KerfuffleV2
  content: "@cr2022 You must be either setting the context size to 200K or maybe letting\
    \ it default to that. I ran the 34B 200K on my 64GB system without issues. But\
    \ see below: there's an issue with these models.\n\n@TheBloke I generated the\
    \ 34B earlier today and found that the token id metadata wasn't getting correctly\
    \ read. There's a fix in https://github.com/ggerganov/llama.cpp/pull/3981\n\n\
    It also contains a script that will let you dump metadata and see the problem:\n\
    \n```plaintext\n$ python gguf-py/scripts/gguf-dump.py yi-6b-200k.Q4_K_M.gguf\n\
    \n* Loading: yi-6b-200k.Q4_K_M.gguf\n* File is LITTLE endian, script is running\
    \ on a LITTLE endian host.\n\n* Dumping 20 key/value pair(s)\n      1: UINT32\
    \     |        1 | GGUF.version = 3\n      2: UINT64     |        1 | GGUF.tensor_count\
    \ = 291\n      3: UINT64     |        1 | GGUF.kv_count = 17\n      4: STRING\
    \     |        1 | general.architecture = 'llama'\n      5: STRING     |     \
    \   1 | general.name = '01-ai_yi-6b-200k'\n      6: UINT32     |        1 | llama.context_length\
    \ = 200000\n      7: UINT32     |        1 | llama.embedding_length = 4096\n \
    \     8: UINT32     |        1 | llama.block_count = 32\n      9: UINT32     |\
    \        1 | llama.feed_forward_length = 11008\n     10: UINT32     |        1\
    \ | llama.rope.dimension_count = 128\n     11: UINT32     |        1 | llama.attention.head_count\
    \ = 32\n     12: UINT32     |        1 | llama.attention.head_count_kv = 4\n \
    \    13: FLOAT32    |        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\n\
    \     14: FLOAT32    |        1 | llama.rope.freq_base = 5000000.0\n     15: UINT32\
    \     |        1 | general.file_type = 15\n     16: STRING     |        1 | tokenizer.ggml.model\
    \ = 'llama'\n     17: [STRING]   |    64000 | tokenizer.ggml.tokens\n     18:\
    \ [FLOAT32]  |    64000 | tokenizer.ggml.scores\n     19: [INT32]    |    64000\
    \ | tokenizer.ggml.token_type\n     20: UINT32     |        1 | general.quantization_version\
    \ = 2\n```\n\nThere's no `tokenizer.ggml.bos_token_id`, etc. I believe the metadata\
    \ for the non-200K models matches, so that also would be a problem for those (but\
    \ I think it used to be different since I generated models that did have that\
    \ metadata)."
  created_at: 2023-11-11 01:15:08+00:00
  edited: false
  hidden: false
  id: 654ed59c4c6f4e3300eb9abd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-11T12:37:17.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952354907989502
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <p>That looks like a Python specific error, not llama.cpp itself.  The error
          is coming from numpy, which I assume is called by llama-cpp-python - the
          Python library that provides llama.cpp inference.</p>

          <p>I don''t know why you''re getting that error specifically, given you
          have plenty of RAM - it''s failing at 47GB apparently, well before your
          RAM limit.  I have seen issues before on Windows where you need to allocate
          a large pagefile, even if you have plenty of RAM. So you could try allocating
          a 128GB Page File and see if that helps.</p>

          <p>If that doesn''t help, please raise it as an issue on the llama-cpp-python
          Github, and maybe they can help.</p>

          <p>In the meantime you might want to try using llama.cpp directly, either
          via the command line or via its server which provides an API you can hit
          from Python code.  That won''t suffer from this issue and will be able to
          use your full system RAM (and some GPU VRAM as well, if you have a suitable
          GPU and want to use it) so you can see how much context you can get.</p>

          </blockquote>

          <p>Thank you for your detailed reply and the suggestions :)</p>

          '
        raw: "> That looks like a Python specific error, not llama.cpp itself.  The\
          \ error is coming from numpy, which I assume is called by llama-cpp-python\
          \ - the Python library that provides llama.cpp inference.\n> \n> I don't\
          \ know why you're getting that error specifically, given you have plenty\
          \ of RAM - it's failing at 47GB apparently, well before your RAM limit.\
          \  I have seen issues before on Windows where you need to allocate a large\
          \ pagefile, even if you have plenty of RAM. So you could try allocating\
          \ a 128GB Page File and see if that helps.\n> \n> If that doesn't help,\
          \ please raise it as an issue on the llama-cpp-python Github, and maybe\
          \ they can help.\n> \n> In the meantime you might want to try using llama.cpp\
          \ directly, either via the command line or via its server which provides\
          \ an API you can hit from Python code.  That won't suffer from this issue\
          \ and will be able to use your full system RAM (and some GPU VRAM as well,\
          \ if you have a suitable GPU and want to use it) so you can see how much\
          \ context you can get.\n\nThank you for your detailed reply and the suggestions\
          \ :)"
        updatedAt: '2023-11-11T12:37:17.473Z'
      numEdits: 0
      reactions: []
    id: 654f757dfd13d5b5ec566a2d
    type: comment
  author: CR2022
  content: "> That looks like a Python specific error, not llama.cpp itself.  The\
    \ error is coming from numpy, which I assume is called by llama-cpp-python - the\
    \ Python library that provides llama.cpp inference.\n> \n> I don't know why you're\
    \ getting that error specifically, given you have plenty of RAM - it's failing\
    \ at 47GB apparently, well before your RAM limit.  I have seen issues before on\
    \ Windows where you need to allocate a large pagefile, even if you have plenty\
    \ of RAM. So you could try allocating a 128GB Page File and see if that helps.\n\
    > \n> If that doesn't help, please raise it as an issue on the llama-cpp-python\
    \ Github, and maybe they can help.\n> \n> In the meantime you might want to try\
    \ using llama.cpp directly, either via the command line or via its server which\
    \ provides an API you can hit from Python code.  That won't suffer from this issue\
    \ and will be able to use your full system RAM (and some GPU VRAM as well, if\
    \ you have a suitable GPU and want to use it) so you can see how much context\
    \ you can get.\n\nThank you for your detailed reply and the suggestions :)"
  created_at: 2023-11-11 12:37:17+00:00
  edited: false
  hidden: false
  id: 654f757dfd13d5b5ec566a2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-11T12:38:33.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7209627628326416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;cr2022&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cr2022\"\
          >@<span class=\"underline\">cr2022</span></a></span>\n\n\t</span></span>\
          \ You must be either setting the context size to 200K or maybe letting it\
          \ default to that. I ran the 34B 200K on my 64GB system without issues.\
          \ But see below: there's an issue with these models.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> I generated the 34B earlier today and found that the\
          \ token id metadata wasn't getting correctly read. There's a fix in <a rel=\"\
          nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/3981\">https://github.com/ggerganov/llama.cpp/pull/3981</a></p>\n\
          <p>It also contains a script that will let you dump metadata and see the\
          \ problem:</p>\n<pre><code class=\"language-plaintext\">$ python gguf-py/scripts/gguf-dump.py\
          \ yi-6b-200k.Q4_K_M.gguf\n\n* Loading: yi-6b-200k.Q4_K_M.gguf\n* File is\
          \ LITTLE endian, script is running on a LITTLE endian host.\n\n* Dumping\
          \ 20 key/value pair(s)\n      1: UINT32     |        1 | GGUF.version =\
          \ 3\n      2: UINT64     |        1 | GGUF.tensor_count = 291\n      3:\
          \ UINT64     |        1 | GGUF.kv_count = 17\n      4: STRING     |    \
          \    1 | general.architecture = 'llama'\n      5: STRING     |        1\
          \ | general.name = '01-ai_yi-6b-200k'\n      6: UINT32     |        1 |\
          \ llama.context_length = 200000\n      7: UINT32     |        1 | llama.embedding_length\
          \ = 4096\n      8: UINT32     |        1 | llama.block_count = 32\n    \
          \  9: UINT32     |        1 | llama.feed_forward_length = 11008\n     10:\
          \ UINT32     |        1 | llama.rope.dimension_count = 128\n     11: UINT32\
          \     |        1 | llama.attention.head_count = 32\n     12: UINT32    \
          \ |        1 | llama.attention.head_count_kv = 4\n     13: FLOAT32    |\
          \        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\n\
          \     14: FLOAT32    |        1 | llama.rope.freq_base = 5000000.0\n   \
          \  15: UINT32     |        1 | general.file_type = 15\n     16: STRING \
          \    |        1 | tokenizer.ggml.model = 'llama'\n     17: [STRING]   |\
          \    64000 | tokenizer.ggml.tokens\n     18: [FLOAT32]  |    64000 | tokenizer.ggml.scores\n\
          \     19: [INT32]    |    64000 | tokenizer.ggml.token_type\n     20: UINT32\
          \     |        1 | general.quantization_version = 2\n</code></pre>\n<p>There's\
          \ no <code>tokenizer.ggml.bos_token_id</code>, etc. I believe the metadata\
          \ for the non-200K models matches, so that also would be a problem for those\
          \ (but I think it used to be different since I generated models that did\
          \ have that metadata).</p>\n</blockquote>\n<p>Thank you for the reply I\
          \ will look into it :)</p>\n"
        raw: "> @cr2022 You must be either setting the context size to 200K or maybe\
          \ letting it default to that. I ran the 34B 200K on my 64GB system without\
          \ issues. But see below: there's an issue with these models.\n> \n> @TheBloke\
          \ I generated the 34B earlier today and found that the token id metadata\
          \ wasn't getting correctly read. There's a fix in https://github.com/ggerganov/llama.cpp/pull/3981\n\
          > \n> It also contains a script that will let you dump metadata and see\
          \ the problem:\n> \n> ```plaintext\n> $ python gguf-py/scripts/gguf-dump.py\
          \ yi-6b-200k.Q4_K_M.gguf\n> \n> * Loading: yi-6b-200k.Q4_K_M.gguf\n> * File\
          \ is LITTLE endian, script is running on a LITTLE endian host.\n> \n> *\
          \ Dumping 20 key/value pair(s)\n>       1: UINT32     |        1 | GGUF.version\
          \ = 3\n>       2: UINT64     |        1 | GGUF.tensor_count = 291\n>   \
          \    3: UINT64     |        1 | GGUF.kv_count = 17\n>       4: STRING  \
          \   |        1 | general.architecture = 'llama'\n>       5: STRING     |\
          \        1 | general.name = '01-ai_yi-6b-200k'\n>       6: UINT32     |\
          \        1 | llama.context_length = 200000\n>       7: UINT32     |    \
          \    1 | llama.embedding_length = 4096\n>       8: UINT32     |        1\
          \ | llama.block_count = 32\n>       9: UINT32     |        1 | llama.feed_forward_length\
          \ = 11008\n>      10: UINT32     |        1 | llama.rope.dimension_count\
          \ = 128\n>      11: UINT32     |        1 | llama.attention.head_count =\
          \ 32\n>      12: UINT32     |        1 | llama.attention.head_count_kv =\
          \ 4\n>      13: FLOAT32    |        1 | llama.attention.layer_norm_rms_epsilon\
          \ = 9.999999747378752e-06\n>      14: FLOAT32    |        1 | llama.rope.freq_base\
          \ = 5000000.0\n>      15: UINT32     |        1 | general.file_type = 15\n\
          >      16: STRING     |        1 | tokenizer.ggml.model = 'llama'\n>   \
          \   17: [STRING]   |    64000 | tokenizer.ggml.tokens\n>      18: [FLOAT32]\
          \  |    64000 | tokenizer.ggml.scores\n>      19: [INT32]    |    64000\
          \ | tokenizer.ggml.token_type\n>      20: UINT32     |        1 | general.quantization_version\
          \ = 2\n> ```\n> \n> There's no `tokenizer.ggml.bos_token_id`, etc. I believe\
          \ the metadata for the non-200K models matches, so that also would be a\
          \ problem for those (but I think it used to be different since I generated\
          \ models that did have that metadata).\n\nThank you for the reply I will\
          \ look into it :)"
        updatedAt: '2023-11-11T12:38:33.453Z'
      numEdits: 0
      reactions: []
      relatedEventId: 654f75c91e13b38aec976d5c
    id: 654f75c91e13b38aec976d5b
    type: comment
  author: CR2022
  content: "> @cr2022 You must be either setting the context size to 200K or maybe\
    \ letting it default to that. I ran the 34B 200K on my 64GB system without issues.\
    \ But see below: there's an issue with these models.\n> \n> @TheBloke I generated\
    \ the 34B earlier today and found that the token id metadata wasn't getting correctly\
    \ read. There's a fix in https://github.com/ggerganov/llama.cpp/pull/3981\n> \n\
    > It also contains a script that will let you dump metadata and see the problem:\n\
    > \n> ```plaintext\n> $ python gguf-py/scripts/gguf-dump.py yi-6b-200k.Q4_K_M.gguf\n\
    > \n> * Loading: yi-6b-200k.Q4_K_M.gguf\n> * File is LITTLE endian, script is\
    \ running on a LITTLE endian host.\n> \n> * Dumping 20 key/value pair(s)\n>  \
    \     1: UINT32     |        1 | GGUF.version = 3\n>       2: UINT64     |   \
    \     1 | GGUF.tensor_count = 291\n>       3: UINT64     |        1 | GGUF.kv_count\
    \ = 17\n>       4: STRING     |        1 | general.architecture = 'llama'\n> \
    \      5: STRING     |        1 | general.name = '01-ai_yi-6b-200k'\n>       6:\
    \ UINT32     |        1 | llama.context_length = 200000\n>       7: UINT32   \
    \  |        1 | llama.embedding_length = 4096\n>       8: UINT32     |       \
    \ 1 | llama.block_count = 32\n>       9: UINT32     |        1 | llama.feed_forward_length\
    \ = 11008\n>      10: UINT32     |        1 | llama.rope.dimension_count = 128\n\
    >      11: UINT32     |        1 | llama.attention.head_count = 32\n>      12:\
    \ UINT32     |        1 | llama.attention.head_count_kv = 4\n>      13: FLOAT32\
    \    |        1 | llama.attention.layer_norm_rms_epsilon = 9.999999747378752e-06\n\
    >      14: FLOAT32    |        1 | llama.rope.freq_base = 5000000.0\n>      15:\
    \ UINT32     |        1 | general.file_type = 15\n>      16: STRING     |    \
    \    1 | tokenizer.ggml.model = 'llama'\n>      17: [STRING]   |    64000 | tokenizer.ggml.tokens\n\
    >      18: [FLOAT32]  |    64000 | tokenizer.ggml.scores\n>      19: [INT32] \
    \   |    64000 | tokenizer.ggml.token_type\n>      20: UINT32     |        1 |\
    \ general.quantization_version = 2\n> ```\n> \n> There's no `tokenizer.ggml.bos_token_id`,\
    \ etc. I believe the metadata for the non-200K models matches, so that also would\
    \ be a problem for those (but I think it used to be different since I generated\
    \ models that did have that metadata).\n\nThank you for the reply I will look\
    \ into it :)"
  created_at: 2023-11-11 12:38:33+00:00
  edited: false
  hidden: false
  id: 654f75c91e13b38aec976d5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-11T12:38:33.000Z'
    data:
      status: closed
    id: 654f75c91e13b38aec976d5c
    type: status-change
  author: CR2022
  created_at: 2023-11-11 12:38:33+00:00
  id: 654f75c91e13b38aec976d5c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-11T13:53:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6744344830513
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;KerfuffleV2&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KerfuffleV2\"\
          >@<span class=\"underline\">KerfuffleV2</span></a></span>\n\n\t</span></span>\
          \ - I am re-generating 6B-200K and 34B-200K GGUFs with the latest llama.cpp,\
          \ with your refactored gguf-py.py code.</p>\n"
        raw: Thank you @KerfuffleV2 - I am re-generating 6B-200K and 34B-200K GGUFs
          with the latest llama.cpp, with your refactored gguf-py.py code.
        updatedAt: '2023-11-11T13:53:08.073Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Hanssep123
    id: 654f874419c62ea90f1711ea
    type: comment
  author: TheBloke
  content: Thank you @KerfuffleV2 - I am re-generating 6B-200K and 34B-200K GGUFs
    with the latest llama.cpp, with your refactored gguf-py.py code.
  created_at: 2023-11-11 13:53:08+00:00
  edited: false
  hidden: false
  id: 654f874419c62ea90f1711ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-11T13:55:00.000Z'
    data:
      status: open
    id: 654f87b41b20e97bf6dd5bed
    type: status-change
  author: TheBloke
  created_at: 2023-11-11 13:55:00+00:00
  id: 654f87b41b20e97bf6dd5bed
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-11-11T15:30:33.000Z'
    data:
      edited: false
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9126783609390259
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> No problem. You\
          \ might also want t o double check the non-200K ones to make sure they have\
          \ <code>tokenizer.ggml.bos_token_id</code> and <code>tokenizer.ggml.eos_token_id</code>.</p>\n\
          <p>Another thing you may want to do is set <code>tokenizer.ggml.bos_token_id</code>\
          \ to <code>2</code>: llama.cpp doesn't respect <code>add_bos_token</code>\
          \ in the HF metadata so it always adds the BOS token. However, those models\
          \ weren't trained with an initial BOS token so it really hurts their quality,\
          \ but an EOS seems (mostly) OK. With the refactor pull we now will add the\
          \ flag of whether to add BOS/EOS to the GGUF metadata but nothing actually\
          \ uses it yet so the BOS still gets added.</p>\n<pre><code>python gguf-py/scripts/gguf-set-metadata.py\
          \ model-filename.gguf tokenizer.ggml.bos_token_id 2\n</code></pre>\n<p>The\
          \ only downside I can think of is if someone wants to fine-tune based off\
          \ those GGUF models and somehow needs the BOS id to be the original value,\
          \ but that seems fairly unlikely.</p>\n"
        raw: "@TheBloke No problem. You might also want t o double check the non-200K\
          \ ones to make sure they have `tokenizer.ggml.bos_token_id` and `tokenizer.ggml.eos_token_id`.\n\
          \nAnother thing you may want to do is set `tokenizer.ggml.bos_token_id`\
          \ to `2`: llama.cpp doesn't respect `add_bos_token` in the HF metadata so\
          \ it always adds the BOS token. However, those models weren't trained with\
          \ an initial BOS token so it really hurts their quality, but an EOS seems\
          \ (mostly) OK. With the refactor pull we now will add the flag of whether\
          \ to add BOS/EOS to the GGUF metadata but nothing actually uses it yet so\
          \ the BOS still gets added.\n\n    python gguf-py/scripts/gguf-set-metadata.py\
          \ model-filename.gguf tokenizer.ggml.bos_token_id 2\n\nThe only downside\
          \ I can think of is if someone wants to fine-tune based off those GGUF models\
          \ and somehow needs the BOS id to be the original value, but that seems\
          \ fairly unlikely."
        updatedAt: '2023-11-11T15:30:33.776Z'
      numEdits: 0
      reactions: []
    id: 654f9e192adb0688a0d2e8bb
    type: comment
  author: KerfuffleV2
  content: "@TheBloke No problem. You might also want t o double check the non-200K\
    \ ones to make sure they have `tokenizer.ggml.bos_token_id` and `tokenizer.ggml.eos_token_id`.\n\
    \nAnother thing you may want to do is set `tokenizer.ggml.bos_token_id` to `2`:\
    \ llama.cpp doesn't respect `add_bos_token` in the HF metadata so it always adds\
    \ the BOS token. However, those models weren't trained with an initial BOS token\
    \ so it really hurts their quality, but an EOS seems (mostly) OK. With the refactor\
    \ pull we now will add the flag of whether to add BOS/EOS to the GGUF metadata\
    \ but nothing actually uses it yet so the BOS still gets added.\n\n    python\
    \ gguf-py/scripts/gguf-set-metadata.py model-filename.gguf tokenizer.ggml.bos_token_id\
    \ 2\n\nThe only downside I can think of is if someone wants to fine-tune based\
    \ off those GGUF models and somehow needs the BOS id to be the original value,\
    \ but that seems fairly unlikely."
  created_at: 2023-11-11 15:30:33+00:00
  edited: false
  hidden: false
  id: 654f9e192adb0688a0d2e8bb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Yi-6B-200K-GGUF
repo_type: model
status: open
target_branch: null
title: 'Can there be a 64K model? I have 128GB of system ram but this model is throwing
  errors in the latest text generation webui:'
