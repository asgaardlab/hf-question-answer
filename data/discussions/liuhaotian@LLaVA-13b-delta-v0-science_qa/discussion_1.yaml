!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Powback
conflicting_files: null
created_at: 2023-04-28 02:13:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/712984751c435c7dd7d2d13ac672b367.svg
      fullname: Mats B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Powback
      type: user
    createdAt: '2023-04-28T03:13:20.000Z'
    data:
      edited: true
      editors:
      - Powback
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/712984751c435c7dd7d2d13ac672b367.svg
          fullname: Mats B
          isHf: false
          isPro: false
          name: Powback
          type: user
        html: '<p>I found that this model consistently gives bad responses compared
          to the LLaVA-13b-delta-v0 model. </p>

          <p>LLaVA-13b-delta-v0 consistently provides an accurate response to this
          question:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/u5W912qWEEmctrZQiYl7l.png"><img
          alt="chrome_UPvlDpnaUX.png" src="https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/u5W912qWEEmctrZQiYl7l.png"></a><br>LLaVA-13b-delta-v0-science_qa
          consistently provides inaccurate responses:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/bpDSDCM27YhTXdWKm9sGP.png"><img
          alt="chrome_TeTKtddUC0.png" src="https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/bpDSDCM27YhTXdWKm9sGP.png"></a></p>

          <p>This is on 4bit 128g models.</p>

          <p>The same applies to text recognition capabilities. Science_qa does get
          it right as well, just not as good.</p>

          '
        raw: "I found that this model consistently gives bad responses compared to\
          \ the LLaVA-13b-delta-v0 model. \n\nLLaVA-13b-delta-v0 consistently provides\
          \ an accurate response to this question:\n![chrome_UPvlDpnaUX.png](https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/u5W912qWEEmctrZQiYl7l.png)\n\
          LLaVA-13b-delta-v0-science_qa consistently provides inaccurate responses:\n\
          ![chrome_TeTKtddUC0.png](https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/bpDSDCM27YhTXdWKm9sGP.png)\n\
          \nThis is on 4bit 128g models.\n\nThe same applies to text recognition capabilities.\
          \ Science_qa does get it right as well, just not as good."
        updatedAt: '2023-04-28T03:24:45.237Z'
      numEdits: 2
      reactions: []
    id: 644b39d0958b7796980b69ec
    type: comment
  author: Powback
  content: "I found that this model consistently gives bad responses compared to the\
    \ LLaVA-13b-delta-v0 model. \n\nLLaVA-13b-delta-v0 consistently provides an accurate\
    \ response to this question:\n![chrome_UPvlDpnaUX.png](https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/u5W912qWEEmctrZQiYl7l.png)\n\
    LLaVA-13b-delta-v0-science_qa consistently provides inaccurate responses:\n![chrome_TeTKtddUC0.png](https://cdn-uploads.huggingface.co/production/uploads/63054c41fca1d8d92b7f5436/bpDSDCM27YhTXdWKm9sGP.png)\n\
    \nThis is on 4bit 128g models.\n\nThe same applies to text recognition capabilities.\
    \ Science_qa does get it right as well, just not as good."
  created_at: 2023-04-28 02:13:20+00:00
  edited: true
  hidden: false
  id: 644b39d0958b7796980b69ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/712984751c435c7dd7d2d13ac672b367.svg
      fullname: Mats B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Powback
      type: user
    createdAt: '2023-04-28T03:14:56.000Z'
    data:
      from: Bad results compared to LLaVA-13b-delta-v0
      to: Bad responses compared to LLaVA-13b-delta-v0
    id: 644b3a30af97dfd24c198ac9
    type: title-change
  author: Powback
  created_at: 2023-04-28 02:14:56+00:00
  id: 644b3a30af97dfd24c198ac9
  new_title: Bad responses compared to LLaVA-13b-delta-v0
  old_title: Bad results compared to LLaVA-13b-delta-v0
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/712984751c435c7dd7d2d13ac672b367.svg
      fullname: Mats B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Powback
      type: user
    createdAt: '2023-04-28T03:18:55.000Z'
    data:
      edited: true
      editors:
      - Powback
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/712984751c435c7dd7d2d13ac672b367.svg
          fullname: Mats B
          isHf: false
          isPro: false
          name: Powback
          type: user
        html: "<p>Also, I must note that it's missing the added tokens. I resized\
          \ it and it seems to work, but I'm not sure if it impacted the model otherwise.\
          \ I get the more or less the same result on a model without the resize as\
          \ well. </p>\n<pre><code>DEFAULT_IMAGE_TOKEN = \"&lt;image&gt;\"\nDEFAULT_IMAGE_PATCH_TOKEN\
          \ = \"&lt;im_patch&gt;\"\nDEFAULT_IM_START_TOKEN = \"&lt;im_start&gt;\"\n\
          DEFAULT_IM_END_TOKEN = \"&lt;im_end&gt;\"\ndef resize(args):\n    # Model\n\
          \    disable_torch_init()\n    model_name = os.path.expanduser(args.model)\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model =\
          \ AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          \n    tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN,\
          \ DEFAULT_IM_END_TOKEN], special_tokens=True)\n    print(len(tokenizer))\
          \ # returns 32000 regardless\n    model.resize_token_embeddings(len(tokenizer)\
          \ + 3) # I just manually incremented, seems to work\n    model.save_pretrained(os.path.expanduser(args.target))\n\
          \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\
          \    parser.add_argument(\"--model\", type=str, required=True)\n    parser.add_argument(\"\
          --target\", type=str, required=True)\n    args = parser.parse_args()\n\n\
          \    resize(args)\n</code></pre>\n"
        raw: "Also, I must note that it's missing the added tokens. I resized it and\
          \ it seems to work, but I'm not sure if it impacted the model otherwise.\
          \ I get the more or less the same result on a model without the resize as\
          \ well. \n\n\n    DEFAULT_IMAGE_TOKEN = \"<image>\"\n    DEFAULT_IMAGE_PATCH_TOKEN\
          \ = \"<im_patch>\"\n    DEFAULT_IM_START_TOKEN = \"<im_start>\"\n    DEFAULT_IM_END_TOKEN\
          \ = \"<im_end>\"\n    def resize(args):\n        # Model\n        disable_torch_init()\n\
          \        model_name = os.path.expanduser(args.model)\n        tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.float16)\n\n        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN,\
          \ DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n\
          \        print(len(tokenizer)) # returns 32000 regardless\n        model.resize_token_embeddings(len(tokenizer)\
          \ + 3) # I just manually incremented, seems to work\n        model.save_pretrained(os.path.expanduser(args.target))\n\
          \n    if __name__ == \"__main__\":\n        parser = argparse.ArgumentParser()\n\
          \        parser.add_argument(\"--model\", type=str, required=True)\n   \
          \     parser.add_argument(\"--target\", type=str, required=True)\n     \
          \   args = parser.parse_args()\n\n        resize(args)"
        updatedAt: '2023-04-28T03:35:03.779Z'
      numEdits: 5
      reactions: []
    id: 644b3b1ff9f1b0cd3d97a659
    type: comment
  author: Powback
  content: "Also, I must note that it's missing the added tokens. I resized it and\
    \ it seems to work, but I'm not sure if it impacted the model otherwise. I get\
    \ the more or less the same result on a model without the resize as well. \n\n\
    \n    DEFAULT_IMAGE_TOKEN = \"<image>\"\n    DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\
    \n    DEFAULT_IM_START_TOKEN = \"<im_start>\"\n    DEFAULT_IM_END_TOKEN = \"<im_end>\"\
    \n    def resize(args):\n        # Model\n        disable_torch_init()\n     \
    \   model_name = os.path.expanduser(args.model)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    \        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\
    \n        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN,\
    \ DEFAULT_IM_END_TOKEN], special_tokens=True)\n        print(len(tokenizer)) #\
    \ returns 32000 regardless\n        model.resize_token_embeddings(len(tokenizer)\
    \ + 3) # I just manually incremented, seems to work\n        model.save_pretrained(os.path.expanduser(args.target))\n\
    \n    if __name__ == \"__main__\":\n        parser = argparse.ArgumentParser()\n\
    \        parser.add_argument(\"--model\", type=str, required=True)\n        parser.add_argument(\"\
    --target\", type=str, required=True)\n        args = parser.parse_args()\n\n \
    \       resize(args)"
  created_at: 2023-04-28 02:18:55+00:00
  edited: true
  hidden: false
  id: 644b3b1ff9f1b0cd3d97a659
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e4a483272cb886b49ecca4354367dc6.svg
      fullname: Han Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Wehere
      type: user
    createdAt: '2023-07-17T08:07:01.000Z'
    data:
      edited: false
      editors:
      - Wehere
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.882745623588562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e4a483272cb886b49ecca4354367dc6.svg
          fullname: Han Wang
          isHf: false
          isPro: false
          name: Wehere
          type: user
        html: '<p>Excuse me, would you mind if I inquire about the library required
          to run "LLaVA-13b-delta-v0"? I attempted to install it using the commands:</p>

          <ol>

          <li><code>pip install git+https://github.com/huggingface/transformers</code></li>

          <li><code>pip install tokenizers --upgrade</code></li>

          </ol>

          <p>However, I encountered a recursion error persistently. Could you please
          guide me on the appropriate library installation process? I sincerely appreciate
          your assistance. Thank you kindly.</p>

          '
        raw: 'Excuse me, would you mind if I inquire about the library required to
          run "LLaVA-13b-delta-v0"? I attempted to install it using the commands:


          1. `pip install git+https://github.com/huggingface/transformers`

          2. `pip install tokenizers --upgrade`


          However, I encountered a recursion error persistently. Could you please
          guide me on the appropriate library installation process? I sincerely appreciate
          your assistance. Thank you kindly.'
        updatedAt: '2023-07-17T08:07:01.600Z'
      numEdits: 0
      reactions: []
    id: 64b4f6a565a7e15eac0e2ad0
    type: comment
  author: Wehere
  content: 'Excuse me, would you mind if I inquire about the library required to run
    "LLaVA-13b-delta-v0"? I attempted to install it using the commands:


    1. `pip install git+https://github.com/huggingface/transformers`

    2. `pip install tokenizers --upgrade`


    However, I encountered a recursion error persistently. Could you please guide
    me on the appropriate library installation process? I sincerely appreciate your
    assistance. Thank you kindly.'
  created_at: 2023-07-17 07:07:01+00:00
  edited: false
  hidden: false
  id: 64b4f6a565a7e15eac0e2ad0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: liuhaotian/LLaVA-13b-delta-v0-science_qa
repo_type: model
status: open
target_branch: null
title: Bad responses compared to LLaVA-13b-delta-v0
