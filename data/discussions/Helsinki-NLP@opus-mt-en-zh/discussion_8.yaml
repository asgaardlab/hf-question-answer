!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Annastya
conflicting_files: null
created_at: 2023-04-03 14:56:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/272cf20a3188000f59548802e031421b.svg
      fullname: Ann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Annastya
      type: user
    createdAt: '2023-04-03T15:56:43.000Z'
    data:
      edited: true
      editors:
      - Annastya
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/272cf20a3188000f59548802e031421b.svg
          fullname: Ann
          isHf: false
          isPro: false
          name: Annastya
          type: user
        html: "<p>When I'm inspecting the cross-attention layers from the pretrained\
          \ transformer translation model (MarianMT model), It is very strange that\
          \ the cross attention from layer 0 and 1 provide best alignment between\
          \ input and output. I used bertviz to visualize all heads from all 6 layers,\
          \ and tried different language, english to german and english to chinese,\
          \ it all gives the same results, which does not make sense because the last\
          \ layers should be more accurate according to the paper <em>Jointly Learning\
          \ to Align and Translate with Transformer Models</em> <a rel=\"nofollow\"\
          \ href=\"url\">https://arxiv.org/pdf/1909.02074.pdf</a><br><a rel=\"nofollow\"\
          \ href=\"https://user-images.githubusercontent.com/44487593/229558767-deeb4fe1-8e62-41aa-9116-cf4e55ccfac6.png\"\
          ><img alt=\"image\" src=\"https://user-images.githubusercontent.com/44487593/229558767-deeb4fe1-8e62-41aa-9116-cf4e55ccfac6.png\"\
          ></a></p>\n<p>But when I'm looking at the cross attention of model <em>Helsinki-NLP/opus-mt-en-de</em>\
          \ and <em>Helsinki-NLP/opus-mt-en-zh</em> , the layer 1 gives the best alignment.\
          \ the code is below:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, AutoModel\n<span class=\"hljs-keyword\">import</span> os\n\
          os.environ[<span class=\"hljs-string\">'TRANSFORMERS_CACHE'</span>] = <span\
          \ class=\"hljs-string\">'/data2/hanyings/.cache'</span>\n\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"Helsinki-NLP/opus-mt-en-de\"</span>)\nmodel =\
          \ AutoModel.from_pretrained(<span class=\"hljs-string\">\"Helsinki-NLP/opus-mt-en-de\"\
          </span>, output_attentions=<span class=\"hljs-literal\">True</span>)\n\n\
          encoder_input_ids = tokenizer(<span class=\"hljs-string\">\"She sees the\
          \ small elephant.\"</span>, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>, add_special_tokens=<span class=\"hljs-literal\">True</span>).input_ids\n\
          <span class=\"hljs-keyword\">with</span> tokenizer.as_target_tokenizer():\n\
          \    decoder_input_ids = tokenizer(<span class=\"hljs-string\">\"Sie sieht\
          \ den kleinen Elefanten.\"</span>, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>, add_special_tokens=<span class=\"hljs-literal\">True</span>).input_ids\n\
          \noutputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n\
          \nencoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[<span\
          \ class=\"hljs-number\">0</span>])\ndecoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[<span\
          \ class=\"hljs-number\">0</span>])\n\n<span class=\"hljs-keyword\">from</span>\
          \ bertviz <span class=\"hljs-keyword\">import</span> model_view\nmodel_view(\n\
          \    encoder_attention=outputs.encoder_attentions,\n    decoder_attention=outputs.decoder_attentions,\n\
          \    cross_attention=outputs.cross_attentions,\n    encoder_tokens= encoder_text,\n\
          \    decoder_tokens = decoder_text\n)\n</code></pre>\n<p>And the results\
          \ are:<br><a rel=\"nofollow\" href=\"https://user-images.githubusercontent.com/44487593/229560299-f6792ad1-5984-4a29-80fb-79403855b43a.png\"\
          ><img alt=\"image\" src=\"https://user-images.githubusercontent.com/44487593/229560299-f6792ad1-5984-4a29-80fb-79403855b43a.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://user-images.githubusercontent.com/44487593/229561124-f84d41d0-ceed-49ac-98b6-91ce47f14424.png\"\
          ><img alt=\"image\" src=\"https://user-images.githubusercontent.com/44487593/229561124-f84d41d0-ceed-49ac-98b6-91ce47f14424.png\"\
          ></a><br>From the above pictures, I observed that the first 2 layers give\
          \ the best alignment whereas the last layers do not align the input and\
          \ output tokens properly. Can you please help me to explain why this happens?\
          \ and If the alignment of the last layer is not accurate, how does the model\
          \ provide correct predictions?  Please! It is very important for my research\
          \ project!<br><span data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\"\
          >@<span class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;sshleifer&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sshleifer\">@<span class=\"\
          underline\">sshleifer</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;tiedeman&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/tiedeman\">@<span class=\"underline\">tiedeman</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;joaogante&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/joaogante\"\
          >@<span class=\"underline\">joaogante</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;bobosui&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bobosui\">@<span class=\"\
          underline\">bobosui</span></a></span>\n\n\t</span></span></p>\n"
        raw: "When I'm inspecting the cross-attention layers from the pretrained transformer\
          \ translation model (MarianMT model), It is very strange that the cross\
          \ attention from layer 0 and 1 provide best alignment between input and\
          \ output. I used bertviz to visualize all heads from all 6 layers, and tried\
          \ different language, english to german and english to chinese, it all gives\
          \ the same results, which does not make sense because the last layers should\
          \ be more accurate according to the paper _Jointly Learning to Align and\
          \ Translate with Transformer Models_ [https://arxiv.org/pdf/1909.02074.pdf](url)\n\
          ![image](https://user-images.githubusercontent.com/44487593/229558767-deeb4fe1-8e62-41aa-9116-cf4e55ccfac6.png)\n\
          \nBut when I'm looking at the cross attention of model _Helsinki-NLP/opus-mt-en-de_\
          \ and _Helsinki-NLP/opus-mt-en-zh_ , the layer 1 gives the best alignment.\
          \ the code is below:\n```python\nfrom transformers import AutoTokenizer,\
          \ AutoModel\nimport os\nos.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\"\
          )\nmodel = AutoModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", output_attentions=True)\n\
          \nencoder_input_ids = tokenizer(\"She sees the small elephant.\", return_tensors=\"\
          pt\", add_special_tokens=True).input_ids\nwith tokenizer.as_target_tokenizer():\n\
          \    decoder_input_ids = tokenizer(\"Sie sieht den kleinen Elefanten.\"\
          , return_tensors=\"pt\", add_special_tokens=True).input_ids\n\noutputs =\
          \ model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n\
          \nencoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n\
          decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n\n\
          from bertviz import model_view\nmodel_view(\n    encoder_attention=outputs.encoder_attentions,\n\
          \    decoder_attention=outputs.decoder_attentions,\n    cross_attention=outputs.cross_attentions,\n\
          \    encoder_tokens= encoder_text,\n    decoder_tokens = decoder_text\n\
          )\n```\nAnd the results are:\n![image](https://user-images.githubusercontent.com/44487593/229560299-f6792ad1-5984-4a29-80fb-79403855b43a.png)\n\
          ![image](https://user-images.githubusercontent.com/44487593/229561124-f84d41d0-ceed-49ac-98b6-91ce47f14424.png)\n\
          From the above pictures, I observed that the first 2 layers give the best\
          \ alignment whereas the last layers do not align the input and output tokens\
          \ properly. Can you please help me to explain why this happens? and If the\
          \ alignment of the last layer is not accurate, how does the model provide\
          \ correct predictions?  Please! It is very important for my research project!\n\
          @patrickvonplaten @sshleifer @tiedeman @joaogante @bobosui"
        updatedAt: '2023-04-03T18:10:56.736Z'
      numEdits: 4
      reactions: []
    id: 642af73ba096201096ee025e
    type: comment
  author: Annastya
  content: "When I'm inspecting the cross-attention layers from the pretrained transformer\
    \ translation model (MarianMT model), It is very strange that the cross attention\
    \ from layer 0 and 1 provide best alignment between input and output. I used bertviz\
    \ to visualize all heads from all 6 layers, and tried different language, english\
    \ to german and english to chinese, it all gives the same results, which does\
    \ not make sense because the last layers should be more accurate according to\
    \ the paper _Jointly Learning to Align and Translate with Transformer Models_\
    \ [https://arxiv.org/pdf/1909.02074.pdf](url)\n![image](https://user-images.githubusercontent.com/44487593/229558767-deeb4fe1-8e62-41aa-9116-cf4e55ccfac6.png)\n\
    \nBut when I'm looking at the cross attention of model _Helsinki-NLP/opus-mt-en-de_\
    \ and _Helsinki-NLP/opus-mt-en-zh_ , the layer 1 gives the best alignment. the\
    \ code is below:\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\
    import os\nos.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\nmodel = AutoModel.from_pretrained(\"\
    Helsinki-NLP/opus-mt-en-de\", output_attentions=True)\n\nencoder_input_ids = tokenizer(\"\
    She sees the small elephant.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n\
    with tokenizer.as_target_tokenizer():\n    decoder_input_ids = tokenizer(\"Sie\
    \ sieht den kleinen Elefanten.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n\
    \noutputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n\
    \nencoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\ndecoder_text\
    \ = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n\nfrom bertviz import\
    \ model_view\nmodel_view(\n    encoder_attention=outputs.encoder_attentions,\n\
    \    decoder_attention=outputs.decoder_attentions,\n    cross_attention=outputs.cross_attentions,\n\
    \    encoder_tokens= encoder_text,\n    decoder_tokens = decoder_text\n)\n```\n\
    And the results are:\n![image](https://user-images.githubusercontent.com/44487593/229560299-f6792ad1-5984-4a29-80fb-79403855b43a.png)\n\
    ![image](https://user-images.githubusercontent.com/44487593/229561124-f84d41d0-ceed-49ac-98b6-91ce47f14424.png)\n\
    From the above pictures, I observed that the first 2 layers give the best alignment\
    \ whereas the last layers do not align the input and output tokens properly. Can\
    \ you please help me to explain why this happens? and If the alignment of the\
    \ last layer is not accurate, how does the model provide correct predictions?\
    \  Please! It is very important for my research project!\n@patrickvonplaten @sshleifer\
    \ @tiedeman @joaogante @bobosui"
  created_at: 2023-04-03 14:56:43+00:00
  edited: true
  hidden: false
  id: 642af73ba096201096ee025e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/272cf20a3188000f59548802e031421b.svg
      fullname: Ann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Annastya
      type: user
    createdAt: '2023-04-03T16:00:34.000Z'
    data:
      from: Helsinki-NLP model cross attention alignment problem
      to: Helsinki-NLP model cross attention alignment problem, inconsistent result
        with paper!
    id: 642af8227604c4e9d51e4919
    type: title-change
  author: Annastya
  created_at: 2023-04-03 15:00:34+00:00
  id: 642af8227604c4e9d51e4919
  new_title: Helsinki-NLP model cross attention alignment problem, inconsistent result
    with paper!
  old_title: Helsinki-NLP model cross attention alignment problem
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Helsinki-NLP/opus-mt-en-zh
repo_type: model
status: open
target_branch: null
title: Helsinki-NLP model cross attention alignment problem, inconsistent result with
  paper!
