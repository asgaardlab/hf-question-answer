!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ameenroayan
conflicting_files: null
created_at: 2023-04-18 17:14:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c5594ba846fe1d79b7f446698ca98a0.svg
      fullname: Roayan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ameenroayan
      type: user
    createdAt: '2023-04-18T18:14:19.000Z'
    data:
      edited: false
      editors:
      - Ameenroayan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c5594ba846fe1d79b7f446698ca98a0.svg
          fullname: Roayan
          isHf: false
          isPro: false
          name: Ameenroayan
          type: user
        html: "<p>Hey ! </p>\n<p>at first it started with giving me errors that it\
          \ was not finding certain checkpoints, so i cloned everything in the folder,\
          \ but then i started getting errors beyond my depth :</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/643eb15c4fa8bccfd7220998/1U_sqlEnHsugWEdjJKBmf.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/643eb15c4fa8bccfd7220998/1U_sqlEnHsugWEdjJKBmf.png\"\
          ></a></p>\n<p>Traceback (most recent call last):<br>File \u201CE:\\AI\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 442, in load_state_dict<br>return torch.load(checkpoint_file,\
          \ map_location=\u201Ccpu\u201D)<br>File \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 809, in load<br>return _load(opened_zipfile, map_location, pickle_module,\
          \ **pickle_load_args)<br>File \u201CE:\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 1172, in _load<br>result\
          \ = unpickler.load()<br>File \u201CE:\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\u201D, line 1142, in persistent_load<br>typed_storage\
          \ = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 1112, in load_tensor<br>storage = zip_file.get_storage_from_record(name,\
          \ numel, torch.UntypedStorage)._typed_storage()._untyped_storage<br>RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 238551040 bytes.</p>\n<p>During handling of the above exception,\
          \ another exception occurred:</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 446, in load_state_dict<br>if\
          \ f.read(7) == \u201Cversion\u201D:<br>File \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\encodings\\cp1252.py\u201D, line 23, in decode<br>return\
          \ codecs.charmap_decode(input,self.errors,decoding_table)[0]<br>UnicodeDecodeError:\
          \ \u2018charmap\u2019 codec can\u2019t decode byte 0x81 in position 1850:\
          \ character maps to</p>\n<p>During handling of the above exception, another\
          \ exception occurred:</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
          , line 96, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 186, in load_model<br>model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)<br>File \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line\
          \ 471, in from_pretrained<br>return model_class.from_pretrained(<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 2795, in from_pretrained<br>)\
          \ = cls._load_pretrained_model(<br>File \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 3109, in _load_pretrained_model<br>state_dict = load_state_dict(shard_file)<br>File\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 458, in load_state_dict<br>raise\
          \ OSError(<br>OSError: Unable to load weights from pytorch checkpoint file\
          \ for \u2018models\\digitous_Alpacino30b\\pytorch_model-00032-of-00033.bin\u2019\
          \ at \u2018models\\digitous_Alpacino30b\\pytorch_model-00032-of-00033.bin\u2019\
          . If you tried to load a PyTorch model from a TF 2.0 checkpoint, please\
          \ set from_tf=True.</p>\n"
        raw: "Hey ! \r\n\r\nat first it started with giving me errors that it was\
          \ not finding certain checkpoints, so i cloned everything in the folder,\
          \ but then i started getting errors beyond my depth :\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643eb15c4fa8bccfd7220998/1U_sqlEnHsugWEdjJKBmf.png)\r\
          \n\r\nTraceback (most recent call last):\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)\r\nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\serialization.py\u201D, line 809, in load\r\n\
          return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\
          \nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 1172, in _load\r\nresult = unpickler.load()\r\
          \nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 1142, in persistent_load\r\ntyped_storage\
          \ = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\r\nFile\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 1112, in load_tensor\r\nstorage = zip_file.get_storage_from_record(name,\
          \ numel, torch.UntypedStorage)._typed_storage()._untyped_storage\r\nRuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 238551040 bytes.\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 446, in load_state_dict\r\n\
          if f.read(7) == \u201Cversion\u201D:\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\encodings\\cp1252.py\u201D, line 23, in decode\r\
          \nreturn codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\n\
          UnicodeDecodeError: \u2018charmap\u2019 codec can\u2019t decode byte 0x81\
          \ in position 1850: character maps to\r\n\r\nDuring handling of the above\
          \ exception, another exception occurred:\r\n\r\nTraceback (most recent call\
          \ last):\r\nFile \u201CE:\\AI\\oobabooga-windows\\text-generation-webui\\\
          server.py\u201D, line 96, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 186, in load_model\r\
          \nmodel = LoaderClass.from_pretrained(checkpoint, **params)\r\nFile \u201C\
          E:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\nreturn\
          \ model_class.from_pretrained(\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 2795, in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile\
          \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 3109, in _load_pretrained_model\r\
          \nstate_dict = load_state_dict(shard_file)\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 458, in load_state_dict\r\nraise OSError(\r\nOSError: Unable to load\
          \ weights from pytorch checkpoint file for \u2018models\\digitous_Alpacino30b\\\
          pytorch_model-00032-of-00033.bin\u2019 at \u2018models\\digitous_Alpacino30b\\\
          pytorch_model-00032-of-00033.bin\u2019. If you tried to load a PyTorch model\
          \ from a TF 2.0 checkpoint, please set from_tf=True.\r\n\r\n\r\n"
        updatedAt: '2023-04-18T18:14:19.812Z'
      numEdits: 0
      reactions: []
    id: 643eddfb0d1194da249bc99c
    type: comment
  author: Ameenroayan
  content: "Hey ! \r\n\r\nat first it started with giving me errors that it was not\
    \ finding certain checkpoints, so i cloned everything in the folder, but then\
    \ i started getting errors beyond my depth :\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643eb15c4fa8bccfd7220998/1U_sqlEnHsugWEdjJKBmf.png)\r\
    \n\r\nTraceback (most recent call last):\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
    cpu\u201D)\r\nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\serialization.py\u201D, line 809, in load\r\nreturn _load(opened_zipfile,\
    \ map_location, pickle_module, **pickle_load_args)\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D, line\
    \ 1172, in _load\r\nresult = unpickler.load()\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D, line\
    \ 1142, in persistent_load\r\ntyped_storage = load_tensor(dtype, nbytes, key,\
    \ _maybe_decode_ascii(location))\r\nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\serialization.py\u201D, line 1112, in load_tensor\r\
    \nstorage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage\r\
    \nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\\
    impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
    \ to allocate 238551040 bytes.\r\n\r\nDuring handling of the above exception,\
    \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile\
    \ \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 446, in load_state_dict\r\nif f.read(7)\
    \ == \u201Cversion\u201D:\r\nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\encodings\\cp1252.py\u201D, line 23, in decode\r\nreturn codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\
    \nUnicodeDecodeError: \u2018charmap\u2019 codec can\u2019t decode byte 0x81 in\
    \ position 1850: character maps to\r\n\r\nDuring handling of the above exception,\
    \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile\
    \ \u201CE:\\AI\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line\
    \ 96, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \nFile \u201CE:\\AI\\oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 186, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint, **params)\r\
    \nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
    \nreturn model_class.from_pretrained(\r\nFile \u201CE:\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
    , line 2795, in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile \u201C\
    E:\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 3109, in _load_pretrained_model\r\nstate_dict =\
    \ load_state_dict(shard_file)\r\nFile \u201CE:\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 458, in load_state_dict\r\
    \nraise OSError(\r\nOSError: Unable to load weights from pytorch checkpoint file\
    \ for \u2018models\\digitous_Alpacino30b\\pytorch_model-00032-of-00033.bin\u2019\
    \ at \u2018models\\digitous_Alpacino30b\\pytorch_model-00032-of-00033.bin\u2019\
    . If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
    \n\r\n\r\n"
  created_at: 2023-04-18 17:14:19+00:00
  edited: false
  hidden: false
  id: 643eddfb0d1194da249bc99c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fd5bd55d8a3a6490e5874c0ba4842c8.svg
      fullname: Glenn Wehmeyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: djstraylight
      type: user
    createdAt: '2023-04-19T22:33:00.000Z'
    data:
      edited: false
      editors:
      - djstraylight
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fd5bd55d8a3a6490e5874c0ba4842c8.svg
          fullname: Glenn Wehmeyer
          isHf: false
          isPro: false
          name: djstraylight
          type: user
        html: '<p>I think you''re running out of memory to load the model.  It''s
          about 24 GB of memory to load which is probably more than you have.  You
          could always try using the 4bit.safetensors version.</p>

          '
        raw: I think you're running out of memory to load the model.  It's about 24
          GB of memory to load which is probably more than you have.  You could always
          try using the 4bit.safetensors version.
        updatedAt: '2023-04-19T22:33:00.851Z'
      numEdits: 0
      reactions: []
    id: 64406c1c194b02fd3096a1ae
    type: comment
  author: djstraylight
  content: I think you're running out of memory to load the model.  It's about 24
    GB of memory to load which is probably more than you have.  You could always try
    using the 4bit.safetensors version.
  created_at: 2023-04-19 21:33:00+00:00
  edited: false
  hidden: false
  id: 64406c1c194b02fd3096a1ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4b4ac134e230bcab148b7d5de3bf477.svg
      fullname: Vergil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rulentor
      type: user
    createdAt: '2023-04-28T11:21:25.000Z'
    data:
      edited: false
      editors:
      - Rulentor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4b4ac134e230bcab148b7d5de3bf477.svg
          fullname: Vergil
          isHf: false
          isPro: false
          name: Rulentor
          type: user
        html: '<p>I''m getting a very similar error on a 3090 Ti </p>

          <p>Loading digitous_Alpacino30b...<br>Loading checkpoint shards:   0%|                                                                |
          0/33 [00:00&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 442, in load_state_dict<br>    return torch.load(checkpoint_file, map_location="cpu")<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\torch\serialization.py",
          line 791, in load<br>    with _open_file_like(f, ''rb'') as opened_file:<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\torch\serialization.py",
          line 271, in _open_file_like<br>    return _open_file(name_or_buffer, mode)<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\torch\serialization.py",
          line 252, in <strong>init</strong><br>    super().<strong>init</strong>(open(name,
          mode))<br>FileNotFoundError: [Errno 2] No such file or directory: ''models\digitous_Alpacino30b\pytorch_model-00001-of-00033.bin''<br>During
          handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "C:\tools\OogaBooga\text-generation-webui\server.py",
          line 914, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\tools\OogaBooga\text-generation-webui\modules\models.py", line 84, in
          load_model<br>    model = LoaderClass.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}"),
          low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else
          torch.float16, trust_remote_code=trust_remote_code)<br>  File "C:\tools\OogaBooga\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2795, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 3109, in _load_pretrained_model<br>    state_dict = load_state_dict(shard_file)<br>  File
          "C:\tools\OogaBooga\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 445, in load_state_dict<br>    with open(checkpoint_file) as f:<br>FileNotFoundError:
          [Errno 2] No such file or directory: ''models\digitous_Alpacino30b\pytorch_model-00001-of-00033.bin''<br>Done!</p>

          '
        raw: "I'm getting a very similar error on a 3090 Ti \n\n\nLoading digitous_Alpacino30b...\n\
          Loading checkpoint shards:   0%|                                       \
          \                         | 0/33 [00:00<?, ?it/s]\nTraceback (most recent\
          \ call last):\n  File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 442, in load_state_dict\n\
          \    return torch.load(checkpoint_file, map_location=\"cpu\")\n  File \"\
          C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          serialization.py\", line 791, in load\n    with _open_file_like(f, 'rb')\
          \ as opened_file:\n  File \"C:\\tools\\OogaBooga\\installer_files\\env\\\
          lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\n\
          \    return _open_file(name_or_buffer, mode)\n  File \"C:\\tools\\OogaBooga\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\", line\
          \ 252, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models\\\\digitous_Alpacino30b\\\
          \\pytorch_model-00001-of-00033.bin'\nDuring handling of the above exception,\
          \ another exception occurred:\n\nTraceback (most recent call last):\n  File\
          \ \"C:\\tools\\OogaBooga\\text-generation-webui\\server.py\", line 914,\
          \ in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"C:\\tools\\OogaBooga\\text-generation-webui\\modules\\models.py\"\
          , line 84, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)\n\
          \  File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\n\
          \    return model_class.from_pretrained(\n  File \"C:\\tools\\OogaBooga\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2795, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\", line 3109, in _load_pretrained_model\n\
          \    state_dict = load_state_dict(shard_file)\n  File \"C:\\tools\\OogaBooga\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 445, in load_state_dict\n    with open(checkpoint_file) as f:\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models\\\\digitous_Alpacino30b\\\
          \\pytorch_model-00001-of-00033.bin'\nDone!"
        updatedAt: '2023-04-28T11:21:25.737Z'
      numEdits: 0
      reactions: []
    id: 644bac35b64fb3f65f5e5346
    type: comment
  author: Rulentor
  content: "I'm getting a very similar error on a 3090 Ti \n\n\nLoading digitous_Alpacino30b...\n\
    Loading checkpoint shards:   0%|                                             \
    \                   | 0/33 [00:00<?, ?it/s]\nTraceback (most recent call last):\n\
    \  File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 442, in load_state_dict\n    return torch.load(checkpoint_file,\
    \ map_location=\"cpu\")\n  File \"C:\\tools\\OogaBooga\\installer_files\\env\\\
    lib\\site-packages\\torch\\serialization.py\", line 791, in load\n    with _open_file_like(f,\
    \ 'rb') as opened_file:\n  File \"C:\\tools\\OogaBooga\\installer_files\\env\\\
    lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\n\
    \    return _open_file(name_or_buffer, mode)\n  File \"C:\\tools\\OogaBooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\serialization.py\", line 252,\
    \ in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError: [Errno\
    \ 2] No such file or directory: 'models\\\\digitous_Alpacino30b\\\\pytorch_model-00001-of-00033.bin'\n\
    During handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\n  File \"C:\\tools\\OogaBooga\\text-generation-webui\\\
    server.py\", line 914, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\tools\\OogaBooga\\text-generation-webui\\modules\\models.py\", line\
    \ 84, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}\"\
    ), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else\
    \ torch.float16, trust_remote_code=trust_remote_code)\n  File \"C:\\tools\\OogaBooga\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 471, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 2795, in from_pretrained\n    ) = cls._load_pretrained_model(\n\
    \  File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 3109, in _load_pretrained_model\n    state_dict = load_state_dict(shard_file)\n\
    \  File \"C:\\tools\\OogaBooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 445, in load_state_dict\n    with open(checkpoint_file)\
    \ as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\\digitous_Alpacino30b\\\
    \\pytorch_model-00001-of-00033.bin'\nDone!"
  created_at: 2023-04-28 10:21:25+00:00
  edited: false
  hidden: false
  id: 644bac35b64fb3f65f5e5346
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
      fullname: Erik
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: digitous
      type: user
    createdAt: '2023-05-19T06:27:35.000Z'
    data:
      edited: true
      editors:
      - digitous
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
          fullname: Erik
          isHf: false
          isPro: false
          name: digitous
          type: user
        html: '<p>In OogaBooga, cut and paste the model.bins and index json out of
          the folder somewhere else; it seems like it is trying to load the model''s
          full weights ignoring the quantized model that fits fine in 24gb VRAM.</p>

          '
        raw: In OogaBooga, cut and paste the model.bins and index json out of the
          folder somewhere else; it seems like it is trying to load the model's full
          weights ignoring the quantized model that fits fine in 24gb VRAM.
        updatedAt: '2023-05-19T06:27:48.079Z'
      numEdits: 1
      reactions: []
    id: 646716d73a7c8dda23fd4b39
    type: comment
  author: digitous
  content: In OogaBooga, cut and paste the model.bins and index json out of the folder
    somewhere else; it seems like it is trying to load the model's full weights ignoring
    the quantized model that fits fine in 24gb VRAM.
  created_at: 2023-05-19 05:27:35+00:00
  edited: true
  hidden: false
  id: 646716d73a7c8dda23fd4b39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1beef2d738a04e080a5be1a012c84612.svg
      fullname: raddo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruttan
      type: user
    createdAt: '2023-07-31T09:09:51.000Z'
    data:
      edited: false
      editors:
      - ruttan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9808319807052612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1beef2d738a04e080a5be1a012c84612.svg
          fullname: raddo
          isHf: false
          isPro: false
          name: ruttan
          type: user
        html: '<p>I have 64GB but I have the same problem and I do not understand
          what exactly I should cut and where... </p>

          '
        raw: 'I have 64GB but I have the same problem and I do not understand what
          exactly I should cut and where... '
        updatedAt: '2023-07-31T09:09:51.324Z'
      numEdits: 0
      reactions: []
    id: 64c77a5f120a85440bcbe498
    type: comment
  author: ruttan
  content: 'I have 64GB but I have the same problem and I do not understand what exactly
    I should cut and where... '
  created_at: 2023-07-31 08:09:51+00:00
  edited: false
  hidden: false
  id: 64c77a5f120a85440bcbe498
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: digitous/Alpacino30b
repo_type: model
status: open
target_branch: null
title: Errors while using this with Oobabooga
