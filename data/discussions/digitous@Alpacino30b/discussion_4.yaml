!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tensiondriven
conflicting_files: null
created_at: 2023-05-01 03:55:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-05-01T04:55:37.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>The Alpaca dataset contains instructions that result in models behaving
          "As an AI" and sometimes refusing certain instruction.  I''m wondering if,
          or how much, this negatively affects Alpachino''s generation capabilities.  Have
          you noticed anything in this regard?</p>

          <p>I''ve also heard that 30b models can sometimes produce lower quality
          work than smaller models like 13b when it comes to narrative over time,
          also just curious if you''ve experienced anything like that.</p>

          <p>Thanks for doing all the hard work to make this possible!!</p>

          '
        raw: "The Alpaca dataset contains instructions that result in models behaving\
          \ \"As an AI\" and sometimes refusing certain instruction.  I'm wondering\
          \ if, or how much, this negatively affects Alpachino's generation capabilities.\
          \  Have you noticed anything in this regard?\r\n\r\nI've also heard that\
          \ 30b models can sometimes produce lower quality work than smaller models\
          \ like 13b when it comes to narrative over time, also just curious if you've\
          \ experienced anything like that.\r\n\r\nThanks for doing all the hard work\
          \ to make this possible!!"
        updatedAt: '2023-05-01T04:55:37.516Z'
      numEdits: 0
      reactions: []
    id: 644f4649f2ada99b2ebb10a6
    type: comment
  author: tensiondriven
  content: "The Alpaca dataset contains instructions that result in models behaving\
    \ \"As an AI\" and sometimes refusing certain instruction.  I'm wondering if,\
    \ or how much, this negatively affects Alpachino's generation capabilities.  Have\
    \ you noticed anything in this regard?\r\n\r\nI've also heard that 30b models\
    \ can sometimes produce lower quality work than smaller models like 13b when it\
    \ comes to narrative over time, also just curious if you've experienced anything\
    \ like that.\r\n\r\nThanks for doing all the hard work to make this possible!!"
  created_at: 2023-05-01 03:55:37+00:00
  edited: false
  hidden: false
  id: 644f4649f2ada99b2ebb10a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
      fullname: Erik
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: digitous
      type: user
    createdAt: '2023-05-19T06:11:24.000Z'
    data:
      edited: false
      editors:
      - digitous
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
          fullname: Erik
          isHf: false
          isPro: false
          name: digitous
          type: user
        html: '<p>No problem, I enjoy the process! So -- I had my initial reservations
          of using Alpaca for this model, however, ChanSung''s LoRA does not seem
          to exhibit this "as an AI model" behavior despite the very well done training
          on its dataset. I''m not completely sure why. Even raw Alpaca30b using ChanSung''s
          LoRA seems to not care; so I decided to use that as the foundation of the
          instruct module of the model merge (I think about 50% a model with his LoRA
          applied and Story+COT). I am honestly surprised what it allows. That being
          said you raise another valid concern that larger param llama models can
          produce lower quality work; as counter intuitive as it sounds, I have hit
          roadblocks with some 30b models and them painting themselves in a narrative
          corner, over reliant on nearly repeating previous information instead of
          applying it and running with generating new information. For this I adjust
          the repetition penalty and raise the temperature of the model inference
          a bit to circumvent the eventual repetition. It usually works, not perfect,
          but overall the larger size does seem to add a better understanding of how
          things work and how to run a narrative.</p>

          <p>I also find when using instructions, it can help to put in persistent
          memory or context "tell the story as if you are a/an X" as in, if it''s
          a horror story "as if you are a masterful horror writer and a fanatic for
          horror fiction" or if it''s something spicy "write it as if you are into
          X" etc. I find that being creative with instruction prompting and balancing
          keeping the instructions just loose enough not to corner the model into
          repetitive behavior is the best.</p>

          '
        raw: 'No problem, I enjoy the process! So -- I had my initial reservations
          of using Alpaca for this model, however, ChanSung''s LoRA does not seem
          to exhibit this "as an AI model" behavior despite the very well done training
          on its dataset. I''m not completely sure why. Even raw Alpaca30b using ChanSung''s
          LoRA seems to not care; so I decided to use that as the foundation of the
          instruct module of the model merge (I think about 50% a model with his LoRA
          applied and Story+COT). I am honestly surprised what it allows. That being
          said you raise another valid concern that larger param llama models can
          produce lower quality work; as counter intuitive as it sounds, I have hit
          roadblocks with some 30b models and them painting themselves in a narrative
          corner, over reliant on nearly repeating previous information instead of
          applying it and running with generating new information. For this I adjust
          the repetition penalty and raise the temperature of the model inference
          a bit to circumvent the eventual repetition. It usually works, not perfect,
          but overall the larger size does seem to add a better understanding of how
          things work and how to run a narrative.


          I also find when using instructions, it can help to put in persistent memory
          or context "tell the story as if you are a/an X" as in, if it''s a horror
          story "as if you are a masterful horror writer and a fanatic for horror
          fiction" or if it''s something spicy "write it as if you are into X" etc.
          I find that being creative with instruction prompting and balancing keeping
          the instructions just loose enough not to corner the model into repetitive
          behavior is the best.'
        updatedAt: '2023-05-19T06:11:24.234Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - tensiondriven
    id: 6467130cab75d9cb3c3f389b
    type: comment
  author: digitous
  content: 'No problem, I enjoy the process! So -- I had my initial reservations of
    using Alpaca for this model, however, ChanSung''s LoRA does not seem to exhibit
    this "as an AI model" behavior despite the very well done training on its dataset.
    I''m not completely sure why. Even raw Alpaca30b using ChanSung''s LoRA seems
    to not care; so I decided to use that as the foundation of the instruct module
    of the model merge (I think about 50% a model with his LoRA applied and Story+COT).
    I am honestly surprised what it allows. That being said you raise another valid
    concern that larger param llama models can produce lower quality work; as counter
    intuitive as it sounds, I have hit roadblocks with some 30b models and them painting
    themselves in a narrative corner, over reliant on nearly repeating previous information
    instead of applying it and running with generating new information. For this I
    adjust the repetition penalty and raise the temperature of the model inference
    a bit to circumvent the eventual repetition. It usually works, not perfect, but
    overall the larger size does seem to add a better understanding of how things
    work and how to run a narrative.


    I also find when using instructions, it can help to put in persistent memory or
    context "tell the story as if you are a/an X" as in, if it''s a horror story "as
    if you are a masterful horror writer and a fanatic for horror fiction" or if it''s
    something spicy "write it as if you are into X" etc. I find that being creative
    with instruction prompting and balancing keeping the instructions just loose enough
    not to corner the model into repetitive behavior is the best.'
  created_at: 2023-05-19 05:11:24+00:00
  edited: false
  hidden: false
  id: 6467130cab75d9cb3c3f389b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-05-26T03:42:27.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>Brilliant, thank you for the response - I think there''s not enough
          information about how lower parameter count models compare to higher.  I''m
          content with your answer, closing!</p>

          '
        raw: Brilliant, thank you for the response - I think there's not enough information
          about how lower parameter count models compare to higher.  I'm content with
          your answer, closing!
        updatedAt: '2023-05-26T03:42:27.360Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64702aa3d1f1b73079f27bdb
    id: 64702aa3d1f1b73079f27bda
    type: comment
  author: tensiondriven
  content: Brilliant, thank you for the response - I think there's not enough information
    about how lower parameter count models compare to higher.  I'm content with your
    answer, closing!
  created_at: 2023-05-26 02:42:27+00:00
  edited: false
  hidden: false
  id: 64702aa3d1f1b73079f27bda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-05-26T03:42:27.000Z'
    data:
      status: closed
    id: 64702aa3d1f1b73079f27bdb
    type: status-change
  author: tensiondriven
  created_at: 2023-05-26 02:42:27+00:00
  id: 64702aa3d1f1b73079f27bdb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: digitous/Alpacino30b
repo_type: model
status: closed
target_branch: null
title: How HHH?
