!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tylersuard
conflicting_files: null
created_at: 2023-05-06 03:54:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-05-06T04:54:04.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: "<p>Sorry to bother you again!  I am sure you are very busy.  I am having\
          \ trouble running your code.  It looks like there is a mismatch in tensor\
          \ size between the key/query and the additive causal mask.  My chunk size\
          \ is 16, and my max_positions is 13008, a multiple of 16.  What am I doing\
          \ wrong here?</p>\n<p> /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:1098\
          \ in     forward\u2502<br>\u2502   1095 \u2502   \u2502   if padding_mask\
          \ is not None and padding_mask.dim() == 0:                          \u2502\
          <br>\u2502   1096 \u2502   \u2502   \u2502   padding_mask = None       \
          \                                                    \u2502<br>\u2502  \
          \ 1097 \u2502   \u2502                                                 \
          \                                    \u2502<br>\u2502 \u2771 1098 \u2502\
          \   \u2502   attn_weights = self.attention_function(query, key, padding_mask=padding_mask,\
          \ ca  \u2502<br>\u2502   1099 \u2502   \u2502                          \
          \                                                           \u2502<br>\u2502\
          \   1100 \u2502   \u2502   value = self.hidden_dropout(value, batch_first=True)\
          \                              \u2502<br>\u2502   1101 \u2502   \u2502 \
          \  kernel = self.attention_dropout(attn_weights)                       \
          \              \u2502<br>\u2502                                        \
          \                                                          \u2502<br>\u2502\
          \ /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:901\
          \ in         \u2502<br>\u2502 softmax_attention                        \
          \                                                        \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    898 \u2502   \u2502   if\
          \ causal_mask is not None:                                             \
          \          \u2502<br>\u2502    899 \u2502   \u2502   \u2502   additive_causal_mask\
          \ = torch.zeros_like(causal_mask, dtype=qk.dtype)          \u2502<br>\u2502\
          \    900 \u2502   \u2502   \u2502   additive_causal_mask = additive_causal_mask.masked_fill((1\
          \ - causal_mask).bo  \u2502<br>\u2502 \u2771  901 \u2502   \u2502   \u2502\
          \   qk = qk + additive_causal_mask                                     \
          \           \u2502<br>\u2502    902 \u2502   \u2502                    \
          \                                                                 \u2502\
          <br>\u2502    903 \u2502   \u2502   if padding_mask is not None:       \
          \                                               \u2502<br>\u2502    904\
          \ \u2502   \u2502   \u2502   # 1 for tokens which are <em>not masked</em>\
          \                                         \u2502<br>\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          <br>RuntimeError: The size of tensor a (16) must match the size of tensor\
          \ b (13008) at non-singleton dimension 3</p>\n"
        raw: "Sorry to bother you again!  I am sure you are very busy.  I am having\
          \ trouble running your code.  It looks like there is a mismatch in tensor\
          \ size between the key/query and the additive causal mask.  My chunk size\
          \ is 16, and my max_positions is 13008, a multiple of 16.  What am I doing\
          \ wrong here?\r\n\r\n /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:1098\
          \ in     forward\u2502\r\n\u2502   1095 \u2502   \u2502   if padding_mask\
          \ is not None and padding_mask.dim() == 0:                          \u2502\
          \r\n\u2502   1096 \u2502   \u2502   \u2502   padding_mask = None       \
          \                                                    \u2502\r\n\u2502  \
          \ 1097 \u2502   \u2502                                                 \
          \                                    \u2502\r\n\u2502 \u2771 1098 \u2502\
          \   \u2502   attn_weights = self.attention_function(query, key, padding_mask=padding_mask,\
          \ ca  \u2502\r\n\u2502   1099 \u2502   \u2502                          \
          \                                                           \u2502\r\n\u2502\
          \   1100 \u2502   \u2502   value = self.hidden_dropout(value, batch_first=True)\
          \                              \u2502\r\n\u2502   1101 \u2502   \u2502 \
          \  kernel = self.attention_dropout(attn_weights)                       \
          \              \u2502\r\n\u2502                                        \
          \                                                          \u2502\r\n\u2502\
          \ /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:901\
          \ in         \u2502\r\n\u2502 softmax_attention                        \
          \                                                        \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502    898 \u2502   \u2502   if\
          \ causal_mask is not None:                                             \
          \          \u2502\r\n\u2502    899 \u2502   \u2502   \u2502   additive_causal_mask\
          \ = torch.zeros_like(causal_mask, dtype=qk.dtype)          \u2502\r\n\u2502\
          \    900 \u2502   \u2502   \u2502   additive_causal_mask = additive_causal_mask.masked_fill((1\
          \ - causal_mask).bo  \u2502\r\n\u2502 \u2771  901 \u2502   \u2502   \u2502\
          \   qk = qk + additive_causal_mask                                     \
          \           \u2502\r\n\u2502    902 \u2502   \u2502                    \
          \                                                                 \u2502\
          \r\n\u2502    903 \u2502   \u2502   if padding_mask is not None:       \
          \                                               \u2502\r\n\u2502    904\
          \ \u2502   \u2502   \u2502   # 1 for tokens which are *not masked*     \
          \                                    \u2502\r\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nRuntimeError:\
          \ The size of tensor a (16) must match the size of tensor b (13008) at non-singleton\
          \ dimension 3"
        updatedAt: '2023-05-06T04:54:04.018Z'
      numEdits: 0
      reactions: []
    id: 6455dd6c679315e4ef1756f5
    type: comment
  author: Tylersuard
  content: "Sorry to bother you again!  I am sure you are very busy.  I am having\
    \ trouble running your code.  It looks like there is a mismatch in tensor size\
    \ between the key/query and the additive causal mask.  My chunk size is 16, and\
    \ my max_positions is 13008, a multiple of 16.  What am I doing wrong here?\r\n\
    \r\n /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:1098\
    \ in     forward\u2502\r\n\u2502   1095 \u2502   \u2502   if padding_mask is not\
    \ None and padding_mask.dim() == 0:                          \u2502\r\n\u2502\
    \   1096 \u2502   \u2502   \u2502   padding_mask = None                      \
    \                                     \u2502\r\n\u2502   1097 \u2502   \u2502\
    \                                                                            \
    \         \u2502\r\n\u2502 \u2771 1098 \u2502   \u2502   attn_weights = self.attention_function(query,\
    \ key, padding_mask=padding_mask, ca  \u2502\r\n\u2502   1099 \u2502   \u2502\
    \                                                                            \
    \         \u2502\r\n\u2502   1100 \u2502   \u2502   value = self.hidden_dropout(value,\
    \ batch_first=True)                              \u2502\r\n\u2502   1101 \u2502\
    \   \u2502   kernel = self.attention_dropout(attn_weights)                   \
    \                  \u2502\r\n\u2502                                          \
    \                                                        \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/mega/modeling_mega.py:901\
    \ in         \u2502\r\n\u2502 softmax_attention                              \
    \                                                  \u2502\r\n\u2502          \
    \                                                                            \
    \            \u2502\r\n\u2502    898 \u2502   \u2502   if causal_mask is not None:\
    \                                                       \u2502\r\n\u2502    899\
    \ \u2502   \u2502   \u2502   additive_causal_mask = torch.zeros_like(causal_mask,\
    \ dtype=qk.dtype)          \u2502\r\n\u2502    900 \u2502   \u2502   \u2502  \
    \ additive_causal_mask = additive_causal_mask.masked_fill((1 - causal_mask).bo\
    \  \u2502\r\n\u2502 \u2771  901 \u2502   \u2502   \u2502   qk = qk + additive_causal_mask\
    \                                                \u2502\r\n\u2502    902 \u2502\
    \   \u2502                                                                   \
    \                  \u2502\r\n\u2502    903 \u2502   \u2502   if padding_mask is\
    \ not None:                                                      \u2502\r\n\u2502\
    \    904 \u2502   \u2502   \u2502   # 1 for tokens which are *not masked*    \
    \                                     \u2502\r\n\u2570\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256F\r\nRuntimeError: The size of tensor a (16) must match\
    \ the size of tensor b (13008) at non-singleton dimension 3"
  created_at: 2023-05-06 03:54:04+00:00
  edited: false
  hidden: false
  id: 6455dd6c679315e4ef1756f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-05-11T20:18:39.000Z'
    data:
      edited: false
      editors:
      - mnaylor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
          fullname: Mitch Naylor
          isHf: false
          isPro: false
          name: mnaylor
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Tylersuard&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tylersuard\"\
          >@<span class=\"underline\">Tylersuard</span></a></span>\n\n\t</span></span>.\
          \ As I mentioned in <a href=\"https://huggingface.co/mnaylor/mega-base-wikitext/discussions/1#6446e575c50af850001913ef\"\
          >your other post</a>, I'm not sure what is causing this. This would be a\
          \ good GitHub issue so the Hugging Face folks can look into it.</p>\n"
        raw: Hi @Tylersuard. As I mentioned in [your other post](https://huggingface.co/mnaylor/mega-base-wikitext/discussions/1#6446e575c50af850001913ef),
          I'm not sure what is causing this. This would be a good GitHub issue so
          the Hugging Face folks can look into it.
        updatedAt: '2023-05-11T20:18:39.016Z'
      numEdits: 0
      reactions: []
    id: 645d4d9f4438da4fcc2020a8
    type: comment
  author: mnaylor
  content: Hi @Tylersuard. As I mentioned in [your other post](https://huggingface.co/mnaylor/mega-base-wikitext/discussions/1#6446e575c50af850001913ef),
    I'm not sure what is causing this. This would be a good GitHub issue so the Hugging
    Face folks can look into it.
  created_at: 2023-05-11 19:18:39+00:00
  edited: false
  hidden: false
  id: 645d4d9f4438da4fcc2020a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-11-30T15:02:00.000Z'
    data:
      status: closed
    id: 6568a3e82693fa22e18edd02
    type: status-change
  author: mnaylor
  created_at: 2023-11-30 15:02:00+00:00
  id: 6568a3e82693fa22e18edd02
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mnaylor/mega-base-wikitext
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: The size of tensor a (16) must match the size of tensor b (13008)
  at non-singleton dimension 3'
