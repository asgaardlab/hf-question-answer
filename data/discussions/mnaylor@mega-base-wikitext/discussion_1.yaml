!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tylersuard
conflicting_files: null
created_at: 2023-04-21 04:00:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-21T05:00:03.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>I am having a hard time understanding Huggingface''s documentation.  I
          have a text dataset which uses only around 200 different tokens.  How would
          I go about training MegaForCausalLM on my dataset?  Thank you for your help.</p>

          '
        raw: I am having a hard time understanding Huggingface's documentation.  I
          have a text dataset which uses only around 200 different tokens.  How would
          I go about training MegaForCausalLM on my dataset?  Thank you for your help.
        updatedAt: '2023-04-21T05:00:03.946Z'
      numEdits: 0
      reactions: []
    id: 6442185328fb8c31ffc5c9a7
    type: comment
  author: Tylersuard
  content: I am having a hard time understanding Huggingface's documentation.  I have
    a text dataset which uses only around 200 different tokens.  How would I go about
    training MegaForCausalLM on my dataset?  Thank you for your help.
  created_at: 2023-04-21 04:00:03+00:00
  edited: false
  hidden: false
  id: 6442185328fb8c31ffc5c9a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-04-21T14:15:01.000Z'
    data:
      edited: false
      editors:
      - mnaylor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
          fullname: Mitch Naylor
          isHf: false
          isPro: false
          name: mnaylor
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Tylersuard&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tylersuard\"\
          >@<span class=\"underline\">Tylersuard</span></a></span>\n\n\t</span></span>\
          \ - I'd recommend checking out the Hugging Face course for both of those\
          \ topics. Chapter 6 covers <a href=\"https://huggingface.co/learn/nlp-course/chapter6/1\"\
          >building a custom tokenizer</a>, and Chapter 7 has a section on <a href=\"\
          https://huggingface.co/learn/nlp-course/chapter7/6\">training a causal LM\
          \ from scratch</a>. There is also a Causal Language Modeling section in\
          \ <a rel=\"nofollow\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\"\
          >this Hugging Face example notebook</a> which is fine-tuning from a pretrained\
          \ checkpoint, but the mechanics should be the same except that you'll be\
          \ starting with a blank model and probably training for longer, with different\
          \ hyperparameters, etc.</p>\n<p>As for MEGA specifically, you should be\
          \ able to use whichever base tokenizer type you'd like, since there isn't\
          \ really a MEGA tokenizer -- the one I used here is the same tokenizer used\
          \ by RoBERTa. When you create your model class, you'll want to set <code>is_decoder=True</code>\
          \ and <code>bidirectional=False</code> in the <code>MegaConfig</code> for\
          \ compatibility with autoregressive language modeling.</p>\n"
        raw: 'Hi @Tylersuard - I''d recommend checking out the Hugging Face course
          for both of those topics. Chapter 6 covers [building a custom tokenizer](https://huggingface.co/learn/nlp-course/chapter6/1),
          and Chapter 7 has a section on [training a causal LM from scratch](https://huggingface.co/learn/nlp-course/chapter7/6).
          There is also a Causal Language Modeling section in [this Hugging Face example
          notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
          which is fine-tuning from a pretrained checkpoint, but the mechanics should
          be the same except that you''ll be starting with a blank model and probably
          training for longer, with different hyperparameters, etc.


          As for MEGA specifically, you should be able to use whichever base tokenizer
          type you''d like, since there isn''t really a MEGA tokenizer -- the one
          I used here is the same tokenizer used by RoBERTa. When you create your
          model class, you''ll want to set `is_decoder=True` and `bidirectional=False`
          in the `MegaConfig` for compatibility with autoregressive language modeling.'
        updatedAt: '2023-04-21T14:15:01.947Z'
      numEdits: 0
      reactions: []
    id: 64429a65ce9c6d85e5b9bb6a
    type: comment
  author: mnaylor
  content: 'Hi @Tylersuard - I''d recommend checking out the Hugging Face course for
    both of those topics. Chapter 6 covers [building a custom tokenizer](https://huggingface.co/learn/nlp-course/chapter6/1),
    and Chapter 7 has a section on [training a causal LM from scratch](https://huggingface.co/learn/nlp-course/chapter7/6).
    There is also a Causal Language Modeling section in [this Hugging Face example
    notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
    which is fine-tuning from a pretrained checkpoint, but the mechanics should be
    the same except that you''ll be starting with a blank model and probably training
    for longer, with different hyperparameters, etc.


    As for MEGA specifically, you should be able to use whichever base tokenizer type
    you''d like, since there isn''t really a MEGA tokenizer -- the one I used here
    is the same tokenizer used by RoBERTa. When you create your model class, you''ll
    want to set `is_decoder=True` and `bidirectional=False` in the `MegaConfig` for
    compatibility with autoregressive language modeling.'
  created_at: 2023-04-21 13:15:01+00:00
  edited: false
  hidden: false
  id: 64429a65ce9c6d85e5b9bb6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-22T15:07:16.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>Thank you!  I really appreciate your help.  I am going through the
          tutorials now.</p>

          '
        raw: Thank you!  I really appreciate your help.  I am going through the tutorials
          now.
        updatedAt: '2023-04-22T15:07:16.390Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6443f8245298d19c9cfee9f0
    id: 6443f8245298d19c9cfee9ef
    type: comment
  author: Tylersuard
  content: Thank you!  I really appreciate your help.  I am going through the tutorials
    now.
  created_at: 2023-04-22 14:07:16+00:00
  edited: false
  hidden: false
  id: 6443f8245298d19c9cfee9ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-22T15:07:16.000Z'
    data:
      status: closed
    id: 6443f8245298d19c9cfee9f0
    type: status-change
  author: Tylersuard
  created_at: 2023-04-22 14:07:16+00:00
  id: 6443f8245298d19c9cfee9f0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-22T21:38:30.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>I am running into an error on line 901 of modeling_mega.py, where
          an additive mask is created from a causal mask.</p>

          <p>I am using max_positions of 13008 and a chunk_size of 16, with chunking
          set to True.</p>

          <p>It appears that tensors will go through that function just fine, until
          eventually a tensor goes through that is only the size of one chunk, and
          that code is expecting a full sequence tensor of 13008 tokens.</p>

          <p>I get this error:<br>softmax RuntimeError: The size of tensor a (16)
          must match the size of tensor b (13008) at non-singleton dimension 3</p>

          <p>Do you know what is going on here?  I went through my dataset and it
          appears that all samples are of uniform length.</p>

          <p>This is what my dataset looks like:</p>

          <p>DatasetDict({<br>    test: Dataset({<br>        features: [''input_ids'',
          ''attention_mask'', ''labels''],<br>        num_rows: 2403<br>    })<br>    validation:
          Dataset({<br>        features: [''input_ids'', ''attention_mask'', ''labels''],<br>        num_rows:
          2423<br>    })<br>    train: Dataset({<br>        features: [''input_ids'',
          ''attention_mask'', ''labels''],<br>        num_rows: 19021<br>    })<br>})</p>

          <p>Thank you for your help.</p>

          '
        raw: "I am running into an error on line 901 of modeling_mega.py, where an\
          \ additive mask is created from a causal mask.\n\nI am using max_positions\
          \ of 13008 and a chunk_size of 16, with chunking set to True.\n\nIt appears\
          \ that tensors will go through that function just fine, until eventually\
          \ a tensor goes through that is only the size of one chunk, and that code\
          \ is expecting a full sequence tensor of 13008 tokens.\n\nI get this error:\n\
          softmax RuntimeError: The size of tensor a (16) must match the size of tensor\
          \ b (13008) at non-singleton dimension 3\n\nDo you know what is going on\
          \ here?  I went through my dataset and it appears that all samples are of\
          \ uniform length.\n\nThis is what my dataset looks like:\n\nDatasetDict({\n\
          \    test: Dataset({\n        features: ['input_ids', 'attention_mask',\
          \ 'labels'],\n        num_rows: 2403\n    })\n    validation: Dataset({\n\
          \        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows:\
          \ 2423\n    })\n    train: Dataset({\n        features: ['input_ids', 'attention_mask',\
          \ 'labels'],\n        num_rows: 19021\n    })\n})\n\nThank you for your\
          \ help."
        updatedAt: '2023-04-22T21:38:30.663Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644453d65298d19c9c049a76
    id: 644453d65298d19c9c049a75
    type: comment
  author: Tylersuard
  content: "I am running into an error on line 901 of modeling_mega.py, where an additive\
    \ mask is created from a causal mask.\n\nI am using max_positions of 13008 and\
    \ a chunk_size of 16, with chunking set to True.\n\nIt appears that tensors will\
    \ go through that function just fine, until eventually a tensor goes through that\
    \ is only the size of one chunk, and that code is expecting a full sequence tensor\
    \ of 13008 tokens.\n\nI get this error:\nsoftmax RuntimeError: The size of tensor\
    \ a (16) must match the size of tensor b (13008) at non-singleton dimension 3\n\
    \nDo you know what is going on here?  I went through my dataset and it appears\
    \ that all samples are of uniform length.\n\nThis is what my dataset looks like:\n\
    \nDatasetDict({\n    test: Dataset({\n        features: ['input_ids', 'attention_mask',\
    \ 'labels'],\n        num_rows: 2403\n    })\n    validation: Dataset({\n    \
    \    features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2423\n\
    \    })\n    train: Dataset({\n        features: ['input_ids', 'attention_mask',\
    \ 'labels'],\n        num_rows: 19021\n    })\n})\n\nThank you for your help."
  created_at: 2023-04-22 20:38:30+00:00
  edited: false
  hidden: false
  id: 644453d65298d19c9c049a75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-22T21:38:30.000Z'
    data:
      status: open
    id: 644453d65298d19c9c049a76
    type: status-change
  author: Tylersuard
  created_at: 2023-04-22 20:38:30+00:00
  id: 644453d65298d19c9c049a76
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-04-24T20:24:21.000Z'
    data:
      edited: false
      editors:
      - mnaylor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
          fullname: Mitch Naylor
          isHf: false
          isPro: false
          name: mnaylor
          type: user
        html: '<p>I''m honestly not sure what''s causing this, and it''s hard to guess
          based on the error alone. If all of your samples are of uniform length,
          then I''m not sure how one batch would have a different number of chunks
          than the others. The only thing I can think of would be making sure that
          <code>pad_to_multiple_of</code> in your tokenizer is set to the <code>chunk_size</code>.
          </p>

          <p>This would probably also be a good candidate for a GitHub issue with
          a minimal example to reproduce the behavior</p>

          '
        raw: "I'm honestly not sure what's causing this, and it's hard to guess based\
          \ on the error alone. If all of your samples are of uniform length, then\
          \ I'm not sure how one batch would have a different number of chunks than\
          \ the others. The only thing I can think of would be making sure that `pad_to_multiple_of`\
          \ in your tokenizer is set to the `chunk_size`. \n\nThis would probably\
          \ also be a good candidate for a GitHub issue with a minimal example to\
          \ reproduce the behavior"
        updatedAt: '2023-04-24T20:24:21.290Z'
      numEdits: 0
      reactions: []
    id: 6446e575c50af850001913ef
    type: comment
  author: mnaylor
  content: "I'm honestly not sure what's causing this, and it's hard to guess based\
    \ on the error alone. If all of your samples are of uniform length, then I'm not\
    \ sure how one batch would have a different number of chunks than the others.\
    \ The only thing I can think of would be making sure that `pad_to_multiple_of`\
    \ in your tokenizer is set to the `chunk_size`. \n\nThis would probably also be\
    \ a good candidate for a GitHub issue with a minimal example to reproduce the\
    \ behavior"
  created_at: 2023-04-24 19:24:21+00:00
  edited: false
  hidden: false
  id: 6446e575c50af850001913ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-11-30T15:02:07.000Z'
    data:
      status: closed
    id: 6568a3ef677a71b8ab23fa5b
    type: status-change
  author: mnaylor
  created_at: 2023-11-30 15:02:07+00:00
  id: 6568a3ef677a71b8ab23fa5b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mnaylor/mega-base-wikitext
repo_type: model
status: closed
target_branch: null
title: First of all, thank you for making this.  Second, how can I train my own model
  (and tokenizer) from a custom dataset?
