!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tylersuard
conflicting_files: null
created_at: 2023-04-24 17:25:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-24T18:25:11.000Z'
    data:
      edited: true
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>This is the example code from the documentation for MegaForCausalLM
          (<a href="https://huggingface.co/docs/transformers/main/model_doc/mega">https://huggingface.co/docs/transformers/main/model_doc/mega</a>):</p>

          <p>from transformers import AutoTokenizer, MegaForCausalLM, AutoConfig<br>import
          torch</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("mnaylor/mega-base-wikitext")<br>config
          = AutoConfig.from_pretrained("mnaylor/mega-base-wikitext")<br>config.is_decoder
          = True<br>config.bidirectional = False<br>model = MegaForCausalLM.from_pretrained("mnaylor/mega-base-wikitext",
          config=config)</p>

          <p>inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")<br>outputs
          = model(**inputs)</p>

          <p>prediction_logits = outputs.logits</p>

          <p>After installing Transformers from source, when I run the above code
          snippet on Colab, I get this error:</p>

          <p>RuntimeError: Error(s) in loading state_dict for MegaForCausalLM:<br>    size
          mismatch for mega.layers.0.mega_layer.ema_gate.damping_factor: copying a
          param with shape torch.Size([256, 16, 1]) from checkpoint, the shape in
          current model is torch.Size([128, 16, 1]).<br>    size mismatch for mega.layers.0.mega_layer.ema_gate.decay_factor:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.0.mega_layer.ema_gate.ema_expansion_matrix: copying a param
          with shape torch.Size([256, 16, 1]) from checkpoint, the shape in current
          model is torch.Size([128, 16, 1]).<br>    size mismatch for mega.layers.0.mega_layer.ema_gate.kernel_projection_matrix:
          copying a param with shape torch.Size([256, 16]) from checkpoint, the shape
          in current model is torch.Size([128, 16]).<br>    size mismatch for mega.layers.1.mega_layer.ema_gate.damping_factor:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.1.mega_layer.ema_gate.decay_factor: copying a param with
          shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model
          is torch.Size([128, 16, 1]).<br>    size mismatch for mega.layers.1.mega_layer.ema_gate.ema_expansion_matrix:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.1.mega_layer.ema_gate.kernel_projection_matrix: copying
          a param with shape torch.Size([256, 16]) from checkpoint, the shape in current
          model is torch.Size([128, 16]).<br>    size mismatch for mega.layers.2.mega_layer.ema_gate.damping_factor:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.2.mega_layer.ema_gate.decay_factor: copying a param with
          shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model
          is torch.Size([128, 16, 1]).<br>    size mismatch for mega.layers.2.mega_layer.ema_gate.ema_expansion_matrix:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.2.mega_layer.ema_gate.kernel_projection_matrix: copying
          a param with shape torch.Size([256, 16]) from checkpoint, the shape in current
          model is torch.Size([128, 16]).<br>    size mismatch for mega.layers.3.mega_layer.ema_gate.damping_factor:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.3.mega_layer.ema_gate.decay_factor: copying a param with
          shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model
          is torch.Size([128, 16, 1]).<br>    size mismatch for mega.layers.3.mega_layer.ema_gate.ema_expansion_matrix:
          copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([128, 16, 1]).<br>    size mismatch
          for mega.layers.3.mega_layer.ema_gate.kernel_projection_matrix: copying
          a param with shape torch.Size([256, 16]) from checkpoint, the shape in current
          model is torch.Size([128, 16]).<br>    You may consider adding <code>ignore_mismatched_sizes=True</code>
          in the model <code>from_pretrained</code> method.</p>

          '
        raw: "This is the example code from the documentation for MegaForCausalLM\
          \ (https://huggingface.co/docs/transformers/main/model_doc/mega):\n\nfrom\
          \ transformers import AutoTokenizer, MegaForCausalLM, AutoConfig\nimport\
          \ torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"mnaylor/mega-base-wikitext\"\
          )\nconfig = AutoConfig.from_pretrained(\"mnaylor/mega-base-wikitext\")\n\
          config.is_decoder = True\nconfig.bidirectional = False\nmodel = MegaForCausalLM.from_pretrained(\"\
          mnaylor/mega-base-wikitext\", config=config)\n\ninputs = tokenizer(\"Hello,\
          \ my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\n\
          prediction_logits = outputs.logits\n\n\nAfter installing Transformers from\
          \ source, when I run the above code snippet on Colab, I get this error:\n\
          \nRuntimeError: Error(s) in loading state_dict for MegaForCausalLM:\n\t\
          size mismatch for mega.layers.0.mega_layer.ema_gate.damping_factor: copying\
          \ a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
          \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.decay_factor:\
          \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for\
          \ mega.layers.0.mega_layer.ema_gate.ema_expansion_matrix: copying a param\
          \ with shape torch.Size([256, 16, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.kernel_projection_matrix:\
          \ copying a param with shape torch.Size([256, 16]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16]).\n\tsize mismatch for\
          \ mega.layers.1.mega_layer.ema_gate.damping_factor: copying a param with\
          \ shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model\
          \ is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.decay_factor:\
          \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for\
          \ mega.layers.1.mega_layer.ema_gate.ema_expansion_matrix: copying a param\
          \ with shape torch.Size([256, 16, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.kernel_projection_matrix:\
          \ copying a param with shape torch.Size([256, 16]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16]).\n\tsize mismatch for\
          \ mega.layers.2.mega_layer.ema_gate.damping_factor: copying a param with\
          \ shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model\
          \ is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.decay_factor:\
          \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for\
          \ mega.layers.2.mega_layer.ema_gate.ema_expansion_matrix: copying a param\
          \ with shape torch.Size([256, 16, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.kernel_projection_matrix:\
          \ copying a param with shape torch.Size([256, 16]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16]).\n\tsize mismatch for\
          \ mega.layers.3.mega_layer.ema_gate.damping_factor: copying a param with\
          \ shape torch.Size([256, 16, 1]) from checkpoint, the shape in current model\
          \ is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.decay_factor:\
          \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for\
          \ mega.layers.3.mega_layer.ema_gate.ema_expansion_matrix: copying a param\
          \ with shape torch.Size([256, 16, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.kernel_projection_matrix:\
          \ copying a param with shape torch.Size([256, 16]) from checkpoint, the\
          \ shape in current model is torch.Size([128, 16]).\n\tYou may consider adding\
          \ `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
        updatedAt: '2023-04-24T18:26:58.710Z'
      numEdits: 1
      reactions: []
    id: 6446c987cffecebe8b5e75c5
    type: comment
  author: Tylersuard
  content: "This is the example code from the documentation for MegaForCausalLM (https://huggingface.co/docs/transformers/main/model_doc/mega):\n\
    \nfrom transformers import AutoTokenizer, MegaForCausalLM, AutoConfig\nimport\
    \ torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"mnaylor/mega-base-wikitext\"\
    )\nconfig = AutoConfig.from_pretrained(\"mnaylor/mega-base-wikitext\")\nconfig.is_decoder\
    \ = True\nconfig.bidirectional = False\nmodel = MegaForCausalLM.from_pretrained(\"\
    mnaylor/mega-base-wikitext\", config=config)\n\ninputs = tokenizer(\"Hello, my\
    \ dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprediction_logits\
    \ = outputs.logits\n\n\nAfter installing Transformers from source, when I run\
    \ the above code snippet on Colab, I get this error:\n\nRuntimeError: Error(s)\
    \ in loading state_dict for MegaForCausalLM:\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.damping_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.decay_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.ema_expansion_matrix:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.0.mega_layer.ema_gate.kernel_projection_matrix:\
    \ copying a param with shape torch.Size([256, 16]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.damping_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.decay_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.ema_expansion_matrix:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.1.mega_layer.ema_gate.kernel_projection_matrix:\
    \ copying a param with shape torch.Size([256, 16]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.damping_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.decay_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.ema_expansion_matrix:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.2.mega_layer.ema_gate.kernel_projection_matrix:\
    \ copying a param with shape torch.Size([256, 16]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.damping_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.decay_factor:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.ema_expansion_matrix:\
    \ copying a param with shape torch.Size([256, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16, 1]).\n\tsize mismatch for mega.layers.3.mega_layer.ema_gate.kernel_projection_matrix:\
    \ copying a param with shape torch.Size([256, 16]) from checkpoint, the shape\
    \ in current model is torch.Size([128, 16]).\n\tYou may consider adding `ignore_mismatched_sizes=True`\
    \ in the model `from_pretrained` method."
  created_at: 2023-04-24 17:25:11+00:00
  edited: true
  hidden: false
  id: 6446c987cffecebe8b5e75c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
      fullname: Mitch Naylor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mnaylor
      type: user
    createdAt: '2023-04-24T20:18:53.000Z'
    data:
      edited: false
      editors:
      - mnaylor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/848184da060a7893a6286fbfc9772142.svg
          fullname: Mitch Naylor
          isHf: false
          isPro: false
          name: mnaylor
          type: user
        html: '<p>Ahh, good catch that the example in the docs doesn''t work. The
          reason is that the checkpoint is for a masked LM, and it seems that bidirectional
          and unidirectional models have different parameter sizes in the EMA layer.
          I must have missed that when setting it up initially. There is no unidirectional
          checkpoint at the moment as far as I''m aware.</p>

          <p>As a side note, this isn''t really the place for this sort of discussion
          - I am neither a Hugging Face employee nor a maintainer of the library.
          I just contributed the initial implementation into the Hugging Face library
          by translating the original repo, and I made this MLM checkpoint for that
          + getting started with BERT-like tasks. If you run into issues with Transformers
          package usage, you''ll get better help by opening an issue on their GitHub.
          I''ll try to answer the question in your other thread as well, but I hope
          you understand that I won''t be able to provide much detailed or ongoing
          support.</p>

          '
        raw: 'Ahh, good catch that the example in the docs doesn''t work. The reason
          is that the checkpoint is for a masked LM, and it seems that bidirectional
          and unidirectional models have different parameter sizes in the EMA layer.
          I must have missed that when setting it up initially. There is no unidirectional
          checkpoint at the moment as far as I''m aware.


          As a side note, this isn''t really the place for this sort of discussion
          - I am neither a Hugging Face employee nor a maintainer of the library.
          I just contributed the initial implementation into the Hugging Face library
          by translating the original repo, and I made this MLM checkpoint for that
          + getting started with BERT-like tasks. If you run into issues with Transformers
          package usage, you''ll get better help by opening an issue on their GitHub.
          I''ll try to answer the question in your other thread as well, but I hope
          you understand that I won''t be able to provide much detailed or ongoing
          support.'
        updatedAt: '2023-04-24T20:18:53.653Z'
      numEdits: 0
      reactions: []
    id: 6446e42dc50af8500018fe85
    type: comment
  author: mnaylor
  content: 'Ahh, good catch that the example in the docs doesn''t work. The reason
    is that the checkpoint is for a masked LM, and it seems that bidirectional and
    unidirectional models have different parameter sizes in the EMA layer. I must
    have missed that when setting it up initially. There is no unidirectional checkpoint
    at the moment as far as I''m aware.


    As a side note, this isn''t really the place for this sort of discussion - I am
    neither a Hugging Face employee nor a maintainer of the library. I just contributed
    the initial implementation into the Hugging Face library by translating the original
    repo, and I made this MLM checkpoint for that + getting started with BERT-like
    tasks. If you run into issues with Transformers package usage, you''ll get better
    help by opening an issue on their GitHub. I''ll try to answer the question in
    your other thread as well, but I hope you understand that I won''t be able to
    provide much detailed or ongoing support.'
  created_at: 2023-04-24 19:18:53+00:00
  edited: false
  hidden: false
  id: 6446e42dc50af8500018fe85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-24T22:42:46.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>Thank you!  I appreciate your help.  I will post this on their Github.</p>

          '
        raw: Thank you!  I appreciate your help.  I will post this on their Github.
        updatedAt: '2023-04-24T22:42:46.590Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644705e61dc59ca8aed4c8b3
    id: 644705e61dc59ca8aed4c8b2
    type: comment
  author: Tylersuard
  content: Thank you!  I appreciate your help.  I will post this on their Github.
  created_at: 2023-04-24 21:42:46+00:00
  edited: false
  hidden: false
  id: 644705e61dc59ca8aed4c8b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-04-24T22:42:46.000Z'
    data:
      status: closed
    id: 644705e61dc59ca8aed4c8b3
    type: status-change
  author: Tylersuard
  created_at: 2023-04-24 21:42:46+00:00
  id: 644705e61dc59ca8aed4c8b3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: mnaylor/mega-base-wikitext
repo_type: model
status: closed
target_branch: null
title: 'Error when running example code: size mismatches for every layer'
