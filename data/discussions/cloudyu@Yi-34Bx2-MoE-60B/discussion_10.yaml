!!python/object:huggingface_hub.community.DiscussionWithDetails
author: regzhang
conflicting_files: null
created_at: 2024-01-18 12:01:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3320b4ec31abe30447aa80eef73d5d9.svg
      fullname: regzhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: regzhang
      type: user
    createdAt: '2024-01-18T12:01:23.000Z'
    data:
      edited: false
      editors:
      - regzhang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8265541195869446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3320b4ec31abe30447aa80eef73d5d9.svg
          fullname: regzhang
          isHf: false
          isPro: false
          name: regzhang
          type: user
        html: '<p>Can the VLLM inference framework support running inference with
          this model? How can it be adjusted or modified to run on a setup with 8
          Nvidia RTX 3090 GPUs?</p>

          '
        raw: Can the VLLM inference framework support running inference with this
          model? How can it be adjusted or modified to run on a setup with 8 Nvidia
          RTX 3090 GPUs?
        updatedAt: '2024-01-18T12:01:23.312Z'
      numEdits: 0
      reactions: []
    id: 65a913131972c812eab4ac4c
    type: comment
  author: regzhang
  content: Can the VLLM inference framework support running inference with this model?
    How can it be adjusted or modified to run on a setup with 8 Nvidia RTX 3090 GPUs?
  created_at: 2024-01-18 12:01:23+00:00
  edited: false
  hidden: false
  id: 65a913131972c812eab4ac4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2024-01-18T13:06:42.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413134455680847
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: '<p>it is mixtral architecture supported by vllm, but I have no idea
          how to setup with 8 Nvidia RTX 3090 GPUs.</p>

          '
        raw: it is mixtral architecture supported by vllm, but I have no idea how
          to setup with 8 Nvidia RTX 3090 GPUs.
        updatedAt: '2024-01-18T13:06:42.682Z'
      numEdits: 0
      reactions: []
    id: 65a92262c5bf09bc939e4f88
    type: comment
  author: cloudyu
  content: it is mixtral architecture supported by vllm, but I have no idea how to
    setup with 8 Nvidia RTX 3090 GPUs.
  created_at: 2024-01-18 13:06:42+00:00
  edited: false
  hidden: false
  id: 65a92262c5bf09bc939e4f88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab4a41d3b9cf0b7e2c22001ba8516773.svg
      fullname: Neo Fung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neofung
      type: user
    createdAt: '2024-01-25T11:19:32.000Z'
    data:
      edited: false
      editors:
      - neofung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244163036346436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab4a41d3b9cf0b7e2c22001ba8516773.svg
          fullname: Neo Fung
          isHf: false
          isPro: false
          name: neofung
          type: user
        html: '<p>i think you are looking for this <a rel="nofollow" href="https://github.com/vllm-project/vllm/pull/2293">https://github.com/vllm-project/vllm/pull/2293</a></p>

          '
        raw: i think you are looking for this https://github.com/vllm-project/vllm/pull/2293
        updatedAt: '2024-01-25T11:19:32.391Z'
      numEdits: 0
      reactions: []
    id: 65b243c44f699ea319a93eaf
    type: comment
  author: neofung
  content: i think you are looking for this https://github.com/vllm-project/vllm/pull/2293
  created_at: 2024-01-25 11:19:32+00:00
  edited: false
  hidden: false
  id: 65b243c44f699ea319a93eaf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: cloudyu/Yi-34Bx2-MoE-60B
repo_type: model
status: open
target_branch: null
title: vllm
