!!python/object:huggingface_hub.community.DiscussionWithDetails
author: masterfury
conflicting_files: null
created_at: 2023-05-10 17:51:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
      fullname: Manish Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: masterfury
      type: user
    createdAt: '2023-05-10T18:51:34.000Z'
    data:
      edited: false
      editors:
      - masterfury
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
          fullname: Manish Kumar
          isHf: false
          isPro: false
          name: masterfury
          type: user
        html: '<p>I am trying to use ''OpenAssistant/pythia-12b-sft-v8-7k-steps''
          with hugging face ''text-generation-inference'', but sometimes it repeats
          last token or sentence,  sample picture attached. Can anyone explain why
          this is happening and what''s the fix, I have already tried playing around
          token length and other parameters.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6451de8ac5d273f95483aaac/whunHcrc7q3wYx6iGi2H1.png"><img
          alt="image_error_pythia.png" src="https://cdn-uploads.huggingface.co/production/uploads/6451de8ac5d273f95483aaac/whunHcrc7q3wYx6iGi2H1.png"></a></p>

          '
        raw: "I am trying to use 'OpenAssistant/pythia-12b-sft-v8-7k-steps' with hugging\
          \ face 'text-generation-inference', but sometimes it repeats last token\
          \ or sentence,  sample picture attached. Can anyone explain why this is\
          \ happening and what's the fix, I have already tried playing around token\
          \ length and other parameters.\r\n\r\n![image_error_pythia.png](https://cdn-uploads.huggingface.co/production/uploads/6451de8ac5d273f95483aaac/whunHcrc7q3wYx6iGi2H1.png)"
        updatedAt: '2023-05-10T18:51:34.049Z'
      numEdits: 0
      reactions: []
    id: 645be7b61c24cd669dd96d6a
    type: comment
  author: masterfury
  content: "I am trying to use 'OpenAssistant/pythia-12b-sft-v8-7k-steps' with hugging\
    \ face 'text-generation-inference', but sometimes it repeats last token or sentence,\
    \  sample picture attached. Can anyone explain why this is happening and what's\
    \ the fix, I have already tried playing around token length and other parameters.\r\
    \n\r\n![image_error_pythia.png](https://cdn-uploads.huggingface.co/production/uploads/6451de8ac5d273f95483aaac/whunHcrc7q3wYx6iGi2H1.png)"
  created_at: 2023-05-10 17:51:34+00:00
  edited: false
  hidden: false
  id: 645be7b61c24cd669dd96d6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
      fullname: "Andreas K\xF6pf"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: andreaskoepf
      type: user
    createdAt: '2023-05-10T22:03:53.000Z'
    data:
      edited: true
      editors:
      - andreaskoepf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
          fullname: "Andreas K\xF6pf"
          isHf: false
          isPro: false
          name: andreaskoepf
          type: user
        html: '<p>Did you prompt the model with the OA dialogue template, e.g. <code>&lt;|prompter|&gt;{query}&lt;|endoftext|&gt;&lt;|assistant|&gt;</code>?
          Which sampling parameters did you use?</p>

          '
        raw: Did you prompt the model with the OA dialogue template, e.g. `<|prompter|>{query}<|endoftext|><|assistant|>`?
          Which sampling parameters did you use?
        updatedAt: '2023-05-10T22:04:16.689Z'
      numEdits: 1
      reactions: []
    id: 645c14c90f9f526e8d01c426
    type: comment
  author: andreaskoepf
  content: Did you prompt the model with the OA dialogue template, e.g. `<|prompter|>{query}<|endoftext|><|assistant|>`?
    Which sampling parameters did you use?
  created_at: 2023-05-10 21:03:53+00:00
  edited: true
  hidden: false
  id: 645c14c90f9f526e8d01c426
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
      fullname: Manish Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: masterfury
      type: user
    createdAt: '2023-05-11T05:08:10.000Z'
    data:
      edited: false
      editors:
      - masterfury
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
          fullname: Manish Kumar
          isHf: false
          isPro: false
          name: masterfury
          type: user
        html: '<p>Yes, I am using this template only. Here are the parameters I''m
          using -<br>{<br>  "inputs": "&lt;|prompter|&gt;What is a meme, and what''s
          the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;",<br>  "parameters":
          {<br>    "best_of": 1,<br>    "details": true,<br>    "do_sample": true,<br>    "max_new_tokens":
          1024,<br>    "repetition_penalty": 1.03,<br>    "return_full_text": true,<br>    "seed":
          null,<br>    "stop": [""],<br>    "temperature": 0.5,<br>    "top_k": 10,<br>    "top_p":
          0.95,<br>    "truncate": null,<br>    "typical_p": 0.95,<br>    "watermark":
          true<br>  }<br>}</p>

          '
        raw: "Yes, I am using this template only. Here are the parameters I'm using\
          \ - \n{\n  \"inputs\": \"<|prompter|>What is a meme, and what's the history\
          \ behind this word?<|endoftext|><|assistant|>\",\n  \"parameters\": {\n\
          \    \"best_of\": 1,\n    \"details\": true,\n    \"do_sample\": true,\n\
          \    \"max_new_tokens\": 1024,\n    \"repetition_penalty\": 1.03,\n    \"\
          return_full_text\": true,\n    \"seed\": null,\n    \"stop\": [\"\"],\n\
          \    \"temperature\": 0.5,\n    \"top_k\": 10,\n    \"top_p\": 0.95,\n \
          \   \"truncate\": null,\n    \"typical_p\": 0.95,\n    \"watermark\": true\n\
          \  }\n}"
        updatedAt: '2023-05-11T05:08:10.243Z'
      numEdits: 0
      reactions: []
    id: 645c783ab0c06125080f29dd
    type: comment
  author: masterfury
  content: "Yes, I am using this template only. Here are the parameters I'm using\
    \ - \n{\n  \"inputs\": \"<|prompter|>What is a meme, and what's the history behind\
    \ this word?<|endoftext|><|assistant|>\",\n  \"parameters\": {\n    \"best_of\"\
    : 1,\n    \"details\": true,\n    \"do_sample\": true,\n    \"max_new_tokens\"\
    : 1024,\n    \"repetition_penalty\": 1.03,\n    \"return_full_text\": true,\n\
    \    \"seed\": null,\n    \"stop\": [\"\"],\n    \"temperature\": 0.5,\n    \"\
    top_k\": 10,\n    \"top_p\": 0.95,\n    \"truncate\": null,\n    \"typical_p\"\
    : 0.95,\n    \"watermark\": true\n  }\n}"
  created_at: 2023-05-11 04:08:10+00:00
  edited: false
  hidden: false
  id: 645c783ab0c06125080f29dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-05-11T08:53:01.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>I face similar issues with <code>OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</code>
          when running in normal fp16 with hugging face text generation inference
          server. But when i enable quantization, then this kind of behaviour  will
          not happen and model generates reasonable text, but very very slow (due
          to know problem with bits and bytes). Not sure where the issue is.</p>

          '
        raw: I face similar issues with `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`
          when running in normal fp16 with hugging face text generation inference
          server. But when i enable quantization, then this kind of behaviour  will
          not happen and model generates reasonable text, but very very slow (due
          to know problem with bits and bytes). Not sure where the issue is.
        updatedAt: '2023-05-11T08:53:01.153Z'
      numEdits: 0
      reactions: []
    id: 645cacedf1e3b219cb074acb
    type: comment
  author: gsaivinay
  content: I face similar issues with `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`
    when running in normal fp16 with hugging face text generation inference server.
    But when i enable quantization, then this kind of behaviour  will not happen and
    model generates reasonable text, but very very slow (due to know problem with
    bits and bytes). Not sure where the issue is.
  created_at: 2023-05-11 07:53:01+00:00
  edited: false
  hidden: false
  id: 645cacedf1e3b219cb074acb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
      fullname: Manish Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: masterfury
      type: user
    createdAt: '2023-05-11T09:34:25.000Z'
    data:
      edited: false
      editors:
      - masterfury
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
          fullname: Manish Kumar
          isHf: false
          isPro: false
          name: masterfury
          type: user
        html: '<p>Yes, even with ''OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5''
          same issue is there. I thought newer version will not have this issue.</p>

          '
        raw: Yes, even with 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5' same
          issue is there. I thought newer version will not have this issue.
        updatedAt: '2023-05-11T09:34:25.138Z'
      numEdits: 0
      reactions: []
    id: 645cb6a18a63155046d7b214
    type: comment
  author: masterfury
  content: Yes, even with 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5' same issue
    is there. I thought newer version will not have this issue.
  created_at: 2023-05-11 08:34:25+00:00
  edited: false
  hidden: false
  id: 645cb6a18a63155046d7b214
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
      fullname: "Andreas K\xF6pf"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: andreaskoepf
      type: user
    createdAt: '2023-05-11T10:27:04.000Z'
    data:
      edited: false
      editors:
      - andreaskoepf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b888106cf60e8c3d00dc86/lNCiIc424q60PC2D33vxN.jpeg?w=200&h=200&f=face
          fullname: "Andreas K\xF6pf"
          isHf: false
          isPro: false
          name: andreaskoepf
          type: user
        html: '<p>Please take a look at the <a rel="nofollow" href="https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/oasst-sft/2023-05-07_OpenAssistant_pythia-12b-sft-v8-7k-steps_sampling_noprefix2.json">sampling
          report</a> .. which was generated with the huggingface transformer library
          for the model .. all continuations that end with <code>&lt;|endoftext|&gt;</code>
          are correctly finished within the max_new_token limit.<br>Is the tokenizers
          end-of-sequence token automatically respected by HF''s  text-generation
          server, e.g. is <code>"stop": [""],</code> correct? For 100 random prompts
          how often does it correctly end and how often do you see the junk?</p>

          '
        raw: "Please take a look at the [sampling report](https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/oasst-sft/2023-05-07_OpenAssistant_pythia-12b-sft-v8-7k-steps_sampling_noprefix2.json)\
          \ .. which was generated with the huggingface transformer library for the\
          \ model .. all continuations that end with `<|endoftext|>` are correctly\
          \ finished within the max_new_token limit. \nIs the tokenizers end-of-sequence\
          \ token automatically respected by HF's  text-generation server, e.g. is\
          \ `\"stop\": [\"\"],` correct? For 100 random prompts how often does it\
          \ correctly end and how often do you see the junk?"
        updatedAt: '2023-05-11T10:27:04.332Z'
      numEdits: 0
      reactions: []
    id: 645cc2f8f1e3b219cb084902
    type: comment
  author: andreaskoepf
  content: "Please take a look at the [sampling report](https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/oasst-sft/2023-05-07_OpenAssistant_pythia-12b-sft-v8-7k-steps_sampling_noprefix2.json)\
    \ .. which was generated with the huggingface transformer library for the model\
    \ .. all continuations that end with `<|endoftext|>` are correctly finished within\
    \ the max_new_token limit. \nIs the tokenizers end-of-sequence token automatically\
    \ respected by HF's  text-generation server, e.g. is `\"stop\": [\"\"],` correct?\
    \ For 100 random prompts how often does it correctly end and how often do you\
    \ see the junk?"
  created_at: 2023-05-11 09:27:04+00:00
  edited: false
  hidden: false
  id: 645cc2f8f1e3b219cb084902
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
      fullname: Manish Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: masterfury
      type: user
    createdAt: '2023-05-11T10:47:00.000Z'
    data:
      edited: false
      editors:
      - masterfury
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d5ca8912e3d77c4db4ca09c339e7b36.svg
          fullname: Manish Kumar
          isHf: false
          isPro: false
          name: masterfury
          type: user
        html: "<p>Sure, I will have a look at sampling report.<br>My observations\
          \ so far around this issue is - </p>\n<ol>\n<li>For 100 random prompts 40-50\
          \ times this issue pops up. (Tested this for 3 weeks)</li>\n<li>This happens\
          \ when the api finish reason is length.</li>\n</ol>\n<pre><code># Generation\
          \ finish reason\nclass FinishReason(Enum):\n    # number of generated tokens\
          \ == `max_new_tokens`\n    Length = \"length\"\n    # the model generated\
          \ its end of sequence token\n    EndOfSequenceToken = \"eos_token\"\n  \
          \  # the model generated a text included in `stop_sequences`\n    StopSequence\
          \ = \"stop_sequence\"\n</code></pre>\n<p>Snippet from - <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py\"\
          >https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py</a></p>\n\
          <ol start=\"3\">\n<li>Sometimes it feels like it is generating this garbage\
          \ token to fill max token length, and when max token is reached by logic\
          \ it stops.</li>\n</ol>\n"
        raw: "Sure, I will have a look at sampling report. \nMy observations so far\
          \ around this issue is - \n1. For 100 random prompts 40-50 times this issue\
          \ pops up. (Tested this for 3 weeks)\n2. This happens when the api finish\
          \ reason is length. \n\n```\n# Generation finish reason\nclass FinishReason(Enum):\n\
          \    # number of generated tokens == `max_new_tokens`\n    Length = \"length\"\
          \n    # the model generated its end of sequence token\n    EndOfSequenceToken\
          \ = \"eos_token\"\n    # the model generated a text included in `stop_sequences`\n\
          \    StopSequence = \"stop_sequence\"\n```\nSnippet from - https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py\n\
          \n3. Sometimes it feels like it is generating this garbage token to fill\
          \ max token length, and when max token is reached by logic it stops."
        updatedAt: '2023-05-11T10:47:00.498Z'
      numEdits: 0
      reactions: []
    id: 645cc7a48a63155046d86973
    type: comment
  author: masterfury
  content: "Sure, I will have a look at sampling report. \nMy observations so far\
    \ around this issue is - \n1. For 100 random prompts 40-50 times this issue pops\
    \ up. (Tested this for 3 weeks)\n2. This happens when the api finish reason is\
    \ length. \n\n```\n# Generation finish reason\nclass FinishReason(Enum):\n   \
    \ # number of generated tokens == `max_new_tokens`\n    Length = \"length\"\n\
    \    # the model generated its end of sequence token\n    EndOfSequenceToken =\
    \ \"eos_token\"\n    # the model generated a text included in `stop_sequences`\n\
    \    StopSequence = \"stop_sequence\"\n```\nSnippet from - https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py\n\
    \n3. Sometimes it feels like it is generating this garbage token to fill max token\
    \ length, and when max token is reached by logic it stops."
  created_at: 2023-05-11 09:47:00+00:00
  edited: false
  hidden: false
  id: 645cc7a48a63155046d86973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-05-11T10:47:09.000Z'
    data:
      edited: true
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: "<p>I was facing issues when using prompt as below:</p>\n<pre><code>&lt;|prompter|&gt;\n\
          {assistant instruction like you are helpful for a purpose}\n{context 1:\
          \ Some text}\n{context 2: Some text}\n{context 3: Some text}\n\nBased on\
          \ above context, answer below question:\n{some question}\n&lt;|endoftext|&gt;&lt;|assistant|&gt;\n\
          </code></pre>\n<pre><code>{\n                do_sample: true,\n        \
          \        temperature: 0.1,\n                max_new_tokens: 1024,\n    \
          \            return_full_text: false,\n                repetition_penalty:\
          \ 1.1,\n                num_beams: 1,\n                seed: 13413423,\n\
          \                top_p: 0.75,\n                typical_p: 0.95,\n      \
          \          top_k: 45,\n                stop: [\n                    '&lt;|endoftext|&gt;'\n\
          \                ],\n}\n</code></pre>\n"
        raw: "I was facing issues when using prompt as below:\n\n```\n<|prompter|>\n\
          {assistant instruction like you are helpful for a purpose}\n{context 1:\
          \ Some text}\n{context 2: Some text}\n{context 3: Some text}\n\nBased on\
          \ above context, answer below question:\n{some question}\n<|endoftext|><|assistant|>\n\
          ```\n\n\n```\n{\n                do_sample: true,\n                temperature:\
          \ 0.1,\n                max_new_tokens: 1024,\n                return_full_text:\
          \ false,\n                repetition_penalty: 1.1,\n                num_beams:\
          \ 1,\n                seed: 13413423,\n                top_p: 0.75,\n  \
          \              typical_p: 0.95,\n                top_k: 45,\n          \
          \      stop: [\n                    '<|endoftext|>'\n                ],\n\
          }\n```"
        updatedAt: '2023-05-11T10:48:45.936Z'
      numEdits: 1
      reactions: []
    id: 645cc7adf36ed281faba19e9
    type: comment
  author: gsaivinay
  content: "I was facing issues when using prompt as below:\n\n```\n<|prompter|>\n\
    {assistant instruction like you are helpful for a purpose}\n{context 1: Some text}\n\
    {context 2: Some text}\n{context 3: Some text}\n\nBased on above context, answer\
    \ below question:\n{some question}\n<|endoftext|><|assistant|>\n```\n\n\n```\n\
    {\n                do_sample: true,\n                temperature: 0.1,\n     \
    \           max_new_tokens: 1024,\n                return_full_text: false,\n\
    \                repetition_penalty: 1.1,\n                num_beams: 1,\n   \
    \             seed: 13413423,\n                top_p: 0.75,\n                typical_p:\
    \ 0.95,\n                top_k: 45,\n                stop: [\n               \
    \     '<|endoftext|>'\n                ],\n}\n```"
  created_at: 2023-05-11 09:47:09+00:00
  edited: true
  hidden: false
  id: 645cc7adf36ed281faba19e9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/pythia-12b-sft-v8-7k-steps
repo_type: model
status: open
target_branch: null
title: Garbage tokens at the end
