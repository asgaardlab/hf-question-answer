!!python/object:huggingface_hub.community.DiscussionWithDetails
author: swapnil3597
conflicting_files: null
created_at: 2024-01-19 07:20:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
      fullname: Swapnil Masurekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swapnil3597
      type: user
    createdAt: '2024-01-19T07:20:33.000Z'
    data:
      edited: false
      editors:
      - swapnil3597
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
          fullname: Swapnil Masurekar
          isHf: false
          isPro: false
          name: swapnil3597
          type: user
        html: '<h3 id="system-info">System Info</h3>

          <p><strong>Text Generation Inference Details:</strong><br>TGI Docker Version:
          <code>text-generation-launcher 1.3.4</code></p>

          <p><strong>Issue Details:</strong><br>I''m able to start TGI server for
          <code>TheBloke/Mixtral-8x7B-v0.1-GPTQ</code> with num shards as 1 and 2,
          but with 4 shards I''m getting </p>

          <p>Command used to start TGI server:</p>

          <pre><code class="language-bash">docker run --gpus all -e HUGGING_FACE_HUB_TOKEN=<span
          class="hljs-variable">$HUGGING_FACE_HUB_TOKEN</span> -p 8080:80 -v /data
          ghcr.io/huggingface/text-generation-inference:latest --model-id TheBloke/Mixtral-8x7B-v0.1-GPTQ
          --quantize gptq --num-shard 4

          </code></pre>

          <p>Getting following issue on terminal:</p>

          <pre><code class="language-bash">2024-01-19T07:13:11.235812Z  INFO text_generation_launcher:
          Args { model_id: <span class="hljs-string">"TheBloke/Mixtral-8x7B-v0.1-GPTQ"</span>,
          revision: None, validation_workers: 2, sharded: None, num_shard: Some(4),
          quantize: Some(Gptq), speculate: None, dtype: None, trust_remote_code: <span
          class="hljs-literal">false</span>, max_concurrent_requests: 128, max_best_of:
          2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens:
          2048, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:
          None, max_waiting_tokens: 20, hostname: <span class="hljs-string">"fc04d78daed4"</span>,
          port: 80, shard_uds_path: <span class="hljs-string">"/tmp/text-generation-server"</span>,
          master_addr: <span class="hljs-string">"localhost"</span>, master_port:
          29500, huggingface_hub_cache: Some(<span class="hljs-string">"/data"</span>),
          weights_cache_override: None, disable_custom_kernels: <span class="hljs-literal">false</span>,
          cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output:
          <span class="hljs-literal">false</span>, otlp_endpoint: None, cors_allow_origin:
          [], watermark_gamma: None, watermark_delta: None, ngrok: <span class="hljs-literal">false</span>,
          ngrok_authtoken: None, ngrok_edge: None, <span class="hljs-built_in">env</span>:
          <span class="hljs-literal">false</span> }

          2024-01-19T07:13:11.235844Z  INFO text_generation_launcher: Sharding model
          on 4 processes

          2024-01-19T07:13:11.235941Z  INFO download: text_generation_launcher: Starting
          download process.

          2024-01-19T07:13:19.270520Z  INFO text_generation_launcher: Files are already
          present on the host. Skipping download.


          2024-01-19T07:13:20.646805Z  INFO download: text_generation_launcher: Successfully
          downloaded weights.

          2024-01-19T07:13:20.647234Z  INFO shard-manager: text_generation_launcher:
          Starting shard rank=0

          2024-01-19T07:13:20.648012Z  INFO shard-manager: text_generation_launcher:
          Starting shard rank=1

          2024-01-19T07:13:20.648628Z  INFO shard-manager: text_generation_launcher:
          Starting shard rank=2

          2024-01-19T07:13:20.648646Z  INFO shard-manager: text_generation_launcher:
          Starting shard rank=3

          2024-01-19T07:13:25.313705Z  WARN text_generation_launcher: Disabling exllama
          v2 and using v1 instead because there are issues when sharding


          2024-01-19T07:13:25.325550Z  WARN text_generation_launcher: Disabling exllama
          v2 and using v1 instead because there are issues when sharding


          2024-01-19T07:13:25.342082Z  WARN text_generation_launcher: Disabling exllama
          v2 and using v1 instead because there are issues when sharding


          2024-01-19T07:13:25.350426Z  WARN text_generation_launcher: Disabling exllama
          v2 and using v1 instead because there are issues when sharding


          2024-01-19T07:13:30.660947Z  INFO shard-manager: text_generation_launcher:
          Waiting <span class="hljs-keyword">for</span> shard to be ready... rank=0

          2024-01-19T07:13:30.661904Z  INFO shard-manager: text_generation_launcher:
          Waiting <span class="hljs-keyword">for</span> shard to be ready... rank=3

          2024-01-19T07:13:30.662078Z  INFO shard-manager: text_generation_launcher:
          Waiting <span class="hljs-keyword">for</span> shard to be ready... rank=2

          2024-01-19T07:13:30.662083Z  INFO shard-manager: text_generation_launcher:
          Waiting <span class="hljs-keyword">for</span> shard to be ready... rank=1

          2024-01-19T07:13:37.771495Z ERROR shard-manager: text_generation_launcher:
          Shard complete standard error output:


          [W socket.cpp:663] [c10d] The client socket has failed to connect to [localhost]:29500
          (errno: 99 - Cannot assign requested address).

          [W socket.cpp:663] [c10d] The client socket has failed to connect to [localhost]:29500
          (errno: 99 - Cannot assign requested address). rank=2

          2024-01-19T07:13:37.771535Z ERROR shard-manager: text_generation_launcher:
          Shard process was signaled to shutdown with signal 7 rank=2

          2024-01-19T07:13:37.867562Z ERROR text_generation_launcher: Shard 2 failed
          to start

          2024-01-19T07:13:37.867594Z  INFO text_generation_launcher: Shutting down
          shards

          2024-01-19T07:13:38.066300Z  INFO shard-manager: text_generation_launcher:
          Shard terminated rank=3

          2024-01-19T07:13:38.128980Z  INFO shard-manager: text_generation_launcher:
          Shard terminated rank=0

          2024-01-19T07:13:39.191216Z  INFO shard-manager: text_generation_launcher:
          Shard terminated rank=1

          </code></pre>

          <h3 id="information">Information</h3>

          <ul>

          <li><input type="checkbox" disabled="" checked=""> Docker</li>

          <li><input type="checkbox" disabled=""> The CLI directly</li>

          </ul>

          <h3 id="tasks">Tasks</h3>

          <ul>

          <li><input type="checkbox" disabled="" checked=""> An officially supported
          command</li>

          <li><input type="checkbox" disabled=""> My own modifications</li>

          </ul>

          <h3 id="reproduction">Reproduction</h3>

          <p>Run Docker command on a 4-GPU server:</p>

          <pre><code class="language-bash"><span class="hljs-built_in">export</span>
          HUGGING_FACE_HUB_TOKEN=<span class="hljs-string">"&lt;your HF token&gt;"</span>

          docker run --gpus all -e HUGGING_FACE_HUB_TOKEN=<span class="hljs-variable">$HUGGING_FACE_HUB_TOKEN</span>
          -p 8080:80 -v /data ghcr.io/huggingface/text-generation-inference:latest
          --model-id TheBloke/Mixtral-8x7B-v0.1-GPTQ --quantize gptq --num-shard 4

          </code></pre>

          <p><strong>Text Generation Inference Docker version Details:</strong><br>TGI
          Docker Version: <code>text-generation-launcher 1.3.4</code></p>

          <h3 id="expected-behavior">Expected behavior</h3>

          <p>TGI server must start.</p>

          '
        raw: "### System Info\r\n\r\n**Text Generation Inference Details:**\r\nTGI\
          \ Docker Version: `text-generation-launcher 1.3.4`\r\n\r\n**Issue Details:**\r\
          \nI'm able to start TGI server for `TheBloke/Mixtral-8x7B-v0.1-GPTQ` with\
          \ num shards as 1 and 2, but with 4 shards I'm getting \r\n\r\nCommand used\
          \ to start TGI server:\r\n```bash\r\ndocker run --gpus all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN\
          \ -p 8080:80 -v /data ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id TheBloke/Mixtral-8x7B-v0.1-GPTQ --quantize gptq --num-shard\
          \ 4\r\n```\r\n\r\nGetting following issue on terminal:\r\n```bash\r\n2024-01-19T07:13:11.235812Z\
          \  INFO text_generation_launcher: Args { model_id: \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\
          , revision: None, validation_workers: 2, sharded: None, num_shard: Some(4),\
          \ quantize: Some(Gptq), speculate: None, dtype: None, trust_remote_code:\
          \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:\
          \ 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048,\
          \ waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens:\
          \ None, max_waiting_tokens: 20, hostname: \"fc04d78daed4\", port: 80, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
          \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
          \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
          \ None, ngrok_edge: None, env: false }\r\n2024-01-19T07:13:11.235844Z  INFO\
          \ text_generation_launcher: Sharding model on 4 processes\r\n2024-01-19T07:13:11.235941Z\
          \  INFO download: text_generation_launcher: Starting download process.\r\
          \n2024-01-19T07:13:19.270520Z  INFO text_generation_launcher: Files are\
          \ already present on the host. Skipping download.\r\n\r\n2024-01-19T07:13:20.646805Z\
          \  INFO download: text_generation_launcher: Successfully downloaded weights.\r\
          \n2024-01-19T07:13:20.647234Z  INFO shard-manager: text_generation_launcher:\
          \ Starting shard rank=0\r\n2024-01-19T07:13:20.648012Z  INFO shard-manager:\
          \ text_generation_launcher: Starting shard rank=1\r\n2024-01-19T07:13:20.648628Z\
          \  INFO shard-manager: text_generation_launcher: Starting shard rank=2\r\
          \n2024-01-19T07:13:20.648646Z  INFO shard-manager: text_generation_launcher:\
          \ Starting shard rank=3\r\n2024-01-19T07:13:25.313705Z  WARN text_generation_launcher:\
          \ Disabling exllama v2 and using v1 instead because there are issues when\
          \ sharding\r\n\r\n2024-01-19T07:13:25.325550Z  WARN text_generation_launcher:\
          \ Disabling exllama v2 and using v1 instead because there are issues when\
          \ sharding\r\n\r\n2024-01-19T07:13:25.342082Z  WARN text_generation_launcher:\
          \ Disabling exllama v2 and using v1 instead because there are issues when\
          \ sharding\r\n\r\n2024-01-19T07:13:25.350426Z  WARN text_generation_launcher:\
          \ Disabling exllama v2 and using v1 instead because there are issues when\
          \ sharding\r\n\r\n2024-01-19T07:13:30.660947Z  INFO shard-manager: text_generation_launcher:\
          \ Waiting for shard to be ready... rank=0\r\n2024-01-19T07:13:30.661904Z\
          \  INFO shard-manager: text_generation_launcher: Waiting for shard to be\
          \ ready... rank=3\r\n2024-01-19T07:13:30.662078Z  INFO shard-manager: text_generation_launcher:\
          \ Waiting for shard to be ready... rank=2\r\n2024-01-19T07:13:30.662083Z\
          \  INFO shard-manager: text_generation_launcher: Waiting for shard to be\
          \ ready... rank=1\r\n2024-01-19T07:13:37.771495Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\r\n\r\n[W socket.cpp:663] [c10d]\
          \ The client socket has failed to connect to [localhost]:29500 (errno: 99\
          \ - Cannot assign requested address).\r\n[W socket.cpp:663] [c10d] The client\
          \ socket has failed to connect to [localhost]:29500 (errno: 99 - Cannot\
          \ assign requested address). rank=2\r\n2024-01-19T07:13:37.771535Z ERROR\
          \ shard-manager: text_generation_launcher: Shard process was signaled to\
          \ shutdown with signal 7 rank=2\r\n2024-01-19T07:13:37.867562Z ERROR text_generation_launcher:\
          \ Shard 2 failed to start\r\n2024-01-19T07:13:37.867594Z  INFO text_generation_launcher:\
          \ Shutting down shards\r\n2024-01-19T07:13:38.066300Z  INFO shard-manager:\
          \ text_generation_launcher: Shard terminated rank=3\r\n2024-01-19T07:13:38.128980Z\
          \  INFO shard-manager: text_generation_launcher: Shard terminated rank=0\r\
          \n2024-01-19T07:13:39.191216Z  INFO shard-manager: text_generation_launcher:\
          \ Shard terminated rank=1\r\n```\r\n\r\n\r\n\r\n\r\n\r\n### Information\r\
          \n\r\n- [X] Docker\r\n- [ ] The CLI directly\r\n\r\n### Tasks\r\n\r\n- [X]\
          \ An officially supported command\r\n- [ ] My own modifications\r\n\r\n\
          ### Reproduction\r\n\r\nRun Docker command on a 4-GPU server:\r\n\r\n```bash\r\
          \nexport HUGGING_FACE_HUB_TOKEN=\"<your HF token>\"\r\ndocker run --gpus\
          \ all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN -p 8080:80 -v /data\
          \ ghcr.io/huggingface/text-generation-inference:latest --model-id TheBloke/Mixtral-8x7B-v0.1-GPTQ\
          \ --quantize gptq --num-shard 4\r\n```\r\n\r\n**Text Generation Inference\
          \ Docker version Details:**\r\nTGI Docker Version: `text-generation-launcher\
          \ 1.3.4`\r\n\r\n### Expected behavior\r\n\r\nTGI server must start."
        updatedAt: '2024-01-19T07:20:33.707Z'
      numEdits: 0
      reactions: []
    id: 65aa22c1ad7763e1d36ddb52
    type: comment
  author: swapnil3597
  content: "### System Info\r\n\r\n**Text Generation Inference Details:**\r\nTGI Docker\
    \ Version: `text-generation-launcher 1.3.4`\r\n\r\n**Issue Details:**\r\nI'm able\
    \ to start TGI server for `TheBloke/Mixtral-8x7B-v0.1-GPTQ` with num shards as\
    \ 1 and 2, but with 4 shards I'm getting \r\n\r\nCommand used to start TGI server:\r\
    \n```bash\r\ndocker run --gpus all -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN\
    \ -p 8080:80 -v /data ghcr.io/huggingface/text-generation-inference:latest --model-id\
    \ TheBloke/Mixtral-8x7B-v0.1-GPTQ --quantize gptq --num-shard 4\r\n```\r\n\r\n\
    Getting following issue on terminal:\r\n```bash\r\n2024-01-19T07:13:11.235812Z\
    \  INFO text_generation_launcher: Args { model_id: \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\
    , revision: None, validation_workers: 2, sharded: None, num_shard: Some(4), quantize:\
    \ Some(Gptq), speculate: None, dtype: None, trust_remote_code: false, max_concurrent_requests:\
    \ 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length:\
    \ 1024, max_total_tokens: 2048, waiting_served_ratio: 1.2, max_batch_prefill_tokens:\
    \ 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname: \"fc04d78daed4\"\
    , port: 80, shard_uds_path: \"/tmp/text-generation-server\", master_addr: \"localhost\"\
    , master_port: 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
    \ None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling:\
    \ None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin:\
    \ [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken:\
    \ None, ngrok_edge: None, env: false }\r\n2024-01-19T07:13:11.235844Z  INFO text_generation_launcher:\
    \ Sharding model on 4 processes\r\n2024-01-19T07:13:11.235941Z  INFO download:\
    \ text_generation_launcher: Starting download process.\r\n2024-01-19T07:13:19.270520Z\
    \  INFO text_generation_launcher: Files are already present on the host. Skipping\
    \ download.\r\n\r\n2024-01-19T07:13:20.646805Z  INFO download: text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2024-01-19T07:13:20.647234Z  INFO shard-manager:\
    \ text_generation_launcher: Starting shard rank=0\r\n2024-01-19T07:13:20.648012Z\
    \  INFO shard-manager: text_generation_launcher: Starting shard rank=1\r\n2024-01-19T07:13:20.648628Z\
    \  INFO shard-manager: text_generation_launcher: Starting shard rank=2\r\n2024-01-19T07:13:20.648646Z\
    \  INFO shard-manager: text_generation_launcher: Starting shard rank=3\r\n2024-01-19T07:13:25.313705Z\
    \  WARN text_generation_launcher: Disabling exllama v2 and using v1 instead because\
    \ there are issues when sharding\r\n\r\n2024-01-19T07:13:25.325550Z  WARN text_generation_launcher:\
    \ Disabling exllama v2 and using v1 instead because there are issues when sharding\r\
    \n\r\n2024-01-19T07:13:25.342082Z  WARN text_generation_launcher: Disabling exllama\
    \ v2 and using v1 instead because there are issues when sharding\r\n\r\n2024-01-19T07:13:25.350426Z\
    \  WARN text_generation_launcher: Disabling exllama v2 and using v1 instead because\
    \ there are issues when sharding\r\n\r\n2024-01-19T07:13:30.660947Z  INFO shard-manager:\
    \ text_generation_launcher: Waiting for shard to be ready... rank=0\r\n2024-01-19T07:13:30.661904Z\
    \  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready...\
    \ rank=3\r\n2024-01-19T07:13:30.662078Z  INFO shard-manager: text_generation_launcher:\
    \ Waiting for shard to be ready... rank=2\r\n2024-01-19T07:13:30.662083Z  INFO\
    \ shard-manager: text_generation_launcher: Waiting for shard to be ready... rank=1\r\
    \n2024-01-19T07:13:37.771495Z ERROR shard-manager: text_generation_launcher: Shard\
    \ complete standard error output:\r\n\r\n[W socket.cpp:663] [c10d] The client\
    \ socket has failed to connect to [localhost]:29500 (errno: 99 - Cannot assign\
    \ requested address).\r\n[W socket.cpp:663] [c10d] The client socket has failed\
    \ to connect to [localhost]:29500 (errno: 99 - Cannot assign requested address).\
    \ rank=2\r\n2024-01-19T07:13:37.771535Z ERROR shard-manager: text_generation_launcher:\
    \ Shard process was signaled to shutdown with signal 7 rank=2\r\n2024-01-19T07:13:37.867562Z\
    \ ERROR text_generation_launcher: Shard 2 failed to start\r\n2024-01-19T07:13:37.867594Z\
    \  INFO text_generation_launcher: Shutting down shards\r\n2024-01-19T07:13:38.066300Z\
    \  INFO shard-manager: text_generation_launcher: Shard terminated rank=3\r\n2024-01-19T07:13:38.128980Z\
    \  INFO shard-manager: text_generation_launcher: Shard terminated rank=0\r\n2024-01-19T07:13:39.191216Z\
    \  INFO shard-manager: text_generation_launcher: Shard terminated rank=1\r\n```\r\
    \n\r\n\r\n\r\n\r\n\r\n### Information\r\n\r\n- [X] Docker\r\n- [ ] The CLI directly\r\
    \n\r\n### Tasks\r\n\r\n- [X] An officially supported command\r\n- [ ] My own modifications\r\
    \n\r\n### Reproduction\r\n\r\nRun Docker command on a 4-GPU server:\r\n\r\n```bash\r\
    \nexport HUGGING_FACE_HUB_TOKEN=\"<your HF token>\"\r\ndocker run --gpus all -e\
    \ HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN -p 8080:80 -v /data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id TheBloke/Mixtral-8x7B-v0.1-GPTQ --quantize gptq --num-shard 4\r\n\
    ```\r\n\r\n**Text Generation Inference Docker version Details:**\r\nTGI Docker\
    \ Version: `text-generation-launcher 1.3.4`\r\n\r\n### Expected behavior\r\n\r\
    \nTGI server must start."
  created_at: 2024-01-19 07:20:33+00:00
  edited: false
  hidden: false
  id: 65aa22c1ad7763e1d36ddb52
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Mixtral-8x7B-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Unable to start TGI service for TheBloke/Mixtral-8x7B-v0.1-GPTQ with num_shard
  as 4
