!!python/object:huggingface_hub.community.DiscussionWithDetails
author: seabasshn
conflicting_files: null
created_at: 2023-12-15 18:35:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c3abd5cfc948a137b40ab4c73088859f.svg
      fullname: Sebastian Bustillo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seabasshn
      type: user
    createdAt: '2023-12-15T18:35:49.000Z'
    data:
      edited: false
      editors:
      - seabasshn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4335114657878876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c3abd5cfc948a137b40ab4c73088859f.svg
          fullname: Sebastian Bustillo
          isHf: false
          isPro: false
          name: seabasshn
          type: user
        html: '<p>I''m trying to deploy this model on SageMaker but I keep getting
          this error:</p>

          <p>Error: ShardCannotStart and ValueError: Unsupported model type mixtral</p>

          <p>Traceback (most recent call last):<br>  File "/opt/conda/bin/text-generation-server",
          line 8, in <br>    sys.exit(app())<br>  File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in <strong>call</strong><br>    return get_command(self)(*args,
          **kwargs)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1157, in <strong>call</strong><br>    return self.main(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/core.py", line 778, in main<br>    return
          _main(<br>  File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main<br>    rv = self.invoke(ctx)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1688, in invoke<br>    return _process_result(sub_ctx.command.invoke(sub_ctx))<br>  File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1434, in invoke<br>    return
          ctx.invoke(self.callback, **ctx.params)<br>  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 783, in invoke<br>    return __callback(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.9/site-packages/typer/main.py", line 683, in wrapper<br>    return
          callback(**use_params)  # type: ignore<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve<br>    server.serve(<br>  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve<br>    asyncio.run(<br>  File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run<br>    return loop.run_until_complete(main)<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete<br>    self.run_forever()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever<br>    self._run_once()<br>  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once<br>    handle._run()<br>  File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run<br>    self._context.run(self._callback, *self._args)<br>Traceback
          (most recent call last): File "/opt/conda/bin/text-generation-server", line
          8, in  sys.exit(app()) File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in <strong>call</strong> return get_command(self)(*args, **kwargs)
          File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1157,
          in <strong>call</strong> return self.main(*args, **kwargs) File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 778, in main return _main( File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main rv = self.invoke(ctx) File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx))
          File "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1434,
          in invoke return ctx.invoke(self.callback, **ctx.params) File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 783, in invoke return __callback(*args, **kwargs) File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 683, in wrapper return callback(**use_params) # type: ignore File "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve server.serve( File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve asyncio.run( File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run return loop.run_until_complete(main) File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete self.run_forever() File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever self._run_once() File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once handle._run() File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run self._context.run(self._callback, *self._args)</p>

          <p>Here''s my code:</p>

          <p>import json<br>import sagemaker<br>import boto3<br>from sagemaker.huggingface
          import HuggingFaceModel, get_huggingface_llm_image_uri</p>

          <p>try:<br>    role = sagemaker.get_execution_role()<br>except ValueError:<br>    iam
          = boto3.client(''iam'')<br>    role = iam.get_role(RoleName=''sagemaker_execution_role'')[''Role''][''Arn'']</p>

          <h1 id="hub-model-configuration-httpshuggingfacecomodels">Hub Model configuration.
          <a href="https://huggingface.co/models">https://huggingface.co/models</a></h1>

          <p>hub = {<br>    ''HF_MODEL_ID'':''TheBloke/Mixtral-8x7B-v0.1-GPTQ'',<br>    ''SM_NUM_GPUS'':
          json.dumps(1)<br>}</p>

          <h1 id="create-hugging-face-model-class">create Hugging Face Model Class</h1>

          <p>huggingface_model = HuggingFaceModel(<br>    image_uri=get_huggingface_llm_image_uri("huggingface",version="1.1.0"),<br>    env=hub,<br>    role=role,<br>)</p>

          <h1 id="deploy-model-to-sagemaker-inference">deploy model to SageMaker Inference</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type="ml.g5.2xlarge",<br>    container_startup_health_check_timeout=600,<br>  )</p>

          <h1 id="send-request">send request</h1>

          <p>predictor.predict({<br>    "inputs": "Mon nom est Julien et j''aime",<br>})</p>

          '
        raw: "I'm trying to deploy this model on SageMaker but I keep getting this\
          \ error:\r\n\r\nError: ShardCannotStart and ValueError: Unsupported model\
          \ type mixtral\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \nTraceback (most recent call last): File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module> sys.exit(app()) File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__ return get_command(self)(*args, **kwargs) File \"\
          /opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\
          \ return self.main(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main return _main( File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main rv = self.invoke(ctx) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx))\
          \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke return ctx.invoke(self.callback, **ctx.params) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 783, in invoke return __callback(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper return callback(**use_params) # type: ignore File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve server.serve( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve asyncio.run( File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run return loop.run_until_complete(main) File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete self.run_forever() File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever self._run_once() File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once handle._run() File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run self._context.run(self._callback, *self._args)\r\n\r\n\
          Here's my code:\r\n\r\nimport json\r\nimport sagemaker\r\nimport boto3\r\
          \nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
          \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\
          \n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
          \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub =\
          \ {\r\n\t'HF_MODEL_ID':'TheBloke/Mixtral-8x7B-v0.1-GPTQ',\r\n\t'SM_NUM_GPUS':\
          \ json.dumps(1)\r\n}\r\n\r\n\r\n\r\n# create Hugging Face Model Class\r\n\
          huggingface_model = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"1.1.0\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\
          \r\n# deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.2xlarge\",\r\n\t\
          container_startup_health_check_timeout=600,\r\n  )\r\n  \r\n# send request\r\
          \npredictor.predict({\r\n\t\"inputs\": \"Mon nom est Julien et j'aime\"\
          ,\r\n})"
        updatedAt: '2023-12-15T18:35:49.953Z'
      numEdits: 0
      reactions: []
    id: 657c9c85e6759943571374d3
    type: comment
  author: seabasshn
  content: "I'm trying to deploy this model on SageMaker but I keep getting this error:\r\
    \n\r\nError: ShardCannotStart and ValueError: Unsupported model type mixtral\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\nTraceback\
    \ (most recent call last): File \"/opt/conda/bin/text-generation-server\", line\
    \ 8, in <module> sys.exit(app()) File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__ return get_command(self)(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1157, in __call__ return self.main(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main return _main( File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main rv = self.invoke(ctx) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx))\
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in\
    \ invoke return ctx.invoke(self.callback, **ctx.params) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 783, in invoke return __callback(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper return callback(**use_params) # type: ignore File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve server.serve( File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve asyncio.run( File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run return loop.run_until_complete(main) File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete self.run_forever() File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever self._run_once() File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once handle._run() File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run self._context.run(self._callback, *self._args)\r\n\r\nHere's\
    \ my code:\r\n\r\nimport json\r\nimport sagemaker\r\nimport boto3\r\nfrom sagemaker.huggingface\
    \ import HuggingFaceModel, get_huggingface_llm_image_uri\r\n\r\ntry:\r\n\trole\
    \ = sagemaker.get_execution_role()\r\nexcept ValueError:\r\n\tiam = boto3.client('iam')\r\
    \n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\n\
    \r\n# Hub Model configuration. https://huggingface.co/models\r\nhub = {\r\n\t\
    'HF_MODEL_ID':'TheBloke/Mixtral-8x7B-v0.1-GPTQ',\r\n\t'SM_NUM_GPUS': json.dumps(1)\r\
    \n}\r\n\r\n\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"\
    ),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1,\r\n\tinstance_type=\"\
    ml.g5.2xlarge\",\r\n\tcontainer_startup_health_check_timeout=600,\r\n  )\r\n \
    \ \r\n# send request\r\npredictor.predict({\r\n\t\"inputs\": \"Mon nom est Julien\
    \ et j'aime\",\r\n})"
  created_at: 2023-12-15 18:35:49+00:00
  edited: false
  hidden: false
  id: 657c9c85e6759943571374d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Mixtral-8x7B-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ValueError: Unsupported model type mixtral'
